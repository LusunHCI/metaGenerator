Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper proposes to find action sufficient representation which might be helpful for downstream tasks. The idea is that states and/or observations often include irrelevant for actions information which can be removed by extracting only reverent information. The method utilizes the concepts of Identifiability, mutual information and sparsity. 1.It is challenging to learn "the environment model and ASRs, with random actions." 2.Since $I(\tilde{s}^{ASR}; a_t, R_{t+1}) $ ... "we can estimate ASRs by minimizing $H(a_t | \tilde{s}^{ASR}, R_{t+1})$". That is not true in general. 3."and meanwhile minimizing the dimensionality of ASRs with sparsity constraints:" The first term is supposed to give the minimal representation regardless the sparsity. Please show what terms in (3) correspond to $I[X; \tilde{X}]$, to $I[\tilde{X}; Y]$ and to $\frac{\delta I[\tilde{X}; Y]}{\delta I[X; \tilde{X}]}$ in the original IB paper you cite. I suggest either to use the original definitions or  to formulate your definitions of "MINIMAL SUFFICIENT". The later requires to show the properties of a new definition and its correspondence to the existing ones. What component in the objective (4) does require "Deconvolution"? 8. typo: "hyperparameter turning." 9 .Eq.1 shows $a_{t 1}$ directly  affects $s_t$. Figure 2 does not show this connection. It might be an unfair comparison because these methods are not looking for sufficient state representations. There might be other factors which contribute to the advantage of your particular architecture type and training. 11.Related to  10  above: please evaluate explicitly the quality of the proposed "sufficient state representations" either theoretically or      empirically. how      does it look for other dynamical control systems? Is it possible to show the     gap of the lower bound on simple examples where one can estimate the relevant mutual information by  e.g.,      clustering/counting/Krasnov MI estimator/etc? 12.I agree "Common strategies for such state representation learning include reconstructing the observation" is not directly relevant for the current work. The relevant works for the comparison are works which do not reconstruct the observation. the discussed problem is important and the proposed solution is reasonable. my concerns are a) the lack of direct comparisons to the relevant methods for minimal state representations, b) the lack of evaluation of the quality of the solution (See please 11 above), c) the lack of the definitions for the essential quantities used in the paper, e.g., What definition of sufficiency do you use?<|endoftext|>This paper considered the problem of compressing the state space representation in a manner that is sufficient for control and of smaller dimension than the original state space. This is done by first running the system to collect data and then learning structural constraints that are sufficient for describing the reward random variable. The resulting low dimensional MDP can be combined with classic model based RL to potentially improve sample efficiency. That said, I must first admit that I found the technical exposition particular hard to follow, particularly from a notional perspective and so I may have missed a few key details. The variational autoencoder objective presented has a number of parameters which the paper does not specify how to select. While I m happy to consider them to be hyperparameters, the exact selection of these parameters seems important when claiming that the resulting state space compression will me minimal. I am particularly worried due to the lack of unique solution, see for example the Linear Gaussian case. 2.I am also confused with the interpretation of the graph structure variables D. They are introduced as binary variables, but then the Gaussian section claims that they denote both the connect *and* the strength of the connection? 3.Its unclear to me if the experiments achieve the goal of using less data compared to the best baseline. For example, reading the appendix, is seems to be that 10k rollouts of 500 steps were used to learn the ASR! Shouldn t this overhead be included in the learning rate figures? 4.The Dyna algorithm proposed is usually outperformed by the Prioritized sweep algorithm right? Would this significantly change the results of the experiments? While I believe this paper has a strong theoretical contribution, I believe in its current state, it is unclear if it paper achieves its goals of minimal state representation (how to tune the hyper parameters) and reducing the sample complexity. These concerns may stem from my misunderstanding, and I would be happy to revise my review if corrected.<|endoftext|>The paper proposes a new state representation learning method for RL, called Action Sufficient State Representations (ASRs) to learn minimal and sufficient state representations for downstream decision making tasks. Different from other related work, they explicitly characterize structural relationships among variables (i.e., state features, observation, reward and action) in MDP. The proposed algorithm is empirically evaluated in conjunction with the model free, model based algorithm in VizDoom and CarRacing. Both the structure constraint and ASR are learned from trajectory data in an end to end fashion. The structure constraint learned is insightful and is of potential to obtain interpretable representation. &nbsp;Weaknesses:  The methodology is not clear enough, somewhat hard to read. &nbsp;My main concern is on the methodology. &nbsp;First, I am not sure whether I well understand ‘the condition of maximizing cumulative reward’ when ASR is defined. Is this to say the ASR is defined for optimal action selection (i.e., the action sequence in Figure 1 should be from the optimal policy)? If so, there should be a gap between ASR and the representation learned based on the trajectory data obtained by an arbitrary policy. If I do not understand it right, please correct me and explain more about how such reward maximization is reflected during the representation training process. &nbsp;To maximizing the mutual information between the action and the given cumulative reward, the second conditional entropy term is considered. Why is the first term neglected? The authors may provide some more explanation on why the minimization of condition entropy is transformed into a cross entropy. Besides, how is the cross entropy $H\left(q_{\phi}, p_{\alpha}\right)$ calculated since $ q_{\phi}$ is a conditional state distribution while $ p_{\alpha}$ is a conditional action distribution? &nbsp;In Equation 4, $\log p_{\theta}\left(o_{t+1} \mid \tilde{\vec{s}}_{t}\right)+\log p_{\theta}\left(r_{t+2} \mid \tilde{\vec{s}}_{t}, a_{t+1}\right)$, can the authors explain these two terms of prediction? “The preprocessor architecture is presented in Figure 17, which takes as input the images, actions and rewards, and its output acts as the input to LSTM.” Figure 17 should probably be modified to Figure 12. On page 18, in the Appendix G section: the paper does not describe Figure 17 in detail. I vote for a borderline rejection at present due to my concerns and questions listed above.<|endoftext|>This subset of the state space, called the action sufficient state representation of the task and environment, can then be used to learn a policy from fewer samples than using the entire reconstructed state space or a predictive world model would need. The general approach of the paper is very appealing from the point of view of learning a latent space representation of the world in a principled and possibly incremental manner. Moreover, by learning the sparsity structure of the generative model, it becomes possible to weed out components of the state descriptor that might be relevant to the overall dynamics of the environment, but are not relevant to the reward function that is being optimized for a particular task in this environment. If a relatively small set of relevant components of the state vector can be identified reliably, it stands to reason that the decision problem can be solved much more efficiently by paying attention only to them, much like humans solve decision problems by ignoring all components of their mental interpretation of the world around them but the very few that appear to be relevant to the decision problem at hand. Although this idea by itself is not new, the specific method of learning the sparsity structure of a DBN appears to be new and highly original. The authors achieve sparsity by including what they call a structural constraint on the DBN expressed as a sum of various regularization terms involving the edges of the DBN, into the objective function to be minimized by means of a so "structured" variational autoencoder. The resulting rather elaborate loss function (Eq.4) contains as many as 8 different weight/regulation parameters, and one concern is how to set them   maybe the authors can provide some guidelines? For example, what would happen if the outputs of two completely independent linear state space models are concatenated, and the cost/reward of a task depends on the state vector of only one of the models   will the proposed algorithm correctly construct an ASR with reduced size? The empirical verification is on two difficult decision problems with very high dimensional observation spaces. (Although, maybe it would be worth stating the exact number of dimensions?) The learning curves show clearly that the proposed method learns faster and better than several recent methods that can be considered to define the state of the art. The paper is very well written, and although the literature on learning latent models for reinforcement learning is vast, I believe the list of included references to that literature is representative and probably sufficient. The paper proposes a novel method for learning generative models of environments with high dimensional observations, identifying the sparsity structure of these models, and selecting only a subset of the state variables relevant to a particular decision problem for use by RL algorithms when computing the optimal policy.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; It adopts VQ network for content encoding, and Cross Attention for Style and Linking Attention at decoder. It is shown to be domain agonistic   worked well in image and audio domain. Interesting idea of content and style disentanglement; validated on both image and audio domain. The paper proposes a new way to define style (for a given dataset) using permutation invariant (P.I) network.<|endoftext|>This paper proposes an unsupervised framework to learn content and style representations of structured data, such as texts, images and speeches. The proposed model performs relatively well on both speech and image domains. This paper addresses an important problem and proposed a framework to learn content and style representations. Weaknesses:W1: It is difficult to follow the paper some notations lack details.<|endoftext|>This paper proposes a framework for learning disentangled representations of content and style in an unsupervised way. Experiments are conducted on speech and image datasets. The idea of the paper is interesting and potentially useful for the community.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; In this paper, the authors proposed to replace the classical activation units in MAML with stochastic local winner takes all (LWTA) units. The authors argued the biologically motivated LWTA units would lead to better performance in the few shot setting where the support set in the target task is small, in which case the embedding representation of the target task would be noisy. The paper is interesting, but is not very clear. I suggest the authors do a thorough revision to make the paper more approachable. 2.What are the definitions of [\xi]_{r, j} and [\xi]_r? I think the paper is interesting, but the presentation needs to be greatly improved.<|endoftext|>This paper proposes a stochastic local winner takes all (SLWTA) approach to learn data driven (stochastic) sparsity. The authors propose a meta learning algorithm for their SLWTA approach. [Strengths]* The idea to embed data driven/learned sparsity in a meta learning framework is really interesting. It is a nice idea and this is backed up by competitive performance of the algorithm compared to original instantiations of MAML. As the reviewer of this paper, I feel that I have sufficient information and understanding to implement it myself[Weaknesses]* I find the methodological advances proposed in the manuscript too incremental for publication at ICLR. I like the idea to embed an stochastic LWTA approach into meta learning. However, the technical novelty of the paper is lacking and does not offer enough methodological advancement to be a strong candidate for acceptance. **** Post rebuttal ****I have decided to increase my score from 3 to 5.<|endoftext|>The paper proposes to use neural networks with so called "stochastic local winner takes all (LWTA)" activations in the place of standard ReLU activations. These stochastic LWTA activations result in a model with sparse representations. ## Strengths* Strong empiracle results * Great set of ablation studies## Weaknesses### Lack of noveltyThere already exist many probabilistic approaches to meta learning which take uncertainty into account, which the authors have failed to acknowledge. While the results on these tasks are good, more tasks are required to paint an accurate picture of the method. – I suspect this line should be at the end of sec 3.3, rather than sec 3.1. ### Lack of clarityOverall I found the presentation to be confusing in many places and occasionally incorrect. ICLR (Poster) 2019While this paper does have some strengths (strong empirical results, great set of ablation studies), it is let down by many issues of clarity, a lack of novelty, and a somewhat narrow set of main experiments. With this in mind, I have increased my score from 3 to 5. 4.It is not clear why it is a good idea to use a fixed competition function that always chooses the maximum activation. 20.What is the architecture that was used for the baseline methods? Would I and O not be more clear?<|endoftext|>This method demonstrated superior performance over MAML and FOMAML over standard benchmarks such as Omniglot and Mini ImageNet. 1.The novelty of the paper is limited. It applies stochastic LWTA to the standard MAML framework. Why does the "sparse representation" induced by the LWTA function help? However, I would still suggest the authors compare their method to other recent state of the art meta learning methods. However, I think the ablation study needs to be broken down into a deterministic LWTA function and a stochastic nature. It s not clear actually the model benefits from which part. These experiments are limited to small model settings without discussion of the role of batch size in the play. The idea has limited novelty and limited insight.
Reject; rating score: 3; rating score: 6; rating score: 6; The paper proposes DecentCEM which uses parallel instances of CEM to learn optimal policies for problems that contain multi modal optimal actions. Instead of using a single policy as sampling distribution and optimizing the actions in the vicinity of this action trajectory, the DecentCEM method uses parallel with multiple policies and cem optimizers. While this problem theoretically exists and one can describe a motivational example where this problem is relevant, the problem is not relevant for the performed model based RL experiments on the OpenAI tasks. The performed experiments of the authors do not show much of an improvement of DecentCEM over the other methods. Therefore, the proposed algorithm does not improve the empirical performance of Poplin. Using n parallel policies + CEM is a trivial extension of Poplin with uncertain benefits. Besides the poor motivation of the approach and limited experimental results, the paper is well written and easy to follow.<|endoftext|>This paper proposes an extension of the Cross Entropy Method (CEM) for optimisation that consists in an ensemble of multiple standard CEM instances, each one being optimised independently, and the solution at each step is the output of the top performer CEM at that step. Simulation results in a continuous control benchmark show that the proposed algorithm matches or outperforms previous CEM variants. The authors also use a toy problem to illustrate how using an ensemble can help to escape local minimum. In particular, having included SAC as a reference is appreciated. The paper is generally well written and easy to follow. Although the presented algorithm is novel, the idea of using ensembles of optimisers is not. I lack a clear practical motivation for the CEM approach.<|endoftext|>This paper studies a novel CEM method for model based RL. While previous approaches used a centralized method (based on a unimodal Gaussian or Gaussian mixture), they propose a decentralized CEM, where each instance independently tracks its own data and top k estimates. As mentioned above, I like the idea (it is straightforward and easy to grasp, yet well motivated), the paper is very clearly written, has clear notation, and gives good intuitive illustration of the idea. I find it really hard to judge this paper, since it has strong and weak points. * The paper is well written, very clear, equations are concise. The appendix is really detailed as well, with full hyperparameter settings. I am convinced another researcher could replicate these experiments. * Toy experiment: I find it suprising that CEM GMM looses the global optimum. In short, I lack some hyperparameter choices you make in the toy experiment. Each instance has a different distribution over policies right, and I can only evaluate one of these policies per environment interaction?
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 10; They go on to describe novel black box sequence design algorithms induced by known LFI algorithms. The link described in this work is interesting and the novel algorithms proposed contain significant differences to existing sequence optimization techniques. Empirical results support claims that these novel algorithms are interesting and bear consideration for future design efforts. Training regression models instead of likelihood models on protein sequence space could yield useful empirical advances, making this an interesting contribution. Results align with the conclusions of the paper. On the other hand, the quantities (E, m) are a set of sequence and a sequence and do not have such a clear distinction.<|endoftext|># Major comments1) The paper is missing a related works section with an overview of existing methods for sequence design and likelihood free inference, and how they relate to the methods that were introduced in this paper. * Important details about the proposed methods and performed experiments are missing, which makes it hard to understand and assess. Both approaches update a VAE iteratively by fitting it on the top scoring sequences.<|endoftext|>The paper builds up a variety of optimization methods, building on a line of work in the LFI literature. Along these lines, the paper has far too few details about the actual optimization approaches. There is no discussion of how this MCMC is done. It would be very helpful if you isolated these by performing oracle experiments, where you assume that the forward model is exactly correct. The paper has some interesting methods, and the connection to LFI is helpful.<|endoftext|>The authors also present a number of "composite" methods that combine ideas from a number of these approaches. I found the paper to be extremely clear and well written, providing an excellent review of both the LFI and black box sequence design literatures and drawing clear, clean parallels between the two fields. The proposed algorithms are all sensible and seem to work well on the empirical tasks, and the connection between the two fields seems like a fruitful area for further exploration (especially using the presented framework).
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; It does seem necessary to do this apply this to segmentation, and the authors give a good description of the reasoning. This "redesign" seems like an important novelty of the submission. But the most recent work on this that they cite, such as Elsen et. The method is interesting and presented well.<|endoftext|>The authors might be the first to formulate it as an actual research problem. Furthermore, considering the conceptual similarity to several prior methods (see the discussion above), I believe that the technical novelty of the paper is also quite limited. I m sure there are many other methods that do this, but these are the first ones / most popular ones that come to mind. + The motivation behind the proposed problem is clear and convincing.<|endoftext|>POST REBUTTAL  The authors have addressed most of my concerns in their response. With this, I will raise my rating to positive inclined. Strengths:+ The task setup is interesting and of great practical value, as well motivated in the paper.<|endoftext|>Is that the end to end latency? But as mentioned by the authors, this strategy was originally proposed by Xie et al., 2020 (ECCV). My main concern is that the technical novelty of this work is somewhat limited. Other than that, this work is solid and has comprehensive results and analysis.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; "Rethinking few shot image classification: a good embedding is all you need?." They try to provide an explanation for such behavior based on NC. They theoretically show that if NC happens on a training set, then it generalizes to unseen samples from the same task under natural assumptions. This would imply that representations learned during training are adaptable to other tasks. #### Strengths   The paper provides a possible explanation as to why representations learned over many classes by a lone classifier can serve as a competitive solution to few shot learning problems, by exploring the idea of neural collapse. Overall the explanation(hypothesis) provided for the success of foundation models in few shot settings due to NC, is well motivated by the authors and the general flow of the paper makes sense.<|endoftext|>Summary:The paper study neural collapse for classification in transfer learning setting, where the large and overparameterized foundation models trained over many classes could provide feature maps that are transferable to new, unseen classes with few shot learning . The paper demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and to new classes. The angle is new and the experiments are done neatly with a good amount of hard works of large scale computing. The paper did solid experiments on Neural collapse on unseen classes for transfer learning.<|endoftext|>This paper studies the ability of foundation models for classification to learn representations that are transferable to new, unseen classes in the perspective of neural collapse. As a result, representation learned by foundation models can be easily classified in the few shot setting. Strengths:+ This paper is well written, and it is enjoyable to read. It is significant that this paper shows that neural collapse generalizes to new samples from the training classes and new classes both theoretically and empirically. + Neural collapse is an emergent phenomenon in deep learning.<|endoftext|>This paper analyzes neural collapse in the transfer learning setting using foundation models. This paper provides some theories on the neural collapse in the transfer learning settingWeakness:The theories are not sufficient, see details below:1. 2.Proposition 1 basically claims that the empirical and population version of V_f are close to each other, which is a variant of generalization bound. The establishment of this generalization bound is the key part in this kind of statement, however, this paper neglects this part by directly assuming there exists a generalization bound for the first and second moments of f, which makes the theoretical contributions not sufficient. 3.Proposition 2 is more interesting, but the current results are established with respect to the expectation, and the RHS does not depend on the sample size at all.<|endoftext|>In this work, the authors investigate the success of transfer learning with foundation models from the perspective of the neural collapse phenomenon. More specifically they study the transfer learning capability of foundation models for few shot downstream tasks. The paper provides both theoretical and empirical justifications to elaborate on this point. Note: The paper is heavily inspired by the pre existing literature. do you think that there could be an alternate explanation?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper is well written and clear to understand. Therefore, the evaluation should be substantially improved (besides adding more domains, potentially adding some results from the appendix). The novelty of the approach is limited and the claims have been shown before, but in a different framework.<|endoftext|>This work proposes an ensemble based intrinsic reward to improve exploration in preference based RL. Adding an uncertainty based intrinsic reward to guide preference based RL is intuitive and at its core, I think the underlying idea can improve existing methods. However, I do have concerns about this particular instantiation of the approach. Is this still a meaningful estimate of uncertainty? I encourage the authors to quantitatively compare the amount of exploration when RUNE is used with PEBBLE. Is there noise in the simulation human feedback? These details are important to ascertain the robustness of the method and results. Exploration for preference based RL is an interesting problem, but the current paper has multiple issues that should be addressed.<|endoftext|>With this work, the authors propose a bayesian active learning approach to the problem of reinforcement learning from preferences. Also, the writing and the structure of the paper is clear. I don t think that any of those is challenging. The disagreement method doesn t need to happen over the forward dynamics model but can also happen over the rewards. Although simple, its novelty is marginal. Finally, the experiments, although they support the argument, there are experiments that don t contribute to the discussion (C2)<|endoftext|>This is both unnecessary and distrupts reading flow. On the other hand, the evaluation is limited and the results are not significant. In many of the plots, it is not easy to see a big difference between different methods. The authors also compare with PEBBLE by using 700 feedback instead of 1000.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; Vertical flip prediction is so closely related that it is not so informative as a second task. The proposed method in particular addresses not just the test time fine tuning to do so, but a training time preparation to do so by meta learning. The different parts of the proposed method all help. On the other hand (3) is more a requirement for proposing a defense than an independent contribution. As outlined in the main review, more could be done here to evaluate that the defense is more robust and not superficially interfering with attacks. As such, I encourage the authors to further test the proposed defense, but must recommend rejection of this edition of the work. The last method is especially related as it optimizes self supervised tasks at test time to improve robustness. More importantly, the response does not acknowledge the prior method of runtime masking and cleansing, which likewise updates model parameters. None of these prior methods are included as baselines, but at least SOAP should be, as it also optimizes self supervised tasks during testing.<|endoftext|>The paper introduces a meta adversarial training method to find a good starting point for test time fine tuning. The results are good in the sense that with Meta AT the accuracy improvement is consistent. I am not sure whether it is at all feasible to have the test samples even without labels before head to do self supervised fine tuning. 2.Test time training (TTT) as a fine tuning for a full test set with the full model might be extremely in efficient and compute heavy that many inference platforms can t support. I think the author meant near optimal, hence please rephrase your sentences. 8.Please do refer and compare with the paper [2] as they seem to solve similar issues of adversarial training. The authors proposes a costly training and slow fine tune based testing strategy to get rid of robust overfitting issues associated with AT.<|endoftext|>This paper presents a nice framework of test time fine tuning through self supervision for adversarially trained networks with the purpose of improving the robust accuracy on test data. The test time fine tuning method for better adversarial robustness performance is very interesting. Recently, the test time training techniques have gained popularity mainly due to their improved performance and adaptability to different domains. While the authors presented the method with proper motivation followed by results that shows performance improvement, there are some points I would like to be clarified on as given below. 2)The method analysis subsection (3.4) is not well explained in my view. It s very important to add results of other SOTA adversarial robustness methods that would improve the acceptance of the proposed method. An interesting work that tried to improve adversarial robustness through test time fine tuning using self supervision techniques.<|endoftext|>Propose to improve the generalization and robust accuracy of adversarially trained networks via self supervised test time fine tuning. The exposition was clear, barring a number of typos/grammatical errors which should be resolved in the revision. Overall, a well executed paper. The analysis showed that if these tasks are correlated, self supervised fine tuning will help, but I do not see where this particular statement in the abstract is supported. Paper is well written and lacks any obvious concerns regarding the content, other than the lack of comparison against existing methods for leveraging test time training for adversarial defence.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; I think that if IRC is violated, the Lasso could still be correct, right? [The diagnostic metric after adaptive Lasso is greater than 1] "explaining why the adaptive Lasso alone will not work in those cases." Furthermore, we restrict ourselves to cases where the original authors have tuned their algorithms and present the cases as being hard ones, see table 1." In particular, although I definitely appreciate the deeper discussion of IRC, the Maddu et al.(2019) (PDE STRIDE) paper also discussed it and used it for motivation to use Randomized Lasso and stability selection. So, this seems worth publishing but doesn t seem like a big step in novelty to me. I m guessing the noise level changed.<|endoftext|>We optimise \tilde\xi, yet \tilde\xi is defined to be w\xi. The author response clarified my technical comments. Unfortunately, the paper does not consider ODEs at all; limits the experiments to few PDEs; and doesn’t do any ablation studies. This limits the impact of this paper significantly. (If one only considers PDEs, the title is then overly broad as well!). varying the noise or data size. Currently the paper feels premature for publication, but no doubt will become an excellent paper in future.<|endoftext|>Having said that I still recommend to reject this submission because I think the empirical evaluation is limited to PDEs where we know the True coefficients and generalisation on more difficult problems in unknown. And finally, the limited theoretical justification behind the work has some strong assumptions which are not sufficiently discussed in the paper. Since this assumption is the basis of the proof, it s important to clarify when this assumption will be met. In those specific cases authors show success of their algorithm in model selection problem.<|endoftext|>This manuscript studies the variable selection consistency in model discovery of PDEs. Weakness:1) (Technical novelty). However, the theoretical analysis of the underlying machenism is insufficient. The problem of variable seclection consistency of model discovery in PDEs is studied by first pointing out that sparse regression may fail with violated IRC and then proposing a randomised adaptive Lasso to tackle it. To me, this work is interesting.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The goal of applying contrastive pre training on unsupervised domain adaptation is a critical topic to study. 2.The paper proposes to use the connectivity model to study contrastive learning. This paper does not propose a new method, but proposes a connectivity measure based on the analysis of existing methods. 2.The main experiments in this article are based on existing methods. Even a simple solution on the toy task can verify the practicality of the proposed measure to a certain extent. Although this paper gives an interesting new idea to solve the problem of domain adaptation from connectivity, it does not extend the idea to a specific method, nor does it conduct sufficient experimental verification.<|endoftext|>The remaining of the paper analyses the features learned by this techniques, and in particular, how they do not learn to align the two domains as is the common intuition in UDA. This analysis relies on a "connectivity graph" defined for contrastive learning techniques, which, in the current setting, can be used to connect samples from the source and target domain, to determine how likely they are to be in the same positive set. The whole analysis seems sound and well though. how much tuning does it require ?) ** The experiments of Table 1 only seem to have one "fair" baseline (DirT/Sentry) because ERM is trained on the source data online. For instance, if I understood correctly, the authors train a classifier to separate the source and target domain on either (i) the input images or (ii) the pretrained contrastive features.<|endoftext|>The paper investigates why contrastive learning method benefits unsupervied domain adaptation. In particular, the authors use SwAV, a recently proposed self supervised learning method to pretrain a model on both targeted and source datasets. The authors found that by using such a model for UDA tasks surprisingly nice results are achieved. This is done by pretraining on both source and target models. Overall, the findings are interesting. But it would be more convincing if the authors report the results of initializing the backbone used in standard UDA tasks with pretrained self supervised model (SwAV) in this case. So this would make comparisons with other approaches more fair. Overall, I think the observations in this paper are interesting. But I think more experiments would make the paper stronger.<|endoftext|>In particular, it finds that CL brings a different mechanism for UDA compared to traditional adversarial DA methods, where the features learned are far apart between the source and target domains. ## Strengths+ The topic of the study is of great interest, and to me is pretty novel and not explored in the literature. The findings could have implications on when / how contrastive learning works when data are from different domains. The paper has a great potential to the CL field, and could be beneficial and inspiring for broader audience; but issues need to be addressed / made clear. I m happy to change my score if the feedback addresses my concerns. The empirical results are well aligned with the proposed connectivity model, which could be promising for understanding the behavior of CL. If so, what will be the results under other assumptions (e.g., label shift)? Will this affect the main conclusion? However, all experiments are limited to image data for visual recognition tasks. The authors only experimented on image data, which might not be enough for a rigorous conclusion. For example, the paper claims "Contrastive pre training is a strong domain adaptation method". This might be true for image data, as it is interpretable by human, and people can design effective augmentation methods (e.g., those in SimCLR).
Reject; rating score: 5; rating score: 5; rating score: 8; This work proposes a new CNN architecture: ConvMixer, which integrates some techniques/techniques used in the recently proposed visual transformer model/mlp model. ConvMixer is simple and effective, and it also prompts us to think deeper about where is key to improving performance in the vision transformer and mlp models. Clear motivation and the problem raised in this work is very interesting. Cons:a.Although the proposed ConvMixer illustrates very promising results compared with its counterparts, such as DeiT, MLPMixer, and ResNets, the main claim of this work:  "the patch representation may be the most critical component" has not been sufficiently well validated:   Compared to other previous convolution based network architectures, such as ResNet, the ConvMixer has too many factors beyond the patch representation itself that may lead to performance differences, such as data augmentation, appropriate hyper parameters, optimizer and other macro/micro network architectures, For example, the ConvMixer adopts the spatial conv + pointwise conv as its basic building block, which is also an unfair comparison with ResNets. In addition, through carefully tuning, [1] reports a much higher ResNet performance than this work. Compared to vision transformer/mlp based networks, this work only considers some computation/parameter unfriendly architectures, such as DeiT, while ignoring some recent progress in this field, like Swin. These under explored factors challenge the conclusion/connection of this work that the  patch representation  is the key. However, its research is not strong enough to support its claims.<|endoftext|>8.Some important ablation studies are missing, like the influence of depth, kernel size, patch sizes, etc (I mean detailed ablation studies). "Approximating CNNs with Bag of local Features models works surprisingly well on ImageNet." The model is interesting and simple, which would inspire the vision community a lot. 3.An interesting but not discussed finding is that, different from VIT  and MLP mixer (and variants) that emphasize the global context, this work did not introduce global context (although the receptive field is large enough). 2.At the end of Section 1, the authors claim that “This suggests that, at least to some extent, the patch representation itself may be the most critical component to the superior performance of newer architectures like Vision Transformers.” I don’t think this is true, and the authors have not fully verified such a claim. 3.Most importantly, I don’t think this method achieves “better” results than others. iii) Small patch properly can achieve better performance. However, this may not be true. Please see Appendix A. I have not seen the strong superiority of ConvMixer over the ResNet in the accuracy with similar parameters. 5.In VITs and MLPs (including their variants), we have two skip connections (it is not called residual connection as shown in Fig 2) in a block. 7.I could not get the idea of “A note on paper length” paragraph in a conference paper, considering some of the important experiments are missing. It is nothing to do with the main idea of ConvMixer.<|endoftext|>On that experiment, they show how their simple architecture is competitive with state of the art architectures. I am wondering if that could be an additional experiment to include. 6.Looking at the results, it is clear that the model has lower throughput than its competitors. Figure 1 revolves around the parameter count of the model, but I believe it would be fair for the authors to include also some discussion about the throughput. **A note of the paper length**: I understand the message from the authors about the paper length, but as a reviewer, I needed to check the Appendix many times to produce a complete review of the paper. Although I can agree this is a good discussion to have as a community, I don t think the note from the authors is fair as if reviewers only looked at the main paper (which is supposed to be sufficient for a full understanding of the paper), many relevant points would be missed. For this reason, I would recommend the authors to move to the main paper some sections which provide relevant insights (for instance, Appendix A gives very relevant details). I believe the paper is good and it should be accepted. The model is very simple and effective. It will be a relevant contribution to the community as a baseline as well as a good datapoint to improve our understanding of SOTA models. Furthermore, I think authors should address some relevant points when comparing to other models such as data scale and throughput. Code is simple and authors seem to imply that it does not need a lot of hyper parameter tuning to make it work. Specially, when taking into account the parameter count, the model seems to show very good performance overall. Patches and architecture: authors introduce the model as validation of the idea of the patches being important for final performance on SOTA architecture such as transformers (see title and abstract). However, in the paper they introduce the idea of using patches in convolutional networks along with a novel architecture, but they never decouple the gains of this two contributions in the experimental section. Otherwise, I am not sure if we can conclude much, as it was already known that these large models were not very good with regimes without much data. 3.The first paragraph of the experimental section is a bit confusing for me. If they think those experiments are not relevant, then they should not start the experimental section by discussing those experiments.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper proposesed an image editing and synthesis system (SDEdit) with SDE. Strengths:The whole idea is interesting, effective and simple. The results are impressive. Although Stroke based Editing is not complex, especially on the face,  I think this is a good start of using the SDE to do the image editing task.<|endoftext|>It would be great to include the running time of the proposed method in comparing with other methods. However, considering the technique novelty and the overclaiming, I vote for a weak reject initially and am willing to listen to the authors and other reviewers. The paper is overclaimming.<|endoftext|>This paper proposes to synthetize and edit realistic images based on stocahstic differential equations (SDE). Overall I am leaning towards a weak accept. The paper has compared with a few state of the art baselines and demosntrate its effectiveness in producing more faithful editing results. One question I have is that, how good the guided stroke sketch needs to be?
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; The decomposition requires accurate estimation of scene depth and the segmentation of the scene into objects. Since the proposed method is based on NeRF, how well does it work with real photographs? RatingThe paper sounds weak in the sense that some of its contributions are on improving NeRFs while the decomposition part is rather marginal. Lack of novelty. There are a few concerns about the experiments and baselines as well. However, the input of the pipeline is 2D images which is prone to be 2D instance segmentation.<|endoftext|>The authors propose a method to synthesize novel views of 3D scenes while inferring a decomposition of the scene into multiple objects. The compositional reasoning is achieved by a slot based encoder so that there s no need for specific supervision on the object categories. I would recommend this paper for acceptance as long as no other reviewers have major concerns. Regarding the novelty of the work, there are two main aspects: First, the unsupervised decomposition of the scene, which is somehow incremental, given that it s achieved by applying the slot based approach of [Locatello et al.2020] to NeRFs, and also very related to [Yu et al.2021].Second, the training speed up using depth training data, which seems to be the most significant contribution.<|endoftext|>The encoder side takes a single image and pose as inputs and generates N latent codes for each object in the scene following  Object centric learning with slot attention (NeurIPS2020). The latent codes condition shared NeRF decoders to define the geometry and appearance of each object. I found the paper a bit hard to follow, as it heavily relies on previous works. Strengths:* Using depth to reduce the number of samples considerably is an intuitive way to reduce the computational complexity of training NeRFs. * The proposed method considerably outperforms (quantitatively) the previous methods under comparison in Tables 1 and 2. Weaknesses:* The segmentation part seems to be already proposed in  "Object centric learning with slot attention (Neurips2020)". * No results on real world data are provided. For the ShapeNet dataset, the results seem to be rather blurred. The results shown in this work seem promising and advance towards single view 3D scene understanding.<|endoftext|>This paper is about unsupervised segmentation both of 3D volumes and of images. The idea is to learn a set of Neural Rendering Fields (NeRFs) which explain a given image. To reduce computationally effort, the authors assume RGB D data and show how that reduces the computational burden by two magnitudes. The experimental evaluations (using simulated data) demonstrate that the algorithm works, at least on very simple data. "a significant chance that thin, high density volumes are missed": Standard hardware for capturing RGB D data likely does not capture thin, high density volumes either, thus the proposed approach would not be successful for such data. Can it be violated? Why are only 2 evaluations of the NeRF necessary? That reduces the claim in the title about the unsupervised segmentation since providing the number of objects can be considered as a weak supervision signal. I like that the authors address this issue and propose a solution.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The theoretical dimensional analysis of SVGD on Gaussian also suggested another modified (damped) SVGD.<|endoftext|>This paper analyzed the curse of dimensionality problem of the vanilla SVGD with Euclidean distance kernel in a qualitative and quantitative way. If I understand correctly, the entire analysis is based on the Euclidean distance kernel assumption.<|endoftext|>The problem studied in the paper, the variance collapse of SVGD, seems to be important and interesting in my perspective. This work studies the variance collapse phenomenon of SVGD.<|endoftext|>This paper provides an understanding of the variance collapse phenomenon of SVGD. The topic is nice.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The goal is to learn an interpreter model from explanations paired with observations for a particular phenomenon. The explanations might be in an unknown language, but the explanations paired with observations can be used to learn a good interpreter. They also define the scientist problem where explanations for the unseen phenomenon is not available. The authors propose a neural network architecture as a solution to the scientist problem. Experimental results show better generalization compared to end to end neural systems. The idea sounds very familiar to explanation based learning. 3.The experimental results are all on the new game like dataset. It will be good to see performance of proposed methods on real datasets, or already existing synthetic datasets.<|endoftext|>The underlying assumption is that the map (interpreter) from sentences to sets of examples is shared. The authors propose an environment to evaluate these tasks and a neural architecture to tackle them. Experiments with real data would have been useful to evaluate the efficacy of the proposed pipeline. Alas, the motivation for pursuing this research direction is unclear. It also shares aspects with few shot learning. real world applications this setup is meant to capture. This makes it hard to evaluate the significance of the paper. As far as I can see, "phenomena" are simply *concepts* and the "explanations" are intensional *descriptions* thereof, except in a language unknown to the machine (but known to the annotator). The work seems to rely on a rather strict assumption.<|endoftext|>The proposed approach to solving the problem seems not to be accompanied by any formal guarantees on its performance. The explanation is in some arbitrary language. The paper implements this pipeline using neural networks, and presents empirical results on a new benchmark dataset to demonstrate its performance. This is discussed, for example, by Michael, "Machine Coaching", IJCAI Workshop on XAI 2019, which seems to be rather relevant to the current paper, especially given that the paper makes an effort to connect to learning theory. Ignoring the obvious difference from the current work that these two works assume that explanations are in logic, the underlying theme of the cited papers and the current paper seems considerably close to ignore.<|endoftext|>agnostic.Such formulation has resulted in the so called Explanatory Learning (EL) framework that is paired up with an environment to test it. The starting point for learning a language is to extract an interpreter based on a given set of observation and their explanation and then use the interpreter to determine whether an observation belongs to the language, in a binary classification setting. If that’s the case, one can learn the translation itself and eliminate the difficulty. Other remarks:   This is a language specific problem, where order of the sequence does not necessarily matters. Overall, I quite like the idea of the paper due to the generality of the approach, namely learning an interpreter from observation.
Reject; rating score: 5; rating score: 5; rating score: 5; This makes the present study seem to be an ensemble of well developed techniques. Strengths: the theoretical analysis is solid. 3.As pointed out in the empirical study section, the predictive model might fail to learning the reward for CACC slow.<|endoftext|>2) As pointed out by the authors, the effective of theoretical bounds depends on the selection of the discount factor.<|endoftext|>First of all, the used assumptions sound to be too strong to model realistic multi agent systems. Q5.What are the relationships between sections 5.2 and 5.3?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The proposed method builds on intuitive assumptions that are easy to understand. There are mainly two concerns regarding novelty and effectiveness. Test time model adaptation is an interesting problem, providing a new perspective to improve model robustness.<|endoftext|>The paper aims at increasing robustness of image classifiers against domain shift using test time adaptation on a single instance level. The proposed method is called "Marginal Entropy Minimization with Ensembled Augmentations" (MEME). As discussed above, there are some ablations/alternative design choice that could be added to the experiments.<|endoftext|>* Novelty of the approach is limited. This work can be seen as combination of these two existing approaches for test time adaptation. post rebuttal  The paper studies an interesting problem setting and also bring improvements in this setting, particularly on vision transformer. Weaknesses:My major concerns lie on author s claims, novelty of the approach and effectiveness of the approach on CNNs.<|endoftext|>The authors write “and we list in boldthe best results from these methods which outperform the test time robustness methods.” I don’t get it: BN adaptation and TENT are also test time robustness methods? I think that the general idea is interesting. Therefore, I think the authors should refrain from wanting to claim state of the art results over methods that are allowed to see the full test set, since it is an unreasonable request. ### Comments on achieving a new state of the art claims:The authors claim to achieve a new state of the art on various benchmarks on several occasions. I think writing the pitch in this way should make the paper much more clear and easier to understand and put into context.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; In this paper, the authors propose a novel loss function called the approximated total group preorder (ATGP) loss function, which aims to address the potential limitations of l2 loss in predict then optimize (PTO) problems. The authors argue that the use of l2 loss in the prediction phase is "misaligned" with the final goal when the ultimate objective is to make discrete decisions in a combinatorial optimization setting. The authors aim to rectify this issue by proposing a novel loss function called the total group preorder (TGP) loss   for better alignment   and then an algorithm for automatically searching for an approximate TGP (ATGP) loss function so that it is differentiable and therefore can take advantage of efficient gradient based learning schemes. Overall, the paper is well written. However, the authors, unfortunately, do not discuss the relation of the presented work with the existing studies based on regret or MOCU. The paper deals with an interesting problem and the proposed method is well motivated and presented in a logical manner.<|endoftext|>This paper studies a problem where the goal is to solve an optimization problem, but key parameters of the optimization problem (like a road network’s edge weights) are unknown and can only be predicted from historical data. This paper studies a way to learn a good loss function for the downstream optimization problem at hand. Let $c \in R^d$ be the true parameters and $\hat{c}$ be the learned parameters. (+/ ) I think that the “strong ranking property” is promising, but it could be fleshed out. This would, of course, be more compelling if the authors could provide an example where the proposed approach is better than using the l2 loss by some non zero margin. Overall, my recommendation is “weak accept” because I think the motivation is compelling and the “strong ranking property” seems to open up this predict then optimize framework to important combinatorial optimization problems (though I’m not very familiar with prior research on this specific topic).<|endoftext|>This paper aims to solve combinatorial optimization problems with unknown parameters that need to be predicted from observations. It proposed a total group preorder loss and its differential version approximated total group preorder loss for predict then optimize (PTO) problems with strong ranking property. It studied a very interesting problem. Is it a reasonable conclusion? Overall, this is an interesting paper. It may help to understand the paper if adding one or two examples other than ranking (constrained optimization problems such as shortest path/knapsack in the modeling section), to illustrate how the new notion of total group preorder and algorithm can be applied.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; I can see there is much effort of this paper to make adjusting different pretext tasks on a graph, especially the authors bring up the important "homophily" property of graph to create pseudo labels for self supervision. However, the essence of this paper is still dynamically adjusting weights for different losses, as the authors mentioned in the section Related Work of Automated Loss Function Search. There is so much work on reweighting different loss functions which have been well studied. Though the authors mentioned that "the problem of self supervised loss search for graphs remains rarely explored", I find out the solved problem in this paper is actually an old problem, which is limited in novelty and significance. Otherwise, I would consider they are simply a combination of existing works without much contribution. Maybe considering the other 3 tasks is not that helpful. Though this paper is well written and studies an important and popular problem in graph learning.<|endoftext|>Authors use homophily datasets and meta learning. For self supervised learning, authors use evolutionary strategy. 1."Is Homophily a Necessity for Graph Neural Networks?" In that paper, it states Homophily is not necessity. If dataset is Homophily, the task comes to easy. In this paper, authors use such easy datasets. 2.If the meta learning is the contribution, it is not a contribution of the paper but is a tool.<|endoftext|>The proposed work looks at the task of automated self supervised learning (SSL) on graphs, by using pseudo homophily as a surrogate objective combined with a search strategy for the proposed approach, AutoSSL . Homophily is defined as the average of sameness of labels over pairs of connected vertices. Experiments are performed on 5 SSL tasks and 8 datasets. Paper is well written and easy to follow. The method is well motivated and quite intuitive, the paper is easy to follow.<|endoftext|>These tasks provide self supervision for training graph neural networks without accessing any labeled data. This study develops an approach (called AutoSSL) for combining multiple SSL tasks for unlabeled representation learning. To this end, it defines a pseudo homophily metric to measure the quality of learned representations. There are many such scenarios in Tables 1 3. (3) It is unclear how the proposed strategy could be used for other contexts, such as link prediction and graph classification. This is not a problem on its own, however, the empirical gains are not convincing.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 10; This paper leverages the implicit neural representations paradigm to build generative adversarial networks for video generation. Applying INRs to this task seems like an excellent direction, and avoids many computational problems of video generation. I would have loved to see a more compelling set of visual examples to highlight the possibilities of the technique, and a potential comparison to other works that have trained on larger portions of Kinetics 600. How are you handling this during training? There is only one example in the anonymized webpage. Why were the other datasets omitted? UPDATE: The provided rebuttal answers most of my concerns, and my confidence about the strength of the paper has increased. The authors show strong quantitative results and a few architectural novelties. The work is solid, but there is a worrying lack of compelling qualitative examples to back up certain claims. If the authors can provide more examples, I believe this could be a strong addition to ICLR.<|endoftext|>The paper present a new method to video generation. They model a video as a function $f(x,y,t)$. This is, however, not an issue of this paper but of the field in general. They further introduce a discriminator that is conditioned on the time difference between frames. How representative are the two examples? The authors had two 4 second examples, now they have 2 8 second example which doesn t bring much difference. So I believe my initial evaluation was correct, the paper is slightly more on the positive side of the bar. I believe, the paper is a good piece of work and is above the bar. I believe this is an important idea not addressed in the past2. I wonder if a similar discriminator can be used to make previous models be able to generate longer sequences. 3.Consider two frames of a video, for example, when the hand is up and the other one when the hand is down. Could the authors comment? For example, in Fig.12 the generated results look like interpolations between two frames.<|endoftext|>Update:The author s response address almost all my questions, however, I m not convinced about the discriminator choice and the author s explanation is not clear and convincing. This is also confirmed by their qualitative results on long videos such as the UCF101 video (64 frames). The result is much worse than DVD GAN (Clark et al 2019). I believe this work has positive aspects in terms of efficiency and ability to generate frames in parallel. Therefore, I m increasing my rating from reject to "marginally below the acceptance threshold". #### Strengths  The paper provides a comparison to recent methods on IS, FVD, and KVD evaluation metrics. This helps to understand better the performance of the introduced model on video generation. This is the feature I like the most. I really don t think that the discriminator design works for long term video generation. I think this discriminator works if the video is short and there is no repetition or high similarity in the video frames. To evaluate, a good experiment would be to generate random videos from UCF101 dataset and try to evaluate the performance with the metric introduced in [3] or a similar metric (S3) used in [2]. Or at least qualitatively show long videos from UCF101. Additionally, the paper misses some related works (and discussion on differences) and details on computational complexity and training time. However, I m open to discussions and will consider the author s response.<|endoftext|>The paper proposes a video generation approach based on implicit neural representations (INRs). In hindsight, this may seem like a trivial extension, but the introduction of INRs to video generation provides a solution to many hard problems in video generation work. It also includes the relevant ablation studies. It is also timely, given the need for more efficient video GAN models. This work suffers from one fatal flaw. Some of its experiments train the proposed model on the full dataset, not the training split as in prior work, yet it presents results comparing performance to models trained on only the training split of the dataset. To allow for honest and accurate comparison to the majority of prior work, I would strongly encourage the authors to benchmark models trained on the training data split for the related datasets, in particular, the "trainlist01" split of the UCF101 dataset. It suffers a fatal flaw that prevents me from recommending it for acceptance. If the issues with benchmarking are resolved, then I will recommend it for acceptance. If additional experiments exploring the efficiency of the proposed approach are provided, then I will strongly recommend it for acceptance. ***EDIT****The authors have sufficiently addressed all of my concerns (and those of the other reviewers it seems). I strongly recommend this paper for acceptance to ICLR22, it is a meaningful step in a promising direction for long horizon GAN based video generation.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; It also talks about a better form of privacy amplification based on that. Finally, it discusses the role of membership inference in the task of machine unlearning. Edit: I have bumped my score by a point after the authors  response. Update: on further digging into the prior work, and having discussions with other reviewers and the area chair for this paper, I have decided to downgrade my review, unfortunately. Their experiments suggest better privacy amplification as opposed to the prior work. Weaknesses: This is not really a well written paper. The latter, in particular, seems counter intuitive based on what the text before that described. Also, what the probability is over isn t mentioned either. The role of $f$ in the accuracy statement isn t clear exactly. Nitpick, but the definition of pure DP is stated twice in the paper (Equations 1 and 2). The technical contributions don t seem that strong. Also, their bounds on the membership inference attacks don t appear super tight. Evaluating some known attacks to compare their accuracy with their own bounds could have been some kind of an evaluation. The ideas are respectable, but the execution could have been better, and more depth would have been appreciated.<|endoftext|>This work studies membership inference attacks and how differential privacy can help mitigate such attacks. Although the connection between membership inference and differential privacy has been known, this work provides a tighter connection. Specifically, on page 1, it states “we observe the affect of our amplification on MI accuracy is significantly stronger than batch sampling”, but does this mean that batch sampling is used to amplify MI accuracy? It is also not clear how batch sampling is different from training data that is sub sampled. I also do not understand the second contribution listed. The tighter MI accuracy bound benefits in what way with subsampling? In particular, the amplification by subsampling converts eps  > O(q eps) and the constants might matter in the plots. This section seems a bit hand wavy in what it is trying to say. If they are considered the same type of sampling, can you take advantage of the DP amplification, getting O(q epsilon) privacy loss, and the dataset subsample in the MI bound? ### UPDATE ###The author feedback has addressed my concerns, so I will increase my score. I would also like to see more discussion on whether MIA are the right attack to consider with DP, as some recent works have shown the attack to be weak.<|endoftext|>The authors also analyze their bound from the perspectives of privacy amplification schemes and its connection to machine unlearning. Specifically, The bound is given in terms of $\mathbb{P}_{\mathbf{x}^*} (1)$, which indicates the probability of drawing $x^*$ into the dataset from a larger set of candidates.I have verified the proofs and they are correct. The authors further compare the amplification effects of $\mathbb{P}_{\mathbf{x}^*} (1)$ with those in DP. Besides, the author also gives the connection between the bound and data deletion in machine unlearning. Usually, the defender does not have the prior knowledge $\mathbb{P}_{\mathbf{x}^*} (1)$, which is completely controlled by the attack. Could you please elaborate more on how to use the information of the bound when the defender who wants to to know the limit of MI accuracy when the probability of drawing $x^*$ into the dataset is unknown to the user and how it improves from the Erlingsson et al.(2019) and Sablayrolles et al.(2019)?Minor:Besides, in Figure 2, is $\mathbb{P}_{\mathbf{x}^*} (1) 0.5$? In order to do so,  I suggest the author should provide the analytical form of previous bounds of MI in the appendix. 1.The paper is well written and the bound is insightful.<|endoftext|>The paper provides a tighter bound for accuracy of membership inference attacks for (pure) differentially private machine learning models. Moreover, the authors also demonstrate the benefits of their bound on machine unlearning. This is very consequential in privacy preserving machine learning. 2) The bounds provided by this paper are able to handle situations where the inclusion probability of samples is not 0.5 by providing separate bounds for positive and negative accuracy. 4) The authors highlight the limitations of their approach by demonstrating that their bounds don t readily extend to approximate DP. The main weaknesses of the paper are the following: 1) The background needs to substantially expanded upon. I would like to at least see the following improvements: i) a formal definition of membership inference, ii) a formal definition of machine unlearning (as used in the context of this paper). I believe this could be improved by presenting some background on privacy amplification by subsampling. While the paper presents an important and novel technical contribution, it has some writing issues. In particular, the background section needs to be somewhat expanded and a particular section would benefit from more clear writing.
Accept (Poster); rating score: 8; rating score: 8; rating score: 3; rating score: 10; The paper proposes a Knowledge Distillation (KD) based approach for producing MLPs able to achieve comparable performance to Graph Neural Networks (GNNs) on node wise classification tasks. The paper proposes a novel approach for effectively training MLPs for node wise classification tasks on graph structured data. The experimental evaluation presented in the paper is generally sound and well highlights the benefits of the idea proposed in the manuscript.<|endoftext|>To overcome inference latency of GNN models, this paper proposes a GLNN framework that trains a student MLP with supervision from a teacher GNN model. I thought it would not work well in inductive cases, but it can. This stands between GLNNs and GA MLP models. 3.I am wondering whether it will enhance the performance further if the input feature is combined with 1 hop neighorhood, such summation of features in 1 hop neighneighorhood, which is very simple.<|endoftext|>This paper applies KD in the context of the graph. It aims to distill the teacher output of a GNN model into a simple MLP model. Empirically, they show that this simple KD design is able to improve the student MLP model by a large margin and can match the results coming from a teacher GNN model. Besides, it shows empathically, the inference time can be greatly improved. The literature discussion is not thorough, missing out on quite some important related works which discuss the knowledge distillation in the context of graph structured data [1] [2] [3]. Graph Free Knowledge Distillation for Graph Neural Networks.<|endoftext|>The paper proposes to distill knowledge from GNNs to MLPs so that at inference time it requires less burdensome. The approach is very simple, which copies a classical KD approach proposed by Hinton et al.At inference, it shows significant reduction of time compared to baseline GNNs and existing approaches. I would include other datasets, for example, coming with heterogeneous node features [1] (where MLP and GBM can work quite well) and heterophilous datasets [2] (where LP can work well).
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This work focuses on the problem of speeding up adversarial training in the $\ell_\inf$ threat model. Strengths: the work outperforms Fast AT GA at $\epsilon 16/255$, obtaining better standard accuracy in less time while matching robust accuracy (standard accuracy ~68% instead of ~59%). Using this type of evaluation, a reader can see the speed/adv. As a suggestion, one natural way of trading off time for accuracy is to treat the number of epochs as a free parameter; an illuminating here would be adv. performance vs training time as the number of epochs changes for each method. How is it that $\epsilon 8/255$ and $\epsilon 16/255$ trained models always have the same runtime, as reported in Table 1? The authors claim to solve an issue with Fast AT stemming from its tendency towards catastrophic overfitting. The authors should give more background on when Fast AT fails to catastrophic overfitting; how often does it happen? While the work shows a promising approach (as measured by outperforming Fast AT GA at $\epsilon 16/255$ in terms of standard accuracy while beating it in speed and in robust accuracy), the evaluation is lacking. In particular, the evaluation does not give a speed/accuracy tradeoff for each training routine, and does not properly hyperparameter search for each compared algorithm in the comparison. Finally, the work has some minor issues in its characterization of previous work.<|endoftext|>Finally, I still have some doubts about the contributions of the paper to the BLO literature. The new technique is then compared empirically to other existing approximations of the robust training problem (referred to in the paper as Adversarial Training) and shown to provide improved or comparable results in terms of standard and robust accuracy, for different attacks. This contribution seems however straightforward and the deployed techniques for optimizing the problem (linearized proximal form, differentiable projection) are standard in the literature. The paper presents some inaccuracies when reporting the related work and in the derivations:Contrary to what stated in the paper, Adversarial Training was proposed before [Madry 2018] in the seminal work of [1]. In [Madry 2018] adversarial training was formulated as a Robust Optimization problem. It does not come directly from the linearization otherwise this term would be multiplied by the second order derivate of $\ell_{atk}$ and there wouldn t be any $\lambda$ hyper parameter. ## UPDATEMy final evaluation is based on the current version of the paper, as the authors had the chance to update it. Doubts about the significance of the empirical results has been raised by other reviewers as well.<|endoftext|>This paper aims to interpret the fast adversarial training methods from the perspective bi level optimization. And then the authors proposed a new linearization of the lower level optimization problem and introduced a new FAST BAT approach for improving both accuracy and robustness. Various experiments were conducted to verify the effectiveness of the proposed method. However, in the current version of the paper, there exists several key points that requires careful justification to make it as a much stronger piece of work. I list them in the below. 1.In the Remark 1, it seems an very important conclusion the authors want to claim on the role of the second term associated with alpha2. Is it just an empirical observation or is there any good theoretical justification? What about the non ReLU activation function for which the function is not piece wise linear wrt to input?<|endoftext|>The paper studies adversarial training as a bi level optimization problem. the proposed adversarial training approach has nice theoretical motivations. The empirical results show that compared to the baselines, within the same order of computational cost, the proposed method enjoys improved stability and mitigates the catastrophic overfitting present in other baselines. the paper is well written for the most part. The results are only averaged over 5 random experiments, therefore, given the complexity and the size of the task, I’m not sure how reliable the estimates of mean/variance are. I think the empirical results can be much more compelling if the authors include more experiments. In particular, some claims are not well justified, there are some issues with the notation and the presentation lacks mathematical rigor. I do not see how FAST AT GA has a poor accuracy robustness tradeoff in Table 1. Regardless of whether this claim is true or not, the justification is far from satisfactory. I suggest authors include careful proof here.
Reject; rating score: 3; rating score: 3; rating score: 3; Strengths:The assumptions, lemmas and theorems are clearly stated. I think the paper makes an interesting theoretical contribution in a limited setting. Weaknesses:The main weakness of the paper is its usefulness to batch RL learning.<|endoftext|>16.A full and detailed proof for Theorem 1 would have cleared the paper for both the authors and the reader. Flushing out the whole detailed proof will be helpful. "Batch policy learning under constraints." This is highly misleading as there is a substantial literature on understanding the necessary and sufficient conditions on the sample complexity of batch RL under varying assumptions on the data distribution, approximation power and statistical complexity of the value function/model class and algorithmic schemes. As such, I feel that this paper is not ready for publication and needs a significant and detailed revision. The technical tools and proof approach used here is basedf on the paper by Hardt et al (2016) which analyzed the stability and generalization properties of stochastic gradient method for convex, strongly convex and non convex losses. 14.Again, in this section, various references to structure of the optimization problems are made, but the optimization problem itself is unclear.<|endoftext|>As far as I can see, this work tries to leverage the connection between stability and generalization studied in supervised learning settings, and tries to build a similar connection for batch RL settings. These deficiencies affect the paper s readability and quality. PROSI d like to say that extending the stability based analyses to bath RL settings is an interesting line of thought. My evaluation is based mainly on considering the amount of fixes that need to be done for this work to be acceptable for publication. Section 3.2 describes algorithmic stability and sets the definitions and notation, which are borrowed from the supervised learning setting I think. It is not obvious what s the connection between optimization of Eq.(10) and the batch RL problem.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; They experiment on the game of Overcooked, compare with TrajeDi, and run a human experiment to check performance of coordination with humans at test time. To me, optimizing an approximation of the population diversity seems much more promising. I also thought section 3.4 was not well motivated, and it is unclear what the takeaway of this section is. Related to this, why does the paper claim uniform sampling "does not provide any guarantees" (can t we also claim that training with all agents uniformly increases the overall reward?) I m also not convinced by the discussion in Q1 regarding Figure 3. In fact the plots are very spiky and hard to interpret. I found the writing to be poor. It s unclear to me what precise claim they are making. The paper tackles an important problem, but leaves much room for improvement. The experiments can also be improved (TrajeDi is missing in the human experiments), and the current results are hard to interpret (the reward in Table 1 does drop significantly when the entropy term is pushed up).<|endoftext|>The authors focus on training agents for zero shot human AI coordination on Overcooked. **Missing baselines and ablations in human experiments**: none of the important baselines or ablations were evaluated with humans. While results with the human proxy model are helpful and suggestive, I think its important to include the key baselines and ablations in the human experiments as well, i.e.TrajeDi, alpha 0, beta 0, and the other baselines and ablations mentioned above. Can the authors comment? In addition, I could not find the hyperparameters or tuning procedure for the TrajeDi baseline. Structurally, I found section 3.1 and Theorem 1 unnecessary. For this reason, I don t think its a prerequisite of a human data free method to *beat* PPO_BC, but it is still an important, informative, and easy baseline to include for comparison. 8.TrajeDi is described as "concurrent" work, but the paper has been out for several months. "Concurrent" is I suppose of debatable definition, but I think its more appropriate for work that is in submission at the same time.<|endoftext|>Honest discussion of this would make the work stronger. Results on the Overcooked domain show improved performance over a series of baselines. The framing of the method is incorrect, it does not cite PFSP until related work (not in Section 3.4) and it claims TrajeDi is "concurrent" which is not true. The paper is interesting and well written, and addresses an important problem. TrajeDi was presented at AAMAS 2021 (in May) and again at ICML 2021 (in July). This to me is not concurrent. Meanwhile, PFSP uses almost the same prioritization scheme, but the only difference is that it is used in competitive games vs. cooperative. **If significant changes are made to the claims in the paper and presentation of the methods I would be happy to raise my score**. Particularly the inclusion of the prioritization scheme with TrajeDi. What happens if you use other hyperparameters for TrajeDi? * Figure 3 has a lot of redundancy. * The human experiments are interesting. Does the addition of PFSP impact the human coordination? Nonetheless, as it is this is a strong result.<|endoftext|>This makes my recommendation borderline at this point, so I m looking forward to discussing the manuscript with the authors and the rest of the reviewers to understand how to improve it towards possibly acceptance. Considering that they are comparing their methods against theirs, it seems unreasonable not to benchmark MEP on at least some of the toy problems proposed in that work (whilst I would accept the authors not trying to compete with their well tuned Hanabi results, as that would result in quite a lot of potential work). 5.The focus on AI human coordination seems a little weird. In principle this paper is proposing to increase the diversity of states seen by the agent policy by maximising the types of behaviours generated during training time. This means that the method is applicable to be tested against any kind of out of distribution( ish) agents. 6.The paper at times is unnecessarily handwavy, unclear, or makes dubious statements that are not well backed up by the literature. b.It is claimed that "prioritized sampling [of agents policies from the population] [makes] the collaboration between the AI agent and any agent in the population as good as possible in general"   I don t understand how to interpret the sentence: is the manuscript saying that prioritized sampling is generally optimal (a fairly strong claim!) wrt.learning with PBT for cooperative MARL? Section 1: the experimental section is not a contribution of the work   there s nothing intrinsically new about how MEP was tested, as far as I can see? Figure 3: hard to interpret   it feels like it could have been reduced to two plots by grouping wrt.<|endoftext|>This paper tries to find a new approach by enforcing the diversity in multi agent RL  via the maximum entropy to address the zero shot human AI coordination problem. The empirical results on the overcooked show that MEP outperforms other baselines with both simulated and real human players. The paper is well written and easy to follow. Therefore, I think the authors can discuss the broader impact of the paper and clarify more about why test this method on specific human ai coordination problem. It would be good to discuss the relationship between the proposed method and the diversity promoting solutions in the PSRO framework. Is it a fixed policy pool, or would it be expanded by adding the new learned policy during the training?
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper explains GNNs by identifying important message flows. The authors use the concept of Shapley Value in Cooperative Game Theory and calculate Shapley Value as the initial assessments of flow importance score. Most existing methods use nodes, edges, or subgraphs to explain GNN. 2.The proposed method is tested on several datasets. The experiments also show the time cost of different methods. 3.The paper is well structured and organized. The practical significance of information flow as an explanation is not clear. Some practical examples may help readers to understand. 3.In the experiments, some baselines are missing, such as Gem[1], GraphSVX. 5.Fidelity score performance on seven datasets with GCNs under different sparsity levels is inferior, which shows that the proposed method may not be effective. The average Fidelity score has no practical significance, and a high average fidelity score cannot indicate the effectiveness of the method. Minor points: The definition of s should be clear, s represents both marginal contribution and importance score. Though this paper proposed a different way to explain GNNs, using message flows instead of nodes/ edges/ subgraphs, it lacks clear intuition of the novelty of using message flow. Contributions are somewhat incremental compared with the existing Sharpley value based approaches for GNN explanation.<|endoftext|>With explainability in Graph Neural Networks (GNN) still in a nascent stage, most graph explanation methods generate explanations in terms of nodes, node features, edges, or sub graphs. The paper proposes FlowX to identify important message flows by employing concepts of Shapley values, where an approximation scheme is used to estimate the Shapley values as initial assessments of flow importance. The proposed work is the first method that leverages message flow in GNNs to generate explanations for node  and graph classification tasks. 2.Leveraging Shapley values for estimating the importance score of message flows is an interesting approach. For instance, $M$ denotes both the total number of Monte Carlo sampling steps and the obtained mask from FlowX. The contribution scores are then uniformly averaged and assigned to the different message flows. Note that this can cause spurious distribution of scores causing higher importance to message flows that were not useful for a given node  or graph classification task, especially in the final layers. 3.The notion of refining the Shapley importance scores is intuitive, but the use of optimization tricks like exponential scaling and encouraging discreteness can bias the refinement step to obtain edge masks that achieve better scores using the evaluation metrics in Sec.4.4.What does the weight vector $w$ represent in Eq.9?Is it just some kind of mapper function that translates Shapley scores to edge mask scores? Would this cause the distribution of the layer edge importance scores to be skewed? Was it for computational complexity? If yes, then mentioning the error bar in the results would increase the significance of Sec.4.7.In Table 2, FlowX* performs on par with FlowX on many datasets which questions the use of the refinement stage. An intuitive explanation for such behavior would be great. Thank you for submitting a well thought work! Check the main review.<|endoftext|>The paper proposed to identify important message flows as the explanation of the GNN models. To achieve acceptable fast calculation of Shapley values for identifying the message flows, the authors use n Monte Carlo (MC) sampling. Cons:1).The initial generation of candidate sets of flows and the permutation algorithm seems to be time consuming. The authors are required to analyze the time complexity of the proposed algorithm. Although, the authors list the computation time in the appendix, theoretical analysis of the time complexity is also useful. Moreover, in table 1, the author listed the number of graphs, however, to what extent the algorithm can handle large graphs. Thus, it is suggested to list the #nodes of the largest graph in the set. This is also used in SubgraphX. However, in PGExplainer and GNNExplainer, they use accuracy as the metric. It is suggested to also report the comparison of the accuracy of different methods. 3).Typos and grammar errors:maybe not sensitive to model >maybe not be sensitive to modelmay not suitable >may not be suitableTowards explanation of >Towards an explanation ofBasically, the idea is interesting and the solution is also sound. I would champion the acceptance. However, I have still some concerns as listed. Thus, I gave a weak acceptance.<|endoftext|>I do not believe it makes things more simple. Also, for node classification tasks, is the importance of a message flow from a node to itself really used? The notation ‘For all’ in equation (10) is not rigorous, it should be in the sum. There are experiments with several datasets and baselines, as well as a small ablation study. However, I am a bit skeptical about the authors’ approach, which involves computing the importance of message flows from edge importance in each layer of the GNN model, before converting it back to edge importance for each GNN layer—ultimately provided as an explanation. I would have liked to see how FlowX compares to SOTA baselines on the evaluation conceived by GNNExplainer, and followed by most other baselines. So overall, I would say that this is an interesting work. Although this method seems very computationally expensive, the paper does not offer a proper complexity study, which appears essential. How does FlowX s performance vary for different values of $M$? There is no mention of how many Monte Carlo iterations $M$ are needed in practice (for the obtained results). In Section 3.3, the paper proposes a learning based algorithm to refine the initial assessments. In fact, to the best of my understanding, it learns to weight the marginal contribution of a player when added to a given coalition of players, depending on the size of the existing coalition. This simplification aside, it would be interesting to see the distribution of the weight $w$ to see if a trend emerges—validating or not the intuition of SHAP. ** The paper produces explanations for edges in different GNN layers. The output explanation of the proposed method is not easy to understand for a human, especially as the number of GNN layers increases. Besides, it only targets graph structure and ignores features.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper proposes new metrics based on the activation patterns of a model. It propose an activation pattern entropy metric and a graph theoretic metric of neuron communities. They found that entropy is negatively correlated with training accuracy, modularity is correlated with training accuracy, and that the number of well defined neural communities increase with accuracy. Strengths:  The metrics and evaluation procedures are clearly explained. It also does not mean that a representational metric is predictive of the performance of the fully trained model. I would be more convinced that these are important metrics if they could be incorporated as a regularization term in a loss function (Ex.Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs (Leavitt & Morcos 2020)) or they could be used to predict which models will generalize better (Ex.Fantastic Generalization Measures and Where to Find Them (Jiang et al.2019))  I do not think that the entropy metric is entirely novel. Such an experiment could demonstrate if the metrics studied in this paper are measuring an important model representation structure necessary for high performance or a representational coincidence of their particular training method. Overall, the metrics in this paper are sensible and clearly explained, but further experiments are necessary to demonstrate insights into what models are learning or what directions for performance improvements the metrics can suggest.<|endoftext|>An important problem of understanding the performance of deep learning models is studied by this submission. Two graph theoretic and information theoretic metrics are checked in some datasets with MLP networks. Preliminary results with expected correlation are observed. Understanding why deep learning works by inspecting the internal representations beyond loss value or performance (accuracy) is an important topic. This paper tries to advance our understanding with two newly proposed metrics, which is a good point. However, several major concerns are obvious:1. Not enough literature survey. Given the bold literature on both understanding deep learning and entropy utilization, it is hard to say this paper is the first to do so. 2.No baseline methods to compare the significance of the two proposed metrics. Frankly speaking, I m not an expert in explaining deep learning, but I believe there have exist many methods to explain deep models  performance. The significance of methods proposed in this paper should be compared against existing approaches. It is not clear whether the method scales to larger and real world problems, as admitted by authors in the paper. By the way, I m curious about how the experiments are done. In section 6, what do you mean by "iteration"?<|endoftext|>However, I have several important questions/comments to the authors:1) In page 6, the authors clearly state that "we only used fully connected (FC) and dropout layers". However, in the supplementary material (page 1), the authors present a graphical representation of the deep learning model architecture used, and they include convolutional blocks. Three questions in this regard:  do the authors employ or do not employ only fully connected layers? if what is stated in the paper is correct, why to employ only fully connected layers if the problem tackled is image classification? H2: The modularity of the activation pattern graph is related to a deep learning model s performance. H3: The entropy of the activation pattern is related to a deep learning model s performance. Regarding H1, the authors conclude that "for models with good performance, the number of well defined communities increases with training". It is not totally clear to me how this hypothesis (H3), and the associated experimentation and conclusions, effectively contribute to a better understanding and evaluation of the performance and explainability of neural networks. 4) All experimental evaluation is performed on quite "small" architectures. In other words, I have the feeling that the results are hardly reproducible. On the other hand, there are no competitor approaches to compare the performance obtained by these metrics/methods. It would be interesting to see how the techniques presented in this paper compare to other techniques already present in the literature. Interesting paper and relevant subject.<|endoftext|>This paper examined novel neuron activation pattern analysis on neural network classification models via graph theoretic and entropy based methods. The main technical contribution of this paper comes from explaining the neural classifiers by combining the graph theoretic and information theoretic approaches. Strengths  Suitable and persuasive application of the graph and community structure to explain  the activation behavior and relations of the neurons. Appropriate usage of the entropy and modularity score to effectively measure the tendency and the predictability of the neuron activation. The scalability of this work is suspicious. There ‘s no guarantee that proposed activation analysis methods will also show the same correlation tendency in other neural networks with more complicated architectures than  multi layer perceptron networks. To compute the entropy, activation pattern matrix is required for each layer. Computational overload will be inevitable when it’s trained on larger data and models. Aside from the architecture, the authors only had done experiments on the dropout conditions. Experiments on other various hyperparameters/conditions that can affect neuron configurations or learning ability of the neural network(e.g., pruning) could add more validities to their ‘comprehensive experimental study’. Nevertheless, the scalability and practicality of the work should be examined carefully.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This work studies the loss landscape of domain adversarial neural networks for domain adaptation. The authors then proposed to apply a previously proposed approach to enforce smoothness (Sharpness Aware Minimization), so that only smoothness with respect to the task loss would be enforced. and a sound empirical validation of the proposed approach. Further experiments showed that SDAT performed better than other smoothing techniques, as well as yields improved robustness to label noise in comparison to not enforcing smooth minima. The main contribution of this work is to propose applying Sharpness Aware Minimization (SAM) to smooth only the task loss term of domain adversarial training. Motivation:    Although the authors (to some extent) empirically show that SDAT can improve the performance on the target domain on the considered cases, there is no justification on why one would expect better out of distribution generalization for smoother minimum. Results:    The relevance of both theoretical results are not clear to me.<|endoftext|>Empirical studies are implemented to verify the theory and show the soundness of the proposed method. 2.It fully discussed why we should not apply the smoothing method **on the discrepancy term**, and verify the theory empirically through experiment, further developing smooth theory with regard to **adversarial objectives. The method for smoothing ERM is more directly adopted from SAM rather than a novel idea, and the relation **between** smoothness **and** better generalization has also been discussed by previous papers (SWAD), therefore renders this paper not insightful enough. Overall, the author has well established a positive correlation between smoothness of classification loss and generalization capability on target domain, and a negative correlation between adversarial counterparts. The method of acquiring a flat minima also brings promising results on multiple DA classification and detection datasets. However, as I can see the experimental studies of the proposed smoothing method are well designed and theories are rigorously proved, I am not completely convinced by the novelty of the work.<|endoftext|>This paper introduces a smoothness penalty for domain adversarial training. * Ablation experiments provide insights to the effect of a smoothness parameter. For example, how does the smoothness of the classifier (focus of this study) interplay with smoothness of the domain discriminator? This combination is not well explained in the paper. [3] Miyato et al."Spectral normalization for generative adversarial networks." Paper with theoretical motivation and results on both classification and object detection. However, the limitations of the proposed method are not clear. When would smoothness not be useful and how do the improvements depend on the other tricks for training Domain Adaptation models.<|endoftext|>The paper analyzes the role of smoothness in domain adversarial training (DAT). The main insight of the paper is that smoother minima of the classification loss improve generalization in the target domain. To further improve the generalization performance by leveraging this phenomenon the author(s) introduce(s) smooth domain adversarial training (SDAT) for classification and object detection. STRENGTHS:  The paper is very well written and introduces all used concepts appropriately. This makes the paper easy to understand even for readers that are not familiar with domain adaption. It is shown that SDAT can be combined with the existing methods CDAN and CDAN + MCC to increase generalization performance. Furthermore, improved performance on domain adaption datasets for object detection is reported. Additional experiments show that SDAT also increases the robustness to label noise and that SAM outperforms other smoothing techniques. The theoretical and empirical results are convincing and the paper is of high quality.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 5; ICCV 2021This paper applies knowledge distillation to image based monocular 3D detection and achieves state of the art results. However, there are some concerns in terms of novelty and generalization ability. The overall model achieves state of the art on the KITTI benchmark while maintaining impressive efficiency.<|endoftext|>The paper presents MonoDistill, a way to enhance RGB based 3D object detection through knowledge distillation from a LiDAR based teacher. Each component is carefully ablated and experiments are conducted on the KITTI benchmark. W2.Minor notation information: In eqn. (3) information is missing on the index k.W3. These are (in order of appearance):"v.s."<|endoftext|>In this paper, authors propose a monocular image based 3D object detection method based on knowledge distillation. At the test time, the model detects 3D objects without any intermediate depth prediction and ranks 1st on the KITTI benchmark dataset. Hence, this contribution to infuse depth cues directly to the monocular network via distillation is an important contribution. (5) Table 3. However, there are some minor issues in some of the claims (please see above).<|endoftext|>This paper proposes to distill the features from a LiDAR teacher model to a monocular based student model. The comparison with the baseline model in KITTI validation is missed and should be supplemented. As there are some papers [1] that demonstrates the effectiveness of cross modal distillation, I think the paper should at least be compared with the baseline KD approaches and demonstrate the superiority of the proposed modules such as ``scene/object level distillation , whether to distill affinity map and discuss why the proposed methods work better than simple KD for monocular 3D object detection, or particularly depth estimation.<|endoftext|>(1) This paper choses this KITTI to conduct experiments. And there are many writing errors in this paper. The results on the KITTI test set should be submitted to the KITTI official website, so please submit your results anonymously on the KITTI official website, otherwise the authenticity of the experimental results in this article will be seriously questioned. (2) In the 3D object detection task, more data sets have emerged, such as Waymo, nuScences, etc.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; While the ideas and methods are quite clear at a high level, the details are hard to follow, at least without deep familiarity with the Pertsch and Rakelly work. Similarly in Eqn. The paper proposes a nontrivial and effective way of combining skills extracted from offline training data with meta learning from training tasks. This combination introduces meta learning at the skill level.<|endoftext|>The experiments show a clear improvement over methods like SPiRL and PEARL. Weaknesses   Novelty and contribution. However, I did not see any convincing evidence to backup this claim in the environments considered in the paper. The backbone could be finetuned or frozen at meta training time. In other words, it is not clear the type of constraints one should impose on the offline data in order to guarantee robust performances at test time. One of the main claim of the authors is that the method is able to cope with long horizon.<|endoftext|>The paper proposes a meta RL method to learn to solve long horizon tasks with sparse rewards efficiently. The goal of this policy is to learn to compose learned skills to solve new tasks efficiently. To my understanding it is a clean extension of SPiRL where the main point is to formulate the high level policy learning as a meta RL problem. * The meta training task distribution analysis shown in Figure 6 is also an interesting experiment. * The proposed method use extra annotations at test time which are target trajectories. The proposed method builds on SPiRL and proposed a meta RL learning of the high level policy.<|endoftext|>This work considers a new setting where you can leverage both 1) offline "play" data that does not contain reward or task labels, and 2) meta training tasks in order to quickly learn new tasks at meta test time. The main strength of this work is its strong experimental results in the new setting it proposes. At the same time, the main contribution of this work is just a straightforward combination of two existing algorithms, SPiRL and PEARL.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Even though no new biological insights were obtained in the scope of this paper using that knowledge, the parallel between the two frameworks deems useful for models of the hippocampus in follow up works and highly relevant to the community. I think it would be nice to include a slightly more detailed description of the TEM model in the paper.<|endoftext|>Transformers are related to TEM models and Hopfield networks in computational neuroscience. A neurobiological model of TEM transformers is presented, which contains feature neurons and memory neurons. The problem with this logic is that the addition of the quadratic term changes the mathematical properties of the model. This paper nicely connects several ideas discussed recently in the AI and comp.<|endoftext|>This well written and clear paper clarifies the relationship between transformers and a recent exciting model of the medial temporal lobe in neuroscience.<|endoftext|>The paper postulates a task that requires the ML model to capture the spatial nature of the task in order to perform well. From there the works suggests the existence of a special relationship between the Transformer architecture and the mechanism by which hippocampus encodes spatial information.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper presents an attention mechanism that also serves as intrinsic regularisation for a two agent reinforcement learning framework of collaborative tasks. The method is evaluated in simulation using two manipulator robots, and has been compared against a simpler version of the proposed method (attention without regularisation) and a standard multi agent RL method. The paper presents a simple method for regularising the training of a multi agent system. They do not provide statistically significant results, as they only evaluated with 3 seeds, and this is evident in the results. I find the idea simple and intuitive. I have strong concerns regarding the plenty assumptions being made, in particular for the pre computed interaction areas, and the scalability of the method to more than two agents. The authors falsely abuse the term bimanual manipulation. Apart from that the authors abuse the term safety. The authors seem to also be using HER for a goal based RL setting.<|endoftext|>The empirical evaluation demonstrates that the proposed method is generalizable to unseen situations thanks to the attention mechanism with the regularization. ### Strengths  The proposed idea is very intuitive and well motivated. The paper is well written. Clarification on HER is required. Given the high variance in the proposed method and comparable performance of the "Attention" baseline, it would be more convincing to have more random seeds. This paper proposes a novel inductive bias (the attention network and regularization) for efficient bimanual manipulation, which outperforms the simple MLP and multi agent baselines. Can the authors provide the video version of Figure 5 over an episode to make sure it is not cherry picked? Although this can generally prevent collision between two robots, it also limits close collaboration between two robots. The explanation about the generalization capability of the proposed method can be more thoroughly investigated.<|endoftext|>This paper proposes an attention based solution to dual arm robot manipulation from sparse rewards that relies on a novel idea for intrinsic regularisation. However, there are some details around the claims and contributions that appear not clear from the current state of the paper. The proposed approach aims to reduce the problem of extracting a dominating agent in collaborative settings and to reduce the number of collisions between operating robots in a shared workspace. This work is evaluated in simulation and the obtained results demonstrate the ability of the proposed solution, DAIR, to not only improve both the success rate and sample efficiency of the learning process but also to reduce the number of conflicts between the two operating arms. Perhaps the solution results in faster learning but it does not seem more efficient. There is a lot of work in robotics done on safe collaboration in bimanual settings too [3,4,5] too, so it is not clear why this is a contribution for this work. The problem of interest in this work asks for more thorough evaluation with respect to the number of seeds. These are not novel observations and are therefore not a contribution to this work. If the methods are not comparable for some reason, then its reasons should be explained clearly.<|endoftext|>The proposed method is evaluated in simulation, using two robot arms to solve a variety of tasks on table top environments. The paper is written in a clear way, it is well motivated and it clearly defines the domain challenges that are going to be addressed. Is it defined based on the robots’ states in simulation? The previous point on state encoders and agents is not made clear in Fig.2 either: here, all encoders have the subscript “i”, does it mean that there is only an agent “i”? Do they indicate the target position of the two robots + objects? Success rates for adaptation stages in Table 3, even though better than the Attention scores, show a significant drop in performance with respect to the learning cases. Can you discuss this drop? The paper presents a well formed idea and method, but has some points to be clarified that would improve the manuscript.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; The paper discusses conditions under which SGD with momentum (SGDm) would diverge. For the specific Gaussian setting of linear regression with oscillatory covariate shift, a driven (with time dependent parameters) linear oscillator ODE is formulated for the parameters under SGDm. This allows to derive conditions, related to the learning and momentum rates, under which parameters would con /diverge. Theoretical predictions are tested empirically, and it is shown that the basic phenomenon (resonance or at least suboptimal convergence) is also present in setups with noise, non harmonic oscillations, other optimizers (Adam), and nonlinear regression models (NN). In general, I really like this study. It is very systematic and thorough, with theoretical considerations as a starting point, also very well written.<|endoftext|>This paper analyses the phenomenon of resonance in momentum SGD (SGDm) under a time dependent covariate shift. In the paper, it is shown that such a setting corresponds to a _parametric_ oscillator of the parameters, which explains instability and divergence in a non iid SGDm setting. A hypothesis is proposed about a phenomenon, the relevant math is derived, and then tested empirically in various settings that help us assess the correctness of the hypothesis and the relevance of the concept when departing from restrictive assumptions about the setting. That being said I do think the current empirical content of the paper is valuable. I recommend accepting this paper. The empirical results which I am able to evaluate are correct, useful and informative.<|endoftext|>This paper studies the behavior of SGD with momentum when the training data are not iid. Numerical results show a dependence of the optimizer performance and the covariance pattern in much broader settings than linear regression, even including neural networks. It is important in the fields of reinforcement learning, online learning, etc. However, it is still possible to prove the convergence of the discrete dynamics (the SGDm), because the dynamics is linear. Hence, the reviewer suggests the authors to do this analysis. But I don t think it unimportant, because this is the quantitative description of resonance phenomenon that lies on the focus of this paper. However, it is hard to say this connection, as shown by figures for nonlinear models like neural networks, originates from the same mechanism as the resonance. Is SGDm still shows an oscillating behavior in this case? The paper studies the influence of data covariance shift to the performance of SGDm.<|endoftext|>This paper studies the convergence of SGD with momentum under linear regression with covariate shift. For example, the periodic covariate shift in Section 4.1 does not converge to a stationary distribution. Then, the paper proceeds to show that if the covariate shift is periodic, the convergence/divergence of the ODE can be determined by looking at the spectral radius of a matrix called the monodromy matrix. However, I believe this is not the case; the paper is providing a sufficient condition for divergence. The paper moves on to show experiments that test the theoretical characterizations and also investigate if different settings under relaxed assumptions show similar "resonance" behavior. Although the paper studies an interesting setting and identifies an interesting failure mode of momentum methods, I believe the theoretical contributions as well as implications to practice are not strong enough to grant acceptance (comments 2 & 5). There is room for improvement in terms of presentation (comment 3), and also the analysis is carried out only using the expected gradients which I find somewhat doubtful (comment 4). I think the solutions $\psi(t)$ are obtained numerically and it is hard to get any closed form solutions, but at least providing a more detailed explanation on the frequency responses and "peaks" should be helpful.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper proposes ViDT, a high performance Detection Transformer with an impressive accuracy speed trade off. A lot of experiments as well as ablation studies are conducted to prove the effectiveness of the proposed detector. The design principle of ViDT can also generalize and inspire future detector design. It shows that Detection Transformer family also has the potential to be as strong as the highly optimized detectors, *e.g.*, the YOLO series. These results are quite meaningful and can be served as valuable references & baselines to the community as well as future work. #### Weaknesses  I believe there is one missing detail about the model design. Overall, I believe this is a meaningful paper from the perspective of both object detection performance and research.<|endoftext|>Strengths  Compared with other transformer based detectors, the proposed detector is effective and efficient, which has a high performance with a fast speed. The comparison with other transformer based detectors is written well and readable. Weaknesses  Though the proposed detector is effective and efficient, the novelty is limited. There are some concerns about  Reconfigured Attention Module (RAM). Could you provide more details about how to process [DET] $\times$ [DET] and [DET] $\times$ [PATCH] at once? Second, the proposed RAM is not novel and may not have great contribution to the performance. The function of this module is the same as the transformer decoder. Could you provide an experiment that removing [DET] $\times$ [DET] from the RAM module except the last stage? I appreciate the effective and efficient of the proposed detector, but I prefer to reject this paper for the above weaknesses.<|endoftext|>Could something similar be done with YOLOS? This should be discussed as well. DETR (vanilla and deformable) results are not given with the base models. Post rebuttal update  I am not entirely convinced that a simpler baseline improvement to YOLOS can not be devised. But the proposed method is surely one way to go forward. The results and discussions could also be valuable to the readers. I am not upgrading my rating but I wouldn t argue strongly for rejections, specially when there is some support from the other reviewers. The paper is generally well written, however one of the main contribution, the reconfigurable attention module, should be more precisely and formally described, at least an appendix. The experiments should be made more complete with fair comparison for baseline backbones etc. And the results with conv backbones should also be given for completeness and benefit of the reader.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper proposed a novel viewpoint in domain generalization, i.e, the loss function search. Overall the paper is interesting since the author provides an alternative perspective in domain generalization: searching the proper loss function. I am wondering how you addressed this. Although this idea is not completely novel in the related domains such as NAS or AutoML, applying them to the domain generalization seems quite interesting and novel. The empirical studies and analysis are extensive. This paper adopts the idea of NAS or autoML to search for the proper loss in the domain generalization. The proposed idea currently seems a bit ad hoc. **I totally agree with the author’s viewpoint about the problem revealed in domainBed. However, in this paper, the author fails to clearly justify why the proposed parametric loss $w$ can effectively solve this problem. "Scale of the domain shift is similar enough between meta train and deployment datasets." “But please note that something like MNIST >SVHN transfer is not exactly the problem setting we address.” It is not domain generalization (trained on digit A and deployed on digits B)? To address this, the author should provide a clear theoretical analysis to show when the proposed method can work. Why do we choose this specific loss? Algo 1, Line 15 there is no description of the grad surgery approach. Clearly, one source could not generalize to other related targets. But it requires a major revision of your paper to fully justify it. For example, I would suggest testing the linear combination and showing its results. In the current version, it is more or less we proposed loss family A (without strong motivation or benefits) and it works in some classification benchmark.<|endoftext|>This paper proposes a loss for domain generalization. The loss is learned by meta learning on based on RotatedMNIST. The paper evaluate the loss s generalization ability in several other datasets. + The idea is interesting. The proposed loss is actually multi label loss which regards the multi class problem as a multi label problem although the label vectors are always in the form of one hot vectors. So it s more fair to compare the proposed loss with multi label loss, instead of the multi class cross entropy loss, as the multi label loss might be a good baseline for DG. I doubt this paper can achieve SOTA performances in all the benchmarks. The writing and clarity can be improved in the following aspects:a. the texts in fig.<|endoftext|>Abstract: "enables simple ERM to outperform signficiantly more complicated prior DG methods"  > I disagree with this statement   the amount of work needed to come up with the loss function is arguably larger than e.g.the CORAL technique. The authors design a scheme for meta learning loss functions for domain generalization: Based on the RotatedMNIST problem, a new implicit gradient algorithm is used to select a loss function from a chosen parametric family, and then applied to other domain generalization problems. The paper is well written and well structured. The idea of meta learning a loss function based on a given domain generalization task is interesting and (to the best of my knowledge) novel. I assign a score of (5) for now, but would be very happy to increase it   based on the results already in the paper, I think the paper could become an interesting contribution to ICLR, if presented correctly. Table 2 is missing confidence intervals or standard deviations / standard errors. I suggest to re run the experiment in Table 5 on a different dataset. If the authors add a bit more critical discussion on their method to the paper, I would hence recommend to accept the paper. It is good that the full tables are in the appendix to give a better overview of where the method works, and where it fails. This is not the only example: E.g. The conclusion does not match the paper story and should be improved. I highly recommend to write an extra paragraph on the limitations of the presented experiments. ### Additional questions  What informed the choice of 0.025 as the signficance threshold? Could you clarify what was actually compared? Looking at Table 6 suggestions that e.g.CORAL is often close in performance (despite being a quite old algorithm), the only clear improvements are observed on TerraIncognita and to some extend on OfficeHome.<|endoftext|>Authors introduce a bi level optimization procedure aiming to learn parametric loss functions that result in predictors able to better generalize to new data sources, unseen during training. Specifically, an outer optimization loop iteratively updates a parametric loss function to minimize the empirical risk (under a standard loss) on a left out data source, not presented to the model during the inner optimization loop, which trains the model on a set of domains using the current version of the parametric loss. Strengths:+ The paper is well written and easy to follow, the idea is presented clearly. + The idea is novel and interesting; the empirical observation that learned losses transfer to new tasks results in a practical framework. + The proposed loss can be directly combined with other mechanisms to further improve performance; e.g., it could be used along with some invariance inducing strategy. Weaknesses/questions/suggestions:  It s possible that the proposal requires assumptions that are not discussed in the manuscript. [1] for an in depth discussion on the effects of conditional shifts. In particular, if the proposal induces some type of domain invariance, be it in the feature or prediction level, it would likely damage performance relative to standard ERM in such cases. The fact that learned losses transfer to new tasks is clearly supported by the reported evaluation. In that case, it would be also interesting to get an idea of the computational cost involved in running Algorithm 1 in a larger scale task such as PACS. Is there any reason why this particular approach would work better than simply averaging? Authors follow the common practice within domain generalization literature and perform evaluations using models pre trained on ImageNet.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper explores the "memorization" in adversarial training. Through some empirical experiments and theoretical analysis, this paper points out two findings: (a) memorizing atypical samples can improve DNN’s accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) memorizing some atypical samples can even hurt the DNN’s performance on typical samples. However, although the direction is promising, the major concern is about the novelty of the main contribution since the similar findings in this paper have already been pointed out and analyzed in previous studies [1,2,3], and simply dropping some called "atypical" data can also improve adversarial training [3]. Overall, this paper explores the memorization effect in adversarial training and proposes benign adversarial training which further enhances adversarial training. Pros:1.This paper makes a connection between the memorization effect of DNN with adversarial training and provides some comprehensive experimental results on analyzing the effect of training data for adversarial robustness. The similar "important" findings in this paper have already been pointed out and analyzed by previous studies [1,2,3]. For example, in [1], the authors also investigated memorization behavior in adversarial training and have pointed out that overfitting some rare examples in training can just improve the clean accuracy, and ignoring these rare examples helps in adversarial robustness. It may be better to add some citations to justify or provide some empirical results, especially for the memorization behavior in adversarial training. 3.The motivation for exploring or connecting memorization with adversarial training is not clear and not strong (At the top of page 2). Can benign adversarial training also effectively utilize the long tailed data since it seems to be similar to the atypical samples mentioned in this paper?<|endoftext|>This paper investigates the memorization effects in adversarial training, especially on the effect of memorizing atypical examples. Based on the empirical observations, the authors propose benign adversarial training (BAT), which is evaluated on CIFAR 100 and Tiny ImageNet. The writing is clear, with detailed descriptions of the proposed method. This largely weakens the performance of baselines [b]. Besides, more advanced methods listed in [c] should be considered on CIFAR 100. CIFAR 100 and Tiny ImageNet are not quite widely used in the adversarial community. It would be more informative to have the results like Table 1 on CIFAR 10, where many benchmarks exist to better estimate the effectiveness of BAT. Minors:  As suggested by [a], weak attacks like FGSM should not be treated as an evaluation, at least not in the main text. [b] Overfitting in adversarially robust deep learning. Interesting observations on the memorization effects of atypical samples in adversarial training, but the empirical evaluations should involve stronger baselines (e.g., using early stop) and adaptive attacks (e.g., considering the BAT modules), while it would be much more informative to have, e.g., AA results on CIFAR 10.<|endoftext|>This paper studies the role of atypical samples in the context of adversarial robustness, and further introduces a variant of adversarial training based on these observations. Even though the paper is initially motivated by atypical samples, the mechanism relating atypical ness to adversarial robustness is not clear and moreover it seems to me that the analysis in "Poisoning Atypical Samples" and the algorithm design that follows can be better understood from the perspective of margins and adversarial robustness; examples with smaller margins (closer to the decision boundary) are necessarily more susceptible to adversarial attack. It would be useful to isolate the effect and importance of the atypical ness of an example in the algorithm, for instance by applying the algorithm to all examples not just the atypical ones. Other comments/suggestions:   Metric learning was previously used for adversarial robustness in Metric Learning for Adversarial Robustness, NeurIPS 2019 though in a different way  In light of the relation to margin, some related work from the margin perspective that could be discussed include   * Helper based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade off, ICML Workshop on Adversarial ML 2021  * MMA Training: Direct Input Space Margin Maximization through Adversarial Training, ICLR 2020    In particular, the first paper achieves similar results as the current work by restricting the margin increase through different means; the downweighting of low (or even negative margin) examples in BAT has a similar effect. Typos:  * Sec 3.2 on p4: trails  > trials  * Theorem 1: the condition $d_{+1} \leq 2\eta$ should be on $d_{ 1}$ insteadOverall the proposed BAT algorithm achieves promising results that improve clean accuracy at similar robustness levels, but the design of the algorithm seems to involve elements that are orthogonal from the initial motivation and study of atypical examples. **Post response update:** The authors have comprehensively addressed my concerns about relation to margin and importance of the atypicality to the components of BAT through ablation studies.<|endoftext|>The authors study the interaction between adversarial training (meant to produce robust models) and atypical examples in standard datasets (examples where generalization is driven by a handful of examples, cf [Feldman 2020]). They also find that adding atypical examples to adversarial training can actually hurt test accuracy, in contrast to standard training where such examples typically help. Finally, the authors propose a method that down weights  atypical examples during robust training and introduces a contrastive regularizer, which they term BAT (benign adversarial training). While previous work has discussed "atypical examples" in the context of adversarial training, this is (to the best of my knowledge) the first work that performs such a study in a rigorous manner by considering proper measures of atypicality through influences. The findings are thus not always surprising given prior work, but are nevertheless interesting and the proposed method does show promise. There are however a few issues that I would like to point out:  The theoretical setting is actually quite similar to the one in [Sanyal et al.]([https://arxiv.org/abs/2007.04028](https://arxiv.org/abs/2007.04028)) (cited). This does diminish the impact of the paper somewhat. Is there a more rigorous way to explore this phenomenon? E.g., by choosing samples with high influence and verifying them through human annotation? It is not entirely clear why the discrimination loss regularization would be beneficial in this setting. The authors do provide a justification based on the class margin in representation space. Overall, despite its shortcomings, the paper presents some findings that would be of interest to the community.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper proposes to learn partial equivariances from data by training a group equivariant CNN (G CNN) where the group elements are pruned at each layer. Results show that the partial equivariance performs outperforms the full equivariance when the data itself is not fully rotational equivariant. One of the weaknesses of group equivariant CNNs is that one has to assume what is the group that acts on the dataset, which might be hard to know in practice. I think the paper addresses an important problem and proposes a seemingly reasonable method that might be useful in practice. The paper proposes a way to start from some group of transformations and learn to prune it differently at each layer. Please report similar figures for the SE(2) case. 2) Results would be more convincing if compared with the aforementioned handcrafted methods of breaking equivariance. These would highlight the advantages learning partial equivariances per layer. # Questions1) Please elaborate on which discrete groups are unstable to train as mentioned in section 6. # MinorFigures 1 and 2 are hard to read.<|endoftext|>## Strengths  1. The writing is clear and easy to follow. The relationship with relevant works is discussed sufficiently. 2.The proposed partial equivariance is novel and several different cases of the subsets are properly discussed (e.g., group to subset, subset to group, etc.) One major concern is that the partial equivariance seems not motivated properly. It is not clear why and when the partial equivariance will be needed in the natural datasets. More analysis/visualization/experiments/concrete examples should have been included to motivate the need for partial equivariance and show its specific benefits. More experiments on larger and natural datasets are needed.<|endoftext|>This paper introduces the notion of partial equivariances which are defined as being equivariant with respect to a subset of a group. Is there a case where it would be beneficial if the learned distribution is not uniform? The authors validate their proposed model on simple datasets that require different degrees of equivariance such as RotationMNIST, CIFAR10, and CIFAR100. "On the generalization of equivariance and convolution in neural networks to the action of compact groups." The writing in this paper is also well done for the most part and the overall presentation is of high quality. For example, in Eqn 3. I appreciate the authors outlining the important samples in red but still it wasn t clear to me. ***Technical Issues:***This paper has a few other technical issues. Furthermore, from the experiments it is unclear if the authors used Steerable G CNNs for their baselines.<|endoftext|>The paper studies an interesting problem proposes a simple solution and has sufficient empirical support for main claims. However, the technique parts and over clarity needs significant work before publishing. Existing G CNN literature mainly focuses on full equivariance, and partial equivariance is seldom explored. The proposed partial group convolution is a simple yet effective solution. This paper proposes a solution that only requires a small modification of existing G CNN architectures, and the authors have empirically shown that the solution is effective. Clear messages from the experiments. **Weaknesses**:My main concern of this paper is its presentation and technical statements, which is detailed below:  Misleading terminology/notations. I still find the technical parts very hard to follow and check their correctness, mainly due to the somewhat misleading usage of terminology and notations. Does $p_i$ in Section 4.3.1 (the discrete group case) define a categorical distribution over groups? If that is the case, then the probabilities should sum to 1? But then the paper set all the probabilities $p_i$ to $1$. **Questions**:  If there is an image dataset, for some of the data, the corresponding group subset is $[ 60, 60]$, for the rest, the corresponding group subset is $[0, 90]$, what would the overall learned group subset be? But I think it still solves an interesting problem with a simple solution, and the experiments are sufficient.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; Strengths:1) The paper is easy to follow with clear motivation. The discussion and analysis about “false negatives” on contrastive learning in Fig1 is quite valuable. Besides, the ablation study also validates the contribution of the main component of the proposed IFND. It might be better if the authors can explore some robust learning techniques (e.g., self paced learning) to improve learning efficiency. 3) It seems that the experiment results do not validate the effectiveness of the introduced attraction strategy according to Table 5.<|endoftext|>I like the idea used in the paper and analysis shown in the paper. One of the most interesting experiments they show is that as they increase the number of classes in the dataset the performance drops more and more. Adding these results would make the paper stronger. Strengths: 1) Well written and easy to follow paper with a very solid hypothesis and analysis of effects of “false negatives” on contrastive learning.<|endoftext|>Overall, I think the paper is on the boarderline. While the paper briefly introduces PCL in the related work section, the paper should clearly state PCL as the preliminary in the main method section. Limited novelty and inappropriate credit for PCLThe proposed method is somewhat incremental over the prior work, PCL. Nice details on the method, e.g., linear scheduling of the acceptance rate, to be hyperparameter free.
Reject; rating score: 5; rating score: 5; rating score: 5; In absence of that, it is only fair to compare the proposed method to state of the art. However I find it hard to draw meaningful conclusions from the paper due to lack of proper apples to apples comparisons. Using a BEIT style framework (as authors also recommend for future work) would make this paper much stronger in that regard.<|endoftext|>This paper proposed a new pre training method named VIMPAC for video understanding, which is a combination of specially designed masked token prediction objective and contrastive learning objective. * I m also interested if VIMPAC can work for tasks beyond action classification (might be hard due to the architecture design). Ablation studies on contrastive learning also lead to some counter intuitive conclusions so I would expect more analysis and insights from this part. * Although the mask token prediction task has been proven to be an effective pretraining objective for image tasks, the block masking method designed by authors is well motivated.<|endoftext|>Off she shelf VQ VAE is used in this paper for discrete video tokens generation. + the proposed block masking scheme makes sense in terms of avoiding information leakage from nearby tokens and it is also validated to be quite better than i.i.d.+ extensive experiments are carried outWeakness:  novelty: （1）Combining masked token prediction (MP) and contrastive learning (CL) is quite straightforward. performance and evaluation:   (1) the performances on "spatially heavy" datasets, such as UCF101, K400 and HMDB51, are far from the state of the arts, rather than comparable. (2)  Now that VQ VAE can be harmful to the spatial information, why it should still be leveraged. There is also no evaluation on this part.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The strengths are:  The authors propose a new algorithm based on the idea of  energy  variable from the work  [AEGD: Adaptive Gradient Descent with Energy]. The weaknesses are:  Firstly, the theoretical results use a lot of assumptions. The bounded function value is a strong assumption because it assumes that the algorithm can not go to some bad points with large output. Secondly, the convergence of this method depends heavily on the lower bound of $r_{T,i}$. In addition, we can not choose an arbitrarily small learning rate to satisfy a positive lower bound, because it may harm the result of Theorem 4.2. Finally, the experiment of SGDEM does not show encouraging performance (over existing methods) for CIFAR datasets. Since this is a hyper parameter, it must be chosen in advance to satisfy that $ c$ is a lower bound of every possible input. Note that this hyper parameter $c$ also depends on the stochastic function value, not only $f$To summarize, the authors proposed a momentum method based on AEGD using gradient and function value information. Hence I will encourage the authors to improve this paper and resubmit to another venue.<|endoftext|>The paper  extends the adaptive gradient descent with energy method by incorporating momentum to obtain a new variant called stochastic gradient descent with energy and momentum (SGDEM). Numerical experiments demonstrate the effectiveness of SGDEM over AEGD and other baselines. Strengths:  Convergence analysis for SGDEM are shown for both nonconvex and convex settings. Weaknesses:  There is not much novelty in algorithm design as the idea of using momentum is quite common. It limits the applicability of the proposed method. For convex analysis, the assumption that the domain is bounded and bounded function value are also strong. However, the theoretical results rely on additional assumptions which are rather strong including bounded gradient, bounded function value (and bounded domain for convex setting) apart from standard assumptions. Numerical experiments only consider deep learning examples which is not extensive enough to illustrate the practical performance of SGDEM.<|endoftext|>The authors provide an energy dependent convergence rate for the non convex stochastic setting and a regret bund in the online setting. The readers can easily understand the paper. Weaknesses:1: The contribution of SGDEM over AEGD is limited. Although theoretical analysis is provided to verify the effectiveness of the proposed algorithm, the advantages of SGDEM over the AEGD are unclear. 2:  The motivation to incorporate the momentum mechanism is straightforward since the momentum is widely used for optimization methods such as the popular SGD with momentum. For most of the experiments, the proposed method performs worse than baseline AEGD even with the existence of oscillation. 4: The experiments are only conducted for vision tasks while NLP is a very important application in deep learning. The optimization method should also be essentially tested for NLP tasks. I have checked the response. I still think the idea is not novel enough and the experimental results are weak.<|endoftext|>In this paper, the authors propose an algorithm called SGDEM, which is a combination of the AEGD method and the momentum method. For the proposed method, the authors have shown a convergence to the stationary point for the nonconvex case and a regret bound for the online convex case. However, there are some issues w.r.t.the current analysis, for which the reviewer has no idea how one can fix that. The experiments of the paper have shown some performance improvements. The main issue:In Theorem 4.2, the reviewer finds the LHS of the bound is deterministic (after taking expectation), while the RHS is stochastic due to the presence of $\min_i r_{T,i}$, which is confusing to the reviewer. This seems to be the following mistake in the analysis. In the first equation under Eq.(33), the authors seems to argue that $$E\Big[ \min_i r_{T,i}\cdot \sum_{t 0}^{T 1}||\nabla f(\theta_t)||^2 \Big] \geq \min_i r_{T,i}\cdot E\Big[ \sum_{t 0}^{T 1}||\nabla f(\theta_t)||^2 \Big].$$However, this is not right in general since $\min_i r_{T,i}$ is a random variable and is dependent on $E\Big[\sum_{t 0}^{T 1}||\nabla f(\theta_t)||^2 \Big]$. It is also not possible to write $E\Big[ \min_i r_{T,i}\cdot \sum_{t 0}^{T 1}||\nabla f(\theta_t)||^2 \Big] \geq E\Big[\min_i r_{T,i}\Big]\cdot E\Big[ \sum_{t 0}^{T 1}||\nabla f(\theta_t)||^2 \Big].$ The reviewer does not have any idea to fix this issue. And it should be formally pointed out by the authors. In the worst case, the authors should at least formally make the following assumption, and point out that this assumption is UNCHECKABLE. The authors use both $f(\theta;\xi)$ and $f(\theta,\xi)$ in this paper, please unify the notation.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper proposes a method to construct graph embeddings that are well tuned for answering shortest distance queries (SDQs), based on betweennes centrality distance sampling. The motivation argument appears to be false: it is claimed that one should take into account remote nodes under a graph’s shortest distance metric, yet it is not clear why such remote nodes should be relevant. The method is compared experimentally to classic graph embedding methods on several real world datasets. The largest graph use in the comparison comprises some 5K nodes. That complexity would allow running the algorithm on larger data, yet no experiment to that effect is shown.<|endoftext|>Specifically, the authors propose a new random walk framework based on betweenness centralities, and a distance resampling strategy that uses the shortest path distances, and the betweenness centrality scores and is shown to capture well shortest path distances (Proposition 1). The authors also evaluate their framework experimentally, verifying that compared to other popular node embedding methods, it can be used to represent shortest path distances faithfully. Based on my comments above, I suggest that the paper is not ready yet for publication, despite having some interesting ideas. There should be at least a comparison to such combinatorial approaches, or solid experiments that show that embeddings you propose are also useful in other tasks (or ideally both).<|endoftext|>This paper proposed a new graph shortest distance embedding method. This method uses a betweenness centrality based random walk to sample paths in the graph and distance resampling step before optimization. They show that the estimated distance after embedding has a linear dependence with the original distance in the graph They also show that this embedding method preserves the shortest distance relation between points. In experimental results, they show that this algorithm achieves better accuracy than previous algorithms. Strength: This paper proposed a novel approach for graph shortest distance embedding. I think this paper provided some interesting ideas. The method proposed in this paper is interesting.<|endoftext|>This paper presents a new Shortest Distance Query technique, namely, the Betweenness Centrality based Distance Re  sampling (BCDR). The objective of this technique is overcome some drawbacks of traditional embedding based distance prediction methods based on truncated random walks and point wise Mutual Information (PMI). These drawbacks are a limited distance exploration and the lack of preservation of the shortest distance relation due to local optima. How is this handle by the technique is not thoroughly described in the article. Thus, works in this area should be cited. Overall, the paper has strengths as I highlighted above that merit publication as long as the clarifications on the policy for dealing with cycles and the clarifications on the proofs is provided. I recommend also including some related work on betweenness centrality for random walks and random walk betweenness centrality.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; It seems authors proposed a new deep equilibrium model through incorporating a layer of optimization into the network. Experiments based on CIFAR demonstrate the advantage of the proposed solution in their settings. Theortical analysis with two propositions justify the properties of proposals. Though authors include the concepts of interpretability in the motivation of this work. I failed to collect evidence to reject this paper, so I believe it might be a solide work. I hope AC and SPC could hear from other reviewers.<|endoftext|>The paper propose a new method which can efficiently utilize inputs with different scales, the propose method obtains better quantitative results in experiments. The performance improvements seem to be trivial, although it is impressive to see the model size comparison in multiple scale experiment. More experiments on higher dimensional datasets are suggested;3. Could the author provide some ablation studies investigating the contribution of each component (inner product term and diversity term)? There are several questions remaining, I m looking forward to the responses and discussions.<|endoftext|>Taken together, these design choices lead to better performance and smaller model size on image recognition tasks on CIFAR 10 and CIFAR 100 datasets, as compared to  prior work on Equilibrium networks that use single/multiple branch architectures. 2.Explainibility and ablation studies: The detailed ablation study shows the effect of design  choices such as Hierarchical Heritage and Diversity modules, perturbation size and regularizers. : The key ideas proposed in the paper (multi scale, Hierarchical Heritage, Diversity) are quite common in the explicit models (i.e., standard neural networks). 3.How would the approach scale and compare to state of the art architectures (or even a larger ResNet) trained on ImageNet? The paper proposes some interesting ideas for using multi scale inputs and feature hierarchy and diversity for training equilibrium models. The results on CIFAR 10 look promising, but I have some concerns about the writing and the novelty of the work.<|endoftext|>The paper introduces a multi scale network (called Multi Branch OptEqs or MOptEqs) that extends on recent results of [1]. Specifically, the authors propose to use several DEQ models to incorporate samples of multi resolutions. The authors further propose a modified training strategy (called perturbation enhanced training) to improve the performance of the model. Empirically, the proposed model shows better performances compared to the previous DEQ models on CIFAR 10, CIFAR 100, and Imagenette datasets. Concerns:* I have one concern about the additional computational/memory overhead of “unrolling”. I believe that the direct comparison is unfair. I believe the idea of the paper is interesting and the authors show the impressive results on CIFAR experiments. However, there are some concerns regarding scalability (since the proposed method performs unrolling), additional overhead, and experimental fairness.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; In an earlier version of this review, I wrote that I felt that the objective proposed by this work is very similar to an existing linear regression objective from 2018, known as Anchor Regression [1]. The present paper proposes to consider linear regression *only*, which of course makes for an easier problem, but it also means there is lots of existing work in this area (for which there is insufficient discussion in the related work). However, after further thought, I ve realized that the two are only equal in quite specific circumstances. Further, their explication of performance in the non linear setting is incomplete. We arrive at $2X^TX(w w_e^*)$. This tells us two things:1. Both of these points indicate to me that there is indeed something interesting going on with this objective, but this paper does not adequately explore the implications; nor does it properly discuss the previous related work (this paper takes existing work on logistic regression and applies it to linear regression, but there is a great deal of existing work in the causal literature on linear regression which is not discussed). Returning to the paper itself, I find the empirical and theoretical results quite weak and I think this submission needs work even if it was a totally new idea. I encourage the authors to revisit this section and work to clarify what precise mathematical claim they are making, and what this actually means in English. [1] Anchor regression: heterogeneous data meets causality.<|endoftext|>The authors provide some insights on how to avoid the potential failure of IRM based on the failure cases. And empirical results also validate the effectiveness of the proposed method. ii.Practical insights on avoiding the failure case are provided for a better understanding of the counter examples. The proposed method is restricted to mean squared loss and cannot be extended to more general cases. iii.There lack of enough baselines in experiments. This paper analyzes the counter example of IRM and proposes a new invariance penalty based on that. And baselines, as well as the empirical results, are not enough to support the method.<|endoftext|>Based on the newly proposed penalty, this paper introduces another implementation of the IRM, named IRMv2. Theoretical results guarantee that the proposed scheme can capture invariance when  in the linear case. This paper is well written but the innovation is insufficient. The form of proposed invariant penalty is similar to the one of invariant penalty proposed in [1] and limited to the least square loss. Similar theoretical result was obtained in [2] when  in the linear case. 4）There are a few places that are not very rigorous.<|endoftext|>If it turns out that the proposed approach can only be used with (non stochastic) gradient descent, this should at least be explicitly stated in the manuscript to ease the way for future work. While this is acknowledged by the authors, which is appreciated, I do believe this to be an important practical limitation, as this loss is rarely used for supervised classification. 4.The authors make a point that there might be a link between ill conditioning of the Gram matrix $\Phi_{e}^{T}\Phi_{e}$ and failure modes of IRM penalties, providing theoretically that some of the most well known counterexamples to these penalties do exhibit ill conditioning. However, the empirical results in Figure 2 in the appendix are somewhat conflicting. Thus, I consider the paper s topic timely and of great relevance to the field. + The experimental results do not provide compelling evidence that the proposed penalty is statistically significantly superior to the original formulation.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The paper proposes a novel method for the neural network verification problem. However, the proposed variable splitting is novel as it allows closed form updates for the primal variables. However, authors have not compared against recent state of the art published baselines. 2.Missing baselines in incomplete verification experiments:* The paper is using weaker baselines. Because [A] outperforms Bunel et al.2020.Can your method be extended to the relaxation from Tjandraatmadja et al., 2020? It is not clear that this work can outperform [A] or work with tighter relaxations. * The authors should also compare with Fast and complete (Xu et al., 2021), Beta CROWN(Wang et al.2021) as they use the same LP relaxation as this paper, and are very effective. This has been done in works that have been used by the authors as a baseline (Bunel et al.2020).Even the recent propagation based solvers provide results on complete verification (see Xu et al., 2021 and Wang et al.2021).* They are quite important to judge the practical relevance of the algorithm. The authors should use a branch and bound framework from the open sourced codes from VNN comp and plug DeepSplit into them. * You should also add these b&b baselines in the speed comparison experiment. b&b solvers can be used as incomplete verifiers, and that should be possible here since the network size is small. This will help in checking if the method can in fact verify this size networks or if the bounds are too large to be relevant.<|endoftext|>The authors present DeepSplit, a novel solver for a popular neural network convex relaxation. Computational results are presented for incomplete neural network verification, showing that DeeSplit scales to a ResNet18 and achieves better bounds in the same time compared to relevant baselines. Suggestion: perhaps inverting the ordering of sections 3 and 2 could improve the paper s readability. Furthermore, in neural network verification, one is typically concerned with verifying the largest number of properties in the smallest possible time, rather than solving a given relaxation exactly. Compare against the more recent bounding algorithms from [Bunel et al.non convex 2020] and (Xu et al.2021).While introduced in the context of branch and bound, the "alpha CROWN" method, or LiRPA with optimized slopes, is a valid incomplete verifier, and the global optimum of its optimization problem coincides with DeepSplit s. Furthermore, it might yield tigther bounds than DeepSplit in case of joint optimization over intermediate bounds. Compare speed accuracy trade offs against works operating on tighter relaxations, such as [Active Set]. Complete verification performance is tightly linked to the quality of the speed accuracy trade offs of an incomplete verifier. It would be quite informative if the authors could provide complete verification experiments on the model from Table 1. ": this is incorrect, as a valid bound can be obtained in closed form by minimising the Lagrangian, rather than Augmented Lagrangian.<|endoftext|>This paper proposes an operator splitting method which solves a convex relaxation of a learge scale nonconvex optimization problem for analyzing the worst case performance of deep neural networks against input perturbations. Experimentally, the authors demonstrate that the proposed method has tighter bounds on the worst case performance of large CNNs in image classification and RL settings. While this paper is well written and has proposed quite an interesting idea to improve the scalability of the neural network verification problem, the idea of applying operator or variable splitting to neural networks is definitely not novel, although neural network training and verification might be differet tasks. This paper has proposed an interesting idea (i.e., operator splitting) to improve the scalability of neural network verification, but this idea is not novel for its applications to neural networks (e.g., neural network training). The paper will better position itself if more detailed comparisons are made with such methods in the existing literature.<|endoftext|>This paper proposes ADMM for neural network verification problems. This paper proposes an operator splitting method (i.e.ADMM) to deal with neural network verification problems. This is because the ADMM is scalable to large datasets. Specifically, they solve the convex relaxations of problems analytically. Extensive experiments on network verifications demonstrate outstanding performance as well as excellent scalability. Some statements in the paper are unclear. For example, in the introduction, the authors state that neural networks lack formal guarantees. As another example,  the background of the neural network verification problem is missing. It is unclear what are potential applications of this problem, and examples should be provided to better understand required properties. Moreover, the ADMM usually converges slowly to the solution, how do authors address this issue? So it is difficult to reproducible experimental results.<|endoftext|>This paper proposes an efficient solver for computing the convex relaxation of Neural Networks, which is an important component of verification methods. The appendix is very detailed and provide clear explanations for how to implement each component of the algorithm efficiently. The Introduction and Related work are a good summary of the current state of neural network verification, and Section 3 does a great job at highlighting the differences between the proposed methods and the most closely related published work, highlighting why this results in significant improvements. # WeaknessesA potential improvement that this paper could benefit from is a comparison to methods such as the OptimizedLirpa bounds from the Fast and Complete paper, which the authors cite. This would not need to include comparison to the branch and bound aspect of it, but simply to the bound computation, which rely on fast linear bounds, but iteratively optimizes the slope of the relaxation. This is equivalent to solving the same problem and has been empirically shown to be quite efficient. A comparison of the tightness / speed tradeoff that would result would be extremely useful. (The method described in this paper are valuable even if they are slower, as they provide convergence guarantees, which OptimizedLirpa bounds do not.) Very well written paper, with interesting and significant contributions, and which provide a great summary of the state of the field.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper proposes a few shot (untargeted) backdoor attack (FSBA) against siamese network based visual object tracking. Contributions can be summarized as follows: First, this paper treats the attack task as an instance of multi task learning and can be regarded as the first backdoor attack against VOT. Besides, a simple yet effective few shot untargeted backdoor attack is proposed and achieves significant effectiveness in both digital and physical world scenarios. Strengths:This paper reveals the vulnerability of VOT to backdoor attacks caused by outsourced training or using third party pre trained models.<|endoftext|>This paper investigates a few shot backdoor attack for single object visual tracking. The authors empirically show that the presented attack is effective in both digital and physical world scenarios. * The authors claim that this is the first backdoor attack against VOT models. * The proposed method is claimed to be able to operate in few shot or one shot mode.<|endoftext|>1.The paper introduces a variant of backdoor attacks against visual object tracking (VOT) networks. 3.An improved attack (FSBA) based on maximizing a particular loss in feature space is proposed, with better empirical results than the baseline attack. 3.Threat model requires a very strong attacker with ability to modify the training algorithm. 4.Only very simple defenses are considered. Overall this is an interesting new application of backdoor attacks with good empirical results.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; In _iii)_, SimpleBits reduces the image complexity in dataset condensation even further. The major contributions can be summarized as follows. Simplifying images to interpret the neural models is not new, while this paper performs the novel simplification on image complexity. What is the motivation for simulating forgetting? The gradients of the returned loss are taken w.r.t to which parameter, simplification network or $f$? In the rebuttal, the authors can explain the details of combining simplification and dataset condensation more. The paper proposes a novel direction of simplifying images (in terms of image complexity) to interpret neural networks. The paper applies the proposed, SimpleBits, in three different settings *i)* simplification during training *ii)* simplification after training *iii)* simplification with dataset condensation.<|endoftext|>This paper proposes a method to simplify images by reducing their information content, and measure the resulting effects on several aspects of learning: removing redundant image features for network training, understanding classification errors via visualization, and data condensation. Similarly, the $simplifier$ function (Eq.2) is not described in the main paper. I think that the visualization tool proposed for diagnosing errors could be useful, so I would encourage the authors to improve the manuscript. AFTER REBUTTALI have updated my score for this paper, following the authors  clarifications. However, I still think that the paper is incomplete with respect to baselines using alternative image compression methods or other techniques.<|endoftext|>Hence, I score 6, but I will not fight for this manuscript in a discussion. Authors are suggested to clarify this. The work can be considered as a good early work in this direction. However, after reading other reviews and authors  rebuttal I feel that the manuscript needs some more improvements. For instance, primary aim of the manuscript is to investigate the effect of reducing information on classification, and I feel the findings are not very nontrivial or significant.<|endoftext|>In this paper, the authors proposed SimpleBits, which is a method to create simplified input images that retain the most relevant parts for classification while removing irrelevant details. The authors applied SimpleBits in three settings: (1) during training, (2) after training, and (3) dataset condensation/summarization. I have some questions/concerns regarding the use of SimpleBits for posthoc explainability analysis of an already trained model (Section 4). Section 4 needs a lot more explanations than what is currently written   it needs to be rewritten for better clarity. Also, how does the proposed method compares with existing posthoc techniques (qualitatively or quantitatively)? Right now, there is only an algorithm for how to compute the loss of the optimization problem, but I cannot find the optimization algorithm itself.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes the use of user comments in addition to videos and titles to learn better representations for retrieval. Since user comments may be loosely related to the video, they use an attention based mechanism to ignore irrelevant user comments. The paper is well written, and the problem is well motivated as well as clearly formulated. However, my main concern about the applicability of this module for realistic scenarios where the number of comments is very large still remains. It is unclear if it would work in a setting with large number of comments, where the number of distractor comments is larger than the non distractor ones.<|endoftext|>1.The main idea of this paper is taking users  comments as a noisy signal to train multimodal (visual text) video representation learning. This is an interesting idea and seems to work well in practice, according to the experiments. 2.This paper focuses mainly on the effect of "comments", but I think the proposed idea may be applicable more broadly. 3.In this sense, this paper uses an idea which can be more generalized only within the narrow scope of "comments", and the experiments are verifying the efficacy of comments, not that of the model. For instance, the conclusion we can learn from Table 1 is that training and testing with comments help visual text retrieval. 4.The authors constructed two new datasets instead of using existing publicly available ones. Given the scale of the two datasets, they should be valuable sources to advance this field.<|endoftext|>There needs to be more discussion on the generalisability of the result. The main contribution is a context adapter module based on transformer that allows relating visual input and textual input. Therefore, the paper looks into attention mechanism to filter out the irrelevant content. The paper proposes an intuitive idea of leveraging on user comments to improve video text retrieval, but the technical novelty is rather incremental.<|endoftext|>The authors propose a text based video retrieval method based on context from weakly related user comments. However, CLIP4clip needs a much larger scale training, so the superiority of the proposed method is claimed. [Writing]  First of all, what is "video text retrieval"? Please reconsider the vocabulary, and also neatly define the task at the beginning of the paper. [Evaluation]  I am not convinced that the proposed method is superior to CLIP4Clip. Please explain in more detail the merit of the proposed method compared to CLIP4Clip. This could actually be a demerit in some situations. The proposed method works well in case user comments are available, but even so, compared to a conventional method; CLIP4Clip, it performs similarly, so the technical merit is limited.
Reject; rating score: 3; rating score: 3; rating score: 5; In this work, the authors provide an empirical evaluation of various sampling mechanisms which can be used to create few shot tasks during an episodic training regime that s common with meta learning training. * Lack of Explanation and Unfair Comparisons   The paper does not do a good job in explaining why some of the sampling methods work better than others and in some cases, the explanations are trivial. Through experiments on multiple popular few shot learning datasets, the authors show which sampling methods do work well and which of them do not. Strengths * Narrative is clear and the paper is enjoyable to read.<|endoftext|>In this paper, the authors investigate the effect of task diversity in the training process of meta learning. The findings indicate that increasing task diversity during the meta training process does not boost performance. More analysis and experiments are needed for such an analysis paper.<|endoftext|>The paper studies how the diversity of tasks in the training phase affects the performance of meta learning algorithms. Significance: The paper follows an existing line of work that empirically shows task diversity (in the training phase) does not help with meta learning. This is an important step towards a better understanding of meta learning. For this reason, I find the novelty of the paper limited. 1.The explanation of the empirical results is not insightful and convincing enough. For example:(a) what noise the authors are referring to? The paper provides an extensive set of empirical evidence to demonstrate that task diversity (during training) is not beneficial for meta learning.
Reject; rating score: 3; rating score: 5; rating score: 8; This paper proposes a new RL algorithm whereby the policy selects a new state rather than action, with constraints to ensure the next state selected is a valid one. "overall sample efficiency of RL procedure" → "overall sample efficiency of *the* RL procedure". More detailed comments:* The opening paragraph makes claims about poor sample efficiency and interpretability of RL algorithms, but does not cite any papers for this. Then the paper itself does not really address this. This is an empirical RL paper introducing a new algorithm, but the results are not convincing either based on their strength or intuition. The comment "due to improved exploration in some sense" shows that even the authors do not know why their method did well on the environments where it did.<|endoftext|>This work presents a a new reinforcement learning algorithm in which planning is performed purely on the state space instead of the more common state action space. While the problem is of interest, I have several concerns about this work. a) The dual variable \lambda needs to be projected to the non negative orthant in Algorithm 1, 2 and 3.b) The author s algorithm takes the form of a primal dual algorithm for CMDPs. For example, section B.2 presents a policy gradient theorem, but this is not the gradient that the authors take in Algorithm 1 (the gradient is taken with respect to the Lagrangian). How sensitive are the results to its value? How to handle infeasibility in the problem?<|endoftext|>The paper describes an approach where a state state mapping islearned (called state planning policy or SPP) coupled with anoff policy RL system (the authors included experiments with DDPG, TD3, and SAC). Experiments were performed in different domains and with differentstate of the art DRL systems. Why is that? If would be interesting to discuss how to include other constraints,e.g., a cost function for violating safety, within the proposedapproach. Why there is a better exploration?
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 6; The paper proposes a new method to combine global and local image features, targeted at image retrieval applications. It designs a new local feature model branch where both spatial and channel attention are used. The local feature branch undergoes supervision directly (in the paper called “intermediate supervision”), and this branch’s output is also concatenated to the global feature branch’s output in order to produce a final image embedding at the end. Weaknesses:W1) The authors naively claim several times in the paper that their method is learning homography transformations (Introduction, Methodology, Experiments, Conclusions). W2) The paper is not well written, and several parts are hard to understand. The comparison does not seem appropriate.<|endoftext|>In this work, the authors propose a Unifying Global and Attention based Local Features Retrieval method (referred to as UGALR). UGALR accelerates extraction speed and reduces memory consumption by removing the re ranking process and learning local feature matching with convolutional neural networks instead of the RANSAC algorithm. In addition, UGALR learns more accurate and semantic local information by combining spatial and channel attention with intermediate supervision. The proposed method is feasible. Also, many points are confusing for readers. Due to the limited contributions, I do not recommend this paper to publish on ICRL.<|endoftext|>This paper addresses the problem of image retrieval and proposes a method that tries to learn better representations through the use of local feature attention and reduces memory and latency overheads by not requiring re ranking with local features and only using global one. For this, the paper proposes a module called Local Attention Learning Module (LALM) that performs both spatial  and channel wise attention on local features and that it s plugged between two consecutive blocks of the backbone architecture. The output of this module is then concatenated to the input of the next block. The rationale behind this is that the following blocks will act as an homography transformation of the local features and will produce a better global representation. Strengths:  The paper proposes an interesting approach that obtains very good results using only global features, outperforming two stage methods that use local features for re ranking. Experimental evaluation is also complete and thorough, with a very extensive ablative study. Some paragraphs are difficult to follow and in my opinion should be rewritten. Nevertheless, I highly encourage the authors to do another pass on the writing. Overall, the contributions of the paper are not clear enough.<|endoftext|>This paper proposes a new pipeline architecture for image retrieval. In order to do that, UGALR is proposed as a single stage pipeline able to combine global and local features to speed up the retrieval process, removing the re ranking step. This is achieved using a CNN able to learn the homography transformation in local feature matching. Moreover, the architecture combines spatial and channel attention with the aid of intermediate supervision, obtaining better performances than the SOTA models presented in the analysis of the results. If not, this is a real problem and the results cannot be taken into account;* there is no error analysis. The comparison with a lot SOTA models on two datasets confirms the robustness of the proposed model. Even if the improvements in terms of accuracy are not so high (not a negative point, there is however a clear improvement) there is a huge improvement in terms of speed and memory used. Moreover, is not clearly specified if all the models have been tested on the same GPU (by the authors) to test speed and memory usage, this is an important point to specify because is one of the main goals of the paper.
Reject; rating score: 6; rating score: 6; rating score: 8; This paper points out that model update frequency and update quantization level are not disjoint decisions in federated learning systems. Strengths:1) The paper considers a relevant problem. 2) Have the performance improvements shrunk on the CIFAR10 dataset which is a more complex dataset than MNIST? If so, why does this happen and what is AQUILA s benefit for more complex data distributions in the real world? I m marginally recommending accepting the paper mainly based on the assumptions that the problem statement is novel and the theoretical methodology is sound to my current understanding of the paper.<|endoftext|>The authors theoretically show that such a bit allocation strategy leads to reduced communication compared to the bit allocation strategy of AdaQuantFl. **B Weaknesses:**1)  It was disappointing not to see an explicit convergence rate of the proposed method. 2) Building on the previous point, a lack of comparison with local methods in convergence rate and experiments was disappointing. I think these comparisons are needed for a fair assessment of the paper. Why was this particular metric chosen then?<|endoftext|>In this paper, authors propose the framework for federated learning that adaptively adjusts the frequency of the communication rounds and the quantization efficiency in a synergistic way. However, I still have a question to the Figures representing "transmitted bits" do they assume the communication TO server  only or together with the broadcasting. The combination of these two ideas allows to reduce the communication round amount in the late iterations of algorithm comparing to AdaQuantFL. Also I have a minor comment to the notation $\theta^{\star}$ that seems to denote $f(\theta^\star}$.According to the experimental section, I failed to follow the plots: they are quite small on A4 paper; they are not colorblind friendly and I am colorblind.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes a latent mapping mechanism based on StyleCLIP to disentangle the semantic attributes of human face. The authors provide some qualitative results and ablation study on CelebA HQ dataset. 2.Qualitative results are provided, demonstrating the proposed method performs fine on CelebA HQ dataset. The comparison to StyleCLIP is not sufficient, especially considering this paper is a follow up work of StyleCLIP and there are no quantitative experiments to support the authors  claims. The paper only conducts experiments on CelebA HQ dataset and the title is "text driven image manipulation" rather than "text driven celebrity face manipulation. 3.Typos like "definded" in the paper should be corrected.<|endoftext|>The authors proposed a directional latent mapping network for facial attribute editing via text inputs. This paved the way to a novel semantic directional decomposition network (SDD Net) for text driven facial attribute transfer: SDD Net transfers semantic aware attributes from reference images to a target, with the multi modal approach guiding the process via text input descriptions. CelebA HQ dataset was used to compare results with recent SOA methods. It was intuitive and a clever idea, end to end. Nonetheless, I believe the authors are not transparent enough in this aspect of the paper  do we need K models for K attributes? A better explanation of the variations is needed. More attribute types would be best.<|endoftext|>This paper proposes a new loss function for unsupervised facial attribute editing and transfer. Experiments show some comparisons and ablations for the "smile" attribute. The main contribution is essentially a loss term that measures latent similarity/distance in CLIP latent space. Moreover, it seems unreasonable that text feature y_t can be treated as a semantic direction. Besides, the role of the reference image in attribute transfer is confusing since the text prompt plays a significant role in the presented results. The authors have not justified whether their method works on a wide range of attributes with quantitative evaluations. The detailed comments are listed above.<|endoftext|>This submission proposes a novel test drivern directional latent mapping network with a new semantic direction consistency constraint for semantic level facial attribute transfer. By treating the facial attribute transfer problem as an attribute semantic feature projection problem, a semantic directional decomposition strategy is proposed. Strengths:  the proposed semantic directional decomposition strategy seems interesting and novel. the experimental results reported in this submission seem better than prior works such as StyleCLIP. Weakness:  more experiments with various attribute transfer are expected to further demonstrate the efficacy of this proposed method. On one hand, only a few attributes are considered in the current submission, more experiments with various attribute transfer are expected; on the other hand, it is expected to know whether this method can still work when more than one attributes need to be transferred.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper tackles the problem of open ended knowledge grounded natural language generation, in the context of free form QA or knowledge grounded dialogue, where models must ground their generations on passages relevant to the input context. Specifically, the authors explore improving the retrieval component of retrieval augmented systems by utilizing posterior signal from the label. The authors find that their model improves three fold over baselines: 1) it retrieves relevant passages more frequently; 2) its generations are more grounded in retrieved passages; and 3) its generations are closer to human generations. # Main Review### Strengths  The authors indeed find improvement over a strong baseline (using a marginalized loss instead) in all three points across two unrelated datasets. The results are plainly displayed in tables in figures and are clearly described in the text. Are these fundamentally different metrics? 3.Why were different pre trained ColBERT models used for each task? al.2021, Knowledge Grounded Dialogue Generation with Term level De noising[3] Lewis et. The paper indeed supports its claims, as mentioned in the summary, though some work is missing that could bolster the proposed efficacy of the method.<|endoftext|>The paper tackles a problem of knowledge grounded open ended generation and proposes a new model that is an extension of an end to end training of the retrieval and the generator. It is used as a weight of each evidence in the generation in order to encourage the generator to ground more to the evidence that is more relevant to the response. Experiments are done on Wizard of Wikipedia and MS Marco NLGen. The model achieves significant improvements over the baseline retriever, based on three evaluation metrics: relevance, groundedness and generation quality. Weaknesses:* One major concern is that the pipeline training baseline    the approach that trains the retriever and the generator separately   is missing, although it is (arguably) the most widely used approach and is a potentially stronger baseline. Pipeline training (Karpukhin et al, Khattab et al and more) has been much more widely used in recent retrieval work compared to joint training due to its simplicity, stability, and better empirical results (e.g., as shown in [1] which compares joint training and pipeline training under the same condition). Although I agree with the motivation for joint training in the paper, there is no justification for not adding a pipeline training baseline, especially given that the datasets come with annotated gold passage so it is pretty straightforward to train two models separately. If not, there should be some evaluation that shows that F1 / Nov F1 correlates with human evaluation of the groundedness of the generated response. Does any of findings in the paper change if the model is not pretrained on another dataset? [1] https://arxiv.org/pdf/2101.00408.pdfOverall a strong paper, tackling important and underexplored problem of knowledge grounded open ended generation, proposing novel objective, significant empirical improvements on two datasets in multiple metrics. There are a few concerns like absence of a potentially stronger baseline, justification of evaluation metrics, choice of terms, choice of implementation details.<|endoftext|>This work focuses on the knowledge grounded text generation tasks. They argue that multiple passages can be valid and relevant to the context, but not all of them are observed/used in the target response. A retriever is jointly trained with the generator and the guide retriever, and the KL divergence between the retriever and the guide retriever is included in the loss function. The evaluation and experiments are also relatively comprehensive. The weakness is that this paper only compares their method with one baseline (MARGINALIZEDLOSS), but there re more baselines are designed for the similar tasks and should be analyzed: e.g., "REALM: Retrieval Augmented Language Model Pre Training" by Guu et al., and "Retrieval augmented generation for knowledge intensive nlp tasks" by Lewis et al.This work proposes a novel approach to the knowledge grounded text generation tasks via the design of a new "guide retriever". The methodology is reasonable and the experiments show significant improvement over one baseline.<|endoftext|>In this paper authors describe an approach to use the responses/ answers to guide retrieval during training of document grounded response generator. Documents are retrieved using the dialog context and the top k documents are used for training the posterior as well as the prior. The idea of doing variational training using a posterior network isn t particularly novel but the authors have made it work for open ended response generation using this approximation for computing the ELBOLoss. The networks use BART for language generation and CoLBERT for modeling the retrievers. Experiments have been presented using the Wizard of Wikipedia dataset and the MSMARCO NLGEN. Experiments show an improvement over the baseline model (RAG   referred to as MarginalizedLoss) in both retrieval (success@k, MRR) as well as response generation (text F1 overlap between response and grounded document and textF1 overlap between response and ground truth output response). Overall this is a well written easy to read paper Strengths 1. Improvement in performance over a baseline RAG model by use of the posterior for training the retriever   2. Experiments on aspects related to retrieval as well as response generation Weakness 1. The paper discusses using an alpha mixture but doesn t clearly explain the need for the same. I d imagine there s a risk of not seeing the ground truth document when sampling from P_eta (because of a poorly trained prior network) and that is probably why alpha is set low initially, but what happens if you only sample from Q? Do you do anything in your training to address this problem? 4.Why was pre training of the retrievers done differently? This is a simple and interesting idea which shows promise in performance. However, the experimental results are a little in weak   in particular there is no study about hallucination/memorization in these models and some of the modeling choices aren t well studied.
Reject; rating score: 3; rating score: 6; rating score: 8; rating score: 8; To tackle the unsupervised skill discovery problem, the authors attempt to maximize the mutual information between (latent) skills and states $I(\tau; z)$ by using the nonparametric particle based entropy estimation for $\mathcal{H}(\tau)$ and noise contrastive estimation for $\mathcal{H}(\tau|z)$. **Strengths*** The experiments are done on URLB, which provides a good evaluation scheme for unsupervised RL. * The motivation for using noise contrastive estimation is not entirely clear. * The authors employ $I(\tau; z)   \mathcal{H}(\tau)   \mathcal{H}(\tau|z)$ as the decomposition of the mutual information (the 2nd line in Sec.4.1), but they use $q(z|\tau)$ instead of $q(\tau|z)$ for the rest of Sec.4.1. * I think the derivation of the noise contrastive estimator in Sec.4.1 needs more details. For instance, if there are noise samples, where do the correct samples come from? * Lack of empirical analysis with different values of $\alpha$. While the empirical results basically show that CIC can outperform multiple baselines on URLB with an appropriately tuned value of $\alpha$, I am mainly concerned about the correctness of the claims and the novelty of the work.<|endoftext|>In line with previous works in unsupervised skills discovery, it proposes a method, called CIC, to maximize a variational lower bound to the mutual information between the code and the visited states. Instead, potential shortcomings are:  The novelty seems limited, as CIC is essentially similar to APS (Liu and Abbeel, 2021) with a different discriminator loss (which has been employed for unsupervised skills discovery before);  It is not completely clear from the paper what are the specific factors that lead to such a performance improvement over previous works. The paper provides an empirical analysis of CIC over a set of continuous control domains. MUTUAL INFORMATION OBJECTIVE1) There is now a bunch of works targeting the mutual information between a code and the visited states to the purpose of unsupervised RL (some of them are summarized in Table 1 of the paper). EXPERIMENTS5) How can we rule out the possibility that CIC is just a better way to pre train a DDPG agent in these settings w.r.t.other baselines? 6) Moreover, DDPG is known to be quite strong on continuous control tasks. However, in the reported results CIC is way better than APS. Do the authors think it is the different discriminator loss the main cause for this performance gap, or there is some other factor at play?<|endoftext|>In this paper skills are learned in an unsupervised way using a mutual information based objective. The mutual information between latent states T and skill vector Z, I(T;Z), is decomposed as I(T;Z) H(T) H(T|Z) which the paper argues leads to explicit maximization of diversity of latent states T as well as distinct skills with focused effects by penalizing with H(T|Z). I liked the empirical decomposition of reward into entropy and discriminator terms. Technically I(T;Z)     H(T)   H(T|Z)     H(Z)   H(Z|T) so theoretically it should not matter which way it is decomposed. The way the terms are calculated, however, could have a significant effect on the practical performance. The paper proposes a new algorithm for unsupervised behavior learning that is rigorously shown to be more effective and clearly argues for its design choices through supplementary experiments.<|endoftext|>The paper proposes an algorithm (Contrastive Intrinsic Control) for unsupervised skill discovery by maximizing mutual information between skill latents and state transitions. Strengths:The empirical evaluation carried out in the paper is extensive with many baseline algorithms for skill discovery from each class (knowledge based, data based, competence based) on the recently proposed URLB benchmark. These large scale empirical evaluations and analysis of several algorithms for unsupervised skill discovery methods on a standard benchmark such as URLB would benefit the community. The discussion and analysis on the reasons for failure of current competence based skill discovery algorithms. Could the authors comment on this? This seems rather counterintuitive to me. 3] The authors argue for the need for increasing the dimensionality of the skill latents to ensure skill can be decoded back to a diverse set of behaviors. This could allow for greater representation flexibility when the skill latents are decoded back to the action space and ensure that skill latents give rise to a diverse set of behaviors. Writing/Presentation:The paper on the whole is well written and easy to follow.
Reject; rating score: 3; rating score: 6; rating score: 6; The key advantage is less training time. 2.The proposed approach is simple, easy to implement, and already provides a gain over conventional methods. The technical contribution is unclear, since reservoir computing is not new. Would be interesting to see if we learn an embedding space (using simpler methods) with a given set of reference time segments and use that embedding to reconstruct other segments to generate error signals, whether we could achieve the same gain as reservoir computing. 2.It is unclear what would be a good choice of “reference” time segments/sequences. For ECoG, rest period would make sense, but is there a way to decide in the general setting? Is the contribution of this work actually the use of a reference to generate error signals for classification, which tends to increase accuracy? 4.For the sequential MNIST experiment, it is not clear which digit would be a good reference and how results change with this choice. 5.Though TRAKR is fitted in one shot, does performance improve with a few more rounds? My score is mainly based on TRAKR not evaluated on harder classification tasks, lack of comparison with state of the art DL models, and unclear technical novelty. However, the motivation as written remains to be faster runtime for real world applications, but accuracy is lower than MLP and runtime is not that much shorter.<|endoftext|>The paper proposes a new method to classify neural time series. The OFC experiment raises the same questions as the MNIST experiment in terms of choice of the baseline methods. The method then uses the reconstruction error of the reservoir as a representation of the signal, to be used in a RBF kernel SVM classifier and perform a given task. The method is then compared with four baseline methods. The method seems novel, and valuable because it requires very little training to create a representation of the input that can be used for further tasks. Weaknesses:The experiments proposed in the paper do not seem sufficient to assess the strengths of the proposed method. MNIST is composed on images, which are not naturally represented as sequential time series. Why use MNIST as a dataset for time series classification ? Why not include other methods based on reservoir computing ? 4.How were these baseline methods applied? 5.Is the classification task a one vs one, one vs all, or multinomial task?<|endoftext|>Here, the authors propose a reservoir computing based framework for time series applications. The proposed methodology involves training the output units of the echo state network to recapitulate, or autoencode, a test signal. Next, time series are fed to the network and the error signal between the time series and the network s response is used as input to an SVM that is used for classification. The authors report that the network performs well on sequential MNIST and decoding of neural data. Additionally, they argue that a key advantage of their approach is that it is computationally lightweight – both training and inference are fast. The manuscript is straightforward and well written2. Deep learning methods, especially for large network sizes, are particularly data hungry in the training phase. However, inference with deep networks can be extremely fast, to the point of being sufficient for real time applications.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper mainly discusses the training of neural networks without residual connections. The main contribution of this work is to adapt DKS to models with ReLU family activations. 2.Effectiveness of  the proposed TReLU has been demonstrated by a series of ablation studies. With K FAC optimizer, the TReLU ResNet can maintain performance without any residual connections and normalization in some cases. However, existing methods seem not to be so bad as claimed in the paper.<|endoftext|>This paper makes solid theoretical contributions on analyzing the initial conditioning of feedforward networks without shortcuts. I feel the only weaknesses of the paper are the clarity and the significance of the results. In addition, I have following questions or suggestions:1. Most tables show better results achieved with K FAC than SGD under the same number of epochs, and it has been shown in Figure 5 that K FAC can be made 2x as fast without losing accuracy, but the wall clock time comparison with SGD is not shown. It would be better to include some form of pseudo code to specify the weight initialization and setting the parameters of activations. However, the method also introduces an extra hyperparameter $\eta$, and results of shortcut free networks are still slightly worse than ResNets.<|endoftext|>The paper extends the Deep Kernel Shaping (DKS) method to Tailored Activation Transformation (TAT). As a result, deep vanilla neural networks with ReLU family activations match the performance of deep residual networks. Moreover, given the marginal improvement compared to DKS, a more extensive discussion on smooth and ReLU activations is desirable. Maybe the authors would like to include an algorithm box at the beginning to explain the proposed method (without understanding how to derive it).<|endoftext|>This paper studied the problem of DNN training and generalization in vanilla architecture (without BN and Skip Connections in ResNets). A study of an interesting problem with rigorous theoretical analysis. 2.Both theoretical extension and empirical improvement. So overall,  I feel some of the claims frequently mentioned throughout the paper are a little bit exaggerating. DHK, in their paper, claims quite differently, "small decrease in generalization performance" for removing BN, and "similar results" for removing skip connections. The other claim is ** Using TAT, we demonstrate for the first time that a 50 layer vanilla deep network can nearly match the validationaccuracy of its ResNet counterpart when trained on Imagenet.**. I would suggest the authors be careful about their claims.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper proposes a method to detect boundaries in an image by interpreting boundaries as 1 D surfaces and formulating a one to one vector transformation function. The paper provides theoretical justification of the vector transformation representation. The paper does not show the limitation of the proposed method. The paper proposes a new method with careful theoretical justification. The paper is well organized.<|endoftext|>The paper is worth known to the community. Only one backbone architecture is tested for all losses, and Vector Transform only outperforms others in some but not all metrics as shown in Tables 3 and 4. It is not convincing that the vector transform representation can be useful and outperform the binary representation in practiceThe paper takes a first principles approach to thin boundary estimation.<|endoftext|>The authors proposed a new loss function for end to end edge detection to overcome the label imbalance and edge thickness problems.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper focuses on graphs representing sets of facts in a knowledge base, and casts knowledge graph completion as a graph labeling problem. The authors propose a particular encoding of facts into graphs, and introduce a particular family of "monotonic graph neural networks" (MGNNs) that share properties with logical inference rules in Datalog: namely, they prove that MGNNs are "monotonic under homomorphisms", which effectively means that renaming objects or introducing new facts never causes the classification to switch from True to False. However, the authors have added new results showing that the subset of extracted rules with at most two body atoms still have fairly similar performance to the MGNN on their datasets. Specifically, after the sentence "we can explicate the rules that are implicitly captured by the model and use them to fully explain each credit recommendation", I d suggest adding another sentence stating that the existing algorithm for extracting rules may only explain a subset of predictions of the original MGNN (and, optionally, that the extracted rules could just be used directly if it is critical to be able to explain every prediction). From the results, it seems like their method is not strictly better or worse than either of the baselines they present, and the different methods have a wide range of performance across the different datasets. It seems like fewer rules should be better for a good explanation, right? ### Updated review (Nov 18)After discussion with the authors, I have raised my score from 5 to 6.<|endoftext|>The authors provide a detailed theoretical analysis of the model’s ability to explain the GNN based predictions by logical rules. However, the comparison with GNN based methods is missing. The authors may want to compare their approach with the state of the art GNN based ones, such as GraIL[1] . In terms of the proposed model, MGNN assigns a feature vector to each link, which significantly increases the computational cost. This makes the model difficult to apply to large scale knowledge graphs. In terms of the writing, my concerns are as follows. The authors may want to specify some important details. In Section 2, the authors say that the result of MGNN can be decoded to an output knowledge graph by "inverting the encoder". This paper proposes a GNN based method, named MGNN, which allows the predictions to be explained symbolically as logical reference to improve their interpretability. However, MGNN does not outperform existing approaches on some benchmarks, and it may be difficult to apply to large scale knowledge graphs.<|endoftext|>The paper proposes the Monotonic Graph Neural Network (MGNN) model that can be exploited to learn tasks on knowledge graphs. A method is defined to encode a given dataset (i.e.a KG) as a colored graph, whose nodes correspond to the constants and pairs of constants. Edges of one out of four different "colors" (types) are added to encode the relationships between constants and the occurrences of their pairs in binary predicates. A vector is stored in each node to represent the truth table of each predicate when computed on the corresponding grounding. The MGNN processes such a graph to computed a new KG in which new facts are inferred. A theoretical result shows that an equivalent set of Datalog rules can be derived from the trained MGNN, thus providing a direct explanation of the learned model. The paper provides an interesting model to learn tasks on knowledge graphs. The most valuable property is the possibility to derive the datalog rules that the model implements, as guaranteed by the provided theoretical results. Theorem 10 is claimed to provide results on the expressiveness of the model, but beside the proof in the appendix and a few  lines after definition 9, this aspect is not detailed to make the reader understand the actual limitations in practice.<|endoftext|>This paper proposes a new family of GNN based transformations for knowledge graphs, which is called monotonic GNNs (MGNNs). Moreover, the authors provide dedicated proofs for their formal statements. Pros:* It first transforms the original knowledge graph into a colored graph where each vertex is labeled by a feature vector and represents the facts of the corresponding entity or entity pairs. The idea of the paper is interesting but the technical contributions are not very significant and the advantage of the proposed approach is not obvious. I discovered that Equation 1 is sufficient to describe the model. Some baselines, such as DRUM and AnyBURL, can also capture rules.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper proposes a method to do MLP / LSTM based inferences/predictions based on incomplete data. While typical methods would impute and then predict, the proposed method predicts directly based on incomplete data. The method learns an “importance” matrix which is multiplied with the first weight matrix of the neural network. The learning is done using reinforcement learning, with the negative prediction error being the reward.<|endoftext|>The paper proposes gradient importance learning for predicting labels on incomplete data. The matrix is trained using reinforcement learning. I have several concerns as follows:1. Can a shared matrix across all data samples solve the problems caused by the missing values with diverse patterns? 1.3.Why it must be on the gradient level, not the parameter level? But why this gradient level modification with RL is preferable for this task? 2.I note the authors discussed the limitation of the proposed models, mainly about CNN. 6.A very basic "imputation free" baseline is to directly train a model to predict the label given an incomplete input.<|endoftext|>This paper targeted at the missing data issue in time series data and proposed a imputation free method to handle missing data. The paper is well structured and easy to follow. My main concerns about this work are as follows:1. For example, the results on the MNIST dataset seems to confirm that the proposed method cannot significantly outperform the baselines even with simple zero imputation. 2.There might be an efficiency issue when using RL to weigh the importance of gradients.<|endoftext|>The paper proposes a method to train on data with missing features. It does so with a single model that can handle missing features, not with extra imputation methods. This is done by weighting the gradient update of the first weight matrix of the neural network with a vector a that is produced by an RL agent.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; They provide the state of the art convergence results for these methods using standard assumptions. Furthermore, "the main advantage of (their) algorithms is that they do not need to compute any full gradients". Again, I am surprised that the authors did not discuss the storage cost needed for ZeroSARAH. For that reason, it is reasonable that your algorithm does not need many full gradient computations while still achieves the same state of the art rate. Similarly in the distributed setting, although we don t need a full gradient, we still need to compute a sum of $n$ variables (Line 11 of Algorithm 3). This should be added and discussed in this paper. The authors might want to edit some statements that could be confusing/misleading. Also the statement "(our methods) improve the previous best known result" is also misleading. Therefore I do not see any advantage and do not support publication at the moment when these issues are unresolved.<|endoftext|>I am of course open to increase my score if my concerns in 1,2,3 are addressed. 2.Due to my previous point, it is not clear if it is possible to give a fair comparison between the current complexity and previous best complexity. The new complexities depend on $G_0, \hat \Delta_0$ and their relation to the other terms is quite unclear. Since one of the main motivations of the paper is distributed optimization, I think the authors need to compare with the algorithms from Table 2, for example SCAFFOLD, or other SOTA algorithms in this setting to see if the new method is really useful in practice.<|endoftext|>The main benefit of ZeroSARAH and D ZeroSARAH is that they do not require any full gradient computations, in contrast to other known variance reduction algorithms. The communication complexity (i.e.communication rounds) is one of the most common metrics considered by the distributed optimization literature, which also corresponds to the fact that setting up a synchronization round is expensive in practice. Perhaps the authors can add some discussion on communication complexity. Typos/Mismatch: In Corollary 3, the parameter is set such that $b_k  \sqrt{m}$, but in its proof it seems that $b_k \sqrt{m/n}$. Overall, the paper proposes ZeroSARAH and its distributed version D ZeroSARAH, as variants of SARAH that avoid full gradient computation. Under the distributed setting (where such feature can be beneficial), requiring less number of gradient computations per iteration means that D ZeroSARAH requires more iterations, i.e.communication rounds, than distributed SARAH to converge, since the  gradient complexity of these 2 algorithms are asymptotically the same. This is a new perspective for this class of problems and can facilitate future work in this direction.<|endoftext|>They offer convergence results and shows that it is on the same order of gradient calls to $$\epsilon$$ gradient norm as other state of the art variance reduction methods, but claim it is without ever requiring full gradients. My main counterthought is that the paper didn t really address SAG and SAGA, which are other mainstream VR methods that also do not require full gradient calls, do not even require full minibatch computations, and also have competitive convergence rates. The downside of SAG and SAGA seem to be the same as this method, which is the high memory cost; the need to store the yi s seems to be similar to the issues of SAG and SAGA. As it stands, having such a high memory cost in general makes these methods prohibitive to most deep learning applications, and is on the same order of "issues" as SVRG. However, this is necessary for the proofs of ALL VR methods, not just the minibatched versions. It seems like the authors bypassed this issue via telescoping, so perhaps it just isn t an issue with SARAH.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The presented architecture is motivated by, and applied to the problem of missing/noisy data. Even though empirical study is suggesting potential benefits of the approach in applications with a pronounced data quality issues, the article haven t assured me that proposed modulation layer is more applicable/useful (or sufficiently distinct) from the attention layer.<|endoftext|>This submission contributes an approach to handle missing values in Neural networks by modulating inside the architecture the missing values by factors which decrease  the role of the feature in the architecture. The contribution is based on intuitions that do not seem very solid and should be better studied. The empirical results are not conclusive.<|endoftext|>This paper proposes a fully connected neural layer to add to a DNN to deal with missing or noisy data. Weights in the additional neural layer are modulated by quality indicators of the inputs. The paper is well written and easy to read. 2.Many small typographical errors, mainly spelling mistakes. However, the approach does not do much better that existing state of the art.<|endoftext|>Especially it is very important to show that MFCL is not a simple zero imputer. Despite good motivation, the proposed method seems to have several theoretical and practical limitations and the current experimental results do not provide enough evidence on what are the benefits of the proposed method. While in the caption, the error bars are missing in Figure 2,3,4. The definition of $W_0$ is not clear.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; In this paper, the authors analyses the convergence rate of episodic memory based continual learning methods. Based on the analysis, the authors propose an adaptive learning rate scheduling methods to adjust the learning rates based on the gradients computed in each iteration. 2.The main issues of paper are the experimental results. This implies that ∆f is not a reason for moving away from stationarypoints of f by catastrophic forgettingTypos:i. gradient descent based algorithm reach  > gradient descent based algorithm reachesii. However, the writing of the paper needs improvement and  the experimental results are not convincing enough to support the theory.<|endoftext|>The authors propose a stochastic gradient descent algorithm with adaptive learning rates for continual learning providing an  analysis of convergence. The main novelty of the paper lies in the application of  the SGD ideas in the context of the continual learning. In principle, this seems to be an interesting direction. Comment after authors  responses:I appreciate the efforts of the reviewers to address my comments and the changes they have made. Moreover, the writing needs improvement and I encourage the authors to further work on it and resubmit an updated version of it to a future conference.<|endoftext|>They show that for these two memory based methods, with a good initialization of replay memory, the gradient estimation error vanishes. To address this error, they further propose some adaptive learning rate schemes, which show some effectiveness in several experiments. This paper provides a good step to understand the optimization properties of this direction. Overall, I feel this paper provides some initial steps to understand the convergence of continual learning, but given some weakness I list above, I am slightly negative about it. However, I am open to increase my score based on the authors  feedback and other reviewers  comments.<|endoftext|>The paper proposes mini batch stochastic gradient method of the framework of nonconvex memory based continual learning, and conducts  a theoretical convergence analysis. Additionally, the paper studies catastrophic forgetting and outfitting in the context of the proposed approach. The paper is interesting, provides new insights and theoretical guarantees, and in my opinion should be accepted for publication.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper analyzes deterministic uncertainty estimation methods, in terms of their calibration under distributional shift. They provide a literature review, propose a quantification metric, and compare performance of several uncertainty estimation methods. Title of the paper seems too broad and not representative of the content. “Practicality” is a very general term. It mainly seems to be that ”DUMs are not well calibrated under continuous distributional shifts”. However, the current version of this paper falls short in helping in both aspects. Given that the work is mostly a review of other methods, it should at least serve as a good survey, for a non expert reader. The terms out of distribution detection, epistemic uncertainty estimation, aleatoric uncertainty estimation, and calibration. are not very well defined in the community when it comes to their quantification. When it comes to adding synthetic corruption to the image (input), how is that not aleatoric uncertainty? If the authors disagree with these conventions they should at least provide a comprehensive justification on why they think things should be different, and they should propose clear evaluation protocols for each type of uncertainty estimation. Maybe if the paper’s focus, description, and claims were concentrated around “evaluating calibration of deterministic epistemic uncertainty models under distribution shift”, it would be more representative of the experiments, and it could become a more conclusive work.<|endoftext|>This paper mainly summarizes and evaluates the existing methods for estimating epistemic uncertainty through a single pass of the neural networks. Specifically, they demonstrate that DUMs cannot generate well calibrated uncertainty under distribution shifts compared to the MC dropout and Ensemble methods. Overall, this paper provides the evaluation for some determinist uncertainty estimation methods focusing on the robustness of uncertainty under distributional shifts. However, the paper should include more related methods in comparison with a discussion about the pros/cons and possible assumptions for each method. Weaknesses:For the method:(1)	Some important deterministic uncertainty methods such as the Dirichlet based methods [1,2,3] are missing in the paper. Moreover, the authors cite the Posterior network [2] work as one of the DUMs but did not evaluate it. The authors should provide a discussion and comparison of their work with [4]. Some necessary equations should also be provided. Then, we can have a clear mind about which method to choose for experiments(4)	The authors mainly evaluate the uncertainty robustness under distributional shifts. For example, why do most DUMs have much worse calibration performance than softmax? It is hard to distinguish different colors from figures.<|endoftext|>The reasons invoked to include some of the cited DUM approaches are not very convincing, since the point of a benchmark is to evaluate all the relevant approaches according to the same yardstick. Nonetheless, the provided comparisons can be useful for future research aiming at proposing novel approaches and possibly avoiding all the possible comparisons, focusing instead on the methods which seem to perform well in this paper. These comparisons also point to DUM approaches being generally insufficiently well calibrated (especially those relying on the distribution of hidden representations). There is another deterministic estimator of epistemic uncertainty that could have been considered, of a very different nature (but only applicable with one can have an estimator of aleatoric uncertainty, e.g.for pairs of examples with the same x but different y): DEUP (Lahlou et al, 2021). They train a 2nd network to predict the per example out of sample error of the main predictor. This could be useful to practitioners of the field.<|endoftext|>This paper is an analysis and benchmark comparison of deterministic uncertainty quantification models under dataset shift. There is the important question if these methods work well in more complex settings. The survey and description in the first pages of the paper is also very useful for the community. The authors argue why standard calibration errors cannot be used, but I think it could be an improvement to normalize confidence scores produced by DUMs to be able to use calibration errors, I do not see this as a big issue more than using a standard metric. The supplementary material contains many additional results of interest, like variations of DUM hyper parameter effects, justification about issues with intermediate activations, full results for corrupted versions of all datasets, and OOD detection without dataset shift (standard OOD detection benchmarks), showing that some DUMs work better in some datasets, and ensembles on SVHN. I find that the analysis in this paper is original and novel, there is small epsilon contribution to the state of the art, which should enable future research on DUMs. Minor Issues  It is possible that the paper would benefit from a "page 2 figure" to introduce the reader. This will probably solve the issue. There are no major issues to be dealt with.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper studies the applications of graph filters in graph neural networks, and attempts to understand the links between graph spectral analysis, random walks, and homophily. The authors introduce "interaction probability" as a metric for understanding homophily and its relationship to random walk transition matrices. In this framework, the authors claim to understand a notion of graph information, as well as an understanding of how the performance of graph filters depends on both graph structure and input signals, with applications to graph neural networks. The conclusion that graph neural networks should use a filter bank rather than a single filter is nothing new. al.(2020).Overall, the contributions of this paper are extremely lacking.<|endoftext|>The paper presents an insightful analysis of the induced graph filters in GNNs. To accommodate the heterogeneity of graphs, the authors provide a family of novel GNNs for learning data specific filter banks. of interaction probability. And, what is the connection between Proposition 3.1 to the proposed algorithms? [Confusiong Notions] In Def. (3) the theoretical analysis and algorithm feel disconnected. The studied problem of this paper is interesting, while there may exist some issues in the theoretical foundation of the proposed algorithm.<|endoftext|>It is observed that for Graph Neural Networks (GNNs), the correlation between the labels and the graph structure matters. The paper gives a theoretical analysis of this behavior via interaction probability and frequency distribution. Theoretical analysis is clean, simple and gives interesting insights. I admit it was a bit of an effort to keep up with all the notations and the paper will definitely strengthen by bringing in lot more clarity in notations.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The proposed method include a design of a hierarchical projection network that produces multi level latent representations. The cross level contrastive learning shows the improvements in comparison to previous methods. Although the experimental results show some improvements, the contributions of the paper is incremental from previous works and the intuition for the improvement is unclear.<|endoftext|>The method is straight forward and it is clearly present in the paper. As there is potential to have different combinations of cross correlation dependencies between the layers. Although the performance improvement over the existing methods is marginal in most of the cases, the pair have done evaluations on multiple benchmarks and also present the ablation studies.<|endoftext|>The advancement over the full training settings is still small due to the natural limitation of the consistency assumption mentioned above. The improvement for the full training sets and full training epochs are, however, limited. The approach proposed in this paper is straightforward and well implemented.<|endoftext|>The authors proposed an interesting hierarchical comtrastive learning approach that achieves noticeable improvement over serval benchmarks. 2.The proposed approach is fairly easy to implement. 3.The proposed approach is evaluated on multiple benchmarks and achieves noticeable improvements compared with previous states of the art. But the training epochs for Places 205 is 14 for Barlow Twins, but 30 for HCCL. Can it outperform BYOL?
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; rating score: 3; An NLG metric for problems where there is many plausible options for the generated text (e.g.story telling, poetry generation, dialog continuation) is proposed. The paper makes many claims about how widely applicable the proposed metric is, and lacks evidence for nearly all of them. Assuming this, both parts (objective, universal) are not accurate. The "only requirement" of the discriminator is accuracy, yet the paper then uses a bi lstm, yet all recent work in NLP would suggest you d build a more accurate classifier with a transformer model.<|endoftext|>This paper proposes a reference less evaluation metric for natural language generation tasks. It presents experiments on Chinese poetry generation. As it is, all we know is that it can tell between two systems one of which is made to be worse than the other. As it is considered by the authors a contribution of this paper to be able to guide the generation, a comparison is needed against these methods is needed. The evaluation is flawed as the proposed natural language evaluation metric is not compared to human judgements.<|endoftext|>The paper proposes a new evaluation metric, OUMG, to measure human likeness of machine generated text without reference. The two main parts that are unclear to me are the need for *relative* metrics and what the metrics are actually capturing (supposedly similarity between human  and machine generated texts). The reduced accuracy of the discriminator after training on such outputs is interpreted as the evidence that the guided generation produces more human like texts. The idea of the proposed metric is somewhat intuitive and straightforward. ), which I suppose will be fixed in the next version of the paper. how many did you use? It is unclear exactly what the metrics are capturing.<|endoftext|>This paper presents a new evaluation metric for natural language generation models based on the discriminator that distinguishes between human written texts and models generated texts, OUMG. The authors also suggest guiding text generation by OUMG. What concerns me most is the lack of experiments to support the claims in this paper. First, U in OUMG stands for universal, but the authors only show the performance on poetry generation task. This paper does not mention these related work.<|endoftext|>The paper describes OUMG, a model based metric for evaluating text generator systems which does not rely on reference standards or gold standard data. Further clarification is needed. The authors indicate that for systems like story generation and poetry generation, the final suitable reference answer is “not unique”. The addition of this discussion should be in order since this is the main (and only) task tackled in the paper. There is no discussion of how the proposed metric discriminates between human and model generated texts. What textual factors do you look for? Likewise, there seems to be no discussion on the two models based on their similarities/differences by nature. Lacks specific details of retraining.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; In this paper, the authors proposed a data augmentation approach to improve the conditional set generation task. The authors conducted experiments on simulated data and three NLP data sets to demonstrate the effectiveness of the method. Stronger baselines are missing in Table 2. When using partial order to construct the graph, it is possible that there will be circles. Typos:      o  The recent successes of pretraining finetuning paradigm has  > The recent successes of pretraining finetuning paradigm have      o  From the results 2  > From the results in Table 2This authors propose to use PMI for label sorting in the set generation.<|endoftext|>The authors propose to perform set (order invariant) generation with seq2seq models via two concepts: 1) to impose an informative order over labels using a fixed graph ordering, and 2) explicitly predicting the cardinality (size) of the predicted set. This post processing step seems necessary regardless, as the proposed method here does not explicitly constrain the generated sequence to contain unique elements. I would like to see a comparison with previous work that constrains seq2seq models directly to generate sets without data augmentation, such as [1] which incorporates a cardinality penalty and changes the decoding strategy for a seq2seq model to generate sets. I would like to see comparisons against more fair baselines as well as non seq2seq approaches to better contextualize the applicability of this work, especially with regards to methods for seq2seq set generation that do not rely on data augmentation.<|endoftext|>This paper explores the task of conditional set generation using sequence generation models. The authors propose to model order invariance and set cardinality into the seq2seq models. Additionally, the authors introduce a novel data augmentation approach based on topological sorting. Relevant baselines have not been used for this task, such as non seq2seq or classification based models. It is expected that the model might be performing well for shorter set lengths, which could help understand the impact of the proposed approach.<|endoftext|>The authors propose to train a standard seq2seq model by ordering the discrete elements in the target sets (as a sequence) under a partial order defined by taking $y_i < y_j$ if both $y_i$ and $y_j$ have sufficiently high PMI and $p(y_i | y_j)$ is sufficiently larger than $p(y_j | y_i)$. The authors show that this approach improves over baseline seq2seq approaches for set generation on synthetic and NLP tasks, and also that prepending the cardinality of the set to be predicted to the target sequence is generally helpful. Strengths:  The paper proposes a simple, interesting idea and deals with a timely topic. It s possible that seq2seq would outperform such an approach, but the authors do not provide evidence of this. The paper proposes a simple, interesting idea, but needs additional baselines to show the proposed approach is useful.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper proposes a method to fuse tweets and stock prices for stock trend prediction flexibly. The authors claim that the proposed method outperforms other existing fusing methods. Furthermore, according to the results of the market trading simulation, this method achieves higher profits than other methods. CommentsThis paper proposes an approach to fuse text information and stock prices. However,  I have some concerns regarding the current form of the paper. 2.The paper quality is poor; there are many fundamental grammatical errors and mismatches in the paper, some of which are listed below. is not grammatically correct. (see the above comments)<|endoftext|>This paper proposed a novel approach to jointly model text and stock price information and fuse them for stock market forecasting. It encodes text and stock price information in parallel and then fuses them using a co attention transformer. 3.The intermediate results of the proposed model are not very clear, e.g.what are the input and output dimensions of each component?<|endoftext|>This paper proposes a co attention based model that fusion the representation of tweet and stock price data to make stock trend predictions. It emphasizes too much on some technical details that are not strongly related to the proposed model and contributions (e.g., the difference between GELU, ReLU, and ELU), but ignores many details of their own implementation. On the one hand, it is hard to identify the novelty of the proposed model. This paper solves the interesting text based stock prediction problem with a well motivated method. However, it is hard to identify the novelty and understand the method in detail based on the current manuscripts.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; Strengths:   The proposed method is simple and easy to understand. The method outperforms baseline methods. How to partition a long sequence into spans? For time series? Will the final results be sensitive to the partition of spans? If each word in NLP sequences is a span, the proposed method is the same as the word level dropout baseline studied in previous works, e.g., [1]. What s the tech novelty of this work? While the authors focus on the "learning problems for long sequences where not all input elements contribute equally to the desired output", the uniqueness of this kind of problems is not clear to me. In most (if not all) real world sequence classification and prediction problems, not all input elements contribute equally to the desired output; otherwise, the problems will become much easier. Experiments need to be improved and enhanced, e.g.,       As reviewed in the second paragraph, there are many Transformer variants proposed for long sequences, but none of them is compared. The four NLP datasets used in experiments are not representative tasks for long sequences. I suggest to test on the public benchmark datasets, e.g., [2].<|endoftext|>The paper focuses on distilling supervision signal from long sequences. They focus on cases where the input is a long sequence of length n, but the target prediction is determined by a small subset of size m of sequence fragments, where m << n. The authors propose augmenting data by randomly dropping spans from the input sequence. I find the solution similar to word dropout, and would be interesting to see comparison to that. 2.The gains on real datasets are not high. The augmentation procedure is very similar to word dropout. It is not clear if similar benefits can be obtained by using word dropout at training time. The experimental results on real datasets are weak.<|endoftext|>In this paper, we instead investigate learning problems for long sequences where not all input elements contribute equally to the desired output. In experiments, consistent improvement is shown. The paper is well written. In section5, the author discuss a bunch of related works on data augmentation, however, in natural language experiments these are not compared. It s not surprising that this data augmentation is effective, especially for the catfinding task. I don t quite understand why when in data the contributing portion is small or sparse, then the problem is "underspecified". It s still specified by the small contributing portion I think? If you replace a span with a random token, it gives counterfactual sequence with the same length. I have two major concerns (1) limited novelty. (2) The lack of comparison of other baselines.<|endoftext|>This paper proposes a data augmentation method, SpanDrop (and its variant Beta SpanDrop), for long sequence data, especially where supporting facts take small portions. They provide a theoretical background on their method and evaluate the method on a synthetic task (FindCats) and four real natural language processing tasks requiring reasoning over long texts. Experiments in the paper are well designed to show the effectiveness of the method. The method generally assumes that the decision of drop can be done independently. SpanDrop is only applied at the input level. The expectation of length is the same for SpanDrop and Beta SpanDrop. However, the expectation of length’s square would be different. Therefore, I guess their computational overhead will be also different. Don’t it require modification of other regularization such as standard dropout? The paper is well written and well structured.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 8; rating score: 8; The paper analyzes contrastive learning by bounding the supervised loss with contrastive loss. The bounds are tighter than those proposed by previous works. The paper address an important theoretical problem in contrastive learning. The paper is well written and the contributions are stated clearly. 1.Is CURL really unsupervised? The paper claims to analyze the “unsupervised” representation learning, however, the positive samples are drawn from class conditioned distribution, which requires ground truth labels to estimate. Does the observation still holds for general unsupervised contrastive learning, where positive samples are drawn via data augmentations? At least, in the experimental section, it would be great to see some results with standard framework such as SimCLR. 2.The class sampling assumption is stronger than Arora et al.[1].In particular, the latent classes used in [1] is not equivalent to the output classes for supervised classification task. Can the current framework generalizes to this scenario? Giving a high level intuition of proof technique can help reader understand the theorems better. 4.Are the bounds also tighter in term of generalization error? With access to only finite samples, the bound could behave different from its expected value. However, I am concern about the applicability of the bounds. The insights from the analyses seem to not directly applicable to practical setting. The authors claim to analyze an unsupervised algorithm, but the assumptions do require the access to full supervision.<|endoftext|>This work establishes a downstream classification loss bound for contrastive learning which shows that larger negative samples improve the classification performance. (2)	The authors also do some experiments to investigate their theories. For example, Arora first establish the relation between pretext contrastive loss and the downstream classification loss and shows why contrastive loss can work well for downstream tasks. However, in this work, I did not find such kinds of insights. So R*cont may not be achievable for one real contrastive learning or apart from the real optimal contrastive loss since the simplified case are different from the real contrastive learning. (3)	The authors should discuss the novelty of the proof techniques compared with existing one, such as Arora et al, Nozawa & Sato. It seems that the authors use the property that the function f is bounded to give a tight bound. So it is better to discuss your technique differences.<|endoftext|>  After Rebuttal  The author(s) have made many clarifications and improvements to the paper per reviewers  requests. arXiv 2107.01152I am recommending weak rej at this stage. This paper provides some new theoretical insights on how contrastive representation help to improve downstream classification performance. My main arguments are that (i) the author(s) actually analyzed supervised contrastive learning, not unsupervised contrastive learning; (2) need evidence to show the gains from large K are really from the variance reduction of sandwich bound, not from the variance reduction + bound tightening from the MI estimation perspective. However, I do have a few outstanding concerns that prevent me from granting a positive recommendation. **Strength*** Strong theoretical analyses, clear writing, and nice illustrative figures. Overall the technical contents of the paper are easy to follow. In that case, it is not surprising that contrastive loss bounds the prediction loss: because they are essentially the same thing. In general, I do not think unsupervised learning can directly benefit supervised learning without some strong assumptions (see for example [2]). * Apart from the above, another point I do not agree on is why larger K helps. The author(s) claimed that "the quantitative relationship between the negative sample size K and the MI estimation error has yet to be clearly known", which is not true, see theoretical analyses in [3,4]. Strength from weakness: Fast learning using weak supervision.<|endoftext|>The paper studies the learning bounds of popular representation learning. The paper is well written with thorough discussion and nice example for illustration. Strength:* The new upper bound is tighter than the existing works, and provides a better insights of having the large size of negative example that people found useful in practice. After having lower bound, the discussion of feasible region is interesting, and also provide some aspects of having "small K", which is also quite aligned with some empirical findings. * The numerical figures in Sec 4 is a great illustration to understand and compare different bounds. * Currently, the success of many contrastive learning heavily relying on data augmentations. It would be great the augmentation can be formulated into the analysis (e.g.as some stochastic process .. etc)The paper is well written with many great figures to explain ideas. The improved upper bound and lower bound are studied, which I think it s good to the community. However, I don t closely follow the theory development of representation learning, which I can only follow the derivations in the paper, and judge from a practitioner perspective.<|endoftext|>This work proposes a new theoretical upper bound for contrastive unsupervised learning loss and its downstream supervised loss. The proposed upper bound on both contrastive loss and downstream supervised loss is lower compared to other upper bounds, which make it sharper. 2.The upper bound on supervised loss decreases when K (the number of negative examples) increases. This does not happen for other upper bounds and is important for understanding why the performance of models gets better with more negative samples. I wonder how this interaction can also be introduced into the current upperbound, especially giving the intuition that with more negative samples, there will also be more examples in the same class treated as positive examples. 2.The proposed upperbound on downstream supervised loss is based on the intermediate upperbound on the mean supervised loss, which essentially uses the low dimensional embeddings as the final representations to do the classification. Although this would serve as a right intermediate upperbound when the embedding is only one linear layer away from the typical layer the category readout is extracted, this would not be true when the embedding is several layers away from the typical layer, which is actually the case in most SOTA algorithms now. This work proposes a sharp loss upperbound for contrastive learning and its downstream supervised learning. It is validated by the empirical experiments conducted by the authors on different datasets, but lacks the interaction between the number of classes and the number of negative samples.
Reject; rating score: 3; rating score: 5; rating score: 6; This work proposes a method to disentangle epistemic from aleatoric uncertainty for avoiding the noisy TV problem which occurs when intrinsically motivated agents get rewarded for visiting states that have high irreducible uncertainty. Although far from optimal, such approaches are already being used in epistemic uncertainty driven exploration. Kendall & Gal work uses a Bayesian Neural Network, whereas deterministic uncertainty estimation was proposed here [https://openreview.net/forum?id Fu7D6kQPzs4](https://openreview.net/forum?id Fu7D6kQPzs4) (not cited in the text)C2: For the epistemic and aleatoric uncertainty you cite Hullermeier & Waegeman. Do you mean that through the regularization, epistemic uncertainty is reduced? Finally, I d like to suggest to the authors to try to use \citep instead of \cite whenever possible. Also try to use a more accessible colour for the citations as currently, it makes it challenging to read.<|endoftext|>The better exploration, proposed here, is important, but it may or may not lead to better performing agents. I like the neuroscientific inspiration of the proposed algorithm. This makes the model more useful. The authors have performed an impressive set of experiments, involving a significant number of baselines. *Weaknesses*Provided the close relation of the proposed method to prior work (as mentioned by the authors), it would be reasonable to expect a strong Results part of the paper (see below). Most of the experiments in this paper were performed in environments specially altered to introduce stochastic traps. In the proposed experiment for establishing the role of acetylcholine in the brain, the prediction seemingly boils down to the fact that epistemic uncertainty decays over time, whereas aleatoric uncertainty remains constant. These predictions seem to directly follow from the definitions of such uncertainties, so it is unclear whether the proposed design of the experiment is necessary. The claim here, supported by the literature, is that stochastic traps are the major problem in curiosity driven exploration.<|endoftext|>This paper suggests an intrinsic bonus for exploration that avoids noisy TV by adding a penalty for the estimated variance of the reached state $S_{t+1}$ given previous state $S_{t}$. I expect this approach to work well, and I trust the good experimental results exposed in the paper. Here everything works as long as the agent is not changing his policy, but depending on the learning dynamics of the policy, the model may never converge, this would not happen with action taken into account. But all the method section needs to be re written from scratch.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper describes a newly created library for a geospatial data processing. The library is based on PyTorch and combines the functionality of common geospatial libraries. TorchGeo can serve as a platform for performing geospatial machine learning research. Comparison to previous work  Previously, the same result (loading the data, aligning the bands, loading datasets) could be achieved by using a combination of a few packages, including gdal, rasterio and geopandas and common Python libraries (TF/Pytorch, etc.). Theory Theoretical part is not included in this research, it is more engineering paper. Pros   This is a great contribution, useful for beginner researchers in the field of ML for Earth Observation data.<|endoftext|>The paper presents a new PyTorch library, TorchGeo, specifically for processing deep learning models on geospatial datasets. The paper does not contribute to any aspect of representation/deep learning, and hence ICLR would not be a right fit. The contribution, the TorchGeo package, is more relevant for the remote sensing field.<|endoftext|>Introduces the torchgeo Python module, which provides torch Datasets, augmentations, samplers, and pretrained models to provide reproducible data ingestion for deep learning on standard geospatial datasets. * Benefit of a torch specific library are by design restricted only to torch users. * Provides ready to use torch datasets for 18 different benchmark geospatial datasets. This may be out of the scope of this paper, so I m not sure how big of a weakness this is.<|endoftext|>The paper introduces TorchGeo, a Python library for integrating geospatial data into the PyTorch deep learning ecosystem. TorchGeo provides data loaders for a variety of benchmark datasets, composable datasets for generic geospatial data sources, samplers for geospatial data, and transforms that work with multispectral imageryStrengths : The authors implement various architecture combined with popular datasetsWeaknesses: The technical novelty of the paper is low (mathematical modeling, deep learning architecture, processing, etc..)Even though the library is interesting, the overall contribution of the paper is low and does not advance the field<|endoftext|>This paper tries to create generalized pipelines that both domain scientists and AI researchers can use to quickly get up and running with their experiments. The paper gathers existing datasets and makes them available through TorchGeo, a PyTorch domain library. Give examples of tasks  Detail how this pipeline will actually be used, eg reduced time, reduced lines of code etc   Encourage people to add their own datasets to this library, and detail the process for doing so   Have datasheets for datasets also publishedThe paper establishes a new domain specific library that is ML ready
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper proposes a UniFormer architecture, which combines 3D convolutions and spatiotemporal self attention for efficient and effective video classification performance. The technical novelty of the paper is limited. I appreciate that the authors included this section in their draft, as it makes it easier to understand their approach. + Very strong results on multiple action recognition benchmarks.<|endoftext|>This paper proposes a new transformer model for video understanding task. The proposal algorithm has the benefit of both 3Dconv to efficient capture local context and transformer for global reasoning. +SOTA accuracy with 10x more efficiency compared to strong baselines. Solid paper with clean formulation, experimental analysis, and well written.<|endoftext|>Despite the bigger design space of L and G, the ablation studies show that the CNN+Transformer architecture works the best. The bold highlights are inconsistent. Strength:  The paper makes an insightful connection between X3d s conv block and transformer block, and build a unified computational block. Empirically, this paper achieves SOTA results in both accuracy and efficiency.<|endoftext|>In this paper, a new architecture, Uniformer, is proposed to learn spatial temporal pattern in videos. Extensive empirical studies demonstrate the strength of the proposed method. The paper introduces "DPE" module and the description can barely be found. The experiments are carefully designed and results are encouraging. The reviewer would appreciate it if the authors address the above concerns.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper proposes iterative structured pruning methods using activation based attention feature maps and an adaptive threshold selection strategy. Inspired by attention transfer, Activation based attention feature maps are constructed as the important evaluation of filters in each layer. Strength: + The proposed method is simple and easy to follow. Please clarify the difference between these works and the proposed methods. Lack of some important experimental evaluation: 1. The actual speedup of the pruned models should be evaluated if minimizing FLOPs. After this rebuttal, the authors well answer my comments about the comparison to SOTA methods. Moreover, the rationale of threshold T is based on the assumption that a layer is more likely to have redundant filters to prune if it contains more remaining parameters. I think it is not a correct assumption, as the entire filters of a layer with a smaller number of parameters may be redundant to be removed safely, especially for ResNets.<|endoftext|>This paper proposed an activation based, adaptive threshold, iterative structured pruning method, combining several existing techniques together to perform a comprehensive pruning. The method can automatically meet users  several requirements generally, like accuracy, latency, memory, and so on. Each part of the technique is not that novel. It is like a combination of several existing tricks to perform comprehensive pruning, not enough analysis of the convergence or correctness. 2.The comparisons on Imagenet are too weak. There are many SOTA pruning methods on Imagenet. The paper only lists four of them. I suggest comparing the paper with more recent SOTA pruning papers and comparing on more benchmark models besides Res50. Due to the concerns on the novelty and the weak comparisons on Imagenet. I temporarily think this paper is marginally below the acceptance threshold.<|endoftext|>This work proposes a technique for iterative structured pruning, without necessarily requiring too much manual human intervention. There are two parts to this paper that are important:1. It is argued that we should prune channels based on the activation maps generated, rather than focusing on the weights of the channel. The paper needs to be (completely) rewritten to match the expected format in the computer science literature. The automated approach is relatively simple, and far more interpretable than approaches such as AMC. Firstly, there is no direct reason to couple the pruning technique, with the method for iteratively pruning; can these not be compared separately. I know that they re harder to prune, but I d also like to see some experiments on VGG models, for example. I can be swayed at rebuttal time if there is convincing new experimental evidence.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; "However, despite the availability of differentiable simulators, ithas not yet been convincingly demonstrated that they can effectivelyaccelerate policy learning in complex high dementional andcontact rich tasks, such as some traditional RL benchmarks." "There are several reasons for this: 1. Perhaps, "possible reasons" would be better. The simulator is fast, so it may have been better to do more than 4 environments. I have not seen such results using differentiable simulators before, so I think it warrants publishing.<|endoftext|>Pros:* The paper is clearly written and well organized. Is there a way the authors could show that this exploitation is avoided in their simulator? * Are the smoothness assumptions made in the simulator actually transferable to real life robotics?<|endoftext|>This paper demonstrates the effective use of differentiable simulators for significantly speeding up the training of policy gradient methods. The main idea is to backpropagate through the simulator only on shorter partitions of the trajectory, which seems to lead to a smoother loss landscape and better gradient signal. * If section 3.2 would be more elaborate and the experiment would include more environments, this paper could be further improved.<|endoftext|>The proposed method is mostly the same as the method introduced in [1], where the only difference seems to be that the proposed method uses exact gradient from the simulator instead of gradient of a learned dynamics model. Pros1.The empirical results of the proposed method are very strong. Now I recommend acceptance of this paper.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; Strengths  The motivation is clear. Is it  implemented as a fully connected layer? It gives the reader the impression that the gain is for general image to image tasks. The paper is well motivated and the technical part is straightforward to follow. But the actual gain with this wavelet based compression of feature maps is not convincing, mainly the added complexity is not reflected in the Bits Operation metric, the results on experiments are not consistently performing well.<|endoftext|>If necessary, it is possible to apply the proposed method to the network for classification and compare this result with various SOTA studies. 3.It is difficult to agree with the argument of this paper because the motivation of this paper is not sufficiently presented. 6.I hope that the description of joint shrinkage can be clearly presented through the figure. 11.There are some typos. In addition, the experiment part that supports the contribution needs to be thoroughly supplemented.<|endoftext|>The weaknesses of this paper:1， It seems that only harr wavelet is used. 2， WCC is designed for 1x1 convolution. However, the kernels of CNNs is usually larger than 1x1. 3， The method used to compress actitation maps is borrowed from the traditional image compression technique. I do not find the novelty of this paper.<|endoftext|>In International   Conference on Learning Representations, 2018   Do so as well for image generation and pooling tasks. How are the (tensor ?) Springer.The process is currently not obvious. ## Strengths:  The paper presents a neat and clever way to apply traditional wavelet image compression in CNN.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; There are no empricial results nor theoratic analysis to back the authors  claims. Some issues:  There are a few typos and incorrect latex references that impacts the reading experiences. after "not just a complete observation,". It would be better to provide some empricial results to show the effectness of the proposed models. The face recognitions task is used as an example in many places in the paper, but there are no experiments to evaluate how the proposed model performs. It s also worth to show the proposed model can run in a reasonable speed for modern datasets. I don t think this paper is ready to be read.<|endoftext|>However, I only see the method descriptions instead of in depth theoretical analyzes. Besides, this paper is very hard to follow. It is not well structured and has many typos. I think this paper s contributions are not well justified. Given its current status, I cannot vote for its acceptance.<|endoftext|>The authors present a framework in which Tensor Factor Analysis is incorporated in a causal graph structure implemented by a series of "capsule" auto encoders. It states that the paper seeks to "connect tensor causal factor analysis with deep learning" but does not adequately articulate why this is needed or a good idea. There is a lack of clarity in definitions: (i) several acronyms are not defineds (e.g.MPCA/MICA). *Most crucial is the lack of experimental evidence. *Conclusion is quasi non existant and there is no discussion about future directions for this work and how it may be improved in the future.<|endoftext|>This might be due to the lack of knowledge on my end. The authors should provide more background in their paper. I request the authors to highlight them in the paper. 2 and some minor typos. Some comments on the scalability of their approach will also be appreciated. I am willing to reconsider my ratings if the authors address some of my concerns.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This application involves some ML pipeline, and this new combinatorial optimization problem helps with one of the steps. The problem is shown to be not submodular, and therefore the most standard methods don t quite work. I would say this part of the paper is interesting but unsurprising. The stronger contribution in my view is the observation that this problem helps in the COVID vaccine design pipeline. This is a nice paper with good contribution for the (ML based) pipeline for COVID vaccine design.<|endoftext|>The submission is concerned with vaccine design and proposes to model the task as a combinatorial optimization problem that is essentially a generalization of set cover. Experimental results show that the ILP comes close close to the optimum solution. The submission is built with a clear application in mind. It provides two reasonable approaches for a well modeled problem. There are little theoretical contributions, but I found the practical contribution well executed and convincing.<|endoftext|>The paper introduces a variant/generalization of multi set multi cover problem, where the aim is to maximize the weight of the elements covered at least n times by up to k overlays (subsets of a given input familiy of sets over the elements  universe). The authors show that the objective function is not submodular, hence does not admit a classical greedy approach. The problem is meant to model vaccine design and show empirical evaluation on two toy data sets and presenting comparisons between the greedy approach and the ILP approach for a covid 19 vaccine. The new problem appears to be interesting. There are a couple of concerns I have:   the pseudocode for algorithm 1 is not clear to me. It is also not clear to me from the description where the weights enter in the evaluations of the design for covid 19. Due to these concerns I am not giving a fully acceptance to the paper but I am willing to receonsider my score on the basis of the authors  reply. Strenght: The paper offers an interesting and apparently unexplored variant of set cover that can model a vaccin design problem.<|endoftext|>Update: I have read the author feedback and other reviews. In this work, the authors propose a variant of the set covering problem which aim to formalize peptide based vaccine design as an optimization problem. The authors first formulate the problem and then draw connections to other set covering problems; they then show that the problem is NP complete. Considering the experimental results, some obvious baselines are missing. The paper is generally clear and well written. The reproducibility and provided resources of this work are in line with the field. Considering the lack of theoretical novelty and biological accuracy, it is difficult for me to see the impact of the work as it is currently presented.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; An additional result they show is that in the absence of any input structure the learning problem becomes hard under the statistical query model. The primary results of the paper are theoretical. Firstly, the lower bound to show that in the absence of any input structure learning can’t happen follows almost directly from a similar lower bound in Daniely and Malach (2020). Can you elaborate on how this condition is satisfied in the case of parities? It builds on a line of literature which theoretically capture settings where neural networks trained with gradient descent are provably more powerful than data independent kernel methods. Moreover, they authors clearly outline the mechanism by which neural nets derive their additional power.<|endoftext|>The authors demonstrate that in the absence of structure (i.e., when features are independent of each other), no algorithm can recover low error, which hints at how data dependent feature learning is necessary for generalization in neural networks. Strengths:  The paper presents an interesting analysis of gradient descent under data dependent feature distributions and the results appear to be novel. Can the authors provide explicit details on the improvements / differences with comparison to the results in Daniely and Malach (2020), specifically the lower bound?<|endoftext|>This work proves the feature learning ability of a 1 hidden layer network (Theorem 1), under the assumption that the data is generated from a dictionary learning model. The proof proceeds by closed form analysis of gradient dynamics, which is possible thanks to the simplicity of the data generation mechanism. * The theoretical results are empirically verified. Please consider reporting quantitative result (e.g., in terms of certain distance measure between model features and the ground truth $M$), and examine across different data dimensions,sample size, and optionally with respect to class imbalance, etc. which slightly weakens the quality of this contribution.<|endoftext|>The authors create a setup motivated by real data where they can analytically show that a single hidden layer NNs can achieve good error (Theorem 1), whereas models with fixed features need to be exponentially large (in the number of relevant features) to match (Thm 2). The paper (to the best of my knowledge) is technically sound.<|endoftext|>They also prove some lower bounds suggesting the necessity of the some of the model assumptions. +  a simple model+ a positive learning result analyzing GD+ a relevant lower bound  the model is very specific with many assumptions ensuring that the GD analysis works out (e.g., irrelevant attribute effects are balanced out)  all the interesting action in the upper bound happens in the first two rounds of GD, which suggests that this particular problem can also be solved by simple combinatorial methods  the claimed SQ lower bound simply consists of showing that the parity function fits the model (i.e., no new lower bound insight here)  it is not clear that kernel based learning cannot solve this problem; the text suggests this but I could not find any proof/evidence in the paperThe contributions of this paper, in my view, are a simple model that seems to require learning input specific features, and a careful detailed analysis of the first two steps of GD applied to a 2 layer network with truncated ReLU activations.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper proposes a method called Generative Planning (GPM), which aims to improve exploration for model free RL. The evaluations are solid, comparing with 5 baselines on 8 benchmarks. GPM is compared with 5 baselines on 8 benchmarks, and experiments show that GPM converges faster than the other baselines. Further analyses show that GPM indeed explores more effectively, and it also generates interpretable short term plans Questions:  What is the computational overhead of GPM compared with the baselines? I would like to see a discussion on this from the authors. This paper is well written and well motivated. The proposed technique is interesting and the experimental section is strong. Overall, this is a good paper and I recommend acceptance.<|endoftext|>The core idea is a creative one that performs well, and will promote future work and discussion. The method builds on SAC. Overall the benefits of the temporally extended action plans are:(a) temporally coordinated exploration; (b) more effective than action repeat; (c) some degree of interpretabilitygiven that an action sequence plan represents then intent of a policy in a given state. Strengths:  the method is (to my knowledge) a novel approach that I see as sitting somewhere between model based and model free. Regardless of the actual performance numbers, the results will be of interest to readers. Is this a critical design choice? It took me a while to understand that there was no other policy network, aside from the plan generator. I believe that the results will be of strong interest to the community.<|endoftext|>This paper proposes a method for exploration called Generative Planning method (GPM), which generates a multi step action sequence such that the exploration is more temporally consistent and "intentional" compared to regular single step action noise exploration. The multi step action sequence is output by a generator with an RNN structure, and the generator is optimized by maximizing the plan value function. As far as I know, the idea is novel and very interesting. 2.The topic is definitely very related to the conference and of significant interest. 3.The paper is generally well written and easy to follow. Furthermore, since this paper concerns exploration, it will also be useful to demonstrate the effectiveness in sparse reward environments. I believe the idea and the approach are quite novel and interesting, however, I do believe some extra experiments should be performed and some improvements over the paper presentation are needed.<|endoftext|>This paper presents a method called generative planning method (GPM) to improve exploration in RL. Each time step, the plan from the previous time step is shifted forward by one time step and compared to the newly generated plan. The absence of more competitive baselines makes the significance of the paper relatively small. The authors compare GPM to a variety of action repeat based exploration methods, from epsilon greedy policies to policies with learned action repeat counts (DAR & TAAC) and find that it is competitive with or outperforms these methods on various low dimensional robot domains, as well as the image based CARLA environment. After the author s response: The additional experiments and updated related work section have addressed my primary concerns. # StrengthsThis paper is well written and tackles an important problem. I strongly suggest adding the details about the method from the rebuttal (which I found useful) to the paper. # WeaknessesThe main weaknesses of this paper are that it does not demonstrate strong empirical results, that the related works section is missing some important discussion, and that the method is missing some important details. ## ExperimentsIn terms of experiments, the paper does not compare to competitive exploration methods and the tasks, other than CARLA, are relatively easy.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; During training the model receives data from a non i.i.d.source as well as random samples from a sample buffer (the episodic memory). I think this work is good in general. E.g.are two teacher networks necessary? The authors discuss recent and related works and their method seems original but I lack the expertise to judge the novelty of the approach.<|endoftext|>In this paper, motivated from the CLS theory in neuroscience that efficient learning requires short term adaptation and slow learning of structured information, the authors propose a novel dual memory experience replay method for continual learning. Overall, it is a well written paper with an interesting idea. The two models are updated with a Mean Teacher fashion during training with different frequencies. The authors clearly describe the proposed approach and also give a detailed analysis.<|endoftext|>This paper proposes a novel dual memory experience replay method to store the knowledge of previous tasks. The contribution of this paper is novel. Weaknesses:1, Can this method be applied to online continual learning task? Experiment results on more datasets should be shown in this paper. In my opinion, the contribution of the proposed method is novel and interesting. However, more experiments should be designed to prove the proposed method further.<|endoftext|>They propose a dual memory experience based on complementary learning systems (CLS). I am not an expert in this domain and have not much experience with related works. Compared to existing methods, is it a fair comparison in terms of training cost such as using two models including plastic and stable models and using reply data to train the working model? The working model is updated using consistency loss with the selected optimal semantic memory from plastic and stable models.
Reject; rating score: 5; rating score: 5; rating score: 5; The above mentioned datasets also can be used for evaluation of the out of domain generalization of the method which has not been discussed in the paper. Baselines and state of the art results on the test data: One important baseline is to make the prediction using only radiomic features. Consider boosting the results and observation using a few more test data such as MIMIC and CheXpert that have the similar label space as NIH 14 and publicly being used in most of the related papers.<|endoftext|>This paper has proposed a machine learning model to utilize radiomics features in order to improve chest x ray image classification performance. 1) Radiomics is a term that most people in the ICLR community are not familiar with. 2) Prior multi modal feature fusion work in the chest x ray domain should be discussed, examples below. 3) I don t see "prior knowledge" and how it s utilized in this work. Why/how did the authors select the 8 classes out of the 14? 7) Are the AUC values generated on the test set?<|endoftext|>While the proposed method has novel and interesting components, the experimentation and evaluation in the paper need to be improved. The proposed CheXT should be compared against the more recent CNN baselines such as ChexRadiNet [1]. There are also some concerns with the experimental details and analyses that need to be addressed. Strengths+ The paper is fairly well written and presents a well studied literature review. Weaknesses  Exposition of results is not good. In Fig.3, two images look different. (1).It would be more viable to report both classification and localization performances on the same 880 images.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposes a novel method to tell whether a unit (feature map) is effective in CNNs quantitatively, via "feature entropy". This would be useful in some real cases where labels are expensive and we want to use all the labeled data to train a model for the potentially best performance. 3.Network pruning is mentioned in the related work. Thinking backwards, does this suggest symmetry in the feature map is important or not? However, the fact that experiments are all based on one kind of CNNs is concerning. I would consider it as a board line paper with the current version. The improvements on the paper with answers to Q1&3 are substantial.<|endoftext|>In this paper, the authors propose a technique to measure the state of each neural network unit. The features of the neural network units are given by a method that applies TDA to a graph representation of the activity degree of each unit. Since the proposed method can be computed only for square matrices, it cannot be applied directly when the number of elements in the data or filter is not square. In this case, there is no effect of using TDA. On the other hand, the proposed method is heuristic and limited, and there are some questions about the theory behind feature extraction, which require deeper investigation in the future.<|endoftext|>If this paper can be extended to address this, I believe the work will have impact. I am not sure also not sure about the impact of the paper, in its current form, and its utility for enabling improvements in design of neural net architectures. For units that the authors term, effective units for a given class and a given input image class population, the analysis is expected to yield to similar birth time of the holes. The main essence of the paper is to translate the activations of the neural network for a given pattern class into a graph representation whose topology evolution is analyzed (using TDA) as a function of activation threshold.<|endoftext|>The paper proposes a topology based approach to measure the effectiveness of a unit in a neural network for a set of inputs (i.e.images from a specific class). Maybe “importance” of a unit will be clearer. Filter pruning via geometric median for deep convolutional neural networks acceleration. Overall, the paper is interesting and the proposed approach of measuring unit importance is well motivated. However, as mentioned above, I do have some concerns regarding the empirical evaluation of the proposed method.
Accept (Poster); rating score: 6; rating score: 6; rating score: 3; The paper discussed the problem of spectral bias in MLPs (i.e., dependence on the higher frequency part of input feature is under fitted) within RL domain. It is a bit weird to put Related Works in the end. Overall, I think the paper successfully addressed it by suggesting the usage of a random Fourier feature network. While the empirical results are good, the novelty is a bit weak. My understanding is that the authors borrowed the RFN from recent studies and made an investigation of the hyper parameters so as to fit in RL.<|endoftext|>The choice of bandwidth as well as the number of features required are tested. Strengths:  The paper tackles an important problem and has an exemplary exposition of the problem and the proposed solution. Its results suggest that much more could be done in RL to take into account the peculiarities of the functions being estimated to find good architectures (that may depart from their supervised learning counterparts). The text legends in Figures 8 11 could be larger  An "ablation" is when something is removed.<|endoftext|>This work proposes Random Fourier Networks (RFNs), MLPs with random Fourier featurization within the first layer. The authors present an initialization scheme for the Fourier layer that is better suited for high dimensional inputs. This work includes claims about "evading the spectral bias" and, in some sections, uses performance to "show" the efficacy of the proposed method. Additionally, the first paragraph largely repeats content from earlier in the paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper generalized a data splitting conformal prediction approach to the adversarial attack setting by combining conformal prediction with randomized smoothing, which yields a prediction set with finite sample coverage guarantee under an l2 norm bounded adversarial noise. I like the proposed idea of using randomized smoothing to construct a non conformity score that forms a prediction set with a finite sample coverage guarantee under adversarial noise on the features. The paper is also written in a clear way which gives enough background knowledge to readers who are not familiar with the field of conformal prediction. The motivation of introducing randomized smoothing into the score function was very well illustrated, and the theorems 1 and 2 provided convincing results of the effectiveness of the proposed approach. I have one major concern of the experimental section. Since the comparison was only done with the non robust conformal prediction approach, I am wondering if it is possible to compare to other robust CP methods such as those mentioned in Section 4.1. In particular, the authors mentioned that this work "handles a full distributional shift induced by the adversarial perturbation". It would be helpful if the authors can clarify on this point. Overall I like the idea proposed in this paper. My major concern is about comparing with other robust conformal prediction methods in the experimental section.<|endoftext|>This paper proposes a generic method to construct conformal prediction sets in an adversarial setting. Since standard conformal prediction method assumes an i.i.d.assumption for training and testing input, its generated prediction sets for adversarial examples will not satisfy the coverage guarantee. It then proves that the prediction sets constructed by the proposed method satisfy the coverage guarantee for worst case scenarios. Empirical evaluations are also performed on benchmark datasets, which justify the effectiveness of their method. The paper considers the conformal prediction problem in an adversarial setting, which is new and important in my perspective. It is very well written, which I enjoyed reading. The idea of using randomized smoothing to construct a robust conformity score is quite novel and theoretically sound for conformal prediction, despite the techniques it used are well known in the field of adversarial robustness. The empirical studies clearly demonstrate the vulnerability of standard conformal prediction method in the presence of adversarial examples, which support the motivation of the paper. Nevertheless, I have the following general questions for the authors:1. In Figure 5, you show the marginal coverage against adversarially perturbed inputs for different methods. 2.The method “CP + SS” is not able to satisfy the desired coverage. Is this because the base classifier is trained to be robust against Gaussian noise instead of against worst case adversarial perturbations? I am wondering whether you could achieve the coverage by using the conformity scores outputted by an adversarially robust classifier? Do you have any empirical results supporting this? In general, how to decide which method to use if we want to deploy the conformal predictors you proposed. Overall, I think the paper is well written and reach the acceptance bar of ICLR.<|endoftext|>Violation of the i.i.d.assumption invalidates the inference, and adaptations to some structured distribution shifts have been proposed recently in the literature. The authors consider a popular setting where test data contains adversarial examples with bounded $\ell_2$ norm. For a modified procedure to work, the non conformity scores have to satisfy a certain property, and the authors propose a way to incorporate any ``base  score into the framework via randomized smoothing. I was wondering whether the authors could comment a bit more on the following questions:  Q1: While it is intuitive that getting robust prediction sets comes at the cost of larger prediction sets, I couldn t find a simulation that considers a null case. Is it indeed the case that such simulation was not present in the paper? It is clear that at the inference stage, for constructing a prediction set, it is necessary to perform sampling of perturbed input $n_S$ times for each class (as the authors point out in the Appendix S7). It is interesting how computationally feasible the framework is. Q3: I was wondering whether the authors can comment a bit more on the sensitivity of the smoothed scores to using approximations obtained via sampling. But it seems that if the original non conformity scores are bounded, then it is possible to look at the sizes of the confidence intervals for the quantities that are being approximated. it seems that $\tilde X_{n+1}$ (an observed feature vector) should be stated in place of $X_{n+1}$ (an unobserved one). In general, the work is well written equipped with important parts of (relevant) literature review and well designed simulation studies. The current work represents a solid piece of work that takes a step forward towards robust conformal prediction. This work focused on a different setting which hasn t been covered in conformal literature before where at the test stage adversarial examples invalidate the vanilla CP. **Update after rebuttal**I would like to thank the authors for the detailed responses. Taking into account the general contribution of this work and points mentioned in this and other reviews, I tend to keep the current score.<|endoftext|>This paper tackles the conformal prediction problem under adversarial perturbations, where the conventional exchangeability is violated. The proposed approach is evaluated over three different image benchmarks (i.e., CIFAR10, CIFAR100, and ImageNet) and demonstrated its efficacy by comparing the naive conformal prediction and the naive conformal prediction with the randomly smoothed non conformity score. (novelty on theoretical guarantee) The novelty on the theoretical result is less significant; the correctness of the main theorem is heavily dependent on the condition of a non conformity score in (7). In particular, finding the Lipschitz constant M_\delta is the challenging problem and actively researching area (as the paper pointed out), especially the score function is a highly non linear neural network and an input is high dimensional. However, given the score function that satisfies (7), constructing a conformal prediction set is relatively straightforward (even though I agree that it s a new result). 2.(preciseness on theoretical guarantee) The statement of Corollary 1 needs to be described more precisely, considering the fact that the literatures of conformal prediction are trying to be rigorous on making the theoretical guarantee; Corollary 1 requires randomly smoothed score function, but we cannot evaluate this function due to the expectation without approximation (as described in the paper). In this case, it would be more appropriate to say "the prediction set is asymptotically valid" or consider the samples required for approximating the expectation as a part of sample complexity analysis for the proposed prediction set. I think the latter making this paper theoretically more interesting and novel. The related work section points out the limitation of this work, i.e., "the f divergence measure is notoriously difficult to estimate in practice"); however the proposed approach of Cauchois et al.(2020) is actually evaluated on CIFAR10 and ImageNet; in particular, the core part of this approach is convex optimization, so could be computationally not expensive. If the constructed prediction set is conditioned on one fixed calibration set and evaluated over multiple testing examples, the empirical coverage probability of the standard conformal prediction is around the nominal level \alpha (see Figure 2 in Tibshirani et al.(2019)).In this sense, it s unclear whether RSCP is better than CP+SS. If the paper wants to claim that RSCP is better than CP+SS, I think PAC style prediction set definition is required (e.g., [R1], [R2], [R3], or [R4]), which construct a prediction set conditioned on a fixed calibration set for correctness guarantee. Other comments  The description on conformal prediction is not precise; the paper said it requires i.i.d.but exchangeability is enough. "however, none of them addresses the adversarial setting specifically" and "Both approaches differ from ours, which handles a full distributional shift induced by the adversarial perturbation": I think they are strong or incorrect since the adversarial setting is a special case of covariate shifts.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; 6.The proof of Lemma 10 includes statements about the properties of $\mu$ which are not stated in the lemma statement (though they are mentioned in the main paper with a pointer to the supplementary). The lemma statement should be enriched. The analysis given here only successfully explains how to fix the proof of Lemma 2, not lemma 1 (which contains second order terms.I think the authors should also redact the relevant part of the paper and appendix better. Currently, it is not unambiguously stated at the top of appendix K that the extension proposed does not actually suggest *training* the second layer weights. Questions for the authors related to comment [1]: 1.1. It would be nice to have a one page summary of the results and proof techniques in [2] in the supplementary. There are only very minor issues.<|endoftext|>This paper presents the first theoretical analysis of self training on one hidden layer neural network with gaussian input. Experiments validate the theoretical results. The paper tackles a challenging and important problem. **Cons**  There may be significant technical flaws in the proof. Note that this function is **convex** in $W$! Zhong et al.**Update**I thank the authors for the detailed response.<|endoftext|>This paper theoretically analyzes the iterative self training algorithm with 1 hidden layer neural network. The theoretical analysis is rigorous and contains novel technical contributions. I wish the theory discusses when this happens. 2.I wish the theory offers guidance on how to improve self training algorithms in practice. 3.Section 3.3 talks about  0 generalization error .<|endoftext|>The paper theoretically analyzes self training in neural networks with one hidden layer. Thanks to the authors for patiently addressing the various concerns and adding required appendices. However, I find that the paper in its current form does not push the boundaries on our understanding of self training.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors proposed a method to offer explanations for a multitask prediction model with a single explainer. The method provides explanations in cases where the GNN is trained in a self supervised manner, and the resulting representations are used in future downstream tasks. (1) Competitive baselineOther methods, such as PGExplainer, are training multiple explainers on different downstream tasks and evaluating each explainer on different downstream tasks. I speculate the reason maybe that many aspects of the two methods are different. Therefore, it will be interesting to see under exactly the same setup of Task Agonistic, but one trainer per task, if the proposed method doesn t lead to much information drop then it demonstrates the good performance. (2) NoveltyThe paper is quite similar with graph pretraining to learn a useful embedding and contrastive learning with mutual information , but with additional explanation part. (3) Intuition of design of p Page 4 sec 3.2 "We introduced a masking vector p  to indicate specific dimensions of embeddings on which to maximize MI. Duringexplanation, we obtain the masking vector from the importance vector computed by any downstreamexplainer T_down. How can the randomly selection of p during training help with specific dimension of embedding from importance vector during testing? The paper is tackling an interesting problem   explanation of graph. However, some results are a bit counter intuitive.<|endoftext|>The authors propose TAGE, a task agnostic explanation method for explaining GNNs. TAGE explains GNN embedding models without downstream tasks and allows the explanation of multi task models. This paper maximizes the mutual information of masked graph embedding and masked subgraph embedding as the objective function. The proposed method uses the downstream explainer to get dimension importance and highlight the important dimensions. 3: It is a novel setting that is different from existing ones. 2.There is very little intuition provided to justify the practical significance of the explanation provided by the embedding explainer if there is no downstream task. 3.Some key baselines are missing. For example, Gem [1] is also a learning based explainer published in ICML2021. I d like to see the comparisons between the proposed method and [1]. Specifically, graph embeddings are required when training the embedding explainer. Although node embeddings can be simply compressed to graph embeddings, the reversion is difficult. Therefore, an embedding explainer targeting a graph level embedding model will fail to predict node pair importance scores. 7.The empirical evidence could be more substantial. Visualizing a few instances is not convincing in terms of model explainability. Further analyses are expected. On the experimental design, some scenarios do not satisfy the multi task setting defined in general. It is unfair to compare with the baseline PGExplainers trained and eval on this problematic experimental setting. Therefore, the results in Table 2 do not justify the use of TAGE. In Figure 3, on Task0 of the PPI data set, the fidelity scores of the proposed method are much lower than those of DeepLIFT.<|endoftext|>This paper is motivated by the fact that existing task specific explainers are too expensive to be applied to generating explanations for a model trained for multi tasks. They decompose the typical end to end learning based GNN explainer into two parts: the embedding explainer $\mathcal{T}_{\mathcal{E}}$ and the downstream explainer $\mathcal{T}_d$ ($d$ for down). The embedding explainer is associated with the embedding model $\mathcal{E}$, which maps a given graph into a subgraph of high importance, conditioned on the embedding dimension importance. In the experiments, TAGE outperforms the SOTA explainers w.r.t fidelity, sparsity, and especially time cost. 3.The method is simple but effective. If there is a performance gap between this two formulations, I wonder how each of them affect the quality of generated explanations. 3.During the self training of the embedding model, $p$ is sampled from a multivariate Laplace distribution, while later, the input is the conditional embeddings generated by the gradient. Can the authors comment on this a bit? 4.Some typos: last row of page 5 “as input” should be “an input”; In Section 4.1, “include four graph classiﬁcation tasks” should be “include three graph classiﬁcation tasks”. I think this work is overall good, however, some technical details need to be clarified and the proposed model can only be necessarily applied to a limited range of explainers.<|endoftext|>In this manuscript, a new explainer for GNNs is proposed. The newly proposed method aims to provide a task agnostic explanation for the embedding GNNs rather than a specific downstream task. The motivation of the proposed method is that the modern GNNs are typically trained in a two stage manner, where the embedding GNN is trained in the first stage and then the task specific lightweight MLP. The proposed method is trained based on the MI between the whole graph and sub graph generated by the explainer. 1, The setup of the explainer in this manuscript is a little different from the traditional ones. 2) the proposed explainer, under its setup, is more like an input distillation model, where the input graph will be shrunk during the inference time. 2, The proposed method seems to be a general method for the DNN models, not limited to the GNN. I m wondering if the authors have tried to apply the proposed method to the CNN or transformer? 3, For the evaluation, it would be interesting to apply the proposed method on the generated synthetic datasets like that used in GNNExplainer, where the ground truth explanations are available. The main concerns are about the setup of the explainer and the evaluation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper proposes an imbalanced classification strategy for GNNs. The proposed method alleviates the neighbor memorization problem by synthesizing ego networks. Extensive experiments are conducted to evaluate the proposed method. 2.I like the preliminary analysis on the importance of neighbour sampling, putting the neighbor memorization problem in a quantitative manner (i.e.section 3)3. Would the model benefit from more elaborately designed distributions? The paper is well written and technical sound, with strong results.<|endoftext|>2.More details should be given for some confusing points. Experiments on several datasets show that the proposed model can outperform the baselines. However, there are several major concerns in the paper. And this study fundamentally lays the foundation for the following part of this paper. However, there are several issues in this case study. However, on different datasets, node representations calculated by GNNs may be influenced differently by the node self features and the neighboring information, due to the distinct characteristics of datasets. Only comparing with the most conventional approaches is less convincing to prove this hypothesis. However, this conclusion is not quite reasonable. The authors should give more explanations.<|endoftext|>This paper addresses the class imbalance problem for node classification in a graph and points out some issues faced by existing GNNs and  methods to address the same. 2.The related work and baselines used for experiments are quite exhaustive to the best of my knowledge. The proposed algorithm is simple and intuitive, but not that rigorous.<|endoftext|>This paper exploring neighborhood memorization problem and proposes a neighbor aware data augmentation method for node classification task. This work empirically indicates out that the learned node representations will be influenced by the neighbor label distribution. The neighbor memorization seems to be similar with the definition of heterophily, where the target usually has different labels from the neighbors. The notations in this work are not easy to follow.
Reject; rating score: 3; rating score: 5; rating score: 8; This is an interesting paper, however, as it stands, its contribution is quite limited. It seems that the model itself is essentially a mixture of conditioned goal oriented learning and social learning. This is perfectly fine, but the authors consider a rather specific example and it is rather hard to generalize for it. In particular, the part about social learning and curriculum learning appear to be effective for the task considered in the paper, but this might not be the case in general. The reviewer would suggest the authors to consider either another set of tasks and/or a more in depth discussion of the theoretical foundations of the approach. The authors should also try to improve the presentation of the proposed solution by separating the discussion of the proposed solution and the application to their specific task. The authors consider a solution of a task (5 block manipulation) using "self directed"/goal conditioned learning and social learning.<|endoftext|>Generally, the extensive experimental results have demonstrated the effectiveness of proposed method. However, there are still a few concerns. It seems that it exists some transition probabilities that related with the semantic graph, which shouldn t be considered as KG. 2, It is not clear in A.3 that how to alleviate the attention bias in small steps. E.g.why the k 5 is the shortest path? Generally, this paper propose an innovative research question in teachable autotelic agents.<|endoftext|>•	The realization and implementation of the model seems to be solid and consists of a graph neural networks to represent the objects in the environment and their relations, and a knowledge graph to store previously seen semantic configurations. It presents experiments for exploring object manipulation with 5 different blocks, in which an agent discovers configurations based on its own intuitive exploration, as well as guidance for other configurations from which it can further explore. The mount of intervention by the social partner is varied (from 0% to 100%), showing that 2 20% are sufficient for significant improvement over exploration without social partner. The paper targets goal oriented reinforcement learning tasks with possible intervention of a virtual social partner assistant.
Reject; rating score: 1; rating score: 1; rating score: 1; Generally, I don t (and shouldn’t) reject a paper because of writing. Among hundreds of papers I have reviewed, it is the most distracting one. Even if there are significant technical contributions, I still cannot accept this paper because of the writing. However, the idea of applying a tree like structure to generalize neural networks is still interesting. I am sorry that I cannot complete the reading because of the writing.<|endoftext|>From what I understand, I really like the research idea and direction of the paper. * Apart from having a new architecture, I do not understand the exact problem setting of the paper: is it supposed to be multi modality? Multi task learning? I understood this eventually, but it created some confusion for me in the beginning. * I am not perfectly sure on the experiments. However, I still want to point out that the baselines are out of date. Overall, I like the goal and the ideas behind the paper.<|endoftext|>To be honest, it wasn t clear to me what were the actual contributions of the paper, and how the experiment results are backing up the claims made in this paper. For those reasons, I recommend rejecting this paper for ICLR. ## What I like about this paper  The idea of unifying different architectures (RNN, Convolution, MLP) into one big neural network organized as a graph tree. Dealing with multiple types of input modality is an important research direction. Several sentences are not well formatted. I would have liked to have the source code, to test it myself. I doubt there is no other work that can be related to this one. Otherwise, I d try to reduce their usage as much as possible.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper focuses on proposing the local calibration error (LCE) to span the gap between average and individual reliability, which measures the average reliability of a set of similar predictions, proposing a metric to measure calibration locally around a given prediction. The motivation of this work is very clear. I think you can refer to [1]. 3.Is the promising experimental result due to the superiority of the method, or is it an accidental result from a specific kernel function and feature map? 5.The difference between the confidence and the accuracy for one sample is not reasonable when using it in Eq.4.Since the accuracy is not a good metric for only one sample, although the authors propose using neighbors (what about the case the prediction of the current sample is wrong?) Other concerns:    1.<|endoftext|>Strengths:  This paper is generally well written and clear. The notion of local calibration seems novel to me. Further, it seems as though computing the proposed metric (LCE, MLCE) depends on 1) a specific feature map, 2) a specific method for further reducing dimensions, 3) a specific kernel function, and 4) specific settings of the hyperparameters for the kernel. Have the authors experimented with other kernels? Choosing a suitable $\gamma$ seems quite arbitrary. If I understand correctly, LCE is meant to measure ECE in localities over the input feature space.<|endoftext|>In general, the idea of evaluating a model on both the feature space and output space is quite interesting. On the negative side, one main concern with this paper is the ambiguity of the proposed definition of Local Calibration. On the one hand, we have an existing definition of a perfectly calibrated model, which is solely defined in terms of the probabilistic output space. Also, while the authors used NLL and BS for their experiments, they did not discuss much the framework of Proper Scoring Rules. However, the local calibration concept is not linked with the existing definition of calibrated / Bayes optimal classifiers, so it might not stand firm as a measure to optimise.<|endoftext|>The paper proposes a novel calibration metric, the local calibration error (LCE) that measures the average miscalibration degree of local predictions, which complements the existing global ECE metric. Strength: the paper is well written and the proposed LoRE method seems simple yet effective. But the connection is not clear to me. Furthermore, the authors should  provide some comments on the pros and cons of LCE over existing fariness related group wise calibration measures (ref i). As discussed above, my primary concerns for this paper is on the motivation of local calibration, as well as some possible improvement over the experimental section (such as comparison with existing approaches). I hope that those concerns can be addressed in the rebuttal process.<|endoftext|>Strengths:  local calibration is a sensible idea and it s good to see it worked out like this  local/individualized notions of calibration are an important problem  same thing for the proposed method: it s simple and seems like the thing you would try in this situation  the experiments show fairly consistently that this method provides gains in improving local calibration metricsWeaknesses:  it s not clear to me why we are filtering for prediction bin as well as weighting along input space   my intuition around what "local" calibration should mean would involve reliability among all similar inputs, rather than just those that also have similar outputs. I would be interested to see exploration of which regions are uncalibrated for modelsThis is a sensible idea (metric and method) applied to an important problem. The experiments show that the method works as intended, although they could benefit from some deeper analysis.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; rating score: 8; This paper proposes an approach to learn VAEs with binary latent variables. On the one hand, I found the paper to be well motivated at a high level as a way to avoid gradient bias and obtain sparsity; and the approach taken by the authors is quite original and deviates from most VAE works. On the other hand, the proposed method is much more computationally intensive than regular training of VAEs, and I also wonder if this might be a case of over complicating things for the sake of the idea sounding interesting. The paper is mostly well written, although I would appreciate some additional background on genetic/evolutionary optimization algorithms in the appendix, as I do not believe these are often in the area of expertise of the intended machine learning audience of this paper. Given that the cost is highly increased over the baselines, I believe it s particularly important for the authors to report running times. I will thus maintain my current score. This paper proposes an interesting and novel way of training VAEs with binary latents; although the method adds a lot of computational complexity for benefits which are not completely clear.<|endoftext|>The authors present a specific evolutionary algorithm for learning encoders for binary latent variables. The experimental section outlines some applications to denoising and inpainting. It is hard to predict how the proposed approach will behave in a more complex case (i.e., convolutional layers, recurrent layers, residual layers, etc.). To my knowledge, it is the first time I see an application of an evolutionary algorithm to learn binary encoders in VAEs. The experiments are promising. Overall, I find the paper interesting, however, it is not ready to be accepted to the conference. However, I miss a deeper comparison with existing methods (e.g., RELAX, Gumbel softmax). The Eq.3 suggests we deal with the Gaussian distribution. However, the comment on the MSE is a bit misleading because it is a well known fact that the likelihood function for the Gaussian distribution results in the MSE. In the Eq.5 (and before it), there is a new variable called “prior parameters” introduced. From the equation it seems they are the expected values of z’s. I miss a deeper comparison and discussion with other approaches, namely:      MCMC techniques for binary variables:        * Strens, M. (2003). Evolutionary MCMC sampling and optimization in discrete spaces. A., & Tomczak, J. M. (2021).<|endoftext|>This paper investigates VAEs with binary priors able to learn sparse latent code and studied how such codes can efficiently be learned. It also proposes a method that uses a direct discrete optimization of binary latent vectors. Pros: The theoretical derivation of this article is very clearCons:(1). This paper used discrete latents to solve present and absent problems of objects or edges. It would be better to prove the proposed method on a large scale dataset, such as ImageNet;(2). The advantages mentioned by this paper are: fewer algorithm elements, fewer hyperparameters and fewer model parameters. It would be better to carry out experiments to prove that images generated from this model has better performance while maintaining fewer model parameters.<|endoftext|>In this paper, a novel variational autoencoder (VAE) model with binary latent distribution is proposed. Compared to traditional VAE models, aside from the latent distribution here is Bernoulli (instead of Gaussian), the encoder part is an explicitly expressed model (instead of a DNN) and can be optimized directly, which makes the model to have a simpler form compared to previous DNN based sparse latent code learning methods. The model is numerically evaluated on (zero shot) denoising and inpainting tasks, and shows comparable or better performance than competitive methods. In this paper, the authors avoid using DNNs and directly model the encoder explicitly, whose optimization does not require gradients. This paper sheds light on a new direction of discrete latent distribution VAE models as it uses a direct model for the encoder part instead of DNN. The formula variational optimization for the direct model are novel and concrete. This is also validated in the experiments, which provides great advantages in some application scenarios. A minor concern is the scalability of the proposed method. The proposed method is novel and well presented.<|endoftext|>This allows TVAEs to be well suited for denoising and inpainting tasks. **Strenghts**I found the paper to be a very interesting read, since it presents a novel idea to train discrete VAEs, which is notoriously challenging. Discrete VAEs are commonly trained with continuous relaxations of latent variables such as using the Gumbel Softmax distribution. This implies that this method is able to avoid the amortization gap (but also that its training will be slower and less scalable). The core idea of the algorithm is based on a sample based approximation of a high dimensional integral in the ELBO, where samples are obtained with evolutionary strategies. 2.How diverse are the states at the end of the evolutionary loop? Due to the non amortization and the usage of EA, for scalability reasons this method can only be applied to a restricted task domain, that is however well defined and discussed in the paper. The presented idea is interesting and novel in the VAE setting, and could inspire new research in generative models with discrete latent variables. While the experiments show that the model performs well in inpainting and denoising tasks, I feel that to be more impactful this paper lacks a clearer analysis and discussion on what makes this model work. I d be happy to increase my score if the authors improved the paper in this direction.
Reject; rating score: 3; rating score: 3; rating score: 6; But the authors argue multiple times that the method proposed in this work is better than end to end generative model without enough justifiation. Thus I cannot say that this paper is above accept threshold at current stage. 2.Compared with other methods, the scalability of the proposed method is indeed strong. 5.The ability to sample novel scenes is impressive. While the author did a comprehensive literature review, I am not convinced by some of the claims. 2): "End to End object centric approaches are difficult to train." 3): "Full scene models trained in an end to end fashion are not shown to scale to a more realistic dataset yet." 2.DatasetThe dataset proposed is indeed interesting and challenging. This should be an easy modification. Similar modification can be done to SCALOR as well I think. The results provided in the paper is impressive.<|endoftext|>This work proposes an object centric generative model. The object model is trained to reconstruct an object as if it is not occluded. The authors have introduced a new dataset, called Fishbowl, which provides inmodal and amodal segmentation masks of objects. Object centric generative models have gained relatively less attentions compared to the normal generation models. However, I have many concerns that make me hesitant to accept this paper. FIrst of all, the dataset is far from the realistic setting. There are many datasets for amodal segmentation. There are many other amodal datasets, including KINS and COCO A. My second point is that the proposed model has not been evaluated well.<|endoftext|>The paper introduces an object centric generative model for visual scenes. To my understanding, these three components are trained independently. The paper has some weaknesses, e..g. some claims not thoroughly backed up, lack of clarity on some details. Is it mostly that in this submissions there is also control on scale and position? It would be interesting to see this method run on other datasets like the Atari dataset or the 3D room dataset from the SPACE paper, to further test generalisation of the multi stage approach in other domains   do ground truth segmentations exist to test there? It also has the potential to be practically useful, by allowing users more fine grained controls on the parameters of the generated scenes. All in all, I think the paper is above the acceptance threshold, and addressing these weaknesses would make me lean towards a clearer accept
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; rating score: 5; In this paper the authors present a method for deep anomaly detection. The first loss term is based on contrastive learning with an additional term. This needs to be introduced and explained. No anomalies at the points?<|endoftext|>·	The author propose a contrastive learning framework for sequential outlier detection. ·	Weakness:o	The acknowledgements in the paper implicitly reveal the identity of authors, which may against the submission policy. o	The novelty of this work is limited. More justification on this assumption may alleviate the concern. Also, many selections for the model seems arbitrary, which needs more justifications to support the motivation; and the dataset selection seems conflict to the model assumption.<|endoftext|>The paper would be stronger if the authors could reveal sensitive hyperparameters to reduce the critical hyperparameters for tuning. The main idea is to exploit both the sequential relation between data and the normality of the data itself. If analysis on important design factors can be provided, the paper would be more thorough.<|endoftext|>Inspired by SimCLR, this paper proposes a simple neural network framework for detecting anomalies on sequential data.<|endoftext|>The author proposes a self supervised sequential anomaly detection network that optimizes a context adaptive objective using feature argumentation and contextual information. ### Strength1) The paper is mostly well written. Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 2018.[b] Su, Ya, et al."Robust anomaly detection for multivariate time series through stochastic recurrent neural network."
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; For example, whether the feature transformation obtained by DNN meets the definition of metric (or part of the definition), and whether the perspective of metric embedding can bring new inspiration to the theory of DNNs. 2.The metric learning theory in this paper basically comes from the generalization theory of neural networks [Bartlett et al.(2017)].Compared with the previous theoretical results, the metric perspective analysis proposed in this paper does not give better results. In a word, I think this paper does not provide a new theoretical guarantee for nonlinear metric learning, and its results are basically the same as the existing generalization theory of DNNs.<|endoftext|>The authors consider the setting of metric learning. eq (5): what does the "O with a line on top" mean? (I presume you mean the norm of the features, but please be explicit.) I think the paper would benefit significantly from being reorganised. Could you move the citations to the first time you mention the datasets? Paper is not fit for publication in its current form: it should be structured more coherently and the results discussed in more context. What exactly is the difference between those bounds and the ones in this paper? The bounds from Bartlett et al 2017 are mentioned multiple times.<|endoftext|>The paper provides two new generalization bounds for non linear metric learning with deep neural networks, by extending results of Bartlett et al.2017 to the metric learning setting. More thorough experiments say with more datasets and degrees of regularization can potentially give interesting insights. Overall the paper gives interesting generalization bounds for the metric learning setting but does not seem ready for publication.<|endoftext|>However, my opinion on this can be changed. **Strength**The authors show that for the metric learning setting the bounds from Bartlett, Foster, and Telgarsky can be improved by checking if the last layer of the network is dense. The authors show that their bound matches known bounds for the linear case. This is my main concern. I would have liked more discussion on how the bounds translate to generalization error bounds? **Questions**1) I am not sure how the bounds are dimension free. Overall the paper presents a nice adaptation of the bounds from Bartlett, Foster, and Telgarsky for the problem of metric learning.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The net itself seems to be C defined in the next sentence? For example, [1] below, but there are many more recent papers (see citations to this paper). It would be interesting to investigate this possibility here. As far as I see the results are consistent with the literature and I do not have any particular concerns. The paper executes about the first thing you would try. This can be a strength and a weakness. The paper could be made stronger by investigating any or all of the directions suggested in (1) (4) above. Anyway, someone more expert than me should probably comment on the novelty. * P5.J is introduced abruptly and before it has really been defined.<|endoftext|>The general results can be applied to the specific settings of GP Bandits and are shown to yield competitive regret guarantees for the Matern kernel. This does not reflect from the contribution section of the paper as well. Finally, the experimental evaluation section of the paper is extremely weak, there have been no comparisons made with state of the art methods, even the algorithms of Kernelized MAB as listed in Table 1 which is surprising. A separate problem formulation and technical contributions section would also be helpful for the readers. The theoretical findings of the paper are sound but it is hard to appreciate the novelties of this work over the existing techniques and analysis of GP Bandits.<|endoftext|>This paper is motivated by learning optimal actions in tasks where both the reward function, and policy (actions) are nonparametric. The challenge is well motivated in the introductory sections and the theoretical results show the proposed approach has strong performance. I have not been able to fully check all of the mathematical work in the appendices, but what I have inspected seems to be accurate and non trivial. While I am assigning a positive score, I think the exposition around the improvement over UCB style algorithms could be improved. This doesn’t feel as strong as it could be – could you supplement this with some more details as to what features make the difference?<|endoftext|>Overall I feel this is an interesting work and well written. I have some specific comments below. (2) is essentially the simple regret? Maybe the authors are from other communities but I hope you could relate or comment with bandits language in Section 2. The regret bound in this paper is clearly sub optimal when reducing to GP bandits. The bound you compare with is not sharp. Their model is a strictly sub class of your model I think. 4.In Section 5, do you require the space of input points to be the full unit ball? If it appears in the title, I feel you should explain what do you mean by optimal design explicitly.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper introduces "ExMix," a collection of 107 pre existing NLP datasets across 8 identified task families. ExMix is proposed as a pre training dataset, to be mixed with the typical self supervised pre training prevalent in NLP. They find that ExT5 largely outperforms T5 on the majority of downstream evaluated tasks, including tasks whose training data was included in ExMix as well as tasks which were not seen during pre training. Finally, the authors perform some additional analysis of the training decisions made around ExT5. Pros* The scale of the experiments is very impressive and the results are very strong   ExT5 is generally a much stronger model than it s T5 counterpart. * From a resource perspective, both the pre trained ExT5 model and the ExMix collection would be valuable to the NLP research community. * The authors take a principled approach to selecting a subset of tasks and demonstrate that it is generally not successful over increasing the number of tasks, highlighting the importance of the number of tasks. Cons* The methodological novelty of the paper is fairly low. ExMix is a collection of pre existing datasets and using multi task signals during pre training is not new either. The scientific value of this work predominantly comes from the scale of their multi task setting, and their analyses.<|endoftext|>They also perform thorough experiments on a variety of intersting research questions. Finally they build a large pre trained model ExT5 on which they perform really extensive evaluations and the demonstrate that the multi task pretraining clearly helps and improves on a strong baseline across many tasks, including tasks which have not been included in the ExMix training data like translation. strengths  Well written and convincing  The variety and size of their corpus ExMix  The number of interesting research questions explored  The improved efficiency of their approach as compared with the baseline methods  They evaluate on a large number of varied text suites (SuperGLUE, GEM, Rainbow CommonSense, CBQA) and compare against a strong baseline T5 and mostly perform better. This paper pushes forward work on large scale multi task transfer learning, with a huge number and variety of tasks (107 for training) with many test suits and research questions explored. It is well written and convincing and I would like to see it published in ICLR.<|endoftext|>This paper revisits the idea of multi task learning for natural language processing (NLP) and scales it up to 107 supervised NLP tasks as EXMIX (Extreme Mixture) across diverse domains and task families. It extensively analyzes the co training effect between the different families of NLP tasks and proposes a model pre trained using a multi task objective of self supervised span denoising and supervised EXMIX named EXT5. Or is that specifically tailored to SuperGLUE as mentioned in the paper. The significance of the results on different task/task families can be further validated by reporting std/mean results with multiple seeds as in (Section 2.5)   The zero shot performance might also be a point to add given the multi task prompting / finetuning seems to be helpful both for GPT families and T5 families [1, 2]. [1] Wei, Jason, et al."Finetuned language models are zero shot learners." [4] Subramanian, Sandeep, et al."Learning general purpose distributed sentence representations via large scale multi task learning."<|endoftext|>The paper presents an analysis of multi task learning with the T5 model at a large scale. The tasks are formatted as text to text tasks and trained alongside the span denoising objective on C4, same as the multi task strategy used in the original T5 paper. In addition, there are some ablations on the number of tasks, the ratio of C4 vs multi task, and comparing multi task with pre finetuning. if it is some statistic like standard deviation across random seeds, why is this not present at 0 and 107. **Cons**  While the results improve performance over downstream benchmarks, I didn’t learn anything new from the paper. The original models also include downstream supervised tasks in their pre training, just like this work. The task scaling experiment (2.5) seems to be core to the contribution of the paper. Often the evaluation is in terms of “average” super glue performance. The main point of difference that I see from the rebuttal is of using multiple seeds in Fig 3.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; Given a visual representation of an Angry Birds screenshot, they train an LSTM to model the natural language guides. This paper is clearly not ready for publication. The only strength of the paper is that the task of learning human guides from visual inputs is potentially interesting. The evaluation should be much more thorough than a single game. The paper also doesn t even seem complete. The evaluation of this paper is extremely limited and uncompelling.<|endoftext|>The paper proposes to use DL to predict the optimal strategy (in words) for angry birds levels. The topic is interesting, but I m not sure if the contribution is novel enough or if the model/experiment achieve the given goal. There are only a small set of possible objects in the screenshot and in the strategy description. However, predicting the optimal strategy description from the screenshot does not seem to serve as a planning task (or a task that involves a sequence of actions).<|endoftext|>The paper specifically addresses the game angry birds and reports good results for mapping image embeddings to word embeddings. Paper does not clearly formalize the problem, misses to draw concise relationships to established, relevant prior works, and does not report the empirical results in an interpretable form.<|endoftext|>The paper aims to study the training of Deep Learning (DL) models from examples and to this end trains a DL agent to predict actions given screenshots of the game "Angry Birds" and observe that the trained model is able to predict actions for test samples and concludes that possibly DL expert systems can learn only by examples. [Eplainability]  I was expecting more examples of the outputs of the model. One of the goals of the paper is to study whether the instructions output by the model sounds reasonable to a human expert but this is not well discussed in the paper. This sounds very vague to me.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The model is validated by studying the scaling properties of the models. Empirical results are very limited. Cross entropy scores are limited because language modeling or a masked language modeling does not reflect how good the model can be at tasks that requires pairwise information. Lacks a comparison with stronger baselines of efficient transformers. Lacks ablation study on the selection of window size and order in the LMU. The paper is weak in baseline. Plenty of related work (such as gMLP [4]) have been demonstrated useful in the pretraining task, but they fail to outperform transformer in the down stream tasks with finetuning.<|endoftext|>The key idea is similar to convolutions, but the weights of the convolution layer were static (non trainable) and produced a representation of the sequence that can also be represented as a RNN. The authors compare the proposed LMU against transformers on a language modeling task with the Webtext 2 dataset. The paper presents an incomplete comparison that plays to the strength of the LMU architecture by comparing only parameter matched models. Questions or comments:1. References:[1] Xception: Deep Learning With Depthwise Separable Convolutions, Chollet et al.This work proposes a very interesting application of a novel, parameter efficient architecture on a large scale language modeling task. However, the paper presents an incomplete comparison that plays to the strength of the LMU architecture by comparing only parameter matched models.<|endoftext|>This paper aims to address two issues with standard transformers for language modeling, namely their large data requirements and their computational cost due to self attention complexity. Having additional ways to improve data efficiency by changing the model design is definitely of interest. * Using the Legendre Memory Unit to substitute self attention in transformers is interesting and has several potential merits: it can reduce the complexity and does not increase the size of the layer. Weaknesses* Something that stands out is the lack of discussion and comparison to related works that employ recurrent formulations of attention. * Even though this paper proposes a new efficient transformer, the evaluation does not focus on computational efficiency aspects and comes across as incomplete. It would be great if the authors discuss this or provide some supporting evidence about its correctness. The framing and the delivery, however, are greatly lacking. For these reasons, I think that the paper is not ready yet because it lacks sufficient evidence for the main claims and requires additional experimental efforts to better demonstrate the effectiveness of the proposed method.<|endoftext|>This paper proposes an efficient approximation of the transformer model. This projection is fixed and not learned, which allows for memory savings. First, the paper focuses on the main idea of using LMU, but the proposed model contains many different components, that are orthogonal to it. Further, the FFN layer after the input was found to be useful by the authors. The paper presents a very interesting idea with strong empirical results on language modeling, that could inspire a new generation of transformers.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; How long can trajectories be? 2) In Algorithm 1, specify what is input and output of the algorithm3) In Algorithm 2, it is a bit confusing that  w  is used as the argument of the exponential, which is different from Eq.(17), where it is used as the exponential itself4) In Theorem 2, Condition 1 is only defined in the Appendix. Could the authors state at least the order of magnitude of how much time the proposed method takes compared to the baselines? PMLR.The paper is overall good. The method is based on taking the established framework of inference as control via Schrödinger bridge problem and utilizing NNs as a parameterization for the policy. After rebuttalI thank the authors for addressing my comments. I trust that the authors will include the promised changes and additions regarding the training time and the comparison to ParVI into the final version of the paper.<|endoftext|>This paper shows that sampling can be treated as a stochasticoptimal control program and introduces a novel sampling algorithmthat works by simulating a stochastic differential equation underoptimal control with appropriately chosen dynamics and cost function. The work ishighly novel and one that would be greatly valued by thecommunity. The proofs are clear and easy to follow as well. The experiments are rigorous and persuasive. There is a rich literature offlow models that is referenced in the paper but aside fromSNF not really compared against. This paper makes a solid contribution with the strengths andlimitations of the approach very clearly articulated.<|endoftext|>The paper presents a control approach to sampling. Algorithm 1 and Algorithm 2 could be similarly simplified. I find the idea proposed in this paper interesting but I also think that a significant part of the paper was spent discussing unnecessary material and that important details were omitted. Similarly, I was somewhat frustrated by the lack of details on the gradient computation. 10 temperatures as a default for SMC is certainly not standard practice. Similarly please simplify the presentation of Algorithms 1 and 2. This should be better motivated. Section F.1 didn t address my concerns and has to be rewritten/completed. Post rebuttal comment: The authors didn t submit a revised version of their manuscript on time so I revised my score accordingly. Two points should be addressed:  I believe that it would be good to give much more details about neural network architectures, initialization, gradient computation etc. "Our approach avoid long mixing time theoretically and is more efficient": this claim is not justified and should be removed from the paper.<|endoftext|>The authors propose a Path Integral Sampler based on Schrodinger bridge. The idea of using stochastic control for sampling is very interesting and the empirical performance seems improved. I have the below questions. 1.Compared with previous MCMC methods, one benefit claimed in the paper is that PIS can generate samples with fewer steps. It is not clear to me why this is the case. Besides, what does “until converged” mean specifically in Algorithm 1? Should the sampler be closer to the target distribution as the number of SDE time discretization steps increases? There seems no discussion about how to choose the neural networks. Surely the evaluation metric of PIS seems better, but it will be much helpful to provide an explanation on why it is the case. This kind of interpretation of empirical results sheds more light on when and how to use the proposed method for users. A discussion with this paper would be helpful.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper proposes a scalable ensemble training with random gated block to enhance the model adversarial robustness. 2.The random gated method looks very similar to applying dropout not only during the model training phase but also the model inference phase. It would be expected that some similar theoretical discussions like the following paper can be included in the draft. In international conference on machine learning, pp. The model capacity can be a big concern and the technical depth is somehow limited.<|endoftext|>This paper uses ensemble learning to improve the adversarial robustness. The authors introduce the concept of ``Ensemble in One",  in which a simple but effective method called random gated network (RGN) is ultilized to enlarge the ensemble size in a very effective manner. 2.Strong empirical results when compared with SOTAs. Instead of just presenting the numerical results, the authors could try to provide some theoretical analysis on the proposed methods. The concept of the random gated network is similar to the work of stochastic path network (``Deep Networks with Stochastic Depth"). Why not consider the case where we just aggregrating the results, possibly by average  pooling, from different paths at each layer ? The idea is quite interesting and the results are good. However, the current version lacks the theoretical analysis and other key empirical comparisons as mentioned above.<|endoftext|>This paper proposes a robust training and defending method RGN by applying control gates with binary status. During the training, the proposed method generates adversarial examples in a clean label attack manner and mitigates the adversarial perturbation through training on another path. During the inference, RGN finds a subnetwork to defend against adversarial attacks. But how can the method guarantee that (a) the memorizations will not be rewritten by other perturbations? And what is the benefit of using such a method compared with forcing the neural network to remember all perturbations? In fact, Equation 2 and its variants can be used to generate clean label attacks. However, its application here is not for defending such an attack. I don t understand why the authors choose to use equation 2 instead of using the conventional non targeted adversarial attack. Does the robustness come from its complicated training process, or does it come from the distillation?<|endoftext|>This paper proposed a new way to generate an ensemble of networks against adversarial attacks. Different from other methods, which train different sub models, the proposed method repeats convolution layers multiple times and controls them with random gates. As shown in Figure 5&6, the AT can easily beat EIO as perturbation strength increase from 0.01 under the White box setting. Considering the AT only takes less than half the training time of EIO (Figure 7), it makes EIO less attractive. It seems that EIO can be easily combined with AT together. 2.Lack of interpretation of results. 3.The major contribution of the paper is to construct ensemble by repeating the original layer by n times. However, the increase of this key parameter n (from 2 to 3) will lead to performance degradation (Figure 3), which does not match the intuition of the paper. Given enough training time, will the model trained with n 3 beat the model trained with n 2? Although this method can beat other ensemble methods, it is less attractive than basic adversarial training since the latter takes less training time and is flexible to be tuned with different perturbation strengths. If my concerns are addressed, I would like to raise the score.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; If you train for enough epochs, the model would eventually reach the original accuracy. To this end, the authors propose to utilize non transferable learning to achieve both the goal of ownership verification and usage authorization. The computing time of the MMDs during each time step is at least twice your training time? I support the acceptance of this paper for a better ICLR conference.<|endoftext|>Most of the paper is really nicely written and is pretty easy to follow. Basically, the authors design a clever technique for learning nuisance dependent representations.<|endoftext|>In the era of deep learning, pre trained models have been regarded as intellectual properties of AI companies. Thus, protecting these models has been more and more important. This approach provides effective solutions to both model verification and authorization. + This paper is easy to follow. It is better to use \sP_X to represent the distribution corresponding to a random variable X.
Reject; rating score: 3; rating score: 6; rating score: 8; A shift tolerant perceptual similarity metric is proposed based on LPIPS. Strengths:The elements of shift tolerant metrics in the context of convolution neural networks are well analyzed and validated. Dated back to the era of wavelets, there were researchers, trying to come up with shift invariant wavelets. The reviewer is reluctant to directly point out those references but highly encourages the authors to dig them out. 2.Eq.(2) is not a good measure of shift invariance in the context of IQA. 4.3 pixels are not sufficient to test the robustness of IQA models to mild shift. 5.The authors may contrast their algorithms more to DISTS than LPIPS, and may even build their models on top of DISTS for quality prediction, texture performance, and perceptual optimization reasons. 7.Is the feature mapping of the proposed metric injective (or bijective)?<|endoftext|>The paper proposes a few modifications to the existing archiectures for perceptual image similarity that would be robust to the tiny shifts in the image. Finally the authors conclude that using anti aliasing strided convolutions and pooling operators and reducing stride size are helpful to make a learned similarity metric shift invariant. 2) The novelty of the paper is limited by minor alternations to the exiting architecture. The authors point our several times "PIM achieves shift robustness by training on neighboring video frames that often have small shifts. We work on an orthogonal solution by investigating neural network elements to make the learned metric robust and therefore only train our metrics on the examples without any shift through data augmentation." , this requires a more elaborative discussion on why we would like to choose this second route (which is also not optimal for a single pixel compared to PIM, as evedient from Tabls 1 and 3) There is no discussion of the effect of these additions on the runtime and comparison of the runtime with PIM. Table 2 shows that when evaluating bigger images the suggest approach is better than PIM, but the authors do not offer explanation why their approach works better4) Many experiments yield somewhat not intuitive results, like the padding influence on performance in Table 5. The explanations offered are not very clear.<|endoftext|>The authors propose to make perceptual similarity metrics (PSM) invariant to small shifts (few pixels translation) and still consistent with human judgement. To this end, they use an approach based on network architectures to evaluate which elements (anti aliasing, pooling, striding, padding, skip connection) can achieve shift invariance. The paper have multiple contributions:1. A study on the human perception of small shift estimating their sensitivity2. A systematic study of neural network architecture elements in relation to shift invariance4. A updated/shift invariant version of the LPIPS metric5. An ablation study to evaluate which elements contribute or to shift invariance### Strengths* Well written and easy to follow/understand* Improving architecture to achieve robustness is a better approach than data augmentation which relies on a longer training (energy consumption)* Fine analysis of neural network elements (their analysis is useful beyond PSMs)* Human data### Weaknesses* No comparison of their psychometric data and proposed PSM* (minor) figure and table fonts should be similar to main text font### Detailed commentsI understand that this is not the authors  main goal, however, their psychometric data could be exploited more to enforce the consistency of PSM with human judgement. This a well conducted and strong paper which achieve is goal and give interesting insight about neural network architecture elements.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The authors propose COMLN   a gradient based meta learning algorithm with continuous time inner loop adaptation. I think the novel derivation of memory efficient forward mode differentiation is a significant contribution in the few shot learning context. Also, by using a continuous formulation for the inner optimization, the authors are able to compute meta gradient w.r.t.horizon T and adapt that as well.<|endoftext|>In this paper, the authors propose a continuous time formulation for the inner loop adaptation in a MAML like setup. The authors show that this works well for a variety of meta learning problems. The use of continuous time formulation for the inner loop adaptation step, while also being able to learn the length of time this adaptation should be simulated is an approach that promises to be quite powerful. The discussion of the related work seems complete to my knowledge. The one major weakness of this approach (specifically the efficient forward mode differentiation method) seems to be that it cannot be used for non cross entropy losses. Overall a very strong paper with novel and significant contributions without major flaws.<|endoftext|>This paper proposes using a continuous time formulation along with forward mode differentiation to perform meta learning. The proposed method relies on this setup in order to efficiently compute derivatives, which the authors note have a low rank structure that can be exploited. Pros:    Interesting use of forward mode differentiation and structure of the gradients to compute meta gradients efficiently. (ii) How stable is the optimization? However, in the continuous time version, changing T changes the amount of computation. The approach is interesting; the main novelty is being able to train T, kind of like a learning early stopping.<|endoftext|>In COMLN, the base level loop is modelled as a continuous time autonomous ODE that is the gradient vector field of the inner loss. The authors restrict their discussion to a meta learning setting where the only base level parameters are the parameters of a linear classifier. I hope that these comments will be included in the final version. In my view, the main bottleneck is runtime rather than memory, since forward mode differentiation, not required to store the intermediate states, can always be implemented in a way that is efficient in memory. The paper is well crafted and the method presented is an interesting addition to the few shot meta learning literature, with possibly some limitations that are not well discussed. 2.In my view, one of the most interesting novelties of this work is the possibility of treating the adaptation time as a meta parameter. I believe that the paper in its current stage misses on the opportunity of exploiting this. The loss $\mathcal{L}$ is used with different inputs: cf (2) and (8).
Reject; rating score: 3; rating score: 5; rating score: 5; This paper presents an exciting approach to improving the prosodic diversity of generated speech; however, more work is needed before bringing this research to the publication stage. Another critical area for improvement is writing. Presenting the baseline TTS system and its prosodic components before DPP allows a more integrated presentation of the DPP section motivated from the problem of interest rather than done abstractly. The authors need to unify the symbols used in the DPP sections and TTS sections to enable the reader to understand the proposed approach.<|endoftext|>The paper addresses the challenge of synthesizing diverse prosody in text to speech systems. One of these is probably incorrect, unless I have completely misunderstood this metric. The authors propose a prosody diversifying module (PDM) which is based on conditional determinantal point processes (DPP). The work is well motivated and novel. They compare the diversity as well as the naturalness of the samples. The method also achieves diverse prosody which is clear in the samples provided and the MOS scores. The proposed method involving the PDM and DPP kernel is a novel application of existing methods to the task of speech synthesis.<|endoftext|>The goal of this paper is to build a text to speech model that can generate diverse prosodic features (in particular pitch and duration patterns) without sacrificing the audio quality. The authors should have compared with a baseline of their system that does not use the DDP module. The idea of formulating prosody generation as a determinantal point process (DPP) is novel to the best of my knowledge. This is clearly an overstatement in terms of speech naturalness   the proposed model is 0.84 behind VITS on MOS score, which apparently are not of the same level of naturalness.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper proposes a method for learning convolutional filters with trainable size, that builds on top of multiplicative filter networks. __Contributions.__ In my opinion, the authors make overstatements about their contributions.<|endoftext|>This paper presents a novel convolutional operation named FlexConv, to produce high bandwidth convolutional kernels with learnable kernel size at a fixed parameter cost. The author even discussed different parametric kernel functions in that thesis. Any explanation on this? The novelty of the paper is incremental and further explanation and experiment are required to demonstrate its effectiveness.<|endoftext|>This paper proposed a new convolutional operation with varying sized kernels. The experiment on several sequential datasets showed that the classification performances from both TCN and ResNet based on FlexConv outperform that of state of the art networks. 2.Flexconv operation is robust to fit high frequency signals in large kernels by MagNets. Cons.1.“Large kernels” and “small kernels” are too obscure. However, their method demonstrated state of the art performances compared with recent approaches.<|endoftext|>The FlexNet obtains state of the art across several sequential datasets, and matchs recent works with learnable kernel size on CIFAR 10 with less compute. [1] Dai, Jifeng, et al."Deformable convolutional networks." Aside from ablation study in the settings of FlexNet models, the authors are encouraged to provide more evidence such that the community is willing to discard common practices and adopt your methods on some occasions. This is a very interesting study, the method and theoretical analysis seem sold, but the motivation and comparative experiments should be improved.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper proposes an interactive learning algorithm for image processing tasks based on structural explanations. The results show that the proposed approach generates better results after human corrections and can be used for novel classes. The evaluation results are interesting and promising. However, a lot of useful information are buried in the end of the suppl. Similarly, it is vital to motivate why the evaluation suffices for the paper claims, which is also not part of the main paper.<|endoftext|>The paper proposes a technique for allowing humans to interact with NNs. Strengths:   The idea is an interesting one, the paper does a good job of building on prior work, and the methodology seems sound. The results show that this method is useful in fairly low data regimes. Weaknesses:   Overall, the experiments don t do a great job of higlighting how the method could be useful:	   While low data regimes may be interesting, all of the ones studied in this paper are "artificial" in some sense:  4.1.1  > sub samples ImageNet, 4.1.2  > sub samples (in a deliberately biased way) iLAB 20M, 4.2:  uses synthetic data.<|endoftext|>This paper extended VRX (Ge et al.2021) with an interface to allow human modify the class specific structural concept graphs (c SCG) as well as a procedure to distill the human changes in the c SCGs back to original task s neural networks. This is an interesting piloting idea. The fidelity of the SCG lacks theoretical guarantee or at least lack systematic after math accounting means. 2.Similarly, there is a need for high fidelity accounting  on human to nn path regarding whether the distilled NNs truly reflect the  human s modification on the reasoning of NN explanation. 3.The experiments are conducted at the scale of 200 images and 20 classes. 2.Page 8, how "deleting" all edges compares to the original modified c SCG?<|endoftext|>The paper provides a way for explaining the reasoning of a neural network to humans in the form of a class specific structural concept graph (c SCG). The c SCG can be modified by humans. The modified c SCG can be incorporated in training a new student model. Experiments show that the new model performs better on classes that their corresponding c SCG have been modified.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This work proposes a novel backbone network for hyperspectral image reconstruction for snapshot compressive imaging (CASSI). Weaknesses:  The main idea of the proposed method is to use skip connections between blocks and groups of blocks.<|endoftext|>It is not related to the network model. This paper introduces a simple but effective and efficient method for hyperspectral image reconstruction. However, the novelty and significance of the contribution of the paper are not high.<|endoftext|>This paper proposes a new backbone for hyperspectral image reconstruction for CASSI system. About analysis and motivation. I have concerns about the motivation of network design, together with the position of the proposed network.<|endoftext|>Different from the previous works about Unet structure for HSI reconstruction, this paper introduces a new backbone, which is efficient and lightweight. The authors proposed three kinds of variants on the bisis of the proposed backbone. In fact, the authors proved in the experimental section that the proposed architecture can achieve better performance compared to Unet.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; Unlike NPs, the function space view of Gaussian processes model the function distribution directly as a Gaussian using mean and covariance functions. However, in the proposed model, I can t see how this could not be violated. * The description of Neural Processes in the paper is very confusing and gave me the feeling that the nature of the model is mis interpreted. It is unclear from Figure 2 how the attention module works (which is the main contribution of the work). This might be related to the fact that the proposed model may not satisfy the consistency requirements of stochastic processes. However, is it also fair to say their construction ensures the model is a valid stochastic process but yours doesn t?<|endoftext|>This paper proposes Decoupled Kernel Neural Processes (DKNPs), a new neural stochastic process, which improves uncertainty estimation by learning a separate mean and kernel function to directly model the covariance between output variables in a data driven manner. Weaknesses:(1) The novelty is limited. The motivation is interesting, but the model somewhat lacks novelty.<|endoftext|>The paper proposes Decoupled Kernel Neural Processes (DKNPs), which is a variant of neural processes first proposed a few years ago. For example, in the abstract, "we introduce a new neural stochastic processes" should be "we introduce a new class of neural stochastic processes", and just above the start of Section 2, it says "a neural stochastic processes that explicitly learn..." which should also be corrected similarly. Perhaps it is because of my lack of background on neural processes, but from reading this paper and from a brief scan of the literature, it seems that the major leap forward for neural processes was made by [Kim et al., 2019] with the use of attention. The main contribution is to decouple the process of learning the mean and the covariance functions separately.<|endoftext|>The authors proposed a novel extension of the neural process by explicitly and separately modeling the mean and the covariance of output variables via multi head attention. It would be helpful to add some computational analysis. There is no corresponding explanation of figure 1 in the main text. 4.In the original ANP paper, they showed examples of the image completion task with a reasonable diversity. The connection between DKNP and GP is interesting.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper proposes a graph convolution based multiscale spherical deep neural network. Experiments on point cloud classification under arbitrary SO(3) transformations as well as predicting electronic state density of graphene allotropes, demonstrate the validity of the devised method. Is this only hypothetical or does it also happen with real point clouds? The spheres seem to have more structure than a simple graph. Representation  is a fundamental aspect in 3D analysis. I would suggest the paper to be more careful or conscious with the use of the word  volumetric . I feel that some kind of a theoretical analysis on the minimum number of concentric spheres required for rich feature extraction would be a nice to have. I might also be wrong, but these points could be better described in the paper. The related work does not really discriminate the proposed approach from the others so as to position this method among the state of the art. Simple idea, clear writing, but lacks thorough discussions and evaluations.<|endoftext|>This paper proposed a concentric spherical representation of 3D space, formed by nesting spatially sampled spheres resulting from the highly regular icosahedral discretization. The proposed convolutions are rotationally equivariant, and also scale (near) linearly with grid resolution. It shows some experiments in 3D object classification, and resolving electronic structure of atomistic systems. The paper overall is technical sound with some interesting results. The application in atomistic systems is interesting and seems new at least to the reviewer for the 3D point clouds. It seems to me the authors might not be very familiar with the literature in this area. I would like to see the paper has side by side comparisons with some of these leading papers in some of the standard benchmark datasets.<|endoftext|>The paper presents a concentric sphere representation and icosahedron based spherical CNNs for 3D point clouds. Weakness:  1. rotation equivariant: authors claims the proposed method to be rotation equivariant and results on ModelNet40 also support this claim. This is the most important property of the proposed method. It is advisable to have a better explanation of both proposed method and related method mentioned in related work, i.e.why some methods are not rotation equivariant. In other words, how does disabling loss(FDOS) hurt? Description of the method is clear and complete. Experiments include one for 3D point cloud classification and one for DOS estimation. For the latter one, I m not familiar with the experiment difficulty and details and thus could not evaluate the methods from the results.<|endoftext|>This paper addresses a challenging problem in 3D representation learning of point clouds, which aims to generalize the representations well to arbitrary orientations. The idea is interesting and reasonably well presented. In general, the proposed CSGNN is practical on spherical representation learning. However, there still remains some issues that should be addressed. 2.It is time consuming and redundant to adopt all the points to calculate the contribution to vertices, the reviewer is curious about how to select the effective points or how to avoid the redundant information when converting the point cloud to concentric spheres. It is not clear whether the method is robust. 5.Experimental results in some aspects are not state of the arts, and some failure cases should also be discussed if there exists. However, some technical details should be further discussed and analyzed.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The work  introduces a custom loss function for the molecular conformation similarity. The main contributions of the paper are as follow: authors proposed a novel loss function for conformation generation, that can be used in *straight forward* models, also provide Taylor Expansion and Multiplier Truncation techniques to speed up this metric. **Significance:** The novelty of the idea is high, but the significance is not proven. **Drawbacks / questions**:The claim of this work is that the new loss function is as accurate as best of its alternatives, and is faster to evaluate at the same time. This creates a bad impression.<|endoftext|>You may do not need to compare with it in experiments, but the technical connection should be clarified. The papers proposed a new objective function for comparing two 3D structures, in the molecular conformation context. I think this is due to this technical issue. The paper proposed an interesting idea for molecular conformation comparison.<|endoftext|>The authors proposed a new method for molecular conformation generation. In Table 2, the paper lacks a comparison with previous methods like ConfGF, CGCF, VAE based methods and so on, which makes me hard to evaluate the effectiveness of the method. 2.Although it is a new perspective to use D $\Phi$ $\Psi$ for molecular conformation generation, I did not see a clear evidence about why it is helpful and how much improvement over previous is brought.<|endoftext|>This paper proposes a new loss function for conformer generation tasks. Is this not batchable? I think it would make an outstanding workshop paper, or complement to some new conformer generative approach, but in and of itself it does not provide enough novelty and applied impact
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper proposes to combine Normalizing Flows with generative Diffusion Models in such a way that the target data is first non linearly transformed via the flow and then the distribution over latent embeddings is modeled with the diffusion model. The authors call their model *Parametrized Diffusion Model (PDM)*. The experiments show that the method leads to reduced variational gaps between the data likelihood and the evidence lower bound that is used for training the model, compared to several baselines. **Strengths**:  Combining a Normalizing Flow with a Diffusion Model as proposed in the paper has not been done before. **Weaknesses**:  It is mathematically elegant that we can formally relate the method to a non linear diffusion in data space via Ito s Lemma. But what is the practical value of that? The method is very incremental compared to LSGM [1]. It is essentially equivalent with the only difference being that LSGM uses a VAE framework with separate, non invertible encoder and decoder, while PDM uses an invertible neural network, so encoder and decoder are simply the inverse of each other. However, as mentioned, this advantage is not practically leveraged in any way, it seems, and the model is still trained separating the regular flow and the regular linear latent space diffusion. Related to that, I think the statement "LSGM cannot provide the diffused data learning at the middle of the process because LSGM does not induce the diffusion process on the data space due to the lack of VAE’s invertibility" is a bit misleading. This work should be cited and would also be an appropriate baseline. Hence, it would be more appropriate to also compare to *LSGM (NLL)* and *LSGM (balanced)* (Table 2 in [1]). Also the FID is significantly higher than recent works in the literature. Now, we additionally train the flow. What if we trained the other way around and first trained the flow to transform the data distribution into a Normal distribution (standard flow training), and then trained the diffusion model on the actually achieved embedding distribution? We may see different results here. I am not sure I am fully understanding the value of Theorem 2: If we assume that our score model, i.e.diffusion model, is flexible enough to cover any path measure, then why do we require a flow component in the first place? With this assumption, the diffusion model should be able to model the data distribution itself (diffused for all $t$) perfectly without requiring an additional flow component, I would think? In Section 3.2, the paper discusses the variational gap and refers to previous works on diffusion models. I think we are essentially assuming here that ODE and SDE based models are similar. I believe this subtlety should be highlighted.<|endoftext|>The core contribution of the paper is to show how a nonlinear diffusion model may be constructed by cascading a normalizing flow with a linear diffusion in the resulting latent domain. The paper comes with a strong theoretical flavour and the proposed approach is well grounded. This paper is set up in the emerging toping of generative diffusion models, that is gaining some important momentum. As highlighted by the authors, current applications of such diffusion models come with linear dynamics, since those were the only ones so far to allow for close form expressions for the required densities as a function of the time step. As a convenient way to circumvent this limitation, their solution is fundamentally quite simple: they propose to jointly train a normalizing flow from the data space, from which a classical linear diffusion is performed. The key point is that the method actually turns out to be equivalent to a nonlinear diffusion that would take place in the data domain, and they explicitly derive its dynamics by exploiting the Ito lemma and the fact that normalizing flows are invertible. As can be seen, I tend to think that this paper does have merits and that the study definitely may be inspiring to colleagues working on diffusion models. However, I must also say that all these remarkable developments come with an unacceptably low quality of english usage. Comments on the go:* "by maintaining the static linear diffusion": have not innovated on the topic of the diffusion mechanism that was always kept linear. ("destroy the evidence based justice system", come on...)very interesting paper but with a very questionable english usage<|endoftext|>The paper proposes parameterized diffusion model (PDM) that combines a normalizing flow on top of a linear diffusion model, effectively modeling a nonlinear diffusion. Empirically, such nonlinearity results in a tighter variational gap compared to the linear diffusion and other baselines. In particular, the variational gaps obtained using the proposed method are a lot lower than the alternatives. I find the exposition of the paper very hard to follow. Also, mention again how $\mathcal{L}$ is defined in Sec 3.2 where it is used. Throughout, the role of $\mathcal{L}$ is really mysterious to me. I found the connection a bit far fetched, if it s just regarding $q(x|z)$ and $p(z|x)$ are "forward" and "backward" directions respectively. In the last paragraph of Sec 3.2, I don t understand how the variational gap has to be strictly positive for diffusion models. How is this really different from the nonlinear case Eq(10)? Before Theorem 2, it claims "this variational gap converges to zero as we train ...". This does not seem correct. The paper proposes an interesting way to model nonlinear diffusion using normalizing flows, and presents convincing experiments results to support the main claims, although the clarity of the writing can be improved.<|endoftext|>The author proposed a framework that combines normalizing flow with diffusion models, allowing non linear diffusions to be used for inference and generation. Specifically, the author first discussed the similarity between VAE and diffusion models to motivate the importance of using non linear diffusion and the reducing variational gap. Then, the author proposed to decouple the non linear diffusion into an invertible transformation with the normalizing flow and typical linear diffusions on the transformed variables. A training objective based on variational lower bound was also proposed. Further, the author also showed that the variational gap between the lower bound and log likelihood can be reduced with the proposed non linear diffusions. The overall idea and relationship to VAE are also discussed at the very beginning, which helps the understanding of the proposed method. First, if I understand correctly, the structure of PDM is very similar to LSGM mentioned in the related work. The only difference is the replacement of the encoder and decoder with normalizing flows. From the experiment, it seems that LSGM produces lower FID scores compared to PDM. However, since the PDM produces better NLL, I guess it can produce better distribution coverage than typical linear diffusions. Thus, most of the image details are generated by linear diffusions in latent space. Why does normalizing flow not contribute to image details? At the beginning of section 4.2, I don t think all non linear diffusions can be decoupled into normalizing flow + linear diffusion. So why optimizing $\phi$ w.r.t.NELBO alone can reduce this gap? Improved precision and recall metric for assessing generative models. If the author successfully addresses the above concerns, I can improve my rating of the paper. I have read the revised version of the paper and the author s responses. It addressed most of my concerns. However, although I understand the difference between PDM and LSGM from a theoretical point of view, I think demonstrating it empirically is also important because the inter changeability between latent space and data space during inference is mainly due to the merit of normalizing flow, not the PDM framework. Since concatenating the flow and diffusion model is straightforward, the resulting model (from a structure point of view) has limited novelty. On the other hand, I think the argument of reducing the variational gap is interesting, and along with extensive empirical evaluations, I will raise my score to 6. But I still suggest the author to further argue about the novelty of PDM from both theoretical and empirical points of view.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 5; That fold could be anything at all. And trained on PDB structures of natural sequences. I think this is a fun idea, but too flawed. Also a misuse of AlphaFold the same way as in #1. AlphaFold is not convincing validation, because it is too similar to the oracle used for optimization. Unclear that the AlphaFold pLDDT score is meaningful for non natural sequences. The method and my problems with it:1. Are the predicted folds for final output the same as for seed sequences?<|endoftext|>In order to improve the paper the authors should1) Provide a more concrete definition of de novo design and more clearly state what the goals of this paper are and why trDesign does not achieve those goals. Most of the technical contribution seems to come from trDesign. The primary issue is that there is a lack of available training data. It is possible that the entire difference can be explained by using DMPfold2 rather than the structure prediction method in gcWGAN.<|endoftext|>This suggests that the developed method may be extremely overfit to this specific task/dataset. It should also be noted that UniRep is capable of generating sequences that are structured, which is shown in the paper. The number of sequence clusters (as a measure of the diversity among generated sequences) was shown for the three DARK iterations, but it is not clear how to interpret numbers of 1000, 973, and 962.<|endoftext|>The first of these two is worth trying but is not a conceptual contribution. The main idea of the paper is the second: self training to generate diversified  intermediate  initializations for further refinement. If the authors could please comment on this. The core methodological idea is worth exploring, and ultimately publishing, in the context of sampling protein sequences. But that s OK.<|endoftext|>The main contribution is a new iterative procedure to generate a dataset of increasing size in an unsupervised fashion. Overall, the paper is well written and easy to follow. The main ideas are clearly explained, the contributions and the empirical evaluation protocols and results are well presented. On the strenghts side, the presented method make use of strong state of the art components that are assembled to provide an end to end approach for de novo protein sequences and structures design.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper discusses the potential weaknesses in previous sample selection criteria in learning with noisy labels. And then propose a new selection criterion by incorporating the uncertainty of losses, together with theoretical justification. Experiments on both synthetic noisy balanced/imbalanced datasets and real world noisy datasets validate the effectiveness of the proposed approach. 2.The goal of this paper is well motivated, with a detailed discussion about the uncertainty of losses in sample selection. 3.The technical steps are clearly explained with theoretical justification. And the proposed method is clearly explained and theoretical analyses have been provided. I still tend to accept this paper and keep my score.<|endoftext|>This paper proposes a novel algorithm to improve the sample selection in the case of noisy labels training by incorporating the uncertainty of losses. To select samples, the authors propose to use the lower bounds of the confidence intervals derived from concentration inequalities instead of using point estimation of losses. The authors validate empirically their results on four benchmark datasets (MNIST, F MNIST, CIFAR 10, CIFAR 100) and use a diverse set of possible noise functions. Strengths  The proposed approach is justified theoretically  Many datasets and comparison methods used and thus the proposed approach is also validated empiricallyWeaknesses  I could not find a major weakness in the paper.<|endoftext|>This paper proposes a probabilistic based sample selection approach CNLCU to distinguish whether the training sample is mislabeled or ignored. The experimental results show that the proposed method outperforms most up to date sample selection methods. Learning with noisy labels is an important issue and has attracted much attention. CNLCU is based on the probabilistic constraint relaxation and can select ‘good’ samples with time intervals. Overall, the method proposed in the paper is technically sound. The comparison with well selected baselines in different noise levels shows the advantages of the proposed method. The evaluation matric as well as the analysis should be diverse. I also notice that on some datasets, the performance of CNLCU appears larger variances, which should be further explained. This paper assumes that the training losses in L_t conform to a Markov process. The position of W in parameter space depends on the starting points and search paths, but not merely the last positions.<|endoftext|>The paper proposes a novel sample selection method for learning with noisy labels (LNL). Based on the typical "small loss" assumption, the motivation is to consider uncertainty about large loss samples in order to distinguish the two confounding cases: truly mislabeled samples, or clean yet underrepresented samples that are less frequently selected or learned by the model so far. Concentration inequalities are used to obtain the final selection criteria to perform conservative search. Some of my major concerns are:1. To implement the proposed method, it seems that naively one will need to keep track of the history of losses of every sample in the training set. Can authors add some details of the space complexity overhead to use the proposed method? 3.The experiments mostly are designed with relatively low noise (20% and 40%). The proposed method is a novel method concerning an interesting yet under studied direction with a potentially significant impact to many applications (e.g., worst group generalization, fairness), which the paper does a good job brining attention to. Some concerns should be addressed during open discussion with authors.<|endoftext|>The paper performs a comprehensive analysis on a sub category of noisy label classification techniques. Following that, the paper proposed a new improved method to handle sample selection based noisy label classification techniques. Strength: (1) Provide comprehensive analysis on one category of noisy label classification method. (2) experiment dataset is diverse covering many commonly used datasets and setups. (2) A critical state of the art is not provided (see below). (2) Experiment results could be improved (need a t test proving the experiment results are significant). It discusses the weakness of some noisy label learning methods. MentorMix is also a sample selection based approach. It has demonstrated superior performance in dealing with the noisy labels. Because MentorMix uses mixup to enrich data, it should be robust against data imbalance. I would like to see a comparison of MentorMix and the proposed method.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper studies the directional bias of SGD in kernel regression. Consequently, the authors show that such directional bias of SGD can result in an estimator that is closer to the ground truth, which further leads to better generalization. However, my major concern is regarding the novelty of this paper, given the closely related prior work [Wu et al., 2021]. **2.** It is true that the SGD algorithm considered in this paper is different from the epoch wise SGD in [Wu et al., 2021], but it seems that the proof technique will not be that different as claimed by the authors. Given my concern about the novelty and contribution of this paper, I do not recommend acceptance.<|endoftext|>The authors link this feature to generalization using the properties of quadratics. The paper is interesting and deals with a very important issue: generalization in simple problems, and the effects of training with SGD compared to GD. However, I think the paper has a few drawbacks and deserves a few months of more work.<|endoftext|>This paper studies directional bias of SGD vs. GD in the setting of kernel regression. It is good to learn that the directional bias of SGD also works for kernel regression. # Cons:  Most of the proof techniques are from (Wu et al.2021).Given prior results for linear regression, an extension to kernel regression seems a bit incremental to justify a conference paper. I cannot recommend an acceptance to this paper as most of the results are more or less incremental given prior work.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 6; + Three three large scale OOD settings are presented, together with datasets for empirical evaluation+ A set of experiments is given on all three settings to provide baseline for future work+ A anomaly scoring method called MaxLogit, which is a minor variant of the popular method MSP, is introduced, and shows effective performance on the three settingsWeaknesses. For example, rather than what it is claimed in the paper about small scale multi class OOD detection, there have been increasing efforts put on large scale multi class settings, such as [1 2], but the paper fully ignores those closely related work; in terms of large scale anomaly segmentation settings, the datasets and tasks introduced in [3] clearly have more advantageous features (dataset scale, diversity of anomalies, realistic of the settings, etc.) than the proposed one in this work. + All the proposed datasets are created by some simple combination of existing benchmarks, wherein I cannot find any major contributions+ The presented method MaxLogit is a trivial variant of the popular method MSP, so no major technical novelties are presented+ The competing methods across all three settings are outdated models or simple baselines. Without comparison to recent, state of the art models, it is difficult to evaluate how the presented method advances the area. References\[1]  "Are out of distribution detection methods effective on large scale datasets?."<|endoftext|>The paper presents a collection of somewhat disjoint contributions to outlier detection. First, the authors propose Species   a novel OOD test dataset. Second, the authors propose to detect outliers according to the max logit criterion. The authors claim that max logit is especially suitable for OOD detection in multi label environments. StreetHazards is especially interesting since it allows proper rendering of introduced outliers. max logit has not been compared with its differentiable counterpart (log sum exp logit). Previous work shows that logsumexp logit can be interpreted as likelihood of the input [gratwohl00iclr] and that it can be used as a principled criterion for outlier detection [liu20nips]. [grcic21visapp] and the references within  Figure 5 should report AP and FPR95 (the official benchmark metrics) in order to allow comparison across the leaderboard; it would be a good idea to include these metrics into all other tables since they are also used in Segment Me If You Can. Thus many OOD pixels get very low OOD scores. Thus, Fishyscapes, Segment Me If You Can and Wilddash 2 present better opportunities for dense OOD detectionSuggestions  some details are missing or unclear: which Pascal VOC (2007 or 2012?), why does the training use a small batch size so that batchnorm has to be freezed, which batch size has been used, why does Table 2 show per category results.<|endoftext|>The paper presents the negative of the maximum unnormalized logit (MaxLogit) as an anomaly score for out of distribution (OOD) detection. The proposed metric shows promising results compared to the maximum softmax probability (MSP) in the proposed setup (in distribution ImageNet 1K and out distribution Places365). For the multi label experiment, the PASCAL VOC and MS COCO are in distribution and ImageNet 22K out distribution. The proposed MaxLogit works better than in this MSP too. + The proposed anomaly MaxLogit score (negative of the maximum unnormalized logit for an anomaly score) leads to good performance for large scale OOD. It s straightforward how is computed but a formal distribution is necessary since it composes a major contribution to the paper. This claim could be well supported by a corresponding analysis. It is important to include one more baseline for comparisons. Multi label OOD: There is already the setup with ImageNet 1k as out of distribution (A benchmark for anomaly segmentation, 2019). There is not a single standard evaluation on this setup although approaches from the small scale evaluation are used, e.g.MSP.Improvements:  "In contrast to medical anomaly segmentation and fault detection, we consider complex images from street scenes." Both contributions are valuable and can be considered novel too. Post rebuttal: The rebuttal showed that there are still changes to be performed in the paper. It would need further work before acceptance.<|endoftext|>This work explores out of distribution (ODD) detection in three large scale settings: multi class OOD detection, multi label OOD detection and anomaly segmentation. To facilitate large scale experiments, it introduces a novel species dataset and a road anomaly dataset for multi class OOD detection and anomaly segmentation respectively. In addition, this work establishes a new baseline via a simple detector based on the maximum logit in all the three large scale settings. This paper is well motivated, which aims to push out of distribution (ODD) detection from small scale settings to large scale and real world settings. 3.The proposed method (MaxLogit) shows good performance in all settings. This manuscript is not very well written, some contents of this manuscript are not clearly organized. It would be better to list contributions for three different settings, and introduce datasets and methods/findings more separately. More specific descriptions of the method would be better. This paper is well motivated and solid, but the writing needs to be improved.<|endoftext|>The authors extend the out of distribution (OOD) detection from not seen in small scale settings to large scale multiclass and multi label ones. Additionally, they propose a simple yet strong baseline for this practical problem. 3.The proposed MaxLogit is practical and the experiment results on large scale benchmarks demonstrate its consistent efficacy. I like the large scale benchmarks constructed in the paper. But the proposed solution (MaxLogit) is just a simple extension of MSP. I cannot even tell it is a new method even though it can get significantly better results. 3.The benchmarks are interesting, but the comparison baselines are not sufficient. But they need to study more SOTA baselines.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; Strengths:  The paper is well written and fairly easy to follow and their is sufficient detail in the supplementary material to suggest that reproducibility is not an issue. The simulation results and their presentation (both the discrete element method used as ground truth and the simulation of the boundary graph GNN) are well done. I found the title to be a bit misleading. While the paper agues for a dynamic modification of a GNN at its boundary for 3D simulations, the experiments and the particular instantiation appear to be almost entirely for granular flow. I do not doubt that the method has practical value, and clearly a lot of work went into organizing the experiments, conducting them, and putting together the results. Perhaps this paper would be a better fit to a simulation conference or one with a more engineering oriented focus. I m also puzzled by the choice of title   the choice "3D simulations" appears to be quite general, and it is not adequately supported by the paper s content (the focus on granular flows).<|endoftext|>This paper presents BGNN to dynamically model geometric boundaries for high quality 3D simulation, with the particular focus on the granular flow simulation. To this end, it is potentially of broad interest in the community. The construction of the BGNN largely follows the framework proposed by Sanchez Gonzalez et al.(ICML’20), which makes the technical novelty of the work a bit limited. And there is no comparison with previous methods such as Li et al., 2018, Ummenhofer et al., 2019, Sanchez Gonzalez et al., 2020 and Pfaff et al.,2020. In addition, there also exist some minor issues:1. The title of the paper is inappropriate as the term  3D simulation  is exremely general.<|endoftext|>The paper proposes BGNNs, which are an extension to existing GNNs for particle simulation (e.g.Li et al, Ummenhofer et al, Sanchez Gonzalez et al), aimed at improving the efficiency of the model when particles are near boundaries with complex triangular geometries. The main claims given the model are:C1. This approach to describe boundaries is more efficient than those approaches used in prior work. W3: Authors claim that the model can be faster than state of the art simulation methods. I believe this is probably true, but I feel it is not very well supported as it is a very small part of the paper. Other comments:O1: The description of the contribution could be a bit more clear. Once you have added those particles, just apply *almost* the same model as in the baselines on teh resulting graph. O2: Some of the math seems missing some descriptions and parts.<|endoftext|>This paper presents a novel contribution to the use of Graph Neural Networks (GNN) in the simulation of 3D physical phenomena. The paper is very interesting and well written. However, there are some parts of the paper that should be explained better to improve its understandability and reproducibility:1. 3.  mij appears in (4) and (9) but authors do not explain its meaning. It could be the message passing of Gilmer et al 2017. Please, this part must be explained better. Which information do the edges have? 6.A comparison with Sanchez Gonzalez 2020 would be interesting to show the limits of the improvement introduced.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 10; This work proposes a new framework to learn long horizon control policies on multi tool deformable object manipulation tasks. A differentiable physics simulator is combined with a trajectory optimizer to gather expert demonstrations of short term skills. ### Strengths  The paper is well written and easy to follow. ### Weaknesses  For the optimization problem in (2), shouldn t you maximize the cost function C(k,z) instead of minimizing it? Is the goal representation as an RGB D image practical for example for real robot applications?<|endoftext|>the results show that the proposed method is much better than both traditional RL and the trajectory optimization offered by differentiable physics alone. The paper addresses the challenging problem of long horizon RL by using differentiable physics and imitation learning. It is a inspirational method in terms of the way differentiable physics is coupled in large scale complex problems and the decoupling of different sub steps.<|endoftext|>This paper proposes to combine a local controller with a global planner for long horizon deformable object manipulation with different tools. All observations are embedded in a continuous latent space, on which the optimization is further performed. The authors find a clever way to circumvent the challenges. 4.What could be the possible limitation and future work of this method. This paper formulates a pipeline of applying differentiable physics to long horizon tasks.<|endoftext|>The paper provides a framework for learning how handle deformable objects. **Weaknesses:**  W.1 Maybe this is just me but the writing could be a bit more polished for accessibility. Also in the first sentence, calling elder care "deformable object manipulation" is... interesting. Honestly, that s all my criticism. The paper is pretty good. ICLR 2021.) UPDATED (2021 11 21): The authors answered my questions and updated a few sections of the paper.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper proposes SpaLoc a Neural Logic Machine variant that improves the model scalability on the graph completion tasks. These techniques do not pose significant changes to the original NLM model nor extend the model s capability in learning new classes of patterns. Furthermore, the techniques are specifically designed for the NLM, limiting the scope of this work. With that being said, I consider this work more or less incremental whose contribution is minor and limited. In summary, SpaLoc is an incremental variant of the NLM model.<|endoftext|>This is not obvious, nor is it supported theoretically or empirically in the paper. I thank the authors for the rebuttal and the additional ablation study and details on the hyperparameter tau. An ablation would help convince the reader. ## Additional Feedback and Questions  Much of the paper is written with the assumption that the reader is familiar with the NLM paper, making it less accessible to a broader audience. What are the hyperedge feature domains considered in this work?<|endoftext|>However, currently, the paper is very dense to read and needs to be improved. I would also like to see an experiment on inferring relations that exists between more than a pair of entities. Update 11/26 Thank you for the rebuttal. I am keeping my score to weak accept.<|endoftext|>2) One of the most important claims in the paper is efficiency. 3) the experimental results looked very strong, especially on the synthetic datasets, which solved a large amount of instances that could not be solved by other methods given a fixed amount of memory budgets. However, I would really like the authors to improve on the presentation of this paper. Specifically, the authors should improve on describing their methods and there is a lack of very important details in the experiments (even in the main part).<|endoftext|>On synthetic data, they show that the original NLM fails to scale on very large families and a GNN based baseline (R GCN) performs worse than SpaLoc. **Weaknesses:**My main concern is with the experiments in the paper. Please see the ‘summary of the review’ for the details. Also, in the hyper edge path, the subscript on the last node of the 2nd edge should be r_2 and not r_1. It’s an incremental work, focused on scaling an existing method by converting a dense representation into a sparse representation.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; rating score: 5; The Shapely value based methods are much better when instances have interactions. 2.Several straightforward methods and Shapely value based methods are evaluated on three datasets. 3.Authors propose a novel weight sampling strategy to improve sampling in SHAP. 4.The paper is well written and easy to read. I m not sure whether the interpretability requirements for MIL are original ideas, since MIL is not a new problem. 2.Related methods such as LIME[1] should be discussed. LIME is also a model agnostic local interpretability method. Attention based deep multiple instance learning.<|endoftext|>The manuscript aims at the design of a model agnostic method of the the interpretability of models and methods addressing multiple instance learning (MIL) problem. On the positive side, the manuscript is well written and to a good extent its presentation and flow of contents is clear. The proposed method is simple and sufficient details are provided regarding its implementation and parameters. When reporting results in Tables 1 3, more than one parameter is changed for the SHAP based methods. As stated on my review, there are quite some merits regarding the presentation/reproducibility of the manuscript, and the simplicity of the proposed method. However, I have concerns regarding the validation of the proposed method.<|endoftext|>This paper presents model agnostic Shapley value approaches for interpretable multiple instance learning, with a focus on identifying key instances and identifying which positive classes these key instances support. The paper presents a suite of sampling based approaches to compute Shapley values for interpretable multiple instance learning. The focus of the paper is on answering "which" questions (for identifying key instances in a bag) and answering "what" questions (for identifying which positive classes are supported by key instances). There are only three datasets used in the evaluation section, with two of them having independent instances. From the text, I think the authors intend interpretability to be the correct identification of which are the key instances as well as the correct identification of what classes the key instances support.<|endoftext|>The paper presents six methods for explaining the output of MIL models. The idea is pretty simple but, indeed, effective. The most advanced method is based on SHAP, and it is used to assign to each instance in the bag a value that weights its importance. Experiments confirm that the method is able to improve over competitive baselines. The paper is nicely written and easy to follow, but, in my opinion, it totally overlooks existing prior work that should be cited. In particular, I would like to see comparisons with the techniques presented in the following paper: https://www.jmlr.org/papers/volume21/18 811/18 811.pdf It is from 2020 and already mentioned interpretability, and it should be used as a baseline. Yours is more an "explainability" method. The paper presents a SHAP based explainability method for MIL models.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 8; **Major Comments**   My impression from reading this paper is that the authors are proposing NeuPL as a more "general class of population learning algorithms" than PSRO (Sec 1.2). Is N step BR meant to refer to training a new policy for each i \in N player? The ideas presented within the work are interesting and provide a way to frame PBT methods and could facilitate the design of many future algorithms.<|endoftext|>The idea of representing an entire population of policies within a single conditional model to solve the shortcomings of existing population based training algorithms is very elegant and novel. The authors provide a framework called NeuPL to improve the performance and convergence speed of population based training algorithms.<|endoftext|>Second, it uses a conditional network to represent the population of policies, so as to enable skill transfer. 3.The empirical study on MuJoCo Football is appreciated especially for population learning. I think NeuPL is new and interesting.<|endoftext|>The authors propose a new framework of  population learning that optimizes a single conditional model to learn and represent multiple diverse policies in real world games. Experimental results are strong and have many interesting empirical findings.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The paper proposed a gradient based algorithm GBMS to solve PDEs based on the solutions of other similar problems. Numerical experiments are performed to show the effectiveness of the method. Weaknesses:  The authors spent a lot of effort to create a new terminology “meta solving”, which has a board meaning and many other algorithms can be formulated in this way. However, this is only a new terminology, but it is not a new idea or a new algorithm. From the paper, there is no clear evidence that why we would need this new terminology, or for what problems we have to use this new terminology. There is no comparison between the proposed method and other methods in terms of inference speed and accuracy.<|endoftext|>The paper applies gradient based methods to important problems of learning initializers for iterative methods in scientific computing. 2.The authors provide a guarantee for initializing the Jacobi iteration, albeit under what seems like a restrictive assumption on the model capacity. 2.There is no demonstration of practical application utility, i.e.whether going through the trouble learning this initialization is actually useful. As an example, in the field of neural PDE solvers there is often a demonstration of end to end computational savings provided (c.f.Li et al., (2021)). ICLR 2021. *Algorithms with Prediction*. While the problem setup is reasonably well motivated and some of the empirical results are interesting, it is not clear to me how practically relevant the empirical results are for the problems being studied. The very general framework is also only discussed in the restricted case of linear system solving for PDEs.<|endoftext|>The authors define the task of solving a family of differential equations as a task of gradient based meta learning generalizing the gradient based model agnostic meta learning to problems with differentiable solvers. The presented work is solid.<|endoftext|>A general gradient based method is proposed, which is applied to generating initial guesses to differential equation solutions. This problem is formulated as a meta learning problem. Strengths:  The paper proposes a very general formulation of “meta solving” numerical problems. As the paper mentions, other applications, such as root finding, are applicable. Only evaluating the framework on one application does not showcase its general applicability. The formulation of the dataset for the experiment in 3.2 seems arbitrary. Are there stronger baselines that can be compared against? I’m not very familiar with meta learning or PDE solvers, so I’m not very confident in my assessment.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper generalizes the discounted objective in RL with "delayed" discounted objectives. I m recommending rejection at this time, due to the following concerns:1. Can the authors further comment on why the area above the frontier is dramatically inconsistent? There was discussion about delayed discounting, and how Equation 1 provided temporal weightings which discarded short term information and emphasized a delayed geometric weighting.<|endoftext|>As a micro point, I really struggled with the notation used to introduce the idea. That said, I certainly acknowledge issues that arise when using geometric discounting and I encourage authors to further explore this idea.<|endoftext|>In this sense, I am sympathetic with the goals of the paper. I did not see it defined. ** I find the motivation of this paper to be compelling.<|endoftext|>Or am I missing something? Specifically:  The considered discounts $\Phi_D$ are introduced with no motivation whatsoever. Where does this particular definition come from? _J.Machine Learning Res._ 19:1 49, 2018Although the premise of the paper is interesting, and the proposed approach is novel, to the extent of my knowledge.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6;   The paper tackles the task of object detection in LiDAR  point clouds, with a strong emphasis on automotive  applications. [S4] The writing is good and the main ideas of the    paper are presented clearly. However, the authors do touch on this in    the "Limitations" section.<|endoftext|>This paper aggregates LIDAR scans from past traversals of an AV route to improve 3D object detection. This is a well written paper and a good contribution to the field.<|endoftext|>The paper proposes a HINDSIGHT framework for object detection on lidar scans, with query of scene feature extracted offline from previous traversals.<|endoftext|> The paper proposes a method for improving object detection in lidar scans, which works in cases where a lidar scanner visits the same environment multiple times. The paper proposes to take point clouds from previous traversals and put them through a trainable network, before temporally aggregating features from each of the traverals. # Pros:+ The overall idea is a good one.
Reject; rating score: 5; rating score: 5; rating score: 6; Accurately predicting the performance at early stage is important for efficient model selection without incurring too much computation. The paper proposes a neural capacitance metric as a predictive measure to capture the performance of a model on the downstream task using only a handful of early training results. The metric is derived from a line graph mapped from a neural network by modeling the dynamical system of the network. The proposed method is novel. Experiments show that the proposed neural capacitance more accurately predicts the accuracy of the model. Ablation study have been performed to study the effect of parameters such as starting epochs, size of training set, etc. Overall, I think the paper is potentially a good paper, though I m not an expert in this topic and does not fully understand the mathematical derivation in Section 4.2 and 4.3. My major concern is  I m not sure if I fully understand NCP units. Why the weights are randomly initialized and freezed during finetuning? I do not find the relationship between the new layers and what is described in section 4.2 and 4.3. Why two dense layers are chosen to be the architecture of NCP units? I think this part is missing from the paper and should be elaborated to build the connection between the previous section. Overall, I think the paper proposes an interesting idea to model the training NN as a dynamical system and is potentially a good paper, but the some part of the method needs more elaboration.<|endoftext|>The paper formulates neural network training as a dynamical system and uses existing theory to formulate a metric (beta_eff) to predict how a pre trained model will perform on a downstream task without requiring to train the model to convergence. While the theory described in the paper is not novel, the application of it to the task of predicting model performance is novel as far as I can tell. ClarityThe paper uses multiple prior ideas from dynamical systems and machine learning literature but many of these choices are not well motivated or explained. This makes it difficult for a broad ML audience to understand the paper and its significance. The paper draws heavily from theory described in Gao et al 2016 which proposes a method to convert the reslience function on a multi dimensional system to a single dimensional function x_eff where the critical points B_eff are a one dimensional value. Gao et al 2016 describes this approach for a non neural network system and it is unclear why the assumptions introduced in this paper apply to neural networks. Thus, it is unclear why this approximation is appropriate for neural networks. This is briefly touched upon in section 4.2 where it is stated that "the others’ contribution as a whole is implicitly encoded in the activation gradient" however the connection to the original theory described ini Gao et al 2016 and why this is an appropriate simplification needs more explanation. Next, the motivation for the NCP is not discussed in the paper. By initializing it randomly and freezing it, a random amount of fixed noise is introduced into the process and it is unclear what the purpose of this is. Additionally, is this randomly initialized each time for all the experiments? This will result in beta_eff being undefined. 3.Details  In the last sentence of section 4.2, the self dynamics part, f(w_i) is defined as F(w_i*), however, I couldn t find where this was defined. What does this correspond to in the context of neural networks? 4.Figures  The figures are small and difficult to read. A better presentation of the results such as a table could be added to the main paper while the figures are either added to the appendix or enlarged. The paper introduced a novel formulation of neural networks based on prior dynamical systems theory for an important machine learning problem.<|endoftext|>This paper proposes a framework to select the neural networks for downstream tasks. To identify the better generalization model, the authors propose a new metric (Neural Capacitance, NCP) to predict precise learning curves. Then the authors have verified the advantages of the proposed method when being applied to different datasets (CIFAR10, CIFAR100, SVHN, Fashion MNIST, Birds) with 17 CNN models. I am not an expert on this research topic. Strengths:The whole paper is written in high quality and a clear manner. To validate the outstanding performance of the proposed method, the authors conduct experiments on many popular CNN models. The authors conduct the experiments on well known models, what about applying this framework to select the subnetwork on many NAS benchmarks e.g., [1]. [1].Hw nas bench: Hardware aware neural architecture search benchmark NA
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper introduces a method for semi supervised keypoint localization based on pseudo labeling with auto curriculum learning. 4.Informative ablation studies and evaluation of the generalization ability on domain transfer. The paper is well written and easy to follow. 2.Although all components are widely used techniques in the field, the application of RL to tackle pseudo labeled sample selection for keypoint localization is novel.<|endoftext|>1.This paper introduces Curriculum Learning to semi supervised keypoint localization, which is an automatic pseudo labeled data selection method. The task is practical and motivation is well. This paper proposes a novel and effective threshold selection method for semi supervised keypoint localization. 2.Overall, the method of this paper is technically reasonable and novel.<|endoftext|>A semi supervised learning method (PLACL) is proposed. It consists in iteratively (these iterations are called rounds):1. predicting pseudo labels to unlabeled data using the current model,2. training a series of models from scratch using the labeled data and selections of pseudo labeled data. 2.The paper is well written and easy to read.
Reject; rating score: 3; rating score: 3; rating score: 3; The authors propose the aggregation rule Tmean for federated learning. 2.The empirical results in this work are not solid enough. This paper is of good readability.<|endoftext|>This work analyses the use of trimmed mean as robust aggregation rule in federated learning. The theoretical setting considered is the one of Distributed Learning and its resilience to Byzantine attacks has already been heavily investigated too.<|endoftext|>This paper considers the trimmed mean function as the aggregation rule for the byzantine resilient distributed learning. Lack of baselines. The secure aggregation and encryption are not defense to these attacks.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; ## Update after rebuttal and discussionsI thank the authors for taking the time to discuss the issues pointed out in the reviews at length. Unfortunately, I am still not convinced that the paper is ready for publication. 2) I continue to have doubts about the subsampling amplification. 3) I still think the paper can be improved a lot by taking the time to rewrite it focusing on the main contribution of certified robustness under DP and clarity of the presentation. Strong points:i) The problem of data poisoning is an important one, and the connection between robustness and privacy is a nice one to use for getting provable guarantees.<|endoftext|>The paper states that the model produced by differentially private federated learning to be already certified against poisoning attacks. Evaluating the proposed method on the Language Modeling task might further strengthen the submission to provide a realistic privacy training regime. However, I would agree that in cases where differential privacy is inevitable, the conclusions seem helpful and could further promote the use of DP.<|endoftext|>This paper proves that differential privacy indicates certified robustness against poisoning attacks in federated learning. I was very intrigued by the paper at first but ended up having a mixed feeling after reading it. User level DP is also not limited to federated learning. Third, if the authors really want to focus on federated learning, then the instance level DP section does not make sense because all federated protocols should preserve user level DP in practice. "Certified robustness to adversarial examples with differential privacy."<|endoftext|>Theoretically, the authors build two definitions for certified robustness against data poisoning attacks, draw the connection with user level and instance level differential privacy. The key proof is based on the definition of individual privacy and group privacy. Empirically, the authors verify the correctness of the bounds by performing real attacks. I read the authors  response and other reviewers  comments. I think this paper provides an interesting perspective for robust machine learning. 3. the proposed algorithms are practically usefulWeaknesses:1. I think the results are correct but not surprising. 2.It would be better if the authors could provide some theoretical/empirical intuition for the utility. It is known that both robust learning algorithms and private algorithms would cause the performance drop.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors propose a novel PEG layer to account for positional encodings in graph neural networks and state some necessary conditions for its stability and permutation equivariance while providing theoretical insights for the same. Overall the paper is very well written, has some interesting theoretical insights and very practical results. Given the recent upsurge in the amount of work done to incorporate positional encodings in GNNs, work such as this that addresses the shortcomings of previous methods and also provides a theoretical framework was necessary.<|endoftext|>The paper focuses on tasks of a set of nodes handled by graph neural networks (GNNs), which generally requires random feature (RF) or positional encoding (PE) to recognize the node identity. Rigorous theoretical analysis shows PEG is permutation invariant to node features and rotation equivariant to PE, and its stability is guaranteed. I only have some comments on it.<|endoftext|>The paper considers the topic of equivariant and stable positional encodings (PEs) for graph neural networks. This work formally studies the two conditions of equivariance and stability for graph PEs (Section 3.1). is reported. However, it seems the empirical results may not be adequate to reflect on the technical contributions of the paper. In summary, my evaluation of ‘correctness’ and ‘technical novelty and significance’ measures of this paper are positive, while the evaluation of ‘empirical significance’ does not seem to align to the technical contribution.<|endoftext|>This paper proposes a novel GNN layer called PEG layer to address the issue that existing positional encoding are not generalized to unseen graph very well. The experimental results have also demonstrated the power of the proposed approach. Mathematically, this paper proves the proposed PEG layer is able to preserve permutation equivariance as well as to achieve PE stability. The proposed technique is solid and novel. There are some minor concerns as raised in weaknesses, but overall it is a paper worth of accept.<|endoftext|>This paper studies positional encoding (PE) for Graph Neural Networks (GNN). This paper address this issue with some care (correcting some mistakes made in previous works). This notion is interesting. The notion of stability will likely be picked up in future works and the authors show how to construct PE stable layers which can be useful. To validate their theoretical results, the authors should consider a task where equivariance is  crucial . There is a mismatch between the theoretical part of the paper and the experiments.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; In this paper, the authors propose GLASS, a GNN designed for subgraph tasks using labelling tricks. The zero one trick version is also claimed to have more expressive power than plain GNNs and can capture the six properties in the state of the art method SubGNN. The paper addresses the overlooked problem of subgraph level tasks, as most previous GNN models focus on node or graph level tasks. It can be easily implemented and can be trained efficiently. The novelty is somewhat incremental. Node labeling trick has been used on GNNs, although not specifically on the subgraph level. Any additional property that SubGNN does not have? In other words, if SubGNN is also trained with the same self supervised loss, would its performance become much better?<|endoftext|>This paper focuses on predicting the properties of subgraphs in the whole graph. The authors rigorously analyze the effectiveness of the proposed method. 3) Experiments demonstrate that the proposed model with labeling tricks outperforms state of the art subgraph representation learning methods on several synthetic and real world datasets. Experiments on running time also show that the proposed model is more computationally efficient than the subgraph level learning method SubGNN. Weakness:The authors may want to improve the presentation of this paper. The authors propose a labeling trick to enhance plain GNNs for subgraph tasks, and the theoretical and empirical results demonstrate its effectiveness and efficiency.<|endoftext|>This paper proposes a simple subgraph modelling framework by adding a subgraph level position encoding to node (whether a node belongs to subgraph) to node feature. This paper argues that a subgraph level positional encoding is enough for subgraph representation learning. It gives some theoretical analysis that such trick could fulfill the six properties that subGNN proposed. I think the approach is reasonable and the analysis is sound and convincing. I think this comparison is not fair. On the one hand, I think SSL learning should be agnostic to GNN model. I think you should compare your model with other GNN model in the same setting, i.e., with SSL training or without. 2) The motivation and insights of the SSL learning task is less than the subgraph encoding. I recommend the authors also compare this method into subgraph level modelling.<|endoftext|>The paper proposes a simple GNN approach to predict labels of subgraphs. The authors study the problem of subgraph classification and propose a simple adaptation of existing GNNs for this task. Authors can improve the paper in the following ways. What if you employ the same losses for other approaches? What batch size you used in experiments? 4.One of the biggest concerns, however, is not with the paper, but with a studied problem. The same is true for the employed datasets, even though they are inherited from the previous work. This is a well written paper, that could be improved by providing more motivation and exploring the performance in more depth by using other baselines and providing ablation studies.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; They proved the success of the proposed procedure on CIFAR 10 and CIFAR 100 by showing improvements in provable robust accuracy with a comparable standard accuracy. My major concerns are about the explanation of the numerical results. I only got what it means when reading later sections of the paper.<|endoftext|>(2) The relu function in the maximization term ensures that the optimization tries to increase the certificates only for the correctly classified inputs. 3.The figures and tables are clearly designed. Figure 1 illustrates the HH activation very clear and makes it easier to be understood. 4.This paper contributes a lot to the research area of 1 Lipschitz CNNs, especially the class of piecewise linear GNP activation functions HH. The paper proposed three novel methods to address some limitations of existing works. The experimental evaluations convincingly support the main claims.<|endoftext|>This paper addresses the problem of provable L2 robustness in image classification. Comparisons against the original SOC work are reported on CIFAR 10 and CIFAR 100.<|endoftext|>Overall, I feel the paper is well motivated, and the results are convincing. Pros:  Excellent numerical improvements  The paper is clearly written  The contributions are mainly from the engineering point of view and this paper demonstrates the method works well, with coherent explanations. Theorem 1, I think, should be phrased differently: it requires to introduce formally the robustness certificate first. Again, I feel Theorem 2 is a too strong statement for this. It’s also strange to see that the Theorem 1 uses phrasing from the Definition 1 but is still above this latter. Otherwise, a suspicious reader could believe the accuracy has been optimized on the test set.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes a federated learning algorithm called FedProf, by using a training rule that updates the model based on divergences between data representation. Weakness:Overall the presentation on theoretical results can be improved, and the connection of some statements to the main results is not clear. 1.For example in Proposition 1 and 2, all non linear components in the neural network seem to be bypassed and the statements are simply equivalent ways of stating CLT.<|endoftext|>The authors tackle the federated learning scenarios where local data is biased, noisy, or even irreverent, which could significantly degenerate the global performance, and the authors propose FedProf, which utilizes data representation profiling and matching to mitigate the impact of low quality clients during training. This paper is well written and easy to understand. I have a few comments, particularly for the evaluation part. More Datasets: it seems to work well on the reported datasets. I would like to see the variance of the proposed model. The Current Baseline Models: I found that the baseline models used in the paper are slightly outdated.<|endoftext|>In this paper, the authors propose a user selection algorithm for federated learning (FL). The key motivation is to select high quality clients for update and thus to reduce the impact of low quality data on FL training. This hypothesis needs further justification. Specifically, let’s say there are two clusters of clients with different focuses. I understand that this makes it more efficient to calculate distributional difference. However, in principle, this can be applied to cases where Gaussian distribution does not hold, right? In summary, the authors propose to more favorably select FL clients whose representation distribution is more similar to that of the global model.<|endoftext|>This paper proposes FedProf, a new client sampling scheme that speeds up the convergence of FedAvg type algorithms. Strengths: The proposed method is intuitive and simple to implement. It also comes with theoretical guarantees (though the assumptions are highly idealized). It is not clear to me if the independence assumption would hold in practice. Following my previous point, I am curious about what will happen if we replace the product Gaussian (Eq.2) by a Gaussian with some non identity covariance matrix. I appreciate the simplicity of the idea and the performance boost it brings about.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 1; This paper reveals the flaws of these methods by designing two effective methods (called "defenses") to defeat that protection mechanism. Hence, the model trainer can wait to obtain a better face recognition model and properly safely finetune it on the perturbed images. This paper has a good amount of references### Weaknesses  In the adaptive defense, the authors assume access to the poisoning function, which may be unrealistic. The poisoning function should be secret for a better protection,  Obvious defense can break LowKey only once (Figure 6). There is no guarantee that robust models, like CLIP, will be popular in the future facial recognition models. This paper reveals the flaws of previous works that use poison attacks to protect user identity from face recognition systems. There are still some concerns about the experiments and the significance of the previous works the paper is targeting.<|endoftext|>It is especially interesting to see how even "oblivious" systems can just wait and see to get better. Ethical concerns are well documented in ethics statement. WeaknessesThis is a strong statement "Ultimately, we believe that the only viable course of action for privacy conscious users is to avoid posting pictures online, or to support policy and legislation that restrict the use of facial recognition (Singer, 2018; Weise & Singer, 2020; Winder, 2020)." and may not be a feasible solution to the problem, although supporting policy is certainly feasible. Minor point: The paper argues that it is not an arms race, however, it can still be considered an arms race with the constraint that the defender is in a better position to win. This is arguably just a semantic difference, and a minor point. The paper does a good job analyzing poisoning attacks. It shows that the methods fail over time and they do not generalize to future attacks. It is in interesting paper and has value for the community to consider longer term security measures for facial recognition privacy concerns.<|endoftext|>### Weaknesses:From technical point of view, there is less innovation in this paper. Experimental results suggest that those data poisoning attacks can be easily defensed by adaptively tuning the face recognition models or using more advanced algorithms which would be developed in the future. The main conclusion is that people should not rely on technical solutions to protect users privacy and legislation actions are what is actually needed. However, I think the technical contribution from this paper is limited as little new insight is provided.<|endoftext|>This paper studies the effect of data poisoning in face recognition and the relation to the defense techniques. Conventionally, the poisoned data will fail the face recognition models who is trained without defense strategy. Two solutions of defense are given: oblivious trainer and adaptive trainer. The claim is that, any existing poisoning methods cannot protect the privacy of users in the face images. + The insight of this paper is very useful for the proctecter of user face. It is not clear how the oblivious trainer works. Rather than fancying legislative alternative, a research paper needs to propose technical solution. Overall, this is a facial privacy analysis with insightful claims, but the presentation and the discussion is very confusing. The final rating will depend on the authors’ feedback.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; 5.**Page 5, "... our algorithm is expected to have better performance. The authors also provide a sketch of the proof in the main text which is useful. ** This is a trivial statement. **The proofs contain mathematical mistakes that break the main result of the paper. If the authors resolve the mentioned issues during the rebuttal, I will increase my score. 2.**Assumptions on the parameters in Theorem 1** are strong and not well defined for some special cases. The second option describes a theoretically simple case since the number of local steps is small.<|endoftext|>This paper analyzes the convergence of the local gradient clipping. The theoretical results show that the convergence is guaranteed when both the number of workers and the number of local iterations are not too large. Overall, I think the paper is well written and easy to understand. But the contribution is incremental. On the other hand, if we just need a rough solution (e.g., a moderate $\epsilon$), which is typically the case in deep learning, then $N$ has to be small.<|endoftext|>This paper propose a novel distributed optimization method CELGC. The proposed CELGC is simple yet effective. The only difference from local sgd is that CELGC adopts normalized gradient if the norm of gradient is large. The authors also prove its convergence rate for relaxed smooth and non convex functions. I notice that in the three experiments, these hyper parameters are quite different. 3.What the difference between the naive version of the parallel gradient clipping algorithm and CELGC with i 1?. This is not consistent with Figure1(a). This paper has many things to be improved.<|endoftext|>The paper considers the effect of gradient clipping in Federated Learning and how it affects the convergence rate. However, there is not much work on analyzing its effect in distributed or federated learning scenarios. This paper is among the few ones that tries to theoretically analyze the behavior of FedAvg and similar algorithms when the workers use gradient clipping in their local SGD updates. The paper is generally well written and the claims are well supported. The paper has theoretically analyzed the effect of gradient clipping in local SGD updates in Federated Learning and FedAvg.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; Since the bound that the practical algorithm optimizes is not tight, it is unclear to which extent the proposed method mitigates the objective mismatch. NoveltyTo the best of the reviewer s knowledge, the idea is novel. While the idea is promising, the submission in its current state needs substantial improvements. The literature review could be improved. SignificanceThe significance of the paper is limited. In particular, relevant references include [1] that characterize the set of models that are optimal for planning; [5] that optimize the policy and the model using the same objective; [6] that learn the model to directly optimize agent’s performance, not a lower bound on it; [7] that learn the model that is helpful for policy improvement using a weighting scheme.<|endoftext|>This paper studies the the problem of objective mismatch problem in model based reinforcement learning, which states that the goal of the model is different from the goal of policy optimization. Both share similar components, with similar method of learning. Writing: The paper is written clearly, although it has a few typos. This paper is written well and the idea sounds very interesting and promising.<|endoftext|>They also give a global lower bound that holds for any dynamics. And the proposed MnM is motivated by the idea of GAN to solve this problem. The whole paper is easy to follow and I also appreciate the theorems in the paper.<|endoftext|>Despite these advantages mentioned above, I have some comments below that I’d like to hear from the authors in the responses. Maximizing this augmented reward function is proved to maximizing a lower bound of the logarithm of the original expected return. The method is well motivated, and the proposed augmented reward function is novel.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes improving the generalization performance offline RL algorithms by using a new approach to aggregate observations based on the similarity of their expected future behavior. The writing is generally clear now, but there are still quite a few issues (e.g., $o_{1}$ and $o_{2}$ in Eq.(3) now have superscripts, but this now is inconsistent with other ocurrences of $o_{1}$ and $o_{2}$ around Eq.(3)), and a careful revision will help readers to better understand the idea. * There is a consistent confusion of MDP and POMDP throughout the paper. In the motivating example, it seems the first images for Level 1 and Level 2 should be considered similar, but it is not clear why. * It s not clear what the different MDP/POMDP models are in the experiments. The technical writing for the proposed approach requires a careful revision too. Here are some issues:  The term cumulant function is confusing as cumulant function is a well known concept in statistics. Section 3.1: for POMDP, a policy is usually maps a belief to an action, not an observation to an action. Also note that the Q function in Eq.(1) is a function of the state and the action, and thus not directly computable because the state is not observed. Eq.(3): RHS depend on $i$ and $j$, but LHS does not.<|endoftext|>This paper studied  zero shot generalization in an offline reinforcement learning setting. The authors proposed to improve the generalization via a better representation learning. Specifically, the authors hypothesized that observation with similar future behaviors should be assigned to similar representations. To this end, the generalized similarity function (GSF) that aggregates latent representations with the future behavior is proposed. The proposed approach outperforms baselines across different tasks in Procgen. The proposed GVF is a general framework that can recover objectives of existing works. Weakness:   The reviewer found the approach section is not easy to follow. Some technical details seem missing. It is unclear to the reviewer how to select the cumulant function c. What cumulant function is used in the experiments. In addition, the reviewer found the notations confusing. If yes, why does it take a latent representation as input? Experiments      The authors discussed F BRC (Kostrikov et al., 2021) and MOReL (Kidambi et al., 2020), which are state of the art offline RL approaches, in Section 2. It would be interesting to see an ablation study on the data augmentation and the proposed GVF. However, the reviewer has some concerns about the experimental results and the clarity of the presentation.<|endoftext|>The authors build on previous work which regularized states with similar value function or future action sequences to have similar representations, by instead looking at successor features, i.e.the accumulated value of future cumulants. They also propose an offline version of Procgen as benchmark. The proposed method is an interesting method for defining  similarity  which can then be used for contrastive learning. The paper is well written and does not appear to have any technical or theoretical flaws. The topic of generalization in RL is relevant and the proposed method is a valuable contribution to this area. One disadvantage of this method is that it requires the ability to pre traing the generalized value function, hence the authors  decision to only apply it to offline RL. Hence the results will depend heavily on the authors effort to not only tune the hyperparameters of their own method, but also those of the baselines. Hence, I think the paper could be strengthend significantly by evaluating on additional domains and/or providing detailed information about they applied hyperparameter search.<|endoftext|>Concretely, the authors propose an approach of self supervision for improving offline RL based on similarity learning over the considered state space. In this context, the concept of generalized value function is introduced. More particularly, the proposed approach claims to be particularly efficient in improving zero shot generalization performance on one offline RL benchmark, offline Procgen. The offline Procgen dataset is a second contribution of the paper. Strenght:* The paper is clearly written and the various choices are reasonably justified* The proposition of an offline version of Procgen is a valuable asset for the offline RL research community* the approach of quantization in this context is novel to the best of my knowledge* The experimental results seem significant for the considered benchmark dataset* the introduction of generalization value function seem novel and pertinentWeakness:* The comparison to cURL (https://arxiv.org/abs/2004.04136) is missing, especially considering the fact that this approach is also leveraging metric learning in latent state space. * The approach relies on PPO, which is an online RL algorithm, and CQL which is a conservative approach of Q Learning. Maybe it would have been interesting to evaluate how the method behaves with SAC also. The proposing is interesting while novelty to existing approach could be improved. The proposition of a new dataset for zero shot generalization of sequential decision making in an offline context is valuable. The introduction of generalized value function seems novel and pertinent in the context of self supervised reinforcement learning and is clearly illustrated in the proposal of this work and could be valuable to the state of the art.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; I am not sure what the authors want to highlight, but the latter is the more interesting research question to me. This paper introduces Illiterate DALL·E, a zero shot image generation model inspired by slot attention and DALLE without text. I am writing a few below. 2.The main idea of learning compositional slot based representations with DALLE regardless of text is interesting. the green cube is in the middle, sitting on a blue cube. The authors proposed to modify DALLE, which is a zero shot image generation model that works well on natural images, to work on synthetic datasets. My main concerns are that I cannot see how the proposed approach can generalize to more realistic domains, and I am confused about the main paper s story. [1] Multi object representation learning with iterative variational inference, ICML 19. The rebuttal addresses most of my concerns. I would be happy to hear the author s thoughts about it. It should be at least discussed why the authors approach is better than using a more structured approach for image generation. ** I am somehow confused about the novelty of the paper.<|endoftext|>This paper proposes a  model that uses visual prompts to generate images. S2: The performance is fine based on the qualitative examples. The datasets used in the paper are synthetic and easy to model. So I am wondering the performance on natural images, such as ImageNet, MSCOCO and Celeb1M, can the proposed model still learn meaningful prompts and what are the prompts for natural scenes? Another weakness is that the author does not conduct a human evaluation on the generated images, so it is difficult to judge how well the model performs. Basically, the idea in the paper is interesting, but the experiments should be improved.<|endoftext|>The aim is to simultaneously learn latent concepts from base images that can then apply to the generation process (as opposed to input text, which contains somewhat discretized "concepts" already). Experiments with 4 datasets (including a mirrored version of CLEVR) demonstrate that the Slot2Seq approach is effective for novel image generation from slots, reconstruction, and out of distribution generation. WEAKNESSES  Lacks detail on why CLEVR Mirror was mirrored from CLEVR and how/why it was used in place of CLEVR. "Compositional visual generation with energy based models." The paper introduces a simple way to perform image composition using slot attention and a GPT based decoder in place of pixel mixture decoders.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper presents DrQ v2,  an improvement over DrQ, for solving visual based RL problems. And thanks for pointing out the importance of comparing wall clock time, which is usually neglected in related work. It will be great if this point can be investigated further. When and how should I tune this parameter? This paper presents various improvements for visual RL with continuous control. One interesting aspect of the paper is that it provides different perspectives on RL benchmark. If my concern in the main review is addressed appropriately, I believe this will be valuable to the community.<|endoftext|>DrQ v2, on the other hand, switches SAC with DDPG and proposes a series of algorithmic, hyperparameter choices and implementation improvements. ##########################################################################Pros:  The paper solves the humanoid environment from visual input, a task long overdue in the RL community, with an easy to implement and easy to understand algorithm. 1.Are you sure that the wall clock time improvements are related to the algorithm and not to your better implementation of replay buffer and data augmentation (f)? ##########################################################################Questions during rebuttal period:Address and/or clarify the cons above. ##########################################################################Reasons for score:Overall I am towards acceptance of the paper.<|endoftext|>This paper proposed a new model free RL method for visual continuous control problems. This method is efficient and straightforward. **Weaknesses**:  The generalization of the DrQ v2 method on different tasks and RL algorithms:    Generalization on different tasks: How DrQ v2 performs on Atari games? If the author can conduct more experiments on Atari games and with on policy algorithms (such as PPO) can further improve the value of this work.<|endoftext|>The paper presents DrQ v2, a  model free reinforcement learning (RL) algorithm for visual continuous control. The paper is an improvement over a previous algorithm DrQ. The improvements in terms of sample efficiency and speed (on a single V100 GPU) are a step change with respect to results which have been published before. The main limitation of the paper is its limited scope which significantly reduces the technical novelty and significance to the field. The paper is overall a good paper but I think it s marginally below the acceptance threshold because of its limited technical novelty and significance.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; To obtain an isotropic word embedding space, the authors proposed a new transformer based autoencoder in which each word is represented as a normal distribution, and a variational loss is used to make it closer to an isotropic normal distribution. They also tried to show some of the advantages of the proposed approach:1. 3.Solving sentence level tasks using word representations obtained by the proposed model outperforms MiniBERT. **Strengths**Isotropy of word embedding space has been a topic of strong interest to the NLP community in recent years, and it is reasonable to adopt VAE as the underlying method. **Weaknesses**Unfortunately, almost all of the experiments have some flaws that are difficult to ignore, and the manuscript might not show the superiority of the proposed method. Since there is no comparison between these existing methods and the proposed method in the manuscript, we must say that the superiority of the proposed method is unknown. Additionally, only one example is given for the output of the proposed method, and we cannot shake the concern that this is champion data. The persuasiveness of the paper will be improved if the proposed method is appropriately compared to existing methods through descriptions and experiments. In addition, as the authors stated, if it does not work well in general, then it is somewhat inadequate to convey the merits of the proposed method. Therefore, even if the proposed method outperforms BERT mini in controlled experiments, it is hardly validated that the proposed method is a valuable model. The empirical persuasiveness of this study would be significantly improved by increasing the parameters of the proposed model and comparing it with commonly used models such as BERT base and BERT large. However, almost all of the experiments did not show the superiority of the proposed method, and it would be difficult to accept it to ICLR, a leading conference.<|endoftext|>This paper presents a variational autoencoder model for learning representations of tokens in a sequence of text. The authors address this problem by learning Gaussian distributed representations (instead of point estimates) which are regularized using a KL divergence loss. Empirical results on classification tasks are competitive with point estimate baselines. The idea of using a variational autoencoder to address the degeneracy of the representations of BERT like models is sensible, intuitive, and appears to be novel. However, the paper contains significant weaknesses:  In the experiments, it is not clear that MiniBERT is the fairest baseline. Although the encoder of VAT may have the same number of parameters as the MiniBERT model, VAT also has a decoder during training. Therefore it can be argued that a more appropriate baseline would be a BERT model which has the same total number of parameters as the entire VAT model (decoder included). Are the authors able to provide justification for this? As it currently stands, it is difficult for the reader to infer the differences between your work and the various prior methods referred to. If this is this what happens in the experiments, the explanation for using KL annealing should be amended. It is not clear that the authors have understood this phenomenon. The core idea presented in this paper is strong, but the lack of an appropriate baseline and the significant rewriting required make it unsuitable for acceptance.<|endoftext|>This paper proposes to add a variational loss for each token, in the transformer architecture, to increase isotropy of deep language models. 2.The variational sampling and interpolation is interesting. Although the paper mentioned it is not going to provide on par performance with the SOTA BERT, it is promissing that since the small sized model can achieve better performance, potentially also applies to larger models. This makes it very hard to judge the contribution of this paper, as it does not obtain even a close to SOTA (e.g.for MRPC, 70 in this paper vs 85 in tinyBERT) performance. If comparable results could be obtained, e.g.both on 12 layer fully trained transformers, then I can believe the variational autoencoder layer would no hurt the performance. The idea is very similar to the variational information bottleneck, which is also used in language models e.g.[1], as a regularizer in model training/fine tuning. The main concern is the experiments, especially table 4, that is too far behind other state of the art models that have a similar size.<|endoftext|>This paper proposes Variational Auto Transformer to encourage isotropy in the latent representation space. The resulting encoder decoder architecture allows interpretable embeddings. Contributions:  A novel architecture based on Transformer with a token level variational loss. **Strengths**  Clear description of the proposed model. Extensive experiments show the usefulness of various design choices (e.g., scaling factor) of the proposed method. **Weaknesses**  For the evaluation of isotropy, currently there seems to be only those of VAT (with different hyperparameters). Some comparisons to other models (esp: MiniBERT_*) would better support the effectiveness of VAT in isotropy. There are already a lot of evaluations, but various virtues of the models can be connected by further evaluations. E.g., how do the isotropy and the performance on the transfer classification tasks relate? In what way are the examples special? Adam optimizer: Please cite the paper proposing this algorithm. Several other papers use variational methods in language modeling. 3946–56.Proposes a novel method and clearly elaborates it. Extensive experiments verifies the effectiveness, as well as the design choices.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper presents Yformer to perform long sequence time series forecasting. The key idea is to employ skip connection to improve the prediction resolution and stabilize the encoder and decoder by reconstructing the recent past. The experiment results on two datasets showed the effectiveness of the proposed method. Weaknesses* The organization of this paper is not well. The reader will have to rely on details in the appendix or other papers to fully understand the proposed technique. Although it is a new application area for skip connections, the overall technical novelty is limited. 4.Standard deviations of the prediction results are not provided.<|endoftext|>The authors propose a new Transformer based architecture for long sequence temporal forecasting (LSTF) utilising ProbSparse attention mechanisms to efficiently capture long term dependencies with L log(L) complexity. The Yformer builds on the Informer architecture with 3 key innovations:1. 2.Using a common decoder to process encoder representations jointly. The is also contains an upsampling step inspired by U Net, although the benefits of upsampling are not explicitly evaluated. The strong improvements over the Informer baseline in numerous experiments also convincingly demonstrate the benefits of the proposed model for the LSTF problem. Weaknesses However, there are several key limitations that need to be addressed before the paper can be recommended for acceptance:1. *	Is masked self attention essential in the Y Future encoder, and any reason why ProbSparse is not preferred? This is particularly important for time series datasets, which can be prone to overfitting with complex models   as shown by the short horizon outperformance of DeepAR on the ECL dataset in the Informer paper.<|endoftext|>Recent works such as the Informer have used efficient attention mechanisms and shown significant performance improvements in the long sequence time series forecasting problems. In this paper, the authors proposed the Yformer model by combining the Informer and the U Net architectures. They adopted direct connections from the multi resolution encoder to decoder to leverage both coarse and fine grained representations. The authors claimed the effectiveness of the proposed method through three benchmark datasets used in the Informer paper. According to the authors’ problem formulation, the Yformer predicts the future targets y’ based on the three inputs: past predictors x, past targets y, and future predictors x’. On the other hand, the Informer does not rely on the future predictors x’. Future predictors such as power load features in the ETT dataset would not be the “known” variables for the prediction. While, in the abstract, the authors stated that they used four benchmark datasets, the manuscript only contains experiment results for the three benchmark datasets. While the proposed methods hold great promise, it has several issues to be addressed regarding the fairness of the experiments, a missing experiment dataset, and more detailed explanations to be self contained.<|endoftext|>A Former model is proposed in this paper, based on a Y shaped encoder decoder architecture that(1) uses direct connection from the downscaled encoder layer to the corresponding upsampled decoder layer in a U Net inspired architecture, and (2) combines the downscaling/upsampling with sparse attention to capture long range effects, and (3) stabilizes the encoder decoder stars with the addition of an auxiliary reconstruction loss. The paper is well written and the proposed framework is easy to understand. For example, section 3 describes a standard time series forecasting problem with its corresponding notations. I think some of them are not being used afterwards. 2.Since the results provided is an average of three runs. It would be beneficial if the authors could provide the standard deviation of the results as well. However, (I could be missing it somewhere), seems like only two datasets   ETT and ECL, are being evaluated on.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; This paper introduces learnable compressible subspaces, which attempts to learn a set of models that can be switched at inference time to adapt to different resource requirements.<|endoftext|>The paper present a method for learning a compressible subspace of neural networks that contains a fine grained spectrum of models that range from highly efficient to highly accurate. The proposed method allows choosing the proper point of trade off between accuracy and efficiency at inference time, according to the available resources. However, the paper does not clearly explain how the compression is performed, with important details like choice of alpha missing.<|endoftext|>This paper proposes to balance the inference accuracy and efficiency by training a subspace of neural networks and then adapting the network within the subspace at inference time.<|endoftext|>However, as the paper claims, they bias the subspace to contain high accuracy solutions at one end and high efficiency solutions at the other end. It constructs either linear subspace or a single endpoint for compression. The paper proposes to learn compressive subspaces which can adaptively compress the network during inference.
Reject; rating score: 5; rating score: 5; rating score: 5; This paper proposes an alternative optimization technique (Mirror Descent) for adversarial inverse reinforcement learning, claiming it will resolve some of the issues with previous methods. Weakness:  The presentation of the paper needs improvement, especially on the connection of the previous works and the proposed method. It is unclear how MD affects the resulted reward function (or the corresponding policy) for examples as opposed to SGD? Experiments are not sufficient as authors compared the proposed method only with one other imitation learning method (RAIRL). "A gradient may not be the direction of the steepest descent in this case due to geometric constraints" How to verify this claim in IL setting, faster convergence or superior reward function? At convergence and without any prior knowledge on the optimal step size, can you claim that it is expected that MD will result in a higher quality reward function as opposed to other AIRL methods? Can it be shown in the experiments? However, my main concern is the accuracy of the claims and the presentation of the paper.<|endoftext|>This paper presents a novel mirror descent adversarial inverse reinforcement learning (MD AIRL) algorithm. MD AIRL considers the reward function as an iterative sequence in a proximal method. The MD can be implemented on top of existing adversarial imitation learning method. 2.In Figure 8, the results show that the proposed MD AIRL using Tsallis regularizer with p 2 is competitive or slightly better than RAIRL. Despite its novelty and dense theoretical analysis, the empirical evaluation can be further improved to better support the robustness of MD AIRL. The performance improvement is not significant and the written clarity is not ready to be acceoted by ICLR.<|endoftext|>The paper proposes an algorithm MD AIRL, where each iteration involves policy and reward updates, and an expert policy estimation. Minor issues: 1. The algorithm is motivated by MD, especially the reward update is modeled as a projected update of an MD update. The idea of using MD for reward updates is novel. A detailed theoretical guarantee on sufficiency and necessities on the choices of stepsizes are provided. The authors are encouraged to elaborate more on the illustration. I am assuming that it should be a reward function. 3.Equation 5: It is a maximization problem of $\pi$ but I cannot find $\pi$ in the objective. Also, I am not clear how the form of $J_\Omega(\pi_t, \psi_{t+1})$ is derived in equation 5, and how it is related to equation 2. What is this $s$ here? Is this an assumption of the results?
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; The paper proposes a mean Huber value error in the TD learning. And the paper demonstrates the robustness under such loss. The robustness is important in RL learning. And the authors also develop reformulation to solve it in practive. The major contribution is to develop a new type of loss, which is not sufficient according to the ICLR standards. My main concern is that the contribution is not sufficient. I think the main contribution of this paper is the introduction of a robust loss. The conjugate reformulation is standard. For example, MHBE and MABE are never defined.<|endoftext|>In this paper, the authors studied robust losses for learning value functions in reinforcement learning. The main contribution of this paper lies in the development of two novel robust loss functions for reinforcement learning and a saddle point reformulation based on the Huber and the absolute Belleman error and the biconjugates. Overall, I think this paper is interesting and is well motivated. My main concern is about its novelty. The authors highlighted that their main contribution is the introduction of two novel robust losses which are reformulated based on the least absolute loss and the Huber loss. This leads to the problem: why specifically the two losses? It seems to me that the comments and the analysis of this paper may also apply to other convex Lipschitz losses. It seems to me that it is the Lipschitz constant of the loss that "change the solution quality". I would expect more comments in this regard in the paper. In addition, as a minor comment, the presentation of the paper could be further improved. For instance, the abbreviations, MHBE and MABE, were used without being defined beforehand. See above.<|endoftext|>This paper proposed using Huber Bellman error to robustify the loss function in learning value function and proposed using conjugate function to avoid double sampling. It also conducted experiments to justify its algorithm. The main contribution of this paper is using Huber Bellman error instead of mean squared Bellman error, since using the conjugate function to solve double sampling has been proposed in [1]. Weakness.In the last paragraph of page 8, it said that it applied QRC without a target network. Would the performance of QRC become better and outperform QRC Huber when combined with the target network? This paper is novel in the sense that it cooperates Huber loss with value evaluation to robustify value evaluation. Therefore, I think this paper is marginally above the acceptance threshold.<|endoftext|>This paper starts with the premise that squared error minimization, despite its wide use, might not be the most effective option for learning value functions. To address this, they consider the absolute value and huber errors alongside the squared error objective and propose a saddle point reformulation for these objectives that requires learning an auxiliary learned state function which, essentially, attempts to predict the residual error at each state. Based on this motivation, starting with Section 4 the authors then make a connection of the proposed robust loss framework involving a learned auxiliary state function with prior work on algorithms for improving TD learning, namely, GTD2 and TDC. Strengths:* A nicely executed paper that starts from a simple premise and connects several prior ideas together   namely the TDC/GTD2 algorithms with the conjugate formulation of Bellman error minimization and uses this to derive a more robust versions of the aforementioned algorithms. * One of the novel conceptual contributions in this paper, appears to be making a connection between the auxiliary model $h$ used in the conjugate formulations of the Bellman error with the very differently motivated "secondary weights" model in GTD2 and TDC algorithms, where the additional model $h$ in both cases can be viewed as attempting to predict a residual error corresponding to the "primary weights" of the value function. For example, it would be interesting to see how a more naive baseline like semi gradient update corresponding to the DQN + with Huber loss metric instead of DQN + squared error performs alongside the other three curves in each of the environments. * For the last term in Theorem 3.2, $p_\tau$ is applied to the vector $\mathcal{T} v   v$, but without specifying any reduction operation   looking at the proof, it seems to me that there might be a need to add a max operation over the states to this bound. Could you please confirm? The combination of the multiplicative term involving a matrix inverse norm and the max operation over the states above makes this a potentially fairly weak bound in practice. * Another case of using an auxiliary model that learns to predict the TD errors has also been proposed for improving learning in actor critic models, e.g."Characterizing the gap between AC and Policy gradient", Wen et. al., ICML 2021. This is a nice paper that makes a novel connection between the secondary variable update in GTD2/TDC with the conjugate formulation of Bellman errors involving an auxiliary state function (both of which involve predicting the residual error with a separate model). While the technical contributions in Section 3 (e.g.Theorem 3.2) are not particularly significant, I believe the main value in the paper is the conceptual linking of two different lines of work to derive an improved algorithm over well motivated baselines. The empirical evaluation is well motivated and quite thorough, even if only for a limited set of benchmarks.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper studies an inverse (linear) contextual bandits (ICB) problem, where, given a $T$ round realization of a bandit policy’s actions and observed rewards, the goal is to design an algorithm to estimate the underlying environment parameter, along with the “belief trajectory” of the bandit policy. A particular emphasis is placed on the belief trajectory being “interpretable” and capturing changes in the policy’s “knowledge of the world” over time. The paper’s main contributions are (i) formalizing the inverse contextual bandits problem, (ii) designing two algorithms for this problem based on two different ways of modelling beliefs of the bandit policy, and (iii) providing empirical illustrations of how their algorithm can be used to investigate and explain changes in medical decision making over time**Problem definition and setup**:I found much of the content in section 2 to be unnecessarily general.<|endoftext|>The authors present the ICB an offline method for providing an interpretable description of observed decision making whilst capturing the agent s non stationary understanding of the world. I wonder w.r.t.introduced policies if you attach a particular reward to a particular organ allocation policy? I think it would be better if you had a similar figure or that figure closer to the top; it really captures the problem you are trying to solve. This is a good and interesting paper. You may as well just show a Markov chain of rewards and simply say that the directed edges propagate through different regimes and ergo the model as stands is non stationary. One bandit is conditional on a past bandit which also took actions in the same system. # Inverse contextual bandits  Notation: perhaps you should stick to convention w.r.t.to the state space and denote it with $\mathcal{S}$ as is common. "In this work, we consider state transitions that occur independently of past states and actions"   that s an interesting assumption. We assume that the organ allocation problem operates on the same dynamical system (your  environment ) no?<|endoftext|>This paper addresses the problem of Inverse Contextual Bandit. They raise an important question: given demonstrated behaviorfrom an agent, how has the agent’s knowledge been evolving over time? Formally, Given a contextual bandit problem$(X, A, R, T )$,  where $R, T$ are unknown to the agent. Given an observational dataset $D$, and a family of reward parameterizations $P$ and belief parameterizations $B$, the inverse contextual bandits problem is to determine the true environment parameter $\rho^*$and the belief parameters $\beta_{1:T}$. They propose two algorithms to learn these parameters. The first uses the agent’s knowledge in terms of Bayesian update. They demonstrate their algorithm through simulated and real world data for liver transplantations. It would be interesting if the authors can upper bound the error $|\rho^*   \hat{\rho}^*|$  to show that their algorithms converge. Based on strengths and weaknesses as I mentioned above, I think this paper is on borderline.<|endoftext|>The paper formulates and studies the problem named “Inverse Contextual Bandits (ICB)”, which asks “Given demonstrated behavior from a decision making agent, how has the agent’s knowledge been evolving over time?”  Then, the paper proposes two concrete learning algorithms, imposing different specifications regarding the agent’s behavioral strategy. Finally, some simulations using both simulated and real world data are provided, showing how ICB can be applied as an investigative device for recovering and explaining the evolution of organ allocation practices over the years. There are many Strengths in the paper:  the paper tackles and formulates the important and relevant problem of ICB  the paper is very well written and the motivation was easy to follow  related work is covered rigorously  simulation setups are clearly stated and seem comprehensive A weakness might be that the implementations used in the experiments are not shared at this moment.
Reject; rating score: 1; rating score: 1; rating score: 1; rating score: 3; rating score: 3; More specifically, it attempts to design a zero shot object detection algorithm that can detect unseen objects with the knowledge learnt from seen objects. The authors conduct experiments on YCB Video dataset to valid the effectivness of the proposed method. Strengths: The paper is well organized and the problem is also well defined.<|endoftext|>The method is evaluated on the YCB Video dataset, where 4 out of 21 objects are held out as unseen objects for zero shot evaluation. The paper reports unconvincing results, and no comparison with related work or ablation studies. Thus the method is reduced to an objectness detector. There are still many grammar/typo issues in the paper. The experiments are not complete or convincing. Therefore I recommend rejecting this paper.<|endoftext|>This paper tackles on a generalized zero shot localization task. However, the novelty of the paper is very limited, and the experiment is not enough for showing the advantage of the proposed method over prior works. 1.The second contribution claimed by the authors is weak.\`2.A novel splitting method for YCB Video dataset that splits the dataset by seen and unseen objects.`\ Actually, they just provide one of the arbitrary splits, and this cannot be regarded as contribution. The technical novelty of the proposed method is very limited.<|endoftext|>The paper performs zero shot detection of seen and unseen objects in scenarios with more fine grained division of objects. Questions1) Why is the mAP which is the standard score for object detection, not reported? Further, just a single dataset is used and there is no comparison to prior art. The authors perform zero shot detection using a simple approach of predicting attributes.<|endoftext|>The paper tries to generalise the generalised zero shot learning (gZSL) problem to full images as an object detection problem instead of an image classification problem. In such a way they can then use the attributes to suppress background objects/predictions based on the overlap of seen and unseen objects. In the related work the claim "The varieties of objects in one category can increase the generality of the trained algorithm. ", is there evidence behind this (needs citation)?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper aims to establish an explicit multi agent credit assignment approach with interpretability. I believe this paper aims to tackle a fundamental problem for MARL, i.e., the credit assignment problem. However, I find that most discussions and presentations in this paper are rather vague, which is very hard to understand for me. The ground truth objective is $Q_{joint}$. I vote for rejection since I cannot capture the main ideas of the proposed method from the current version.<|endoftext|>This paper tries to tackle the multi agent credit assignment problem by an explicit method. If the authors can convince me otherwise, I would be glad to change my review. I believe that some adjustments to the approach will make this work suitable for another submission. In a centralized weighting scheme, there is no such thing as "disagreement", so what is being compared? The colorbars in the legend in Figure 2 are too thin, I can t distinguish them and match them to the curves easily even at 200% magnification. This paper purports to optimize credit assignment for the MARL objective but the method does not do so.<|endoftext|>**Strengths:**  *The paper studies an important problem. *Related work is covered well and the paper explains how its main algorithmic contribution is grounded in theory. * The paper extensively surveys the prior work on the credit assignment problem in  MARL, providing satisfactory explanations for the most important references. While I believe I understood  the main contributions of the paper, I generally found the details hard to follow. *Theoretical results are not rigorous enough. * Regarding the experiments, I ve found the main results interesting, but this whole section could be considerably improved. a) There are quite a few formatting issues.<|endoftext|>In this paper, the authors proposes a novel scheme named ECAQ for learning an explicit credit assignment scheme for CTDE tasks, which in its theoretical form should converge to the optimal solution and also empirically evaluates the DNN form. The ideas and contents in the paper seems to be interesting and novel, and the experimental results are also providing some reasonable justification of the values of the proposed new approach. The authors conducted experiments to demonstrate that the proposed ECAQ technique achieves interpretable credit assignment and superior performance compared to several advanced baselines.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; Transferring standard graph operators to the hypergraph setting is non trivial and several message passing operators have been considered in the setting of hypergraphs, including clique expansion and tensor based. In this paper the authors propose a general framework where learnable multiset functions are used in order to learn the hypergraph propagation map from the data. The motivations for the model design are well discussed as is the related work. The proposed idea is simple and neat. The experiments consider a wide range of hypergraph datasets, which is somewhat rare in hypergraph classification literature. [1] Tudisco et al   Node and edge nonlinear eigenvector centrality for hypergraphs  [2] Arya et al   Adaptive Neural Message Passing for Inductive Learning on Hypergraphs  [3] Prokopchik   A nonlinear diffusion method for semi supervised learning on hypergraphs  Overall, this is a solid paper which proposes one simple, yet novel and efficient idea to generalize propagation maps on hypergraphs. I like this paper and I would like to see it accepted. I will raise my score if the weaknesses are properly addressed I am happy with the feedback provided to my concerns and those of other reviewers so I will raise my score<|endoftext|>This paper proposes a generalization of the hypergraph neural networks using multiset functions. Instead of doing clique expansion as in many existing methods, the proposed method learns explicit hypergraph representations, and uses two multiset functions to perform message propagations, one from nodes to hyperedge and another from hyperedge to nodes. The method is shown to be a general framework that encompasses existing GNN methods. Experimental results show that the AllSetTransformer achieves competitive results or outperforms existing methods on a large set of benchmark datasets. The proposed method unifies various methods for hypergraph neural networks, and gives a competitive formulation across a wide spectrum of problems. This is a nice contribution to the community. The paper makes a nice contribution to the community.<|endoftext|>Several hypergraph neural networks have been proposed in the literature to exploit both group relationships among nodes and node features for learning with hypergraph data. The contributions of the paper are 1) generalisation of most existing methods into a single framework (named AllSet), 2) exploration of AllSet based on Deep Sets [NeurIPS 17] and Set Transformer [ICML 19], and 3) empirical evaluation on existing benchmarks and three curated hypergraph datasets ### **Strengths**1. The methods used (AllSetTransformer, AllDeepSets) are appropriate for the downstream task (node classification in hypergraphs). It is interesting to see that AllSet generalises several neural networks on hypergraphs and message passing neural networks on graphs. 2.**Clarity of Presentation**The paper is clearly written and well organised. The authors have also released the source code as part of the supplementary material. Moreover, AllSetTransformer and AllDeepSets are straightforward applications of SetTransformer and DeepSets in the AllSet framework. The Walmart (and House) datasets do not come with node features, so it would be more compelling if hypergraph only methods are used as baselines [e.g., Re revisiting Learning on Hypergraphs: Confidence Interval and Subgradient Method, ICML 17]. 3.**Positioning with Prior Work**Experiments are restricted to transductive node classification in hypergraphs. It would be interesting to see how AllSetTransformer and AllDeepSets would perform in these tasks. ### UpdateAfter reading the responses to all the reviews, I have updated my recommendation from 5  > 6Overall the paper is clear and of good quality in which claims are well supported by theoretical analysis. Positioning with missing prior work and evaluation on other hypergraph tasks would improve the paper.<|endoftext|>It argues that existing work propagates a hyper graph by first transforming it into a regular graph trough clique expansion, which may lose information. The proposed framework is called AllSet, with two instantiations AllDeepSets and AllSetTransformer. Experiments are conducted on several graph datasets. The problem is well motivated, with significant coverage of existing work based on clique expansion and tensor based propagation. S2.Experiments include a significant number of baselines (ten models), as well as on ten different datasets. S3.There are theoretical analyses showing how the proposed framework is a generalisation of some existing works. As the proposed framework is in some sense a generalisation or a unification of existing propagation mechanisms, such as clique expansion and tensor based propagation, there should be an ablation analysis to understand the contribution of each component. A more discursive analysis that explains why the method works better than the existing methods would help to reveal where the sources of improvements are. It would be good to have additional evaluation tasks beyond semi supervised classification. The paper proposes a framework for hyper graph neural networks, which is an interesting problem. Though rigorous in its treatment and experiments, further analysis on what makes the work better then the baselines would help in better appreciating the significance of the contributions.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper targets transductive zero shot action recognition. Another concern is that the importance of some critical components is not adequately evaluated. This is also related to the comments above mentioned. Is this critical to obtain a good performance? Partial information is given in Figure 3 and the Fusion paragraph on page 7.<|endoftext|>This work tries to address the problem of zero shot action recognition. Using the distribution of the unlabelled test set, the embeddings of unseen actions in the target domain are reweighted and repositioned along the geodesic such that they are better aligned with embeddings of training actions in the source domain. The proposed approach heavily depends on the distribution of the unlabelled test set. This paper proposes a sensible solution to reduce the bias between the source domain and the target domain for the task of action recognition. But novelty seems incremental.<|endoftext|>This work introduces transductive universal transport for zero shot action recognition, where no training examples for unseen classes are available. Experimental results on several action recognition datasets demonstrate the effectiveness of the proposed method. 2.The experimental results show the effectiveness of this new method and also better performance than prior states of the art. 2.Many symbols are not clearly defined, making the math descriptions in this paper hard to read. The idea of using transductive universal transport for zero shot action recognition is new, and the performance is good. The writing, especially the math part, needs improvement.<|endoftext|>The performances are good when compared with other state of the art methods. However, the details of the technique  is a little hard to follow for me. In my opinion, the paper forms well and the writing is very good. I give accept currently because of my unfamiliarity of such domain. By the way, I will follow up the work in the following reviewing sections.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper aims to solve the problem of causal summary graph extraction and time series reconstruction at the same time. The proposed method is based on conditional VAE. It considers an auxiliary variable s (states of all the edges in one TS) which is conditioned on by the prior. The presentation really really needs improvements. In the spring data they did not compare the proposed method with anything but just show a sensitivity analysis on the number of states. 3.It really needs clarification on all the notations. If this is the case, how do you guarantee $G_{ji}$ and $G_{ij}$ are consistent with each other.<|endoftext|>This paper considers causal discovery from nonstationary time series, where the nonstationarity is due to some unobserved state factors. To handle this setting, the authors propose an approach based on probabilistic deep learning. Overall, the considered setting is interesting, but the paper organization and writing need to be improved. For example, in some parts, it seems that the authors are considering instantaneous causal relations, while in some parts, it seems the authors are considering time delayed causal relations. It is not clear to the readers. Another concern is about theoretical identifiability. When claiming causality, it is essential to show the theoretical properties, e.g., identifiability of the causal graph. However, the authors did not give any theoretical results in the paper. ICML, 2020. Interesting setting, but essential theoretical results are missing, and the paper organization and writing needs improvement (some parts are inconsistent throughout the paper)<|endoftext|>The paper presents a causal discovery method for time series that generalizes [Lowe et al 2020] to the conditionally stationary case. While the paper seems to provide an interesting and novel method, it also seems to extend the work of [Lowe et al 2020] to conditionally stationary time series in a relatively straightforward fashion. One of the issues I have with this paper is that it fails to address if the model is indeed identifiably causal or just a graph that fits well the data. In [Lowe et al 2020] the graph is shown to be causal under  the assumptions of first order Markovianity, no latent confounders, no instantaneous relations and assuming the optimization procedure finds the global optimum. Another issue I have is that it doesn t adequately cover the related work, for example there are other methods that model non stationarity in a similar way, e.g.https://arxiv.org/abs/1903.01672. This also extends to the evaluation seems to be focused almost exclusively on [Lowe et al 2020] (which unsurprisingly doesn t work if the data is non stationary).<|endoftext|>This paper proposes a new method for discovering the causal graph from time series data when the time series are generated by a non stationary process. The method relies on previous work from Lowe et al, 2020 and proposes to condition the causal summary graph driving the (causal) edge generation between variables by a categorical state variable. Extending on the previous points, the experiments are limited to synthetic and rather low complexity cases, although as I said above, this is a hard problem and likely harder settings would be not possible likely. I am not sure about the validity of some statements.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The paper considers the problem of privacy preserving inference on deep learning models using homomorphic encryption. The paper claims to improve upon the existing state of the art HE based inference approaches significantly   two orders of magnitude. Cons:1.The paper is rather straightforward rehashing of existing (which are themselves rather simplistic, in my opinion). In the abstract it is claimed that the current work improves upon the existing work by two orders of magnitude. While this is true when compared by CryptoNets   the more recent works already seem significantly better. I think this is an important problem and the paper is reasonably well written. For instance, there is no clear comparison of inference time with LoLa (does it improve the inference time at all?).<|endoftext|>This paper follows the line of work that leverages holomorphic encryption (HE) operations on encrypted data for privacy preserving inference on MLPs and CNNs. In section 3.2, the authors compared with two types of representations in LoLa: the sparse representation and the stacked representation. I would appreciate it if the authors can provide a clear explanation to this. So is the variant proposed by the authors? Also, there are flaws in the statement, and the writing is not professional enough.<|endoftext|>Overall, this paper addresses an important problem in privacy preserving machine learning and the results show that the method is effective. However, the originality and the evaluation of the paper are not strong enough. For example, only a limited part in table 2 is discussed in the text. More analysis about table 2 would improve the results section of the paper. By focusing on the introduction to the HS method instead of the introduction of the baseline LOLA method, the authors can explain the HS method better.<|endoftext|>The paper applies the state of the art technique for matrix vector multiplication in homomorphic encryption to encrypted inference for convolutional neural networks. The technique is explained well and there is an depth analysis of the resulting complexity with a comparison to prior work.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; 1.NoveltyThe paper proposes a “multi point” adversarial attack which works in black box hard label settings and could be parallelized. This is a novel attack, which to the best of my knowledge was not considered in literature before. First of all authors assume the black box setup for the classifier under attack, at the same time authors assume that adversary has the knowledge of parameters of the detector. Authors claim that adversarially trained network might be easier to fool using black box attack.<|endoftext|>In this paper, the authors propose a new type of query based black box attack called multi point attacks. Please see comments above for details. In general, it seems this proposed attack is only useful when there is a KNN detector. In practice, a black box setting should only provide query access to attackers.<|endoftext|>This paper proposes a query based black box adversarial attack that can only query a model with the class output. This paper considers a less studied aspect of adversarial attack that the input points are not independent and a defender can utilize this information for better detection. The paper only considers untargeted attacks. 4.Only one network architecture (ResNet 50) is used in experiments.<|endoftext|>First, this paper a new black box attack method that can avoid a query similarity detector for an attempt to generate an adversarial example. 2.First, robustness gain is only briefly described in the introduction and I cannot see any formal definition of it.<|endoftext|>## Weaknesses:Unfortunately, this paper is lacking in the experimentation to back its claims. Most interestingly, this paper claims that adversarial training is a weak defense against black box attacks of the type it develops. The paper makes strong claims with important and interesting implications for the field of machine learning and develops a novel attack mechanism defeating a proposed defense.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper proposes TOME, a Transformer that additionally performs cross attention over contextual mention encodings (in 1 or 2 layers   TOME 1/TOME 2) along with the usual self attention. STRENGTHS  The idea to attend over mentions rather than entities during feedforward is ambitious and refreshingly novel in the context of existing methods that focus on retrieving KB entries.<|endoftext|>__Method:__ This paper proposes a new approach to integrate knowledge sources into Transformer based models. The knowledge representations in a __Mention Memory__ are accessed from a __TOME__ block, which is a stack of transformer blocks with a __memory attention__ layer. In the claim verification experiments, the TOME models (both 1 and 2 blocks) outperform all baselines on HoVer, which require reasoning using multiple sources.<|endoftext|>The resulting architecture is made up of two parts: a mention encoder, which is used to build up the "mention memory"; and the transformer model augmented with attention over the memories. These are pre trained in two stages for efficiency reasons. The model is evaluated on two claim verification datasets and two QA datasets, showing convincing performance against comparable methods. * The paper is easy to follow.<|endoftext|>The TOME model retrieves most possible entity mentions from this memory and performs attention on them to generate aggregated embeddings, which will be integrated into the output representations of the current layer. The paper is clearly written and easy to follow. The experiments are solid. evaluations conducted are comprehensive and sold.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The authors consider the problem of covariate shift in recent adaptive curriculum learning methods.<|endoftext|>This work proposes a solution to the problem of covariate shift appeared in adaptive curriculum learning where the distribution of parameters of the environment at test time is different from the one at training time. I know that this is inherited by off belief work but would be good to clarify the notation. **Weaknesses:**There are a several issues with the clarity.<|endoftext|>The definition of the belief model in Eqn. Although the paper tackles an important problem, the proposed technical solutions do not seem to be entirely convincing and the experiments are insufficient. I believe this paper needs another iteration before being ready for publication.<|endoftext|>Paper proposes a method to fix the covariate shift issue induced by curriculum learning RL methods when dealing with parameterized POMDPs. I think the paper falls short because of strong assumptions made by the proposed method on simulators, uninformative theory, and poor presentation of the problem being addressed. The proposed method only selects/corrects for a distribution over a known set of environments. They are not used anywhere else again. I don t see this notation being used like this anywhere else in the paper. 10.One of the biggest drawbacks of the work is the assumption about access to the simulator.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper presents a KG augmented LM which fuses graph and contextual representations. The model, when evaluated on multi choice QA, outperforms strong baselines. The context, question, answer pairs are encoded with a standard LM and the nodes of a relevant subgraph which is retrieved from a KG are encoded with a GNN. ConceptNet is used as the KG for the first two datasets and a self constructed KG is used for the third. especially when the questions are complex and have prepositional phrases, negation terms, etc. Overall I like this paper since the method is simple and the results are good. The authors say that they initialize different LMs for different datasets to show that the method is agnostic to them. I am not convinced by this argument.<|endoftext|>The paper presents a novel method of fusion of information from two modalities: text (context and question) and a Knowledge Base, for the task of question answering. 3.Ablation studies show that the model achieves good performance on more complex questions. 2.The results presented in the paper show strong gains against baseline methods on 3 different datasets.<|endoftext|>This paper proposes a model which fuses representations from language models and graph neural networks through multiple interactions layers for multi choice question answering tasks, which outperforms baseline methods over three datasets from two different domains. The idea of multi layer fusion of two modality through additional interaction token and node is interesting. 2.The model achieves good results on three multi choice question answering datasets. 3.The ablation study shows some interesting finds such as the importance weight sharing in Mint layers. In summary, this paper proposes an interesting idea to combine language model and graph neural networks for question answering and shows good results. But more experiments and analysis should be conducted to justify their design on modality interaction modeling. Adding them will make it a strong paper.<|endoftext|>In this paper, the authors introduce a new model for the QA tasks. They improve the existing approaches by adding the interaction components (the interaction token for the LM and interaction node for the GNN) to fuse knowledge from LM and knowledge graph in a more interactive manner. The GNN is a simplification of the method of [1]. Clear explanation of the proposed model. Consistent performance improvement over the baseline models on all of the three QA benchmarks. QA  GNN: Reasoning with language models and knowledge graphs for question answering. Although there remain some issues and questions, the paper should be able to bring to the community some new aspects.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The central idea of the work is to use variationa inference and have a neural network predict the parameters of the variational distribution directly from the input data rather than optimising them using ELBO. Experiments on several substantial regression and classification datasets conclude the paper. The available code was not checked for this review. b) The empirical evaluation is lacking at least three simple baselines. ii) A sparse GP using FITC approximation. iii) A neural network predicting the inducing points rather than the variational parameters which is then used in combination with FITC or similar. b) I have the impression that a direct neural network prediction of the class probability or the mean/std (possible mean/log(std)) would do a similar job as IDSGP and that the training would be much faster.<|endoftext|>However, it is not always true and the paper has not presented a related analysis about that. Paper considers the sparse approximation Gaussian processes (GPs) which tries to scale GPs for large data sets. The authors in this paper propose a novel method (IDSGP) to improve the computational cost of sparse GPs. Here, instead of H nearest inducing points, the model estimates M inducing points and variational distribution parameters in a DNN. The other parts of the algorithm in IDSGP are the same as the proposed algorithm in Tran et al.(2020).Indeed, using DNN to estimate variational parameters is not a novel approach. 2   Limitations of the Method: The limitations of the method are not properly discussed. The optimized bound is exactly equal to a sum of GPs with one point each i.e.$\sum_i N(y_i, | 0, 1 + \sigma^2)$, and this does not depend on the prior at all. For instance, in Appendix D.1, when the number of nearest inducing points (H) increases, SWSGP presents better results and its uncertainty is close to the standard GP.<|endoftext|>The major changes with respect to the classic Titsias, 2009 paper on variational Gaussian process regression are:* The inducing inputs are data dependent, in the sense that $Z Z(x_i)$. How would I do this with you method? Can you clarify how these can be combined to approximate, for example, the posterior covariance between $x$ and $x $? This is essential to justifying maximization of the lower bound with respect to $Z$ as a form of variational inference. I am curious to hear if the authors have any thoughts as to whether it does or does not. ## Summary of ReviewThe method is interesting and the empirical results seem promising.<|endoftext|>This paper proposes to combine sparse GPs with a neural network architecture to compute theinducing points locations associated to each input point. The inducing points are given by a mapping from the inputs provided by a neural network. The paper demonstrates improvement on training and prediction time while the proposed method is able to perform similar or better than the standard sparse GP approaches. Strengths:* Optimizing the ELBO for Sparse GP using deep neural network is interesting for improving the running time (computational cost)Weaknesses:* Training a deep neural network may require a large amount of data as opposed to the standard sparse GP. Learning such mapping may require a significantly large amount of training data. Suggestions:* One of the key advantages of GP against other supervised learning approaches is the uncertainty quantification. Minor points:* in the last paragraph of the introduction, the paper claims to have different sets of inducing points associated to each input location. However, given the network, it seems that each input will only produce a single set of inducing point, not a set of including points? The paper is currently in the border line.<|endoftext|>The paper describes a method that combines variational Gaussian processes with a neural network that produces inducing points and associated variational parameters depending on the evaluation location. The method also does a good job at respecting the (exact) GP prior structure on the latent functions. I think the idea here is quite interesting, and hope thinking about these points will make the paper more accessible! "To achieve this, we consider a meta point xtilde"   I still don t completely understand the role of the meta points (or what is meta about them)! It seems like they end up coinciding in the algorithm as implemented, but are conceptually distinct from the data points? In Algorithm 1, I think standard fonts (vs the math spacing) would be better for ELBO, KL_div, and Log_lk. But it s unclear to me how one would compute the approximate posterior covariance between two points, which is also sometimes useful. Though some points of the presentation could be improved, I think this is an interesting idea illustrated by good experience.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; This paper proposes a new method, Dual Multi Scale Attention (DMSA), as a plug in module for CNN backbone. The proposed method is a simple combination of previous methods, and basically, no substantial new technique is introduced. **(Unconvincing) Experiments*** The experimental results are too good to be true. While the idea is simple and has been explored by many related works, authors still achieve unbelievable improvements over all these vision tasks. **Presentation*** The writing is rather poor, as well as the presentation. "combined both these mechanism", "respectivly", "The channel wise attention weight of the multi scale feature maps are extracted ...", "comparision", etc. These simple mistakes show this paper is far from well prepared.<|endoftext|>In this paper, the authors proposed a new attention module, named Dual Multi Scale Attention. By introducing the new module (which is a simple combination without any novelty) to ResNet, it SIGNIFICANTLY improves the performance, e.g, 75.20 to 80.02  (and 76.83 to 81.54 for ResNet101) on ImageNet, 36.4 to 41.4 on MS COCO. This is a simple combination of multi scale and DANet. Could authors explain why a simple "multi scale + dual attention" idea could achieve such big improvements over all vision tasks? I would recommend strong reject for this paper, which introduces NO novelty and the experimental results are suspect.<|endoftext|>The paper proposes a kind of multi scale attention module capable at capturing spatial and channel attention informations considering long range dependencies. 6  Lack of analysis and discussion of the results. However, no mention and comparison have been conducted w.r.t U net models which are targeted to achieve multi scale representation and trading off local and global information.<|endoftext|>In this paper, the authors propose a new attention module that shows better performance and lesser computation than most existing attention modules. Results shows that the proposed network has some advantages than previous works. Better results are achieved on typical benchmarks. Overall, I think this work is very limited in its novelty. This work provides a new attention module to improve the performance of backbone networks. The main concerns are the limited techinical contributions and insufficient experiments.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; This is an interesting paper that proposed a number of technical advances (formulation of the compositional loss function and its optimization) with reasonable results, albeit there are weaknesses in the experimental evaluation. The proposed approach is termed compositional deep AUC maximisation which can be trained end to end.<|endoftext|>The details are missing in the paper. Weaknesses:Overall, this paper is a good work.<|endoftext|>However, there are no comparisons made to the results from this paper, which seem to report higher AUC results for many of the datasets (eg.CheXpert, Melonoma, PatchCam). Thus depending on how the data is partitioned, the same patient data will be in both training/testing sets, which may partially explain the extremely high AUC results.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper investigates if invariances learned by the model transfer across classes. The paper shows this is not true i.e.invariances do not transfer well to small classes, and suggest that improving this can help increase performance on imbalanced datasets. 3.The evaluation and experiment design is solid and rigorous. To me, it seems that bulk of the experiments are on quantifying their approach as a solution for class imbalance. However, if instead the authors believe the main contribution to be the evaluation and solution for transfer of invariances, numbers mentioned in point 1 above should be mentioned.<|endoftext|> Summary: This paper studies the problem of how well do neural networks transfer class agnostic invariances from head classes to tail classes. Weaknesses:  Despite the good motivation of this paper, the technical details of the paper are not satisfactory. For example, I think the authors confused the classifier with the entire model. In algorithm 1, you only mentioned the classifier updating, but I guess it means the entire model here. Unlike the head tail trade off in the accuracy, I don t think the learned invariances should be forgotten. Besides, I don t think the GIT is necessary to increase the invariances of tail classes and the current ablation study is too simple. It s much simpler than learning a generative model and I m pretty sure that it can increase the invariance.<|endoftext|>The paper investigates if robustness (or "invariance") to nuisance transformations which do not change the class label such as lighting, rotation etc are learned across all classes or if such robustness is sensitive to the class size. The paper demonstrates that such invariances seem not to transfer across classes: ie the classes with fewer examples suffer more. In the setting where performing well over all classes is required, thus changes P_test from P_train. Ie: optimizing *average* risk seems not the final goal here? Page 5: I m a bit confused what conclusion is now given by "Hence modeling the transformations directly is the more natural choice on imbalanced datasets". This paragraph seems to argue that a generative model does *not* suffer from class frequencies? Is this the goal of the paragraph? And if so, is that claim then true? The paper is incomplete. I think the question it asks is insightful, fundamental and important. Ie: in unsupervised settings there can also be an imbalance in the  semantics  as measured by the nr of samples (but no explicit class labels are used). It seems to me that the generative approach proposed later by this paper does not suffer from this effect?<|endoftext|>This paper works on long tailed or class imbalanced learning. The authors thus proposed to learn such class agnostic (in)variance via a generative model, and then use it to augment the training data of minor classes. They point out one direction for future research. 3.Overall, I think the direction the authors proposed to approach imbalanced classification is quite interesting. Generative models have not been widely used in this task. 2.When DR or DS is used, do the authors only apply the proposed augmentation to the final learning steps or the whole learning steps? Overall, I enjoy reading the paper as it is well motivated and well written. The authors only use small scale datasets, but not large scale ones like iNaturalist or mini/tiny imagenet. These can be found in Fig 7. It seems that the proposed method cannot effectively resolve these variances (see Fig 4).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; Strengths:(1) This paper is well written and easy to understand. In one batch, it is likely to have only one instance in some class. Also, I have a question about the unknown aware training objective. The unknown aware training object is similar to the dynamic focal loss with the virtual outlier. I agree that the motivation about sampling virtual outliers in the feature space instead of pixel space. Given that I am not very familiar with OOD, I would like to check other reviewers  opinions.<|endoftext|>The probability p(O|x,b) is modeled as sigmoidal activation of a generalized energy score of the classification head logits. The manuscript proposes a reasonable baseline for open set object detection. The artificial outliers are sampled from low likelihood regions of per class Gaussians which are fitted to latent features of groundtruth ROIs. It appears that the authors consider ROI wide representations of RPN candidates. These Gaussian distributions are modelled according to per class means (\mu_k) i and overall covariance (\Sigma). Sampling artificial training samples in the feature space appears effective. Reading the paper requires a lot of guesswork. It is not clear why \Sigma is used instead of \Sigma_k. Would it make sense to include OOD AP in Table 1?<|endoftext|>This paper proposes an effective method for OOD detection and model regularization, which does not rely on real OOD datasets. Specifically, the proposed method synthesizes virtual outliers by sampling low likelihood samples in the feature space of class conditional distribution, and adds a novel regularizer to the original ID training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. Specifically, what are the motivation and mechanism of the proposed generalized energy score? Weaknesses:A few details in the proposed method are not very clear.<|endoftext|>This paper presents an approach for detecting out of distribution (OOD) samples by synthesizing outlier samples and considering an unknown aware learning mechanism. The synthetic outlier samples are used to learn a tighter class boundary for in distribution samples. The model is trained with an uncertainty aware loss function that encourages a high probability score in distribution samples and a low probability score for OOD samples. Authors synthesis the OOD samples in the low dimensional feature space which is more efficient to generate than the actual image space. In this case, the object detector may miss a significant number of bounding boxes as some of the objects instances may not appear in the in distribution data. The authors proposed an OOD detection approach the relies on synthetic outlier samples.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This submission treats multi objective combinatorial optimization problems and aims to approximate the pareto set. The idea is to build a single ML model that represents the pareto set by providing a pareto set solution for any desired trade off. The authors prove that the pareto set is approximated well if individual trade offs are approximated well. In my opinion, the authors provide an interesting and novel approach to multi objecive optimization problems. Even representing the potentially exponentially large pareto set is a challenge, and is interesting that this can be done via an ML model. A concern could be that in order to achieve a good approximation of the pareto set, it seems that learning the single objective problem well is required (which often is a difficult problem in itself). Would it be possible to solve the single objective variants with a traditional combinatorial algorithm? The theoretical contribution of this submission is limited. A minor concern is that I found the tables with the experimental results hard to read.<|endoftext|>The proposed model is capable of predicting approximate Pareto optimal solutions from various preferences by a single model, via attention networks, and by a so called "hypernetwork". Experiment result on the multi objective versions of TSP, VRP, and KP shows the effectiveness of the proposed approach. Multi objective combinatorial optimization is a family of important problems but is really challenging to solve by traditional methods. This paper may inspire more ML researchers in this interesting direction. The proposed neural network MOCO method outperforms traditional approaches and other single objective deep learning baselines. The assumption that the model can generate $\epsilon$ dominate solutions for any preference seems to be non trivial for models with small enough $\epsilon$.<|endoftext|>The paper proposes a novel preference conditioned method to approximate the whole Pareto front for Multi Objective Combinatorial Optimization (MOCO) problems with a single model. According to the authors, this method provides extra flexibility for decision makers to directly obtain arbitrary trade off solutions without any extra search, which is a more principled way to deal with MOCO. The introduced method is novel, might be significant, and the quality of this article seems to be on par with other papers applying ML techniques to solve TSP published at top tier conferences (which are also cited in this paper).<|endoftext|>This approach uses a preference agnostic encoder along with a weight dependent decoder to generate approximate Pareto optimal solutions for any arbitrary set of weights of a weighted Tchebyshev scalarization at virtually no additional cost. The description of the proposed approach is hard to follow. 2.The use of the term "preference" is confusing. The title and abstract suggest that the proposed approach allows the incorporation of user preferences to focus the search on especific regions of the Pareto front, but that does not seem to be the case. In particular, scalarization based approaches are prone to explore the Pareto front unevenly. 4.The novelty in this paper is limited, as it reuses several ideas from single objective neural combinatorial optimization. The only novel (but key) idea is to make the decoder weight dependent. This paper proposes a novel approach for neural multi objective combinatorial optimization. However, it is technically sound and exhibits a very competitive performance in a broad range of problems.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; In domain generalization (DG), label set of target domain is that of source domains. This should be provided to verify that the experimental results are stable. This paper actually considers a more challenging problem: open set single DG (OS SDG) that extends the problem setting of DG to a more general situation. To address this very challenging problem, this paper designs a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. In general, this paper contributes a novel problem setting and a validate solution to this setting. However, some points should be clarified and strengthened in the revision. POST REBUTTAL The authors have addressed my concerns. In this paper, they propose a new problem setting and a new method to handle this challenging problem. + Introducing multi binary classifier to the OS SDG problem seems very interesting, since it may identify the unknown class region well. I would not like to make this problem can only be addressed by some heuristic methods.<|endoftext|>The paper presents a new task: open set single domain generalization, where only one source domain is available and unknown classes and unseen target domains increase the difficulty of the task. A new method CrossMatch is proposed to solve this new problem. Firstly, auxiliary examples are generated for unknown classes out of the source classes. Then, the paper proposes a cross classifier consistency regularization that minimizes the multi binary classifier s output and one vs all multi class classifier s output. Strength+ OS SDG is an interesting and realistic problem, and the paper clearly described its difference from existing methods.<|endoftext|>This paper proposes a new method called CrossMatch for open set single domain generalization where only one source domain is available to train the model. In particular, CrossMatch designs a new strategy to generate auxiliary samples for unknown classes and develops a novel consistency regularization to help identify samples from unknown classes in target domains. Results on several datasets verify that the proposed method achieves impressive results on three widely used datasets. The results of the variant "L_unk^ +L_unk" are missing in Table 4. 3.Also, the results of open set domain adaptation are missing in the experiments, which are vital and could be considered as the upper bound of this new studied problem setting. However, CrossMatch is mainly built on several previous methods, e.g., the multi binary classifier (Liu et al., 2019; Saito & Saenko, 2021), adversarial data augmentation (Volpi et al., 2018; Zhao et al., 2020a), making the overall novelty incremental for ICLR.<|endoftext|>This study tackles a novel domain generalization task, called open set single domain generalization, in which a model is to be trained with single source domain but needs to generalize well at unseen target domains that may include unseen classes. To solve this problem, the authors extend single domain generalization methods to be able to learn detection of unseen classes by adopting adversarial data augmentation. Experimental results show that the proposed scheme can facilitates the capability of existing methods on detecting unknown class data. Strength  Open set single domain generalization is a novel, challenging, but practically important problem setting. Weakness  I have several concerns regarding the design of CrossMatch. Do we need to assume that all target domains share the same label space? Considering the motivation of cross classifier consistency regularization, it would be better to stop gradient for F_b. Although open set single domain generalization can be seen as SDG + open set classification task, the authors only use SDG methods as baselines in the experiments. The arguments of the loss functions sometimes change in the manuscript. Open set single domain generalization is an interesting and important problem setting. However, the proposed method is not approapriately designed for this problem setting.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; Based on the observation in Cho&Hariharan that student degrades by oversized teachers, this paper proposed an adaptive temperature solution. A new metric called sharpness is introduced to quantify the teacher student gap. Experiments are done on CIFAR100 and ImageNet. 2.Table 4 is supposed to be resnet18 as student and resnet34 as a teacher but ATKD with 73.01 is from teacher resent50. The intuition of this paper is clear but the method proposed is not well presented, making understanding this work is hard.<|endoftext|>This work raised several interesting questions. The limited capacity of the student may not be a candidate cause since the student model can multiply a large number to the logits to make the sharpness score large, thus reducing the sharpness gap. 4.Would the proposed sharpness score and observation generalize to other cases? The observations in the paper are mainly based on varying the depth of the model. The work provides an interesting insight into KD by investigating the sharpness gap between the teacher and student model. The ATKD method built based on this insight is simple but can significantly improve the KD results.<|endoftext|>StrengthsThe authors:(1) Provide a new perspective to explain the degradation problem of KD. WeaknessesThe authors should provide more analysis:(1) What is the best sharpness gap between the teacher model and the student model? It would be better to contrast the sharpness gap of the best student with that of ATKD. The authors should provide more analysis to demonstrate the effectiveness of this method comprehensively. I would recommend this paper for ICLR2022.<|endoftext|>The paper proposes to mitigate the performance degradation by controlling the sharpness gap between a large teacher and a student model. During the training, the temperature is set according to the sharpness of the logits. Strengths:  The proposed method can effectively reduce the sharpness gap between the teacher model and the student model and achieve better performance when a large teacher model is used. However, some detail and assumption are not well presented in the paper.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper is not easy to follow due to complicated notations. If this is the case, Theorem 2 implies that the adaptive regularizer $L_i$ becomes a diagonal matrix. 5.Theorem 2 requires the matrix $X$ has positive elements, which is a very strong assumption and may not hold in practice.<|endoftext|>Also, it is not clear that the experiment using fixed L at different iterations is so useful. Effective heuristics for regularization parameters are chosen so that they do not need to be tuned. It is also not clear if overfitting can become an issue with enough iterations.<|endoftext|>Based on these factors, my current score is a 5. There is also no discussion on why the network is chosen in this particular fashion. What is the intuition for this? In that case, however, the laplacian was not learned, which seems to be the crucial difference.<|endoftext|>##########################################################################Cons: * Eq.(1) is an optimization problem not a model. In Theorem 2 at the end, I would rather write: "D is a constant which equals [...]". Howeever the presentation of the paper is too confused for publication.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The exact setting of this paper (policy fine tuning) is presented by Xie et al.2021.**The message of the paper is not new. **The empirical comparisons are not fair.<|endoftext|>compared with the purely online setting, it is beneficial to use the offline/batch data to warm start. The offline online setting is interesting, but it is not novel, as there are some work studies this, such as [1], [2]. The empirical experiments are valuable but too specific to the particular algorithms, DQN and TTN. QT Opt: Scalable Deep Reinforcement Learning for Vision Based Robotic ManipulationStrength:  The paper studies an important setting that combines online and offline RL, which seems more closely related to some real world applications, while typically a large amount of historical data exists and could be used properly to warm start the algorithm. It is interesting to see the offline learning part based on some of these STOA algorithms, though a similar conclusion might hold there.<|endoftext|>(i) The proposed algorithm is a simple modification to an existing algorithm (Two Timescale Networks), and therefore the algorithmic contribution is limited. (ii) The empirical observations are not supported by theoretical analysis.<|endoftext|>The proposed algorithm, offline online TTN, learns by separately learning a representation, and values given a fixed representation, on two different timescales. However, I am concerned with the novelty of the paper. [1] https://arxiv.org/pdf/2006.09359.pdfOverall, I feel that the novel components of this paper compared to existing work in offline online RL is not clear.
Reject; rating score: 3; rating score: 3; rating score: 6; The method was evaluated with multiple datasets. The authors attempted to analyze the model performance vs the annotation burden. I would suggest the authors do some annotation cost study in a fair setting. But, the annotation costs are all estimated using different sources. This paper only provides one set of randomly sampled image pairs, but does not analyzes how the model performance will be affected by the randomness.<|endoftext|>The weakly supervised setting could reduce the annotation cost and still achieve acceptable performance in several object counting benchmarks under specific conditions . I hope the authors could address my concerns listed above and I think we could not accept this paper at current stage.<|endoftext|>If this is a saturation point for the accuracy it should be clearly mentioned in the paper otherwise the authors should provide the effect of increasing N in the final performance for some of these datasets. I think the paper could be stronger with more experiments including dataset size vs. accuracy plots which show where the accuracy plots saturate. Most of the points raised by the reviewers are just and important.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; Considering the problem of adversarial manipulations of graphs, the authors propose a framework for "cleaning" the graph structure and its features to obtain more robust predictions. Or put differently: The graph cleaning could then be used in combination with ANY standard GNN.<|endoftext|>In summary, this paper studies a robust GNN against adversarial attacks on both graph structure and node features.<|endoftext|>The authors propose an interesting idea, to jointly sanitise the graph structure and the features as a defence against adversarial attack. Although it should be noted that the Pro GNN framework does consider both the graph and the features [2].<|endoftext|>The paper proposes a robust model for graph neural networks (GNNs) to defend against adversarial attacks. Negatives:  The novelty of the paper is not clear.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper presents a CVAE based approach for conditional image generation, in which a pretrained VAE is utilized to speed up the training process. The paper clearly demonstrates the benefit of using a pretrained artifact in terms of time/resource of training as well as the image generation quality and diversity. Regarding the Theorem 3.2, which only applies if the artifact are learned on the same dataset as the conditional VAE is trained on: Although it has been empirically shown that pretraining on a different dataset is still "useful", experiment in Fig.4 suggests that the gap to the scenario where IPA pretrained on same dataset and on ImageNet is large, especially for the datasets that the domain gap to ImageNet is large. It would be great to have authors  view on feasible ways to bridge such gap when using pretrained artifacts? Generally,  Results are impressive, given the training budget; the method achieves near state of the art performance in terms of visual fidelity and even better in terms of diversity.<|endoftext|>The paper proposes a method of leveraging state of the art VAE decoders (foundational models) to create new conditional VAEs by training a new encoder for prior of the latent space. The authors demonstrate quite convincingly that the new conditional VAE can generate images with more variation than a state of the art GAN while achieving a comparable quality of image. They then propose a task in the domain of X ray imaging where the improved variation leads to improved performance in targetted X ray measurements. The paper presents a simple, but nice idea. The use case scenario seems reasonable. It clearly works and I may have just missed the explanation, but it is rather unclear to me what the objective described in Equation (7) does. I am therefore going to score this lower than I would otherwise. If you can clarify this either by showing me the explanation that I have over read or by adding a short explanation (which given the page limit could be in the supplementary material), then I would be happy to reconsider and increase my score. Overall this is a strong paper. I am happy to increase my score if I can be satisfied on this point.<|endoftext|>This paper proposes a method, IPA, that converts an unconditional VAE to a conditional one by reusing the pretrained weights and training a partial encoder. Experiments on the image completion task show favorable results compared to GAN based approach. The comparisons to CoModGAN on completion tasks with very little observed input area are very interestingWeak Points:1. An example in the unconditional case could be found in equation 1 in [a]2. LPIPS GT would be a good metric for measuring the mode coverage of the training targets, but cannot capture diversity that well. Clearly, the latter is more diverse but the two scenarios will have the same LPIPS GT score. Though the authors pointed out    that the focus in that paper is different, the novelty in the technical formulation is not significant. In appendix B2: how do you get from equation (16) to (17)?<|endoftext|>In this paper, the authors focus on training conditional variational autoencoders. The approach effectively infers the latent variables of the original unconditional VAE given the new conditioning input. They demonstrate the method on a number of datasets, specifically focusing on image completion. They show competitive performance with adversarial approaches. They also demonstrate an application in the realm of medical imaging. 2.The approach makes intuitive sense; it follows that pieces of an unconditional generative model can be used to build a conditional one. In addition, while quantitative results are competitive with respect to GANs, the performance margin is not particularly exceptional. While there is pretraining on the ImageNet dataset, it would strengthen the paper to evaluate on the ImageNet dataset in a manner like CIFAR 10 and FFHQ 256. First, the technical and empirical novelty is limited. In light of the other reviewers concerns and the response from the authors, I upgrade my score.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The idea itself is simple but there are enough supporting materials in the paper. This paper also reveals that  head part only training is enough for personalization and this leads to faster learning speed.<|endoftext|>Nevertheless, there are some issues. Second, I am not entirely convinced that the empirical evolution of the baseline methods is fair. The analyses in the paper are exhaustive and indeed support the author s claim for the most part. The paper is written clearly and easy to understand.<|endoftext|>The paper provides an interesting suggestion in the field of federated learning. Overall, I vote for accepting. The authors show the proposed method is efficient especially when the level heterogeneity of the data is more intense. 2.The proposed training scheme is novel and very easy to implement.<|endoftext|>## Major Comments* The paper makes some very strong claims which I don t find to be fully supported by the experiments. Overall, I feel that the paper is promising, but empirically underpowered to support the claims it is making. * On p.5, the claim that global training pays attention to "unnecessary and confusing information for personalization" seems like an incorrect characterization.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper presents a g data efficient approach that encourages invariance to both data and model stochasticity that works for both classification and regression tasks. Furthermore, the proposed minimax loss function can specifically enhance invariance to model stochasticity. The extensive experimental results verify that the proposed method is effective. Strengths:*  Data efficient regression is a very interesting direction. Most previous works on data efficient methods focus only on classification setups. For example, pseudo labels, which is proven to be one of the most effective methods for semi supervised classification, cannot be exactly defined in a deep regression problem. This paper sheds some light on this new direction. * The proposed method seems to be simple but effective. Weaknesses:* One missing related work on pseudo labels [1]. It would be great to include some discussion or even add as a baseline in the classification experiments. * It is unclear whether the proposed method can generalize well to more challenging tasks (for example, depth estimation or larger dataset). Though, I m satisfied with the current evaluation (from a single value toy dataset to dense value prediction datasets). CVPR 2021. In general, the proposed method is well motivated with sufficient experimental comparisons. To this end, I believe this paper has enough merit to be accepted.<|endoftext|>This paper proposes the so called Chi model, which combines data augmentation with a two headed network architecture to include model stochasticity. Easy to read, experiments clearly described. Approach is fit for regression as well. ## Weak pointsTwo heads which i assume do not share weights are similar to an ensemble and can hence be compared with ensembling based approaches. Even more if comparing with a MC Dropout based ensemble, where the dropout might be applied in the final layers only, hence effectively creating an ensemble with a backbone with shared weights. Training one model on the label provided by another model is known as co training [1] and has been applied for Semi supervised learning (SSL) also recently [2] e.g.on CIFAR100 results, especially w/o data augmentation would aid in judging the contribution of individual components of the proposed approach. ## QuestionsDo both heads of the chi model have the same architecture and do not share weights? ## Minor commentsEvaluations w/ and w/o data augmentation especially when comparing with other methods would be helpful. 1998.[2] Qiao, Siyuan, et al."Deep co training for semi supervised image recognition." Approach seems promising, especially the empirical results. I am lacking a bit of information to set this into relation to previous works as outlined above. Without these details it is hard to judge how much can be attributed to the proposed approach.<|endoftext|>Experiments are conducted on various tasks like age estimation, key point localization, and object recognition. * The paper has good flow, it builds from the existing literature, and first points out the drawbacks of the existing methods, which then motivates the proposed X model to tackle those issues. * Extensive experiments on both classification and regression tasks show the ability of the proposed method’s ability to exploit strong data and model stochasticity, resulting in enhanced performance. It is shown that the proposed method is able to outperform all the methods compared in the paper. Concerns* One of the major concerns is regarding the technical aspect of the proposed approach. It seems to be very similar to another method proposed for semi supervised domain adaptation [1], which is also proposed to improve the data efficiency of the model. Furthermore, the overall idea to enhance model stochasticity has a lot of similarities with [1], [2], [3], [4] which are not mentioned in the paper at all. Hence, it is essential to have the method proposed in [1] for comparison and needs to have a discussion on the differences between the proposed method and [1]. Another experiment to consider would be Mean Teacher + [1] which would be almost the same as the proposed idea. * The paper does provide an analysis with the UDA method, not sure why semi supervised DA methods are not considered as it is more similar to the proposed problem setup than UDA, having such comparison would strengthen the claims provided in the experimental section. [1] Saito, Kuniaki, et al."Semi supervised domain adaptation via minimax entropy." 2018.[3] Saito, Kuniaki, et al."Maximum classifier discrepancy for unsupervised domain adaptation." 2019.My concerns are primarily due to similarities to some of the works in semi supervised DA techniques. Considering that the paper addresses the problem in a more general setting of classification and regression, I am leaning towards accept for now.
Reject; rating score: 3; rating score: 5; rating score: 6; The technical contribution over the vast amount of other work on disentangling latent representations is clear but perhaps slight and therefore the applied angle seems to be emphasized (indeed, ‘surgery’ is even in the name of the model) but, apart from using some relevant datasets and showing some qualitative results, there is almost nothing about the applied context that seems to influence the design, nor the analysis of the results beyond the standard ML metrics. Gaussians are used w/o consideration, but this is as usual so it is not taken as a negative.<|endoftext|>sVAE was evaluated using two private datasets and one public dataset, and showed its efficacy on both classification tasks and regression tasks. 1) Regarding reproducibility, an experiment using a public dataset is more important. Thus it would be better to have MIMIC III dataset results in the main text rather than one of the private datasets. Thus the authors should at least discuss generalization of this approach.<|endoftext|>The authors also suggested a new measure of correlations between the latent variables and  to use that instead of TC. Was this consistently observed in other datasets? 2.There are two hyper parameters introduced in this method, but the authors did not mention the reasons for the choice of the hyper parameters. However, it was a good application of VAE for an important clinical question.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; **List strong and weak points of the paper. Be as comprehensive as possible. * Data quality is often a major problem and solved with overparametrized models. This paper is a valuable and interesting contribution. * I think it s an important contribution to raise awareness for the limitations of only one predefined validation set. * The empirical behavior off DIVA is only shown on image data. **I think this is a valuable and interesting contribution and I vote for accept. **Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.<|endoftext|>This paper adds a new tool in the arsenal for figuring out which data is best for a given learning task. I believe the focus of this paper would be of interest to a broad audience. The Dataset Extension, Curation, and Reweighting capabilities would also be of great use for models trained on tabular data. A relationship that I would like to see the authors explore in the discussion section is overfitting of the model. Ruling this out would make DIVA appear stronger, as this would mean it is impossible to improve test set accuracy by simply feeding a model the most canonical images during training.<|endoftext|>This paper proposes a method for dataset optimization that learns sample weights without a separate validation dataset. In addition, this paper uses leave one out cross validation (LOOCV) loss instead of validation loss on a separate dataset, which enables to conduct evaluation without a separate validation dataset. Although the authors conducted experiments in multiple tasks, it is not sufficient. It is unclear which loss was used ($L_{val}$ and $L_{LOO}$)  in the main paper. $z_i$  > $x_i$ in eq.(13)?in Figure 4, the meaning of blue and red lines would be oppositeAlthough the motivation of this work is interesting and the paper has some technical novelty, experiments can be improved. Thus, I m slightly leaning towards the reject side at this moment. After rebuttal Thanks for the response.<|endoftext|>Important points for the rebuttal are marked with a (*). Methodology: The proposed experiments in the paper all use the deep network as a feature extractor and the last linear layer of the network to make decisions on classification. Comparison to baselines: A number of important comparisons to baselines / discussion around those approaches seem to be missing in this current work. I think this work should definitely be compared to the current framework. This would help to contextualize how much benefit we have from the current approach.
Reject; rating score: 3; rating score: 3; rating score: 3; Experimental results show a modest improvement for existing approaches such as BERT and DistilBERT. I was easily able to follow the paper, yet I am unable to understand the contribution of the work. To me, emotion detection is a more challenging task than sentiment classification (the author also highlight this), whereas the proposed work is employing emotion classes to improve sentiment classification. Moreover, the way to incorporate emotion information in the existing transformers is straight forward. ##########################################################################Questions for rebuttal: Feel free to answer to all the concerns.<|endoftext|>This paper proposed to add special emotion tokens and embeddings to pre trained BERT models to improve performances on the sentiment classification task. Most of the related works are not closely related. Strengths:  The proposed model is simple and easy to reproduce. The usage of citation styles is not correct in most cases.<|endoftext|>The paper requires significant improvement on all fronts including experimental design as well as paper organization. This paper proposes a simple way to supplement emotions related information to a downstream classifier. ### Strengths:  The authors propose an intuitive, easy to implement way to provide additional information in the form of emotion words to improve a downstream sentiment classifier. ### Weaknesses:  The Paper lacks a research question that the authors are trying to answer. The paper in the current form does not thoroughly answer a single research question, hence I don t think it s ready for publication.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; Fig1, caption: "the gradient updates for the 3D coordinate long with the updated atom"  > "the gradient updates for the 3D coordinate _along_ with the updated atom"  p.5 "perspective of neural energy minimization, these are equivalent to optimizing an energy function that depends on both the atom presentations and inter atomic distances just like our basic"  > "perspective of neural energy minimization, these are equivalent to optimizing an energy function that depends on both the atom _representations_ and inter atomic distances just like our basic" ? The paper has strong empirical results. I went with a lower confidence score as I have not gone through the proofs in the appendix in detail and only have a higher level knowledge of some of the pieces of related work. For instance, is it because of the unrolled optimization routine, different architectures, the optimal transport loss, etc.? Could this be teased out with an ablation study? ii.The number of time steps (i.e.the max value of $t$) used/required in the different experiments is not very well discussed/evaluated. "_ for the empirical novelty and significance. How can one choose this value? # 3.Clarification Questions to the Authorsi.<|endoftext|>Empirical results demonstrate superiority of the proposed method. 2.The proposed method outperforms the existing methods in the experiments. First, I am not sure if the proposed method can be called an "neural energy minimization" method and an "unrolled optimization algorithm." I think the authors should add a new terminology to denote the set of atom features. Maybe the authors could make minor clarifications or changes to clarify this aspect? 2.Additional experimental results can strengthen this paper, especially for future works to build on this paper. Minor:1.Given the training scheme used in the paper, it seems that the DNN based "molecular conformation generation" methods are applicable to "molecular conformation optimization".<|endoftext|>However, many of the mentioned approaches do not rely on distance matrices as intermediates. However, this difference between model evaluation should be clearly stated in the main text. G SchNet, for example, uses an iterative approach based on factorized probabilities to directly generates a new 3D structure. It would also be helpful to provide an in depth discussion on the parallels between ConfGF [Shi 2021] and the present approach in the related works section. Nevertheless, it would still be helpful to know how the labels for these experiments were generated. One of the main points of criticism is the proposed neural energy minimization framework for constructing equivariant networks. It is well known, that the Cartesian gradients of functions are one way to systematically construct equivariant filters for interactions/updates. Due to the two latter points, I tend towards rejecting the work in its present form. Most of the time, heuristic approaches as e.g.used in RDKit followed by ab initio optimization are used.<|endoftext|>Although the main claim of the work is energy minimization, I don t find any relation between the mathematical formulation and energy minimization. This is not clear to me. For example, how many steps are used in experiments? Are the final results sensitive to the number of steps? Concerns about experiments    In Section 4.1, Table 1 shows that the three atom model is better than the two atom model for conformation optimization. Does this mean the proposed model can only work well with a good initialization?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper proposes an incremental improvement to existing methods that encourage monotonicity through a regularization term. The contribution of the paper is about how to sample the data to compute this regularization term, which is an expectation w.r.t.a data distribution. The monotonicity constraint is often overlooked by the ML community, but it does play an important role in many real world applications. Did the authors simply interpolate between 0 and 1? Why we need to use both interpolated examples as well as random examples (not clear from the paper)?<|endoftext|>The results of Section 3 are interesting for applications were partial monotonicity is desired but is not a hard requirement (e.g.for robustness and interpretability). I think, therefore, that it would be good to complement the local $\rho$ with an estimate of the probability that Definition 1 would not hold over the distribution in question (training, test or random). I found the paper written clearly and fairly easy to understand.<|endoftext|>This paper proposes techniques for using monotonicity both as a requirement and as a regularizer. I believe that this model somewhat undermines the usefulness of the models described in the paper. I think the paper should make more clear why we would prefer to use the methods described over a model such as Monotonic Kronecker Factored Lattice given that the model does scale well wrt the input space. This is not to say that the methods described are not useful; rather, this is to say that it should be made more clear why the methods described are as useful as claimed.<|endoftext|>Clarity:  Overall, the work was clearly written with a few minor typos here and there. Significance:  The significance of this work, in my opinion is entirely in the new group monotonicity property. I didn t see an explanation of the choice of tau? An interesting paper that fills some gaps in existing work and posits some interesting applications of monotonicity. I feel that the second approach needs some better motivation and possibly a better application    I would prefer that you drop the first approach to include it.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 5; As the success of Vision Transformer ViT has shown its potentials for computer vision tasks, this paper investigates a more effective way of training a ViT understand a standard ImageNet pre training setting such as no extra training data and no strong data augmentation. In general, without those conditions, a typical ViT can not perform as good as widely convolutional based network architectures such as ResNet. With the proposed SAM, ViT can achieve better accuracy significantly understand standard ImageNet training/testing protocol. In addition to the improved performance on ImageNet, this paper further shows the visualization of the attention map and the improved performance on other applications such as contrastive learning and adversarial training. This paper is well written and organized. The motivation comes from the loss landscapes comparison for different network architecture such as ResNet, ViT and Mixer. Such that this paper adopts SAM as an answer. Although, SAM is from existing work, this paper gives a good explanation on why among different optimizer, SAM could be a good choice. In addition to the method, this paper has shown that on image classification,  contrastive learning and adversarial training ViT can be consistently improved with SAM. It could be the problem from the usage of existing optimizer. On the other hand, the problem may also comes from the nature of current ViT design. Recent vision transformers such as SWIN, PVTv2, VOLO, all of them has clearly shown that by optimizing the architecture of ViT, it can also avoid the local minima. Based on current manuscript , it is unknown that how SAM can be used for SWIN or PVTv2 and whether SAM can be used as a general optimizer for arbitrary vision transformer training. In the experiments, ResNet SAM results are promising, the claim could be stronger if other vision transformers can be evaluated as ViT has been widely argued for its inefficient architecture design.<|endoftext|>The authors analyze the effects of sharpness aware minimization (SAM) when applied to vision transformers (ViTs) and MLP Mixers. They then show that SAM improves the performance of ViTs and MLP Mixers in a variety of image classification scenarios including adversarial attacks, naturalistic corruptions, contrastive training, and transfer learning. They also examine the effect of SAM on activation sparsity, activation maps, and the relationships between different model architecture components and loss surface sharpness. I am convinced that SAM works well for ViTs and MLP Mixers. My main concern about this paper is that it is insufficiently rigorous. I am not confident that this work is suitable for publication in its current state. However, I am optimistic that it could be suitable for publication after revision. In **Section 3. This claim seems speculative, and should be tested. ConViT (d’Ascoli et al., 2021), for example, has a convolutional inductive bias but is completely unconstrained in that it can learn to be a vanilla ViT. The authors could examine the effect of SAM on other architectures with varying (and controllable) inductive biases, such as ConViT (d’Ascoli et al., 2021), CvT (Wu et al., 2021), CMT (Guo et al., 2021), a ViT with a convolutional stem (Xiao et al., *Early Convolutions Help Transformers See Better*, 2021), or a CNN reparameterized into an MLP á la (d’Ascoli et al., *Finding the Needle in the Haystack with Convolutions*, 2020). The effects of SAM on activation norm (Table 3) in the ViT are not consistent, so I think the authors should avoid making the claim that “we find that the norm of the post activation value...become even bigger” (*Section 4.4 Greater Weight Norms*). >By analyzing some intrinsic model properties, we find that the models after SAM reduce the Hessian eigenvalues by activating sparser neurons (on ImageNet), especially in the first few layers. The authors need to conduct experiments showing that SAM causes sparsity and sparsity causes a reduction in hessian eigenvalues. This is a very interesting idea, and should be better explained. This distinction between average  and worst case curvature is an interesting result. I think the authors should extend their analysis and assess the average flatness of other models with vs. w/o SAM. I understand that there are space constraints, but the ViT sparsity results should be depicted in a figure. Perhaps consider combining Figs. MLP Mixers and ViTs are more difficult to train than CNNs.<|endoftext|>The paper presents an application of sharpness aware minimization (SAM) to training of visual transformers (ViT) and MLP Mixer. The idea of the paper is that models with weaker inductive priors (transformers and MLP Mixer) suffer from sharp local minima more, than CNN models and SAM fixes this issue, also significantly improving test accuracy on various version of ImageNet. The paper is experimental, non theoretical kind, mostly studyng the effect of SAM on ResNet, ViT and MLP Mixer training and its interplay with dataset size and augmentation. All sota methods for image classification rely on the augmentations  to provide good results and it is not easy to come up with new augmentations. The most papers, which present “learned augmentations” in fact just learn some parameters and combinations of _known_ augmentation. However, for other domains, like NLP, or some kind of medical data, it is not clear, how one could augment the data. If one can instead just take general purpose architecture (transformer) and train it with general purpose optimizer (SAM), that is a big deal. The experiments are done rigorously and with care. The only possibly missing experiment would be to train a model without an inductive bias at all, like vanilla MLP and check if SAM would greatly improve its results or not. The paper also states the limitations of the approach, the main of which is computational costWeak points:   In the current form (I was asked to review a paper after the rebuttal stage), I do not see any major weaknesses. The paper tackles a single problem and studies it extensively. The problem in my opinion is significant and the paper would likely have an impact. Therefore I vote for clear acceptance.<|endoftext|>The main contributions of the paper are: 1 ViTs with existing sharpness aware minimizer (SAM) outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large scale pre training or strong data augmentations. 2 The motivation and experiments, including the ablation studies are well organized and adequate. The paper reveals that the ViTs and MLP Mixers have extremely sharp local minima of converged models through loss landscape visualization and Hessian dominate value. Currently, the augmentation and large dataset pre training are vastly employed in the entire community. The impact can be enlarged by conducting experiments on ViT L and comparing ResNet152x4 pretrained on BiT L.The paper is pretty well written with extensive results and figures. The impact of the paper can be largely improved if the large scale experiments can be conducted and a better accuracy than the current state of the art methods can be achieved.<|endoftext|>This paper has demonstrated that the sharpness aware optimizer could be leveraged to boost the performance of ViTs and MLP Mixers without pretraining and strong augmentaitons on different tasks including supervised, adversarial, contrastive, and transfer learning. + The paper has conducted an abundance of experiments on different tasks to demonstrate the effectiveness of the sharpness aware minimization (SAM). + The authors present a comprehensive analysis on SAM. The differences and similarities between SAM and data augmentation are well explained. Besides, the authors demonstrate the effectivenss of SAM in various tasks by showing a large amount of experimental results. The experimental part is comprehensive and convincing. The major drawback of this paper is that the SAM is an existing idea, even though the authors use it to overcome the issue brought by heavy pre training and data augmentations. Therefore, this paper is like proving the effectiveness of an existing idea by doing many experimental validations.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The authors point out that for one class, this is equivalent to making the new class vector (w) to be the normalized average of all the feature vectors for the new class. In effect for each new class, a weight vector is generated by computing the normalized average of the feature vectors for examples from that class. They consider different settings   joint and disjoint, and evaluate on a few different datasets. 2) There is little detail about the triplet/contrastive methods used in the comparison. A lot of space in the paper is given to The Pl@ntnet dataset. In section 4.5 the authors compare their method to incremental learning. (In this case by training a new layer to the new classes whilefreezing all other weights). I like the method   it is simple and intuitive, and the initial results appear impressive. 2) what happens when a set of new classes are very similar to a known class?<|endoftext|>The paper presents how the normalized softmax loss can be used on the open set problem. Many papers addressing this problem are ignored. Basically only two metric learning approaches are introduced. "Recent advances in open set recognition: A survey." The proposed method is loosely motivated and the comparisons are somewhat limited. 3) Paper contributions are not highlighted, difficult for a reviewer to understand the novelty of the paper if any.<|endoftext|>This paper is about classification of images in an open set setting. Data coming from new classes are introduced to the network after training on data from a fixed set of known classes. The authors chose to have metric learning methods as a baseline and compare their classifier with triplet/contrastive networks. The method proposed by the authors seems to be an easier setting than the method proposed in the stated paper since they are classifying in a zero shot manner (the new classes are not seen in training). It would have been interesting to compare with state of the art incremental learning method or the newest metric learning method. It would have provided a strong support for claiming that the authors  work is new and performant. I am not convinced of the novelty proposed in this paper and there are too few comparison with existing work to strengthen the validity of the performance on the proposed datasets.<|endoftext|>+ The proposed approach is simple and efficient. #### Weaknesses:  The paper states that (deep) metric learning approaches (DML) suffer from high computitational complexity due to reliance on pairwise learning constraints. However, many approaches in DML effectively circumvent the computational burden using sampling strategies [e.g.A] or class proxies [e.g.B], etc.In particular the latter has shown to be both very computationally efficient and competitive with the state of the art in DML. The class weights are set to the average over inferred weights based on few representatives of the new classes and subsequently fixed for classifiying unseen samples. Only Tripet and Contrastive Learning seems to be considered – however, reference about the exact implementation details of these approaches are missing as well. Evaluation on the PlantNet dataset lacks comparison to other approaches/baselines. The paper lacks novelty and significance both for the presented approach and the empirical results. Further, the quantitative evaluation is based on non standard datasets, evaluation protocols, architectures and baselines, hence does not provide a proper basis to evaluate the effectiveness of the proposed approach.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; Specifically, the memoised wake sleep algorithm requires computing the joint probabiltiy of a discrete latent variable and an observation, which would require integrating out any continuous latents. **Strengths*** The paper deals with a relevant issue   how to extend an existing approach to hybrid discrete continuous graphical models. It is made easy to see where the authors  work fits in to previous work. * The proposed method is principled and based on a well established method from approximate inference (importance sampling). * Experiments are favorable towards the approach and show that it is better suited to program synthesis than other applicable methods. **Weaknesses*** Somewhat weak in novelty. * In theory we could approximately integrate out the continuous variables using other methods. The main weakness I see is that the contribution seems to be limited to using importance sampling for marginalizing out continuous variables. The actual presence of continuous variables can be seen in the Gaussian mixture models experiment of the memoised wake sleep paper [1], where it was possible to treat them analytically. Of course, the general case doesn t allow for that and therefore the paper is still making a valuable contribution by investigating what to do in the general setting. The experiments show that the proposed approach has an edge over other general methods in this setting. [1] Hewitt et al., "Learning to learn generative programs with Memoised Wake Sleep", https://arxiv.org/pdf/2007.03132.pdf<|endoftext|>However, memorised wake sleep can currently be used only on models that are purely discrete. This paper extends memoized wake sleep by providing a mechanism for using VI to integrate over continuous latent variables. Good paper, giving an interesting new method. My main concerns are around the empirical evaluations and wallclock times. In particular, I can easily see that memoization in MWS is going to significantly help runtime. This isn t so clear in the present algorithm, because there s an awful lot more going on (all the steps involving continuous latent variables don t seem to benefit much from memoization). Could the authors clarify why their method should help runtime (or whether it won t, and its primarily intended to help performance), and include wallclock time comparisons? Intuitively, it would seem that HMWS is doing something very similar to RWS, but memoizing the best samples from the approximate posterior over latent variables. That would seem to help, but it isn t clear to me that it would help _that_ much, as I wouldn t expect the best samples from the approximate posterior to be that different from typical samples. This is especially concerning, given that:* There is no hyper parameter optimisation (Adam with default hyper parameters).<|endoftext|>The paper extends the previous memoized wake sleep method to train complex generative models with discrete/structural and continuous latent variables. The method incorporates an assumption on the conditional dependence between the discrete and continuous latent variables and essentially employs a clever importance sampling procedure to capture the dependence. In general, I think the method is an interesting and valuable contribution, I also enjoyed the precise notations the authors adopted. The paper is clearly written with good motivation and methods sections. As a machine learning paper, I do not find the comparison with VIMCO and RWS to be sufficient. Further, although the two problems in the experiments involve reasonably complicated generative models, these models are not the main contributions of the paper, and they are arguably low dimensional in latent space. For example, in the second experiment, there are only 4 discrete locations among other latent variables. Again, as a machine learning paper, the question of scalability is not well addressed. 2.The fact that the problems are relatively low D raises two further questions: how does this method scale to larger latent dimensions? Either of these can be augmented to RWS for improvement. How do these two contribute to performance? I am not asking the authors to "just compare more baselines", but to conduct a more careful comparison between contributions of various components in their model. How does this method not suffer from this problem? Why does this make sense?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; ## Opinion & RecommendationI am in support of accepting this paper. The paper makes a nice connection between meta learning and variational dropout, resulting in an overall elegant approach. The use of dropout for adaptation of a model to different tasks is (as far as I am aware) novel, which would already justify publication. I recommend acceptance of the presented paper.<|endoftext|>The authors present an interesting model based meta learning approach, Neural Variational Dropout Processes (NVDP), by constructing a conditional dropout posterior to predict task specific dropout rates of model parameters, conditioned on the observations. They show the NVDPs improve the model s adaptation, functional variability and generalization to new tasks empirically in the experiments. I recommend this paper to be accepted, as its novelty on the newly developed variational prior differs from existing works. This is based on the hypothesis that the conditional posterior model to approximate the optimal prior shall depend on the whole data set.<|endoftext|>The paper proposes a novel Bayesian meta learning approach to model conditional posterior distribution via an amortized variational inference framework based on task specific dropout. + The experiments are comprehensive. + The reasoning of the novel prior is clear and understandable.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The paper proposes to approach the generalization prediction problem in the domain of big neural networks using the classical notion of leave one out (LOO) error. "Deep multiple kernel learning."<|endoftext|>This paper studies generalization of deep learning with LOO error using neural kernels. Studying the generalization ability of deep neural networks through LOO error is an interesting direction opened up by correspondence to kernel methods.<|endoftext|>As examples of such models, the authors review certain variations of neural networks. The spiking effect of the generalization error is derived from the closed form solutions of the leave one out computations for ridge regression based methods. I find the idea of considering the double descent behavior emerges from the closed form leave one out formulations to be interesting and it may deserve more attention.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper proposes a new method for l0 CCA using stochastic gating which allows for an efficient algorithm and also permits a deep version of the l0 CCA. Comments:Originality & Quality: The paper proposes a stochastic gating based approach for l0 CCA which uses ideas from a couple of recent papers on gaussian relaxation of discrete/bernoulli random variables. The results on various synthetic and real world datasets show the superior performance of the proposed method. Both these are valid concerns. However, it doesn t show the number of features selected by the different methods and how they compare to each other. Significance: The paper addresses an important problem of sparse CCA using a l0 penalty. Results are comprehensive, but the focus is less on feature selection and more on predictive power.<|endoftext|>The main contribution of this paper is to combine two existing works, Gaussian based continuous relaxation of Bernoulli random variables, gates (Yamada et al., 2020), and sparse CCA (Suo et al., 2017). Please comment on how could the algorithm avoid the wrong number of coefficients? However, I think the model interpretation is more important than the prediction for multimodal learning/CCA, specifically, the key strength of this proposed paper is feature selection.<|endoftext|>The paper proposes a sparse canonical correlation analysis method based on l_0 norm. A continuous relaxation scheme is adopted for solving sparse CCA. The proposed model is then extended to nonlinear function estimation and combined with deep neural networks. Experimental results on synthetic and real world datasets demonstrate the effectiveness of the proposed methods. This paper is technically sound and interesting, in that the authors demonstrate how sparse CCA with l_0 norm can be effectively solved, and how the sparsity constraint can be integrated into the deep CCA model. 2.How are lambda_x and lambda_y selected since they directly control the sparsity of variables? The paper is technically sound, but the authors should carefully revise their experimental section to address my concerns.<|endoftext|>Empirical results on synthetic and real world data show efficacy of the method. Algorithm presented that does l_0 feature selection using Gaussian relaxed Bernoulli variables is interesting and novel application of for sparse DCCA problem. Novelty of the paper is extension of idea of nonlinear CCA method to sparse case where where number of sample are less than max input feature dimensions (D) N << D. Results presented are convincing and support technical details.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper presents a technique for learning image embeddings from similarity data provided as odd one out judgments over triplets of images (i.e., ball is more similar to apple than car). The authors build on an earlier technique called SPoSE that learns sparse, non negative embeddings for images by maximizing the probability of choosing the right pair (where this similarity is calculated using the dot product of image embeddings) with an L1 penalty on embeddings. In this paper, the authors argue that 1) a spike and slab prior is more suited for this setting and 2) a more principled approach to choosing the number of dimensions in learned embeddings is possible. This should be mentioned earlier in the paper. I like the spike and slab prior and the variational treatment. My main concern with the paper is that the motivation is not really clear. My other concern with the paper is that it s not very clear if the contributions are significant enough.<|endoftext|>The paper introduces a variational inference method for concept embedding in the odd one out task. The objective is to learn representations that allow to predict the odd object from a triplet. A variational inference problem is established to learn the representations through a Gaussian variational family with a mixture of two Gaussians as prior (spike and slab). The experiments show improvement over the Sparse Positive object Similarity Embedding (SPoSE), and the method is also evaluated with random initialization and it shows to be stable. The results show improvement over the SPoSE baseline. Why is the truncation of the dimensions needed? Similar to other parameters, why not to do an ablation study over the truncation too?<|endoftext|>The proposed approach learns a variational model to approximate the distribution over the triplet. ## StrengthsThe paper clearly describes the main idea of applying variational approach to the concerned odd one out task. Compared to SPoSE, the proposed approach introduces a stochastic process to explicitly consider uncertainty, and switch the prior distribution to Gaussian mixture for better fitting to the data. ## WeaknessesThe paper has limited significance due to the narrow focus on improvement to SPoSE [Zheng 2019]. B Roads and B Love, Enriching ImageNet With Human Similarity Judgments and Psychological Embeddings, CVPR 2021It might be the case that there is a scientific value in improving the benchmark performance of the odd one out dataset [Zheng 2019], but as a reviewer, I do not have any background in judging the statement in Sec 1 saying “growing interest by cognitive scientists using SPoSE”. On learning embedding representations, there are different attempts other than Gaussian. I feel that this work should better contextualize the scientific motivation among language literature, and consider generalization over the THINGS dataset. My initial rating is below the acceptance threshold.<|endoftext|>The paper address a problem in cognitive science: identify the embedding of objects in the brain s semantic space based on the subjective judgments of objects  similarity (odd one out task). It proposes a new model (VICE) to address a few issues of the previous model (SPoSE). The new method uses a different form of prior (spike and slab) for the embedding, variational Bayes with Gaussian posterior, and appear to perform as well as or better than SPoSE in several metricsThe topic addressed by the paper is an interesting and important one for cognitive science. Does it still make sense to use such a p value to assess non zero embedding mean? What does the representation mean, if the p value is high for such item over all dimensions? It sounds like the object would be treated as "noise"? GMM assumes each component is a latent cause. I find the work generally interesting.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes to study Auto induced Distribution Shift (ADS), the phenomenon that models can create a feedback loop: the prediction of a model influence user behaviors when it is deployed and retrained iteratively, which, in turn, affects the accuracy measure of the model. The paper empirically shows that a meta learning algorithm called PBT causes a distribution shift instead of maximizing accuracy. We, the machine learning engineers, are meta learners.<|endoftext|>I could not find this line of work in the citations of the paper, and I think it may need to be there. The paper looks at the problem of auto induced distributional shift, which is what happens when the inputs to a machine learning algorithm are affected by the algorithm used for learning itself. Strengths:  the topic of the paper is important and interesting. Weaknesses:  this seems very, very related to the idea of strategic behavior in machine learning, and of "Performative Prediction" (Juan C. Perdomo, Tijana Zrnic, Celestine Mendler Dünner, Moritz Hardt).<|endoftext|>The paper realizes that the ability of a model to induce a distribution shift can provide incentives to the learner that are not necessarily in line with the intention of the algorithm designer. You mention that it is hard to specify an objective to anticipate such shifts. Let me elaborate on this: I am not sure the authors are aware of the very related literature on performative prediction. Some references:  *Perdomo et al.* Performative prediction. 2021  *Brown et al.* Performative Prediction in a Stateful World. 2020I think the incentives perspective offered by this paper on model induced distribution shifts is interesting. I really think connecting more to this line of work would be very interesting and beneficial for both communities.<|endoftext|>The paper talks about a self selection phenomenon which the authors call Auto induced Distributional Shift (ADS). Self selection bias is a big topic and hard to quantify precisely. Changing the distribution of users such that predictions are easier to make"   RecSys do not change the distribution to make the predictions easier to make. There are other interesting ideas like the close examination of meta learning and q learning. However, my main concern of this paper is the lack of validation in real world scenarios. As the paper proposes tools to study the techno societal impacts of technology, having empirical validation seems like an important step. I like the idea to simulate self selection bias with POMDP. I also do not have strong intuitions in the analysis of the q learning algorithm.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The paper tries to bound the generalization error of deep learning models and generative adversarial networks with the Lipschitz coefficient of the model. The main idea supported by the paper s discussion is that bounding the Lipschitz constant of the neural networks will lead to good generalization performance. This paper attempts to understand how the generalization error will depend on the Lipschitz constant of the neural networks in GANs. To reach this goal, the paper shows Theorem 1 bounding the generalization error with the Lipschitz constant of the neural network function (L) and the diameter of the learned variables B. Then, the paper applies this result to the GAN problem and connects the assumption on the Lipschitz constant to spectral normalization and dropout regularization techniques. The paper studies a highly interesting theoretical problem to understand the success of Lipschitz regularization methods. This is because the generalization result in Theorem 1 aims to bound the generalization error uniformly over the entire space of 1 Lipschitz functions. Therefore, the bounds do not address the curse of dimensionality in deep learning experiments and only show the generalization error is bounded with an exponential function of dimension n which does not apply to practical deep learning settings. On the other hand, the generalization bound in (Arora et al 2017) does not exponentially grow with data dimension through bounding the number of variables of the neural net players as a specification for the GAN problem. Also, the paper does not provide any numerical evaluation of the shown bounds and how they change with the actual generalization error in GANs. Due to these reasons, I do not recommend this paper for publication in its current form. The paper s generalization bounds can be significantly improved by taking the optimization algorithm and the design of neural network players into account, and I suggest the authors improve their results in that way. While the paper proves several generalization bounds, its main result (Theorem 1) models a neural network by only its Lipschitz constant and tries to bound the generalization error uniformly over all 1 Lipschitz functions. Therefore, the bounds grow exponentially with the data dimension and lose their power for even moderately large data vectors including all practical deep learning settings.<|endoftext|>The authors provide generalization bounds on GANs and some DNNs in terms of the Lipschitz continuity of the networks and loss functions. These bounds show that by decreasing the Lipschitz constant of the networks and loss functions, one can generalize better. Finally, they claim that this theory supports the empirical results we see when GANs which use Lipschitz penalization (e.g., WGAN, GP WGAN, SN GAN, etc.) For example, in the second paragraph it is written “... two players competing each other.” This should be “... competing against each other.” 3) The introduction does very little to explain the actual problem in the context of literature. The authors say that “little has been known about the generalization of the trained players” and “The standard learning theories still lack an efficient tool to analyze GANs”. These are too vague for a reader to understand what THIS paper is addressing specifically. Without being specific, the paper is confusing and the reader must guess what is going on. The following are all weaknesses:1) In the abstract itself, the authors say that GANs are “complex” in their first sentence. This is true in many ways so it is unclear how the authors address this. In section 3, they call the data distribution (as far as I can tell) z, which is confusing. The typical GAN loss stated in this paper is not bounded and hence Theorem 1 does not apply. The result that you can generalize better with simpler functions seems to go against the intuition that a highly complex hypothesis class is beneficial. Although this raises the important question… Since the generalization error is based on the loss f (and F), are we just generalizing better because we are making BOTH $F(P_z, h)$ and $F(\hat P_z, h)$ less able to distinguish differences? Because of this, one cannot conclude that the models are consistent. The authors mention that sometimes $\epsilon_0   0$ but this just shows that $\epsilon_0   0$ is attainable. More specifically, if L becomes 0, then the loss becomes meaningless. In this case, we have that we generalize perfectly but it is not interesting. If it is just an upper bound on the loss (as mentioned on assumption 1) then this is interesting but not intuitive. However, if the whole loss is bounded as explained in the second paragraph of Section 3, then it is clear why generalization is good, because with C   0, the loss will always be zero and hence we generalize perfectly again but the problem is not interesting. The authors need to address these core ideas and hence this is a weakness. 3) The consistency results are also questionable because they have a hypothesis in the theorems that is almost as strong as the result, which causes an almost circular reasoning.<|endoftext|>This paper analyzes the generalization and the consistency of a function with Lipschitz continuity. Furthermore, the generalization of a GAN, whose discriminator and generator are both Lipschitz continuous, is given. In order to get a meaningful generalization bound, previous results require the number of layers to be linear to the number of samples in the worse case, whereas the authors reduce the number of layers to be logarithmic in the sample size. Furthermore, the authors derived the Lipschitz based generalization bounds for GANs, showing that imposing zero order and first order constraints on GAN loss could improve GAN generalization. Overall I think this is a work with very interesting theoretical results. The theoretical results are novel and the analysis seems to be solid. I have the following questions:1. What does "curse of dimensionality" mean in this paper? When talking about the "curse of dimensionality", I thought that the dimensionality of each data sample is high, but it seems that the "curse of dimensionality" means something else in this paper. Is it the number of neural network layers? 2.In Theorem 1, 1 there is a $\lambda$ which is multiplied on $L$. Does that mean you can set an arbitrarily small $\lambda$ such that you do not need the Lipschitz continuity to ensure a function to generalize? In one line below Dropout DNNs in Theorem 3, there is one sentence “If the number of layers $K \ge  \frac{1}{2} \log_q m$”, which $q$ is this? 4.I understand in Theorem 1 that a smaller Lipschitz constant $L$ could lead to better generalization performance. But I’m also wondering, could a smaller $L$ also induce a higher  $F(P_z, h^*)$ value in Theorem 5 and 6? Similarly, in Sec.4, could a smaller $L_g$ for the generator reduce the power of the generator to model the real data distribution? 5.The bound in Theorem 8 is loose. 6.The authors analyze the effectiveness of using Lipschitz constraints in GANs from the generalization perspective. But I don’t think simply from the generalization error is sufficient to explain the effectiveness of imposing Lipschitz constraints.<|endoftext|>This paper s primary focus is to provide insights into the relation between Lipschitz continuity and the generalization of DNNs. Compared to previous work, tighter bounds connecting the Lipschitz constant of the "loss" and the expected loss of the learned function are provided. Some quite insightful theorems connecting the generalizability of GANs to various empirical tricks commonly used to improve their performance are also provided. The paper is very interesting and pretty much self contained. It is fairly organized and connected to previous theoretical and empirical findings in the community (good related work). A few minor issues can be improved to make it easier to understand some sections and claims of the paper. * notation of Z used both for data and noise in GAN isn t a good choice. * output of D to be in [α, β] ⊂ (0, 1) as suggested by Salimans et al.(2016).While I agree with the authors that many implementations of saturating GANs do so (e.g.α   0.1 β 0.9), to the best of my knowledge. Salimans et al only set  β < 1 but keep α at 0. * page 15, Definition 2: I think \bf{S} is a subset of Z^m (not an element) and ε(·) is from the power set of Z^m to R?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This work proposes a method for performing unsupervised conditional generation from imbalanced attributes. The key components of this work are the following:* A (architecture agnostic) GAN with a GMM latent space, where we have one component per attribute in the data. * The parameters of the components $(\mu, \Sigma)$ are learned via implicit reparameterization, and leveraging stein s lemma to derive gradients for $\mu$ and $\Sigma$. * The components are encouraged to be distinct by introducing a contrastive objective. I found this approach to be quite reasonable, and it consists of components that work well together. While GANs with mixture latent spaces have been proposed before, I am not aware of a method that efficiently updates the prior components. I thought the introduction of the contrastive loss was interesting, but I had some concerns about the diversity of generated samples when the encodings are encouraged to match just the mean vector, ignoring the covariances. From the experiments, I mostly wanted to see how this method handles a large imbalance on high dimensional data. Is 2:1 possible? ### Post Rebuttal CommentsI thank the authors for their additional efforts in addressing my concerns. I think this is a reasonable combination of components that make progress on the task: facilitating learning of samplers from data with highly imbalanced class attributes. Given the performance increase compared to [1], it looks like using implicit reparameterization and a contrastive loss (the main novelty here) make a clear difference in performance. I think each of the components are justified and perform well.<|endoftext|>This paper uses Gaussian mixture as a prior to address the scenario of imbalanced attributes in GANs latent space clustering. The study derives Stein latent optimization that provides reparameterizable gradient estimations when assuming a Gaussian mixture prior for the latent space. 2.In the theoretical aspect, this paper derives Stein latent optimization based on Stein’s lemma to estimate the gradient of parameters. 3.The paper is well written, and the result looks promising. The imbalance datasets (MNIST and CIFAR) contain two classes. I would expect to evaluate over more imbalance settings of the datasets, such as one class is 0.1 fraction of the other nine classes. 4.In the current version, it is difficult to infer how U2C loss helps the performance of the proposed model. I would expect the study presents the quantitative results (e.g., Tables 1 and 2) of the proposed without U2C loss. *****************************Post Rebuttal Comments:The authors have addressed my concerns in the revised version. I am satisfied with the response and change my rating from 5 to 6. Although the derived Stein latent optimization for imbalanced attributes is interesting and promising, the current experiments are not enough to support the claims.<|endoftext|>The paper proposed a Stein Latent Optimization for Generative adversarial networks, which can perform conditional generation in an unsupervised manner. The authors perform empirical studies on diverse datasets to conclude that the proposed algorithm is effective in learning balanced or imbalanced attributes. It is interesting to introduce Stein’s lemma to provide a first order gradient identity for multivariate Gaussian distribution. 2.The way of using a reparameterization trick to estimate gradients of the parameters of Gaussian mixtures is quite neat. It is reasonable to conduct visualization comparisons with the most recent methods. 3.How to obtain a suitable attribute ratio for the probe data? What will happen if we change the ratio from 14:1 to 1:14 for eyeglasses? Overall, the paper is somewhat novel and easy to follow. The assumptions and decisions are well supported. The stepwise experiments are helpful and provide good insights to evaluate the proposed algorithm.<|endoftext|>They devise a solution (SLOGAN) for addressing the problems current models have, especially when dealing with datasets with unbalanced features/attributes. In addition, to learn attributes from data and improve the conditional generation, it suggests using a new contrastive loss (U2C). However, I find some of the paper s claims are neither trivial (or well accepted IMO) nor are they supported/backed in the paper:For example, the abstract says "However, existing unsupervised conditional GANs cannot cluster attributes of these data in their latent spaces properly because they assume uniform distributions of the attributes." This claim needs to be supported by some reasoning/intuition and evidence/references as to why this is true. I can argue that this is not necessarily true, for example, [Self Cond Gan](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Diverse_Image_Generation_via_Self Conditioned_GANs_CVPR_2020_paper.pdf), shows that clustering is possible using D s features in an unsupervised manner. The two models: 1  [Self Cond Gan](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Diverse_Image_Generation_via_Self Conditioned_GANs_CVPR_2020_paper.pdf) which is an unsupervised conditional GAN, based on the DCGAN architecture, and 2  [PGM GAN](https://openaccess.thecvf.com/content/CVPR2021/papers/Armandpour_Partition Guided_GANs_CVPR_2021_paper.pdf) which is also a GAN for unsupervised conditional image generation and also uses contrastive clustering Are both missing in the comparisons, both in the literature review and empirical comparisons. The experiments and empirical results need to be compared with these relevant works or discussed why the comparison might not be fair. EDIT: Given the proposed method is moderately novel and the newly presented comparisons after the discussions with authors, I increased my rating from 5 to 6.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; * Amari et al., ICLR 2021. https://arxiv.org/pdf/2006.10732.pdf. derived the limiting risk under the same problem setup for general regularization parameter $\lambda$, whereas [Amari et al.] and Appendix D.10 of [Amari et al.], and the resulting "multiple descent" has also been reported. **Additional Comments**   At a high level, I personally do not find counting the number of peaks in the risk curve to be that interesting of a research problem, as least in the case of minimum $\ell_2$ norm interpolation. Intuitively, wouldn t this violate the condition that the eigenvalues do not depend on $d$ which goes to infinity? Due to the significant overlap with prior results, I cannot recommend acceptance.<|endoftext|>Authors study the generalization risk of ridge and ridgeless linear regression. I would appreciate it if the authors can comment on this matter. Among the contributions, the authors show a formula for the limiting bias and variance, show that sample wise multiple descent happens when the covariance matrix is highly ill conditioned, and study optimal regularization. To me, that assumption is an artifact for the proof techniques, which undermines the contributions of the paper.<|endoftext|>The current paper focuses on studying the theoretical rates for (iii) in the asymptotic regime where the number of samples and the dimensionality of data tend to infinity at similar rates. The paper would benefit from a deeper discussion around this. 4.Given that prior work already theoretically shows that sample wise multiple descent can occur in linear regression, the main contribution of the paper appears to be the result that optimal regularization can remove double descent even in certain anisotropic settings.<|endoftext|>This paper studies the generalization risk of ridge and ridgeless linear regression under Gaussian data and Gaussian noise settings. The authors show that sample wise multiple descent phenomenon can take place or be avoided if optimal regularization is applied. I believe it is among top 20% of accepted ICLR papers. Second, this seems to limit the application as in most tasks the number of features is fixed.
Accept (Oral); rating score: 8; rating score: 8; rating score: 6; This paper describes and tests GeoDiff, an end to end denoising diffusion model for generating molecular conformations from a molecular graph. Molecular conformations are directly parameterized in Cartesian coordinates. Therefore, an important property that should be imposed on the reverse process is *equivariance*. Equivariance is implemented by drawing the initial conformation from a distribution that is invariant under rigid transformations (isotropic Gaussian that removes the center of mass) and by using equivariant Markov kernels for the reverse process. The equivariance of the Markov kernels is implemented with equivariant convolutional layers (graph field network (GFN)). * GeoDiff outperforms various state of the art methods for generating molecular conformations. * What is the accuracy of the structures at intermediate stages of the diffusion process?<|endoftext|>The work introduces a novel **GeoDiff** model for conformation generation task, based on a promising diffusion model approach that shows state of the art results in other domains. The main contributions of the paper are: the authors are the first who propose an architecture based on principal novel generative diffusion framework for molecular conformation generation and explore suitable roto translational invariant architecture to parameterize the kernel of the method. **Originality:** The idea of the work is novel   **GeoDiff** is the first generative model for molecular conformation generation based on a diffusion framework. Authors provide energy metrics for the generated ensemble on GEOM QM9 split, which covers only 30 molecules. 3.Diffusion models suffer from high computational complexity.<|endoftext|>This paper tackles the problem of conditional generation of molecular conformations (i.e.3D cartesian atom positions) given a molecular graph. The authors formulate the generation process via diffusion probabilistic models; Conformations are generated by learning a reverse diffusion process from isotropic gaussian noise to molecular conformations. They use a SE(3) invariant formulation of the diffusion process based on Kohler et al 2020, and they operate directly on atomic positions (i.e.a point cloud) instead of interatomic distances or an intermediate bond geometry representation. The authors show state of the art results evaluated by COV/MAT metrics on GEOM Drugs and GEOM QM9 datasets. This paper brings together recent ideas and methods (e.g.diffusion, SE(3) equivariance) to the established task of molecular conformation generation with impressive empirical results. The paper is well written.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; Everything looks a direct application of existing technique. No story in this paper.<|endoftext|>The parameters of the model are of course the tunable weights.<|endoftext|>However, the paper also has a number of concerning issues:1.<|endoftext|>The authors proposed a new method for learning imbalanced medical image datasets. 2.The technical novelty of the paper is limited.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This work is aimed at distributed "privacy preserving" training of neural networks for image processing tasks like deblocking, denoising, deraining, and deblurring. "One of the most important contribution" [pg 2] is breaking down the neural network model into task specific convolutional head and tails (trained on "clients"), and a common shared (across tasks) Transformer based feature backbone, which is trained on the server. The heads/tails and the transformer backbone are trained in an alternate manner by assuming the other model to be fixed. Experimental results demonstrate: (i) successful training of the neural network models with the proposed method. (iii) better performance using the Vit backbone as compared to CNN backbones, and also with the proposed multi task vs. single task setting. In view of the above, I can recommend this paper further, but perhaps with some reservation. Post authors  response:Thank the authors for taking the time to address the concerns raised in the review. However, as detailed in individual comments, their response is not convincing.<|endoftext|>The paper presents an architecture for image processing tasks that splits up a network into three subsequent parts: head, body, and tail. Head and tail parts are CNN based and can be trained on multiple client devices using federated learning (FedAvg), while the body part of the architecture is transformer based and is trained on a central server. Head and tail parts are trained for specific tasks, while the body part is trained in a task agnostic manner by selecting clients from each task for loss optimization. Code and models are available, which greatly eases adoption. Privacy preservation is a strong claim that needs to be backed up rigorously. If the focus of the paper is to position TAviT as a general distributed multi task learning framework, then the diversity of presented experiments for validation could have been expanded. Post rebuttal:I thank the authors for their valuable comments on my review and their revision. It offers a good proof of concept of the proposed architecture decomposition, but lacks a robust discussion on communication cost/overhead as well as privacy guarantees. In particular, the ethics statement relativizes what the title claims ("privacy preserving").<|endoftext|>In this work, the authors present a multi task distributed learning framework called TAViT. The task specific head CNN and the tail CNN are distributed to clients with their data connected to a standard Transformer body placed in the server. With an alternating training scheme, the heads and tails on client sides are trained by task specific learning, while the body is trained by task agnostic learning. Experiments on four different image processing tasks show the success of task agnostic learning of the Transformer body and its synergistic improvement with the task specific heads and tails. Why the core motivation of this paper comes from "the success of ViT". Does it have any advantages (research or application value)? Does it still work well? Prior works  [*1,*2] have also addressed the task agnostic problem in federated learning.<|endoftext|>This paper presents a new distributed learning framework exploiting the vision transformer for various image processing applications. Specifically, it employs a task agnostic vision transformer to learn universal representation at the server, and several CNN based task specific heads and tails to handle different image restoration tasks at the client side. It also gives a training strategy to learn this model. # Strengths* It gives a new and practical distributed learning framework for image restoration tasks. * It gives state of the art or competitive results on the evaluated restoration tasks. * The paper is easy to follow. # Weaknesses* The emphasized privacy preserving property of the given framework is not experimentally or theoretically validated. The results are convincing.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; They introduce the concept of generalized hypergraphs and demonstrate their equivalency with undirgraphs in terms of random walk. This paper provides some theoretical understanding of the equivalency of hypergraphs and undirected graphs. The paper is not very well written. 2.Some concepts and definitions are not clearly presented. Are the equivalency through random walks good enough for downstream tasks? Specifically, why there are suddenly two Q matrices, what are their relation to the common definition of hypergraph, which only has a single matrix Q. Is it defined upon the corresponding undirected graphs? 4.In Section 3, are these two spectral operations developed based on the Laplacian matrix for the corresponding undirected graphs? If so, how are they different from the existing methods for undirected graphs? Also, other baselines for undirected graphs such as GCN and APPNP (the SHSC is a bit similar to APPNP) should be included.<|endoftext|>Real world relational datasets (e.g., academic data, protein data) contain group relationships and can be modelled via hypergraphs. One way to capture group higher order relationships is to define a hypergraph with edge dependent vertex weights (EDVWs) and such a hypergraph has been shown to be equivalent to directed graphs [Chitra et al., ICML 19]. Extensive experimentation across different domains adds to the quality of the paper. The authors have also released the source code as part of the supplementary material. **Originality of Contributions**The theory of equivalence between EDVW hypergraph and undirected graph is a small contribution of the paper in which vertex weights are added into the first step of the random walk. Moreover, SSGC and ChebNet on the clique expansion are also important missing baselines in all the experiments. However, the novelty is incremental and experimental results are marginally significant.<|endoftext|>This paper proves under what conditions the equivalency between a generalized hypergraph (a hypergraph with node weights $Q_1$ and $Q_2$ for two step random walk) and an undirected graph holds. The theory for the equivalency between generalized hypergraph and undigraph is interesting, which allows using any GNNs designed for undigraphs to solve hypergraph problems in principle. 2.The Introduction contains many unfamiliar terms for people not working on hypergraphs, such as edge dependent (independent) vertex weights. The theory only shows that performing two step random walk on weighted hypergraphs can be equivalent to performing random walk on a weighted graph under some conditions. However, it does not clearly tell whether the mapping from hypergraph to graph is injective, i.e., from the weighted graph you can perfectly recover the original hypergraph.<|endoftext|>This paper considers the problem of showing equivalency of Generalized Hypergraphs to undirected graphs (in the sense that a natural random walk on the hypergraph is equivalent to the natural random walk on a weighted undirected clique graph). They do so via establishing a Hypergraph Laplacian and identifying its properties that help prove this equivalence. They leverage this equivalency to build a Hypergraph  convolution neural network by viewing the weighted undirected graph as a lower order encoder of hypergraphs. They use empirical studies to show that the constructed Hypergraph Convolution NN on four different network based classification task, where the underlying network structure forms a hypergraph. The technical contributions are sufficiently novel and deep. The experimental results justify the main claims made by the paper.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; Such an approach should work well on the kinds of low dimensional examples considered in the numerical experiments. The paper seems to have a fundamental misunderstanding of the Wasserstein gradient flow for the relative entropy, and the experimental evaluations may not be appropriate.<|endoftext|>The paper addresses the issue of sampling from an unnormalized distribution. The challenging part is to estimate the density ratio that appears in the gradient term. Numerical results show the usefulness of the proposed method. I understand that there is a limit on the page count, however, that s not a justification for having figures that are hard to read. I find the approach interesting. However, there are some issue with the paper as it is, both theoretical and empirical.<|endoftext|>This paper considers the problem of sampling from an unnormalized distribution. This paper leverage the microscopic equivalence of the Wasserstein gradient flow of the relative entropy to sample from an unnormalized distribution, but the derivation of the key step in the proposed approach is not well explained.<|endoftext|>The paper proposes a novel way to sample from unnormalized distributions. The main idea is to track the gradient flow of the relative entropy in the Wasserstein space of probability distributions.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 6; This paper proposes a loss learning framework for the timeseries forecasting regression problem. They introduce two learning blocks, one for producing the next step prediction and the other for learning the loss function. The experimental results show that the proposed method slightly increase the performance compared to MAE. Most importantly, the performance improvement is very marginal.<|endoftext|>This paper focuses on meta learning of loss functions for time series forecasting, where the loss function to be used to train the actual model is learned. The idea seems interesting.<|endoftext|>The proposed framework has improved the regression accuracy in multiple time series datasets, outperforming classical fixed learning based models, which do not employ any learning procedure to train a target loss function. What is the reason for this?<|endoftext|>This work proposes meta learning of loss functions for regression tasks. The authors have presented this model as generically applicable to regression, however, the experiments are conducted on time series forecasting tasks. How are y_t and \hat{y}_{t} decided for the loss learning DNN?
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; These large GNN models are evaluated on a set of tasks from the Open Catalyst 2020 (OC20) benchmark and show improved performance compared to the smaller baselines. Strengths:  Overall the paper is well structured  Methods that enable effective training of large scale GNNs on datasets containing many graphs could be very impactful  The large GNN models are benchmarked on a set of tasks that are important in the real world (catalyst design), and show quite large improvements in performance compared to the smaller baselines  Weaknesses:Not sure how reproducible this work is without code  Other comments/questions:  Some additional information on the computational cost of Gemnet XL and Dimenet++ XL models in the 3 tasks would be useful. Overall, I vote for acceptance. The paper proposes a method to train large scale graph neural networks and shows that very large GNN models can have quite large improvements in the Open Catalyst 2020 (OC20) benchmark<|endoftext|>This paper proposes a distributed training method for large graph neural networks (GNN) up to billion parameters. Strength: * Good empirical results: OC20 is an open, comprehensive benchmark and the method achieved decent improvement over strong baselines in the leaderboard. Weakness and questions: * Since this paper is about distributed training, it should be compared with other distributed training methods, including data and model parallelism. For data parallelism, one could consider splitting the batch to multiple GPUs. The paper states that model parallelism can be combined with the proposed approach, but I think this paper should compare with standard model parallelism, such as pytorch DDP. * I am not so sure about the novelty of the distributed training approach here. Using 8 GPUs, the method can only reduce run time by 50%.<|endoftext|>The paper is based on the idea that most of the computation costs to extended graphs come from triplets. The authors proposed a way to parallelize the computation of triplets in a distributed way. The author also discussed two models that fit into this framework and showed their increase in performance due to the larger parameter count enabled by their parallel framework. In the experiment section, network structures and hyperparameters are given and the experiment should be reproducible. The results support the authors  claim that larger graph networks have better performance. I do not know much about this field.<|endoftext|>Higher order interaction terms make these networks very compute intensive. The approach is well motivated and well explained. I am familiar with the DimeNet architecture but not very familiar with the recent literature on distributed GNN training, so it is hard for me to judge the novelty of the proposed parallel approach. The results on the OpenCatalyst benchmark are compelling. Among graph neural networks for molecular structures, the DimeNet++ architecture is specifically memory and compute intensive (and as such naturally stands to specifically benefit from the proposed approach). It would be great if the authors could include a discussion of the applicability of their approach in the context of other graph neural networks for molecular structure, such as equivariant message passing in Jing et al.(ICLR, 2021) or Schuett et al.(ICML, 2021). I will defer to other reviewers regarding the technical novelty of the approach. The paper could be made stronger by explicitly discussing how this approach is applicable to other GNN architectures that have been designed for 3D molecular structure.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper proposes a mechanism to train disentangled generative models without relying on regularization losses. It aims to enforce independence in blocks of the latent representation of a VAE by 1) corresponding different blocks of the latent representation to different depths of the decoder by injecting noise in an Ada IN inspired block 2) sampling each block from a fixed learned set of k latent vectors, similar to the codebook of VQ VAE (Oord et al 2017). Specifically, there is no theoretical justification for why this model should result in the disentangled representations that it advertises. The disentanglement metrics presented are not especially convincing. The metrics which are most emphasized in the paper seem to be FID and Reconstruction FID.<|endoftext|>This paper structures how latents are used in an autoencoder to improve its performance. They are motivated by causal structure and independence of latent variables. Strengths:The paper is fairly easy to follow, the motivation is generally good, the problem is important (generation and structured unsupervised latent structure / representation learning). Weaknesses:The main idea of the paper, independence of the latent variables, only seems empirically supported, but it s hard to find convincing motivation in the paper that the precise way they use the latents in generation / algorithm *should* lead to independent latents. Other comments / clarifications:So the latents are a flat vector output of the encoder correct? When you split this latent vector, how do you distribute it amongst the layers which generating when the splits are different? Is AdaAE your method? From what I can tell, you are just using all of latent vector (not splitting) at every layer? The FID scores are distributional, generated (e.g., using hybrid sampling) vs test, correct? I wonder how close the train distribution is to the samples in FID. I feel like this needs to be tested.<|endoftext|>Rather than a latent feature map/vector, the latent variables are used to condition affine transforms in the decoder. The authors combine this with a ‘hybrid’ sampling strategy, effectively sampling from the aggregate approximate posterior. Figure 6: $\beta$ VAE is lower than VAE on all of the metrics. I see this as an interesting and potentially useful result. For the most part, the paper is clear. Multiple previous works, some of which the authors cite, have shown that hierarchical latent variable models are capable of learning disentangled, hierarchical representations, e.g., Zhao et al., 2017. Thus, it’s already clear that model architecture can bias representation learning. The authors’ specific claim, then, appears to be that some aspect of the latent independence and structural transform layers is helpful for disentanglement. One could also feasibly combine SAE with priors, either via fixed or learned priors. Thus, while it’s clear that the presented SAE model performs well, it’s not entirely clear which aspects of the model are responsible or whether techniques from previous works would improve performance further. Some aspects of the model were not entirely clear to me.<|endoftext|>The paper proposed a new structure call SAE (structural autoencoder) along with a new sampling technical, named hybrid sampling, to train a encoder decoder generative model. The authors conducted extensive experiments to show the proposed model and sampling method lead to much better results for generating higher quality images and get more meaningful/smooth images in experiments involving feature disentanglements and extrapolations. * The authors provide their codes in the supplimentaries. This paper is well written and its claims are well supported by the experiments, analysis, discussions. I believe this paper will bring values to the broader researcher community.
Reject; rating score: 1; rating score: 5; rating score: 6; The approach uses a deep learning framework, and the primary novelty is to include in the model information about peptide secondary structure. Unfortunately, I think this work suffers from a major conceptual flaw, namely, that there is not satisfactory gold standard to evaluate the utility of the proposed method. The manuscript does this by asking whether the sequences capture some of the known properties of AMPs and whether they get assigned high scores by existing AMP classifiers. Second, it is certainly plausible that a synthetic AMP sequence could do well according to these measures yet not have AMP function. In practice, the only way to know whether an AMP simulator works is to synthesize that peptides and test them. For example, if I randomly substitute a single amino acid in a known AMP, then the resulting "simulated" AMP is very likely to be functional. What does it mean to "kill bacteria in a physical way"? "multiply ideal"  > "multiple ideal""proprieties"  > "properties"This paper addresses a problem for which no good gold standard is available, short of experimental synthesis and testing of the proposed sequences.<|endoftext|>In particular, it seems that the generated secondary structure is not diverse as we can see Table 5 in the appendix. The model both train on the amino acid (aa) sequences and on secondary structure. The author evaluate the model by comparing attributes of the generated sequences with real AMPs. The authors also perform an ablation study of their model, an extensive benchmark of existing generative models that have been used for peptides. A large set of experiments to validate their approach, comprising the study of attributes of the generated sequences, benchmark of existing methods, evaluation of the generated sequences with existing predictors and an ablation study. To summarise, I believe that the paper is below the acceptance threshold as:  the proposed method do not seem to beat the state of the art (MLPeptide)  some analyses of the generated sequences are missing, such as similarity study between the generated sequences and the real ones, and diversity of the generated sequences. Also, while the model of the author uses pretraining, I am not sure to understand whether the proposed comparison partners do. I believe that it could be fair to use pretraining for at least a few of the comparison partners in order to have a fairer comparison, especially because pretraining in this application is not a new idea (see Das et al 2018). 3.The results of the proposed approach are not entirely convincing. And if no, does it mean the comparison partners better capture these simple rules?<|endoftext|>This paper assumes a latent representation is both associated with the amino acid sequence and its secondary structure of peptide. Random peptide sequences can be generated at this point. This result suggests that secondary structure is an important factor to consider in AMP design. The physical properties of generated peptides (table 1) are also impressive. I feel the paper is on the boarder line due to the following concerns. 1.In table 2, the performance is generally worse than MLPeptide, which suggests that the proposed method have space to improve. 6.It will be nice if a section could be added regarding the reproducibility of experiments in the paper. I tend to accept it if the secondary structure information could be proven to be useful also for other methods.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; The target problem is few shot image generation. This design is loosely inspired by the recent discovery of "grandmother cells" in V1. A novel prototype based layer which significantly improves the respective augmented image generation models: FastGAN, StyleGAN2. The consistent and somewhat significant improvements over the two baselines are impressive. Writing is clear and well structured. Only 2 baselines were used to validate the novel design. This is despite the paper s claim of being "inspired by neuroscience discoveries". As paper did not claim to be based on neuroscience, this is not a major concern. While there are a couple of minor issues, the overall quality is sufficiently high for an acceptance by ICLR.<|endoftext|>They showed that the prototype memory bank improves image synthesis quality, learns interpretable visual concept clusters and improves the robustness. Strengths:The idea of using prototye memory concept is interesting, although it is not a new idea. For example, the prototypical network for few shot learning [1]. The sparse activity of V1 neurons in the monkey brain doesn t mean they are grandmother cells. The authors should be more careful about this. 2) the memory bank idea has been proposed in many other works (the implementation detailed may be different), the authors should compared the difference of their work and others. Why not only use  $K_i$ to represent the prototype memory? The authors carried out adequate experiments to show the proposed module (MoCA) is can be used to improve few shot image generation quality and they linked the module to the grandmother cell in the brain. However, the memory bank idea is not new and the novelty is limited.<|endoftext|>In this paper, the authors propose a new “grandmother cell” like memory mechanism for improving image generation performance in GANs. Then at image generation time, activation vectors are augmented with the sum of the stored memories at the closest cluster. This seems to improve GAN performance for few shot image generation tasks. The authors also visualize the learned memory clusters, which seems to reveal some semantic clustering. This paper proposes a timely and interesting neural network memory mechanism, MoCA. However, it is not clear that this comparison is entirely fair: do the StyleGAN2 and MoCA StyleGAN2 (or FastGAN and MoCA FastGAN) networks have the same number of parameters? In the example image figure (Figure 3), the authors should show the most similar image from the training set alongside each generated image. Here it seems like the authors are interpreting the tiny generated patches in the left hand side of each image in an attempt to understand what the semantic selectivity of each cluster is. I was left unsure about the claims made based on this Figure. I would recommend that the authors re work this to be more clear about what they are showing, but I am afraid that I can’t really offer any specific and substantive recommendations.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes to use TIC as a metric for measuring the generalization gap. The relative magnitude of the eigenvalues of $H$ and $C$ may be indicative to the generalization gap given that the TIC is a good measure of generalization gap. This may not be directly relevant to this paper, but training variance has been a more important issue for generative neural network than for a typical predictive neural network.<|endoftext|>Overall, there’s issues with clarity. In this case,  why not just compute validation acc/loss which we know is a good estimate of generalization performance? Shouldn’t one also try to approximate the WAIC as similarly be done in the paper?<|endoftext|>This is not true. First of all, Appendix A1 is quite long and it is not immediately clear which part of it they are referring to. That is a bit confusing.... / Minor issues, typos, etc. First of all, $q_\theta$ is not defined anywhere in the paper as far as I can tell. After Rebuttal I took a quick look at the rebuttal. I understand that this is "known" from the related work, but it would be nice to make the paper more accessible by explaining it.<|endoftext|>I have not found the descriptions. 2.What is the role of G(\theta) in the main text? Do the authors use it during the TIC estimation process? Typos:1.Page3: they a different different distribution. Overall, I think this paper focuses on a challenging topic and has the potential to be accepted.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; * This paper is interested in the problem of one step retrosynthesis, i.e., predicting the reactants that formed a given product. * For this task, the authors propose a new model (SemiRetro) that combines ideas from previous template based (TB) and template free (TF) methods. * SemiRetro consists of two parts:  	i. First, DRGAT (a new GNN architecture based on edge attention) predicts the reaction center, and breaks down the product molecule into synthons. When does it fail? p.6 "Reactants can be determinately generated from semi templates and synthons": "determinately" should be "deterministically"? It s great that you draw attention to the issues with information leakage and atom mapping. SemiRetro seems an interesting method for retrosynthesis that combines ideas from both template based and template free approaches. It also seems to get impressive empirical performance (with the caveat that some SOTA methods are missing from the comparison   see W4). W4.In the retrosynthesis experiments there are missing comparisons to state of the art (SOTA) methods, e.g.:  > Sun, Ruoxi, et al."Energy based View of Retrosynthesis."<|endoftext|>This paper presents a novel template based retrosynthesis prediction method. The authors propose to break full template into several simpler semi templates, and conduct synthon completion based on the semi templates. Besides, the authors also show the detailed performance of two steps of the retrosynthesis method. If I understand them right, the semi template in this paper and local reaction template are the same stuff. Could the authors clarify that? As the authors mentioned, this is an information shortcut with the USPTO dataset. Sometimes, it is really difficult to recognize the shortcut. The proposed method has achieved impressive experimental results on the small dataset USPTO 50K.<|endoftext|>This paper proposes a new variant of retrosynthesis models, with mainly two new components: DRGAT for powerful embedding computation and the semi templates for accurate synthon completions. This significantly reduces the redundancy of the templates, allowing connecting the template based retrosynthesis (less scalable but powerful) and the template free retrosynthesis (scalable but less powerful). Please discuss the relationship with the proposed model, and if it is appropriate, compare in experiments. Some strong competitors are also introduced by other reviewers. I expect the authors to come up with a manuscript including all new updates in near future conferences.<|endoftext|>A new algorithm for retrosynthesis prediction based on a new GNN architecture and semi templates is presented, which yields state of the art results on some benchmark tasks. notes:In the introduction, the authors write:`Fortunately, with the rapid accumulation of chemical data, machine learning is promising to solve this problem (Szymkuc et al., 2016; Coley et al., 2018; Segler et al., 2018).`   However, Szymkuc et al 2016 argue *against* the use of data driven approaches, so I would suggest to not to cite them in this context.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 8; I think this is a valid problem setting. 3.The emprical comparisons with DP SGD is not fair. Final update: After the discussion period, I still vote for rejection. Then how can you make that promise? This would also satisfy data independent DP. "The complexity of differential privacy." Strengths: 1. this problem is an extension of PATE to a multi label setting, which is practically interesting. However, the authors did not formally prove it. The gap between data dependent DP and data independent DP can be huge (also reported in this paper).<|endoftext|>The paper is proposing differential privacy (DP) solutions for multi label classification (MLC). The main goal of the authors was to show how DP can be enforced in MLC but the results presented in the paper are not sufficient to draw any conclusion: although the DP requirements are analyzed, the MLC performance remains unclear.<|endoftext|>Differentially private multi label classification is an interesting and well motivated problem. I found the writing quality to have some issues (some minor comments below). It’s somewhat surprising that this seems to be optimal among the mechanisms (in the case where voters aren’t constrained in how many candidates they can approve) unless there is a lot of correlation between votes, at which point the (exponentially sized?)<|endoftext|>Private multi winner voting is the task of revealing k hot binary vectors that satisfy a bounded differential privacy guarantee. They also use these mechanisms to enable privacy preserving multi label learning. This paper introduce mechanisms for private multi winner voting and multi label learning, which in important in machine learning. The paper structure and the writing are good. It s good but may be not enough<|endoftext|>This paper considers the design of differentially private multi label mechanisms. Strengths:  The authors consider an interesting problem as designing a privacy preserving mechanism with multi label outcomes would be applicable in many contexts. Moreover, the theoretical bounds can be derived by building on top of the existing DP guarantees. But probably there is a way to combine $\tau$ voting and the powerset voting for large values of $k$. I think the paper considers an important problem as the design of differentially private aggregation mechanism for multi label outcomes is applicable in a lot of domains. However, I felt that the experiments were not quite exhaustive.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; The paper discusses learning strategies for energy based models, short sampling for image generation, midrun sampling for adversarial defense, and longrun sampling for density estimation. EBMs are more expressive than GANs in the sense that they also give un normalized densities, but the paper never used these densities for any meaningful tasks (like out of distribution detection). If the method claims "density estimation", then we should expect to see results other than FID (since that is image generation), even if partition function is not possible to compute, it can still help to see if density estimation have other uses, such as out of distribution detection.<|endoftext|>This paper discusses 3 applications for EBMs: image generation, adversarial robustness, and density modeling. Unfortunately, I do not find their arguments convincing regarding density estimation and the use of FID to evaluate the method. The experimental details in the paper are quite scant. The proposed method is more complicated than CoopNets and PCD and provides additional hyper parameters to tune. There are some interesting ideas presented in this work, but I do not feel the results presented demonstrate that they are a sufficient contribution to the field for acceptance. While the proposed method makes sense, and appears to work, there are considerable issues with the method’s evaluation, experimental details, and theoretical justification. Thus, in its current form, I do not advocate for its acceptance.<|endoftext|>This work presents different MCMC initialization techniques for training EBM with differerent lengths, for the purposes of image generation, adversarial defense, and density estimation respectively. The paper is well written and easy to follow. 1.For image generation, why not use a pretrained generator? Did the baseline EBMs use generator initialization? Is annealing applied to other baseline methods? Overall, the paper provide some interesting empirical findings for EBMs with different MCMC lengths.<|endoftext|>The paper describes techniques to train EBMs according to the length of MCMC trajectories required. The paper explores three applications of EBMs: image synthesis, adversarial defense, and density estimation. One fix is to use batch normalization in the generator.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper proposes a search algorithm for instance conditional data augmentation. ** **[reasonable and well motivated]** The basic idea of this work that makes the augmentation conditional on data itself seems reasonable and worth exploring. Could the authors explain this? **W2.** **[unclear performance improvements]** The performance reported in Table 5. shows that large $\delta$ (*e.g.*, 0.4) would bring substantial improvements.<|endoftext|>Overall, the writing is clear and easy to follow  **Well motivated problem and novel approach**. Instance awareness of AutoDA is an essential but not explored aspect for better performance on the learned dataset and transferability to other datasets. In Figure 2, the authors show the learned probability p to provide empirical evidence of class/instance dependent augmentation.<|endoftext|>The motivation of this work is clear and the idea of searching data dependent augmentation policies seems reasonable. There are some related works this paper omitted. Did the authors ever compare with this method? 2.The performance improvement of the proposed approach seems marginal. Overall, the paper is well written and the idea is novel.<|endoftext|>This paper illustrates an adaptive based data augmentation method named as AdaAug that searches adaptive augmentation policies in a class dependent and potentially instance dependent manner to improve the generalisation capability of deep learning models. The empirical studies on some datasets shows that the performance increase with AdaAug. I can not decline that this is an interesting paper for this community. 3.The paper is well written and easy to follow.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper introduces three variants of Anderson acceleration: short term Anderson mixing (ST AM), modified short term Anderson mixing (MST AM), and regularised short term Anderson mixing (RST AM). also the standard condition given by equation (14)). These may be used to obtain the solution to linear systems, nonlinear systems, and optimisation problems. This appears to be an excellent paper. My first comment is that I think it would be really beneficial to more explicitly state the delta between this work and previous work. When is it worth the effort to implement the introduced methods as compared to some off the shelf competing method? My main two concerns are (a) readability wrt applications and (b) experiments across a broader range of regimes. I would regard the following as being particularly important extra comparisons:  Section 4.1: how does the proposed method compare to Newton and standard quasi Newton methods (BFGS, L BFGS, chord, Levenberg–Marquardt, etc.)? Section 4.1: the test problems considered are all very simple. Focusing on the implicit ODE case: at present this is ubiquitously done via Newton s method or the (quasi Newton) chord method. A few examples:    [1] consider a case in which GAN training with SGD/Adam fails completely, but training with Adadelta produces stable/convergent results. [2] derive optimisers by solving the gradient flow ODE. [Usually not a great idea in the supervised learning case, but the unreliability of other optimisers in the adversarial case motivates a return to this idea.] (This is commonly also done in theoretical studies of optimisers.) These seemed to help a lot on MNIST.<|endoftext|>The paper proposes a new class of memory efficient Anderson mixing (AM) methods. I think overall this is a paper with solid theory and experiments. In my perspective, the first part may not be of too much interest to the audience of ICLR and is more suitable for optimization journals. Yet, the part of the MST AM algorithm could be of great interest since it is applicable to training deep models. For the weakness, I feel the discussion on improvement of MST AM over SAM (Wei et al., 2021) could be improved. The paper mentioned MST AM is motivated by SAM  and talked about which steps are different. It could be better if the authors could provide a discussion on intuitions why MST AM outperforms SAM in the paper.<|endoftext|>The paper introduces 3 new variants of Anderson Mixing methods relying on very limited short term memory. The paper also provides detailed analysis of the performance of each of these AM algorithms, as well as thorough experiments, showcasing improved performance of select neural network training. 2.A claim is made that the memory footprint of the algorithms is close to that of GD or SGD, but a more precise theoretical comparison would be helpful. For instance, we see that the memory footprint seems to only increase by 5% over SGD in some experiments, but are bigger than Adam in others which leaves the reader wondering how these seemingly inconsistent results came about. While thorougly reported (which is appreciated), it relies on toyish datasets which make the claims of the paper more difficult to support. Another issue is that the authors chose RNNs for language tasks while transformers have obtained SOTA results for years, and Adam seems to be more critical to good transformer performance than good RNN performance. All told, this is a nice paper which clears the bar for publication at ICLR this year, but which could be much improved with a revamped, more ambitious experimental section. Details  section 3.4, first paragraph: optimizaiton  > optimization  fig 1 caption : Probrem  > Problem  page 34 precondtioner  > preconditionerInteresting new algorithms, well motivated, delivered with good theoretical analysis. The experimental section is a bit lacking, but that s not sufficiently concerning not to recommend the paper for publication.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper proposes a method to utilize a SPEN as a trainable loss function for training a neural network. However I have several major concerns about the paper. For example, one of the major claims in the paper are not supported. f1  > F1.In addition, the empirical comparisons appear to be limited and evaluated only on low dimensional datasets. For example, the authors claim that their new loss prevents adversarial training.<|endoftext|>The paper presents extensive experiments on multi label classification on a range of feature based and text based datasets. It is hard to evaluate the gain of SEAL over these approaches. The paper only considers one type of structured prediction benchmarks: multi label classification. Some related literature that is missing: recent works that have used NCE for training energy based models for text based datasets [1, 2]. However, the experimental analysis could be strengthen by improving the presentation of the results, and by considering more benchmarks.<|endoftext|>7 feature based 3 text based datasets are used for testing SEAL and showed better results and even better with combinations. [the appended information from the authors answered this question in detail and thanks for that.] This paper is well written and the direction of using of dynamic loss functions is an interesting direction. Any other baselines of not using energy related networks in table 2? Now I have a better understanding of the benefits of applying "energy based methods" to the corresponding tasks.]<|endoftext|>This paper proposes a framework to use Structured Prediction Energy Network (SPEN, a prior work) as a loss function. Empirical results in 10 multi label datasets are given, which shows good performance boost from using the proposed methods. Why the ranking based methods (in table 2) are significantly better than others in delicious and cal500 dataset? 3.Why DVN performs so well on cal500? The writing is good. should probably say "Lastly, these models were designed for multi class classification" (remove "do"). I am more inclined to marginally accept the paper given its current status.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; The authors of the paper have introduced a multilingual cardiac signal captioning framework. The authors add an auxiliary task of identifying the language of some of the tokens to improve the performance of the decoder. The proposed framework achieves on par performance with state of the art pre training methods. The paper is well presented and the problem introduced in the paper is interesting as well as important. But in the case of the proposed framework the gold label captions for languages other than English are trained on captions generated via Google Translate API. 2.I do not understand the use case of generating all languages at once. It is an interesting approach but I do not see an added advantage of the proposed model. This would reduce the model parameters and make the whole framework more efficient.<|endoftext|>The goal of this paper is to develop an approach for generating multilingual ECG reports. The authors propose a new multilingual pretraining method in which tokens are randomly replaced with those from a different language, and the model must learn to identify the language of all tokens. The paper compares monolingual and multilingual versions of the models and find that RTLP benefits from multilingual training. Weaknesses: I have several concerns about the clinical utility of this task as well as the evaluation approach. There are some existing approaches for automatic ECG interpretation. How does your method compare to those automatically generated reports? Can you ask clinicians to evaluate the utility of the generated reports or evaluate clinical utility by using the generated reports to predict conditions identifiable from the ECG? I think that it’s fine that the RTLP method performs comparable to existing methods, but I am not sure from the current paper what the utility of using RTLP is. My understanding is that reports are generated in other languages using Google Translate. Other Comments: 	Why do you only consider ECG segments with one label assigned to them? How well does this work in languages that have very different syntactic structures compared to the source language?<|endoftext|>Experiments show that the proposed RTLP framework can achieve comparable performance with SOTA models. This paper proposed a novel multilingual pretraining language model settings. The idea is very interesting and the experiment setting is solid. In my opinion,  the performance of clinical text generation should be validated based on its content, although it is very hard and requires extensive manual work. 2.The gold standard reports for non English languages came from Google translation. It would be better to conduct a sensitivity analysis by using other translation tools. 3.It is not clear which network structure was used to represent the cardiac signals? Based on this low baseline, the improvement from multilanguage has limited value. While the evaluation metrics in clinical text generation are not persuasive. And some of the important details were missing in the current draft.<|endoftext|>This paper aims to build a multilingual cardiac signal captioning system to generate ECG reports, which describe the clinical findings in the input of electrocardiogram (ECG) signals. The experiments on a public dataset verify the effectiveness of the proposed approach, which performs on par with state of the art language pre training methods. The paper is well written and the motivation sounds reasonable. 2.The targeted problems, i.e., cardiac signal captioning and multilingual captioning, are novel and important in both artificial intelligence and clinical medicine. 3.The proposed multilingual cardiac signal captioning system is well motivated, novel, and interesting. The presentation can be further improved. In other words, have you solved the challenges and problems that are unique to the multilingual cardiac signal captioning? Firstly, the evaluation metrics used in this paper, e.g.,  BLEU and ROUGE, are all general metrics for text generation tasks. In what aspects can the proposed method improve the performance of the multilingual cardiac signal captioning? Thirdly, the Google Translate model is not specifically designed for biomedical texts, so you can give more analysis of the Google Translate model. However, I am more interested in knowing if the approach brings errors? And why?5.The related work is insufficient.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This paper proposes an offline RL algorithm named IQL, which generalizes between Bellman expectation equation and Bellman optimality equation with expectile regression. Theoretical analysis also backs up the algorithm well, and empirical performance is also presented well, showing its advantage over other algorithms. I believe that the main strength of this paper is the novel idea of using an expectile regression on dataset actions to learn a value function that is based on high performing actions in the dataset.<|endoftext|>This paper proposes in sample Q learning which composed of fitting value function with expectile regression, training Q function and extracting the policy using advantage weighted behavioral cloning. The paper overall is easy to follow and well written. The empirical evaluation is significantly better than the existing baselines on challenging tasks like Ant Maze, and I thereby recommend accepting the paper.<|endoftext|>This paper proposes an algorithm In Sample Q Learning (IQL), which learns a Q  and value function under behavior policy and applies advantage weighted behavior cloning for policy learning. Do you try to remove the value function learning by just doing expectile regression over target Q values and Q values? The paper is well organized and easy to follow and read. In one sentence, the main idea of the paper is to learn an approximately optimal Q/value function for better behavior cloning.<|endoftext|>This paper proposes a new offline RL algorithm that uses in sample policy evaluation and advantage weighted regression during policy improvement. Particularly, it utilizes sarsa like TD to update Q function and avoids the query value of unseen actions during training. How will performance change?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; Compared with the original extragradient algorithm, the extended algorithm is proved to converge with a larger range of stepsize choices and MVI related constant $\rho$ (implying a larger set of applicable problems) in both deterministic and stochastic inclusion problems. Finally, an improvement of this extended algorithm is proposed using Lipschitz constant backtracking. Pros:(1) This paper studies the constrained/regularized nonconvex nonconcave minimax optimization and its generalization called weak MVI problem, which are popular, important, general and hard problems. (3) Both theorems and experiments convincingly demonstrate that there are inclusion problems where the original extragradient algorithm diverges while the proposed algorithm with adaptive stepsizes converges. A similar comparison of convergence rate might also be made between your Theorem 3.5 and Theorem 4.5(i) in Diakonikolas et al., 2021. (7) In the left subfigure of Figure 3, Adaptive EG+ seems to converge to the limit cycle instead of a stationary point. (9) In the experiment, for the algorithms with adaptive stepsizes, I think it better to give the specific stepsize values, such as $\gamma_k 1/k$, etc. (11) The convergence of CurvatureEG+ is assumed not proved in Lemma 4.1.<|endoftext|>The authors also studied the lower bound in the simpler case when $A\equiv 0$, showing a difference compared to EG+ in (Diaonikolas et al., 2021). In the deterministic case, it proposed an adaptive stepsize strategy to allow larger range of the MVI parameter $\rho$, and further a curvature based strategy to avoid the lower bound requirement of $\rho$. The authors also executes several experiments, showing that CurvatureEG+ can avoid cycling in the experiments. The paper focuses on general minimax problems, which is a very important problem (even though with the special Weak MVI structure). Or do you think the lower bound for the whole class of weak MVI problems is already achieved? Generally I view the results in the paper pretty interesting.<|endoftext|>This paper constructs methods named CurvatureEG+ (and Adaptive EG+ and CEG+), built upon a recently proposed EG+ [Diakonikolas et al., 2021], a variant of EG, that works under the weak MVI condition. The corresponding nonconvex nonconcave setting includes non trivial problems illustrated in the paper, where the proposed method converges, while other existing methods reach limit cycles. Unlike EG+, the CurvatureEG+ can handle both constrained and composite cases. (This bound of EG+ could be weakened, but the authors show that this cannot be better than $\rho> \frac{1}{4}$.) For the stochastic case, the proposed SEG+ works for a same range of $\rho$, while a stochastic version of EG+ in [Diakonikolas et al., 2021] is shown to work for a more restrictive setting. (This also allows a larger step size for EG+.) What is new in that perspective seems missing in this paper.<|endoftext|>Building on the analysis of (Diakonikolas et al.2021),  the paper relaxes the setup therein by allowing a larger range of values for the $\rho$ parameter (see Fig.2) of weak Minty Variational Inequality (MVI) which parameter controls the degree of nonconvexity. that under some assumptions (Asm 1), Alg. 1 converges on weak MVI problem. # Strengths  The paper has several theoretical contributions referring to different options for the two step sizes of EG (see summary above), and importantly, it relaxes the setup of prior work in particular of (Diakonikolas et al.2021).The technical contributions are relevant for the community, given past negative results and the current state of existing provable convergence results. Some typos:      in contribution 3: our result are $\rightarrow$ is     related works: approaches has been $\rightarrow$  have been     Hsieh et al .. considers  $\rightarrow$ consider     last sentence Sec.1: they only considering $\rightarrow$ consider  Consider naming Alg.1   In my opinion, the title could be misleading, it could be interpreted as a novel technique to escape limit cycles for general non convex contained problems. Does CurvatureEG+ escape unstable critical points?
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper studies the effect of data quality on adversarial robustness. Specifically, they focus on one measure of data quality (number of times there is a perturbation that is misclassified across training iterations). They study the effect of data quality on robust overfitting, robustness accuracy tradeoffs and "robustness overestimation" (gap between strong and weak attacks). The main conclusions reported are that data quality as measured by their metric plays an important role in all three aspects, and a suggested takeaway is that we need data of higher quality to improve robustness.<|endoftext|>Especially, this extra data case normally fits the message of the paper so it is a missing element in the paper. Regarding the **results** and **contributions**:* The authors propose a quantitative definition of "quality"  based on the average training robust accuracy over epochs per sample. * Extensive experiments on CIFAR 10 to analyze the robust performance of "high quality" data (selected by their proposed criterion) under three angles commonly discussed in the Lp norm community. It really lacks of an analysis in the extra data case. This case should be actually perfectly suited for the message of the paper as extra data for CIFAR 10 is known to be of lesser quality (as per the references in the paper). Second, all the first rows are labeled "PGD". *Fixing data augmentation to improve adversarial robustness*.<|endoftext|>This paper proposes a metric for evaluating the learning stability of data and point out that unstably learned instances are of low quality for adversarial training. Through extensive controlled experiments, this paper investigates the impact of low quality data on three issues in adversarial training, i.e., robust overfitting, robustness overestimation, and robustness accuracy trade off. The quality of data is closely related to a particular dataset. It should be a limitation for the practical use of low quality data. Overall, this paper systematically study the influences of data quality on the three problems in adversarial training.<|endoftext|>The paper investigates the impact of data quality, measured by the proportion of epochs in which the model classifies a specific input correctly during training, on the robustness, generalization, and robustness accuracy tradeoff of adversarially trained models. This work expands along this direction by observing how the quality of data impacts generalization of adversarial training. Good scope in experiments, significant results  The authors perform experiments on multiple datasets and adversarial training methods and the trends look consistent across methods. Patterns discovered between data quality and properties like robustness, robust overfitting, and robustness estimation also look significant. Weaknesses:  In the main text, it say that results for WideResNet architecture are in the appendix, but when I looked at the appendix I couldn t find figures for WRN corresponding to the experiments in the main text. I vote to accept this paper since I find the empirical contributions significant; the authors rigorously study the impact of data quality on various properties of adversarially trained models including robustness, robust overfitting and robustness overestimation, and robustness accuracy tradeoff.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The approach is based on stochastic frank wolfe and the results are impressive for a wide range of sparsities. 4.The results demonstrate the value of pruning aware training using the proposed approach. The motivation for K sparse constraint set is not clear. More clarity is required on how the authors chose to use such a constraint set. 2.If I understand correctly, the \tau is a constant for all the weights in the network and therefore, the SFW update at any iteration is basically based on the sign of the gradient. It is intriguing that even with such a restrictive update the final weights are competitive to SGD based training.<|endoftext|>In this paper, the authors proposed to use stochastic Frank Wolfe with K sparse polytope constraints to train deep neural networks and make them pruning friendly. In addition, experimental results are not enough. The core benefit of the proposed method is not clear. 2.The authors should provide a more detailed analysis of K and $\tau$ for K sparse polytope constraints, and how they affect the weight distributions. After the authors  response, most of my questions were addressed, and I increased my score to 6.<|endoftext|>This paper proposes a Frank Wolfe based approach for efficient pruning. There still exist concerns regarding the scalability of the algorithm. The authors are encouraged to address it with more experiments. Questions:(1) Which step in the proposed algorithm other than backpropagation has the largest cost per iteration?<|endoftext|>This paper swaps gradient based optimisation for training neural networks with Frank Wolfe algorithm with a constrain that pushes less important weights towards smaller values. I think the proposed method here is actually quite interesting but this work fails to discuss a whole body of relevant literature that also aims to train sparse neural networks. I was wondering if authors have an explanation for this? This paper proposes an interesting alternative to gradient descent that can be used to train models with a weight distribution suitable for one shot pruning. I find the method section well written and convincing however I was expecting to see many other relevant work here and the experiments section fails to compare to these relevant works, so it s difficult for me to evaluate the effectiveness of the method compared to simpler/other alternatives.
Reject; rating score: 3; rating score: 5; rating score: 6; This work proposes an approach to polyphonic music composition based on reinforcement learning. The approach constructs a reward function based on inverse reinforcement learning (IRL) using human demonstrations, in conjunction with a hand crafted, music theoretic reward. Section 2 is far from a comprehensive discussion of music generation, and the particular choices of references are strange: for example, why cite WaveNet at all in a paper about symbolic music generation? I note that there is not a single reference to the literature to support any of the claims made in the introduction to this paper.<|endoftext|>There are a few other minor issues in the paper listed above. The related work section does not mention some of the more recent work in music generation. There have been a lot more recent works focusing on polyphonic and piano music generation. IRl  > IRL  Lahk  > LakhThe paper proposes a novel approach to tuning an LSTM based music generation model using a learned reward function in addition to music theory based rewards. This addition leads to an improvement in the generated music. However, the quality of the generated music does not hold up to other recent work for polyphonic music generation.<|endoftext|>This paper introduces a novel pipeline for generating polyphonic music. The authors present the samples of music generated by various algorithms, but not the baseline human generated pieces which were the part of the comparison reported in the paper. Please provide a comprehensive description of AIRL in section 4.1., e.g.describe the exact relation between the discriminator network and the reward function. I guess that the model would work just fine with no pre training.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper presents a method called ESAM for improving the efficiency of SAM. * While SAM has proven to be helpful for many applications, this technique can be helpful in practice. * For SDS, an interesting baseline to be compared is estimating $\epsilon$ with a subset of randomly sampled data, which could match the origin $\epsilon$ in terms of empirical estimation. This technique can be useful since SAM has been proven to be useful for many networks. Therefore, I would recommend this paper as weak accept. I would be more convincing the practicability of this paper if the authors can show the proposed ESAM works on ViT and MLP mixer, where the SAM has been proven to be very effective.<|endoftext|>Paper proposes techniques to improve the efficiency of Sharpness aware minimization method. They are Stochastic Weight Perturbation (Select subset of the parameters at any step) and Sharpness sensitive Data Selection. Strengths+ simple modifications that improves over existing SAM results+ Wide range of baselinesWeakness  lack of ablations across batch size (especially with the data selection algorithm)                            On cost of SAM  Cost of SAM is mentioned as 100% in the text of the paper. It would be good to know results against this standard version of SAM. Hyperparameter tuning   It would be ideal if the authors normalized for the training cost.<|endoftext|>I am a bit surprised that not computing the gradients for a random subset (e.g.50%) of weights leads to a significant improvement in performance. Do you have to do any tricks to achieve this improvement? It could be interesting to compare it to e.g.a random subset and $\mathbb{B}^{ }$. ## ImageNet resultsOne concern I have with the paper is that the original SAM paper [1] seems to report better results for SAM in some of the same settings than you do. Similarly, their results for ResNet 101 are better, and the results for PyramidNet on CIFAR are better. If the authors address this concern in the rebuttal, I am happy to recommend this paper for acceptance.<|endoftext|>This paper investigates the crucial efficiency issue of the sharpness aware minimizer (SAM). By analyzing the min max procedure of SAM, the authors observe the computational redundancy and then propose a method ESAM to improve the efficiency from the data and parameters perspectives. The authors argue that SAM can be approximated properly with fewer computations. Figures 3 and 4 are good for demonstrating. What is the exact impact of using SWP in large scale deep neural networks?
Reject; rating score: 5; rating score: 5; rating score: 6; The paper considers uncertainty estimation in overparameterized shallow neural networks with quadratic activation function. This is of course a pessimistic view, but to have an entire picture, we would need to have an algorithm dependent bound on the norm. The bound is a combination of the uniform convergence type bound on the excess risk and an interesting observation about strong convexity of the risk in the parameters of the network.<|endoftext|>Under the generative data paradigm (e.g., the hypothesis space contains the ground truth), the paper extends bounds on test (generalization) error under data distribution shift from the common setup when parameter identification is assumed to the setup where the ground truth parameter may not be identifiable (not unique). Then the paper shows how this technical extension can be applied to obtain test error bounds under distributional shift in three scenarios: bandits represented by an over parametrized quadratic NN; transfer learning with both covariate and label shift; neural network modulesStrengths: I think the technical extension and its applications presented in the paper are conceptually interestingWeaknesses:1. The technical details of the paper (at least the main paper; I don t have access to the Appendix yet) is not very satisfactory: here are some examples   Lemma 6.2, the statement contains G (Rademacher complexity of G), but G is never defined   In section 6, it seems to me that the neural networks in neural network modules can only be quadratic NN (in order to apply Theorem 3.6).<|endoftext|>The reason to adopt the definition of function identification analysis is that it is hard ti make parameter identification with neural networks which usually are over parameterized. The authors mentioned that the analysis is valid for over parameterized networks multiple times, but it is applicable simply because the formulation is very strong and avoids the parameters. The derivation then follows the analysis of quadratic neural networks. 2.Identifying the functions are very different from identifying the parameters.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper presents a hardware friendly sparse matrix multiplication that performs competitively on many transformer neural networks. I found the paper very interesting and timely, given the huge interest in transformer based networks. The first order approximation of butterfly matrices is a neat trick and there is empirical evidence as well as NTK analysis to show efficacy. LRA is renowned for being underspecified, with many of their hyperparameters inconsistently reported between the paper and the github repo. How did you ensure a fair comparison to the results reported in LRA? In python, how did you write the kernel? Is it simply $\gamma B+(1 \gamma)UV^T$? All of my comments/questions were addressed.<|endoftext|>The authors present a new method, Pixelfly, for static sparsity patterns in sparse training of matrix multiplication based neural networks. It is a pretty fundamental assumption in all of your work, that flat block butterfly + low rank performs better than either of them alone. This part is most interesting as it should show the advantage of your method over the “standard” butterfly approach. You should further check for typos in your appendix ;) (e.g.L.3)This is an interesting, well explained and soundly tested method.<|endoftext|>This paper proposes a method to accelerate training by replacing GEMM based compute intensive operations with sparse multiplications. Concretely, the Authors focus on a family of sparse matrices known as Butterfly matrices which have well studies properties and, crucially, have a fixed sparsity pattern with a very low non zero ratio. I think this is an interesting study that supports part of the claims of the paper. The Authors report large speedups on emerging architectures for Image Classification.<|endoftext|>This work proposes a new sparse parametrization method for layers of neural networks that rely on matrix multiplication. * The paper is well written and easy to understand; I especially appreciated the diagrams that illustrate different versions of butterfly matrices.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This work studies the problem of training class conditional GANs in limited data settings. The authors do not show their observations and experiments on C10 and C100. [W3 minor] The technical novelty of the paper is not much. and then finetuning for the target task. Having said that, the observations made as well as the application of this simple trick to mitigate mode collapse in cGANs are quite novel (empirically). The authors discover an interesting behavior of cGANs in limited data settings and propose a simple yet very effective method to solve the problem.<|endoftext|>In this paper, the authors work towards training conditional GANs with limited data. Weaknesses:1) In the paper, the authors claim that "the class conditioning training is more robust to data volume reduction" is an intuitive belief. Based on such intuition, one contribution of this paper is pointing that cGANs are easier to suffer from mode collapse. Usually, a stronger condition would lead to worse diversity. 2) Although the solution/proposed strategy is simple and useful, some principles are not discussed or explored very well.<|endoftext|>Overall, this is a good paper that addresses the limited data issue for conditional GANs. The paper is easy to follow for people familiar with the conditional setting in StyleGAN2. However, the conditional setting is not well introduced in the original paper which may confuses people when they read this paper. 2.How do the authors determine the subset of classes and images to be used for the experiments?<|endoftext|>In this paper, authors proposed a new training strategy which transfer the StyleGAN2 ada to the conditional version gradually by injecting conditional information into the generator and the objective function during the training phase. As the authors claim: " The proposed method for training cGANs with limited data results not only in stable training…" . More experiments should be conducted to verify that the proposed method can reduce the need for data. The results provided are only implemented on stylegan ada. Minors:   Details of L^D_uc, etc.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; I think this paper has a solid theoretical foundation of an interesting problem on tournaments. Overall, I think the paper should be accepted provided that the authors make more of an effort to relate their results to learning. You might want to list them in the introduction where something similar is also mentioned.<|endoftext|>The paper studies the relationship between dimensional representation of tournament and their structural characterization. Admittedly, the paper needed significant explanations from the authors. The results seem to be sound, although I had non insignificant troubles trying to verify them due to the writing style (see next paragraph on weaknesses)I think that the paper needs a thorough stylistic rehandling and it is not ready to be published in its present form.<|endoftext|>This paper studies the theory of tournament representations, i.e.low rank matrices $M$ whose sign agrees with the sign matrix of a tournament $T$. They also characterize completely rank 2 tournaments, and provide a forbiden class for rank $d$ tournaments. The proof of Lemma 3 would also be easier to understand if the connection between rank 2 representation and angles was made clearer.<|endoftext|>The results of this paper are overall nice, but I find some of results in the first part slightly weak. For rank 2 tournaments, the authors use this framework to describe precisely the forbidden configurations and show that they are actually equivalent to locally transitive tournaments, a class of tournaments closely related to transitive tournaments.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; Why are the policies in the memory buffer mixed by a weighted average of their parameters? In its current form, the paper is somewhat confusing to the reader, and could use from a clearer rewrite. More importantly, the novelty of the paper appears to be minor and the the design decisions of the author s algorithm do not seem to be very well explained. Following the author s argument for their  virtual policy, would it not be sufficient to use simply a properly designed trust region with respect to to it?<|endoftext|>The virtual policy is built by a convex combination of past policies in the memory through attention weights. The motivation to use attention is to ensure the virtual policy has good performance on current data. I agree with the authors that enforcing policy updates in constrained on policy methods considering only previous policy can be suboptimal and it is better to utilize the history of past policies to make the update more efficient and it can be a right direction to improve the performance of on policy methods. That being said, there are some concerns in terms of novelty, related works, and experimental results:   While using attention weights to combine the past policies is interesting and shows some promising results and I consider this is the main novelty of this paper, the idea of improving trust region constraint in on policy methods has been extensively studied in recent years ( which is not a weakness though). However, one of the main concerns with this paper is it fails to discuss and compares with two important related works [1,2] which are more relevant to this paper.<|endoftext|>And the authors propose MemoryConstrained Policy Optimization (MCPO) that uses an additional trust region relying on the virtual policy, the one that represents the history of policies. Pros: The overall paper is easy to follow, and the motivation is also attractive. The suboptimality of the previous policy in the trust region indeed cases undesirable constraints for follow up optimization. Experimental results also validate the effectiveness algorithm. It would be helpful to provide some theorem support, maybe under ideal cases (e.g., when the memory is not constrained).\3.<|endoftext|>This paper proposes a new trust region based policy optimization method with two trust regions constraints for policy updates which enforces the proximity of the current policy to (1) the old policy prior to the current update and (2) a virtual policy constructed using an attention mechanism and memory buffer which stores several past policies. I think this is quite a well written paper. The experiments are very well run, and I think it is one of the main strength of this paper. The authors demonstrated results on a large set of environments. Overall, I think this paper proposed some very novel and insightful ideas with well run experiments that would be beneficial to the community.
Reject; rating score: 5; rating score: 6; rating score: 6; The paper introduces a boundary aware self supervised learning framework for video scene segmentation. Based on the contrastive learning protocol, the authors employ DTW to generate dimidiate video segmentation from unlabeled video. The proposed pre training the main contribution of the whole paper with complex loss functions design. However, from table 1, we can see that without finetune, the proposed method is inferior to the old Grouping method.<|endoftext|>(+) According to their experimental results, the proposed method can achieve a new state of the art on the MovieNet SSeg benchmark. (+) This paper studies an interesting problem that make a video network aware of boundary infomation for video scene segmentation. I also would like to see the opinion from other reviewers. Experiments on pre training and transfer learning show that this method is critical to improving the video scene segmentation performance, and it achieves the new state of the art on the MovieNet SSeg benchmark.<|endoftext|>This paper presents a boundary aware self supervised learning framework for the task of video scene grouping. The proposed method achieves good results and outperforms previous methods. Cons  The proposed framework is not generic and hard to be applied to other methods for the task.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper derives a family of top k cross entropy losses which is a novel practice. 	The idea of this paper is pretty novel and exciting which makes the classification model robust. 	The extensive experiments conducted on five data sets are sufficient to show the advantages of the proposed ideaWeaknesses:	The details of the differentiable sorting networks is not represented.<|endoftext|>Performances are mainly compared to cross entropy showing low improvements. another main weakness is that no significant improvement of the proposed loss over cross entropy is shown. The reported top K accuracy gains are not systematic and so low that they may be not statistically significant. Using sorting networks for top k is an acceptable strategy from a practical point of view but a bit over kill and not very new from a theoretical point of view.<|endoftext|>Based on the recent progress on differentiable sorting and ranking, the author proposes a loss function for top k classification where the k is not fixed but follows a given probability distribution. To improve the efficiency, a splitter selection network is proposed so that fewer layers are required for the sorting network. In experiments, the loss function is shown to be effective in training a model from scratch on Cifar10. The idea of using different probability distributions for k is interesting. The results also demonstrate the effectiveness of this idea.<|endoftext|>The loss is used for fine tuning in most experiments (apart from the CIFAR100 case). Strengths:  The efforts on optimizing the top k classification learning through differentiable sorting appear novel to me. The best results come when you have the top1 and the sum of the top five values. 88.35 to 88.36 is certainly not statistically significant. I think that would be a fairer comparison.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; rating score: 8; The teacher policy in all experiments in a neural network trained with PPO. The conversion from pixels to objects ("geometric features") is assumed solved. for the results to be reproduced. There is a bit of missing related work. A similar work is "Planning From Pixels in Atari With Learned Symbolic Representations" (Dittadi et al.2020).Additionally, the litterature of inductive logic programming and case based reasoning applied to (video) games is relevant, as inferring case based policies from demonstrations has been an active area of research (e.g."Case Based Planning and Execution for Real Time Strategy Games" (Ontanon et al.2007)).The experimental results show that a more robust policy than the PPO and A2C learned policies gets extracted for most of the subset of games that the authors picked (also exemplified in Figure 4.b.). But, this does not necessarily justify the choices of the RoundTourMix algorithm (central contribution of the article). Indeed there is no justification why one has to make a random selection (2.Guess and Observe) of the geometric relations, and why we can t simply be exhaustive for those games (I believe we can at least for Pong and Seaquest). "Transferability validation" is an interesting experiment, that shows that the symbolic policy (distilled from a CNN trained with PPO on AdventureIsland 3) transfers to a similar but different game, AdventureIsland 2. Sadly, there was no apples to apples comparison by having a PPO trained policy simply plugged onto the geometric symbol representation. This would have allowed to show how much of the transferability comes from the features and how much comes from the "if this then that" policy. The central contribution of the paper (the RoundTourMix distillation procedure) seems ad hoc and many of its decisions are not well enough justified. The experimental validation has some flaws that need to be addressed to make it a convincing contribution. In its current form, the contribution is too "heuristic y" for publication at ICLR.<|endoftext|>Results are shown on Atari games and AdventureIslands, which demonstrate that the distilled symbolic rules retain the original performance level while being more resistant to domain shift. The proposed algorithm is also clearly explained and contains enough details to reproduce. * To my knowledge, the proposed method is a novel approach compared to prior works. In Figure 4, the distilled symbolic rule recovers and even occasionally outperforms the original teacher NN policy, which demonstrates the effectiveness of RoundTourMix. **Weaknesses**While the results on Atari games and AdventureIsland are strong, I am primarily concerned with the applicability and effectiveness of the method on domains beyond toy 2D pixelated games. The current symbolic representation cannot handle any of these complex scenarios. What about deformable objects? Objects that change state (like cooking tasks)? Navigation in 3D world? 3.The symbolic rules do not seem to have any mechanism for long term memory. How can it be applied to tasks that require long horizon planning and reasoning, rather than short term "muscle memory"? PPO + LSTM solves this out of box. 4.The "transferability validation" experiment in section 5 gives an unfair advantage to rule based policy, because it makes the strong assumption that there exists a perfect object recognition system for all game levels, even with very different visual appearance. However, similar "denoising" techniques can be applied to PPO as well, in the form of robust representation learning. Furthermore, I strongly disagree with the general claim that "our symbolic policy distillation approach captures the causal structure through data driven experience" (introduction section). First, the symbolic rule distills from an NN trained policy, which necessarily means that it will inherit any spurious correlation learned from the teacher. Even though the authors introduce tricks like "denoising", it will not be able to recover the true causal structure without any explicit mechanisms like do calculus or counterfactual reasoning. For example, what if the Pong game is slightly stretched and the `X_pong` coordinate threshold is 45 instead of 40? While the paper is overall clear and easy to follow, it makes many strong assumptions that are tailor made for simple 2D games. It is unclear how the symbolic rule framework can scale to more complex domains. I give my rating due to the limited usefulness and problematic assumptions of the proposed approach in realistic tasks.<|endoftext|>While the symbolic entities for each task are predefined, their algorithm is able to extract out a symbolic representation of the visual input as well as a symbolic policy in terms of a decision tree. The authors show that the distilled policy is highly human interpretable and even generalises better than the original teacher policy to tasks that are visually different but symbolically identical to the original training task. It by no means solves all the problems need to bridge symbolic and black box deep RL, but having this work out there will allow others to build upon it. The authors  approach of distilling a black box teacher policy into a symbolic one is quite interesting. The main strengths and weaknesses of the paper are as follows. * the symbolic policies show superior transfer compared to their original teacher policies, showing the promise of symbolic policies in offering better generalisation**Weaknesses:*** some of the empirical results could be cleaner. This is the main weakness of the paper and is what makes me hesitate in fully recommending the paper for acceptance at this stage. Some of these issues, I believe, can be remedied by the authors during the discussion phase. * how many random seeds were used for PPO, A2C and the distillation for all tasks? * the Atari results (Pong and Seaquest) are non standard, making comparison with baselines in the literature difficult. First of all, they only allowed the agent a single life, and they train for 10M steps only (whereas 200M is more standard for published Atari results). Because of this, the paper s scores for PPO and A2C are far lower than what is in the literature. * it would also be helpful to know whether the authors are using an open source release of PPO and A2C or a reimplementation. If it is an open source release, could the authors reference which implementations they used? The Botvinick et al paper is about hierarchical RL (hRL) in neuroscience. * p5, 2nd paragraph of the subsection "The geometric operator search space"   "the nearest class $i$ ($i$ is given) object from the **protagonist** in the current observation": usually, "protagonist" refers to the main character of a story. So this paper does have promise. But what prevents me from fully supporting it just yet are issues with the empirical evaluation. I would, in particular, like to ensure that the PPO and A2C baselines are accurately generated and that the results are robust to the choice of random seeds before I can fully support the paper s acceptance.<|endoftext|>The paper presents a novel algorithm that distills expert experience in the Gym Retro Games environment into an interpretable and symbolic policy. Finally, their novel algorithm makes use of genetic mutation to discover rules that consume these relations and take actions. The last part is trained to match the data from the expert and only then deployed for evaluation in the environment. Their empirical results show that their method can perform well and even outperform the expert policy used to collect the initial experience. Moreover, they show that their proposed method transfers much better compared to the end to end baselines (PPO and A2C) from AdventureIsland3 to AdventureIsland2, making it a more robust method. Strengths:  To my knowledge this is a novel algorithm that shows promising results on a small number of games and shows to be more robust to changes in the environment. Resulting decision making is easily interpretable by humans  The work is very clearly presentedWeaknesses  Minor: “Abstracting pure symbolic rules from data is not new. The family of symbolic regression (SR) methods (Cranmer et al., 2020; Runarsson & Jonsson, 2000; Gustafson et al., 2005; Orchard & Wang, 2016) have been studied to to directly.” Remove double “to”. It would be very interesting to know whether the authors have thought about whether their proposed method could be extended to learn from online data generated by itself. I am curious about how their method would perform in a game like sokoban that requires careful planning, or others, where the agent needs to remember information that is no longer visible in the current observation. I recommend a weak acceptance for the paper, due to the novelty of their algorithm and the interesting empirical results presented, where the final policy is very interpretable. My main concerns are with the wider applicability of the method.<|endoftext|>The authors propose a novel method for finding symbolic policies for image based RL environments. This can result not only in interpretable, lightweight policies, but also policies that generalise better than the original teacher policy. Train a DRL agent directly on the environment, and then, given symbols extracted from each state, "distill" the policy into a symbolic policy (note that supervised learning applied in the context of imitation learning is known as behavioural cloning [1], and what is being done here also falls into the category of imitation learning/learning from demonstration). It is difficult to find symbolic policies from scratch, so this procedure makes sense, and results in policies that are (generally) more interpretable and generalise better. As it is, the algorithm cannot be applied to more complex visual domains. However, I think we should not discourage research along these directions, as the general idea of distilling black box policies into symbolic policies has good use cases. Some questions:  How important is the entropy threshold? Any way for setting/tuning this automatically? Are there any (quantitative) ablation studies on the denoising procedure? It would be useful to know if the algorithm fails without this or just has worse performance. Considering that the authors  method incorporates imitation learning, they may find methods such as DAgger [2] of interest. Assuming access to an expert policy that can be queried interactively, interactive imitation learning methods benefit from reducing the shift between the teacher s state action distribution and that of the student (which is a known problem in behavioural cloning, and is potentially an issue for this method as well). To the best of my knowledge, there are solid contributions in this work, and I would recommend accepting this paper.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors present a novel, high resolution dataset for precipitation downscaling collected from observational data in the southeastern US. In addition to the dataset, several novel metrics for evaluating statistical downscaling methods are presented, as well as a novel deep learning algorithm based on video super resolution and "implicit dynamics" estimation. This work was recently submitted to NeurIPS 2021. The authors have made a few improvements to address the concerns raised in this original discussion:1) Cross validation results are presented to provide a more robust comparison between models. 2) The discussion of metrics in section 4 is much more thorough than in the previous version. 1) **Quantitative results do not include standard errors or any indication of the variance in cross validation. 2) **Inadequate discussion of hyperparameter tuning, particularly for the baselines. ** The literature review is sparse and does not provide any discussion of how the authors work fits into the broader context of the literature on applying deep learning to statistical downscaling. 5) **Language and formatting errors. Several such issues that were pointed out directly by reviewers in the previous discussion still remain, which seems to indicate that the authors did not bother to further proof read or revise the text. Due to these quality deficiencies, I cannot recommend acceptance of this paper in its current form.<|endoftext|>This paper proposes a dataset called RainNet for studying precipitation downscaling. Finally, the paper compares several superresolution methods from computer vision literature, traditional statistical approaches to downscaling, as well as the proposed architecture/method on the proposed dataset to form a set of benchmark results. However, this paper is lacking in several key dimensions:  First, the paper does not adequately describe the proposed dataset itself:    From Section 3.1 it seems that the data comes from the NLDAS dataset and the NCEP Stage IV 4km gridded precipitation data. Which products exactly were used? However the paper does not adequately describe the dataset, and the benchmark results are not reported with measures of uncertainty, which makes comparisons impossible. Second, the reported results (Table 1) are from a 17 fold cross validation procedure over time, however are shown without standard deviations or confidence intervals. As is, it is difficult to draw conclusions about the quality of the different approaches. Finally, there are many claims and details throughout the paper that need to be cited/addressed. "The precision of weather and climate prediction is highly dependent on the resolution and reliability of the initial environmental input variables, and spatial precipitation downscaling is the most promising solution."<|endoftext|>This paper presents a $0.4^\circ$ dataset for precipitation downscaling. It further shows 14 SOTA models with/without ML for evaluation on the dataset. From that perspective, this paper is a good contribution to the field of AI driven meteorology where such datasets are in abundant need. The evaluation of the different models on this dataset provides a good benchmark as well. I feel that a more meteorology focused journal is a better venue for this work. As I mentioned in my review, this is a great paper proposing a new dataset that is well constructed and evaluated and is high resolution. However, I don t feel that ICLR is the right venue for this work.<|endoftext|>The paper provides a baseline method that generates a dataset of high+low res precipitation maps. Provide well thought out experiments (reproducing other methods etc) along with their baselines on their new metricWeakness:   One way to evaluate how much better are high res images is to use them for potential Applications to downstream tasks which is not provided. The paper does not provide any uncertainty estimation. As the authors mention Ml readiness; its also important to think about how will an actual domain scientist use this work. This is where evaluating downstream applications and understanding uncertainty in the results is crucial. This part is missing, other than that its a clear and concise approach.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposed a stable and effective regularizer to prevent overfitting. Specifically, this paper proposes individual Flood. Different from Flood which constrains training loss on mini batch level, iFlood gives instance level constraints on training loss. + The idea of iFlood is neat. My main concerns are on the effectiveness of iFlood:+ There are many regularizations which can prevent overfitting.<|endoftext|>This paper introduced a simple but effective method to mitigate overfitting, which modifies the existing Flooding scheme. This paper is well organized and easy to read. The theoretical analysis is straightforward. My primary concerns are listed as follows:1.<|endoftext|>The empirical improvement compared to standard flooding is encouraging. I found the paper interesting and potentially have a strong impact. Experiments on noisy labels show significant improvement.<|endoftext|>This paper extends the idea of Flooding to a sample specific level and call it iFlooding. The idea of extending Flooding to iFlooding is very intuitive, thus it s fairly clear that such extension is needed.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper proposes a new theory for understanding contrastive representation learning. Prior work has identified alignment as one of the factors of contrastive learning, but have not investigated how different types of augmentations may affect the learned embeddings. Empirically, the authors verify that too weak or too strong augmentation harms performance. Weaknesses:+ The theoretical results are a bit weak. E.g., as pointed out in paper, Thm 4.8 only talks about the minimizer of the contrastive loss. But can there be a version with the perfect alignment assumptions relaxed into approximate alignment? The paper provides a theoretical analysis on the interplay between alignment and augmentations. Empirical experiments nicely complement the theory, and lead to interesting metrics that reveal the properties of this interplay. Overall the paper is also nicely written. Thus, I recommend acceptance.<|endoftext|>This paper seeks to augment that understanding of contrastive learning using a new perspective, focusing on the role of data augmentation. It is well known that contrastive learning techniques are highly sensitive to the data augmentation schemes used, most notably discussed in [1]. In this work, the authors interpret augmentation as a way to connect different intra class images together. The metric is inspired by their augmentation oriented understanding, and was also found to align well with downstream performance. They then build off the analytical work of [3] to prove guarantees for the downstream performance with a relaxed assumption. [3] Saunshi et al., A theoretical analysis of contrastive unsupervised representation learning, 2019. In this exploration of data augmentation, much emphasis has been placed in the concept of "augmentation strength"   but what about the choices of augmentations themselves? They provide theoretical guarantees on downstream performance, and propose an interesting new metric that can be evaluated using only the given unsupervised data. Overall I think this is a strong submission, and would recommend an accept.<|endoftext|>The authors provided a new understanding of contrastive learning from the perspective of data augmentation for intra class samples. Theorem 4.2: For the downstream classification, the loss is upper and lower bounded in terms of the L_NCE loss. Please add discussion on the practicality of this assumption, and show an example on some datasets if possible. .In the experiments, RandomResizedCrop is used to illustrate the relationship between Aug Strength and ACC(ARC). The best performance for different datasets all achieves at Aug Strength   0.92. The authors only showed results on RandomResizedCrop. Do you have similar conclusion as that for RandomResizedCrop? 2) Based on the proposed analysis how to find the sweet spot of data augmentation for contrastive learning is crucial, but this is not discussed. The idea of understanding contrastive learning from the perspective of data augmentation for intra class samples is interesting. However, 1) some key assumption for the analysis is too strong; 2) the analysis on the existing contrastive learning algorithms is preliminary and needs more work; and 3) the authors emphasized the importance of finding the sweet spot of data augmentation (ie, perfect overlapping).<|endoftext|>Given that their results for spectral contrastive learning hold for the setting being considered in this paper, it is worth making a more detailed comparison to that paper. While there are some interesting aspects in the paper (especially the ACR metric), the theoretical analysis seems to have raised many questions and concerns that I have summarized below (details in main review). More on this in point (W3) of main review. **Strengths**:  (S1) The problem being addressed is very relevant. but connection to theory is weaker than it is made out to be. The ACR metric that can select good augmentations using just unlabeled data is also an interesting finding. Proposition 4.7 should only be true for $f^*$ and not all $f$. **Note that the concern here is not just that the assumption is too strong or unrealistic (which is often unavoidable and acceptable), but that its not clear when the assumption can even be true and whether or not it is mathematically compatible with the rest of the setting.**. (W2) (Non )vacuousness of bounds: I found Theorem 4.2 interesting since it can show a bound similar to the bound from [1] but without the conditional independence. 2020.The paper aims to provide some theoretical analysis for contrastive representation learning under weaker assumptions than prior work (like conditional independence) and has some interesting empirical findings about how performance of augmentations can be ranked using a metric that depends just on unlabeled data. (here I used $\|f(x)\|   1$). A discussion about the vacuousness (or not) of the bound can be critical in understanding whether the bound is indeed meaningful.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The authors propose a graph autoencoder architecture using what they term as Neighborhood Wasserstein Reconstruction (NWR). They show experimentally that this reconstruction loss improves the embedding performance in structure oriented graph mining tasks. Experimental benefits on both toy and real world datasets and additional exploration as to why these improvements might be occurring. Well motivated and well placed in the literature, the empirical comparisons are extensive. Some of the choices in matching neighborhoods seem a bit arbitrary and not sufficiently justified (see questions below), although this is somewhat understandable as there are large number of tricks used in this paper. Is it the Wasserstein distribution matching that is important, or just that it helps to reconstruct something about the neighborhood of each node.<|endoftext|>The paper proposes a novel approach to graph representation learning. In particular, a graph auto encoder is proposed that aims to better capture the topological structure by utilising a neighbourhood reconstruction and a degree reconstruction objective. An optimal transport based objective is proposed for the neighbourhood reconstruction that optimises the 2 Wasserstein distance between the decoded distribution and an empirical estimate of the neighbourhood distribution. An extensive experimental analysis is performed, highlighting the benefits of the proposed approach on a range of synthetic datasets to capture structure information. The experimental results also highlight its robustness across 9 different real world graph datasets (ranging from proximity oriented to structure oriented datasets). The method is intuitive and the way that the neighbourhood information is reconstructed appears novel. How is the neighbour sampling handled when a node has less than q neighbours?<|endoftext|>This paper studies the problem of graph representation learning with graph autoencoder. As a result, the paper proposes a graph autoencoder architecture that trains the GNN in an unsupervised manner. The key idea is to develop a decoder to reconstruct both the node degree and feature distribution. Experimental results show that the results outperform existing autoencoder baselines in several datasets. However, the concept of the structure or proximity information of a graph is not defined and introduced well at all in the introduction, making the paper hard to follow at the very beginning. **Method**  The time complexity of the proposed method looks very high. The paper briefly describes the time complexity of Eq 8. The idea of training a graph encoder in supervised manner links to the early graph embedding and the recent self supervised learning (SSL) for graph data. But it is unclear why GraphCL and MVGRL are only compared on the real life datasets but are missing in the synthetic datasets in Table 1. These datasets are identified as *Proximity* oriented datasets in Table 2. 3.All datasets are relatively smaller in this paper. However, the scalability of the proposed method is questionable.<|endoftext|>The paper proposes a new loss in graph auto encoder (GAE) model for unsupervised learning, composing of degree prediction and Wasserstein distance, which helps to identify structure information better vs the original loss. The appreciated side of this paper is the correctness of the method, detailed illustration of implementation and the thorough experiment comparison. The disadvantage is on the novelty side, considering that i) enforcing awareness of the context of nodes to highlight structure information is not new in the graph field (https://arxiv.org/abs/1905.12265, etc); ii) the employment of the OT theory into networks existed in even a more fancy way (https://arxiv.org/pdf/2003.03892.pdf, https://openreview.net/pdf?id ATUh28lnSuW, etc). I would recommend digging further into either theory or ablations to strengthen this interesting work, to characterize the real "structure information" captured by the proposed method compared with literature, probably following the thought as: 1) defining structure information (SI) and its quantitative assessment metric; 2) showing the proposed method can capture certain SI but the existing methods cant; 3) showing OT can capture better.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper proposes an extension of GCSL where the goal conditioned BC loss is weighted by a variable that correlates with the number of steps necessary to achieve the desired goal. results of the random   >  results on the random . Overall I vote to accept this paper, but would appreciate responses from the authors on my questions. The idea, although somewhat an obvious followup to GCSL, is well explained and analyzed, and experimental results are convincing.<|endoftext|>The paper proposes weighted goal conditioned supervised learning (WGCSL) as a tighter lower bound to optimize than the typical GCSL objective. The paper is generally positioned well wrt prior work but more discussion of some closely related work is needed for clarity. While the empirical results of the proposed approach looks promising, as pointed in the weaknesses, I have concerns about clarity of the paper, presentation of the theory and empirical rigor. If the authors can address these issues, I d be willing to re assess the paper. The empirical reporting and rigor of the results should be improved. Thus, are the comparisons fair?<|endoftext|>The proposed method proposes a method for goal conditioned RL that can be interpreted as a weighted version of prior work.These weights resemble a combination of discounting and advantage weighting. My main concerns are about the clarity and correctness of the paper (the empirical results are already great!). **Minor comments*** Abstract: It s not clear what problem is being solved. I d recommend making this more clear in the first 1 3 sentences. This should be trivial to implement because the proposed method already includes a form of advantage weighting. Conference on Robot Learning.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper proposes a new type of anchor based GNN by implicitly exploiting node positioning within customized message passing steps of MPNN. • The notation system used in this paper appears to be very confusing.<|endoftext|>The paper proposes a GNN method (GIR) that incorporates the paths produced by the BFS algorithm in the message passing algorithm, such that it enables the method to mimic the shortest path algorithm. The experiments are conducted both on synthetic and real world datasets. However, the current form of the paper lack clarity and I consider that many aspects of the method/experiments should be better present in order to be clearly delivered to the community.<|endoftext|>This paper proposes the Graph Inference Representation (GIR) model, which first applies an anchor indication strategy and a forward process that mimics Bellman Ford algorithm. The idea of mimicking Bellman Ford algorithm sounds interesting and novel to the reviewer. The presentation of the paper could be improved. 2.The experiments between MPNN and GIR in Table 2 is not consistent. interesting idea, but the presentation is hard to follow and the empirical results do not seem very significant<|endoftext|>In this paper, the authors propose the anchor based framework for position encoding. It is well known that the message passing framework inherently has a limitation to encode graph structure and sometimes fails to discriminate isomorphic subgraphs. The authors show that the MPNN framework can keep track of the shortest paths using anchors. Even some might be suited for effective coverage.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper deals with the following setup: offline imitation learning in the presence of both an expert dataset and a non expert dataset. More precisely, the goal is to learn a policy as close as possible to the one(s) that generated the samples in a dataset $D_e$, while making the most of samples in a non expert dataset $D_o$. They interleave the training of a discriminator and a policy. **Strengths**  The paper is overall well written and easy to follow. The setting is very interesting and has a great potential impact on the community. with very random data in D_o? Writing      In the abstract, the authors say “both optimal and non optimal expert behaviors”.<|endoftext|>The paper proposes a new offline imitation learning algorithm, DWBC, for datasets that combine both optimal and suboptimal demonstrations. The approach is based on a modified behavioral cloning loss that weighs expert and non expert data based on a learned discriminator. As a by product, the method learns a discriminator that can be used to estimate the relative performance of any policies without rolling them out in the environment. The main innovation behind DWBC is to learn a discriminator and condition it on the policy that is being learned. The datasets used in the experiments have a really particular form as they are collected during online training and thus the expert and the sub optimal data are highly related. In the offline policy selection experiment, do you use separate datasets for training the discriminator and evaluation? The results presented in the paper are promising, but there are several issues with the clarity of the derivation and some with the experiments, and thus the paper is not yet of sufficient quality to be published as is.<|endoftext|>This paper proposes an offline imitation learning framework that incorporates both optimal and suboptimal datasets to learn decision making tasks, without requiring any reward annotations. On one hand, the introduction, preliminaries, and related work are well laid out. I hope that the authors can diligently address the concerns that I raised, at which point I will reconsider my recommendation. The method appears to be novel and the discussion framing this method as a weighted behavior cloning objective connects nicely to prior work. In particular, why is “providing as little information in $\log \pi$ as possible” the best way to learn a discriminator? This subsection is, in my opinion, the weakest part of the paper.<|endoftext|>The authors consider the problem of offline imitation learning in the presence of suboptimal datasets. In the presence of suboptimal data, classical baselines like behavior cloning suffer performance hits, the drop in performance often correlates positively with increase in number of suboptimal trajectories. Strengths:+ The paper is well written and clearly communicates the motivation and contributions. + Sufficient empirical experiments which support the core thesis of the paper. + Using the jointly learned discriminator seems like a novel idea for policy optimization and evaluation. While the authors demonstrate some improvement over the provided baselines, I would encourage the authors to consider adding couple of more ablations, particularly across datasets with a mix of human and random trajectories.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; 2.The claim in abstract that with label smoothing "kNN density estimator performs better as a OOD detection method theoretically" is not supported at all. While from the experimental results it appears that label smoothing generally improves kNN based OOD detection, I have many issues with the paper:1. The authors claim that softmax probabilities cannot distinguish between ID and and OOD points because "softmax probabilities sum to 1". While I think there is some merit in the proposed algorithm, a lot of the claims made in the paper are not well supported and are extrapolated too much from the results of Muller et al.The theoretical results are perfunctory and the writing it very informal.<|endoftext|>This paper proposes a OOD detection method based on the k nn density estimator. While the theoretical results in the paper are interesting, these results are not connected to the implementation of the methods. Pros  The paper is mostly well written and easy to follow. However, the proposed method is not applicable to the large dataset and the authors performed experiments with k 1.<|endoftext|>By empirical experiments, the author(s) demonstrated the effectiveness of the proposed method. And theoretical results for the proposed OOD method were provided. In addition, this paper is generally well written, but some places (some issues) in this paper should be further clarified/fixed. The format of many references is inconsistent, for example,     Conference names are sometimes abbreviated and sometimes not;     Authors  names are sometimes abbreviated and sometimes not;  Some tables are presented as a Figure, resulting in inconsistent font sizes in the tables; for example, Figure 3. However, I think that while this is an interesting paper, it would be better to compare more of the new OOD baselines to make the proposed method persuasive.<|endoftext|>This paper proposes a method for Out Of Distribution (OOD) detection that relies on the k nn density estimate in the feature space of a classifier model. Experiments also include ablation studies showing that the proposed method is not very sensitive to some hyperparameter settings, such as the number of nearest neighbors and the amount of smoothing. The experimental results reflect the benefits of label smoothing on the OOD prediction task. However, it would make more sense to derive similar results for the statistic T(x), which is used as the OOD score. Based on the experiments, however, the authors suggest setting k   1. The paper is well written.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; In this paper, the authors fine tune BERT to find affective lexicons and show high correlations between estimated values and dictionary values. However, the paper is filled with a lot of drawbacks from methodology, style of writing, and results.<|endoftext|> This paper uses a fine tuned Bidirectional Encoder Representations from Transformers (BERT) model for finding affective lexicons based on ACT theory. Strengths:The idea of using a pre trained model to  aid in finding a sentiment lexicon based on ACT theory is interesting. Weaknesses:The experimental analysis is clearly insufficient.： The data set used for the experiments was not presented. The parameters involved in the methods, such as learning rate, batch size, etc., are not described  No relevant work of others was cited. The analysis about the experimental results are too superficial.<|endoftext|>Comments on Methodology:Firstly, the authors state they use a regression model to finetune the output of their neural network, the details of which are unclear. The authors need to elaborate on how finetuning was done with the model. For instance, the authors could also compare the model s performance using shallow word embeddings as well as contextual embeddings from advanced transformer based architectures. They could further include the data preprocessing and implementation details for the experiments in a separate section. This line is incomplete.<|endoftext|>This paper proposes a new sentiment representation method by using Affect Control Theory (ACT) and BERT model. +Ves+ The usage of Affect Control Theory is very interesting and the first work. Concerns The ablation studies and some experiments should be expanded. Many experimental details are insufficient, such as parameters. How to use this model to extend affective dictionaries? The idea is interesting and results show the effectiveness.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; w2) In the experiments, the evaluation setting is too simple. This work proposes a pipeline to learn factored world models to predict the robot actions’ effects. This paper applies the proposed approach to pick and place tasks with simple shapes, and extensive experiments indicate its effectiveness.<|endoftext|>Unlike previous works, where new objects are discovered by the model from raw pixels, the authors here use the simulation environment to segment objects appearing in the scene and factor them as part of the state representation. This work s novelty focuses on the application of an attention mechanism and stacking multiple GNN layers.<|endoftext|>al, ICLR 2020Overall I feel that the paper lacks a concrete quantitative evaluation of the action attention module (which is the core contribution of the paper). This is also shown clearly in Table 1, where the performance is almost similar (a slight increment/decrease on the RMSE/Hits@1) to FWM. Could the author(s) clarify this? These two are often worded one after the other which makes the reader think that it is the action attention that is major contributing factor to the generalization.<|endoftext|>The presented approach introduces interesting and novel ideas and shows that the model is able to generalize to tasks that it was not trained on. Experiments are conducted in simulated environment using a robotic arm interacting with two sets of shapes. Based on the ablation study, however, that this mechanism does seem overly important. The submission is well written, the authors are able to communicate their ideas well, but placing the related work section at the end of the submission is confusing.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; ## Summary  This paper proposes a probabilistic model called Attention Driven Variational Autoencoder (ADVAE). This model is another instance of $\beta$ VAE whose encoder and encoders are composed of Transformers rather than previous neural architectures such as RNN. Moreover, this work presents a new way of quantifying syntactic disentanglement between latent variables, relying on the information obtained from the attention matrices of the Transformer architecture. The experiments show that the proposed method is quantitatively better than the normal VAE, and that swapping the value of a specific latent variable can impact the generation of the target word (decided by the syntactic role of the latent variable we choose). ## Strengths (Reasons to accept)  The paper is generally well formed. ## Weaknesses (Reasons to reject)  I m not sure this work is the first that attempts to combine Transformers into the VAE framework. Considering that the main novelty of this paper comes from the fact that the authors suggest exchanging the previous RNN to Transformer, it is doubtful this work has a meaningful novelty. It is encouraged for the paper to self contain the specification of the exact Transformer architecture (one with co attention, Lu et al (2019)) utilized in this work. There is no (theoretical) guarantee that the proposed method is specialized for assigning specific syntactic roles to latent variables, except that the method (by relying on the existing $\beta$ VAE) encourages the latent variables to "generally" represent different aspects of sentences. Please explain why the proposed framework should be decent for disentangling "syntactic roles" rather than other linguistic or semantic properties. To summarize, even though this paper demonstrates that Transformers with $\beta$ VAE can be an attractive option for generating latent variables representing some syntactic roles, its novelty is generally limited to the fact that it proposed a new evaluation protocol aimed at estimating the extent to which each latent variable is mapped to a specific syntactic role.<|endoftext|>This paper proposes a new model ADVAE, which uses a sequence of latent variables which are constructed using cross attention, which are then used to condition the inference model. Despite this, I still feel that a more general approach, able to disentangle different factors, or in different languages would add more substance that I feel this paper is lacking. The work claims that the latent variables, in this case, are able to disentangle content effectively, which I am inclined to agree with, however, I feel that the experimental setup is lacking to solidly support this hypothesis (which I detail in the “cons” and “questions” section). Please either make the proposed model LSTM based or the baseline a Transformer. I feel that the ADVAE, while explained relatively clearly in the figure, could benefit from a step by step explanation in the text. An example of a paper that I feel does this well is Zhang et al., 2016. Rather than the method per se, I feel that this insight into unsupervised disentanglement is interesting, however, constraining the method to syntactic roles can be limiting. For example, for the “low complexity” datasets (e.g.sentiment analysis), it would be interesting to see when running your method, is there a vector that emerges that can control sentiment, or is it able to both disentangle syntactic roles and sentiment, etc….? (As far as I know, John et al.2019, did this in a supervised manner). is not defined. I would be curious what would happen if only cross attention would be used. I would be willing to raise my score if experiments on other aspects or in other languages were done. This has clarified some of my concerns and added extra evidence to the paper. For this reason, I will raise my score to a 5.<|endoftext|>This paper propose a framework to obtain the disentanglement of syntactic roles as latent variables for sentence representations. The model is an attention driven VAE which maps syntactic roles to separate latent variables using an encoder decoder framework. In the second part of the paper, the authors introduce an evaluation protocol to quantify disentanglement between latent variables and spans both in the encoder and in the decoder, which includes syntactic role extraction, latent variable influence on decoder, encoder influence on latent variables and disentanglement metrics. Providing a series of evaluation protocol aimed at measuring the disentanglement of syntactic roles, which can be used by other work focus on syntactic disentanglement for sentence representations. Weakness:  The paper is hard to follow. The evaluation protocol only contains intrinsic evaluation, but how these learned latent variables can help the downstream tasks is not clear. Overall, I think the paper provides an interesting perspective for learning syntactic disentanglement. To help the reader get the background knowledge, the authors can provide some related work in learning disentangled representations at the very beginning of the introduction (basically just remove some part in Section 6). There are some essential information in the appendix that really need to be moved in to the main body, e.g., some analysis of the latent variables and syntactic roles. The syntactic roles used in evaluation protocol are from a dependency parsing instead of gold standard one. Have you considered testing on texts that have annotated dependency structures? What does the predicative structure in Figure 1 do with the syntactic roles discussed in the paper? As a final comment, this work does some contribution for learning and evaluating syntactic disentanglement for sentence representations, and will be helpful to the community.<|endoftext|>The paper proposes a method for unsupervised disentanglement of text components and shows its ability to identify semantic roles. To this end, they investigate how resampling of individual latent variables impacts the semantic roles in the text generated by the decoder, and how syntactic roles are aggregated into latent vectors via attention. They find that their proposed architecture is more successful at disentangling semantic roles into the latent variables than standard VAEs. Strengths:* The paper studies a research question that is at the heart of representation learning, which could potentially be very impactful* The model is based on an intuitive and simple idea* The results are convincing and well presentedWeaknesses:* No ablation studies. In my opinion, the results are not actually conclusive about what model components are responsible for the improvements. Moreover, the VAE baseline seems to be significantly smaller than the Tranformer. I think a fairer comparison with minimal changes between the baseline and proposed model could be made as follows: Pool the outputs of 2b) before estimating a single Gaussian distribution, which can then be used as a standard VAE. * The concrete architecture is not well motivated: It is not clear to me why the inference network architecture has to look the way it does in 2b). Does the model actually use co attention (i.e., sentence representations and latent variables serve as queries for each other)? Figure 2b) looks like a vanilla Transformer encoder decoder architecture to me, which uses cross attention, not co attention. Moreover, it is not clear that the model actually needs self attention to refine the latent variables. Moreover, a similar work (Behjati and Henderson (2020)) is not discussed, which casts doubt on the novelty of the approach. I therefore recommend to reject the paper in its current state, but I am happy to change my scores upwards if my concerns above can be resolved.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The paper proposes an objective for contrastive training of representations based on noisy cluster information. In particular, the objective encourages samples from the same noisy cluster to have similar representations and representations from distinct clusters to have dissimilar representations. The authors show that representations learned with this approach often outperform representations learned with only self supervision (and no noisy clusters), and also that the proposed approach to using noisy cluster information outperforms other baselines using noisy cluster information. The authors further show that they are able to alternate representation learning and clustering in order to learn better representations that self supervised approaches even without initial noisy clusters. Strengths:  The paper is very interesting. Weaknesses:  Fairly minor:    Some of the presentation could be a bit clearer. If so, is it actually necessary to alternate K means and Cl InfoNCE, or can we just use the initial random clustering? This is an interesting paper with results that do an impressive job of supporting the paper s claims.<|endoftext|>This paper proposes a weakly supervised contrastive representation by using the auxiliary clustering information. Data are clustered with auxiliary tags and the clustering InfoNCE loss is introduced to optimize the system. Strengths：  The idea is clear and simple, and the authors give theoretical definitions to support their assumptions and experiments. The authors also present experiments to validate it. The proposed approach can also be applied without auxiliary information with K means and EM optimization. Comparison with related work is required, such as [1][1] Weakly Supervised Contrastive Learning. ICCV 2021. Overall, this paper proposes an easy but effective method for weakly supervised contrastive learning with auxiliary information.<|endoftext|>The paper proposes a weakly supervised contrastive loss based on using existing auxiliary information from datasets (such as shoe attributes for classifying shoe categories (UT zappos50K)). The auxiliary information is used in a contrastive loss formulation similar to the unsupervised contrastive loss of (Chen et al., 20) and the supervised contrastive loss formulation of (Khosla et al., 2020), but instead using clustering information from auxiliary information (Eq (1) of the paper). There are additional experiments comparing to a few other clustering based contrastive loss approaches. (I d like to see more investigation of the labels though, see below.) Weaknesses:  My main concern is that the paper does not sufficiently address other relevant works using weakly supervised contrastive loss, and so the contribution of this work is not clear. The authors explain that "Cl InfoNCE performs better with less informative auxiliary information": it would make the paper stronger if the authors specify more concretely when their method should be used over CMC. [B] Zheng, Mingkai, et al."Weakly Supervised Contrastive Learning." ICCV 2021. Overall, the experimental section of the paper was clear and included a good variety of image classification datasets.<|endoftext|>This paper proposes a two stage weakly supervised contrastive learning approach, where it first clusters the data according to the available auxiliary information (or data  driven clusters if no auxiliary information available), and then generates similar representations for the intra cluster samples and dissimilar representations for inter cluster samples via a clustering InfoNCE objective. Experiments on four datasets with and without auxiliary information validate the effectiveness of the proposed method. ++ Well written paper. WEAKNESSES:  Lack of novelty. Weakly supervised methods refer to those using noise labels for learning certain tasks. For example, Zheng et al.Weakly Supervised Contrastive Learning, ICCV 2021. This paper proposes a structure aware constructive loss with theoretical justification for representation learning. The theoretical analysis is very interesting and insightful.
Reject; rating score: 5; rating score: 5; rating score: 5; This paper proposes a new offline multi task RL algorithm named conservative unsupervised data sharing (CUDS). The authors first prove that $\hat{Q}_\text{UDS}\le \hat{Q}_\text{sharing All}$ for all state action pairs. Furthermore, they argue that the CUDS can achieve better performance through weighting transitions. Finally, CUDS outperforms or performs competitively with the previous works. This work has empirically shown that a simple constant reward (zero reward) is enough to achieve good performance in practice. However, CUDS is only supported empirically without any theoretical analysis. In addition, it is trivial since $\hat{r}(s,a)\le r(s,a)$ for all $(s,a)$ pairs. The remarks and empirical results are interesting. However, CUDS is a simple variant of CDS – the only difference is how to set the rewards.<|endoftext|>This paper proposed a method for multi task offline reinforcement learning setting where it can assign shared and annotate the reward, where its major strategy is to label only useful transitions. The authors also study the behavior of their proposed methods with s set of arguments and experimental results. Pros:  The problem addressed in the paper is indeed critical in offline RL. It has a reasonable motivation. Issues:  The biggest concern is the novelty. In this work’s multi task offline RL setting, the authors assume the dynamics to be the same across all tasks and use a single task conditioned policy. The major issue of this paper is it seems to be an incremental study from existing works. I would expect the authors to address this issue and add more insights.<|endoftext|>This work targets useful data sharing between multi tasks in an offline reinforcement learning setting. Motivated by the same transition kernel $P$, for each task, data sharing from other tasks is considered to be beneficial for finding an optimal policy. Different from previous works, the author claims that under some assumptions, data sharing using a constant reward/no reward can be better than using a reward predictor and competitive with using ground truth rewards relabelled. Although there are some experiments such as meta world, etc., it is still confusing whether these data sharing methods can be utilized in other application scenarios. 2.There is an assumption in the targeted MDP is that the reward is binary. Can you give more explanation about it? 4.The writing of the paper needs to be polished a lot. The text "CQL" is also introduced without reference, although it may refer to conservative Q learning. I recommend that this paper be below the acceptance threshold.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; Specifically, the authors use opportunistic forward and backward passes to train a neural network model that estimates the value function, which is subsequently used to initialize and guide the basis SDDP iteration procedure. The procedure also leverages the actual value functions SDDP updates to re train the predictive model for further epochs. Numerical results on inventory and portfolio optimization suggest better error ratios in comparison to SDDP variants and a reinforcement learning model. Could there be a case where the convergence of learning is so slow so that the procedure just adds an overhead? The paper develops an intuitive and natural idea, with promising results on two benchmarks.<|endoftext|>The paper is relatively easy to follow given its high degree of technical content** Weaknesses:  The subject area is of limited significance to ICLR community. The first problem is completely synthetic and as for portfolio optimization, I doubt there are real world instances of portfolio optimization that do not optimize a quadratic objective that includes a risk term. I believe the paper presents a sound and functional method that improves algorithmic performance on the MSSO problem. It is well written and evaluated.<|endoftext|>The paper studies the neural variant of stochastic dual dynamic programming for solving multistage stochastic optimization, which overcomes the limitation of SDDP. The empirical investigation shows that $\nu$ SDDP outperforms SDDP and reinforcement learning in several synthetic and real world process optimization problems. Strength:  The paper is well written, motivated, and organized. The comparison of MSSO and MDP makes the paper easier to follow for different communities. The motivation of the structure of the value functions is well explained.<|endoftext|>This paper considers a class of problems called multistage stochastic optimization (MSSO) (also known as stochastic programming) problems, which are essentially MDPs but with linear rewards and constraints. The main contribution of the paper is a way to learn a neural mapping from MSSO problem instances to value functions, which can be used to warm start the SDDP solver. Strengths: The application to MSSO is interesting.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper proposes DSE, an approach to optimizing parameters of neurosymbolic programs (programs with mixed neural and symbolic components) while satisfying a safety constraint. This safety loss is constructed by symbolically executing the program, executing discrete transitions probabilistically according to a uniform distribution over concrete states. The approach is only validated on small scale programs, using a manual heuristic of splitting the input space into 100 subregions.<|endoftext|>This paper is well motivated and targets an important problem. Results show that although the proposed method is not guaranteed to be sound, it in practice produces programs that are safe on the example benchmarks. The proposed technique of combining symbolic execution with sampling  and REINFORCE styled learning is interesting.<|endoftext|>The key contribution is an algorithm to compute gradients for the worst case safety loss by symbolically sampling control paths in the programs and using a modification of the Reinforce estimate to approximate the gradients. The approach is compared with DiffAI on several synthetic tasks. **Strengths**The paper is well written and proposes a mathematically rigorous algorithm to compute gradients in the presence of discontinuous conditionals. The formulation looks sound to me. And these safe RL approaches don’t need to take gradients through the environment code. — Another potential issue with optimizing through symbolic code is the presence of numerous minima.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper presents a  MLP like architecture, CycleMLP. The proposed CycleMLP can be used for dense prediction, which is more suitable for object detection or image segmentation tasks. 2.CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. 2.CycleMLP is slightly insufficient in discussion of the design and ablation studies. Compared to modern MLP architectures, the CycleMLP can cope with various image sizes and achieves linear computational complexity. The benefits of each component and the performance difference are shown. The proposed method is applicable to many vision tasks in the future.<|endoftext|>In addition the method has linear computational complexity as compared to previous MLP work which have quadratic complexity. The overall architecture uses the previous work on patch embeddings and idea of stages with stacked blocks. Comprehensive experimental results are show improvement over previous MLP models and competitive results to other methods such as CNNs and transformers on multiple tasks including classification and segmentation. The paper is strong in the sense that it proposes a simple yet effective idea to extend previous MLP work for image tasks. Also as CycleMLP should be generic in terms of tasks the additional results on detection and segmentation make the idea more convincing. Overall the basic contribution of improving MLP models to have linear complexity and applicability to varying image sizes is solid.<|endoftext|>The paper proposes Cycle MLP architecture, the idea is to bring spatial context into Channel FC and increase its receptive field. The main objective of the paper is to address the challenges faced by the current MLP Architectures. Cycle MLP allows flexible image resolution and avoids quadratic computational complexity in dense architectures. Strengths:   Simple idea with promising results. Some discussion and experiments around this point would be useful.<|endoftext|>This paper shows competitive results of MLP like architecture on image classification, object detection and segmentation. Pros:(1)  This paper designs a new MLP like architecture that can cope with various image sizes and achieves linear computational complexity. Currently I rate it as a borderline paper and the authors need to consider the above questions. What are the specific differences between CycleMLP and these architectures?
Reject; rating score: 5; rating score: 6; rating score: 6; The authors  solution approach begins with fully observed trajectories that are used to train a sequential VAE as an inference model. Then, using the pre trained VAE, an RL algorithm jointly learns the control and feature acquisition policies. Weaknesses:  The authors claim that AFA POMDPs generalize POMDPs is false—an AFA POMDP is a special case of a POMDP. Despite having much higher MSE, the baselines are very competitive on the tasks. I still believe the paper is not ready for publication.<|endoftext|>In this paper the authors propose an approach for simultaneously learning how to explore more efficiently in POMDPs via targeted feature acquisition, and learning a reward maximizing control policy, balancing the cost of feature acquisition with the expected reward. I am also concerned that essentially learning the VAE component offline makes the approach brittle in practice. However, I do have some questions,   My first concern regarding the proposed approach is that it seems wildly expensive to train even for modestly sized problems, and even in the fraught landscape of POMDP learning strategies. On that note, I am somewhat concerned by the pretraining of the VAEs and the fact that it is then essentially fixed during learning, implying that while there s a sequential component to it, it isn t really an RL approach for hidden state imputation and therefore might be very brittle in practice, especially when a lot of distributions are unknown prior to training.<|endoftext|>This paper proposes a reinforcement learning + representation learning approach for simultaneously learning a control policy and feature acquisition policy in environments where feature observation is costly. Weaknesses:  My main concern is that there is already a fairly similar work by Igl et al (2018) [1] that proposed a sequential VAE and had some promising results on a related set of tasks. But this is all still in the hypothetical. There are a few critical experimental details that are left ambiguous. A3 5: I appreciate the authors  diligence on the Ethics Statement. Post response: A3 1: I m still not convinced about the novelty of the VAE approach.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes a gradient based resampling scheme for de biasing. The authors construct two types of scores that leverage gradients of the biased model: the magnitude and the direction of the gradients. In addition, to mitigate the side effects from the noisy labels, the authors also propose a de noising module, which can be easily applied to any de biasing algorithms. I have the following concerns and questions. This work is based on the hypothesis that the “gradients have remarkable differences between samples generating prejudice and the others”. However, I’m not convinced if the noisy label is the only critical factor that can negatively affect the performance when using the proposed gradient based scheme. I wonder what other factors (e.g., adversarial attacks) can degrade the performance and how to tackle them. Figure 4 is drawn using the Colored MNIST. I wonder how the plots for more natural datasets would look like, and if any theoretical analysis can be included. The model assumes that the biased samples make up the majority of the training set. The experiments are also based on this assumption, and the proportions of the unbiased (minority) samples in the experiments are 5% or less in all the experiments. I wonder how realistic this scenario is and how the proposed method performs on a dataset that contains a higher portion of unbiased (minority) training data (e.g., 10%, 20%, 30%,…). > … an accuracy similar to that of the major case.<|endoftext|>The paper proposes a de biasing method for neural network without requiring explicit annotations of biases. The authors propose a resampling based approach that automatically detects underrepresented samples based on the direction and magnitude of their loss gradients, then balances the dataset by rejection sampling. The paper presents a novel and simple debiasing approach for neural networks, as well as a denoising module for training with label noise. The proposed debiasing method shows promising results especially on synthetic benchmarks, comparing against various state of the art algorithms; however, I believe the concerns listed in the section above needs to be addressed to fully justify the contributions of the work. ## Cons / Questions  The key assumption which motivates the proposed method, namely that minority samples have different gradient distributions than majority ones, deserves a more rigorous validation. Experimental results on real world datasets are relatively weaker compared to synthetic ones. I believe the clarity can be improved by additional columns for the average of major and minor accuracies and making bold only the best numbers in each column. "Probabilistic end to end noise correction for learning with noisy labels."<|endoftext|>The main hypothesis of the paper is that there are differences in gradients for "biased" samples (or samples that are "rare") compared to majority patterns in the training data. Using this, the paper devises a rejection sampling method that tries to balance samples in a minibatch. However, a sample with a noisy label can appear to be a "biased" sample (with a correct label) which can affect the proposed method (as well as many other bias mitigation methods). To this end, the paper also proposes a denoising module that successfully eliminates the effects of noisy labels on the debiasing algorithm proposed. Great results: The results for varying levels of "bias" as well as the success of the proposed "denoising" setup is remarkable for the datasets tested. S2.Interesting Hypothesis and approach: The paper proposes an interesting hypothesis about the differences between gradient magnitude and direction (as measured by its proximity to an "average" gradient direction for all samples) look different for biased as compared to "regular" data sample. One common theme across these datasets is that they can be "learned" (at least the biased version) with a much smaller amount of data than is present in the training set. Hence, a rejection sampling based method can work even when the minority set (m) diminishes. In other words, the algorithm assumes knowledge about which of the factors were biased so that a suitable "bias only" model can be trained by leveraging only the "bias".<|endoftext|>The paper propose to use the per sample gradients from the biased classifier to tackle the dataset bias. The motivation is purely from the empirical observation that samples from the majority group tend to have similar gradient magnitude and direction. From the observation, they derive a scoring function for re sampling. The proposed method work pretty well empirically compared to various baselines. However, the proposed scoring function is not well motivated. Despite the success of better performances, it is hard to be convinced that the per sample gradients are able to differentiate between the samples drawn from majority or minority group. The authors show only 4 examples in Color MNIST in figure 4, which are not representative enough. **Strength**  Extensive experimental results across different setting and different baselines. Strong performance compared to the baselines**Weakness**  Lack of motivation and analysis on the connections between per sample gradients and the majority/minority splits (in more complex datasets)**Others**  For table 3, I would be interested to see the accuracy of the minority as well since it help validate if the proposed method works under real world correlation. The scoring function is the key in this work, but is not well motivated enough. The paper can gains a lot of values by providing more analysis on the connections between per sample gradients and the majority/minority sets.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The authors provide theories to show that the bias of IGLU is bounded. The experimental results are promising as well. The overall algorithms are still vague for me. I appreciate the authors presenting Algorithm 1 and 2 as well. For example, in the algorithms it said things like "Compute G", "using Lemma 1 (1)", but it s unclear how they are used. Overall, I like the technical contributions made in this paper. My concerns are:1.<|endoftext|>This paper studies tries to tackle the scalability challenge of training GNNs on large graphs. Convergence analysis on IGLU is provided and empirical results show that IGLU has better performance than baselines such as GraphSAINT, Cluster GCN, VR GCN. Overall, this paper has some insights on tackling the scalability challenge training GNNs on large graphs. And IGLU is an effective method for scaling up GNN training and shows better empirical performance compared to well known techniques. Moreover, I think authors should provide a more detailed comparison between IGLU and GNNAutoScale.<|endoftext|>The work proposes IGLU, an algorithm to scale up GNNs using stale computations instead of traditional neighborhood sampling. # Strengths* I like the idea of holding some variables constant in gradients and using stale computations to speed up computations. * Strong empirical results show that IGLU scales well and achieves SOTA. * The authors provide a convergence analysis of IGLU. # Weaknesses* In practice, many tricks are used in conjunction with GNNs. Can the authors elaborate?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 10; The proposed IV RL algorithms are evaluated in both simple continuous control as well as some MuJoCo tasks, with favorable performance improvements over related prior approaches. Fantastic framing and decomposition of the sources of uncertainty present in learning an RL policy.<|endoftext|>In two of the tested discrete environments (LunarLander and CartPole noise) there is a clear improvement, while on MountainCar there is no improvement but also the method works with same performance as the baseline.<|endoftext|>**The main components of the proposed method (i.e., down weighting by variance and using probabilistic ensembles) are well known to the community, as cited already by this paper (e.g., Lee et al, 2020, Wu et al, 2021, and Lakshminarayanan et al, 2017).<|endoftext|>It is an important research topic and of very high interest to the community. 2.The approach is very well embedded in the current literature.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper presents a hierarchical, object centric view conditional generative model for scenes. The model is demonstrated to work on CLEVR and a slightly more complex version of CLEVR with more object structures as well as the Jaco arm dataset. Can the authors elaborate on that point? * What happens when you instantiate the model with different number of slots? All in all this is a nice paper, with a bit more depth could be a nice contribution.<|endoftext|>Can the authors check the correctness of the ELBO used in the paper? # Strengths  The paper applies normalizing flows to stable inference and training, which might be empirically useful. It seems that the authors mainly rely on the Structured Prior (Transformer) to achieve this, but there is no reasonable analysis about why the Structured Prior is relevant to physical plausibility.<|endoftext|>The paper would be significantly strengthened by evaluation on a more challenging dataset. I assume this is due to the fact (sec.3.3) that the  prior  is trained post hoc (and therefore unable to guide the training of the encoder). Qualitative results are not very impressive, even on simple datasets of rendered shapes. This should be covered in the ablation study.<|endoftext|>The authors do not provide a theoretical justification for these design decision. In particular, while there are some visualizations of model ablations, the results are pretty inconclusive. Strengths:* ***Competitive compared to MulMON on the tested datasets***The model builds upon MulMON, and the empirical results shown in the paper support that the proposed model is competitive and sometimes outpeforms MulMON.
Reject; rating score: 3; rating score: 3; rating score: 5; Unfortunately, I currently recommend rejection of this paper since it does not have sufficient empirical results to demonstrate the robustness of the proposed improvement and the robustness of their method on different datasets and as a one shot method. I m not sure based on table 1 compared to CREAM if that is accurate?<|endoftext|>It seems that they achieved a better accuracy FLOPs trade off than this paper (~79% @ 300 MFLOPs and ~80% @ 500 MFLOPs).<|endoftext|>In this way, the first selected/sampled operator(s) would have a huge impact on the following sampling process, which doesn t make sense. Also, I checked the revision history and found the authors claimed a 1.71% improvement on their first version.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; The paper examine optimization stabilities encountered during training of GPT 2 models. To alleviate these issues, the authors propose a curriculum learning approach based on the length of the sequence. They show that this approach stabilizes the learning dynamics and drastically improves the training efficiency. The proposed paper indeed demonstrates significant improvements in training stability and speed. If so, is the proposed curriculum learning approach effective there?<|endoftext|>The authors present a curriculum learning approach that gradually increases the input sequence length for training large scale transformer models. Weaknesses:  The paper applies and existing curriculum learning approach on GPT 2. Hence, the paper lacks novelty. It is not clear that truncating text data leads to easier examples. Therefore, proposed method might also be regarded as anti curriculum approach. Minor language corrections:"which helps improves"  > "which helps to improve" or "which improves";In my opinion, the weaknesses outweigh the strengths of this paper.<|endoftext|>This paper focuses on the pretraining of large language models. The paper claims that the new lesson learned is that stability is improved with curriculum learning. If a convincing result for this was shown, I have missed it. Did the GPT 2 authors originally struggle a lot to do "stable" training? What I mean by this is that the current paper seems more like "BERT trains fast and with more stability if you gradually increase the sequence length of examples." It would be a lot more interesting, for example, if it were shown that the author s proposed method worked for other operationalizations of curriculum learning in addition to sequence length (e.g., age of acquisition, word frequency, etc). Related to the above point, there is very little motivation for why sequence length is a correct operationalization.<|endoftext|>The paper proposed a length based curriculum to pretrain GPT models, that linearly increases the input lengths until a maximum value over the first 20 100K steps. Despite its simplicity, the approach achieves more stable training, faster convergence to a target perplexity (⪆50% time reduction), and better generalization as measured by validation perplexity or downstream accuracy. Strengths:  simple but effective curriculum method  large savings in time and improvements in qualityWeakness:  Because of the surprising method simplicity, more analysis would be interesting to add that could shad light on the nuances of interplay between gradients and the CL, and why it helps. Given that Platanios et al reported good results both with length  and word rarity based curricula, it would be nice to run a few experiments with the word rarity difficulty definition. What is "ideal" here?
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper proposes a graph to SMILES framework, which incorporates several recently developed engineering techniques from the community, for synthesis planning and reaction outcome prediction tasks. Experiments on benchmark retrosynthesis and reaction prediction tasks show that the proposed approach outperformed the vanilla SMILES to SMILES transformer baseline, but obtained inferior results than some other advanced methods. The paper is interesting, but both the technical novelty and the experimental studies are weak to me. The proposed framework integrates several recently developed engineering techniques and empirically shows its superior performance over vanilla SMILES to SMILES transformer baseline. Nevertheless, I have the following concerns regarding the paper. 1.The proposed framework is similar to the NERF approach (Bi et al.ICML 2021) as cited by the authors. In this sense, the novelty of this paper is limited to me. 2.Experimentally, the proposed method is not directly compared with NERF. I would expect the paper to include NERF into the main results in Table 1. Also, for the USPTO_STEREO_mixed task, I wonder what the reason was for only comparing with the vanilla Transformer, and why not comparing with the Augmented Transformer or the state of the art method Chemformer? Nevertheless, there is no evidence to support that claim. Input data augmentation may play a significant role on regularizing the deep neural networks.<|endoftext|>The paper proposes a GNN based extension of transformers, which have been shown to be effective for reaction prediction etc. The experiments show that there are sometimes increases of performance in reaction and retrosynthesis prediction   and the approach could be applied to similar problems. ( )  As far as I can see, the technical novelty is limited. The results in Table 1 are only convincing for USPTO_STEREO_mixed. The experimental comparison for retrosynthesis compares to methods which use different forms of pretraining or augmentation, which makes sense. Table 4: Since the paper proposes the attention based GNN, the ablation should be provided for that model. Since transformer is the baseline, I would not consider this as an ablation setting. Overall, I think the authors  proposed model for reaction prediction makes sense. However, as mentioned above, the paper s writing could be improved, the technical contribution is limited, and the experiments also show only limited improvements.<|endoftext|>The authors proposed a new method for retrosynthesis, which does not require the mapping numbers and extracting templates from the literature. The decoder is a Transformer model with relative positional encoding. 2.I think the model contains more parameters than conventional retrosynthesis models like conventional Transformer, GLN. Could you please show compare the number of parameters of different methods? 3.The authors should provide some real cases to show how the method outperforms previous baselines, and why the method can obtain good results without templates. Dual view Molecule Pre training, https://arxiv.org/abs/2106.10234, the authors also work on retrosynthesis using Transformer and GNN models. The results in this paper are good, although the method itself is not quite novel.<|endoftext|>Experiments show that Graph2SMILES is competitive with Transformer baselines but does not outperform state of the art methods on tasks of the one step retrosynthesis and the reaction outcome prediction. The main strengths of this paper are as follows. My major concerns are as follows. 1.This paper states that Graph2SMILES achieves state of the art top 1 accuracy on common benchmarks among methods that do not use reaction templates, atom mapping, pretraining, or data augmentation strategies. However, they do not conduct experiments to demonstrate their claim. 3.Graph2SMILES involves calculating pairwise shortest path lengths between atoms, which can be computationally prohibitive. The authors may want to compare Graph2SMILES against baselines in terms of the computational complexity. However, the empirical results do not show a superior performance of Graph2SMILES to existing methods, and the technical contribution is incremental.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; 2.The paper gives a general analysis of the pros and cons of ML approaches. In Section 1 Introduction, the paper illustrates the necessity of customizing Traffic Engineering schemes for various applications. 2.The paper presents unclear descriptions of technical details. Also, it is unclear what "this" refers to. It is crucial in Traffic Engineering, as stated in Section 1 Introduction.<|endoftext|>The policies used in the evaluation are relatively simple, more realistic workloads would be more desirable. The first concern is related to the contribution of the paper. There is a very limited contribution towards the machine learning techniques.<|endoftext|>Would like to get some more details. Overall, the paper reads well, and it contains detailed information on the architecture and training (perhaps a bit too much for the ICLR audience). There are several issues that need clarification:(1) Do all switches need to have MISTILL installed? Can this work in a hybrid architecture where some switches do not have MISTILL? Can the framework be extended to do that? This has been widely used in the social networks literature.<|endoftext|>Other issues: 1. 3.Model abstraction and retraining of the network: does the proposed model has to be retrained for a different network size and topology? I am not sure "what" the novelty of the paper is. It is most probably due to the fact that the paper significantly lacks in details.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 3; This paper provides interesting theoretical tools that help us study the relationship between empirical risk minimization/ empirical cross entropy minimzation and Mixup minimization. Constructing a dataset on which Mixup training fails to obtain the empirical risk minimization. The main difference with related works is that the paper attempts to understand *why* Mixup works by studying *when* Mixup works with concrete examples. # Strengths:* The approach the paper takes in their study is a meaningful one, and the theory and experiments are well motivated. * The paper takes a complementary approach to existing works for understanding the generalization and robustness of the Mixup optimal classifer. Or am I missing something? * Figure 1 and 8: ERM curves do not depend on $\alpha$? * Figure 1 s labeling is not clear. In ICML.I recommend 8: accept, good paper. Could you clarify it in the paper?<|endoftext|>The paper contributes an interesting perspective for looking into Mixup training, which has been widely adopted by the community. Using these observations to support that “mixup training minimizes the empirical risk” (the first sentence of the last paragraph in Page6) is not very convincing to me. In that paper, mid point samples (a special setting of Mixup) are generated for training. The paper contributes to the understanding of Mixup training from a training data dependency perspective, which is novel and I think it would benefit the research community. On the other hand, the paper is not clear to me in some places as detailed in my reviews, and I would like the authors to comment on them.<|endoftext|>I like the topic of this paper, but I have several concerns as below. 1.Focus on extremely high \alpha values    1. In the illustration of mixup examples, this paper considers \alpha 32, 128, 1024, which is not even considered in the original mixup paper (standard one is \alpha 1, and maybe variant the authors considered is \alpha 2). I think the authors are setting extremely large \alpha values to exaggerate the effect they want to show. This is an interesting paper trying to answer “why” and “when” mixup works.
Reject; rating score: 3; rating score: 6; rating score: 8; This paper studies the optimization landscape of first order methods in stochastic games. In particular, it studies the connections between first order stationary points and Nash equilibrium and its convergent property and learning stability. For Markov potential games, global convergence results are provided. Song et al*** Since this paper deals with SGs, can the author comment on the relationship between the Nash equilibrium in Definition 1 and Markov  Perfect Equilibrium? Song et al**  This paper is a rather long paper and certainly over exceeding the content that a conference paper can hold. However, I believe the result is generally correct, for example, the sample complexity on Markov Potential Games has been discovered, probably in parallel, by many other papers.<|endoftext|>2018.The paper has some interesting results for very generic Stochastic Game settings and Markov Potential Games (very similar to [Leonardos et al.2021]).But I think these settings are too generic to study and extract meaningful results. Given the above comments on the strengths and weaknesses, it seems that there is more investigation and characterization that might be required. After the response:The authors have clarified most of the concerns and the characterization seems adequate for this work to be published. In addition, they also provide a sample based RL algorithm, where the players do not use gradient information from others and estimate based on the observed trajectories. Although, here the gradient of each player does seem to depend on the information from other players. This may be quite rare even when considering normal form games. Finally it would be useful if the authors could differentiate between the results of [Leonardos et al 2021] for Markov Potential Games as both algorithms achieve the same sample complexity and other results are mostly similar. arXiv preprint arXiv:2101.04233 (2021). "Cycles in adversarial regularized learning."<|endoftext|>The paper studies the global convergence of policy gradient for multiagent Markov potential games. Observe that the authors assume that the agents have full knowledge of their utilities and their derivatives (their algorithm is deterministic). They also provide convergence for the stochastic variant of policy gradient (given samples). Then using techniques from non convex optimization, it is known that Gradient Descent converges to stationary points and this is what they show about policy gradient on the potential function. In general, i feel the paper has made a valid contribution on multi agent RL in potential games. The stochastic variant is more challenging due to the limitation of the non Lipschitz gradient.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This work proposes (1) a new subclass of cooperative MARL named homogeneous Markov game; (2) the first decentralized actor critic algorithm with both policy sharing and provable convergence; and (3) a practical technique that transforms centralized training algorithms to their decentralized and communication efficient counterparts by using bandit to select communication links per iteration. Pros: (1) A new subclass of cooperative MARL is proposed. These additional figures might be put in Appendix if the page limit does not allow. **The above three concerns are the major reasons why I reject this paper. A counterexample might be [1] which proposes a fully decentralized algorithm for cooperative MARL, where each agent only knows and independently learns its own policy.<|endoftext|>This paper proposes a consensus based actor critic algorithm for multi agent RL with limited communication during the training phase. The paper motivates the proposed algorithm by an analysis of homogeneous Markov games (MGs), a subclass of MGs where parameter sharing preserves policy optimality. I appreciate the theoretical analysis of policy sharing, a practical technique that is often used in MARL but so far not fully understood why or when it works well. The theoretical analysis for the partially observable setting is lacking. With this, the paper makes a decent overall contribution that is also clear in context of other literature in Dec (PO)MDP and Markov Game models.<|endoftext|>This paper considers a subclass of cooperative Markov games where agents exhibit a certain form of homogeneity such that policy sharing provably incurs no sub optimality. Furthermore, heuristic algorithms have been developed and incorporated into the consensus based decentralized actor critic method to reduce the communication cost during training. Overall, I felt that the paper provides some new insights and perspectives on whether policy sharing will incur sub optimality in MARL. However, for the reason below, I felt that the paper is not ready to publish in its current form. "Networked multi agent reinforcement learning with emergent communication."<|endoftext|>This paper studies homogeneous/permutation invariant MARL setting and prove a theorem that says there is no loss in restricting to identical policies, thus justifying policy parameter sharing. Then, a consensus based MARL actor critic method is proposed. Further, a communication efficient protocol is proposed to improve communication efficiency. This paper provides a nice analysis for the Permutation invariant MARL. Definition 1 (iii) assumes bijective observation, so essentially the state is fully observable? Overall I think this paper considers a very interesting setting.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; As the reward is linear in the number of agents that scan a target (as long as they are more than 2), the task does not specify whether it is better to scan few targets with many agents or many targets with few agents. CASEC is evaluated on a novel benchmark suit (MACO) for coordination dilemmas and StarCraft II micromanagement tasks. The discussion has convinced this reviewer that the paper should be accepted. The score has therefore be raised to 8.<|endoftext|>The paper has an expansive set of experiments with fair number of agents. Overall it s a very interesting paper and would recommend acceptance just for the new variance based metric for coordination edge selection. The paper evaluates the proposed approach on many classic coordination problems as well as subset of SMAC benchmark tasks.<|endoftext|>### Pros  The problem is relevant;  MACO benchmark is a valuable contribution to the community;  Empirical results are good. Proposition 1 is an interesting addition to the paper (I did not check the proof). Can you provide the range of the fraction in Equation 5? Can you provide intuition on why DCG>CASEC on Pursuit? However, I give it a 5 (marginally below the threshold) due to the fact that the authors decided not to include the related work that the reviewers suggested, e.g.Deep Implicit Coordination Graphs for Multi agent RL by Li et al.I am willing to increase the score if:* authors provide a legitimate reason for not including relevant work into this submission, and* include the relevant work and compare to it / explain why such comparison is not meaningful.<|endoftext|>In this paper, the authors study how to learn dynamic sparse coordination graphs, which is a long standing problem in cooperative MARL. Overall speaking, I found the paper interesting to read and easy to follow. In comparison with existing literature on in cooperative multi agent learning, I m convinced that, the approach proposed this paper indeed adds valuable extensions in multiple dimensions and it would be quite beneficial to the research community in this area. existing SOTA methods on MACO benchmark and StarCraft II micromanagement benchmark, also provides some reasonable justification and illustration of the superior performance of the proposed new method. The paper is well written and seems to be of significant contribution to this research area.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The block structure has two seemingly contradictory properties: on the one hand, its constituent layers have highly similar dominant first principal components (PCs), but on the other hand, their representations, and their common first PC, are highly dissimilar across different random seeds. The authors investigated the origin of this block structure with the data and training methods. They found that the block structure arises from dominant datapoints   a small group of examples that share similar image statistics (e.g.background color). The strengths of the paper:  investigating the existing problem which is the block structure phenomenon in large capacity networks trained on relatively small datasets,  proposing a novel principal component regularization method. The weaknesses of the paper:  authors focus on the block structure phenomenon in large capacity networks trained on relatively small datasets. They tried to eliminate the block structure by a novel principal component regularization method, or alternative existing training mechanisms such as transfer learning and Shake Shake regularization. In my opinion, the novelty of this paper is too small for the conference ICLR.<|endoftext|>Building on the work by Nyugen et al.2020  the authors examine the block structure phenomenon where subsequent layers of a neural network have very similar internal representations. They show that a small number of datapoints load very highly to the dominant principle components of the block structure. They also examine the dynamics of the block structure and the fact that it changes through training. Finally the authors show that some regularizations such as Shake shake improves the phenomenon. While several facts mentioned in the paper are interesting, one of the key dissatisfying experiences of reading this paper is that the authors do not place this phenomenon in larger context or really examine its implication. The authors don t address in their paper why this block structure "is bad". The paper has no experiments showing the ability to prune or reduce networks when they eliminate block structure. There actually seems to be an order dependence to the particular representation that arises, thats why different random seeds result in different representations. This could be because the network learns what to do on a the initial set of inputs first and then on other sets of inputs, so the authors could try batching groups of similar inputs which are shown first and which are shown later to the network. But having said that it is not clear why the order dependence is bad again. 2.This would explain the transfer learning helping as well, because in transfer learning a different dataset is trained initially. 2.Can it be removed by explicitly penalizing for betewen layers similarity? Would this affect generalization? The paper does not deeply examine the phenomenon at hand, it makes assumptions that are not explained, and does not explore implications well.<|endoftext|>However, I wonder whether the similar can hold for KCA under some other kernels. To measure the similarity, the authors used KCA (with linear kernel) as the similarity measure, which was also used in (Nguyen et al., 2020). This paper reported that* The dominant datapoints, which lie in the direction of PC, share some common image patterns such as background colors. * The dominant datapoints are different across different model initialization. * The block structure can be suppressed by modifying training, e.g., by regularizing PCs, by using Shake Shake regularization, and by using transfer learning. Revealing general phenomena underling deep models would be valuable to the community. If this is the case, the paper needs to be reorganized with its main focus on (ii) but not (i). This phenomenon itself can be a topic of interest for further researches. ## WeaknessIn the paper, the authors reported that the block structure is induced by the dominant datapoints. In particular, I suspect it can be an artifact induced by the use of KCA with linear kernel for the similarity measure. Indeed, as reported in (Nguyen et al., 2020, Fig3) as well as in Fig4 of the current paper, removing outliers (i.e., PCs or dominant datapoints) eliminate the similarity between layers.<|endoftext|>The claims made by the authors in the paper seem correct and there is no obvious error I can see in the paper. The paper provides a number of experimental results that provide interesting insights into the appearance of block structures in large deep networks. There are some new insights coming out of the experiments carried out by the authors on this paper and therefore the paper has some novelty to it. The paper is very clearly written and the figures are informative and easy to parse. Even though the paper provides interesting insights it is not clear to me what are the benefits of removing the block structure.<|endoftext|>This paper studies the block structure phenomenon in training large capacity neural networks. The paper provides an explanation that the phenomenon is caused by dominant data points. The paper also investigates methods to mitigate this issue. Strengths: (1) The paper uncovers the origin of the block structure phenomenon. (2) The experiments are insightful and thorough. (3) The paper s analysis of dominant data points can potentially be useful for training big networks on relatively small data. Weaknesses: (1) The paper lacks a rigorous theoretical analysis. (2) The idea of influential points is not very novel. This paper provides a reasonable explanation to the block structure phenomenon. It is a useful empirical contribution.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This introduces a meta learning algorithm for learninginference algorithms applicable to any probabilisticprogram. This work is very interesting and novel. It s a unique attemptto learn a general inference algorithm. I m curious about the expressivity of the language. It seems one would needsomething like a label and jump commands to accomplishthat. I have some concerns about the experiments. The results in Figure 4seem to be for during training, but feasibility presumablyrequires similar results on unseen programs at test time? I also worry about correctness. As the neural networks areused as is and not as a proposal, how much can we trustthe posteriors that come out of this method? Is there anythingthat can be said about the learnt posterior? I think that s what the paper meant to refer to. Related work should cover how this approach differs from Stites et al.(https://arxiv.org/abs/2103.00668). While I think the approach has the potential to work, right now it s very hard to get a sense of what was learned by the meta learning algorithm, how well any of this generalises when model structure or even observed data changes significantly, or even if the language is too restricted to make this is a significant enough contribution.<|endoftext|>The paper proposes a new restrictive class of probabilistic programs with fixed number of random variables and without loops. While the authors attempt to cast their work as something different than IC this doesn’t quite come out. These models are so simple, how hard would it be to train ic on these models and then do inference? I would like to have seen ic results in this paper to believe that this work is different. Please show an example of how a program with unbounded random variables can be compiled into this language. Please show some comparisons to vi. It is not a meaningful claim to make (footnote on page 9) that the inference algorithm provides a good coverage of the posterior by covering all the modes. The posteriors shown for multimodal models should at least look multimodal. The claim that the paper provides generalization of compiled inference across models is not supported by the description or the simple examples. The focus on a very restricted class of ppls makes this work very limited. I don’t believe I learned anything from this paper.<|endoftext|>The paper presents an algorithm for composing inference algorithms out of simpler neural net building blocks, one per unique statement type in the probabilistic programming language presented. The language is simple without recursion or loops, reducing issues due to feedback problems from the approximation. There is an empirical study of several classes of small Gaussian probabilistic programs. Are there diagnostics for checking the output when the model is applied to a program which is too dissimilar? The experiment in section 5.3 shows that the system is around 2x faster than importance sampling from the prior, but this doesn t take into account the time necessary to train the neural nets nor the time taken for the importance sampling runs used to generate the training data. How are losses propagated through the program? Could the authors comment on the stability of training? Is it detectable without having HMC or other high quality samples? Overall the paper is well written and explained, and the experimental study is detailed for the areas it covers.<|endoftext|>This paper proposes an algorithm for computing an approximation of the posterior and marginal likelihood by analysing the sequence of programs using neural networks, as well as a meta algorithm for learning the network parameters over a training set of probabilistic programs. It did not seem clear how well the white box inference algorithm performs when the number of commands in the program is very large. I am not at all familiar with probabilistic programming; this does look like a serious piece of work, though I do not know how novel it is or whether the claims in the paper are correct.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; rating score: 5; Their key assumption is that standard MTL can result in catastrophic forgetting of pretrained knowledge and lead to local task optimum in the finetuning stages. Strengths:  The hypothesis is very interesting: is the negative transfer actually caused by the misalignments of task gradients? The experiments are pretty extensive and show many interesting insights, which also verifies the effectivenesses of the proposed method. b).By implicitly adding the task gradient alignment, the resulted finetuned MTL model show less forgetting of the pretrained knowledge, and higher cosine similarities across tasks. c) In Table 5 & 6, it would be good to include the single task learning (STL) performance on zero shot cross lingual experiments. Therefore I would recommend accept the paper.<|endoftext|>The paper presents a new method for multi task learning (MTL), namely sequential reptile (SR). Would that improve their performance? The paper argues that this simple modification can promote gradient alignment naturally. Pros:1.The experimental results are strong and very promising. On all settings considered in this paper, the proposed method significantly outperforms all methods. 2.The paper provides plenty of experimental evidence to demonstrate the effectiveness of the method. The results have shown that the resulting model is closer to the pretrained mBERT.<|endoftext|>This paper proposed a new training method to improve gradients alignment between languages (tasks) in multilingual finetuning of pretrained language models. The method is based on an existing algorithm, Reptile, which was originally developed for meta learning. The experiments results demonstrate the empirical significance of the work. The method is simple and the authors provide insights into the problem and the methods, i.e.the adaptation of Reptile. For example, the paper made several claims such as "As a result of such negative transfer, we see from Fig.5a and Fig.5b that the baselines suffer from catastrophic forgetting.".<|endoftext|>This paper proposes Sequential Reptile, a meta learning method to perform inter task gradient alignment to alleviate negative transfer and catastrophic forgetting. Empirically, experiments on both multi task learning and zero shot cross lingual transfer settings for QA and NER tasks demonstrate the effectiveness of the proposed method. I carefully review this paper.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This work presents an augmentation strategy leveraging the latent space of VAE and adversarial samples, named Identity Disentangled Adversarial Augmentation (IDAA). IDAA perturbs the latent space of a pretrained variational auto encoder to produce adversarial samples that preserve the identity attributes. Experiments show that IDAA improves the performance of existing SimCLR and SimSiam baselines. The proposed IDAA seems to be able to improve the current augmentation strategy by removing the identity attributes when generating augmentations. The authors better tune down their claim or provide more support that IDAA can achieve on par performance even with random augmentation or without any augmentation. Although I like the analysis in section 3, I still have some concerns about the claim that IDAA can improve different SSL methods. The contrastive methods, such as SimCLR, mainly use InfoNCE loss, and the analysis in section 1 well supports the relation of IDAA to these methods. However, the relation to non contrastive methods like SimSiam, SWAV, etc. is not clear since they do not use InfoNCE loss. For example, SimCLR and SimSiam are known to achieve over 90% accuracy in linear classification when ResNet18 backbone with 800 epochs, and the results in Table 1 seem to have significant differences. In Table 1 CLAE seems to harm the performance of SimCLR and SimSiam, which is not consistent with the conclusion in their paper either. Based on these results, it is hard to judge whether IDAA improves existing SSL methods  performance and outperforms other augmentation strategies. According to Eqn (5) in the paper, it also seems the identity is better preserved from wrong augmentation when $\beta$ is bigger (please correct me if I am wrong). Could the authors provide additional justifications with some analytical examples or provide some possible explanation? The technique part is generally correct but is limited to InfoNCE loss, and the experiment results are not completely consistent with existing papers. Based on these concerns, I had difficulty in judging the value of the proposed method.<|endoftext|>To this end, the authors leverage VAEs—generating an augmentation for an image x by adversarially perturbing it s encoding (z   E(x)) to generate a perturbed reconstruction (G(z )), which is then added back to the original residual (R(x)   x   G(x)). The motivation for this approach is to maintain the identity preserving information via R(x), while creating challenging augmentations. Finding better augmentation schemes for self supervised learning is undoubtedly and important and challenging problem. While the proposed method does seem to offer empirical gains with respect to baselines, I am not completely convinced about the motivation and some of the theoretical claims. After all, one could construct a z  that is arbitrarily different from z (using a large delta). In this case, x  could be very different from x and I(x , y) would be very small (possibly violating the lower bound). * Baselines:      Why do the authors only consider SimCLR in Table 2? In Table 3, why are CLAE and random augmentations missing? I am curious whether simply attacking the z of a standard VAE (with beta 1) to produce an x    G(z ) would lead to vastly different performance. * All the plots in the paper need to be made larger and more legible. While the empirical results of this paper are interesting, I still have concerns about the theoretical claims and the comparison to baselines. I would be happy to increase my score if the authors addressed these. #### Post rebuttal Update ####I thank the authors for their detailed response. Based on the additional experimental results that the authors presented to all the reviewers, I have decided to increase my score to a 6. I still have two comments:  The authors  response regarding theorem 1 does not completely address my concern. Thus, it is still not clear how the bound in Theorem 1 can hold for some arbitrary $\delta$. For instance, imagine we have two images $x_1$ and $x_2$ and their corresponding latent $z_1$ and $z_2$. In the caption for Table 8, there is a typo: `copmaring`. #### Post discussion Update ####Over the course of the discussion between the authors and the reviewers, it has become apparent that several of the experimental settings chosen in the paper are non standard, which make it hard to verify the validity of the results. I do share the concerns of my fellow reviewers in this regard, and thus will downgrade my score to a 5.<|endoftext|>The paper studies contrastive self supervised learning. In particular, it proposes an adversarial augmentation method for different view generation in contrastive learning. It claims that using such adversarial mechanism, the generated views are identity preserving while being hard positives/negatives. Empirical experiments on several benckmarks validate the improved performance and efficiency. + The presented method is simple and straightforward. The authors only compare to the vanilla (no adversarial augmentation introduced) and one method based on adversarial augmentation (CLAE). However, the basic idea is still _how to generate effective views / hard negative or positive samples_. Such problems have been extensively studied in the literature for CL [1 5]. How does IDAA perform against these methods? If not thoroughly evaluated, the performance gains are not justified and hence not interpretable. Is it measured in the feature space? For CLAE, if the noise added to the input is bounded by a l_inf purturbation, then by definition the input should preserve the identity of the image. Continue on this point, to me it is very confusing on how you define identity preserving. This also holds for CLAE. The only difference is that the perturbation generated by IDAA is more interpretable, but this has nothing to do with "identity preserving". The augmented images still preserve their identities. Given that the visualization does not directly reflect the point of "identity preserving", the claims that current methods/augmentations "could change sample identity and result in poor representations" is not justified. In the experiments, all results of IDAA are based on some exsiting augmentations (e.g., those from SimCLR). The description of IDAA tends to sell it as a "universal" augmentation methods regradless of inputs, but domain knowledge (e.g., how you augment image data) is still needed from experiements. What is the actual performance of IDAA without using existing augmentations (those in SimCLR)? What is the actual computational cost (training time) compared to the vanilla SimCLR? [3] What makes for good views for contrastive learning. Overall, the idea is interesting, however the current draft is not well justified on the actual algorithm design, the motivation, and (perhaps the most important) empirical comparisons to existing CL methods that considers hard view mining. The paper has its potential to the field, but issues need to be addressed first.<|endoftext|>This work presents a new technique for disentanglement based inspired by the premise of an adversary that generate diverse samples that preserve identities via augmentation. In particular, they use the reconstructed sample of a VAE and its distorted difference (G(x)   x) as an adversary to perform data augmentation and show its effectiveness via success on a number of different self supervised learning tasks. This work studies an important area of machine learning that is important to a vast literature in self supervised learning. In particular, they propose a method that utilizes the strengths of deep variational autoencoders and their information theoretic properties to inherit identity disentanglement. As illustrated on the extensive applications, the method is readily applicable to schemes and I see the work presented in this paper to be extremely useful for the machine learning community at large for applications in data augmentation beyond those described in this work. In particular, this work may be useful for adversarial training via augmentation, which is a closely related area and given the  adversarial  motivation hinted in this paper, can be developed into such a scheme. The simplicity of the paper also serves to some extent as a weakness. This is due to the fact that there are several questions that may be asked albeit not taking too much merit away from the idea. The first is how specific is this method to the VAE? While the main reason for using VAEs is the link to information theory (the appearance of mutual information), which is the reason the main theorems work, I wonder how applicable this result would be at least heuristically with other deep generative methods such as the Wasserstein Autoencoder for example. The paper is very well presented and I found the level of exposition very high with little or no issues understanding the contribution of this work. I am a non expert in this area however one small question that does emerge is the motivation of using an adversary for this task. This is however a minor point as a non expert, it may be a common phrase used for disentanglement. I believe the paper presents a new method and serves to a useful problem of machine learning for a number of tasks.<|endoftext|>This work introduces a novel adversarial augmentation strategy. This is achieved by perturbing the latent space of a variational auto encoder while preserving the identity relevant features through the residual term. Strengths:  The idea is novel;  The main hypothesis that other augmentation strategies result in pair of samples that are either too easy or run the risk of corrupting the instance level information is backed up by empirical evidence;  This work provides a information theory based interpretation of the IDAA strategy with sufficient proofs;  The claim that better hard positives/negatives result in more sample efficient models w.r.t the batch size and the number of epochs is highlighted through various ablations;  IDAA provides significant performance improvements on all reported benchmarks;Weaknesses:  Skipping the conclusion is not an acceptable practice to gain more space. One could move some of the training hyperparameters to the appendix to gain some extra space in the main paper for that;  The ablation on the batch size raises the concern that IDAA does not scale up too while. It is unclear whether SimCLR will surpass SimCLR+IDAA with a sufficiently large minibatch. Indeed, we observe that increasing the batch size is beneficial for SimCLR only on Figure 6 a;  The datasets explored in the experiments of this work are somewhat limited in terms of diversity. Training a contrastive self supervised model on the full imagenet dataset can be resource intensive, however the authors could have explored the use of lower resolution versions of it while keeping all the classes (e.g.ImageNet 64x64);Minor improvements:  Typo in the introduction:  ullustrated in Fig.2 This work introduces a novelty augmentation strategy with sufficient empirical and theoretical arguments. Although some concerns remain about its scalability to larger batch sizes and more challenging datasets, the authors clearly show significant performance improvements when using IDAA across all experimental settings and empirically justify all claims/hypotheses made.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper casts the problem of NAS into a training free evaluation process by using neural tangent kernel (NTK). Specifically, the paper argues that the training dynamics and the performance of a DNN can be determined by the constant NTK of its linearization. Moreover, to efficiently evaluate the constant NTK of any network architectures, the paper proposes to use  the trac norm of NTK at initialization as an approximation. Interestigly, the proposed method is robust when applied in a data /label free search setting. Extensive experiments show that the searched networks have good performance and can be well transferred to other datasets. Pros++ The idea of using NTK as a network performance predictor for NAS is interesting, and the detailed method is non trivial and plausible (including the relaxation and sampling to solve intractable optimization). ++ The paper provides sufficient theoretical proof of its claim, including the assumptions made. ++ The search costs are very appealing and the performance of the searched architectures are good. ++ The proposed method can be data /label free, which is a good property that most NAS methods do not have. How about the results if NASI is directly applied on ImageNet, instead of transferring the best architectures searched on CIFAR 10? In general, this paper is of good quality and exceed the bar of being accepted by ICLR. Although I have some minor concerns about the performance, I think the paper has great contributions to the NAS community and should be recommended.<|endoftext|>This work proposes to search for good candidate neural architectures at initialization (NASI) so that we can completely avoid model training during the search. Strengths:The targeting problem of searching neural architectures at initialization is important and promising. The paper finds that such NASI is guaranteed to be label  and data agnostic under mild conditions, which demonstrates that the searched architecture can be transferred to different datasets or tasks. The retrained accuracy is promising and comparable or even better than other gradient based searching methods. Weakness:The authors claim competitive effectiveness on both CIFAR and ImageNet, while I only find CIFAR results in Table 1 and ImageNet 16 200 in Table 4 (also 16 and 200 are not defined). How comparable are the achieved accuracy to other NAS methods, e.g., FBNetv3, EfficientNet? The theoretical analysis is also provided to analyze the optimization via NTK tools.<|endoftext|>This paper proposes a training free NAS method called NASI, which exploits the Neural Tangent Kernel (NTK) to characterize the performance of the candidate architectures at initialization. To alleviate the costly evaluation for NTK, the authors apply a similar form to gradient flow to approximate NKT. NASI is well supported by the theorem of NTK and achieves competitive results empirically. Weakness:The main concern is the approximations in the paper. Since NTK has a certain assumption about the neural network, the unchanged characteristic may not be satisfied for some neural architectures. To calculate the trace norm of NTK and to solve the NAS problem efficiently, the authors both apply approximations here. Or just verify it empirically? 3.There are some other training free NAS methods (e.g., [1]). International Conference on Machine Learning, 2021. NASI applies approximations for neural architecture search at initialization based on NTK. If the authors can provide detailed explanations about that, I would consider raising my score.<|endoftext|>This paper proposes a new training free NAS method, where it is not necessary to optimize the weight parameters of target networks for architecture search. To achieve this, the paper exploits the capability of NTK for estimating the performance of candidate architectures at weight initialization. Thus, the proposed method can avoid network training during the search and achieve a much efficient architecture search. The experimental results show that the proposed method achieves competitive performance with existing methods and also can adapt to the label  and data agnostic scenarios. + The proposed method does not require the model training for architecture search, thus it is much more efficient than the previous NAS method. The main concern about this method is whether the proposed method finds good architectures. Firstly, recent studies have claimed that NAS methods find architectures showing good performance but the rank of the found architecture is far from the best [Yu+, ICLR 20]. In that sense, even though the proposed method achieves good performance on the benchmark datasets, I am wondering if the found architectures are really good or not. It would be nice to provide its cause and deeper analysis to better understand the proposed method. Is it possible to directly apply the proposed method to a large dataset such as ImageNet? [Yu+, ICLR 20] Evaluating the Search Phase of Neural Architecture Search, ICLR 20. [Yang+, ICLR 20] NAS evaluation is frustratingly hardThe proposed method shows promising results. My major concern is about the empirical and theoretical analysis (see concerns above). Hopefully, the authors can address my concern in the rebuttal period.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper proposes an alternative to straight through estimator by considering the geometry of the likelihood. The proposed method can be viewed as a natural gradient descent algorithm on the Riemannian manifold. Compared to STE, the proposed estimator seems to penalize the quantities that are far from the quantization boundary. Strengths  Investigating principled gradient estimators for the discrete quantization is an important problem to study. The idea of the paper makes sense, but the theoretical and experimental results cannot support the usefulness of the proposed method well.<|endoftext|>The main motivation for this paper is the gradient mismatch problem, which emerges from using the Straight Through Estimator (STE) in training quantized neural networks and leads to an unstable training process. Specifically, the ManiQuant associates the gradient mismatch problem with Fisher information, which can then be exploited to alleviate that problem. Of course, the gradient mismatch problem is caused by using the STE. But why does the root of that problem ONLY come from the STE variance (or its Cramer Rao Lower Bound)? Please elaborate on it. In Figure 2, it seems the demonstrated results are from a single run. The underlying logic is not clear (see the detailed comments above).<|endoftext|>This paper proposed a new gradient estimation method for training quantized neural networks (QNN). Indeed, the proposed method makes use of the natural gradients, which, to my knowledge, are much more computationally expensive to evaluate than first order optimization methods such as the plain SGD. It will be great to have a table (similar to Table 2 and Table 3) to list the acceleration or storage gain of different methods and compare them with vanilla training of full precision networks. Why is the quantization function a one on one map? As a non expert, I find that the method proposed in the paper is novel and empirically well tested.<|endoftext|>But the natural gradient introduced in Eq.10 does not seems to contribute to this goal. This paper proposed to improve the gradient of quantization operator in quantization aware training (QAT). 2.How does optimizer interact with the proposed gradient? Pros:1.The motivation of this paper is straight forward and interesting: Basically, it first pointed out that quantization error is introduced by variance of STE, which can be upper bounded by the FIM.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; In the paper the authors propose TARTAN, methods to enable end task aware pre training. MT TARTAN simply combines the pre training objectives and the end task objective as multi task learning. * The authors overcomes the pathological solution problem in their META TARTAN methods by introducing a separate classification head $\phi^{*}$. * Though the authors put lots of efforts in META TARTAN, it appears that META TARTAN is comparable with MT TARTAN in most cases. * The paper needs more thorough discussion on computation costs. Does the results in the paper include this step or not? Seems to be contradicting. Thanks the authors for their hard work! Weakness: lack of visualization and clear description for the method; comparison in computation cost and data usage is questionable.<|endoftext|>However the experimental setting is a bit limited and the results are also mixed. The key is to incorporate the end task objective function to the pre training stage. The paper focuses on the continued pre training setting and it proposes a multi task end task aware training method (MT TARTAN) and a meta learning variant to achieve better performance than the pre training + fine tuning paradigm. There’re a few minor concerns about the paper:1. Lacking comparisons with other multi task learning (MTL) work about weight selectionOne of the main contributions is to use meta learning for choosing the weights of each pre training task. The experiments cover three datasets and show mixed results. A few more questions and comments:1. I guess it requires some revision to make that clearer at the beginning of the paper. The MTL f design to combine pre training tasks and fine tuning tasks in continued pre training stage is well motivated.<|endoftext|>The paper provides formal formulations of the various set ups. 2.The set up is clean and the experiments are done thoroughly. There are several interesting observations. Intuitively, it is not surprising that the proposed method performs better than other end task agnostic pretraining. When there are a large number of end tasks involved, it become daunting to train/pre train large models for each end task. Though I also acknowledge that the end task aware approach proposed by the paper is a good middle ground when there is a specific end task and medium amount of resources are available, which makes training useful large models more accessible, since the end task aware method are more data efficient (resource efficient.) The paper is well written and easy to follow.<|endoftext|>The paper makes the argument that generic pre training (on auxiliary tasks) is inferior to task specific pre training. The authors argue that the final task should be learned together with the auxiliary tasks in a multi task setup, which they call MT TARTAN. The authors back this up through experiments. In a setting where the auxiliary tasks are potentially more noisy, they also observe an advantage of META TARTAN over MT TARTAN. The paper is clearly written and well structured. The results should still be of interest to the community.
Reject; rating score: 5; rating score: 5; rating score: 5; The problem dealt with in this paper will be a promising application of representation learning based on neural networks. I could not find any positive aspects for accepting this paper to the premier conference in the field of representation learning. Both the methodology and documentation need to be improved for acceptance.<|endoftext|>The contribution of this paper can be somewhat incremental. Experiments on two datasets show the effectiveness of the proposed approach. Minor:There are some typos and grammatical issues.<|endoftext|>The explanation of the considerable gap between BoI and LURM on the two datasets is unconvincing. There are some format problems in the paper, such as no period at the end of the first paragraph of the introduction.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper presents a probabilistic channel pruning method (BWCP) for accelerating CNNs. They evaluate their method on CIFAR 10/100 and ImageNet compared to other recent filter pruning methods. The main spirit of this paper, “unimportant channels are stochastically pruned with activation probability”, namely the so called “probabilistic” pruning, has been explored in the literature. Missing important comparison with more recent methods. In Tab.2, ResNet50, ImageNet.<|endoftext|>The paper presents a probabilistic channel pruning method based on batch whitening to accelerate and compress CNNs. In the paper, the authors argue that  bach whitening increase the activation probability of useful channels while keeping the activation probability of unimportant channels unchanged . This is demonstrated in Fig.3 (a.b). If increasing the activation probability is helpful, we should see BWCP achieves a more obvious performance gain compared to the case (3). If we measure the training budget by the training time, other methods actually use a smaller training budget. The improvement over previous baselines is around 0.3 for ResNet 50.<|endoftext|>In this paper, authors propose to compress convolutional layers via batch whitening channel pruning (BWCP). The training efficiency of the proposed method is not discussed. Will the stochastic pruning scheme require additional training epochs, compared with other channel pruning methods? Authors state that a sigmoid like transformation is used to make activation probabilities approach 0 or 1 during training. Although the stochastic pruning scheme can explore the channel space more sufficiently, it is also possible that more training time are needed for such exploration.<|endoftext|>At the testing time, this sampling is discarded and replaced with a deterministic selection. The benefits of whitening in improving the convergence of NNs is widely explored. Can the authors provide more insights/experiments to this? How much better is this method compared to direct whitened training? a typo in page 8: spare  >sparseThe paper has room for improvements, but it is a nice new attempt to the pruning of networks.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper suggests hybridising neural rough differential equations (NRDEs) with nonlinear dimensionality reduction techniques (autoencoders). This is an entirely reasonable approach, and by doing so the authors demonstrate impressive improvements across long time series benchmarks. I would be curious if the authors have any observations on which is more effective? I would note that I do not think this idea is entirely novel. dim(x)".However dim(s) refers to the dimensionality of the log signature, whilst equation (2) is an expression for the signature.<|endoftext|>The proposed autoencoder embeds "log signature" information into a lower dimensional space and can be trained prior to the main NRDE model. This approach is novel, easily understood, and intuitively should outperform previous log signature based models. The authors demonstrate that it gives significant improvements on long time series problems through several experiments. I very much like the paper, particularly the main idea of using a pretrained signature based autoencoder for time series. For these reasons, I would recommend the paper for acceptance into ICLR.<|endoftext|>The manuscript proposes a new method of training neural RDEs with a pretrained autoencoder to reduce overhead of the log signature transform when dealing with long time series. The proposed model is specifically made for long time series, but not all the baseline methods are.<|endoftext|>This enables the authors to work with lower dimensional log signatures during the main training stages. The method performs competitively on a range of long time series tasks. al.relates to the signature (and its universal non linearity) __not__ the log signature. However, the writing of the paper is sometimes unclear and I am reluctant to recommend acceptance so long as there are (mathematical) statements that I cannot verify. The paper proposes an interesting extension to neural rough differential equations which shows good results in practice and is in principle a reasonable contribution to ICLR.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper presents a framework called adaptive Q learning that integrates the advantage of offline learning and online learning. Inadequate experiments design. This paper, at its current status, is not ready to be submitted to conferences like ICLR. Actionable feedback for the authors might be rethinking the storyline of this work.<|endoftext|>In addition, the authors propose using a CQL variant which uses an ensemble of action value function as done by REDQ to learn in this setting. My concerns about the experimental results make me doubt that this paper is ready for publication. # Questions:  Is my understanding correct that newly collected samples are added to both the online and offline buffers? What are we averaging over? This raises some concerns about the significance of the empirical results, and the usefulness of the proposed regime. How are the offline and online datasets used, are they just concatenated?<|endoftext|>The paper proposes GCQL, a new RL algorithm that is trained on a mixture of offline data and online interactions. Overall, though the proposed algorithm seems like an incremental change over existing ones, it clearly is preferable to use over existing methods that simply fine tune on the online data. I think the paper studies an important problem of offline online RL where the algorithm has access to both offline data and a limited amount of online interactions. One concern is that the performance gain of GCQL over GCQL wg (which I interpret as effectively being CQL with ensemble Q learning) appears to be very marginal.<|endoftext|>This paper tackles a variation to the offline RL setting, where the agent is allowed some limited number of online interaction steps after learning offline. An algorithm, CGQL, is proposed for this setting and uses the idea that online and offline data should be used in different updates. The main intuition for adaptive Q learning make sense to me treating the offline and online data differently, with modified updates for each. It s also interesting to have experiments where the online interactions with the environment are limited. The problem the paper tackles is interesting, offline RL with limited online interactions.
Reject; rating score: 3; rating score: 3; rating score: 5; This paper characterized the scaling of generalization error with respect to the number of training samples $D$ and parameters $P$ for certain smooth estimators in two regimes: the variance limited regime and the resolution limited regime; this divide is somewhat analogous to the parametric vs. nonparametric estimation. The findings are supported by empirical evidence. The authors should discuss the difference from and improvement upon these papers. In particular, 1. Moreover, for neural networks in the kernel regime, the role of optimization (i.e., the number of training steps) can also be incorporated in the analysis, as in [Nitanda and Suzuki]. **(ii)** The theoretical results are somewhat underwhelming, and some discussions lack rigor. Moreover, this does not provide any insight on the scaling law of training speed.<|endoftext|>This paper considered Variance Limited Regime and Resolution Limited Regime to explain the explored neural scaling law. In the Variance Limited Regim, one fixed one of the D and P and in the Resolution Limited Regime, one of the D and P is effectively infinite. The variance limited regime happens when the model size is constrained, if the rademencher complexity is bounded, then you have the 1/sqrt(data number) convergence, it s the variance in the statistics setting. The paper is not well written and theoretical results are not well stated. For the Variance Limited Regime, the statement is not clear. The reviewer also find out that the author lack of knowledge in basic statistical machine learning theory. The paper also igonres standard results in the statistical results, bias variance decomposition vs the four regimes and the non parametric rates. The bound the paper achieve is the same as the approximation rate obtained by [1 4]. They achieved N^{ s/d} approximation error for the s holder function, and is standard and well known in the literature. [1] Schmidt Hieber J. Nonparametric regression using deep neural networks with ReLU activation function[J].<|endoftext|>The paper proposes a phenomenon called scaling laws of test loss for neural networks. Specifically, the variance limited scaling follows simply from the existence of a well behaved infinite data or infinite width limit, while the resolution limited regime can be explained bypositing that models are effectively resolving a smooth data manifold. Overall, the paper studies a very interesting phenomenon. It s quite interesting to see such power law with respect to width and data set sizes with the other being fixed. Hence, an accept has been recommended. However, there are several concerns remaining. Then this is a counter example to your phenomenon? These two terms are bit confusing. 4.In some cases, I find smaller network perform better (say width 32), larger width cannot work. However, authors need to clarify a few concerns.
Reject; rating score: 5; rating score: 6; rating score: 6; The paper deals with (unsupervised) learning with graphs, specifically node level tasks. Inspired by spectral GNNs, specifically Graph Convolutional Networks (GCN), the authors propose a simple neighborhood smoothing technique to capture the graph structure around each node in the given graph. Motivated by the problem of over smoothing of GCN (Li et al., 2018), they propose the so called "over smoothing distance" measuring how close a node s feature is to be over smoothed, which is simply the row wise distance between $A^k X$ and $A^{\infty}X$ with regard to the Euclidian distance, see Def. 3.1.Based on this distance they define the smoothing weight matrix which is then used to weight the neighboring node features during neighborhood aggregation. Simple approach that seems to work well2. Scalability experiments only carried out on synthetic graphs**Suggestion**1. Is your approach also liftable to other GNN architectures besides GCN?<|endoftext|>The paper presents NAFS (Node Adaptive Feature Smoothing), a method that constructs node representations by relying on smoothing only, i.e.without parameter learning. To do this, the authors first provide a formulation for the smoothing operator after infinite steps, i.e.when the stationary state is reached. They then define over smoothing distance as a way to assess how much a node is close to the stationary state after k smoothing steps. Finally, they use over smoothing distances to calculate a different smoothing weight for each node. The paper tackles the problem of over smoothing by proposing a method to differently weight the smoothing for each node. I have read the authors  replies to the other reviewers and myself and they look convincing to me. This makes the paper way more convincing in my opinion and worth being accepted, which is why I increased my score accordingly.<|endoftext|>The method first performs feature smoothing, then combines the smoothed features using adaptive weights which are node specific. The authors have conducted many experiments to validate the model performance, and demonstrate the model s efficiency empirically. The authors took a novel perspective and presented NAFS which does not explicitly require parameter learning. What are the features used for each of the dataset in the experiment (Cora, Citeseer, PubMed, Wiki)? I would be good to state them clearly in the paper. In summary, the paper is well presented.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; The paper considers the problem of establishing causal direction in a bivariate system under the assumptions of additive noise (possibly post nonlinear) and no unobserved confounding. It establishes a connection between functional causal models (FCM) and optimal transport problems in dynamical systems to derive a new divergence metric that can be efficiently computed as a conditional variance estimate, and shows this quantity is zero iff in the causal direction..Efficacy of the metric is evaluated on synthetic data as well as the real world Tuebingen data stand found to outperform existing measures.<|endoftext|>The paper tackles the problem of causal discovery in the basic case where a pair of variables is considered. Then, the main contibution is a novel dynamical system view of Functional Causal Models which aims to identify the causal direction in the case of pairs of variables. The paper exploits connection between functional causal models and optimal transport.<|endoftext|>The authors frame the bivariate causal discovery problem in terms of the analysis of a dynamical system. They use results from the field of optimal transport to interpret additive noise models from this framework. Given that the problem of causal discovery is a conceptual problem as much as it is a statistical problem, the introduction of such new perspectives are certainly helpful for the discussion in this field.
Reject; rating score: 1; rating score: 1; rating score: 3; Taking sequences of the spike portion of COVID 19 genome sequences, this paper builds classifiers for different labeled variants. Two approaches for generating errors in test sequences are introduced and evaluations of the models robustness to these errors measured. The paper is clear about its contributions and provides a clear introduction to the problem domain. The paper takes a very simplistic approach to classification of these sequences not taking into account any biological domain knowledge.<|endoftext|>This paper presents a framework to test the accuracy and robustness of different machine learning algorithms in classifying the COVID 19 spike sequences.<|endoftext|>The concepts introduced in the paper are not novel and have been studied in detail: encoding of amino acids, _k_ mer based models, and simple MLPs have been thoroughly studied in computational genomics. While I appreciate the time and energy the authors have put into this paper, I have some major concerns with this paper:  My major issue with this paper is the sequencing errors are not modeled realistically. They explore several feature encodings and machine learning methods to identify such erroneous sequences. A more realistic analysis would focus mostly on the RBD.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This work proposes a sparsity enforcing structure for GANs by splitting layers into the generator for the sparse vector and for the final image and by training with sparsity enforcing regularizer in the latent space between them. Strengthes:  The proposed method is neat and did improve the performance of other GANs with the proposed structure and the loss. It was claimed that "We propose a novel sparse modeling interpretation of image generators for better understanding them. ", but the work of Papyan et al., 2017b does not have to be limited to non generators, so it is hard to see this as a contribution of this work. It is not easy to be convinced that the proposed method works well since images are too small (is the small image sparse?) There are also prior works on DIP using other regularizers such as the following:[Liu] Liu et al., IMAGE RESTORATION USING TOTAL VARIATION REGULARIZED DEEP IMAGE PRIOR, IEEE ICASSP 2019.<|endoftext|>There are shortcomings in the overall concept as well as its evaluation. The findings suggest that this might be a promising avenue of research, but it would need to be taken further. At present, the paper boils down too much into simply adding a simple regularizer at the end and observing that it somewhat improves some metrics in a limited number of scenarios. Due to the limitations of the evaluation, it remains unclear whether the proposed improvement carries over to state of the art models and datasets. Similarly, the promised elucidation of the purpose of the feature values never really materializes.<|endoftext|>Extensive experiments demonstrate that the proposed methods outperforms state of the art image synthesis methods including traditional sparse coding methods and deep learning based methods. 2).Extensive experiments demonstrate that this method outperforms the state of the art baseline methods as suggested in the claims of this paper. There is one key assumption in the sparse coding that the latent code between backbone network and output is sparse when the output image is synthesised at the best performance. The authors are suggested to prove first that the latent code layer must be a sparse vector (or  tensor)2). It would be great to provide the scores of these two traditional metrics in the paper in order to have an all around evalutions. This paper generally proposes to improve the existing multi layer Convolutional sparse coding frameworks by splitting the tasks into independent and consecutive components.<|endoftext|>The paper describes image generation as a sparse coding reconstruction process. The experiments show that the method can improve FID scores of common GAN networks and it can also increase PSNR of image reconstruction using deep prior when compared to the same network architectures without sparsity objective. 1) The paper is overall well written. 2) The method is relatively simple and it could possibly be widely adopted if the experiments in the paper generalize to a wider range of problems. The paper only tested two scenarios be it with a range of baseline architectures. The evaluation is limited in scope but it appears technically valid. In this case we are getting "better" (based on some metrics) images but at what cost? Minor editing issues:  Abstract: "leading to state of the art performance"   performance in what? I like this concept and I like that it very simply improves performance of established GAN methods.
Reject; rating score: 5; rating score: 5; rating score: 8; rating score: 8; This paper ​proposes some modifications including two normalizations and two new scaling operations to mitigate the issue of gradient magnitude mismatch inside the transformer. However, the contributions are not clearly presented in the Introduction. Besides, since the section of related work is put at the end of the paper, the readers may get confused about how the proposed method differentiates from existing works. (2) The issue of gradient magnitude mismatch is not well explained. As they are the main motivations of the proposed method, it is suggested to provide a detailed explanation about how gradient magnitude mismatch, optimal weighting connects with the proposed method. As shown in Fig.3, gradnorm in only several layers are visualized. (4) I appreciate that the ablation study is conducted to show the performance gain of each component in  NormFormer. (6) Are there other studies investigating training instability? It is good to show the effectiveness of NormFormer through extensive experiments. (7) The results seem convincing and competitive. The proposed techniques are empirically successful but not well motivated. I hope that the authors can firstly present the issue inside the PreLN transformer clearly and then provide a detailed discussion about why the proposed modifications can mitigate this issue.<|endoftext|>To this end, it proposes to add two LayerNorms after the multi head attention and the GELU non linear activation in FFN, respectively. It also adds learnable scaling coefficients for the FFN residual and the attention head outputs. The four modifications are applied to both casual and masked language modeling with improvements observed in downstream tasks. The paper is well written and easy to follow. 2.The four operations are easy to implement and applicable to many tasks using transformer architectures. 2.There is a lack of analysis of why the gradient mismatch issue exists in Pre LayerNorm transformers and why the proposed four operations can alleviate it. In summary, this paper proposes to add four operations, two LayerNorms and two scaling parameters, in the Pre LN transformer layers. However, I think the method novelty is not enough.<|endoftext|>This paper proposes a framework that adds extra layer normalizations and scaled residual layer to the normal GPT 3 model. Empirical results demonstrate faster convergence speed and its robustness towards the learning rate. In general, this is a good paper with sufficient and exhaustive experiment results. The 3 GPT 3 models of different size scales prove the generalizability of the techniques. There are some small points that I think the authors can improve to make their statement stronger:* The current framework, though claimed to be Transformer related modification, is conducted in GPT 3 only. It would be helpful to see whether this Transformer modification can be used in other frameworks such as BERT and T5 or a single Transformer model. It would not be that a strong statement to argue for the added robustness if higher LR would not always lead to better quality in general. The paper shows a simple yet effective way of modifying the existing GPT 3 framework by increasing only a small number of parameters and computation per epoch, which can be a potential addon to many existing Transformer architectures.<|endoftext|>NormFormer improves on Pre LN transformers by making the following modifications: learnable scaling parameters for each dimension of the output of each attention head prior to concatenation across heads (*Scaled Attention*); layer norm on the attention output (*Post Attn LN*); layer norm on the FFN nonlinearity output (*FFN LN*); and learnable scaling parameters for each dimension of the skip connection around the FFN (*Scaled Residuals*). And why did the authors choose only to look at the minimum value of lambda_resid? The layerwise analysis of lambda_resid should be repeated with different metrics. I think this is mostly solid work. There are a few relevant papers that might be worth discussing (or at least referencing). Please include quantitative results from the **Other Experiments** section. However I think the baselines could be more challenging and the analyses more convincing. Transformers are notoriously unstable to train. Personally, I think “x% as much compute” or “x% the amount of compute” are more intuitive, as they clearly mean “(normformer/baseline)*100 amount of compute”. It would be great to see some experiments demonstrating the causal effect of gradient norm changes on trainability. I suggest changing all three plots to a logarithmic y axis. I think it would be very informative to examine the scaling parameters over the course of training—especially early in training. Does initializing the NormFormer parameters to be of similar magnitude to what is observed at convergence yield improvements over the current initialization scheme?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; It considers the distance between the distribution of the input and the distribution of prototypes in the latent space of a VAE (the explanation space). This is an improvement over the previous work of (Oscar Li et al.AAAI 2018). The paper unfortunately hasn t done an experiment with PDL, only comparing to the "no explanation" scenario.<|endoftext|>However, these techniques are not well motivated to me and the particular application of the proposed visualizations, explanation space, is also missing. It also comes with an easier way to visualize the internal activations of the model by the decoder part of a VAE. However, before rushing into the exploration part of the method, there is a missing part of comparing the proposed method with the prior work.<|endoftext|>The paper extends prototypes based explanations by proposing what they call explanation spaces describing the relationships between input and prototypes, the relationship between prototypes, and how prototypes are distributed. They construct explanation spaces using VAEs and suggest a way to find the optimal number of prototypes. This is an interesting idea.<|endoftext|>There are no such designs in the proposed model. The authors also conducted human evaluations via Amazon Mechanical Turks to validate the interpretability of their model This paper improved existing prototype based DNN by enhancing interpretability and while also improving the performance. Do prototypes have redundancy issues?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 8; It s not clear if the incremental learning approach used should, in principle, be better than reinforcement learning based approaches used in the literature (e.g., [3]). Reinforcement learning of theorem proving. They proposed a new representation of logical statements based on transformers but a motivation of this new representation is not given, especially given that graph neural networks have been shown to represent logical statements well. The authors create their own dataset instead of using benchmarks widely used in the literature (M2k [1] and MPTP2078[2]).<|endoftext|>The paper applies hindsight experience replay (HER) to automatic theorem proving (ATP). HER is a technique from reinforcement learning which mitigates sparse rewards by treating failed attempts as successful attempts on a different problem, namely that with the end state of the attempt as the goal. Overall the writing is clear and well structured. The title of the paper seems somewhat misleading given that this is not RL but an HER inspired incremental learning method. TacticZero by Wu et al 2021 is missing in the references.<|endoftext|>The proposed use of spectral encoding in this context can be a useful contribution however no experimental results were reported to ablate that [see below]. Note that incremental learning has been used in other shapes or forms before in this context and thus the main value proposition of the manuscript is the addition of HER to that scheme. So why not train a model (with the same architecture) on this training set and compare it against IL+HER as a baseline? They show that HER indeed helps their incremental learning scheme to a point that the resulting ML aided prover achieves comparable performance wrt.<|endoftext|>The paper is well written. It is fine that the resulting prover does not outperform E which is SOTA, but in that case I would like to see at least one comparison with other learning based approach, evaluated on either TPTP or another suitable data set. Table 1 also seems to suggest that TPTP is probably too easy for both E and IL w/HER.<|endoftext|>The authors propose a very compelling solution to the data scarcity problem in automated theorem proving: relabelling failed searches by the sub goals reached. The main shortcoming of the paper is a lack of repeat experiments, and missing desirable ablations for high level design decisions.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; rating score: 5; The authors provide a thorough theoretical justification to the recently emerging and heuristically proposed label trick, i.e., adopting a random portion of node labels in the training set as the input node attributes. The authors derive general formulations for the label trick as regularizations in both regression and classification cases. Overall, I recommend acceptance due to the thorough analysis and potentially broader use cases inspired by the work.<|endoftext|>I appreciate the formal description of the label trick and the associated theoretical analysis. However, results in the four application scenarios that the authors selected indicate little or no benefit of the label trick. The paper also discusses broader applications of the label trick in order a) to empower graph based methods with trainable weights and b) to get rid of the randomness effect by incorporating self excluded propagation within GNNs composed of linear propagation layers.<|endoftext|>The paper focus on the label trick. The label tricks is a method for graph based semi supervised learning. The authors states that the first method is utilizing labels and the second method is utilizing features based on a graphical model. For a paper that seeks a theoretical explanation of the usefulness of the label trick. I do not have the impression to have learn a lot from it.<|endoftext|>The authors discuss the utilization of labels in graph neural networks (or conversely, the utilization of node features in collective classification algorithms, e.g., label propagation) for node prediction. (–) Some further emphasis/credit should be given to https://arxiv.org/pdf/2009.03509.pdf who (I believe) first proposed the label trick. ### Strengths (+) & Weaknesses (–)(+) The problem that the authors tackle is fundamental/important. Further, which GNN is used?<|endoftext|>The paper proposes a deterministic version for the label trick in GNNs (and other graph frameworks), that is based on a self exclusion operator for the propagation process in order to avoid label leakage. The authors propose some applications in other label propagation settings. The paper provides a relative new framework for GNNs where the stochastic label trick is replaced by a deterministic alternative which focuses on exclusion of self propagation, the alternative seems to be novel2.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The authors only compared the performance with 2 papers, but I think there should be a large number of pruning techniques for binary neural networks. This paper needs better illustration of their algorithm and better justification of the novelty. I cannot get the novelty of this work. In Table 4, the gain is not significant.<|endoftext|>5) The authors claim 190 2200x memory reduction for MNIST, which seems unsubstantiated in the paper. The paper introduces an algorithm for pruning binary networks. The main strengths of the paper are:1) Review and discussion of current approaches that the paper is building on. 4) It is not clear what the authors claim as novelty.<|endoftext|>This paper combines two existing methods (i.e., binarization and pruning) to reduce both computational complexity and memory requirements of deep neural networks (DNNs). As mentioned in the paper, this work relies on two existing works. Weaknesses:1) The contributions of this paper are not novel. The proposed method is a combination of two existing methods and it overlaps with a previously published work (please see https://openreview.net/pdf?id r1fYuytex).<|endoftext|>This paper introduces a neural network compression method built on magnitude based unstructured pruning and binarization techniques. The proposed methods are built heavily on the previous methods, it would be better if the authors could propose more novel ideas. the output of the masked weights are integers, which is confusing because the authors claimed that this paper is working on pruning the binary neural networks. In general, I think this paper is not ready for publish and still has a big room for improvement in terms of novelty, writing, and technique contributions.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The authors study average case representation capacity of random ReLU neuralnetworks, as measured through their linear regions, as in Hanin and Rolnick2019   the difference in the authors  setting is that they consider the inputspace to be defined on a low dimensional manifold, whereas Hanin and Rolnickconsidered inputs on the solid cube. ### Weaknesses  The theoretical content of the paper is very incremental   in effect, it  consists of a translation of the results of Hanin and Rolnick to the setting  of manifold data. In particular, almost all arguments of Hanin and Rolnick  are reproduced in the proofs of this paper (the theorems are in one to one  correspondence, for a start), just with the corresponding concepts for  manifolds substituted in (for example, replace Hanin and Rolnick s coarea  formula with the smooth coarea formula; replace integration over euclidean  space with integration with respect to the volume form on the manifolds, and  handle the corresponding curvature and "reach" quantities that arise, which  leads to the new constants relative to the theorems of Hanin and Rolnick). In addition, the proof has some odd aspects: it seems to  use an asymptotic statement for $C_{\kappa, k}$ in $\epsilon$, but the  theorem asserts the result for all $\epsilon > 0$ and does not show that this  constant depends on $\epsilon$. It seems that this result needs to be  rewritten to be asymptotic for small $\epsilon$   this should also enable a  more precise conclusion to be asserted, similar to in Hanin and Rolnick  2019a. The experiments conducted and the authors  discussion of them does not  provide very much insight into the unique circumstances present in the  manifold case, relative to what is known and expected from Hanin and  Rolnick s work. This is important because it relates to the authors  stated aim  for the paper. In addition, the only works the authors  reference are about approximation properties of deep neural networks on  manifolds: there are also very relevant works on algorithmic results for  training DNNs to perform classification/regression on manifolds, e.g.[2 6]  below.<|endoftext|>This paper theoretically characterizes the number of linear regions of ReLU neural networks at initialization. Compared to existing results, the authors take the data manifold into consideration, and the number of linear regions is measured on the data manifold. Moreover, for uniformly random sampled data on a compact manifold, the expected (geodesic) distance to the linear boundary is analyzed. To further support the theory and also extend to the behavior through the training process, the paper provides synthetic data and real data experiments. There are indications of the number of linear regions does not deviate significantly from their initialization and is faithful to the data manifold dimension. However, the theories and experiments have some issues, which undermine the overall quality of the paper. I believe that the study of linear regions of ReLU networks possesses value and has interesting implications. In addition to theoretical study, the paper also provides numerical experiments, in support of the theory. The reviewer found this paragraph very complicated and hard to follow. Nonetheless, the results in Theorem 2 and 3 seem not comparable with Hanin & Rolnick (2019a). Moreover, it is possible to choose an optimal $\epsilon$ in Theorem 3. However, there is no connection between the performance of the network with the number of linear regions. When changing the number of neurons, does the number of linear regions change correspondingly as predicted by Theorem 2?<|endoftext|>It generalizes the results by Hanin and Rolnick 19’ to the manifold setting. Two theoretical bounds: 1) an upper bound for the number of linear regions and 2) a lower bound for the average manifold distance between points on the manifold to the linear boundary. The extension is meaningful in that it takes advantage of the structure of high dimensional data and makes the theoretical results more applicable to real world data. The experiments on two toy datasets were designed to demonstrate the effects of manifold geometry and the neural net architecture. There seems to be a gap between the toy example and applying the analysis to real world data. Overall I think the paper can benefit from more empirical results and discussion. While the extension to the manifold setting is a valuable contribution by itself.<|endoftext|>This paper provides an analysis of the impact of data geometry on the regions of linearity in deep neural networks. To this end, it extends previous results by Hanin and Rolnick, which were established in terms of the Euclidean ambient feature space of input data, to account here for nonlinear structure when modeling the data as being sampled from a manifold submersed in this ambient space. The quantities related to the local linear structure of ReLU networks are directly related to the number of neurons, and theoretical results are validated empirically, albeit limited to simplified toy examples. To the extent that I could verify, they seem correct, and the empirical validation (even if a bit simplistic) demonstrates them well. The shift from Euclidean space to intrinsic manifold yields interesting differences in the results from previous work by Hanin  and Rolnick, such as eliminating exponential growth in the bounds established in the latter.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The authors also present good rationale why directly skipping layers in ViT might be acceptable: due to the observed layer collapse in many trained ViTs. Also, all tables in the paper are not numbered – very weird and hard to refer to. I also like the rigorous optimization in this paper. And why not more baselines for DeiT Tiny with token?<|endoftext|>This is an interesting work trying to assemble three effective techniques for pruning, layer skipping, and knowledge distillation. The formulation and algorithm are correct to my best knowledge. (Optional) it would further strengthen the paper if more strong ViT variants can be reported such as Swin Transformer or T2T. The authors have well addressed my major concerns. I’m willing to revisit my rating if the authors can provide the above questioned results.<|endoftext|>This method needs to provide a more rigorous comparison, and at the same time, it needs to be verified on more visual Transformer architectures to prove its effectiveness and generalization. Experimental results in ImageNet with DeiT models prove the effectiveness of the proposed method. * The paragraphs in this paper are clear and easy to understand.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The paper introduces Neural Simulated Annealing (Neural SA), a heuristic for general combinatorial optimisation (CO) problems. The information presented in the experimental results could be improved. I believe the core idea of combining learnt local improvement operators with a Metropolis Hasting’s search procedure is very interesting, however experimental results are underwhelming. Vanilla SA is poor (compared to other baselines) on the CO problems presented, and whilst learning the proposal step improves performance, by the author’s admission it is not state of the art. **Errata**A few minor points that do not require and response from the authors.<|endoftext|>The paper proposes a new Neural Simulated Annealing approach to solve optimization problems. My major concern is that the proposed neural simulated annealing is clearly not competitive with other baselines. 2.Although there has been some studies for hybridizing SA and learning methods, the authors have distinguished their work from existing studies clearly. I like the idea of viewing simulated annealing as the Markov decision process, and using reinforcement learning to enhance simulated annealing.<|endoftext|>Simulated annealing is a widely used stochastic optimization approach. The paper presents a general approach for learning the proposal distribution for simulated annealing. The approach is general and amenable to many problems. Weaknesses:  Limited technical novelty: while the idea of learning a policy for SA using RL is novel, the technical solution follows existing approaches for policy learning using RL. In particular:     1) comparisons with "vanilla SA" and not state of the art SA solutions that are tailored to each problem. However, it has limited technical novelty and the experimental results shows it is not as effective as tailored classical or neural solutions.<|endoftext|>The neural network policy is trained by a reinforcement learning (RL) mechanism, which relies on re formulating the Markov chain of the underlying combinatorial optimization problem as a Markov Decision Process (MDP). The results from the authors generally show improvement of their method compared to traditional SA and outperformance compared to other methods in some, but not all, of the cases. **Strengths** I believe the paper has the following strengths:  A modular and practical improvement on a traditional optimization technique (SA), which demonstrates clear improvement on the optimization benchmarks provided. This strengthens the claim that Neural Simulated Annealing is modular, but significant, improvement on SA. An ablation of the state definition to see what information may or may not be critical for NSA to tackle the problem effectively.
Accept (Oral); rating score: 8; rating score: 6; rating score: 6; rating score: 10; This paper presents a method of using external memory called "shared workspace" for communication among different neural modules or "specialists". The key idea is that there are limits on the communication bandwidth and the specialist modules must compete for access. Strengths:  The paper is written very well. The related work compares the proposed approach with other memory based neural models. The proposed approach is original to the best of my knowledge. The key idea is very intuitive and motivated by insights from cognitive science literature. I also like that the authors use different types of backbones (Transformers and RIMs) in different experiments which indicates the proposed Shared Workspace method is not specific to certain kind of backbones. Weaknesses:  Although the authors perform experiments with wide variety of tasks, multi agent tasks are missing where I believe coordination is more important. In all the tasks used in the paper, different neural modules process different parts of the input to make a common prediction. Multi agent tasks are more challenging as each agent would get a different input but also predict a different action. How are these chosen? The authors propose two versions of Shared Workspaces, soft and hard (SSW and HSW). Some experiments one contain a single version. Why are both SSW and HSW not evaluated in all the experiments? The paper proposes a novel method for coordination between neural modules which is well motivated. The experiments are comprehensive although some details are missing.<|endoftext|>The paper proposes a modification for attention based network architectures drawing inspiration from the global workspace theory in cognitive science. Essentially connections are made sparser, with different ‘specialist’ units communicating with each other through a limited bandwidth channel. The exact means by which this interaction is carried out is by using key value attention. This approach when applied to transformers leads to much higher computational efficiency (linear instead of quadratic in the sequence length). Further research in probing similar ideas for much more efficient information processing, backed by ideas from cognitive science, would be very beneficial for the field in my view and this paper would help the community in that regard. The authors also include details about the experiments and algorithm implementation in the appendix, which will aid in reproducibility. Weaknesses 1. I am concerned whether the framework proposed here will also be effective in settings where this is not necessarily the case, such as general modeling of language and images. For example, adding shared workspace to transformers imposes a communication bottleneck between representations at different positions of the sequence. It is possible that for problems where the solution does not depend on only a small portion of the input, considering the pairwise relationships of representations at every point in the sequence is critical for good performance. To study this, the shared workspace model would have to be evaluated on larger, more unstructured datasets, where transformer based architectures have already been demonstrated to do well (for example the data on which GPT 2 was trained). Adoption of this approach would be much more widespread if the authors can demonstrate that on these larger datasets, training transformers with the shared workspace doesn’t lead to worse performance than training the regular transformer based models that are currently used. The paper proposes an implementation of an interesting theory from cognitive science for more efficient information processing in networks, by making connections between entities sparser. While there is extensive evaluation on environments, most of these involve processing a small portion of the input. The paper will be made a lot stronger if the approach can be shown to scale to larger more unstructured datasets where transformers are known to work well.<|endoftext|>This paper proposes a communication framework to have multiple modules communicate and switch precedence efficiently, taking inspiration from Global Workspace Theory in cognitive science. The primary contribution is a scheme to replace complete pairwise communications in modularized architectures with a single, limited capacity workspace that persists and changes over stages of computation. This workspace is implemented with a read/write scheme that iterates through stages of computation (layers in a Transformer or steps in a recurrent architecture): in the write phase, the shared memory is updated according to the current states of the modules that are most informative to the shared memory s current state as determined by a key value attention scheme, with the modules competing via softmax. They can and should be claims/driven. The advantages are claimed to be 1) higher order interaction among modules because every module learns from every other one (at least, more than pairwise), dynamic filtering because the memory persists and updates stage to stage; and linear computational complexity because the number of memory slots doesn t change much (and it s typically small, 1 10). The related work motivates this paper by the classical AI principle that intelligent systems should have multiple specialized modules rather than one general entity. The experimentation section tests different parts of the proposed scheme. The CATER experiment shows a similar result to the triangles experiment (quickly picking out only relevant information) but in a time series, and the Sort of CLEVR experiment again reinforces the power of the shared workspace on sparse tasks. That robustness can be shown by having a claims driven structure for the results section rather than a testbed driven structure, which to the reader is arbitrary and doesn t deliver the important information as well as it could. I think the strengths fall into three main categories: good problem setup that took the reader from concept to formulation well, an interesting idea with good scoping, and an impressive experimentation suite. The qualitative parts of the writing are strong, effectively building a case for reading this paper. 4.Nit: the text seems to switch between "specialist" and "neural module". Are these the same thing? The paper does seem to be a unification of existing ideas, but I distinguish that from (and prefer it over) merely concatenating existing ideas. Interesting idea with good scoping 3. From my understanding, Triangles, CATER, and Sort of CLEVR all tell us that the higher order communication and single channel will help identify relevant information earlier in the pairwise communication, leading to faster convergence   *if* information is sparse in the input.<|endoftext|>Paper proposes a novel mechanism for information exchange between different neural subnets. It replaces the pairwise interactions with a share memory space. The memory is updated by a competitive scheme which the top k updates are selected via the key query valueattention mechanism. The shared memory/workspace then broadcast the updated state to all other upstream specialist subnets. Paper claims 3 advantages of this approach: 1. Higher order (HO) interaction among neural modules2. Finally, the approach was also experimented on model  free RL on ATARI game task. The generic approach cuts across different AI discipline as the experiments are performed on vision, language and reasoning tasks. It s the key contribution on the success of the proposed approach. While I am not entirely familiar with the cited prior work on key query value attention mechanism which forms the backbone of the proposed approach, the entire paper is very well written and comprehensible.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; In this paper Range Net is proposed for calculating the right singular vectors and singular values, which produces nrealy accurate results with less memory cost than existing SkectchSVD. However, no time analysis or real runtime is listed in this paper, which is an important shortcoming of this paper. Strengths:	RangeNet just costs O(nr+r^2) space to calculate truncated SVD, which is memory optimal. Besides, RangeNet produces the nearly accurate results with no more than 5 passes on matrix, which is pass efficient. 2.There is no dataset shown which can be handled by RangeNet but cannot be handled by classical algorithms for truncated SVD. Experiments on large dataset will make this work more solid. 3.It is not fair to compare RangeNet with SketchSVD, RangeNet just produces the right singular vectors while SketchSVD produces both left and right singular vectors. Besides, the experiments in paper just compare them on matrix with size m >> n.Some recent work on randomized truncated SVD should be compared, like:Efficient randomized algorithms for the fixed precision low rank matrix approximationW Yu, Y Gu, Y Li   SIAM Journal on Matrix Analysis and Applications, 2018   SIAMFast randomized PCA for sparse dataX Feng, Y Xie, M Song, W Yu…   Asian conference on …, 2018   proceedings.mlr.pressWithout runtime listed in this paper, although this algorithm is memory optimal, it is not good enough. Besides, using linear optimizer to solve the truncated SVD seems not a novel idea.<|endoftext|>Given an input matrix X in R^{mxn} the algorithm identifies two matrices V* in R^{nxr} and H in R^{rxr}. V* has orthonormal columns that span the top r right singular vectors. H rotates V* so that V*H   Vr, where Vr in R^{nxr} is the matrix of the top r right singular vectors. The algorithm is accurate and uses very little storage. The way this reduction is achieved is by allowing multiple streaming passes over the rows of X, whereas the competing algorithms only allow 1 pass. That s because many algorithms, e.g.linear/logistic regression, kernel methods with the Euclidean kernel, and many neural network architectures, will be agnostic to the rotation H.One small theoretical disadvantage is that the number of passes required is not known. The authors state at most 5 passes are necessary, but actually that number was determined empirically and your number of passes depends on the number of epochs to convergence in stage 1. Table 1 states that Range Net space complexity is r*(n+r), however Appendix E.3 states that the implementation dumps XV*, an intermediate mxr matrix, to disk. For Stage 2, this low rank approximation in the secondary memory is streamed as input, and the extracted singular values and vectors are saved in main memory." so the authors  implementation actually requires r*(m+n) storage." I think it is misleading that the implementation does not obey the claimed memory bound. It is true that the algorithm can be implemented without dumping XV*, but it requires more compute and passes to recalculate XV* during stage 2 and still more to compute the actual singular values. How this extra compute affect the time required for the approximation? In terms of writing, I think that the paper is poorly organized. The main drawback is that there is no clear and concise description of the algorithm. Other notes:. Please formally define tail energy. I didn t understand this sentence at all and Equation (1) does not include any decomposition. The network is two layers that each consist of a single matrix multiplication,  it s two single matrix multiplications which are solved for independently.<|endoftext|>This paper proposes a neural network architecture for computing a rank r approximation of a matrix. The authors claim that the proposed method is extremely memory efficient compared to previous approaches. The proposed algorithm is intuitive and simple   but I think the experimental sections could be augmented with more results. Proposes an interesting algorithm for solving a problem widely applicable in machine learning. It seems the trained neural network is specifically tied to a particular dataset. Can a trained neural network be reused for another dataset of the same size? There are some lingering questions on experiments. Why does range net require more memory than sketchy SVD on dimension reduction (Table 3)? How much is the deviation in the computed singular vectors from orthogonality? I also did not see an appendix in the submission which I wanted to glance at. Perhaps adding more dimensions in terms of algorithm comparison (one more algorithm in addition to sketch SVD) / adding more datasets could help. In addition, the authors could clarify how trained neural network can be adapted to not just one dataset but a dataset that may be related (time series).<|endoftext|>The paper presents a streaming method to compute an approximate Singular Value Decomposition, without requiring to load the entire data matrix into the main memory. Also, the robustness of the underlying methodology w.r.t.the exact choice of optimization framework is also not clear (e.g., AdaMax or some other variant). There is very limited discussion motivating the target use cases; it d be helpful if the authors could add more detail in terms of ensuring the usefulness of having those low rank approximations computed is clear to the reader (e.g., Sandy Big Data use case). The authors could add more detail on why only SketchySVD was used as a baseline; also I believe the following work is a fundamental one in the area, so might worth citing that for the reader to get a more complete view of the topic [not my paper]:Liberty, Edo. "Simple and deterministic matrix sketching." 2013.Not clear if the Range Net being "fully interpretable" is well justified; I am not sure of the practical significance of this statement. Additional Comments:  The improvement in terms of memory requirements should be highlighted more in the description of the results (e.g., Section 4.3, where SketchySVD requires 24.91GB when only 4.19MB are required by RangeNet). "It is well known that natural data matrices have a decaying spectrum wherein saving the data in memory in its original form is either redundant or not required from an application point of view." Strong results in terms of memory efficiency, which do justify consideration for acceptance in my view; however, the paper could improve in terms of clarity of exposing experiment setup details and choices of hyper parameters, description of the main methodology, and having stronger application use cases.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; In particular, I am not sure if the authors are simply trying to create a state of the art fold recognition engine (in which case the results in Table 2 suggest that they have failed) or simply demonstrate that their contrastive learning approach is beneficial. This paper focuses on the general problem of learning a compact embedding of a protein based on its 3D structure. But it doesn t: it learns only from 3D sturctural information. The paper needs to be more careful about defining terms before using them.<|endoftext|>The paper proposes an unsupervised method for learning low dimensional representation of proteins borrowing ideas from contrastive learning in the computer vision literature. 2) The novelty of the method is also limited; the contrastive learning is directly applied to the protein problem and several details of the method including sub structure sampling and protein encoder are borrowed from Hermosilla et al.and Ingraham et al.questions:1) How sensitive are the results of the embedding to the choice of p?<|endoftext|>This paper studies unsupervised contrastive learning on protein structures, using sub structure sampling as the data transformation strategy in contrastive learning. The protein structure representation from contrastive learning is then evaluated for three tasks: fold classification, enzyme classification, and protein similarity. Representation learning for 3D protein structures is a broadly relevant problem. While the evaluation tasks (enzyme classification and fold classification) are clearly defined, they are not particularly well motivated and it would strengthen the paper to further explain the biological/clinical significance of these tasks. Interesting approach.<|endoftext|>This paper presents an unsupervised deep learning approach for learning representations of 3D protein structures. It s not clear how many features and what all is done to the input. Their approach of using contrastive learning seems ill fitted to learn precise representations of protein structures. So protein representation is also 16 dimensional? They consider sub chains of proteins as examples. 9.What are the strengths of the 10. How is it being used for both the tasks?
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; The goal of this paper is to learn world models   where the network must predict actions from images   in an efficient manner. This is done by exploiting symmetries in the data. An interesting idea but a difficult paper to read and understand. Figure 2 is helpful, but I could not understand what is the purpose of the symmetric embedding network S as opposed to the encoder E?<|endoftext|>This paper proposes to learn an equivariant world model without access to group representation on input space $\rho_S$. The problem of learning the equivariant model without assuming any group representation on input space is important and motivated well in the paper. it is not obvious to me why study/build on this model? al 2020 is mentioned here for the first time. The authors are quite transparent about the lack of theoretical novelty when they state they are not proposing a new equivariant neural network design and instead, they build upon previous work to demonstrate working in new domains with unspecified group actions.<|endoftext|>The paper proposes to learn latent representation and transition models that are equivariant to symmetry transformations of states and actions. The goal of the paper in using equivariance in latent space models in RL is interesting and motivated. However, I have questions and concerns about the claims of contribution, the basic setup, theoretical presentation, and experimental results of this paper. "Learning group structure and disentangled representations of dynamical environments." The theoretical setup used in the paper is therefore inadequate for symmetric environments. Also, the motivation for having an equivariant encoder "after" a non equivariant one is not clear, since the embedding will not be equivariant.<|endoftext|>This article assumes knowledge of an underlying group to learn symmetric latent representations, by utilizing equivariant transition models. There is also a rather a strong limitation of their assumption that for the task at hand the relevant groups are known. The authors are indeed grappling with some interesting and very relevant questions, but separating the theory from what is engineered is not so easy (for this reviewer at least). I found this paper incredibly dense in parts, and the presentation quite confusing.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; This is not satisfying especially considering the linearity of the problem. The paper proposed a neural network based method for solving linear PDEs. Unfortunately, I do not think the approach is novel and the results are significant.<|endoftext|>The necessity of inputing the fundamental solution is a big drawback. What happens for source functions outside this set? Discuss.What is the significance of the comparison between PINN and BINet?<|endoftext|>However, from my perspective, the contribution of this paper is incremental and may not be sufficiently novel for ICLR. Here is a list of strength and weaknesses.<|endoftext|>This is the wrong venue for this paper. There are already decades worth of methods for solving PDEs. Strengths: the paper studies linear PDEs and compares a couple methods.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors propose a dynamic sparse pretraining method to reduce the computational cost of BERT pretraining. To maintain the computational regularization, block sparsity is used in this work. The main technique of this paper is to add group LASSO regularization to induced sparsity and prune the network depending on the magnitude of the weights. However, the novelty of this paper is limited because the proposed sparse pruning technique has been widely used to train convolutional neural networks. Moreover, it remains unclear whether the proposed method is tailored specifically for pre training of sequence models. My additional concerns are listed below. However, the proposed method is not compare with these prior works in the paper. 3) Finetuning results: The authors only show the MLM loss after pre training as the quality metric. However, it is also important to show the performance of models after fine tuning on GLUE / SQuAD datasets.<|endoftext|>Overall, this paper proposed a dynamic sparse pre training method for BERT language modeling which showed better performance in the number of FLOPs when compared to statically sparse and dense models across a large scale of network sizes. Strengths:The motivation of using sparse training for BERT language modeling is clear and persuasive. Extensive experiments and technique details are provided to support claims and demonstrate the effectiveness of the proposed method. Weaknesses:It will be better if the author provides more details on the difference between zero and untrained in computation or the time and space complexity analysis of these two methods. The author mentioned in the part of the contribution that the structured DynSparse training of BERT without structured regularization gives performance gains compared to dense BERT baseline. Structured regularization is used in the block sparse DynSparse algorithm. Are these two in contradict with each other? It will be better if the author gives more description of the techniques in the contribution in the part of the methodology. It will help if the author provides more details about how the activations are reduced and how many are reduced compared to the dense baseline. However, I am not fully persuaded by the methodology. More details like complexity analysis and latency reduction experiments could be given.<|endoftext|>The paper studies the feasibility to use the dynamic sparsity (DynSparse) method of continuously pruning and re allocating network weights during training for large unsupervised (self supervised) language models, namely BERT. However, the presented paper does not provide much novelty or method advancement. More importantly, no generalized “lessons learned” from these studies are drawn, and the translation towards other network architectures is not discussed. All results are reported in terms of reduced FLOPS, but actual training time (wall clock time) is not considered. No details on the utilized hardware and training setting are givenMy main concern with this paper is that it is merely a parameter study of an existing method being applied to an existing architecture. I do not see this to be the case here.<|endoftext|>This paper advocates a straightforward, dynamic sparse (DynSparse) pre training approach for BERT for more efficient FLOP utilization while maintaining comparable accuracy. This work also proposes a block sparse DynSparse scheme for higher effective utilization of FLOPS on real hardware accelerators (like GPUs and IPUs). * This work suggests minimum efficiency requirements for a time to train win for DynSparse over the conventional dense training. **Concerns/Questions*** This work in its current form lacks any quantitative comparison against other DynSparse work. The authors argue for random re allocation as a means to increase DOF. However, RigL [Evci et al.2019] suggests that their gradient based re allocation outperforms SET, which also adopts random re allocation, for both vision and NLP tasks (with RNNs, though). * There is no evaluation of end to end time ("time to train"). Depending on the sparsity, the model may fall short of the minimum efficiency requirement of block sparse training to outperform dense training for BERT. Can the proposed technique be effective to other models than BERT? The technique seems quite generic and I am wondering how effective it would be to other models. Is there anything specific to BERT?<|endoftext|>The main contributions of this work are several empirical observations:1. Weakness  The novelty of this work is a bit limited. There is no new algorithm proposed to get better results for this sparse training of BERT. The discussions on what could be the reasons behind the design choice are not very convincing to me. Maybe I haven t done a good review of this field, but I am skeptical that MLM validation loss is a good indicator of the quality of pre trained BERT. It seems a overclaim to me as the paper didn t show the comparison with BERT large. It would be helpful to label the point of BERT large and its DynSparse counterpart in Figure 3. This seems to conflictwith the findings in this work that network topology has to be updated sufficientlyto get good results. The empirical results could be more thoroughly conducted, e.g., the scale of BERT, downstream task evaluation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper uses computation redistribution and sample redistribution for searching the best architecture based on TinaFace. 3.Also, this paper is basically a kind of Neural Architecture Search, and would like to see a comparison with at least evolutionary method.<|endoftext|>+ The proposed method is effective especially for detection of small faces and the experimental results support this (the accuracy improves largely on WIDER FACE Hard which includes many small faces).<|endoftext|>This paper presents a face detection method that aims to deal with two challenges in unconstrained face detection: high computation cost and detecting small faces. •	The proposed computation redistribution method is general and can be adopted by other application. •	The proposed method yields the detection performance comparable to the SOTA face with lower computation cost.<|endoftext|>* The methods of searching the network structure consider each component to achieve both performance and latency. * Comparison with other SOTA network search methods is inadequate but necessary.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper introduces FedLite, a compression method built on top of Split Learning (SL) for better communication efficiency. FedLite tackles that issue via compressing the intermediate output via a vector quantization based method. The motivation of the proposed vector quantization method is also not clear. Convergence analysis and experimental results are provided to justify the effectiveness of FedLite. 3.The detailed hyper parameters and training schedules are provided, which gives good evidence of the reproducibility of the experiments. How does FedLite look like compared against that vanilla baseline? 5.One potential approach to reduce the required number of communication rounds for FedSGD like methods is to increase the batch size.<|endoftext|>The authors propose a method to compress the intermediate activations of a neural network that is split across a client and server in the Federated Learning setting. At the last paragraph of 5.1, the authors compare against FedAvg, which assumes a certain number of local epochs E (unless the authors compare against FedSGD). Introduction & Motivation:  While Split Learning allows to  outsource  the final layers  heavy computations to a server, this inherently reduces the privacy compared to FedAvg based approaches that communicate weights only. What is the assumption here? Background/RW:  The authors mention model parallel training in some places in their paper. Either the authors include a proper analysis of Model Parallel Training or they should remove the discussion around it. If the authors address my concerns I will improve my rating of the paper. Naturally the split learning approach is not constrained to uniform grids (as would be for inference acceleration on e.g.in8 inputs), so you might even consider quantising a transformation of the activations with e.g.the Normal CDF (assuming activations are normally distributed). Computational complexity: Can you elaborate on how expensive it is to compute the quantised activations for a mini batch of data?<|endoftext|>The authors propose end to end training framework that relies on a novel vector quantization. Providing **Pros**:  The paper is well written and structured. Interesting and novel idea aim to reduce the communication cost. **Cons**:  The authors should cite additional previous works that tried to address the problem of client s device heterogeneity [1,2,3]. My main concern is that under supervised learning setup FedLite sends the labels to the server to obtain $\nabla_{z_i}h\(w_s;z_i\)$. Sending the labels to the main server compromises privacy (for example in healthcare systems). I think the authors should add more challenging experiment to check the trade off between compression and accuracy. (Celeb a from LEAF dataset for example). "Personalized Federated Learning for Heterogeneous Clients with Clustered Knowledge Transfer." Questions were raised during the review process.<|endoftext|>In split federated learning, the classification layer that has a larger parameter set is saved and learned in the server, while the client learns the relatively lightweight feature layers. In FedLite, the activation outputs of the cut layer are compressed by vector quantization before sending to the coordinate server. However, there are other FL approaches towards efficient parameter transmission or computation cost. * The experiment section is not extensive enough, as only one splitFed baseline is compared. This paper is a solid extension of the Split Federated Learning work, which improves transmission efficiency with an acceptable performance drop.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; How effective are pre trained language models (PLMs) in zero shot settings across diverse prompting strategies and tasks? Unfortunately, this field is fast moving and there are both other works that need to be cited (e.g., https://arxiv.org/pdf/2105.11447.pdf) and other works that have since come out that undercut the value of this work (e.g., https://arxiv.org/abs/2110.08207). I don t expect you to have profiled the latter   I know it came out after the ICLR submission deadline. The direction that seems like it has the most potential of improving future research is the finding on multiple [MASK] tokens and the more general idea of ensembling different prompts / label words together. They perform several targeted analyses to better probe some of their findings (such as delving more deeply into the multi null strategy) and also offer commentary on several overall takeaways. However, as the authors note, this strategy overall is not a new strategy; [Logan et al.](https://arxiv.org/abs/2106.13353) explore it previously, so the impact of this contribution is limited. That said, in what I think is a new finding, it can be further improved by using multiple mask tokens at different prositions in the sentence and ensembling them together. Unfortunately, as it stands while I think the work is well exectued, I question the impact it is likely to have on future research.<|endoftext|>The paper analyzes these variants of NULL prompting to establish principles behind design choices. 2.Exploration of the limitations of the zero shot capabilities of BERT family models. The breadth of the contributions of the paper are questionable if you account for the fact that zero shot has been explored as a baseline in [2] (Line 1, 2 of Table 1)  and the efficacy of NULL prompting has been pointed out in [3]. I suspect that the other methods would benefit from this too. Manual prompts with multiple mask tokens should be explored. How were the tokens in Table 3 that replaced [MASK] chosen ? If this is an individual task, do the trends hold across other tasks ?<|endoftext|>The paper has a full span of empirical results on BERT models trained by both methods of fine tuning and prompt. The study was on zero shot learning task, and the authors proposed two new methods   multiple null prompts and the search for label words. Please let me know   suppose the authors know such results. The result would clearly worth publishing to researchers working on prompt BERT model. The paper may be around the borderline for acceptance of the conference unfortunately, from the novelty concern that has been talked before.<|endoftext|>They propose a different prompting method, Multi Null Prompt, which is a simple method where multiple MASK tokens are inserted at random locations in the prompt and the outputs with respect to the multiple MASK tokens are aggregated. The paper also tries GLUE tasks and shows that zero shot performance on these tasks is comparatively worse. In Table 1, it seems that only some of the results have error bars? Overall, the results show that other words may be worse or similar to using MASK, so we may as well use MASK. Is there a proposal for how to deal with this limitation? It seems the other way around. The paper provides an empirical study of BERT models for zero shot learning with interesting results, including two simple proposed methods that improve performance. The motivation for the multi null prompt method isn t very clear, and some of the experiments were not completely motivated, but overall it seems like a good empirical study.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; Strengths:+ The proposed idea is simple and easy to understand. Weaknesses:  The paper concludes that the proposed method can significantly improve the out of distribution performance of ViTs. Regarding this, the contribution of the contrastive loss is not clearly ablated in the proposed method. This is hard to be understood as "insensitive". However, I think the statement of "ViTs are insensitive to patch transformations" does not hold, and the contribution of the contrastive loss is not clear.<|endoftext|>This paper draws motivations from the observations that ViTs are insensitive towards patch based transformations. In order to push ViTs from these undesired features, patch based negative augmentation and losses are proposed, which consistently improve the robustness of ViTs. As has conveyed in the paper, the proposed negative augmentation is complementary to positive augmentations. The transformer architecture used across most experiments is ViT B/16, which uses a $16 \times 16$ token input. Will the transformer architectures that use a more fine grained token representations, such as $8 \times 8$, produce dissimilar results?<|endoftext|>1.The exploration is novel: ViTs heavily use features that survived patch based transformations but are generally not indicative of the semantic class to humans. 2.The experiments can support this paper’s claim and the designed patch based negative augmentation is effective. 1.This paper only conducts experiments on the ViT while there are other SOTA vision transformer structures, e.g.swin transformer [a].<|endoftext|>This paper empirically investigates robustness properties of vision transformers (ViT) towards image augmentation strategies that destroy the semantic meaning of the image. 5.Page 7: (...) and Steiner et al.(2021) have the similar observation  > have “made” similar observations6. 4.Expanding on the previous point, the authors test negative augmentation in combination with positive augmentation and with larger datasets, both which may already solve the problem on their own but are shown to still improve in the presence of negative augmentation. Experiments of Fig 2: This experiment is performed to demonstrate the robustness of ViTs towards patch based transformations. I believe that repeating these experiments with a standard non transformer CNN whose performance should degrade much faster would strengthen the argument being made for Fig 2. The resulting ViTs perform clearly worse and the authors use this as empirical evidence to support their hypothesis that the learned features are not generalizable. However, a degradation of performance is to be expected as the model has observed only P corrupted images.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; The paper proposes a method to craft stronger and more efficient attacks on state observation of the RL agent. "Towards optimal attacks on reinforcement learning policies." Adversarial (deep?)<|endoftext|>The paper studies evasion attacks in deep reinforcement learning (RL). Is this supposed to be $\sum_i d_i   1$? Overall, I enjoyed reading the paper, and in my opinion, this paper contains interesting results and provides solid contributions to the line of work on adversarial attacks.<|endoftext|>I find the paper very well written and easy to follow. The details of solving optimization problem (G) could be more explained in the main text.<|endoftext|>The claims made in the paper are likely unjustifiably strong. The definition of admissible perturbations relies on that the attacks would be hard to be perceived.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; They argue that this issue is due to using a single point estimator as a discriminator and that capturing the epistemic uncertainty of the discriminator could serve as an additional signal to guide exploration. They achieve this by using an ensemble of discriminators and incorporating the epistemic uncertainty across the ensemble into an additional intrinsic reward to the diversity of skills reward (through a mixing parameter $\lambda$). A simple and intuitive approach is proposed to fix the issue. The problem and the solution method are both well motivated. It is unclear how the authors have realized the sufficiency of not using bootstrapping for training the ensemble (i.e.they use the same mini batches across the ensemble)   see my Question 1. Also, curiosity has been used to acquire skills by snapshotting (Ref.[1]).While I agree that in some senses the statement in the paper is reasonable, I think this general statement confuses more than it informs.<|endoftext|>This paper is concerned with unsupervised RL where an extrinsic reward signal is not available. To alleviate this, the authors propose a new auxiliary objective, which results in a bonus based on the disagreement of an ensemble of discriminators. **Strengths**The paper is well written and I enjoyed reading it. The authors clearly explain the discriminator based skill discovery via intrinsic rewards given in (3) and why it results in a pessimistic agent. The new exploration method is simple and practical and is shown empirically to improve skill discovery.<|endoftext|>No experiments are presented on continuous control tasks such as MuJoCo locomotion environments, which would have strengthened the paper further. The paper proposes a novel reward bonus that works in addition to a base method such as DIAYN (DIversity is All You Need), such that this bonus “reimburses” the policy for visiting states where the discriminator uncertainty is high (measured using a form of disagreement across an ensemble of discriminators). The paper is well motivated.<|endoftext|>To solve the problem that the discriminator will have low confidence in the unseen data thus providing a low intrinsic reward, the paper derives an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Clarity: The paper is well written and clear in the flow. 3.The experiments are pretty solid including various settings environments including tabular form and image based tasks (Atari).
Accept (Poster); rating score: 6; rating score: 6; rating score: 10; The paper addresses the problem in RL of reusing policies resulting from previously learned (different) tasks in order to perform well on downstream tasks via generalized policy improvement. I have not found problems in the notation used or in the mathematical formulations. However, there are a couple of aspects that make me doubt about the merit of this contribution:It is a clearly incremental work built on the work of Barreto et al.The novelty of the paper and the contributions offered are limited to answering a series of questions (theoretically and experimentally) about the required conditions and the reliability/guarantee provided by the approximations using SFs & GPI.<|endoftext|>The paper extends the successor features framework to answer the following question: which policies should we learn and store so that, when presented with a new task, we achieve the best performance possible? One important point to clarify is the setting   the "unsupervised RL" setting is a bit ambiguous and could lead to confusion, since it could mean the reward free RL (see Jin et al 2020) and is separate from the lifelong setting. This way, the library can be grown in an informed manner, instead of being provided to the agent directly. One of the most important aspects of their work is that they have motivating experiments as to why you would only want to store certain policies.<|endoftext|>The paper focuses on reinforcement learning problems with known successor features and rewards expressible as their linear combination. Experimentally, the authors verify the theory and compare to existing approaches to create policy sets, outperforming all. Finally, they perform experiments in problems without the linear combination assumption and lifelong RL setting, with positive results. This is a strong paper, methodically and clearly presenting its concepts. It is well written, interesting to read, presenting theory before supporting it with experiments. The experiments beyond the original assumptions show that the new algorithm is valuable and interesting for general RL community   i.e., the last two experiments (reward cannot be expressed as linear combination of features / lifelong RL) show that it outperforms DQN. Typos:p. 3   it leaveS open the questionp. 3   clear by the end OF this sectionp. 3   the problem weE want to tacklep. 7   tasks that do noT satisfyp. 7   test whether IF learningWell written and interesting paper with strong theory and results, well conducted experiments.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The premise of the paper is that example train instances with the corresponding label can be an effective explanation of the model s prediction rather than post hoc explanation which is not deterministic. W1 Q1   If the set of prototypes pre defined, do we really need to train the prototype? More details are needed especially on experimental settings, and methods. Strengths  S1   The idea of choosing a similar example from the train data as an explanation is an interesting idea.<|endoftext|>And this work could benefit the explanation community. Experiment results show that adding the proposed ProtoTrex benefit task performances on 3 sentiment classification tasks. When it comes performance of explanation, I only see confusing numbers, thus no conclusion can be made. The nice thing about this paper is that it focuses on NLP tasks, so the framework could potentially benefit the explanation community. However a couple confusing points still. It seems this paper uses prototype embedding to find a training example as it nearest neighbor, and then use this data point as explanation to its prediction. Tab2 indeed shows some examples with explanation that partially depends on the input example. This paper very briefly went over some statements without getting into details.<|endoftext|>This paper proposes Proto Trex model to increase the interpretability of the text classification systems. The proposed model mainly adds a bunch of prototype layers to learn the similarity between the query and prototypes. 2.Through several example cases, the proposed model could generate reasonable explanations. The training loss of the model seems to be too complicated. The proposed model is reasonable, and the results seem to be OK.<|endoftext|>The paper also shows that end users of the system can interact with it by either editing prototypes (if they are experts) or providing weak feedback about which prototypes are good, and this feedback can be integrated to re learn better prototypes. Based on sufficiency and comprehensiveness scores, we see some evidence that the explanations from prototypes are faithful, though it is unclear if these scores are competitive. It is well written, with a comprehensive set of experiments. How hard was tuning hyperparameters? Results:        different models respond differently to the proposed approach. Could the authors comment more about this? However yelp also has examples with “bad service, good food”, “good food, bad service”, examples not related to restaurants at all but the prototypes do not uncover these.
Reject; rating score: 3; rating score: 5; rating score: 5; The paper proposes an extension to ReQueST, which learn learns a neural simulator of the environment from safe human trajectories and then learns a reward model from human feedback. The paper also discusses the amount of training data that is needed for ReQueST to work in such a setting. Their results show that the application of ReQuest results in "3 to 20" times less constraint violations compared to a non safe baseline. I generally like the idea of the paper to bring ReQueST together with human feedback through reward sketches. The experimental setup is also well defined and described. The authors clearly state their contributions and delineate it from previous work. The use of real human data and reward sketches is laudable, and an important contribution. Experiments are replicated 10 times, which is a sensible amount. Additionally benchmarking ReQueST on 3D environments is valuable but not a contribution that is sufficient for publication in ICLR. The authors should identify the problems encountered by ReQuest and their modifications here and propose a solution that improves the performance to be better than the baseline. Some further points: 	In Fig.2 you could more explicitly show the difference to original ReQueST 	While the authors use a large amount of replications, they only include a classical RL baseline. This baseline uses a simplistic, sparse reward, rather then the richer reward sketches used by ReQuest. The paper should also compare itself against well known SafeRL algos and/or against classifier based approaches (such as the original version, why don’t you compare to vanilla ReQueST in order to show the benefits?). I generally like the idea of the original paper (and hence also this paper) but in my opinion this paper is not enough for publication in ICLR.<|endoftext|>This paper is an extension of the previous work ReQueSt, with the focus on learning safe policy from human demonstrations only, without simulators or specifications. There are some concerns about the approach. It has a very nice demonstration in challenging pixel based 3D tasks. The dynamic model is learned from data. * Reward learned from binary human feedback that is generated by a procedural reward function. 2.This method requires a lot of human data, which is ok considering the challenge of the task. One concern is that it seems unclear how to determine when the training for dynamics (and reward) are "completed." 3.A programmatic reward bonus is applied to the proposed method, not baselines. Would this mean that the reward learning component in this method is insufficient to recover the reward function? This is the main reason why in the results, model free RL performs much worse than the proposed method. However meanwhile, this might also indicate that the proposed method cannot completely outperform model free RL when the reward is specified well, such as in the cliff env.<|endoftext|>This paper shows that  ReQueST can be used to train an agent in a 3D environment with an order of magnitude reduction in instances of unsafe behaviour than typically required with reinforcement learning. The paper shows that ReQueST is plausibly a general purpose solution to  the safe exploration problem: where safe behaviour is learned from humans, rather than given by a procedural function, and the simulator can be learned from data. The paper also does not do a detailed comparison with existing approaches. If no existing approaches can achieve what the paper proposed to do, then please clearly say so. Otherwise please do a comparison to strengthen the  claims of novelty of the paper. Overall it is an interesting paper with some good results shown. I am not sure the results are sufficient enough. I would also like to see more detailed comparisons with the state of the art.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper considers the model selection problem in offline RL and proposes a new procedure called pessimistic model selection (PMS) that estimates performance lower bound for each candidate policy. Theoretical results show that under certain assumptions PMS can identify the best candidate policy with high probability. There seems to be a disconnect between this motivation and the content.<|endoftext|>This submission presented a pessimistic model selection approach (PMS) for offline deep reinforcement learning (ORL) and tested the approach with performance comparison on six simulated environments. Under several specified conditions, the authors provided asymptotic analysis to show that PMS can lead to the best model with respect the worst performance of the derived policy. 4.The presentation needs to be improved. Also by simply looking at the tables in the appendix, it is not really clear how the initial models were selected in different experiments.<|endoftext|>This paper proposed a new algorithm for model section in offline RL based on adding the pessimism principle on top of existing OPE algorithms. Under strong assumptions, the paper provides an asymptotic convergence result for the proposed algorithm. It also provides empirical experiments demonstrating that the proposed algorithm outperforms existing baselines. The theoretical part is weak, with many strong assumptions being made, but only providing asymptotic convergence results. 2.In related works, when discussing other works on model selection in offline RL, please elaborate on their techniques and contrast with the current paper. 5.Section 4.1, I think you mean "an estimate of $Q^{\hat\pi_\ell}$ as $\hat Q_\ell$", since there is no assumption that any of the $\pi_\ell$ is close to the optimal policy. Does it mean candidate policies returned by L different algorithms?<|endoftext|>The paper proposes a novel model selection approach to automate ORL development process and to identify a well performed model given offline data. The proposed pessimistic model selection (PMS) method leverages an uncertainty quantification on value functions and a pessimistic idea. https://arxiv.org/abs/2006.10460   there is room for improvement in the experimental section      regret is defined in an additive manner, and Figure 3 contains results across different environments. It is also essential to clarify the contributions of the paper compared to [a] and [b]. Thus, I would recommend weak reject at this moment.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; In this work, the authors propose a new feature learning limit for infinite width neural networks that resolves some of the computational issues with the previous $\mu$ limit. The solution is technically sound, and works well empirically. I would be very happy to see this paper accepted.<|endoftext|>The performance gap with respect to NTK on the CIFAR10 network is only 2 %, making the improvement brought by feature learning and by taking the explicit infinite width limit difficult to appreciate. The formal development presented in the paper is interesting and insightful.<|endoftext|>This makes me wonder whether the studied model bears any resemblance to "real" neural networks. The authors study a certain variant of an MLP trained using a projected gradient descent inspired update rule in the infinite width limit. The authors evaluate their model on Omiglot and CIFAR10 against NTK and finite width baselines.<|endoftext|>I however find that the paper incomplete in several crucial aspects. This is indeed the case in the experiments. Through the rebuttal, some central claims and contributions are clearer to me. The fact that it allows for computation of the infinite width limit, unlike $\mu$ parameterization, while still achieving some nontrivial performance, is valuable in my opinion.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; Inspired by the observation, the paper seeks to find sub clusters within a close neighborhood of each training sample. While being pleased with the motivations, I have several concerns regarding to the details for the proposed approach. The paper uses a memory queue of 65536 samples in 1000 classes. The paper uses a value of K 100 for k nearest neighbors, I wondering how many of the 65.5 samples fall in and out of the k 100 nearest neighbors. Also, It would be also valuable to include the results on detection/segmentation transfer on VOC and MSCOCO. How is the performance when compared with contrastive or supervised baselines? I am concerned about what actually sub clusters are emerged from the LOOK model. Post rebuttal  I am satisfied by the additional analysis provided by the authors.<|endoftext|>This paper proposes LOOK, a new supervised pre training method which can maintain intra class semantic differences for better transferability to downstream tasks. This paper is well organized and easy to understand. 2.The idea of using a stronger classifier (eg.KNN classifier) for preserving intra class semantic differences is reasonable and shows impressive performance. 3.The experiments and analysis are sufficient. Current experiments only present results on downstream classification tasks. It’s not clear how the proposed method will perform on other tasks like detection or segmentation. It would be better to have more analysis on this issue so that the advantages of the proposed method can be better illustrated. The experiments have sufficiently validated the effectiveness of the proposed method on downstream classification tasks.<|endoftext|>The paper presents an interesting approach that expands the recent success of self supervised pre training with *instance discrimination* to supervised pre training. The key insight is to have each class being represented not just by a single weight vector, but in a non parametric fashion via KNN lookup from a MoCo memory bank. The proposed approach is not only quite effective according to the paper s experiments, but also giving some insight about why the popular SSL methods based on instance discrimination has good transfer abilities. + As hinted in the summary, the biggest realization in the paper is to focus on such methods for *pre training*, and not just for training the current task. So it has some empirical novelty. "Improving generalization via scalable neighborhood component analysis." So I am wondering how important it is in the pipeline? Overall I am on the acceptance side of the paper.<|endoftext|>The paper proposes a leave one out kNN supervised method for pre training on large scale datasets. I found all numbers of baseline are lower than what s claimed in the original paper. I suspect the authors might not use the optimal hyperparameters. In my opinion, without reasonable baseline results, it is hard to conclude that one method is better than others. Last but not least, the paper uses a lot of ideas from recent self supervised papers, such as memory queue from MoCo and MLP from SimCLR, and adds kNN loss on top of that. However, based on 1), I cannot say the proposed loss is actually better. See in the Main Review  Post rebuttalOverall, I am still on the fence about the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper presents a theoretical analysis of the RKHS of the NTK of two layer polynomial neural networks without bias terms or nonlinear activations. The primary difference between these networks and standard ReLU networks is the presence of (elementwise) multiplicative skip connections, which the authors show (both in the NTK analysis and experiments) improves the convergence rate for higher frequency target functions. Ability to control the spectral bias of neural networks is an important and mostly open question, at least for networks that operate on high dimensional inputs. This paper offers a valuable step in this direction by showing that multiplicative connections reduce the low frequency bias of neural networks. The theoretical contribution is limited to a very restrictive model: 2 layers, no nonlinear activation function, no bias term, and many more data points than parameters. Further, the analysis is in the NTK regime, which assumes that width tends towards infinity and learning rate tends towards zero. 3.The related work section is very sparse. This should include prior work on spectral bias of ReLU networks (some of which is included in the introduction, but not all for example, https://arxiv.org/abs/1906.00425 and https://arxiv.org/abs/2003.04560). Claim 2 could be more specific. 6.Figure 2 is not very convincing actually I find Figure 11 in the appendix to be a more compelling comparison, both visually and in terms of the relevance of the comparison. The paper offers an interesting approach to reduce the spectral bias of neural networks by introducing multiplicative connections, and supports this claim with NTK analysis of shallow networks and experimental evidence on low dimensional synthetic and real data.<|endoftext|>The paper gives an analysis of polynomial networks in the NTK regime. They show a theoretical speed up in learning higher frequencies when using polynomial networks. They also have some experiments that verify these properties. The main theoretical contributions of this paper are Theorem 1 and 2. Theorem 1 gives the kernel limit of two layer $\Pi$ net. Given the techniques used in reference [9], the proof of Theorem 1 and 2 are quite strict forward. The empirical results are quite clear and well supports the main theorem. It s is quite interesting that $\mu_{k}   \Omega(k^{ d/2  2 })$ rather than $\mu_{k}   \Omega(k^{ d   1})$. However, given the previous paper the, proof techniques are not novel. Therefore, I would only suggest a weak acceptance.<|endoftext|>The paper is dedicated to the study of spectral bias in polynomial neural networks. For feed forward neural networks with ReLU activation function it is well known that learning high frequency terms is made slower (Rahaman et al.). Authors claim that polynomial networks are able to learn both low frequency and high frequency terms almost equally fast. Theory, based on spectral analysis of an integral operator associated with the NTK kernel, is provided. Theoretical approach was taken from arXiv: 1905.12173, but for NTK kernel of the network with an additional multiplicative interaction layer. Then, experiments that support claims are provided. It is claimed that multiplicative interactions in the architecture of a NN may be the reason for an ability to learn high frequencies. So, why can we apply the spectral analysis to Π Net architecture? Experiments seem to support the main claim, though fig 5 shows that the difference from standard network is not so sharp. Eventually, it seems that the standard network is also able to learn the high frequency. Also, experimental setups are taken from well known papers. I would assess that it is a borderline paper.<|endoftext|>I appreciate the authors for spending their time polishing the writing, providing clean theorems, and reviewing the literature. I also think that the problem presented here is meaningful and worth further studying. Personally One of the main concerns is that the paper studies only two layer PNNs, which is restrictive. Also, the analysis leverages an infinite width assumption, far from being practical. The experiments considered the performance of NNs with more than two layers. Though the previous theoretical analysis does not apply to this setting, more layers make the scenarios more practical. Finally, I suggest the authors provide a better discussion in the related work section. In particular, it would be good to add comparisons (theories & techniques & proofs) between the essential references and the submission, i.e., what makes the paper special? While it is common to start with simplified models for theoretical analysis, I expect a more profound theorem addressing broader PNN families given the existing work. I appreciate the authors for discussing the prior work and performing experimental comparisons.<|endoftext|>The empirical evidence supports this tendency. Strength: This paper provides an interesting viewpoint to explain why polynomial neural network performs better than feedforward neural networks in some vision tasks. It has two layers, which at best can express a quadratic function. While I understand that the spectral analysis in previous works is performed on two layer nets, the authors should at least explain in the paper why not analyze the full rank polynomial network and why the ReLU activations. Currently, I feel the theoretical results for the two layer model cannot really be applied to the PNN in practice. 2.Technical Novelty: I feel the technical novelty is lacking. Another drawback is that the main body does not introduce why larger eigenvalues imply faster learning. While the topic and story of this paper are interesting, it has several drawbacks that need to be addressed. The setting is very restricted, and the theoretical results lack novelty. Post rebuttal updates I raise the score from 3 to 5, given that the presentation of the updated version is more rigorous and clear.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This paper proposed an effective framework for the unsupervised RGB D saliency detection task. The proposed framework consists of two key components. Overall, the paper is well written and easy to understand. It seems the "backbone" is identical with the "backbone" of sec 4.3 Analysis of the empirical result. I recommend the author to add a description or point another description.<|endoftext|>5.Inside ATS, given ten epochs, what epoch use step one, and what epoch use step two? However, the difference separating with them is not discussed in the paper. However, I have a few questions listed below:1.<|endoftext|>The paper presents an unsupervised method for RGBD SOD. Based on the strengths and weaknesses listed above, I am slightly negative to the paper.
Reject; rating score: 5; rating score: 5; rating score: 6; This can be the case in the experimentation, since target domain does not consist of independent images, but images from the training set modified with weather change simulation effects. The work poses this problem as a domain adaptation task, from the original domain (original labeled data) to the target domain (data with appearance changes due to lighting or weather). Maybe authors can better discuss and explain the main concerns that hinder clarity on the novelty claims and the validation in the rebuttal.<|endoftext|>I encourage the authors to think about these problems. Weaknesses The paper considers an interesting problem (segmentation model robustness) yet unfortunately the composition would appear in a premature state making it difficult to fully grasp the size of the novel contributions. The current lack of such experimental validation leaves the submission somewhat lacking. Explicitly contrasting these previous works with novel insight towards the problem at hand can help to build a compelling story. As stated, the direction is valuable and the ideas somewhat interesting however I feel this submission is currently in a premature state. I encourage authors to work on some of the methodological, presentation suggestions.<|endoftext|>I would strongly recommend the authors address the limitations mentioned in Pt no 1,2 and 3. If possible, it will be better to discuss this observation in more detail. The segmentation loss is a weighted sum of the loss computed on the original features and perturbed features. In addition to the suggestions above, the following are some minor feedback:4)In this work, all the models were trained with the same learning rate and the same number of epochs.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper proposes a method for scaling the learning rate during training in order to encourage all parameters in a neural net to be fully used. This technique is shown to improve the performance of Transformer models on several different problems, even when used in combination with other adaptive learning rate methods (Adam, Adamax). Analysis experiments verify that the method has the intended effect: compared to the baseline, more parameters have higher sensitivity, and pruning is less effective. Strengths:  The idea is novel, straightforward, well motivated, and easy to implement. Potential for large impact on model training best practice. The paper is extremely clearly and carefully written. It would have been good to test the combination with dropout or something similar. The hyper parameters for SAGE are exhaustively tuned, more so than for the baseline adaptive optimizers, so there’s a potential for bias. Figure 7 counters this possibility, but only by showing heatmaps, so it s hard to gauge the actual numbers. Figure 2: Consider flipping these plots to the standard convention, and stacking them. A simple adaptive learning rate formula that seems to work really well, even on top of other optimizers, as demonstrated by very thorough experiments.<|endoftext|>The paper proposes the SAGE method. The idea is that neural networks have redundant parameters. Some approaches will prune these parameters which has the effect of not decreasing the performance but to decrease the number of parameters. The paper studies if it is possible to learn better these parameters in order to make them more useful for the network. The paper proposes a method that will allow to train differently the parameters of a network in order to have a better use of the weights. Strengths:  **Writing:** The paper is well written and easy to follow. **Tasks:** The paper evaluates its method with different optimisers and on different tasks in NLP and image classification. **Results:**  The results are quite good; the proposed method surpasses the baseline each time. Weakness:  **Architecture:**  The method is evaluated only with transformer architectures. **Optimisation:** In image classification, the pre training procedure used is quite sub optimal since the paper of Dosovitsky et al.[1] many improvements have been proposed such as the DeiT approach [2]. It would be interesting to see if the proposed method still works when the model is trained with more regularisation and data augmentation which may also lead to a better use of weights.<|endoftext|>The authors propose an adaptive learning rate schedule that specifically aims to eliminate redundant parameters. I think that overall the paper is good as it raises and studies an important point: redundancy in parameters is not an axiom we have to accept. However, after this introduction and motivation, it reads more like a typical optimizer paper introducing a new optimizer based on hand wavy intuitions. The EMA "I hat" is a full copy of the model since we need one EMA per parameter, especially for large models this is substantial overhead. 3) It is not clear to me in what scale the quantities U and I hat are, and hence, the multiplier to the learning rate. 6) I do not agree that one can conclude from the results that SAGE is more effective for small datasets than for large ones, since the "points" betwen the datasets/tasks do not live on the same scale. This should be more accurately reflected in the title, for example by replacing the word "training" by "fine tuning". It would further be interesting to see how well it works when training from scratch; even if it does not work, it is still a valuable fine tuning method, and stating this may save a lot of people a lot of resources and time. 9) Typo: "smoothier"  > "smoother"For an "optimizer" paper, it is weak: hand wavy motivation, no real derivation of update rule or even convergence proofs, and experiments only in one very specific domain: transfer of pre trained transformer models. I am not convinced that it will be generally useful at all. However, the paper raises a very important point: we should not take redundant parameters as a necessity, and it does propose a method to avoid them.<|endoftext|>This paper focuses on the parameter redundancy issue in large transformer architectures. To this end, it proposes an adaptive learning rate algorithm SAGE, which automatically scales the learning rate for each parameter based on its sensitivity. The paper is well written and easy to follow. The method section provides helpful intuitions behind the algorithm design. 3.Experiments use multiple benchmarks, including both language and vision, and results show noticeable performance improvements. Jointly using them can bring more gains. Table 1 shows that models with different percentages of redundant parameters have similar performance, implying that performance may not be proportional to "well trained" parameters. This seems not totally in line with the paper s motivation. 3.Sufficiently training all parameters seems a double edged sword. According to Figure 3, models trained with SAGE are susceptible to parameter pruning. This paper proposes SAGE, an adaptive learning rate schedule to train redundant parameters more sufficiently to improve model generalization. SAGE, together with existing adaptive optimizers, shows effectiveness in a wide range of downstream tasks.
Reject; rating score: 3; rating score: 5; rating score: 8; rating score: 8; Heterophily is an important factor for us to understand the performance of GNN on different graphs. I think the paper found a good point that heterophily is not always harmful for all GNNs. The filterbank based method does solve the problem to some extent and gained very good performance. However, 1. Adding that the theoretic analysis of diversification operation or new metric is also not convincing, the novelty of this paper is incremental.<|endoftext|>Several experiments have also been conducted to evaluate the proposed model. In general, the authors focus on an important problem and the paper is easy to follow. My first concern is that the heterophily analysis might be over simplified. However, the analysis conducted has two major assumptions: binary classification and one hop aggregation. The authors briefly introduce some potential implementation of HP/LP filters in Sec.4.2 while the exact implementation selected in the experiments is missing.<|endoftext|>This metric complements the current metrics by considering unharmful heterophily cases. (2) solid and comprehensive experiments to show the significance of the proposed ideas. Cons:(1) some necessary justifications are missing. The raw weights are estimated only based on the results from the corresponding filter. It would be better to put important and necessary discussions in the main text. This paper is well written and novel enough for this conference. The presentation can be improved and more justification can be added to further improve the paper.<|endoftext|>According to the experimental results, the proposed architecture is successful. The paper addresses a well known problem in the literature in a seemingly effective way. Then, it performs very comprehensive experiments that demonstrate the effectiveness of the proposed scheme. Yet, the authors make the best of the available space to cover all necessary information. The paper is well written and motivated and contains convincing experimentations. It could also have a big impact on GNN node classification practice and future research.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The authors show that the classic CGN architecture can perform well on certain label heterophilic graphs   which they argue is disputed heavily in the literature   and provide a theoretical discussion on when a good performance for CGN on such heterophilic graphs is possible. The paper aims to provide a more careful assessment about the role of label homophily for graph neural network. Overall I see the paper slightly above the line, provided some of the above points are addressed (I think this is possible). The authors have to be very careful here to not throw out the baby with the bathwater. There is merit in what the authors do: clarifying when CGNs can perform well (even under heterophily); however, that does not mean that there are no issues with heterophilic graphs in general. Basically, the authors consider a homophily definition at the level of node labels; they also assume that there is a tight correlation between the node labels and the underlying node features; so a label homophily would translate into a feature homophily (or heterophily) as well. However, extending the bipartite examples to multiple classes is already much more complicated: it may well happen that there is no clear label to feature correspondence any more that can be exploited. Finally, the numerical experiments paint a much richer picture than the theoretical analysis and indeed the introduction of the paper. As it stands the introduction and theory parts make the paper feel a bit one sides (to exaggerate: "heterophily is not a problem"); whereas in the end the conclusion is much more nuanced.<|endoftext|>This paper revisits the common belief in prior works, “GCNs require strong homophily assumption.” The paper claims that even under low homophily, if the nodes of each label have distinguishable neighbor label distributions, GCNs can learn distinguishable node representations. However, this paper has several flaws which should be fixed/justified before publication. First, the Table 1 results are not convincing. We now agree that GCNs can learn good representations under some conditions on neighbor label distributions. Or, how about simple other baselines such as GraphSAGE and GAT? I have read Appendix D.4, and it is a standard procedure with reasonable computational budgets. This can work just because predicting one can be reducible to predicting the other in a two class problem, might not because of the distinguishability. For extreme cases, how does the model perform if the node connects to a single different label, or all labels except for itself (with the different number of classes)? The authors discuss the interesting question (in the title) and their answers in theoretical and empirical ways. The paper has merits but also has the following weaknesses:  Table 1 and the following description cannot justify how GCNs outperform other models. The two class graph results seem to be insufficient for the main claim. Only one specific instance of the degree distribution $\mathcal{D}_c$ for edge addition is used for the experiments.<|endoftext|>The paper revisited the performance of GCN on graph with heterophily and provide negative evidence that heterophily does not always result in the poor performance of GCN, which contradicts with the assumptions/observations of many previous papers. They demonstrated that the GCN embeddings are still label distinguishable on a special type of graphs with assumptions that the nodes with same labels have the same node feature distribution as well as the same neighborhood label distributions. They theoretically analyzed the case of CSBM model with two classes, and also empirically investigated on synthetic/real world graphs with multiple classes. The conclusion is that GCNs can achieve good performance on heterophilous graphs under certain conditions. 3.The writing is generally clear. The analysis is only limited to GCNs, while the paper title is too general (GNNs). Could the authors explain more details about it? Same as the heterophily metric, I do not think it can completely decide the GCNs’ performance, because the node feature distribution is also important here. It is obviously not true. So, how to explain this exception? The paper provides a new perspective of heterophily and GCNs, but there remains some concerns both in the theoretic part and in the experiments.<|endoftext|>Strength:The authors provide some valuable arguments about the performance of GCN on heterophilous graphs and verify the claims with some empirical results. In the last line of page 2, where is $X$ in the equation? 3.Kenta’s work does not drop non linearity in their analysis. What is its relation with heterophily and homophily in definition 1? What is its advantages over other metrics? 7.The writing of this paper is not satisfying. I suggest the authors to cut the long paragraphs into smaller pieces, e.g.section 3.3.1, so that it is more reader friendly. 8.Can your theoretical analysis on CSBM generalize to multiple classes or to more general graphs. The novelty and significance are OK but the writing is not satisfactory. I ll consider raising my score if the authors can address my concerns properly.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; rating score: 5; This manuscript does not obey the blind review rule, and should be desk rejected. The code is available here https://github.com/Zehua Yu/TVF anomaly detection/commits?author Zehua Yuand the author is not anonymised.<|endoftext|>This manuscript should be desk rejected because it breaks the blind review rule. In the Introduction, the authors said, "the implementations of our framework without training part are accessible at https://github.com/Zehua Yu/TVF anomaly detection "; by following the link, we can identify the author of the code.<|endoftext|>This paper proposes a multi domain splitting framework for the time varying graph structures, in applications such as traffics in urban areas. The problem is interesting but the paper lacks of some important discussions, and has several limitations. The current presentation and discussion lacks of technical depth and insights. 2) The current solution is reasonable and easy to follow.<|endoftext|>The authors propose a multi domain graph splitting framework for detecting anomalies on dynamic graphs. 2.Some details of the proposed method are not clearly explained. Still, the designed method outperforms the baselines in the paper. 4.Many descriptions should be carefully polished to make them readable.<|endoftext|>The experiments showed that the proposed method has a better performance than the baselines. This paper proposed a graph multi domain splitting framework for the problem of time varying graph structure anomaly detection. However, the writting of this paper is to be improved. The authors should provide more technical details and insights on the design of the proposed approach in Section 4.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper gives a quite thorough comparison between language model based (LM) and encoder decoder based (EncDec) architectures for neural machine translation (NMT). It would be nice if the authors could discuss the applicability of the findings in the author response and the future version. This is an OK paper but the current version is not exciting, which could hardly attract attention from the community. The writing and presentation are clear, the experiments are relatively thorough, the related works are discussed comprehensively, the results are convincing, the claims are well supported by the results.<|endoftext|>This paper is a good empirical work investigating the unified LM for machine translation and shows some good findings. 2.The paper conducted extensive experiments to show how scaling affects LMs for MT, however, few suggestions based on the findings are given for future development of MT. The empirical evaluations of this paper are comprehensive and solid, the results and findings are interesting.<|endoftext|>According to your experiments, it looks like CausalLM (unidirectional) model is not performing well, or at least as good as PrefixLM or EncoderDecoder models, in almost any settings. This is a paper on investigating the gaps of information in a relatively new area of research in machine translation. I find this paper an interesting read. The approach, experiment setup, and evaluation methods are reasonable and provide confident insight into the questions that the authors ask.<|endoftext|>This paper has conducted extensive experiments to examine the scaling and transferring laws of LMs for machine translation and has concluded several interesting findings which could be inspiring to the future work.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The proposed approach is evaluated against prior skill learning approaches on simulated robot manipulation tasks. The main contribution of the paper is a modification of existing latent skill learning algorithms, in particular PARROT (Singh et al.) This paper completely omits discussion of this point and proposes a safe RL method by collecting an offline dataset while being unsafe. 3.In light of the points in 1 and 2, the results on safety violations in Table 1 need to be updated to account for all the safety violations during offline data collection. It is important to compare with safe RL papers in the experiments since the main claim of the paper is number of safety violations. 6.In equation 9, it is unclear how the objective corresponds to a form of contrastive learning. Is there any constraint on the optimization process to ensure that this does not happen? However, as I pointed out above, there are fundamental issue and assumptions on offline data collection that require multiple safety violations in the data collection process, thereby defeating the purpose of the proposed safe RL approach.<|endoftext|>This paper proposes a new safe RL method, SAFER, that uses a behavioral prior learning algorithm to accelerate policy learning under safety constraints. The authors demonstrate the effectiveness of the methods with several complex safety critical robotic grasping tasks inspired by game Operation. Strength:The problem is important and interesting. The experimental results also show the methods have better performance than the benchmarks. Weakness:The key components of the methods, including using conditional normalizing flow to represent the behavioral prior, using chance constraints to represent unsafe constraints, using ELBO to optimize the chance constraints are all well known standard treatments in the field of reinforcement learning. The authors provided heuristics on the choices of these tools but lack rigorous analysis on the proposed methods. Given the authors only one environment to evaluate the proposed method, it is unknown how well the methods could generalize to other problems. It is an interesting paper but lacks insight into the novelty and the application scope of the proposed method<|endoftext|>This paper is an extension of "PARROT" by Singh et al.(2021), which does a form of hierarchical pretaining from offline datasets to a setting where preferences need to be expressed on the kinds of behaviors that ought to be generalized. (Note: this paper is not very self contained: I had to read carefully the PARROT, which I didn t know beforehand, to make sense of the proposal.) A last important conceptual idea of the paper is that of calibration (which they call "safety assurance") of that safety variable. You may want to write it as a saddle point problem if you want to talk about "equivalent optimization", or say something related to the KKT conditions. > Equation 8 and 9 You use $\theta$ while I think you mean to use $\phi$. All this is saying is that you have a mixture model, which is a clear concept when explained in those terms. I was personally struggling to grasp the motivation and the real world grounding of the problem setup. The operation game for the robot is concrete, but arguably a rather artificial safety critical setup. What would be a real real world scenario where your framework would make sense (compared to the alternatives)? I suggest you expand on this in the intro.<|endoftext|>This paper builds on previous ideas of behavioral priors in reinforcement learning (RL), more in particular the paper by Singh et al 2021. Experiment:table1 The results for safety violations show large variations over methods and tasks. This paper is a clear extension of behavior priors to safety domains. The paper argues that the typical BP setup is not adequate for safety contexts because of how they are typically trained vs. out of distribution samples. I think it is relevant. I wish more discussion would appear in the paper about this. Some discussion on the limits of this approach would be appreciated. Nevertheless, it seems this is the first time this (BP + safety) is considered so any solution would be sufficient. I have another issue with the paper when it comes to the "abstract" action space in the function composition of the policy and the BP. "inspired by the game Operation"; I was disappointed it was not about the real game itself :)  p1:par2:line3 "use a generic offline datasets" (grammar)  (see also above): explain the non standard "downstream"  Figure 2 (and especially the caption text) is not useful at all for me.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper proposed a tighter L0 verification method for Top K predictions of classifiers based on randomized smoothing. I recommend the authors to reorganize this section and define the notations separately in a clearer way. * The paper states that for $k 1$ the certified radius is tight. I appreciate the theoretical analysis and the impressive empirical results in the paper but the written is not clear. I also have some questions on the experiments.<|endoftext|>The paper provides both theoretical progress and experimental results on certified robustness of top $k$ predictions. On the experiment side, the paper compares the proposed method to existing competitors and explored the impact of the related parameters. I enjoyed reading this paper, and I think that the approach is “kind of good”. However, I’m still not convinced whether it is a fair comparison or not, and I’m not even sure whether we can directly compare those methods or not. Also, the paper provided enough justifications for its unique contribution to the defenses using randomized smoothing & randomized ablation type, so I believe this paper can be distinguished from other existing works. While I believe that the work itself is publishable, I’m a little bit reluctant to give a score of 8 for this paper yet due to the reasons I provided in Comments.<|endoftext|>In this paper the authors extend the tightness guarantees of other works on randomized smoothing to the $\ell_0$ case and also tighten the guarantees given by previous works by noting the discrete nature of the predictive distribution induced by randomized ablation. Finally, I think the introduction of top $k$ predictions rather than top 1 prediction is an interesting contribution which extends the applicability of randomized ablation (i.e.$\ell_0$ smoothing). The primary cons of this paper are in its potentially incremental impact. I think the clear exposition of prior works may make this paper s contributions seem simple or incremental, but I think ultimately that these are important developments/extensions of prior work that constitutes a strong enough contribution for acceptance. The second primary con (and perhaps the more noteworthy one) is the applications that this method is tested on. I think it would greatly strengthen the paper if such networks were certified and would add yet another point of contribution for this paper. Though one might see the contributions of this paper as incremental over other works in smoothing, I think these are valuable and important developments in the field of smoothing which can enable further applications to gain guarantees via smoothing.<|endoftext|>This paper provides an almost tight l0 norm certified robustness guarantee for top k predictions against adversarial perturbations, which extends certified radius of the top 1 prediction from Levine & Feizi (2019) to that of the top k predictions, and the l2 norm certified radius from Jia et al.(2020) to the l0 norm certified radius. The experiments on CIFAR10 and ImageNet show that the proposed method substantially outperforms state of the art for top k predictions. In total, this is a good work both in the theoretical perspective and the experimental perspective. The author derives an almost tight l0 norm certified robustness guarantee of top k predictions against adversarial perturbations for randomized ablation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; In this paper, the authors theoretically analyze the convergence of gradient descent for an implicit neural network with infinite layers with ReLU activation. 3.The paper is well organized and clearly written. Does it scale to a large scale problem? So I recommend acceptance.<|endoftext|>The paper presents a proof of exponential convergence to global optimality in the over parametrization settings for an implicit model with scaled weights parameters. Although existing work has established similar proofs for feedforward explicit neural networks, such methods don t work with non linearly activated implicit models where the well posedness issue poses challenges to the training process. The convergence result is obtained first on continuous settings and is then extended to discrete settings. This enables the proof of training convergence for implicit models. The paper sets the foundation for the training theories for implicit models. The reviewer believes this result is significant for implicit models which have become increasingly popular in the community.<|endoftext|>This paper theoretically analyzes the optimization of deep ReLU implicit networks. It first shows the well posedness of the problem, i.e., the existence and uniqueness of the equilibrium point, then proves that under over parameterization, both continuous and discrete GD have global convergence in a linear rate, and the approach is similar to the standard proof for DNN. The proof techniques follow the standard approach for DNN, and I think it is a good starting point for the theoretical analysis of implicit networks. (The following calculation heavily relies on the form (2) of $\phi$.<|endoftext|>This is a challenging and open problem for the community of theoretical implicit models. However, the submission considers a different output $\hat{Y}  UZ^* + V\phi(X)$; hence there is no difficulty, and it is meaningless to get the smallest singular values as $\Theta(\lambda_0^{1/2})$, which is the same as previous over parameterized explicit networks $\phi(X)$ and cannot show any difference between implicit and explicit DNNs. Unfortunately, the submission just got the results in this way. However, the same setting and almost the same linear convergence results are given in [1]. consider a important problem, but heavily rely on the the results in the previous work.
Reject; rating score: 3; rating score: 5; rating score: 8; rating score: 8; This paper introduces a framework to prune parameters in transformer based structures. The authors claim that the proposed method leads to a smaller model with faster and more accurate performance. Cons:1.The pruning method is not very novel. 2.Here I quote from the paper, "where N is set to be greater than at least three fourths the number of samples in the dataset. In order to reduce the effect of outliers, the exact value of N is tuned for the different datasets, based on the validation loss of each sample." The pruning of which element matters the most? Here I have two questions: 1. 6.It does not compare with some of the latest work. Also, could it be combined with other works like compression? 7.(Minor) Some tables are using figures. I think these tables can be generated via latex.<|endoftext|>This paper proposes Specialized Transformer that identifies harmful parts and prunes a pre trained transformer in a greedy and hierarchical manner to boost accuracy in a downstream task. According to the paper, the proposed Specialized Transformer is faster, smaller, and more accurate than a standard fine tuned model without any retraining. As far as I understand, it prunes a transformer fine tuned for a downstream task. I am curious whether similar pruned architectures are obtained with different random seeds. Are pruning and hard self attention separate each other? What is the order of them? The paper should be revised further. The related work section is just a listing of works that improve the efficiency of the transformer. Method and result sections include many long paragraphs only divided by headings. I don’t understand how to use Taylor expansion to estimate the importance of each parameter. The results look very promising, but the presentation is bad. Their methods seem too naive to be effective, but the authors do not fully explain why they result in a good performance.<|endoftext|>The authors propose a Specialization framework to create optimized transformer models for a given downstream task. The authors proposed two ways to reduce model parameters, 1) Hierarchical pruning. 2) Replacing soft attention with hard attention. 2.The accuracy driven pruning is reasonable and different from previous pruning based methods. 4.The paper is easy to read and the experiments are solid. And the experiment shows that the proposed method Is better. Also, more analysis on training and inference speed between two attention mechanisms would be helpful. The experiments are quite solid by adopting the method to multiple structures and comparing it with SOTA pruning methods.<|endoftext|>The authors test the effectiveness on GLUE and SQUAD with BERT base, Q8BERT, DistilBERT and XLNet. The method shows effectiveness on both benchmarks and with different models by outperforming the fine tuned counterparts. 3.The paper proposes a progressive and hierarchical strategy that provides a solution for both self attention and FFNN in a Transformer, which has the potential to be adopted in more tasks (e.g., generation, translation). My main concern is novelty   the pruning technique is not new and pruning a neural network progressively is also not a novel idea. 2.Although specialization does not require a complete re training, it does introduce extra overheads for training such a model. Although the idea is not completely new, I would like to recommend weak acceptance for this paper.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper proposes a novel combination of policy gradient and local search algorithms for the traveling salesman problems (TSP). Empirical studies are provided on random TSP instances as well as those in TSPLIB. Finally, ablation studies show the importance for each component of the proposed algorithm. The first one is algorithmic, i.e., including local search in the training loop of policy gradient. I have several questions on the technical details:1. Does subtracting this baseline still yield an unbiased gradient estimator? 3.I also noticed that the compared methods are different for random TSP instances and TSPLIB instances. Clarity: This paper is well written and easy to understand. Good job!Significance: Since I have some important questions regarding the technical contributions, I will need to see the authors’ response in order to have a more informed evaluation of the significance of the paper. And for the more realistic TSPLIB instances, the number of baselines is much smaller.<|endoftext|>This paper introduces a deep RL approach combined with an equivariant model (to handle Euclidean symmetry) and local search heuristics (to improve a tour) to solve traveling salesman problems (TSP), in particular focusing on the generalizability of large scale instances. The model consists of a graph neural network (GNN), a multi layer perceptron (MLP), and an attention mechanism. According to the experiment results, we can see that the proposed algorithm has better generalizability on large scale TSP and realistic TSP than most learning based algorithms. 3.Authors provide detailed experiments and evaluations of different methods and problem instances. Compared with the learning based algorithm [1] in TSP 200, 500, 1000, the proposed algorithm has longer computational time and larger gaps. Generalize a small pre trained model to arbitrarily large TSP instances. Technically, the proposed model uses a standard RL framework with GNN and attention as encoder and decoder. The authors introduce an equivariant model to capture the combinatorial structures of the TSP, which has some novelty and improves the overall performance, but the ablation study does not show that this model provides very significant added value. Therefore, my decision is weak reject.<|endoftext|>The model constructs a tour, one city at a time, by learning a distribution (policy) over unvisited cities which is optimised via REINFORCE. The equivariance in TSP is mostly exploited during preprocessing steps—where rotation, translation and reflection operations are applied to the coordinates of each city—but is preserved by the model architecture which consists of a graph neural network and attention mechanisms. The proposed method show competitive results, generalising to TSP instances of different sizes, which is an important and useful trait that is relatively rare in deep learning approaches to combinatorial optimisation. The paper is well written and easy to follow. In particular, the idea to interleave reinforcement learning with traditional combinatorial optimisation heuristics is not entirely new. Most if not all of the design choices in the paper are very much tailored for the TSP, so I am not confident the approach is *naturally* extendable to other combinatorial optimisation problems as the authors claim. In particular, the use of equivariance properties and the ablation study in the paper should be quite insightful for future work on deep learning for combinatorial optimisation problems.<|endoftext|>The paper proposes a series of techniques and a new model to learn to solve the TSP using RL. The authors propose a modification of the policy gradient algorithms for training by replacing the value of the policy generated solution by its value after applying a local search heuristic, and use the original value as a baseline. The paper is clear and well written 2. It could be leveraged by other learning based method for solving the TSP. * The ablation study is useful to show the impact of the different components of the approach **Weaknesses** 1. My main concern is about the applicability of the proposed contributions beyond the TSP. 2.The proposed approach is presented as a novel approach that exploits “equivariance” but it seems to me that it mostly exploits the properties of the TSP (indeed invariance of the solution to certain transformations of the coordinates) to preprocess the instances. 3.Overall, the proposed approach is quite specific to the TSP (the preprocessing, the local search and the architecture), it is not straightforward how much of it could be exploited for solving other CO problems. My concerns and questions were properly addressed, in particular regarding* the explanation of the proposed smoothed gradient, * the novelty with respect to previous works,* and the applicability of the proposed contributions beyond the TSP. Therefore I believe this paper can be interesting for the NCO community to participate in addressing the well recognized problem of generalization of ML based CO solvers. 3.What is the impact on training time of adding the local search, updating the encodings of all nodes and the preprocessing?
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; rating score: 8; The main algorithm (Algorithm 1 in the paper) appears reasonable, but I have some questions about it. How does this algorithm scale with the number of ReLUs? If so, it is worth mentioning, as this is often a limitation of related methods that involving branching for every ReLU. For example, exact certification of neural network robustness is shown to be NP hard in [2]. The experimental results in this work are primarily focused on very basic datasets (MNIST and Fashion MNIST), so it is unclear how results will generalize to larger datasets (CIFAR, ImageNet). After reading the rebuttal and other reviewer comments, I have decided to maintain my score of "weak reject". As Reviewer r9vz noted, a lot of the most interesting work is left as "future work" at the moment; it is still unclear to me whether this approach will scale effectively to more complex datasets (CIFAR/ImageNet vs. MNIST/Fashion MNIST) or more standard and modern architectures (CNNs, resnets vs. 3 layer fully connected networks).<|endoftext|>Unfortunately I can not recommend the current version of the presented manuscript for acceptance at the ICLR conference. Personally I believe that this is primarily due to the introduction not properly introducing the setting and aims of the paper, but I should also acknowledge that I was not entirely familiar with the work [Bhagoji, 2021] (on which this paper heavily builds) before getting assigned this paper for review. For this reason, I would be very interested in hearing from the other reviewers to what extend they found the paper hard to follow as well. ### Contributions of workIf I understood correctly, the main contribution of the paper is to provide a lower bound on the adversarial robustness of specific models. The contributions of the paper are unfortunately unclear.<|endoftext|>The paper lacks the analysis for justifying the proposed algorithm. My current decision is acceptance. I d like the authors to clarify the motivation for calculating the lower bound on the adversarial loss with the fixed feature extractor. The motivations in the related works, including Pydi & Jog 2020 and Bhagoji et al.2019;  2021, are understandable. In the experiments, the authors evaluated the lower bounds with fixed first 1 or 2 layers as usage of the proposed method; however, it is unclear how to use the evaluated adversarial loss. The theoretical contribution of this paper seems to be incremental. It is necessary to clarify this point.<|endoftext|>This paper investigates the lower bound of robustness, i.e.the best robustness we can expect, with the added constraints of model architecture. Overall I like the idea of the paper: it gives more insights to how different model architecture can affect the robustness lower bound, and thus gives some guidance to how we design a robust system. The paper is written in a rather theoretical manner. 1) Does the robustness lower bound depend on the underlying data distribution? 2) Are the robust entropy evaluated on the training dataset? Therefore, the relation is not immediately clear to me. (Sec 4 gives a pointer to Bhagoji et.al, but it would be good to summarize the key steps in this procedure.) After viewing the author response, my technical questions are resolved. I m raising the score to 6 for the completeness in the idea. MNIST and FashionMNIST is slightly too simple.<|endoftext|>Specifically they note that ReLU seems to reduce the robustness of features in later layers of the network. Experiments are done on fully connected neural networks trained on MNIST and Fashion MNIST with standard and robust training methods. While neural network architectures for which this algorithm is not applicable seem to be ignored and some notational issues made it take longer to read, I think the paper should be accepted. For example, in section 3: "We take the neighborhood of any point to be a scaled shifted version of this ball: N_\epsilon(x): n + \epsilon \mathcal{B}", but x is missing from the right hand side.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The main target application is in the problems of offline RL and policy finetuning, where some closeness to the behavior policy is often desired, in conjunction with optimizing the value function. From Table 1 and Table 2, it is quite convincing that DiME outperforms the baseline method of LS (and other existing non multi objective based algorithms) on offline RL. The presentation of this paper is quite clear. I have two main questions on this end. Section 5.2 contains a short discussion on why MO MPO is not suitable for offline RL. Regarding these two questions, I’m curious whether the authors could either provide some more details on how the actual implementation of DiME differs from MO MPO, and/or present some experiments on the MO MPO algorithm in conjunction with DiME and LS. Overall, this is a well executed paper that presents a new algorithm DiME for multi objective policy optimization.<|endoftext|>The typical scheme is to take a weighted sum; this paper recasts that into a multi objective optimization question, and proposes an objective that combines policies from various objectives in a weighted KL sense. Is there a more fundamental distinction in the tasks one can perform better on vs. the other, especially for a fixed scalar reward (the abstract seems to make a case for this; but the justification for this is not followed up on the paper)? Possibly in terms of some qualitative or quantitative features one might ascribe to the task? But, as I understand, both LS (which may have similarities to existing approaches) & DiME are both algorithms proposed in this paper. MORL is a quite a well established problem in RL. Is there a reason why considering MO MPO s (for instance) experimental setup beyond humanoid walk run (for which results are fairly similar) for MORL are prohibitive here?<|endoftext|>This paper proposes using multi objective optimization as a tool for tackling challenges in RL. The motivation is that the different additional contraints (or objectives) in the policy optimization step are always in conflict, which is natural to view the existing RL as an instance of MORL problem. Typical MORL applies linear scalarization (LS) by taking a weight sum of the objectives. In this paper, first, the authors identifies the disadvantages of prior work (including LS and MO MPO) and proposes a new objective called DiME. The main contributions of this work are in two fold. In the experiments, the paper shows that DiME outperforms LS on standard MORL tasks empirically. However, it did not show comparison to MO MPO   this is wierd because MO MPO seems to serve as a better baseline than LS. The authors provide a general formulation and shows that small modifications to the general objective can recover to existing algorithms. Extensive experiments show that DiME allows better optimization for finetuning algorithms and offline RL algorithms.<|endoftext|>Based on the extensive empirical experiments, the proposed method can outperform other baselines which do not view the problem as a multi objective optimization problem. Strength  The idea can be naturally applied to the offline rl and finetuning setting, since in these two problems, we can view the original objectives as two different objectives when learning the policy. The experiments are conducted thoroughly and demonstrate the effectiveness of the current approach. I wonder if it is possible for the authors to compare some other multi objective baselines instead of linear scalarization, such as multiple gradient descent or pcgrad? since I think linear scalarization is the most native baseline for multi objective optimization. I am always curious how the authors implemented the multiobjective optimization part, ever since your first paper.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper studied the policy optimization problem in the offline constrained MDP setting. In order to guarantee the constraints are always satisfied, this paper provides a novel approach based on CoinDICE to efficiently estimate an upper bound of constraint violation. The author also provide sufficient empirical verifications to support their proposed algorithm. This paper provide a interesting and novel algorithm to address offline safe RL problems. The offline safe RL is a very challenging problem, but this paper provide a very promising approach to address several issues in this setting.<|endoftext|>To do this, they adapt ideas from OptiDICE and find a reduction from a nested constrained optimization problem to a single unconstrained optimization problem that can be efficiently represented with a neural network. The  authors compare their method to a number of baselines on both random grid worlds and continuous environments, and find that COptiDICE achieves both good performance and better constraint satisfaction than alternative methods. The paper is well founded and feels like a "correct" solution to this problem, and the empirical results show the method performs well in practice. The experimental results also seem to demonstrate that the proposed method is better at satisfying the problem constraints than other methods.<|endoftext|>First, the paper presents the algorithm COptiDICE, which directly estimates the stationary distribution corrections of the optimal policy. First, by imposing the closed form solution of $w^*$, the paper reformulates the minimax optimization problem to a single minimization problem. Moreover, since $d^D$ is the empirical distribution of the dataset $D$, such assumption does not hold for continuous state action space. The paper should provide more discussion on the restrictiveness of such an assumption. Thus, I expect less novelty of this paper compared with OptiDICE. However, in real world applications, the dataset may not be able to cover the whole state action space, which is the main challenge of offline RL. In particular, in the case that the dataset does not cover the whole state action space, I encourage the authors to show if the learned policy can outperform the data collecting policy. Overall, the paper is well written and motivated.<|endoftext|>This paper has presented a DICE based offline constrained RL algorithm for constrained RL. Experimental results on tabular CMDPs and continuous control tasks show that the proposed method can achieve a better trade off between reward maximization and constraint satisfaction. 1st contribution: They firstly proposed to tackle constrained offline RL by solving a single minimization problem. Ablation study on constraint cost threshold is required to both show the sensitivity of the proposed method to different cost threshold and consistent advantage over other baselines. However, in the real world, agents normally act safely but have some unsafe attempts.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper proposes a novel solution to infer the graph structure. It starts with the unrolling algorithm for graph structure inference problem, and then adopt the proximal gradient iterative solutions. Strengths:The problem on graph structure inference is not novel, yet the authors provided an interesting aspect by formulating it with unrolling algorithm, which is then solved using truncated proximal projection. Characterization and inference of graph diffusion processes from observations of stationary signals. The proposed algorithm in this paper sounds technically interesting. However, I have some concerns on some of the descriptions, model designs, and empirical results. Thus, for the current version, I would rate it as borderline paper, and I will consider raising the score if the main concerns are properly solved during the discussion period. This may benefit for computation, yet how about the approximation error? Maybe some previous work (if exists) or some textual explanations can be added here.<|endoftext|>The authors should at least describe the losses (if not giving formulas) in the main text. The proposed approach performs better than several baseline approaches. **Strengths:**This work formulates the graph structure learning problem as an inverse problem. **After rebuttal**This paper has technical merits and the revisions done during the rebuttal period solidify the work. Such a formulation is interesting and it differs from many of the latest formulations based on deep learning. The reviewer is a bit skeptical that the supervised setting has very limited use. The first step is simply a least squares problem and the second step can be solved by using proximal gradient. Furthermore, one may let $h$ have $N$ entries and use a regularization to estimate them, leading to again a proximal gradient solution.<|endoftext|>The authors tries to recover the underlying graph structures from observed symmetric adjacency matrix. The key assumption of the paper is that the observed adjacency matrix can be represented as a polynomial of the adjacency matrix of the true underlying graph, which is reasonable. The experiments show that the proposed model can recover graph structures on provided dataset. While the overall idea and the algorithm part of the paper seems to be ok. There is a  fundamental question that need to be addressed. In (1), there is no constraints on alphas and As, in this case, is the solution A in (1) unique? If it is unique, then we can conclude that the problem the author are trying to attack is identifiable. Or if there are multiple A that satisfies (1) with a given A_O, is the skeleton of A unique? One fundamental question about the paper is not clearly stated.<|endoftext|>The authors propose the graph deconvolutional network (GDN), which is a novel approach to graph structure recovery from noisy observed graph structures. It is based on an assumption that the observed adjacency matrix is polynomial in the true graph adjacency, and uses a proximal gradient computation to iteratively optimise for this structure. I found the paper to be well written and easy to follow. Unfortunately the paper is outside my main area of expertise and therefore I am unable to make strong comments on the quality of the baseline approaches used. I will defer to the other reviewers for advice on this. The main concern I would have with the method as is is that it seems to rely on an "observed" adjacency matrix ${\bf A}_O$. Would the authors  method still work if ${\bf A}_O$ were not explicitly given? I recommend weak acceptance because I feel like this paper presents an interesting, novel and well grounded idea, with a decent level of evidence towards empirically ascertaining its contributions.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; In this paper, the authors provided elegant similarities between contrastive learning and deep metric learning. While the uniformity ensures the scattered. Why the unit hypersphere is a nice feature space is not very well addressed in this paper. 2."... We derive the theoretical analysis for the triplet loss to prove that the triplet loss..." Page 1, Para 2. A citation to [1] is missing on Page 1, Para 2. How we mine negative samples is not very clear. Overall the paper seems like an extension of [1].<|endoftext|>Therefore, the theoretical contribution is marginal. I have some other questions:1. Assume k 2, then the number of points with a pairwise distance larger or equal than $\sqrt{2}$ is supposed to be 4, imaging four points of a square in the circle. (If I am wrong, please correct me.) The paper has marginal contributions and limited novelty.<|endoftext|>I like the general idea of the paper and the hyperspherical perspective on deep metric learning. It has been a common wisdom that deep metric learning should be performed on a hyperspherical space, but an rigorous and principled understanding is yet to be found. I believe this is an interesting and extremely important direction. If every adversarial attack method works well for improving tuple based losses, it will make the current conclusion more general. Only the main result is put in the main paper.<|endoftext|>This work introduces novel adversarial deep metric learning algorithm based on the fact that contrastive losses promote intra class alignment and uniformity on the unit sphere. The authors provide a theoretical proof that this is also true for the triplet loss under mild assumptions. Weaknesses:  As stated in this paper, separating the triplet loss into the intra class alignment and the hypersphere uniformity terms is intractable. If that s the case then this would an argument in favor of attacking each one of the two terms separately. I m aware the gradients of the intra class alignment and the hypersphere uniformity terms are only used to generate adversarial samples which are then used to train the model with the triplet loss. Table 4 shows that the three ADML variants result in more robust models. Namely, the loss can be rewritten as the sum of two terms: a intra class alignment term and a hypersphere uniformity term.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper addresses a behavioral cloning (BC) distributional shift problem in a POMDP setting. Weakness:  The paper is difficult to follow and I encourage the authors to improve the writing of the paper.<|endoftext|>Unfortunately, it is hard to confirm the results since the code has not been shared at the time of submission. Additionally, I believe that the paper could do better in substantiating the claims about why this approach should perform better. The paper describes a simple solution to a common problem (copycat behavior) in BC.<|endoftext|>As a result, I think it is insufficient to recognize the authors  novel contributions. The authors proposed a combined approach to address the "copycat" problem in imitation learning when learning with a history of observations. This approach is new, the problem is interesting but the paper lacks satisfactory investigation of the problem itself and how the proposed method can solve the problem.<|endoftext|>Overall this work addresses a well known problem (copycat problem) and builds towards training better imitation learning policies with temporal information. I enjoyed that the method is simple and effective on the NoCrash benchmark, but I find the justification and analysis a bit incomplete in its current state to be recommended for acceptance.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Motivated by the success of physics inspired neural networks, authors investigate the different inductive biases (implicitly or explicitly) encoded in Hamiltonian neural networks (HNNs). Authors identify four biases: the ODE, second order, energy conservation and symplectic biases. They claim and empirically show that, as opposed to what was previously argued, HNNs  performance is mostly due to the implicit second order bias. Does the MuJoCo physics simulator indeed conserve energy? The submission is well motivated and the investigation on HNNs inductive biases is thoroughly performed. Up to my knowledge, the contributions of this submission are indeed novel. I found the submission well written, with the motivation, ideas and empirical findings well conveyed to the reader. ## Relation to prior work. I would suggest to add a Figure showing phase portraits for a symplectic and a non symplectic system, to give additional intuition to readers that may not be familiar with this concept.<|endoftext|>This is a well written analysis of Hamiltonian Neural Networks (HNNs), a class of physics inspired deep neural networks. The work is motivated by a desire to apply HNNs to non toy datasets as well as to gain an understanding of the key inductive biases that explain the majority of their performance. Through controlled experiments on synthetic trajectory data, they explore energy conservation, a symplectic bias, complexity of state representation, and second order structure bias. The experiments and analysis used to examine each inductive bias are well thought out and the conclusions are supported by the provided evidence. I also found the analysis on the relative energy violation of NODEs vs. HNNs in Section 4.1 compelling and illuminating. For the Mujoco experiment in Section 5, only quantitative results are provided. Perhaps it could be re written to more clearly highlight the specific contribution, which to my understanding is: a Neural ODE that models dynamics with second order information (NODE + SO)? I think this paper deserves acceptance. Update after rebuttal: I will be maintaining my initial score and hope that the authors will update their paper for the final camera ready version to include the promised improvements.<|endoftext|>This paper presents an in depth study of the inductive biases in physics inspired neural networks, especially Hamiltonian Neural Networks (HNNs). The authors break down the biases in HNNs into multiple categories, such as energy conservation of the network, second order structure of the output, symplectic vector fields produced by the networks, and the role of the complexity of the coordinate system. The authors show through experiments on a set of pendulum based tasks, that HNNs are not in fact better at conserving energy than competing approaches that dont explictly model this such as Neural ODEs. They also show that Regularizing NODEs with a symplectic field bias does not help in generalization. I think this work presents an important analysis which could have useful downstream applications. It would be good to see more empirical analysis on more complex tasks such as the MuJoCo benchmark tasks instead of the kChainPendulum and kSpringPendulum tasks, especially since we see that the biases of HNNs have difficulty with modeling the dynamics for the MuJoCo tasks.<|endoftext|>This paper has carried out a formal analysis of these methods to identify which specific aspect(s) of these energy conserving networks contribute(s) the most to their superior performance in prediction and generalization. Through theoretical and empirical analyses, this paper concludes that incorporating the second order structure (i.e., using a neural network to model the acceleration) and reducing functional complexity through a change of coordinates, not energy conservation or the symplectic structure, play the most critical role in improving the performance. The main contribution and novelty of this work lie in the insight it provides about the role of various, often intertwined, inductive biases present in the networks that encode Hamiltonian and Lagrangian dynamics into their network structure. Furthermore, through carefully motivated and executed experiments, it shows that incorporation of second order structure into the learning framework is the most important factor in improving the performance of Hamiltonian/Lagrangian dynamics based neural networks. It would be much appreciated if these theoretical discussions were made more precise. On the other hand, recent work by Zhong et al.(*Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models*, arXiv:2102.06794) has extended Hamiltonian dynamics based networks to systems with contacts and collisions. I would encourage the authors to use this approach as one of the baselines while running the experiments for Section 5. The idea is interesting, the experiments are mostly well carried out, and the paper is reasonably well written.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposed a deep reinforcement learning framework for finding dynamic general equilibrium, which is one of the most fundamental problems in economics. Although the authors reported their empirical results, they failed to provide enough theoretical guarantees for supporting the results. In the proposed model, the government has a first mover advantage as in a Stackelberg game. For example, the trick for running both simulation and RL on a GPU is from WarpDrive, yet the details of the WarpDrive framework are not even introduced in this paper. The paper studied a fundamental problem and tested an efficient algorithm for solving it. However, I do not think the paper can be accepted in the present form because (1) the success of the algorithm highly relies on a trick proposed in another paper; (2) besides running the algorithm empirically, no theoretical analysis of the proposed scheme is provided in the paper.<|endoftext|>I have to read back and forth, and keep guessing the meaning of these symbols. The authors propose to use a structured learning curriculum that runs only on GPUs. Using a multi agent reinforcement learning algorithm to solve the complicated game considered in the paper is natural and reasonable.<|endoftext|>The authors note that their method finds only low welfare equilibria. What does the discovery of these equilibria in RBC suggest to policy makers? However, this may be because I am not adequately familiar with this literature (as I would expect the same of the average ICLR reader). I recommend rejecting this paper in a large part because (a) the main contributions (RBC model/reward shaping) have unclear/unspecified novelty, (b) weaknesses in the analysis described in the main review, and (c) need for contextualizing this work in both the MARL and economics literature (help the MARL community understand that this work is important!). In total, I am excited to see interest in this area; however, I do not think this particular unit of research is ready for publication at this time.<|endoftext|>This paper proposes a multi agent deep reinforcement learning (DRL) method to compute general equilibria in economics. The problem is of Stackelberg (or leader follower) type, with the government acting as a leader. The main contribution is algorithmic, but without a precise description of the algorithm, I find it hard to really understand the method and to assess the impact this work could have. As pointed out by the authors, there are relatively few works on reinforcement learning for economics.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposes to learn a retriever (to find relevant documents in a dataset / dual encoder) and a ranker (to re rank retrieved documents / cross encoder) using an adversarial framework (the retriever trying to fool the ranker). This training allows to reach state of the art performance for both the retriever and the ranker. Overall, this is a good paper that is inspired by many recent techniques on neural retrievers, bringing back the IRGAN adversarial training but with more success. Compared to the most related work (IRGAN), the main differences are (1) the use of a contrastive loss rather than a cross entropic one, (2) a different pre training, inspired by most recent works, (3) regularization through distillation (KL between retriever and ranker), (4) a different sampling procedure. The section 3 describes the training procedure. There are some minor problems with the paper, but overall I recommend acception.<|endoftext|>This paper proposes to use adversarial loss and distillation loss to improve dense retriever performance from the help of the ranker. This paper proposes a new method to improve deep retrieval performance. The key novelty of this paper is to formulate the problem as an adversarial training procedure. Experiments show better retrieval performance than existing methods on some popular benchmark datasets. The retrieval performance is good and seem to achieve state of art numbers. But that will limit the contribution of this paper. The significance of contribution is subjective and the reviewer is generally ok with it. The paper proposes a new loss function to leverage ranker to help retriever for text retrieval tasks.<|endoftext|>This paper proposes a minimax multi stage iterative method for document retrieval. The dense retriever is modeled using a dual encoder and the ranker is modeled with a cross encoder, both of which are fairly standard. Is it done in the same manner as for the reranker? In the second step, the retriever is trained using an objective similar to REINFORCE to maximize the likelihood of the negative documents. Summarizing from above:The proposed approach has novel aspects that is also demonstrated by its state of the art results on benchmark IR tasks. The adversarial training of the model makes the proposed model different from the previous work. [Update]I want to thank the authors for doing additional experiments and answering the questions raised in the reviews. As these baselines are highly competitive, I believe that the improvements are significant. 1.Results when just initializing the retriever with either ERNIE or ICT and not applying DPR on them?<|endoftext|>This paper proposes an adversarial method to jointly train the dual encoder retriever and the cross attention ranker. The key idea is to obtain harder negatives for the retriever by training a separate ranker with higher capacity and taking passages that the model is confused with. It also proposes a technique like distillation from the ranker to the retriever. Experiments on three datasets demonstrate improvements over a range of baselines. * Experiments are comprehensive   evaluated on three well studied datasets, compared with a range of recent prior work, and analysis showing impact of core components like iterative training, effect of regularization and the effect of number of negatives. However, prior work including Xiong et al and Oguz et al clearly describe that they iteratively choose negatives, as the proposed model in this paper does. * The base model used in the paper is ERNIE 2.0 with Inverse Cloze Task and coCondenser, while most prior work uses BERT base. This makes it hard to compare the performance with prior work in a fair manner.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; They also show that using E AT fine tune could turn $\ell_{p}$ robust model into a model that is robust against the union of $\ell_{p}$ adversarial perturbations. The authors also provide some theoretical proof for their method. (2) E AT could quickly fine tune models that are only robust against one perturbation type into models that are robust against the union of multiple $\ell_{p}$ adversarial perturbations, and the fine tune results are shown better than simple $\ell_{p}$ norm fine tuning. There should be more proof about the certified robustness of the proposed method. (2) In Fig 1, the paper shows the model adversarial robustness against multiple norm perturbations after E AT fine tuning is better than L1 fine tuning. More fine tuning is needed for the baseline methods to balance the robust accuracy on all norm types.<|endoftext|>The method involves training against $\ell_1$ and $\ell_\infty$ attacks with the hypothesis that this will additionally give robustness for $\ell_p$ threat models with $1 \leq p \leq \infty$. This paper is generally well written. Still, I have some concerns about whether the results support the proposed method. In particular, in Table 4 it seems that adversarial fine tuning (3) is effective even without 1 and 2. Overall, the paper could use some more work to separate the effects of the components of E AT and motivate the combination of it with the fine tuning contribution, which seems orthogonal.<|endoftext|>The paper has a very strong contribution to the field of multi norm adversarial robustness. The results are convincing and evaluated against AutoAttack which is## Strengths1. 3.The overall paper is written very comprehensively, and the authors have done an extensive study of various pre trained models. What about using MAX or MSD as a fine tuning step? I do not think that is the case (it is a heuristic modification that works in practice!), nor do I think this paper needs that to be accepted.<|endoftext|>The paper tackles the problem of robustness against multiple perturbations and proposes extreme norms adversarial training (E AT) that adaptively alternates between $\ell_1$ and $\ell_\infty$ norm. ### Weaknesses  The paper is limited in novelty. Therefore, I would recommend including a discussion about their conducted analysis to clarify the contributions of the proposed method. How is the performance for fine tuning on CIFAR 10 with a single epoch? I think this paper tackles an important problem and conducts an exhaustive experimental evaluation.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The agents use a shared piece of information which is the utilized warehouse space and other shared resources (if any) and rather than that there is no other shared information among them. So, each agent uses that shared information along with its local observation, and local reward and train decentralized agents in an independent learner scheme. The PPO algorithm is used to train the independent policies. Also, there are some questions in the paper that need to be addressed. How can one make sure that the training is not hindered with that? Q1 1 Same issue exists when you do sampling $c^{ i}^t$ and $(s^i_t; a^i_t; c^i_t)$ from the old policy. Does the data have any given feature/characteristic? (2006).An optimal solution technique for the joint replenishment problem with storage and transport capacities and budget constraints. Same as above I ll update my recommendation based on the answers of the authors.<|endoftext|>However, there are several important algorithmic issues not investigated in this paper, and the important related method in the literature is not compared with. This paper considers the multi period inventory replenishment problem for a single store with multiple SKUs. The paper takes a multi agent RL approach to solve this problem, with each SKU being modeled as an agent and having its own replenishment policy. Strengths  The paper studies a very interesting and promising industrial application of RL. Improvement in inventory replenishment has profound real world impact. A model based MARL algorithm is proposed to tackle the challenge of large number of agents by exploiting the structure of the multi SKU replenishment problem, that is, the replenishment of the SKUs are associated only through the global capacity constraint. The paper should add at least one classical non RL replenishment algorithm as baseline. I assume this is the interaction with the global environment.<|endoftext|>The authors propose a MARL style algorithm to solve the capacity modelling problem for the periodic review inventory management problem. Overall, this is a nice paper that addresses a simplified version of a major issue faced by retailers (inventory management and capacity control) as a MARL problem. The approach identifies that in the IM problem   the shared context is only really the resource that needs to be divided between different agents. The paper has a nice explanation of the problem   though it suffers from weak baselines (other PPO methods), rather than trying out a few different approaches (dual SGD, admm etc). This isnt very realistic and could probably be improved a bit. Finally, it seems to be only for one time period? Overall I weakly recommend accepting the paper as it addresses a novel way to approach the capacity control problem. The paper is a nice MARL approach to the IM Capacity Problem. The baselines are not ideal and the reward function doesn t resemble a real capacity issue   but it is a good step towards a more general IM solution<|endoftext|>Meta review I think the paper tackles an important real world problem with an interesting approach and, although the experiments were a bit lacking, the authors have included a sensible baseline and a more realistic scenario. Simulations show that the proposed algorithm is able to outperform other MARL baselines for up to 50 SKUs. Strengths:  The paper tackles an important problem with impact in the real world. I would have expected to see the algorithm working with more realistic settings, including at least hundreds of SKUs, a multi echelon network and ideally stochastic lead times. Second, I would have expected comparison with well known operations research baselines. The authors also claim their approach could work for non homogeneous SKUs, but it is not clear how. Comparison with the latter would be very interesting as they potentially could scale the current formulation to arbitrarily large number of SKUs by relying on a mean field formulation. Minor comments  Some simulation results are not clearly displayed. But the benefit of tackling a real world problem vanishes because the proposed approach has been evaluated in very simple environments and with baselines that are not very relevant for this problem.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper uses Sequence Alignment technique to redistribute rewards, to a similar effect as with LSTM in RUDDER. The main contributions seem to be: a) introduction of the Sequence Alignment technique which works well with few expert demonstrations; b) experimental demonstration in Rooms / MineRL. While the results in Minecraft are presented as impressive, the paper contains several shortcomings from the strict scientific point of view. The comparison is made only on two different GridWorld problems, and it is unclear whether the benefit is from the new reward redistribution technique or division to subtasks. An ablation study and experiments on different domains, perhaps those used in the original RUDDER paper, are required to find the answer. Moreover, some key concepts of the paper are poorly explained. Section "Defining Events" does not explain how the clustering is exactly done. A function "f(s, a)" remain undefined. I don t agree to the authors  claim that "Q function of an optimal policy resembles a step function" and "is mostly constant". Presentation of the paper can be improved: It is not clear why the biological sequences in Fig.2 left is included. Given all the drawbacks, the paper is not ready for publication. I would recommend the authors to focus more on the in depth comparison with Rudder, save some space by moving the figures regarding Minecraft to the appendix, rewrite the key concepts in more depth, verify the key assumptions and demonstrate WHY the new method is better.<|endoftext|>The authors propose Align RUDDER, which aligns given task demonstrations (sharing a common strategy) using a profile model, to identify common events as subgoals and redistibute reward to realize more efficient and effective RL. Strengths:  Simple, effective and general technique for identifying subgoals with few expert demonstrations sharing a common strategy  Large gains over RUDDER and sota imitation learning methods on 2 gridworld tasks  The only technique that was able to mine a diamond in the minecraft challenge with automatically learned subgoals (infrequently)Current Limitations:  As shown in figure 6, the success rate of Align RUDDER is high, but then degrades rapidly in the last quarter of the minecraft task. An analysis of what elements of Align RUDDER contributed to that degradation would be highly instructive in understanding the current limitations of the approach, and future research directions. Align RUDDER s "five steps" are described completely enough, but I feel that the presentation needs to be further improved for those not already familiar with profile models (many). Expanding figure 3 and better connecting the description with it would improve the paper substantially. The assumption of a single underling strategy in Align RUDDER seems like a significant limitation. A clustering step to identify multi strategy profiles will be required in many situations. How common such situations are and how they would be detected and mitigated with e.g.with multi strategy profiles is not adequately discussed. Figure 1, which describes the basic intuitions behind the RUDDER approach, distributes all of the reward to earlier subtasks, leaving none for the end goal, which seems like an issue...Post rebuttal comments:Thank you to the authors for their response. I have looked at the other reviews and re read parts of the updated paper, and I tend to agree with the more critical reviews wrt the following: 1) The explanation of RUDDER is still not complete or detailed enough to appreciate Align RUDDER s relative strengths (and weaknesses), and thus the contributions of the paper. 2) The explanation of Align RUDDER needs significant work wrt both the layering and quality of explanation, it is just not clear enough. A solid approach to the difficult and important problem of learning from scarce expert demonstrations.<|endoftext|>Building on prior work by Arjona Medina et al [1], the authors incorporate demonstrations of optimal trajectories from an expert in the training pipeline. Additionally, to improve the sample efficiency and stability, the authors replace LSTM model of RUDDER with an alignment based profile model. The approach is evaluated on two synthetic grid world based environments and a MineCraft based environment. Strengths:+ The empirical evaluation is well laid out and the experiments are described with necessary details. Weakness:+ The paper writing could be improved to better explain the prior work and segregating the core contributions of the work (e.g reward redistribution builds on RUDDER). It would be useful to have more comparison on other benchmark tasks like locomotion. They identify several drawbacks in RUDDER and propose effective modifications to the learning algorithm to improve performance. While the experiments are promising they are restricted to navigation style experiments, it would be useful to have comparison to more imitation learning based baselines with diversity in learning environments.<|endoftext|>This paper presents an improvement over RUDDER to allow for RUDDER style reward redistribution when given limited sets of demonstrations by using a biological sequence alignment model. ## **Paper Strengths****Motivation**: The authors make very clear the motivation. Improving RUDDER, a reward redistribution method, so that it can work on tasks in which demonstrations of high reward trajectories are given because the task is too hard for exploration alone. **Experiment Results:** Align RUDDER seems to have great performance improvements over RUDDER, especially with a very limited number of demonstrations. The authors also perform hypothesis testing, further validating the results of their experiments. ## **Paper Weaknesses****Grammatical Issues:** A few grammatical issues throughout, it doesn t detract too much but occasionally causes a hiccup in the flow when reading, so please fix these. **Method Clarity**:   The paper figures do not explain the method too well. Figure 3 is nice, but it should have some text explaining things better in the caption. The authors should rewrite the method section to have far fewer details (except those explicitly needed) about the alignment algorithm itself, this should all be in the appendix. Instead, the entire reward redistribution scheme should be used to explain both intuitively and in detail how this 1) encourages alignment of similar trajectories in terms of states and actions, 2) better alignment scores  result in a better reward redistribution scheme, and 3) is far easier to learn with few demonstrations than RUDDER s LSTM. **Experiments:** To me, the experimental setup seems valid. But along the same lines as what was stated above, there should be a bit more analysis in the main text about *why* Align RUDDER performs the way it does in comparison to the other methods. I am willing to adjust my score if the authors can improve on this.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The entire problem formulation is done by lifted CSP. So a method that potentially can be applied to most neural solvers (based on message passing) is important. Cons:1.It would be better if the authors can try their method on the routing problems or other graph problems. Sudoku and Futoshiki are very similar. This paper proposes two methods for the output invariance issue based on the RRN framework. An elegant training method for one of the methods (the multi valued method) is proposed. So I would recommend for accpet.<|endoftext|>**Post rebuttal:**I would like to thank the authors for addressing my concerns. Choice of graph connectivity   I am missing a discussion on the different choices of encoding CSPs as graphs. Writing style   the main technical part of the paper, in which the graph constructions and the message passing are described, is written in a very dense way and is difficult to read. The current figure is nice but not enough. After reading it several times I think I understand what you mean but I think this part can benefit from a revision. Perhaps add an illustration?<|endoftext|>The paper is well written, the approach makes sense, but it does not feel like they are actually accomplishing their goal of not explicitly encoding the constraints (and learning purely from the solved instances). However, I m not sure that the authors succeed in their stated goal of "implicitly learning underlying constraints using only solved instances". At this point, the amount of effort in describing the graph seems comparable to the amount of effort that would have to be put in to actually creating a SAT instance of the problem (which could be solved by a SAT solver). They exploit the graph size invariance of GNNs and convert a multi class node classification problem into a binary node classification problem, and use a generalization of the message passing approach of Palm et al.They show experimental results for their two solutions, and discuss the reasons for the accuracy / scalability tradeoff across the two.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The presented work is can be seen as a more sample efficient improvement/augmentation of the Visual Foresight method (Ebert et al.) in the context of transfer learning for robots. The authors present the notion of robot aware models   the overall dynamics model is explicitly decomposed onto a known analytical dynamics model for the robot and a learned dynamics model (from pixels) for the objects in the scene. This in turn allows us to exclude any robot related pixels from the scene observations and provide the learned dynamical model only with object related pixel information, effectively making the learned model invariant to the robot s appearance. * The overall idea is well explained with a sufficient number of illustrative figures and experiments, both in simulation and in the real world, across multiple platforms and datasets (e.g.RoboNet).* Very interesting few shot and zero shot transfer results! However, that does not mean that P_w does not contain information about the robot s movement too   it operates over the masks of the robot in the images, which evolve over time. However, given the robot aware nature of the work, keeping the world fixed is a fair assumption for this paper s contributions. While there are minor details to be elaborated on and fixed, the idea itself and the evidence provided here are sufficient for publication and are a useful contribution to the field.<|endoftext|>This paper proposes a robot aware RL based approach to transfer learning from a robot to another. The approach factorizes the model into two models, a robot model and a world model. The method is evaluated against several baselines on simulated and real data. The paper claims that it permits zero shot transfer onto new robots for the very first time. This is not the case, as the literature review itself shows. The paper does not provide enough technical details about the models  implementation. It is therefore hard to conclude whether the approach is technically sound. This approach is debatable, as the appearance of the robot and of the world could be very similar on this metric in the real world. For these reasons, I advocate for a reject. EDIT AFTER DISCUSSION PERIOD: The authors have addressed the majority of my concerns. Reading their answers and the discussions with other reviewers, I updated several scores upward and my final score to "marginally above".<|endoftext|>This paper addresses the problem of how to leverage data collected from on a robot to reduce/remove the need for robot specific data. The authors propose a model based RL policy based on coupling robot agnostic and robot specific dynamcis modules. Experiments with simulated and real robots are demonstrated, showing transferability of visual model based policies. The paper works on a quite interesting problem and provides an intuitive approach. Overall, it is a bit hard to follow the details of the methodology in the main text, especially in terms of presenting the novel contributions in this paper. The algorithm for testing in Appendix definitely is helpful, is it possible to add one for the training phase to summarize? Real robot experiments seems to be limited, 50 pushesfor few shot experiments, 30 for zero shot. The paper addresses an interesting problem, presents good results, needs some clarifications.<|endoftext|>The paper presents an extension of previous works on training a visual policy to control a robot arm in a manipulation context. The visual dynamics are used to reach a desired visual state using an existing technique: visual foresight (VF) by Finn & Levine, 2017 and Ebert et al., 2018. First, authors propose a technique to learn a decoupled visual dynamic model: the world model (i.e.a model to propagate forward the world observations consisting of an image of all objects in the scene excluding the robot) is decoupled from the robot model (i.e.a model to propagate forward the end effector pose and a mask which represents the robot body in the visual space). The basic idea is to use such model to analytically compute a mask to decouple the robot specific visual appearance from the task relevant world appearance. An important limitation of the paper is that authors fail at explaining if in the world observation the robot information is completely removed from the information related to objects present in the scene (which is what I was expecting as a natural improvement from the original VF paper). We propose to edit all images ow   ProxyRobot(o, M)".). "The current and future mask are concatenated with the image, and the end effector pose is tiled into the latent spatial embedding". This sentence isn t clear. Even after reading the paper carefully, it s still unclear to me if the world model proposed by the authors is mainly used to propagate forward the information of the state of objects in the scene. Considering that the authors rely on an analytical model of the robot, this is an important requirement and I would like to be convinced that either this is already a property of the proposed world model or that this is not the case for good/understandable/sound reasons.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper presents a contrastive learning approach for unsupervised text retrieval. These papers should be cited here. Also, [4] reports some issues Results Section:7. The authors should pay careful attention to how they are positioning their work in relation to the previous well known work on this topic. This is not correct. It was first done by the ICT paper and later DPR applied their model for supervised contrastive training. and then contrast your work with the results of previous work. First, it should be clearly mentioned that the the view 1 is one sentence while the view 2 is all the remaining sentences in the paragraph.<|endoftext|>This paper proposed a contrastive learning based method for zero shot dense IR. Evaluations are performed on the BEIR benchmark, with the best average result achieved compared to a list of IR (both sparse and dense) baselines in the zero shot setting. The zero shot evaluation and comparison on the BEIR benchmark look good. Cons:  Using contrastive learning in dense IR is not new. Duo to the novelty and insufficient experiment, I think the paper should be further refined and improved.<|endoftext|>This paper studies pre training of Siamese Transformer encoders for the zero shot dense retrieval problem. While the empirical results seem promising, there are very little techincal novelty in this work. The contrastive loss for learning Siamese Transformers is not new. the author should also compare with other pre training frameworks in Figure 1, including ICT, REALM, and SimCSE.<|endoftext|>The paper addresses an important problem for neural models for IR: what unsupervised training task is suitable for an IR model? The existing strategies such as masked language model and ICT fit some of the NLP tasks, but not necessarily the IR task. This paper proposes to use cropping to train a model for IR. The experimental results are convincing. However, the question is about the necessity to build a complexe neural model to compete with much more efficient BM25 model in the zero shot learning situation.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The paper describes a generic class of algorithms that decompose the reward signal into multiple channels and also map the value function into another generic space via arbitrary functions. They show that known algorithms from the literature are instances of this convergent class of methods. The paper presents these in a clear and simple way, which is very nice. The reward decomposition is linear. It is only here that the reader is explained why "Assumption 2" was introduced all the way at the beginning, along with a lay word interpretation. The experimental section does not explain the results. The paper presents an interesting idea of viewing value decomposition by mapping in a different space after separating the reward functions. This framework appears to be useful at least in the Atari domain, but the specifics of the experiment and interpretation of results would benefit some more clarity.<|endoftext|>These individual channels are then mapped into a new space, similarly to the log Q learning approach of van Seijen et al (2019). Additionally, some of the theoretical assumptions in the original log Q learning paper are softened. The main contribution of the paper is a framework for mapping multiple reward channels to a "nicer" space in parallel. I find this general concept and framework to be both interesting and novel. And while a reason for that performance is provided, this behaviour is never demonstrated experimentally. In particular, I could imagine something like a toy domain setup that has a top down view of the reward function and the effect that various choices of mappings have on it, as well as the resulting behaviour when the value functions are learned in the mapped space compared to the original reward function. I m very positive about the framework and theoretical results, but slightly down on the empirical ones, since they do not speak to the particulars of the framework.<|endoftext|>This paper proposes a new RL algorithm that contains two principles of value function mapping and reward decomposition. The paper also provides generic theoretical results to backup the theory. The paper also demonstrates this idea on the suite of Atari 2600 games. The strength:   New general value mapping that generalizes from previous work e.g.Log Q learning  The orchestration of value mappings and decomnposed reward that can allow the above general value mapping. In overall, the paper is well written and pursues an interesting research problem. It s too technical comparing to the contributions in Sections 2 and 4. It will be more useful if there are more choices of $f$.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; 2.In Table 2, the method based on StyleGAN2 with word conditioning and DAMSM loss is the selected best method proposed by authors, shown in Table 1. The paper evaluates the implementation of hyper networks on text to image generation, for the technical novelty, it is not quite significant. The main technical contribution is the implementation of the hyper networks in text to image generation tasks, I would expect to have more discussion about this implementation, like why text conditioned hyper network can improve the text to image generation task.<|endoftext|>After rebuttal Thanks for authors  response. I still feel negative about this paper. One reason is that the proposed method is simple, which seems to use HyperNetworks for text to image generation. It looks interesting.<|endoftext|>The paper proposes a hyper network based conditional approach for text to image synthesis. For a purely empirical paper, the authors have not convinced me their hyper network based modulation is better than a simple channel wise StyleGAN2 style modulation. In these experiments in the rebuttal, https://openreview.net/forum?id z 5BjnU3 OQ&noteId  oi3O8JQB H, the authors have shown that thier model beats a StyleGAN2 baseline with sentence conditioning on CLIP R. But they have not run their StyleGAN2 baseline with word conditioning / DAMSM loss, which makes me a bit sceptic about the gains with the hyper network conditioning framework as opposed to just using the StyleGAN2 baseline.<|endoftext|>The paper proposes a novel approach for text to image synthesis. They also show how to apply word level modulation through their Hypernet. The experiments evaluate their approach on both StyleGAN2 and INR GAN on the COCO and ArtEmis datasets. The new approach outperforms current baselines (e.g.DM GAN and DF GAN) based on FID and CLIP R which are arguably the two most important metrics. You state that this is more powerful than the original approaches of directly conditioning the model on the text. It would be useful to have concrete examples of how/when your model is better or at least motivate in which settings your model would be (theoretically) better than current approaches.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; In the experiments, they showed that performance in some tasks can be significantly improved by cleverly choosing class priors. Is there any principle about it? Minor: The authors should mention that q_i(l) is an energy function (logits) instead of a probabilistic distribution, otherwise they should have \sum_j q_j(l)   1 and the derivation would be in a different place (e.g.the \sum_j q_j term won t exist). In this regard, the citations to cost sensitive learning literature and comparisons are vastly insufficient.<|endoftext|>The main idea of this work is to build an implicit generative model from a probabilistic label prediction network, which is then trained with an ELBO loss. The experimental evaluation validates the efficacy of the proposed learning strategy on the aforementioned tasks with comparisons to the prior works. The paper proposes a new strategy by constructing an implicit generative model that incorporates the weak label priors in a unified manner. Note the summation within the log should be over i instead of l.   As discussed in the paper, there are potentially different degenerate solutions for the loss function, and it is unclear in which conditions those solutions can be avoided. The performance is much worse if the network is trained from scratch.<|endoftext|>The paper addresses an impressive range of learning settings under the same formulation. The paper is clear and well presented. On the positive side, I like how the authors connect fields that are in general not address under the same umbrella frameworks, which makes their approach very complete and appealing. Also, I wanted to highlight the very thorough experimental design and analysis. On the negative side, I would have liked to have seen a more elaborate discussion of the relationship with existing methods for each of the problems the paper addresses. The literature review lacks a bit and it makes it difficult to assess the overlap with previous works. Also, although the experimental work is very good, I was missing any form of theoretical analysis of the proposed approach. There is room for improvement on the theoretical analysis and related work.<|endoftext|>Especially in the introduction section, the background of the problem and the motivation of introducing implicit generative models are not well stated. The experiment validation still has room to improve. It should be better if the authors can provide discussion on limit and failure cases of the proposed method existing in different tasks. It has a very positive impact to the AI community. However, the paper writing and organization, experiment design and presentation still have room to improve.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The paper extends and modifies the expected improvement (EI) algorithm for contextual bandits. In addition, since the uniform constants in the proof do not have clear dependence on the important parameter $\delta$ and there are so many typos, it is a bit hard for me to evaluate the correctness and significance of the main results. To show the technical novelty, detailed comparison with these references is needed. Specifically, LinEI and NeuralEI are proposed for the linear reward function and general reward function (under some assumptions), respectively. There are some existing papers that provide regret bounds for EI type algorithms, for example, [1] and [2]. I think citing and comparing with these papers is necessary. The paper claims that LinEI and NeuralEI achieve the state of the art regret bounds and they work well on both synthetic functions and benchmark datasets. Basically, LinEI and NeuralEI only sample the arm with the largest EI value if and only if the largest EI value is larger than this threshold function; otherwise LinEI and NerualEI sample the arm with the largest posterior reward mean. Do regret bounds still hold without this modification? If not, can the authors elaborate more on the technical difficulties as well as how this modification solves the issues? If so, the uniform constant $C$ also depends on $\delta$ and then the final regret bound could have worse dependence on $\delta$. 4.In Equation (2), it should be $s_{i,t}$ instead of $s_i(t)$.<|endoftext|>This paper applies expected improvement to contextual bandits to balance exploration and exploitation. The authors proposed LinEL for linear rewards and NeuralEI for general rewards using neural network for approximation. Theoretical analysis claimed that compared with UCB based approaches, LinEL s regret bound reduces a factor of $\sqrt{\log(T)}$. While several works analyzed EI in Bayesian optimization and best arm identification, this paper provides the first analysis of EI in contextual bandits, which I believe is the most significant contribution. Still, I would suggest the authors to include more discussions on the technical challenging in combining EI with contextual bandits (e.g., new tools compared with EI for best arm identification). Theoretical results, e.g., regret bounds of LinEL and NeuralEI and the claimed $\sqrt{\log(T)}$ regret improvement, are arguably the second contribution of the paper. 1.The regret of LinEL should be $O(d\sqrt{T\ln(T)\ln(T^3)\ln(T/\delta)})$. This should be intuitive as when bounding the regret, $\sum_t s_{a(t),t}$ contributes $O(\sqrt{dT\ln(T)})$ following Lemma 3; when $\beta < 3$ we have $\sqrt{2\ln(RT^\beta)} \leq \sqrt{2\ln(RT^3)}$ and $v_t$ contributes another $O(\sqrt{d\ln(T/\delta)})$. The authors should revise the proof to reflect $\lambda$. I am open to increasing my score if the authors could fix the problems and make the analysis rigorous.<|endoftext|>The paper introduces and studies the expected improvement (EI) technique as a way to balance exploration and exploitation for the contextual bandit problem. The authors propose two EI based algorithms for linear bandits and for neural bandits for a general class of reward functions. The paper presents regret bounds for both methods and shows the experimental results to support their theoretical claims. The EI idea has been around in Bayesian optimization but has not been adapted to contextual bandits. I am not sure what the analysis of NeuralEI adds to the contribution given by LinEI. Post reponses  Thanks to the authors for the responses, I am staying with the current score.<|endoftext|>The authors study regret analysis of the expected improvement (EI), a popular but theoretically understudied technique to handle the tradeoff between exploration and exploitation in bandits. They propose two novel EI based algorithms for this problem, one for linear payoff and for deep neural networks. With a linear reward function, we demonstrate that our algorithm achieves a near optimal regret. In particular, our regret bound reduces a factor of \sqrt{log T} from UCB. They also present numerical studies using the proposed algorithm. Although it is debatable whether the upper bound is improved from Thompson sampling since the proposed algorithm does not use any randomization, it has a better upper bound than Thompson sampling by \sqrt{dT}. The authors may give a more intuitive explanation of how \sqrt{d} is shaved compared to Thompson sampling. This paper is well written and shows novel contextual bandit algorithms using expected improvement with new regret analysis. This could be a nice contribution to the bandit community.<|endoftext|>This work proposes two Expected Improvement (EI) based algorithms to solve the regret minimization problem for the linear and neural bandits. It provides regret analysis for both algorithms and also comparison to some existing algorithms. Generally speaking, this work is well organized and it is interesting in the sense that it is the first the analyze EI based algorithms for RM in linear bandits. Besides, it shows that the theoretical and numerical performance of the two proposed algorithms. My major comments are as follows:1. Despite the existing discussions, it would be better if the author(s) can further highlight the difference between the proposed algorithm in Qin et al.(2017) and also the challenge to derive the bounds. Moreover, I suggest to compare to the bounds in "The End of Optimism? An Asymptotic Analysis of Finite Armed Linear Bandits" (http://proceedings.mlr.press/v54/lattimore17a/lattimore17a.pdf). Both theoretical and numerical comparisons should be possible. 4.The numerical experiments do not consider a traditional bandit problem.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; The proposed method can be used to the context of  pyramidally structured data with good inference quality and a favorable training time. The paper is not very reader friendly, so it would be better that authors can provide the introduction of some basic concepts and more related works in their final version to improve the readability of their paper.<|endoftext|>This work introduces ADAVI, an approximate inference algorithm for hierarchical Bayesian models (HBMs). Experiments demonstrate the applicability of the method on HBMs of increasing complexity, including a challenging neuroimaging model. My main issue is the lack of quality checks of the approximate posteriors produced by ADAVI. This should matter the most, in my opinion, well above efficiency, wall clock times, and the number of parameters. For now, I do not recommend this paper for acceptance, but I will be pleased to change and increase my evaluation if the authors can present diagnostics of the resulting approximate posteriors.<|endoftext|>Weaknesses:  The proposed ADAVI method only applies to a pyramidal class of Bayesian networks in which dependency structure follows a pyramidal graph. Furthermore, it would be nice to also compare quantitatively the time complexity and inference quality of ADAVI with other alternatives when applied to neuroimaging data. The authors claim the proposed method instead provides more expressivity while failing to show this point in the experiments.<|endoftext|>The paper tackles approximate inference for hierarchical Bayesian models with fully nested structure. The benefits of the model are illustrated in a few synthetic experiments as well as in application to a human neuroimaging dataset. The nature of the contribution is likewise clear. I think the work is below the bar now and am rating accordingly, but I think my concerns should be sufficiently addressable in rebuttal for the work to be appropriate for the conference.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper demonstrates that LSeg achieves comparable performance as state of the art few shot semantic segmentation networks on FSD 1000 even though it is used in zero shot setting. LSeg uses text encoder from CLIP ViT B/32 which is frozen during training while the weights of the image decoder (DPT with a ViT L/16) is updated to maximize the correlation between the text embedding and the image pixel embedding of the ground truth class of the pixel. The biggest concern is that ViT L has 307M parameters while the largest backbone used in comparisons is ResNet 101 which has about 45M parameters.<|endoftext|>This paper uses the pre trained large model (CLIP) to transfer the language knowledge to the unseen labels for zero shot semantic segmentation. The idea of distilling knowledge from pre trained models is reasonable and becomes a new trend for many zero shot settings. Weaknesses:  The method is not that novel.<|endoftext|>This paper is about semantic image segmentation, the template weights of each category is generated by language text. This method is similar to image classification of CLIP on ImageNet 1k, where the template weights of each class derived from language text, and also called zero shot learning. More ablation study necessary. 2 In the revision paper, I still cannot find the training loss. softmax is not a loss objective.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper proposes DeCLIP with three additions to the original CLIP model: (1) single modal self supervised pre training; (2) cross modal contrastive pretraining across multiple views and (3)  cross modal contrastive pretraining across nearest neighbors in feature space. DeCLIP is able to achieve higher performance on most of downstream datasets with much less pre training data (80M vs. 400M). This paper provides three simple yet effective additions to CLIP and show its strong performance with much less pre training data. Visualization show that nearest neighbor pairs can provide reasonable similar supervision signal compared to the original pair.<|endoftext|>## Strengths  (+) The proposed DeCLIP shows better ImageNet zero shot performances than the original CLIP models with the same parameter size but smaller training data size. In other words, I would like to see "reproduced CLIP results" in Figure 1 using the authors  implementation. I wonder whether the performance improvements are from the data collection, not the proposed method. All technical components of DeCLIP are hard to say novel; self supervision uses SimSiam and masked language model, multi view supervision uses the EDA augmentation, nearest neighbor supervision can be viewed as a self distillation method. However, the combination of known techniques can be novel if the empirical contribution is significant. I think this paper has a limited borderline novelty, but the empirical contribution can be a strength.<|endoftext|>DeCLIP improves data efficiency. Pro:This is the first work that I can think of that successfully combines self supervision and supervised training, at such a large scale. This work provides strong empirical evidence that self supervision can be combined with supervised data. The authors provide fair baselines of CLIP in Figure 7 and Table 8 (in the appendix) but the dataset scale is limited. Analysis lacking on why we need self supervision signals / nearest neighbor supervision.<|endoftext|>In this paper, the authors mitigate the data hungriness of the CLIP model. The authors propose three directions: single modality self supervision; multi view multi modality contrastive learning, and nearest neighbor supervision. With the proposed three components, the authors can achieve better or comparable results with CLIP with more than 4x fewer data. Pros:  Clear advantage of the proposed framework on data efficiency. Simple and straightforward methodology. Carefully designed experiments. In addition, is the augmented view the same size (in terms of network input) as the original view but just more zoomed in? Potentially, with more data, this method can achieve much more superior performance than CLIP. Post rebuttal update:  I am very happy with the additional results, which address most of my concerns on fair comparison with CLIP.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The empirical performance of the method is strong but I would like to see the limitations of the method along with a discussion of trade offs. This paper proposes a way to obtain sparse neural networks by repeatedly growing and pruning parts of the model in a scheduled fashion.<|endoftext|>This study provides one new training method of sparse model by scheduled grow and prune (GaP) methodology. With step by step growing and pruning, most of the weights are close to $0$ and will be pruned in the next step. The authors provides  a series of experiments.<|endoftext|>Such a design can help to reduce the time/computation cost during training. + There are rich and solid experiments provided in this paper. Moreover, the performance of dense pre trained model pruning is better provided for comparison. The discussion of related works is comprehensive to include most of the recent and classic works.<|endoftext|>This paper presents a scheduled grow and prune approach to produce a sparse CNN model. In particular, a sparse model is achieved by repeatedly growing and prune subsets of layers during network training. The theoretical convergence of the C GaP is analyzed.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper introduces DISk, an information theoretic skill discovery method that discovers skills in a sequential fashion instead of all at once. Providing agents with the ability to continuouslly acquire new skills in evolving environments is an important research direction. Results are strong and show that DISk can discover more diverse locomotion behaviors than DIAYN and Off DADS, both under fixed and changing environment dynamics. However, if we want to truly develop agents that can discover skills autonomously, we need to start considering more complex environments. Note that previous works have considered unsupervised pre training on Atari (e.g.VISR, APT). For instance, a version of DADS where the policy is checkpointed before every change to the dynamics. This is a very relevant work that combines an intrinsic reward proportional to the magnitude of the speed of the robot with a mutual information regularizer that makes skills distinguishable from each other. Could authors please provide their intuition for why this happens?<|endoftext|>The authors have presented DISk, an unsupervised skill discovery method that incrementally learns a sequence of skills. This allows DISK to adapt to changes in the environment or agent dynamics during training. 3.This paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework. I hope the authors can elaborate on this. 2.Also, I m not particularly convinced by the ablations performed in the paper. The methods that the authors propose is novel. Specifically, I d learn more about its computational efficiency and see additional experiments and ablations.<|endoftext|>They compare their approach to DIAYN and off DADS in both static and dynamic MuJoCo environments in OpenAI gym. I do believe there are good ideas here, however the paper is not ready for publication. **Unclear motivation**: The authors motivate their approach by describing skill learning in a slowly evolving environment, where it is important to remember skills for every version of the environment along the way. That said, it is obviously a baseline of interest for future versions of this work, and would make for good discussion in the present related work section. This alternative take on fixed interval skill expansion is also an important baseline. Ideally, I would like to see *each* combination of these features tried with each baseline, however that be a lot to ask and for example how exactly to adapt DIAYN to learn skills incrementally involves a lot of non trivial choices.<|endoftext|>The paper proposes an approach for unsupervised learning of diverse skills by maximizing an intrinsic reward function. In experimental evaluations on OpenAI gym environments this leads to (a) improved diversity in discovered skills and (b) the ability to learn skills in continually changing environments, where prior approaches struggle. The writing of the paper is clear and the method easy to understand. Post Rebuttal After considering the authors  rebuttal I have increased my score to recommend (weak) acceptance. Priors works will continuously try to adapt all skills to the current environment and thus do not need this additional skill filtering supervision. The paper argues that the baselines work better than the proposed method in the cheetah environment since they share parameters, which improves learning efficiency.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Why not use such a protocol for the main task? The algorithm novelty is low and the experimental part relies on questionable protocols. Since the paper is to introduce a new problem, a justification of the benchmark is expected.<|endoftext|>Following this, the network is given supervised training to fine tune the representation. *** Positive aspects:  The paper is clear, and well written. In this review, I am not arguing for novelty for novelty s sake. I am rather just mentioning that the paper consists in introducing a variation on a classical learning problem (few shot strategies with attributes have been extensively studied in the literature), which is then solved using techniques that were designed for related problems. How are hyper parameters for each of the 3 parts optimized? All together or separately for each?<|endoftext|>The experiment results indicate that the proposed model leads to consistent improvement. This paper introduces an interesting topic and performs extensive experiments to verify the conclusion. However, the technical novelty is hindered since most of the contributions are not new.<|endoftext|>This paper proposes the problem of few shot attribute learning (FSAL) that follows the standard few shot/meta learning paradigm but focuses on attributes instead of object classes. ## More detailed comments  Setting: The claim that the setting with attributes is different from the standard few shot setting on object classes is sound. Novel and potentially useful setting to study but some concerns on execution related to benchmarks and how we should interpret the results.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The authors revisited open vocabulary problems in translation with a character based approach. They claimed that segmentation approaches such as BPE may be affected by the vocabulary size, which is heuristic. To achieve a good trade off, this work explores quasi character level Transformer models for machine translation tasks. They find that the quasi character level MT outperforms their large vocabulary BPE counterparts under the low resource settings. However, the author merely shows some old news for the MT community, such as character level NMT works for extremely low resource settings. Strategies that work for the rich resources are more likely to be adopted by the community. The authors disclose some foundation information, e.g.SELENE project No. 871467 and DeepPattern project No.<|endoftext|>Performance measured across different domains and NMT architecturesThe experiments show that smaller subword vocabularies could lead to better performance when training data size is small. Another result is that catastrophic forgetting is more prominent when finetuning models with smaller vocabulary sizes. However, there are several issues in the experimental protocol used, which make the presented conclusions in the paper non definitive. There are several issues in the experiments that are of immediate concern:1. Since, the handling of long tailed tokens is described as one of the primary modeling objectives of this paper, the exclusion of this standard technique seems to be an important omission. This deviates from other works (e.g.https://arxiv.org/pdf/2010.04924.pdf, which studies long tail in NMT reports 10K vocabulary for training data sizes ranging from 10K to 200K and https://arxiv.org/pdf/2004.02334.pdf, which studies subword vocabulary size shows that larger vocabulary size significantly impacts translation under low resource settings). The paper studies an important problem, but the experimental protocols adopted do not allow any conclusive results. The main claim on the benefit of quasi character level vocabulary in low data regime (over a well selected subword vocabulary size) isn t justified by the presented experiments, due to the issues pointed out above.<|endoftext|>The experiments on domain adaptation, catastrophic forgetting and vocabulary flexibility are potentially useful for practitioners. Weaknesses:  The work ignores a substantial body of work on character level translation; there seems to be an abrupt cut off at 2017. For example, Cherry et al (already mentioned; https://arxiv.org/abs/1808.09943), emphasize that character level models have their greatest advantage when data sizes are small, and Sennrich and Zhang (https://aclanthology.org/P19 1021/) show that reducing vocabulary size improves truly low resource NMT. The main contribution of this paper seems to be the suggestion that being quasi character level allows one to reap the benefits of character level modeling without paying the computational costs, but this is never demonstrated explicitly. This paper adds to the ongoing discussion of character level versus subword level MT, providing more data points in more architectures indicating the character level advantage with small training set sizes. It also adds an interesting point about domain flexibility. Overall, the experimental contributions do not seem to be sufficient to warrant a main conference paper; the results on low resource scenarios were expected, and the results examining domain flexibility and forgetting are not enough to carry a paper.<|endoftext|>I d suggest having some experiments trying different vocabulary sizes and comparing the quality. Moreover, the authors criticize the character based NMT to be slow and inefficient. I.e.comparison of the average tokens after tokenization with standard subword vs quasi character vs character based. I think this experiment will give a good insight into the performance between approaches. is not entirely true. Prior research ([Optimizing Transformer for Low Resource Neural Machine Translation](https://aclanthology.org/2020.coling main.304/) and [Revisiting low resource neural machine translation: A case study](https://arxiv.org/pdf/1905.11901)) show that a lower vocabulary size is better for low resource NMT, which is the same conclusion as your paper. This paper is only comparing 2 approaches: standard subword tokenization vs the proposed quasi character. There is also no report about the performance, despite the author criticizing the performance issue about character based NMT.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; In this paper, the adversarial attack algorithms have been adapted to be applied to event based data for spiking neural networks implemented on neuromorphic hardware. 1.It is not clear how the adversarial attack algorithms that are commonly used for discrete data are applied to event based data in this work. 2.In this work, the novelty is low because the authors use already existing methods applied to an unexplored system. 3.The experimental setup is not clear. Details like training hyperparameters, ANN SNN conversion parameters, and HW level constraints of the neuromorphic chip that has been considered in the implementation. 5.For all the attacks, in particular for the universal attack, the stealthiness, i.e, how much the adversarial examples are noticed by the human eye, should be discussed. 6.The results can be extended with experiments in which adversarial defenses, such as adversarial training or noise filter, are applied to improve the robustness against the attacks. This paper tackles a very important problem with rigor and soundness. However, some key points need to be addressed (see the main review section).<|endoftext|>The paper presents proof of the viability of adversarial attacks on CNN processing event based vision data. Strengths:+ overall this is a solid paper, without unjustified claims, and with a straightforward message: adversarial attacks on event based vision works+ the paper is well written, the figures are clearly showing the effects of the adversarial attacks+ a variety of different attacks is tried out, including adversarial patches+ the paper is to the best of my knowledge the first to try out the effect of adversarial attacks on neuromorphic hardwareWeaknesses:  as the authors note, there is concurrent work by Marchisio et al.2021, which investigates a very similar question, and even goes beyond that by investigating potential defenses against adversarial attacks. Therefore the novelty is severely limited. the results of the paper are not really surprising, and I would have hoped for more specific comparisons how adversarial attacks differ between conventional and event based vision, and also some outlook into defensesWhile the paper is formally correct and well written, there are other published papers that have shown the main claims of the paper, and have even gone some steps further by analyzing possible defenses against adversarial attacks. There are some new elements, e.g.the analysis of attacks on real neuromorphic hardware, but overall this is too little for acceptance at ICLR.<|endoftext|>This paper investigated how to adapt existing attack strategies in order to work with the discrete nature of event based data. Besides,the authors validate the adversarial examples on neuromorphic chip and the perturbation is effective after modified the networks. In a more realistic setting, the authors use adversarial patches to trigger a targeted misclassification. Strengths:This paper draws a conclusion through rigorous and sufficient experiments, which provide quantitative results to prove the effectiveness of the attack algorithms on dvs data. How to deal with the discrete nature of event based data and their dependence on time is explained clearly. Also, the authors verified the effectiveness of adversarial examples on neuromorphic chip for the first time. This paper verified the attack algorithms can be adapted to the dvs data. The authors proposed a next step, which building real world patches that can fool networks.<|endoftext|>Authors present white box adversarial attack algorithms for spiking neural networks (SNNs) with event based visual data inputs, and experimentally demonstrate vulnerability of SNNs in various settings. The paper is nicely written and organized from an empirical contribution perspective. as it seems to demonstrate some inherent robustness. The paper is nicely written, and presented experiments and visualizations are well organized. Empirical validations of the proposed methods on neuromorphic hardware add a significant value to the paper with respect to the state of the art. On the other hand, methodological contribution of the paper is rather shallow, and Section 2.1 is not describing the used methods in detail. However authors also do not evaluate their attacks in comparison to the state of the art PGD like attacks with spike compatible gradient proposed for SNNs by [Liang et al, 2020]. It would be more suitable to compare the same SNNs and proposed attacks with previously presented PGD variant attacks for event based data [Liang et al., 2020]. These observations can be discussed in the revisions. Minor comments:  Authors report that the SNN used for IBM Gestures has a test accuracy of 84.2%. Authors state that 50 iterations of PGD was sufficient and results did not improve afterwards. It would be interesting to plot this observation in the S.M.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper presents a new self supervised learning objective that aims to further adapt the previous decorrelation based [1] contrastive learning method [Bardes, 2021], which largely alleviates the trivial solutions in SSL. The empirical experiment shows only incremental performance gain over the prior arts like Barlow Twins, yet the proposed Zero CL has an obvious complexity advantage as it s not dependent on the feature dimension. Does this indicate that there s a conflict between the instance/feature wise objective? Vicreg: Variance invariance covariance regularization for self supervised learning. The only concern from the reviewer is whether the complexity advantage is a sufficient contribution over the previous regularization method [1], given that Zero CL shows only incremental performance gain.<|endoftext|>Typo: eq 9 Z  > H.The paper is satisfactory. This loss function is somewhat new. The work is good, though at this stage, the authors have only been able to show it gives satisfactory results. The negative samples are unrelated to the positive samples  in the loss function.<|endoftext|>The paper is concerned with preventing the model collapse in the self supervised learning scenario using contrastive losses. Can this approach scale well to larger models too? Overall the idea looks clear and efficient, it s not clear however how problematic for the current approaches the collapse is and some additional clarifications could strengthen the work.<|endoftext|>I am also not clear about the complexity reduction, which seems to be claimed as a very important contribution in this paper. Minor issues include some typos in equations and the discussion of low rank property and ZCA background. Overall, it is a good paper. The key concerns came from the symmetric framework.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; The paper investigates sparse training to accelerate on device federated learning. Indeed, for more realistic deployment and to move federated learning towards practical systems, it is important to accelerate the on device training (and not just the inference which is mainly the current practice). The main weakness of this paper is the experiments section. The results are presented only on CIFAR 10 dataset and do not consider many other datasets from Federated learning benchmarks (e.g., LEAF https://leaf.cmu.edu/). If the experimental evaluation was comprehensive enough, this would be a very good paper (given the interesting and important problem it addresses). Some discussion and possibly performance numbers are needed here. Even though the reviewer is quite happy about the problem addressed in this paper, it lacks a solid experimental section. Post rebuttal:I have read the author response and other reviews. Thanks to the authors for a detailed rebuttal and new experiments. It addressed my concerns and so I have increased the rating from 5 to 6.<|endoftext|>The paper studies the problem of adding sparsity to local training in federated learning. Under the approach each node uses only the top K weights for computations in the forward and backward pass of training (and only top K activations for computations in the backward pass). To mitigate this issue the authors propose to combine sparse training with sparse aggregation since their empirical study suggests that the accuracy drop may be due to dilution of weights that are active in only a few nodes. 2.The proposed solutions are good initial approaches backed by the empirical study and do appear to lead to some improvement in accuracy over the naive approach. For a paper that relies entirely on empirical evidence for its claims, the empirical study appears a bit limited as it considers only a single dataset (CIFAR10) and a single model architecture (ResNet18). 2.The experimental results are also a bit limited. The authors introduce an interesting and important problem but given that the paper relies purely on empirical evidence for its claims the current evaluation is quite limited and not enough for acceptance. I have increased my score to reflect the same.<|endoftext|>This work proposes ZeroFL; a method that allows for sparse neural network training at the edge along with reduced upload communication, both of which being important aspects in federated learning. Overall, while ZeroFL is simple, it does seem to be a straightforward extension of SWAT to the federated setting with a very minor modification. The SWAT method works by setting a target weight sparsity percentage (denoted by $sp$) which is enforced in both the forward and backward pass by a top k operation on the weights. They argue that with the extra $r_{mask}$ weights, a “cleaner” training signal from the clients can be given as these weights are not “corrupted” by the weights of the other clients when averaging at the server. Nevertheless, I believe this submission needs quite a bit of work before I can recommend for acceptance:  There are comparisons with other relevant baselines missing; e.g., federated dropout (which can be done on the weight level as well) is missing. Furthermore, the authors discuss how SWAT also employs activation sparsification but they seem to not use it at all in their experiments. The overall state of the experimental evaluation is a bit weak; there is only one dataset considered where the number of clients is relatively small (100). The authors don’t talk about the extra cost of transmitting the indices of the non zero values in the main text (although they do mention the CSC format in the table caption), which can be an important additional communication cost in the case of unstructured pruning. Could they perhaps elaborate more? Sometimes the authors refer to top K and sometimes to top K%.
Reject; rating score: 1; rating score: 1; rating score: 1; rating score: 1; The authors try to propose a `Prior Knowledge Layer  as an alternative to the CRF layer for the standard sequence labeling framework in NLP. This is not a paper, instead technically more like a toy report. Also, the idea of prior knowledge integration is naive. Introduction: without a motivation; Related work: not informative; Method: no detail & incomplete; Experiment: missing.<|endoftext|>This paper proposes a prior knowledge matrix to displace CRF in sequence labeling. Besides, no experiments are found in the paper. It seems that the authors have submitted an incomplete version.<|endoftext|>The authors do not validate any claims made in the paper with experimental results, benchmarks, or datasets. Strengths:  this paper attempts to address an important problem in sequence labelling of data sparsity, as well as a weakness of CRF classifiers. The paper has no experimental section   no benchmarks vs current state of the art, no ablations for the PKM layer, and no validation of any of the claims made about the effectiveness of the PKM layer.<|endoftext|>The paper formulated an approach to impose valid state transitions from t to t+1 for sequence labeling problems. Strengths:  The authors addressed state transition problems in sequence labeling problems, and proposed an approach that seems sound. Weaknesses:  There is no experimental results that validates the proposed approach. [1] Tsuboi, Yuta, et al."Training conditional random fields using incomplete annotations."
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; 2017.Overall, the authors’ proposed approach is novel and their empirical results show the effectiveness of their approach on several datasets. A meta classifier on top of these features can be trained using a small clean source target dataset to avoid per language tuning. The filtering criteria rely more on hand crafted thresholds like source target length ration, and sub word overlap which would require per language pair tuning to get a decent result.<|endoftext|>The paper proposes a fine tuning method for multi lingual sentence representations trained on monolingual data, such that the final representation is more language agnostic, as well as more contrastive in the sense that artificially infused noise do not break the clustering of these representations. The experiments are well structured and provide reasonable depth in evaluation. A minor point that concerns me is the fact that it seems semantic contrastive. This is a minor point in the use of terminology there, but I do wish the authors would clarify their point of view on the subject. I have only minor concerns regarding terminologies and interpretations.<|endoftext|>Strengths:The idea of rebalancing cluster assignment between language makes sense and the improvement in Table 3 is very strong, showing why the modification to the loss is important. The improvement in Table 4 is also very interesting, showing that rank based entropy and filter suite are important to make the mining effective. Overall I found the work admirable: It is novel and results are strongWeakness:The method itself is fairly complicate.<|endoftext|>Evaluation is performed on standard UMT tasks and an in depth analysis is provided. Almost none of these scores are comparable (use of different post processing, BLEU implementations, and/or data). The paper cannot be published as long as it makes claim based on this table. I am confident that I cannot reproduce the results. I think too many details are given on the approach that are then given again latter in the paper. A good idea overall but with many details and choices difficult to understand.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; My major concern about this work is the performance of the SMILES based transformer is not worse than the graph based model. Can the author provide intuition of why the proposed methods ST KD can outperform both graph and SMILES based methods in table 2, when it is trained to minimize the difference between graph transformer and SMILES transformer (by eq 16)? The authors provide an interesting knowledge distillation framework to learn molecule embedding, however, the reviewer is not fully onboard with the motivation of doing the knowledge distillation (e.g.learning SMILES based transformer from graph based transformer), since smiles based transformer has a competitive performance. Other competitive SMILES based models should be included in the comparison.<|endoftext|>(4)Strengths:(1) Knowledge distillation between graph transformer and SMILES transformer is proposed for molecular representation learning. (3) ST KD achieves strong performance, similar to graph based models, on molecular property prediction tasks, with a fast inference time. Weaknesses:(1)	Recent SMILES transformer models trained on large data have shown performance similar to state of the art graph based models without any knowledge distillation, see e.g.Ross et al 2021 https://arxiv.org/abs/2106.09553. Such works should be at least cited in this work. While it is well understood that the SMILES transformer after KD provides the advantage of fast inference, it is not clear if knowledge distillation is the way to get to SOTA performance with fast inference, as recent pre trained SMILES based efficient transformer models are providing competitive performance.<|endoftext|>The paper targets speeding up the inference process (specifically, SMILES to graph preprocessing) in virtual screening by adopting SMILES based transformer for molecule representation learning. Numerical results claim for the competitive performance and faster inference time. Furthermore, a lot of molecule libraries are stored in .sdf format that we only need to read without conversion. I thus feel the motivation is exaggerated while I am also glad to listen to other reviewers. considering training is usually much more expensive than inference.<|endoftext|>This paper is based on an interesting idea of building a fast and high quality SMILES based molecular fingerprint generator with knowledge distillation from state of the art graph based models. The authors have described the proposed methods clearly, with their claims being supported by experimental results. Although more experiments can be performed to give a thorough evaluation to the model’s performance, I believe the paper should be recommended due to its novel idea and convincing results.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; The paper provides an analysis of priors other than standard isotropic Gaussians for Bayesian neural network. The authors first empirically estimate the posterior distributions of the weights of a BNN and then use these distributions as a prior for these BNNs. *Update after rebuttal* I have read the authors’ response and other reviews. 11.For the correlated Gaussian, it is not discussed why the Matern kernel in particular has been chosen12. Very well written paper. Figure A.12 – not sure I can agree with the conclusions about heavy tailed distributions made in the captionI vote for acceptance of this paper. I believe it provides a thorough empirical insights on the prior choice for BNNs and a recipe for future prior exploration. 2.Very thorough experiments. This submission does. The actual insight from the paper, i.e.that isotropic Gaussians are not the best choice of prior, may be not very exciting and surprising, but the paper provides a thorough analysis to show this and a recipe how to choose a better prior. I believe it provides useful and interesting discussion to the community and therefore I recommend acceptance of this paper. In the empirical analysis of weight distributions (Section 3), it is claimed that the posterior distribution reached by SGD would be a good choice for a prior.<|endoftext|>This paper presents numerous empirical facts about the distribution of the weights after training a neural network. The authors also propose, in a Bayesian setup, to compare the performance of trained BNNs with different priors. # Weaknesses * There is no clear take at home message. The consistency across models and datasets should have been checked. * There is no heuristics or explanation of the observed results (except for the influence of the data augmentation on the "cold posterior effect".# StrengthsThe experimental report is interesting for researchers who want to have a better intuition of the influence of the prior. # EDITI have taken note of the modification made by the authors in the Discussion section, which moderates a bit the main claims. But I do not change my recommendation, since the main claims (in the first sections) are still too affirmative regarding the experiments.<|endoftext|>This paper proposed to use distributions different from isotropic gaussian for Bayesian neural network priors. For fully connected neural networks the heavy tailed distributions such as laplacian and student t are used, while for convolutional neural networks multivariate gaussian with spatial correlations are employed. Cold posteriors versions of the isotropic gaussian prior and the proposed priors are also provided for comprehensive comparison. An expectation maximization procedure is provided to justify the above approaches and empirical results support the choice of the priors. Weakness:1) It would be great if the impact to the computational speed could be discussed further. The "stochastic" part of SGD is with respect to the batch sampling from the data distribution, not with respect to the weight parameters. 3) One motivation of the work from the introduction section is to impose proper prior distributions on the network weights, so that principled Bayesian inference can be carried out without temperature adjustment as in cold posterior. This somewhat weakens the motivation and impact of the paper. In summary this is a nice paper with interesting findings.<|endoftext|>The work studies prior distributions for Bayesian CNNs. The work report that conventionally used priors e.g., Gaussian poorly fit empirical distributions of the trined weights. The empirical distributions appear to be heavy tailed and correlated. The results on the cold posterior effect are also unstable between models, which is reported in the paper. That is a quite important issue that may significantly flip the results of the experiments. Overall that makes me conclude that results are not generalizable and reliable.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The authors present a new method for finding robust policies in reinforcement learning, those policies which give high rewards but also come from a diversity of behaviours. It is easy to follow and the conclusion are well supported by the results. More specifically to the method presented, the authors investigate the applicability to clustering methods for selection of policies in evolutionary strategies. This is an interesting area of study and the authors present a convincing method that performs well on several toy problems. The central comment I have on this paper is it would be interesting to see how their method navigates between clusters and how representative these clusters are of behavioural strategies. Is it possible to quantify/comment on whether the authors method is achieving this? The results give a clear indication that clustering can greatly improve results under certain settings, with good attention given to the impact on changes method parameters such as clustering algorithm and size.<|endoftext|>This paper performs research in the area of diversity/novelty seeking agents under evolutionary optimization approaches. Why not optimize an explicit diversity metric that measures the behavioral difference between the $K$ policies? The specific approach (EDO CS) by the paper suggests using a pool/"archive" $A$ of previous policies, which are then clustered via K means according to the distance metric defined by the behavior function $b(\cdot)$. I admit, I have seen a variant of the clustering approach (termed speciation) in NEAT [2], but it would significantly strengthen this paper if there was a better motivation + explanation for the use of the clustering heuristic. The results show that EDO CS achieves better performance over all other baselines. There is also a nontrivial amount of presentation issues (not just small grammar mistakes or typos) that affect the paper s scientific quality as well. The authors have addressed my questions and concerns. What should I be looking at? * Could you explain why we re using a bandit approach for $\lambda$? I m fairly confused about the involvement of bandits in this scenario. More specifically, from the cluster where the best quality policy locates, we just select this policy."<|endoftext|>The authors present EDO CS an approach to multi objective optimisation that ensures the high quality and diversity of generated RL policies. The quality and diversity are further achieved by modifying the objective function of the ES algorithm so it includes both the fitness and behaviour diversity terms, balanced with a hyperparameter $\lambda$. This hyperparameter is set using a multi arm bandit approach, which is adapted during the training. The performance of the proposed approach is evaluated on several MuJoCo continuous control tasks, as well as compared to state of the art QD benchmarks. The paper is well written with a clear goal developing a method that optimises for diversity in addition to fitness. The main contributions include sampling policies and a novel approach to adapting the optimisation function. However, I have several concerns:* The definition of a behaviour is not clear from the beginning and should be defined earlier. There seems to be a misalignment between the use of behaviour diversity in the proposed approach (mostly for exploration) w.r.t.the approaches in the literature (to maintain a diverse collection of policy behaviours). This leads to the diversity in policy parameterisation but not necessarily to behaviour diversity.<|endoftext|>In the paper "Evolutionary Diversity Optimization With Clustering Based Selection For Reinforcement Learning", the authors introduce a new selection mechanism for Quality Diversity based algorithms. This new selection mechanism is based on the K Means algorithm, to cluster the behaviour space into cells and then select only the best policies within each cell. Multiple existing algorithms are used as baselines, which is great. Overall the paper is very clear and easy to read and understand. Combining MAP Elites with a K mean to automatically partition the behaviour space was firstly introduced in CVT MAP Elites [2] and its extension Cluster Elites [3]. (2017).Quality and diversity optimization: A unifying modular framework. Additionally, it seems that the authors of the QD RL paper are still trying to publish the paper and made several updates to the paper. Updated score after discussion below. Other, smaller comments: The use of the best reward makes sense from a deep RL point of view. However, the sum of the reward of all the policies in the collection (also, called QD score) is more frequently adopted in the QD literature.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; The paper identifies limitations and common pitfalls that practitioners encounter when using the MMD metric to evaluate graph generative models. However, I acknowledge (and the authors should, too) that what is discussed in this paper is only one of the many aspects to consider when evaluating graph generators. I am inclined towards accepting the paper for its much needed practical guidance on the use of MMD in the context of graph generation, which could have very positive implications in this research field. Weaknesses:  while the content is extremely useful, I find the scope of the paper a bit limited.<|endoftext|>However the current scope and focus of the paper is a bit limited. The authors could have expanded this work via considering i) other metrics apart from MMD, ii) other descriptor functions as well as iii) working with non linear variants for correlation, i.e., mutual information. I would like to congratulate the authors for their excellent work which would benefit researchers working in the field of graph based generative models. The authors initially list desirable criteria an evaluation metric should possess and subsequently discuss the usage of maximum mean discrepancy (MMD) for model comparison.<|endoftext|>Finally, the paper describes some procedures based on MMD to avoid some of these problems. The paper has several strengths. The paper is well written even for people that are not related to this topic. The paper focuses on the main pitfalls of this measure. 3) Impact of ideas: The criticism of this measure could have a great impact on the current evaluation process, changing the way that GGMs must be evaluated. Even though the main strengths of the paper, there are some issues that reduce the recommendation score. This makes it difficult to understand some of the experiments and the effects in the final evaluation. Expand the use over other meaningful descriptor functions.<|endoftext|>This paper describes desiderata (expressivity, robustness, and efficiency) for metrics for comparing graph generative models and details the various ways that recent work has used maximum mean discrepancy (MMD) to evaluate graph generative models. Several limitations of MMD are described, as well as the consequences of the various choices in using MMD (kernels, descriptor functions, and hyper parameters) and how these choices have impacted experiments in recent work. The paper is generally clear and well written and makes several interesting and worthwhile observations about MMD and its use in recent work. “depicts an overview of this approach as it is being done today.” Consider adding citation(s).
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This work targets the problem of Continual Learning (CL) and In particular, the problem of populating the buffer memory in the Experience Replay (ER) based paradigm. The paper proposes MEMORABLE INFORMATION CRITERION (MIC) which is a combination of the surprisal and learnability of an incoming example and further motivates this with a  bayesian analysis. Using MIC, it introduces two algorithms, namely, InfoRS and infoGS for buffer update. 3.In the related works section, which talks about dataset distillation, there is another paper [1] (ICLR’21) that also talks about dataset distillation with applications in continual learning. Finally, it reports the proposed method s performance on imbalanced data streams and shows that infoRS outperformed reservoir sampling, GSS, and some other baselines. I think that the paper was easy to follow overall, and the MIC criterion is intuitive. The tSNE plots (of the selected example for the memory of size 200) showing the ablation study was also quite interesting. I suggest including the paper [1] and online k medoid clustering. There are some questions (mentioned in the detailed review) that need to be answered in order to make it a strong submission. It is an interesting work, however, lacks discussion about some baselines (not mentioned in the paper) and other performance metrics (about forgetting, BWT).<|endoftext|>This paper considers an online memory selection task for continual learning applications. The paper proposes a new (information theoretic) criterion to pick informative points and void outliers. This criterion is a combination of the surprise and learnability score. The Bayesian linear regression and the rank one update trick are also well studied. They consider continual learning benchmark as well as ablation studies of data imbalance. The key contribution is proposing the criteria to select and replace the existing points in the memory buffer. The surprise captures how unexpected a new point is given the memory, and allows us to include new information in the memory. The learnability captures how much of this new information can be absorbed without interference, allowing us to avoid outliers. The authors also connect that MIC is related to Information Gain. In the appendix, the paper has compared these criteria (MIC, IG, ER) thoroughly. * The performance may critically depend on balancing how much we want to “surprise” versus “learn” through $\eta$. It is highly recommended to have the ablation study with respect to $\eta$. Presentation: * The paper is in general well written and easy to follow. It could be interesting to consider either replacing (as the proposed framework) versus expanding without replacing.<|endoftext|>This paper considers task free continual learning in the context of online memory selection. The author consider the use of information theoretic principles and propose the surprise and learnability criteria to select informative points and avoid outliers. In particular, the notion of surprise and learnability seem well suited to the current problem at hand. The computation of the Bayesian posteriors is also natural and the Gaussian assumption, together with conjugacy, results in efficient updates, which is nice. + The InfoRS algorithm also seems natural for sampling. Weaknesses:  It would be preferable for the authors to consider the effects of each of their proposed modules separately   surprise, learnability as well as InfoRS. An ablation study would be useful. I am wondering whether the authors have examined the effect of the many hyperparameters in their study such as \eta, \alpha and \beta. The datasets used seem rather simplistic, just MNIST and CIFAR10.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; In this paper, authors have introduced an overfitting phenomenon that has not been addressed in previous works related to policy optimization and importance sampling. They then propose a new constraint on policy and a new algorithm which avoids the overfitting. They have provided theoretical justification on why their proposed method works and show experimental results to show the effectiveness of their approach. One major issue with the proposed approach is the expressivity of the policy class. In other words, the reviewer believes one missing part of this paper is to show how much local constraints on actions (eq.5) is hurting the performance in the non asymptotic setting. How does this loss change as sample size grows? One question here is that is this comparison fair? The writing of the paper needs a major review. However, the reviewer is not convinced enough about the effectiveness of the proposed approach due to the concerns raised in the main review plus the fact that the paper needs a major writing revision.<|endoftext|>Although various similarly ideas have been proposed like regularizing toward behavior policy, this paper improves on importance sampling methods that does not assume MDP that separates itself from recent baselines like PQL. They also show some analysis on this problem. One way is to shift and scale the rewards to be positive, and capped the importance weights to avoid over maximize the reward. C) The novelty of this work is not high as similar ideas have been proposed but not particularly in the non MDP setting as acknowledged by the authors. I think authors can quantify it in both simulations and the MIMIC3 by showing that the importance weights are close to 0 for the baselines in lower reward states. Though I understand you have to choose a metric. C) In Fig.1(a) why there is no blue point around x 200~400? 2020.Pros:+ The method is easy and simple and shows improvement in both experiments that include non MDP simulations and a real world dataset+ The writing is mostly clear+ The improvement shown in a real world clinical environment is encouraging. The distance considered in this paper may not handle missingness or when some input features are irrelevant to the reward. Although I feel the proposed overfitting phenomenon can be an interesting contribution, the authors should quantify if it happens in the experiments and justify their method by comparing with heuristics like shifting the reward. I think addressing these can further push the paper over the acceptance bar.<|endoftext|>The paper considers offline policy optimization. The author(s) discussed the issue of overfitting of the importance weights in existing offline algorithms and developed an algorithm to alleviate the issue. "to be"  > "be". However, I would like to increase my score shall my comments be addressed. * In Equations (3) and (4), suppose we plug in the empirical behavior distribution for $\mu$, then $\hat{p}(x)$ equals $W(x)$. This would solve the over fitting problem. In practice, according to the semiparametric theory, even if the true importance sampling ratios are known to us, the resulting estimator with an estimated ratio would be more efficient (see e.g., Tsiatis, 2006, Semiparametric Theory and Missing Data). * Would you please elaborate the constraint in Equation (6)? * The condition $|\Pi|<+\infty$ is not sufficient to guarantee the validity of Theorem 2. The author(s) needs to provide an upper bound for $\delta_n$ in the proof (as a function of M and $n$) and presents the corresponding condition for $|\Pi|$. * The proposed algorithm relies on several tuning parameters, such as $\delta$, $M$, $\lambda$ and $\alpha$. Have you conducted some related experiments? Minor comments:* Page 3, Line  12.<|endoftext|>The paper studies the overfitting issue in counterfactual policy learning. The paper first  identifies an important overfitting phenomenon in optimizing the importance weighted return, and proposes an algorithm to avoid the issue. Some experiments show the benefit of the proposed approach. **Strengths**  the paper tackles the important and relevant issue of overfitting in counterfactual risk minimization. the motivation is also nicely supported with some toy examples. the paper identifies the source of overfitting that is not addressed in previous works. All experiments are based on real world data, meaning that the policy performances are now measured with OPE on a test set, which might not be accurate. The motivation was easy to follow with some easy examples. I would recommend a weak accept at this moment. Some additional efforts in the experiments (as I described above) would strengthen the contribution more.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors show the usefulness of their VaGraM in simple experiments. ## General Comments  **writing quality**: This is a well written paper. Throughout, it was easy to follow where the authors are going and why. Is that the sum across the state dimension? Is there interesting insight to be gained there? Is there a simpler explanation to all of this   I don t see the VaGraM method being better as described in the text. I see that one of the terms cancels out. 3.4 what constitutes a "small prediction error" for the Taylor expansion to hold? Does this problem come up more in the literature? 3.6 can the authors clarify footnote 2, this was confusing to me. ### 4.Example: Cartpole  4.1 AVI is used but not defined. ### 5 High dimensional  5.1 if the cited paper is for pixel based RL?<|endoftext|>The paper studies the model learning aspect in model based reinforcement learning (MBRL). 2020.POST REBUTTAL UPDATE:I increase the score to 8. The reviewer leans towards recommending the submission for acceptance. Strengths  The paper is novel. The VaGraM loss is an MLE weighted by sensitivity of value function to states. Weaknesses  The experiments miss a few relevant baselines. While the reviewer appreciates that the authors mention [1] in related work and discuss that VaGraM is value based and [1] is policy based, the quality of the paper would be improved if the experiments had these or other related benchmarks that also address the objective mismatch.<|endoftext|>The paper proposes a new loss for model learning in model based RL, called “Value Gradient weighted Model loss” (VaGraM). The paper should comment on the requirement of this assumption and when it may be violated, and whether or not it holds in the selected environments. Is it the VAML error? The paper has potential to be a strong contribution if the empirical analysis can be expanded based on the weaknesses I had mentioned in my review. For the complex continuous control environments, why was Hopper the only environment selected? However, they do not include a comparison of the proposed method vs MLE and VAML trained end to end in an MBRL algorithm (i.e.the opposite of the controlled experimental setting where the policy, value and model all interact and influence each other). This assumption is in addition to the assumption presented in the paper that the model prediction should not be too different from the environment’s next state.<|endoftext|>The paper introduces a new value aware objective for model learning that directly builds on VAML — specifically, using a Taylor series approximation for the next state value prediction w.r.t the state under consideration. The proposed objective is straight forward and can be of use to the MBRL community. Weaknesses:  The proposed objective is limited to deterministic dynamics; a more details discussion of this limitation is preferred in the main paper as opposed to the appendix. This yields a somewhat intuitive form upon simplification — essentially, dimension wise weighted L2 distance between predicted and actual states of the model where the weights correspond to the gradient.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors propose a multi resolutional graph generation tool with a variational autoencoder. Several learning tasks are designed to support the novelty of the proposed method;Weakness:1. A similar issue arises in the second experiment, where the authors select a subset of two considerably small graphs, Cora and Citeseer. The concepts should be first introduced BEFORE further discussion, e.g, equivariance. 3.The reproducibility statement was not addressed. The concern on model complexity should be addressed. Also, the paper should be carefully proofread before it is ready for acceptance.<|endoftext|>The MG VAEs represent generative models that allow for generating graphs in a multiresolution and equivariant manner. However, the evaluation of the proposed method seems limited, especially in the main paper. For example, for the task of molecular graph generation, only validity, novelty, and uniqueness of the generated molecules are evaluated. I can understand if the goal of the results is to merely highlight competitive performance with respect to ‘popular’ methods and not to claim state of the art performance. Under which circumstances might this be useful?<|endoftext|>This study presents a multi scale graph VAE with hierarchical graph coarsening. The Gumbel max trick is employed to generate learnable hard partition for clustering, and permutation equivariant tensor operations are used (Kondor et al., 2018) to construct the group equivariance network. I have some minor concerns below for the authors to address. If the Gumbel softmax is used, then whether the complexity of $O({|V|^2}/{K})$ could not be hold since Gumbel softmax is not hard partition. More concrete, in the molecular generation (Figure 3 and Table 1), while the chemical validity is 100% on both dataset, the generated molecular structure seems wired and are most likely not stable.<|endoftext|>* Appendix C, typo: "preserves equiarience"  > "preserves equivariance". The method is evaluated on molecular generation, community graph generation,  and citation network generation in the main paper. **weaknesses**:* At the end of section 3.1, the authors claim that MGN (multiresolution graph networks) "is more efficient than existing methods in the field." This contradiction with the earlier claim needs to be clarified. What is the source for the high complexity that prevents this method from being trained on the entire dataset (which is not that large)? Furthermore, what is the effect of the learnable equivariant prior over the standard normal prior? 5 and 6 are used throughout the field and don t require that much space.<|endoftext|>This paper proposes Multiresolution Equivariant Graph Variational Autoen coders (MGVAE) which can learn and generate graphs in a multiresolution and equivariant fashion. The model also maintain an end to end permutation equivariant with respect to node ordering. Their experimental results show that MGVAE achieves competitive re sults with several generative tasks and graph link prediction etc. The paper is quite technical. I dont see any specific theoretical issues and the mathematical derivation is solid (anyway this is the standard steps of building VAE models). It is nice for the authors to deal with the non differentiable issue of the clustering assignment, i.e., eqn (1). However I am a bit confused or surprised that the authors did not mention at all on how to deal with the differentiable issue with the cardinality function in the KL term, see eqn (2) or (3).<|endoftext|>Strengths:+ MGVAE is the first hierarchical generative model to learn and generate graphs both in a multiresolution and in an equivariant manner. + Lots of experimental results. The experiments are extensive. The paper needs a stronger motivation for why the authors  specific approach is important considering  that (i) the idea of studying the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices at different resolutions is not novel, and neither is the (ii) permutation equivariance of graph neural networks
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper proposes a post hoc explainability algorithm that is global and model agnostic. Experiments show the proposed method outperforms IG and LIME from aspects of sparsity, sufficiency, and time. I find the paper hard to read, and lots of its parts seem to be wordy and lack clarity. I suggest the authors thoroughly reorganize the lines of writing, especially for the introduction and method section. 3.Regardless of the variety of models in the experiments,  more competitive baselines are necessary to justify the superiority of MAGNEX. I think this work is not ready for publication, thus I lean towards rejection.<|endoftext|>The paper proposes a global model agnostic neural network based explainer. The proposal of the paper is interesting. However, it has some weaknesses especially in the experimental part that should be fixed before publication. However, it should be tested also with respect to other evaluation measures; c) the main advantage of MAGNEX seems to be the low runtime but this aspect is not the focus of the paper; d) there are no examples of explanations for images or question answering; e) the approach should be tested also on the most simple data type, i.e., tabular data; f) I recommend adding a large array of baselines such as surrogate decision trees and random forests returning as explanation the features importance, SHAP, Trepan.<|endoftext|>This paper proposes a global explainers for black box models. Depending on the scores, features are dropped and theremaining input is also bed to the original model. The new explainer is compared to LIME and Integrated Gradients interms of how faithful the produced explanations are (how close theresults of the original input and the trimmed one are when passedthrough the black box model) and the execution time on three differenttasks: image classification, sentiment analysis and questionanswering. I think the approach in this paper is fairly intuitive. I m not sure that once we understand model predictionswe understand its fairness characteristics.<|endoftext|>This paper proposes a global model agnostic explanation method. The method relies on a neural model that learns to predict which input features are important for the original model’s predictions. Through substantial experimentation, the authors demonstrate that their approach outperforms LIME and Integrated Gradients. They compare between explanation methods in terms of the faithfulness of its explanations and in computational complexity.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The authors consider the problem of calibrated probabilistic outputs for multiclass classifiers. They consider the various types of calibrations previously studied, such as confidence calibration and class wise calibration, and then propose a novel notion of calibration based on the choice of top labels. This paper is very well written, tackles an interesting area, proposes a novel calibration, and demonstrates the utility in empirical experiments. Strengths:  The proposed calibration results in a more intuitive interpretation than previous approaches and the authors argue so convincingly. Matching calibration algorithms and metrics places everything into a nice framework, and the experiments demonstrate clearly the value of matching the algrithm and metric. Histogram binning, within the appropriate algorithm, is shown to be a competitive post hoc calibration technique. Weaknesses:  Only histogram binning is considered, though there are other post hoc calibration methods that clearly fit in the algorithms described. It would have been good to see some results, or at least a discussion. Excellent paper with good results and clear practical applications. The work is also well placed within existing literature.<|endoftext|>The authors give arguments, examples and experimental results to show that the commonly used confidence calibration method suffers a number of shortcomings that makes it less useful in practice. As an alternative, top label calibration is proposed, where calibration is analyzed on a per class basis, when a specific class is the top class. In addition, the authors discuss shortcomings of confidence reliability diagrams, and they propose multi class to binary reductions for achieving top label calibration. In the experiments several methods are compared on two classical image classification benchmarks. This is a pretty dense paper that contains a lot of material, but it is very well written. I do not consider myself an expert on the topic, but I could follow the flow of the paper quite well. The authors have been able to convince me of the disadvantages of confidence calibration, and the advantages of the method they introduce. The experimental results also seem to support the claims of the authors. From a more conceptual perspective, the proposed algorithms are also appealing. However, I do see potential problems with the approach of the authors for classification problems with infrequent classes, like in extreme multi class classification, where long tail classes have very few observations. In such situations, confidence calibration will probably work much better, as one doesn t have to condition for rare classes. Conversely, for the approach of the authors, one needs much more observations per class. So, in this regard, the experiments are currently somewhat limited, and probably telling a too optimistic story. I would have liked to see some experiments with extreme classification datasets that have long tail class distributions. Datasets in the experiments are quite simple.<|endoftext|>Evaluation is performed using 1 vs rest ECE, a new top label ECE, and a maximum calibration error (MCE) variant of the latter. Surprisingly, the proposed approach does not perform as well as 1 vs rest calibration when using the top label variant of ECE; it does perform a lot better wrt the MCE variant on the CIFAR 10 data though. The paper also explains how to adapt top K confidence calibration to obtain top K label calibration, but this is not evaluated. The first seven pages of the paper are very nice, and I very much enjoyed reading this material. The problems start with the experiments:1) Reading through the appendix, the authors are obviously aware that a calibration set is normally used to calibrate a model, before it is evaluated on the test set to compute ECE. 2) One of the three empirical observations on Page 9 states that the new TL HB is the best performing method for 1 vs rest ECE. In fact, it is the un normalized variant of 1 vs rest calibration that is shown in Table 3 and that performs best! Assuming this is just a typo and "TL HB" in Observation (c) was meant to be "CW HB", this is the most surprising and potentially most impactful finding in the paper. (The other two observations are about the behaviour wrt TL ECE/MCE, which are the new, less obvious ECE metrics, and the results are also more mixed wrt these metrics.) However, there is no analysis at all in the paper why leaving out normalization in the 1 vs rest method (CW HB) is so much better than using it (N EB)! 3) There is no comparison to isotonic regression, which is trivially applied using the 1 vs rest method, and like binning, is also a non parameteric method. The scaling based methods are all parameteric. 4) The number of datasets is very limited (CIFAR 10 and CIFAR 100). My understanding is that normally, the same bin boundaries are used for all classes when this method is applied. It is unclear how helpful this algorithm and the M2B "notion" are. This aspect of the paper seems somewhat trivial. Longer and longer appendices seem to be becoming the norm, but this submission is quite extreme in this regard, particularly because only some appendices are referred to in the main text (G, A, D.3, D.2, D.4, D.5, B, D.1) while others aren t: E (random forest experiments, extend Appendix B.2), C (CW HB), F (canonical multi class calibration, 8 pages). The role of appendix F in particular is mysterious: it is only tangentially related to what is presented in the main text. Other questions and comments:   Do the calibrated probabilities obtained using top label calibration sum to 1? "However, the distribution of $g$ can be different for different labels, thus they should be treated differently"   I don t understand the reason for having this sentence here. I would delete it. Renumber the algorithms to follow the order in which they are discussed. Why is this bound useful then? Is N HB defined in the text or just in the caption? Why is CW HB not included in Table 2? There are several problems with the empirical evaluation and the analysis. The submission (including the many appendices) also seems heavily overloaded, to the point that the main message becomes quite unclear.<|endoftext|>The paper suggests a definition for calibration in the multi class setting named  top label calibration . The idea is to have only the most likely class calibrated. The authors then observe that many definitions for multi class calibration could be reduced to multiple instances of binary calibration and suggest an algorithmic framework where a binary calibrator is used as a black box to achieve the multi class calibrator. They then test this by instantiating it with histogram binning and measuring the corresponding notion of expected calibration error. It is also true that a natural algorithm to achieve many notions of multi label calibration is to reduce it to the binary case, as the paper suggests. Weaknesses: I think the definition also has obvious drawbacks that the authors do not discuss. In particular I am not convinced that calibrating only the top label makes sense. Thus, the point of calibration is to do it to an existing classifier *with out* sacrificing other good properties such as loss minimization. Detailed comments:  Please explain better why calibrating only the top label is sufficient. Please discuss them and contrast with your definition as well. I m not sure I fully understand the contribution of Section 3. Sure, some notions are reductions from binary classifiers, so lend themselves to be computed via binary calibrators. Is there anything more you can say? In table 2 and 3 it is worth noting that scaling algorithms are not designed to bring down ECE or TL ECE. This variation makes sense under some scenarios, but is still a quite a weak notion on its own. While it s an interesting notion to discuss, I think the contributions of the paper are too thin to justify publications.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper presents a network quantization method for robust learned image compression. It is interesting that post training quantization is enough for the model robustness of learned image compression. However, this paper requires polishing for the presentation and comparisons and the writing clarity should be improved. For example, the term  cross platform inconsistency  is hard to understand until reading the Appendix and the paper of Balle et al.(2019).Figure 1 and 2 present the problem statement and the methods in previous works which limit to emphasize the contributions of this paper. Experiments do not analyze the contributions of the proposed method described in Section 3. More importantly, this paper misses the comparisons to the method of Balle et al.(2019) for the model robustness across devices. On the other hand, the proposed method described in Section 4 seems to be an alternative to post training quantization since it is a predetermined CDF for different devices that minimizes the cross platform inconsistency. Based on the alternative, I m curious about the importance of the cross platform inconsistency which can easily be solved by saving the same CDF as metadata and using it across devices. This paper presents a simple practical solution, post training quantization, for robust learned image compression using neural networks. However, the contributions of this paper are marginal since the problem statement and solution are overlapped with the previous work (Balle et al., 2019). The authors should thoroughly state the differences and improvements of the proposed method to the work of Balle et al.(2019).<|endoftext|>A well known issue in learned compression is that due to inconsistencies in the implementations of low level floating point arithmetic across different hardware architectures, catastrophic decoding errors might occur for the compressed data. ## Strengths     In my view, the most important contribution of the paper is to demonstrate that state of the art image compression methods can be appropriately quantized to retain essentially the same compression performance as their unquantized counterparts     The extension of the look up table technique of [1] to Gaussian mixture models is interesting and clearly useful## Weaknesses     I am not convinced that the authors can claim the application of post training quantization (PTQ) to achieve cross platform consistency for image compression models as a contribution. In fact, the authors seem to acknowledge this a couple of paragraphs above! Could the authors please clarify if my understanding is correct? In case it is, the claim should be changed to better reflect the contribution of the paper. The results of the paper are not compared with relevant methods, most notably [1] and [2]. In particular, the authors present the performance of their method on the method of [3] but do not show the performance of e.g.[2] on the same model. In the introduction, the authors make the following claim "However, we find that on more complex models with context modelling like Minnen et al.(2018) and Cheng et al.(2020), adopting this integer network approach cannot keep the performance loss negligible." ### Minor issues     The 1st and 2nd sentences of the abstract seem to be inconsistent with each other, the 1st implying that LIC is practical, while the 2nd implying that it is not. ## After RebuttalThe authors have addressed my main concerns, and I, therefore, decided to raise my score. ICLR 2018I currently believe that while the work is useful to the community (see 1st point in "Strengths"), I think in its current form the paper makes a few considerable claims that are not supported.<|endoftext|>Learned image compression has a known issue that the nondeterminism of the floating point operation makes it impossible to recover the compressed data on different platforms. This paper experimentally shows in compression models using context modeling (Minnen et al.(2018)) that it is possible to recover the data on different platforms even when using the known quantization method called PTQ. (Table.1 )Referring to the method of computing STD using pre computed sampling points σˆ by Sun et al.(2021), the authors proposed a method called Binary Logarithm based STD discretization. The effect on RD performance is also shown in Figure 6. 2.I think cross platform support for learned image compression is an important issue for practical applications. Cons:1.Issues to be addressed are unclear:  The authors state in chapter 1: "However, we find that on more complex models with context modeling like Minnen et al.(2018) and Cheng et al.(2020), adopting this integer network approach cannot keep the performance loss negligible. ", it is interesting but I could not find evidence for this; if the RD rate performance of the integer network is degraded, it should be shown in experiments. This paper proposes a new method (Binary Logarithm based STD discretization) to aim for more hardware friendly. It is interesting but unfortunately, the experiments in Table 2 do not seem to show a comparison with Sun et al.(2021), which is written like a baseline in chapter 4. If the error rate is 0% with only PTQ without applying the LUT, why is LUT necessary? I am curious about the contribution for prevent data corruption of each of the PTQ and LUT. The authors experimentally showed that PTQ can be used to prevent data corruption of learned image compression, but the value of this research is not clear due to the unclear target issues, insufficient comparison with previous methods (e.g.Balle ́ et al.(2019) and Sun et al.(2021)), and some unclear points, so I considered that currently this paper did not meet the conditions for acceptance.<|endoftext|>The paper tried to solve the non deterministic issue for the learned image compression and reduce the inconsistent cross platform probability prediction. It is a practical problem in the development of learned image compression. The experimental results are convincing. It seems that this quantization method does not introduce too much performance degradation. Weaknesses:Since the major techniques are borrowed from the quantization techniques in model compression, so the technical contribution may be limited. The authors are suggested to make a clear description for the difference between the techniques used in this paper and that in model compression. Besides, the authors only provide the RD curve below ~1bpp. How about the compression performance at high bitrate? I tend to accept this paper though I have some concerns about the technical novelty.<|endoftext|>Many works in the are perform quantization during training, which can make for more complex or longer training setups. This paper shows the effectiveness of post training quantization techniques, shows the results are successful even on context and GMM models, and introduces a faster entropy parameter discretization method. Strengths: The authors do a great job of immediately conveying the problems in image compression (designing deterministic decoders and the speed performance of those decoders). The results are strong, they show very little to no loss until high bitrates due to this quantization technique. Since the losses appear to be more evident at higher bitrate, it would be instructive to know if PTQ is as successful outside of the low to medium quality range. It is an obvious improvement in runtime due the formulation, but are there any significant RD gains for having another level. Questions/Feedback:Table 2: I m not 100% sure what is being compared here. This is a well written paper that shows the effectiveness of existing post training quantization techniques when applied to multiple neural image architectures.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper studies a specific distillation scenario, adversarial distillation, to improve the model robustness. Different from the constant sort supervision in the ordinary distillation, the teacher model will become progressively unreliable along with training, since the adversarial data are dynamically searched by the student model and might not be well identified by the teacher model. However, there are still some concerns about this work. 2) The experiments of IAD + AKD^2 is not well explained. However, the writing about the motivation and the loss designed for each group as well as the experimental parts are not very clear and need to be further improved.<|endoftext|>The computational cost comparison is missing, which is important for distillation based method. Figure 1(b) is a clear figure, but I cannot see the advantages of your method in this figure. This is also the point this paper focuses on. Compared to existed works, this paper argues that the adversarial training data of the student model and that of the teacher model are egocentric (respectively generated by themselves) and becoming more adversarial challenging during training, which causes failure of existing works. + This paper is well written and easy to follow.<|endoftext|>In this paper, a new method called introspective adversarial distillation (IAD) is proposed for conventional adversarial training. Cons.There are some issues to be addressed. This phenomenon should be explained more. The current evaluation is based on image datasets, like cifar and tiny imagenet. Basically, this paper is well written and the proposed method seems promising.<|endoftext|>This paper proposes a new knowledge distillation (KD) method for adversarial training. The authors first observed that the soft labels provided by the teacher gradually becomes less and less reliable during the adversarial training of student model. Based on this observation, they propose to partially trust the soft labels provided by the adversarily pretrained teacher.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The paper extends work on static term action generation (Where2Act, ICCV21) for 3D articulated objects to 1) long term action trajectory generation by learning from data generated data via RL exploration,  2) action trajectory conditioned with task awareness. The paper is well written. Cons:The method itself is not very novel, more about extending the existing Where2Act and a combination of Where2Act and curiosity guidance for RL Policy for Interactive Trajectory Exploration.<|endoftext|>This paper is solving the problem of pushing and pulling objects (mostly things like cabinets) by learning visual action trajectories proposals via a curiosity driven RL / Perception joint training. The system input point clouds the object and outputs the actionable score and the per trajectory success likelihood score on the most likely approach to interacting with the object. However, they do show a comparison against the Where2Act approach that shows that their modifications to the network (including the curiosity exploration and how they find the trajectories) are superior, for this task. This paper is easy to understand and well written. This paper has some interesting elements to it and their approach is validated by both real and simulated results.<|endoftext|>The paper proposes a method for exploration of 3D articulated environments that alternates between collecting interaction data with RL while maximizing a combination of extrinsic and intrinsic rewards, and training visually conditioned action maps, image conditioned manipulation trajectory priors, and success predictors, that further guide the intrinsic reward prediction during data collection. I have two main concerns regarding the paper: 1)The link between the visual perception and the RL policy appears weak as the only feedback is through exploration rewards for the RL policy to try out interactions on places where the visual perception models assigns low success probability. Post rebuttal : the authors have put together the requested baseline and they show significant performance margins over it. Then, at the end, we can train actors that operate directly from images, similar to an ``asymmetric actor critic" setup.
Reject; rating score: 6; rating score: 6; rating score: 6; With a novel zero shot NAS method proposed in this work, named ZenDet, detection tasks can free from the heavy models, time, and resources to archive SOTA performance. Saving lots of time and memory for searching architecture for a competitive backbone. 3.Benefit from Multi Scale Entropy Prior, ZenDet can adapt different sizes of objects, and results of training from scratch. 2.Existing some spelling mistakes，such as retianet >retinanet. What are the failure modes of the proposed msep method?<|endoftext|>This paper proposes a zero shot neural architecture search approach for backbone design in object detection. Overall I think this paper presents promising preliminary results on zero shot neural architecture search for object detection. Post rebuttal update: I decided to raise the score to 6 based on the empirical results presented in this paper. I still think the novelty comparing with Zen NAS is not that large and the challenges of the current approach (e.g.cannot deal with complex connections in the FPN, basically can only search for relatively simple backbones) prevents me from assigning an even higher score to this paper. What is the theoretical implication / insight of the current parameter choice? II.Experiments+ The paper is focused on the ResNet design space, which is shown to be efficient on desktop GPUs. However, it seems that results on edge GPUs or CPUs are missing. The design space is different. Would it be possible to supplement some results on their design space and justify the effectiveness of ZenDet?<|endoftext|>In this paper, the authors propose a zero shot NAS to search backbone for detection task. Also, the backbones searched by this method have the state of the art performance on detection task. Strengths  The proposed method is simple and efficient. This paper is well written and readable. Further, the network searched by this method may be suboptimal. In this paper, the smallest backbones searched by this method is ZenDet S. Due to the limit of edge devices, the detectors with ZenDet S can not run fast. This raise a question: can this method search some networks for edge devices? In Figure 2, there are little details about the search space. I hope the authors would provide the results about how the multi scale weight ratio affect the correlation for more heads in the final paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; However, there are major flaws in the current forms of the paper. The motivation is not strong.<|endoftext|>The main contribution of this paper is the design of Bi InfoNCE loss to optimize the joint distribution $p_{data}(u,i)$. 3.The comparison results to MF in Figure 2 and Table 1 are not promising.<|endoftext|>Please see the comments below. Also, this paper claims that the proposed method was effective in terms of convergence speed and diversity in the experiments. The novelty and effectiveness of the proposed method will be quite weak.<|endoftext|>In experiments, the authors find that the proposed TR model outperforms MF in both recommender system and advertising system datasets. The main strengths of this work are listed as follows:1. The technical novelty is limited.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper studies active learning for domain adaption for a set of Lipschitz functions. 3.The paper is generally well written, and the authors also provide visual insights to help understand the paper.<|endoftext|>The paper studies the problem of active learning for domain adaptation. While the proposed methodology is of limited novelty and not described in a self contained manner, when considered along with the proposed theoretical framework, the contributions of the paper may be worth sharing with the community.<|endoftext|>This paper proposes a k medoid solution for active learning in the context of domain adaptation. Pros:1) Overall, the paper addresses an important problem and formalizes the task of active learning for domain adaptation.<|endoftext|>This paper deals with the important topic of active transfer learning and is considered to be a contribution to both theory and experiment.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; They also test the model’s zero shot capability in cloze style QA, and XLM Indic also achieves better or comparable performance. By measuring lexical fertility, they claim that the success of shared scripts comes from that low resource languages borrow better representation of shared lexical from other languages where those lexical are frequently used. The main strength of this paper is that it provides a simple solution to the script suffering in language modeling, and compares its performance with several multilingual pretrained models as well as the non transliteration version of the model. The transliteration ALBERT shows better performance over the non transliterated version. For example, this work tends to claim that tokens with the same spellings in different languages carry the same meanings. This claim is hard to stand. Additionally, it is not clear to see that shared tokens collapse to the same subword from the subword fertility analysis in Section 4. However, they are not fully discussed because the authors only select a group of very closed languages, and therefore cannot resolve the concerns when this technique is applied. In fact, since many languages can be Romanized, languages like Greek and Russian can be considered to test the hypothesis in this work. In summary, this work studies an important problem and tries to provide an effective solution. The experiment is done on a small scale and limited setting, and many underlying hypotheses of this work are not widely applied based on the insufficient results. I would not recommend its acceptance.<|endoftext|>This paper hypothesizes that transliterating all the languages to the same script can improve the performance of multilingual language models (MLLM). Pros:1.The idea of introducing transliteration into MLLM is interesting, where the ALBERT is pretrained secondarily on the OSCAR corpus. Cons:1.One main concern about this paper is that the contribution is not enough. The purpose of this paper is to use the transliteration technique to improve the performance of MLLMs. However, only ALBERT is selected for validation (Other representative MLLMs such as mBERT, XLM are not discussed.). And there is no significant novelty of improving ALBERT to XLM Indic such as better model architecture or pretraining objective (at least the improvement is not well presented in the paper). Experimental results show that XLM Indic (ALBERT size) significantly outperform existing MLLMs, while it is difficult for readers to get the intuition from the paper about how the performance gain is achieved, although Section 4 tries to provide some reasons (which are insufficient). 3.Many necessary details are missing. In addition, there is limited discussion about existing work. Using translation based models to improve the performance of MLLMs is a widely used strategy, and it would be better if authors can discuss and compare their XLM Indic with existing methods. It would be better if a clearer description about the assumptions can be provided.<|endoftext|>This is particularly useful for building downstream NLP models for low resource languages. Additionally, this would have also have led to nice set of relation to prior work. Transliteration, i.e.using a common script for all the languages can help solve this issue. (2) Although it s important to consider internationalization of language technologies, the core idea and the contributions made by this paper is not really novel. Then, the ALBERT model is pre trained on the transliterated corpora of these languages extracted from the OSCAR corpus. The paper has indeed improved quite a bit from its previous shape. As the other reviewers also pointed out, I am still concerned about the novelty of the work so I am inclined to keep my score the same. However, with the new changes I think now the paper brings some insights that could be useful to some readers in the community. 2) Strong results with a thorough study on many languages and also statistical tests. It s been used a lot in the field of speech and NLP. Although the authors apply this work to a new setup and on a new corpora, the key takeaways are not new or surprising. 3) “However, we would like to mention that, in this paper our purpose is not to achieve the SOTA but to understand the impact of transliteration on the performance of language models.”   → the paper is mentioning SOTA and results everywhere (even half of the abstract) and is studying very quickly transliteration in the last section (section 4) in about 15 lines and one figure which is not very striking and not very commented4) The authors mention "However, we would like to mention that, in this paper our purpose is not to achieve the SOTA but to understand the impact of transliteration on the performance of language models.”. But the paper does seem to be focused almost entirely on results with the final section on studying the transliteration. 6) In section 3.4   the authors mentioned that they modified the evaluation code. However, there are several limitations in the scope and relation to prior work.<|endoftext|>The paper proposes to replace the native characters and words of some South Asia low resource languages into latin based transliteration using a rule based converter and use these transliterated data to pre train cross lingual language model. The transliterated format of a word tends to be shared across different Indic characters of different languages. This may have helped the model learn these languages better. The actual procedure of transliteration is also not invented by the authors, but taken from existing work. * Only Indic languages are explored. And I doubt this method would work in other languages with the same situation as South Asia languages, such as Chinese, Japanese, Korean; or other Asian languages like Thai, Malay, and Vietnamese. Overall, I think the paper significantly lacks novelty as only suggest an adoption of an existing method in a straightforward way that anyone would have thought of. The performance contribution is limited to a subset of languages and there is limited prospect that it would work in other languages and application. Here are my afterthoughts:1. The novelty concern remained the same. It would have made a difference if the authors propose a novel technique Z based on X that makes things better for task B. 3.The paper, and the responses, show that the paper has lots of linguistic contribution. However, this still does not warrant an acceptance for me.<|endoftext|>In this paper, the authors show explore the following question: is it beneficial to represent different languages in the same script for training pre training LMs. The study has been performed on Indo Aryan languages, which is an important representative of this scenario. The analysis shows that the single script model vocabularies have a higher level of subword sharing. These findings are in line with concurrent work by Dhamecha et al (2021). EMNLP.2021.**Weaknesses**  Studies on the impact of vocabulary size, pre training corpus size, and fine tuning corpus size will be beneficial in understanding if transliteration is beneficial in constrained scenarios only. Some previous work indicates that large multilingual models can implicitly map related languages into a common representation space even if the scripts are different. (2019).Investigating multilingual NMT representations at scale. The same hyperparameters have been used for finetuning all models, is there a bias in the selection of these hyperparameters like early stopping, learning rate, etc for certain models? Concurrent work by Dhamecha et al (2021) also confirms the major finding of the paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; > "turn to Half Moons"The authors propose convex potential quantile (CPQ), which is an optimal transport approach through a Kantorovih dual objective, maximizing the correlation functional between a uniform distribution over the unit hypercube and target distribution. This paper builds conditional generative quantile modelling to the multivariate setting. Their approach is heavily based on a recent work of Carlier et al.2017.In my opinion, the experiment part of the paper highlights the beauty of the conditional vector quantile proposed in the work of Carlier et al.2016, through neural networks implementation.<|endoftext|>This paper estimates conditional quantile contours of a multivariate random vector. What is the advantage of this method over that in Carlier et al.(2017), in which an estimation algorithm was also proposed? Although the empirical results are encouraging, it is unclear the method and theory in this paper is sufficiently novel. The convex potential is learned by minimizing the dual problem.<|endoftext|>That paper proposes a multivariate quantile network that may be used for generative purposes. Then, how do you actually use X anywhere in this equation ? A large part of the introduction is dedicated to a presentation of classical facts regarding quantile functions and optimal transport, including basic definitions of the pinball loss, of the Kantorovich dual problem, etc. This looks unclear to me. Furthermore, writing Y \nabla f(U) doesn t tell me what exactly you call the convex potential quantile.<|endoftext|>This paper proposes a conditional quantile generative model using optimal transport. Existing results shows the efficacy and versatility of this method.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; Choosing the pruning termination criterium based on on the extended spectral gap (Algorithm 1) is a good idea and novel to my knowledge. There is also no solid theoretical justification, why the termination criterium should fit (except for the already known correlation between the spectral gap and density). The work is lacking comparisons to previous pruning methods, showcasing the usefulness and significance of the proposed algorithm. If I understood correctly, it should be easy to apply Algorithm 1 to the networks and present the resulting test accuracy in comparison to previous pruning techniques. However, that is not done. All in all, while I like the general idea of utilizing spectral graph information of bipartite network connectivity graphs for pruning decisions, I find the paper is lacking a clear message and the required experimental evidence to back it up.<|endoftext|>Strengths:+ The idea to measure the potential of a pruned network to serve as lottery ticket based on the eigenspectrum is novel and interesting. + The idea that the Ramanujan graph property could be beneficial for sparse network structures could be also useful (but this remains open to show). As a stopping criterion for pruning, it is not relevant, as the validation set performance (or even training set performance) is in fact more indicative. I would expect to see a sharp phase transition and complete loss of network accuracy at the point where the Ramanujan graph property is lost if the made claims were true. Or is simply the connectivity of the graph the relevant property? They all look very similar.<|endoftext|>In this paper, the authors study the properties of the subnetworks in lottery ticket hypothesis (LTH) from the perspective of the spectral graph theory. They argue that the pruned network in LTH remains a Ramanujan graph. The performance of the subnetworks begins to degrade with the loss of Ramanujan graph property. **Pros**: The proposed pruning method is based on the spectral graph theory, which is novel and interesting. Whether the winning lottery ticket can be found more effectively? In general, the paper is well written.<|endoftext|>3: it is the Alon Boppana theoremThe idea of the paper is novel, interesting, and clearly exposed. They then propose an algorithm built on this premise, that prunes the graph as long as the ramanujan property is still satisfied. The authors also test the performance of their Ramanujan pruning algorithm against the same datasets. Problems include:  very small graphs requiring constant zooming in and out, especially since some curves are only distinguished by line and marker style  too many curves per graph, some being borderline unreadable (e.g.Figure 5.d)  the color schemes are confusing: the same colors are used for distinguishing Lenet layers (Figure 3.a, b) and noise levels (Figure 3.c)  the loss of the ramanujan property should be present on the testing accuracy graphs  the legend scheme for the curves should be explained more clearly: as of now, they are quite hermetic and require good knowledge of the studied networks  architecture.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; Likewise, the experimental setting is not detailed. As a result, I believe that the most actionable approach for the authors is that I list all the questions and issues I have in the following, to make clear where this paper needs improvement. * How is inference performed by the set to hypergraph models? * What does it mean to "remove excess rows" from the incidence matrix? * How is the number of iterations determined in the experiments?<|endoftext|>This paper addressed two scaling problems in set to hypergraph prediction by pruning edges and gradients. Proposition 1 seems to be the most novel argument but I think it is problematic (see the later criticism). May I know how many rows of the incidence matrix I? Moreover, how to predefine the number of hyperedges/ the row # of I? Can the authors highlight some intuition between the proof in the main text? It seems that the argument is not rigorous.<|endoftext|>This paper improves the asymptotic scaling and enables the learning task to have higher order edges by only representing and supervising a set of positive edges. The set to hypergraph prediction is an interesting problem. From the experimental results, the method is effective in performance. However, there are some critical defects in this paper. I hope the authors can revise them in detail. 3, The paper claims that specifying a maximum number of edges k is sufficiently large to cover all (or most) hypergraphs of interest. The paper works on a critical problem and proposes a reasonable solution.<|endoftext|>This paper proposed an efficient algorithm to tackle the set to hypergraph problem by utilizing recurrent training, pruning negative edges and backprop with skips. They proved this loss is similar to the overall loss. Con:1.The writing and the structure of the paper are a little confusing. And since only trained on positive edges, how did the author select the predicted hyperedges from the overall k edges? 3.The introduction of the Hungarian algorithm in selecting the edges does not make sense to me. Specifically, the author could explain how they choose the number of edges, how to select predicted edges.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This is a very interesting paper that is based on scattering networks, for which it shows that the so called ‘phase collapse’ leads to state of the art results on par with modern architectures, like ResNet. To derive this, the paper shows that having neural networks on complex numbers is similar to a structure deep network (like with wavelet filters) on the reals. What would be the benefit, considering that all phases are anyways immediately eliminated afterward? Maybe there can be a motivation that relates more to the main story of the paper. What about other non linear activations, like the swish function? Based on the novelty and the strong results, I vote for acceptance. The weaknesses of the paper:  I found the writing involved, if not subpar for the quality of the paper. All in all, I find that I learned something from this paper, which I think is a great result for any publication. The only ‘definition’ I found was that phase collapse eliminates the phase of a complex number with a modules just below eq (3).<|endoftext|>This paper studies within class variability which reduces along the layers of deep neural networks. They show that these classification improvements by eliminating spatial within class variabilities rather come from a phase collapse, which eliminates the phase of network coefficients. They introduced a complex valued neural network in which spatial filters are defined as complex multiscale wavelets and learning is reduced to $1 \times 1$ complex filters across channels. Their results show that such a network is able to reach ResNet 18 performance on CIFAR10 and ImageNet. They supported their hypothesis  (necessary and sufficient) by first explaining the performance of iterated phase collapses by showing that it progressively improves linear discriminability. Formula4: "One can verify..."? However, I found it difficult to find the changes in the rebuttal version. The text is still hard to read and follow (this is also addressed by other reviewers). As promised by the authors the text would be improved (e.g.shorter sentences). The authors show that the classification improvements by eliminating spatial within class variabilities come from a phase collapse.<|endoftext|>This paper proposes that using phase modulus operators instead of thresholding non linearites is beneficial to classification performance in the case of scattering like networks. The authors show that such a network with some learnable operators can come close to performance of small ResNets when they use the modulo operators and performance degrades when using other non linearieis. I also think the paper provides quite a bit of theory to support some of the claims. if it is to show that such a model can indeed be comparable to current architectures then OK, but it does not go beyond that. * Why are the resulting networks so parameter heavy? What can be done?<|endoftext|>The authors present some theoretical results, followed by some experiments, in support of their argument that the linear separability for image classification in deep convolutional neural networks mostly relies on a phase collapse phenomenon. The paper s significance and clarity will be improved if source code is provided, to make the results reproducible By eliminating the phase of zero mean filters they improve the separation of class means.They present a Phase Collapse Scattering network and demonstrate Resnet like accuracy. Eq 4: It may help many readers to derive this in the appendix, it should be a quick derivation The ResNet results while good are not state of the art. I was ambivalent as to whether the paper should be rejected because of these clarity issues, but since they can all be addressed I think I decided to be a bit lenient. But for the final ICLR submission they should be addressed.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper provides a thorough, detailed evaluation of model based RL, as realized via MPC based planning. Or the differences, if these are relevant for the results presented in the paper.<|endoftext|>Cons:1.Some details of the paper are missing. The environments are pretty interesting.<|endoftext|>The paper adds an increment to the collective knowledge of the field about RL methods.<|endoftext|>**W2.2 Narrow set of environments used** The employed set of multi task evaluation environments in the paper is quite small.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; 5.Figure 4 shows that the forgetting is very serious. The paper did some analysis in the experiment section to show how the transfer can happen in some limited experiments. 6.Section 5.2.1 and 5.2.2 are quite hard to follow as it has too many details. Many existing methods do better. Limited work has been done to improve class incremental learning by transferring knowledge from previous similar classes. This paper studies an interesting problem of knowledge transfer in the class IL setting. There are also a lot of issues in the experimental section. It is more like an analysis rather than showing the superiority of the proposed method. The author acknowledged this in Section 5.6. Many such systems have been published recently. Although existing methods do not explicitly transfer knowledge in the class IL setting, but they do have implicit sharing and transfer because the new tasks are learned based on the model built for old tasks. Thus, a comparison with them should be included. 2.Only a fixed sequence of classes and tasks arranged by you is used. Why did not you use all classes in the full data to make a sequence of 50 tasks. What about task 1?<|endoftext|>Strengths:  I really appreciated that the paper explicitly tackles the elementary questions of CL: "we investigate the stability plasticity dilemmato determine which model components are eligible to be reused, added, fixed, or updated to achieve this balance". It was already in SpaceNEt The authors claim that they address only the class incremental learning problem. Many baselines should be added to the experiments: DEN,EWC,LWF,MAD  The datasets used in the experiments are simple and very similar: cifar 10 or 100. This is not part of the method. in Table1, the performances of KAN  are much worse than the other methods but it uses much fewer parameters. In my understanding, it would be possible to use different hyperparameters to have a more fair comparison allocating a more similar number of parameters? From the reported performances, the superiority of the method is not clear. No readme is provided with the code. The presentation of the method could be improved.<|endoftext|>This work aims at addressing the stability plasiticity dilemma in the class incremental learning scenario by exploring which model components shold be reused, added, fixed or updated. There is no explanation on why activation of a neuron could be a good indicator for relevance between classes. It is suggested to calculate the statistics about how much portion of neurons are reserved, reusable and free for switching between similar tasks, and how much for the switch between dissimilar ones. The proposed method also shares some similarity with PackNet although the PackNet does not explore similarity between tasks but tries to compress the used neurons for each new task. Authors should compare the proposed method with it to see which method is more effective in address the stability plasticity dilemma. In addition, comparison on larger datasets such as tinyImageNet and also the one across several datasets such as Oxford Flowers, Caltech UCSD Birds and MIT Scenes should be conducted. Good motivation and extensive ablation study. In addition, it should be compared with more rehearsal free methods on larger datasets.<|endoftext|>### Summary:The paper studies replay free continual learning with the focus on the plasticity stability dilemma. More specifically, the paper proposes the KAN method for class incremental learning where the forward transfer is achieved by detecting similar knowledge and reusing the first few layers,and negative backward transfer can be alleviated sparse connection allocations for different classes of different tasks. The paper also studies a limitation of the softmax layer, which is an interesting contribution. ### Strengths:(1) The paper is well motivated, well written, and easy to follow. ### Weaknesses:Although I enjoyed the experimental design, and agree with the intuition behind KAN, the proposed method have major weaknesses:(1) First, there are several hyper parameters involved, such as the number of layers to be reused, the sparsity level, etc. I believe since the paper is an empirical work, the fact that the proposed method is not performing better than baselines hurts its contribution. (3) Generally, while reporting metrics, it is not clear how much average forgetting for different methods are (see [1, 2] for the definition of forgetting metric). I think there was an error in figure generation. ***UPDATE**   While I am not fully convinced regarding (1), the new version of paper addresses (2) and (3) an I now lean towards accepting this work. I believe the proposed method has its flaws but overall I believe it is an illustrating study and I lean towards acceptance.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The weakness is that the idea is simple and not novel enough. In addition, the experiments cannot verify the effectiveness of the proposed method. All in all, i think this paper is not novel and solid enough to be accepted by ICLR.<|endoftext|>Similar strategy could be seen as early as Viola Jones cascade classifier, but in the domain of semantic segmentation, I do not remember seeing similar design, so in that sense this paper does have some small novelty, though it s incremental. 2.That said, the ideas of merging multi scale predictions in semantic segmentation have already been studied in many existing work, from the early FCN [1], to more focused studies on this such as [2] and [3]. Besides, a few other issues or questions of the paper:   3.1. 3.3.Is r set to a fixed value for experiments on all datasets? The paper proposed a method to selectively merge multi scale predictions, but failed to compare to any other methods aiming for similar goal (e.g.max pooling, attention). There is no experiment to validate the "adaptive" design of the mechanism either (compared to fixed threshold or top k).<|endoftext|>Strong Points:1,  The paper is easy to follow and understanding. Here are the details:1, Several related works are not cited and compared. These work may decrease the novelty of this paper. If not, it may be not suitable for the ICLR submission. Compared with exsiting methods on scene understanding, the proposed Adaptive Confidence Mechanism is little weak.<|endoftext|>The strength of the approach is that experimental results show improvement against state of  the art for mIoU score. However the paper lacks clarity and coherence:  foreground/background are mentioned extensively in the paper but these are not defined. How are these relating to the classes defined with the dataset used in the experiments? class imbalance (between foreground/ background )   is mentioned as an issue in the abstract but it is unclear how this method addresses class imbalance. However clarity has to be improved.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; Typo: "the phenomenon that that neurons that have"I think a lot of claims are not justified either by experiments or citations. After explaining the algorithm and its relation to minimal representation the authors evaluate it on vision and RL tasks. > I need either a citation or evidence for that. In particular I don t see why each predictor would not learn to filter out unnecessary information in the representation. Using the paper s example of the pen: knowing the position of a distractor object is not necessary to solve the task, yet all the predictors could learn to filter out this useless information. Are these results in the paper?<|endoftext|>While the method is admirably simple and the writing clear, the main claims of the paper are either questionable or without any supporting evidence. Nonetheless, to justify the original statement, it would still need to be shown that this improvement is due to the avoidance of spurious correlations. Some performance gains are shown in RL settings, while some stability gains are shown in a particular vision setting. This paper proposes using an ensemble of predictors or heads on top of a shared representation to improve performance.<|endoftext|>Motivated from the idea that minimal representations — that encode information relevant to a task and nothing more — are likely to generalize well, the paper proposes an approach, titled Model Invariance (ModInv) to learn representations. Based on further ablations,  the authors demonstrate how sensitive ModInv is to the number of predictors and the diversity in terms of training dynamics (in terms of learning rate), and whether ModInv is complementary to recent augmentation strategies in reinforcement learning. Given these observations, I am not entirely confident of the utility of the proposed approach. Additionally, I would suggest the authors appropriately adjust the claims regarding performance improvements. In particular, the weaknesses influence my current rating of the paper the most.<|endoftext|>## Strengths  The paper is well written and easy to understand. ## WeaknessesI have two main concerns to be fully confident of the gains claimed. Nevertheless, approach is very closely related. In that sense, I would then believe that the gains are due to the same reason deep ensembles work, than for an information theoretic reason of learning minimal representations. It is instead the same reason that deep ensembles work, i.e.finding a set of good minima which induce a reasonable functional diversity. But the authors approach the problem from an information bottleneck perspective, the robustness claims from which are not entirely supported. Hyperlinks do not work in the main submission.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper presents a mainly theoretical explication of the relationship between natural image statistics and perceptual distances for small image distortions. I have minor comments.<|endoftext|>The authors present few relation between statistical learning and perceptual distances. Nothing new is presented about the relation between perceptual distance and human perception. However, I also think this paper is too preliminary. Psychophysics: No new data is presented.<|endoftext|>The work presented in this paper aims to analyze the relationships between the probability distribution of the data, perceptualdistances, and unsupervised machine learning. (I assume).<|endoftext|>Interesting (but debatable) observations between image prior, image quality, and perceptual distances. For Observation 3: The reviewer believes this observation will largely depend on the capacity (i.e., information bottleneck) of $f$.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; This paper presents a method for the detection of backdoored neural networks. It is assumed that the user only will have access to the trained model and a few clean validation samples, not the training data. Based on these loss terms, triggers are reverse engineered for a dataset containing malicious and benign models. The performance of the proposed approach is tested under various experimental settings and ablation studies. It is pointed out that this way the topological loss would limit the search space of all possible triggers, which as a result would increase the chances of finding the true one.<|endoftext|>This paper proposes a diversity loss and a topological prior to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. These loss terms significantly improve the efficiency in finding trojaned triggers. The experiments results show that the proposed method performs substantially better than the baselines on the Trojaned MNIST/CIFAR10 and TrojAIdatasets, respectively.<|endoftext|>The experiments show fantastic Trojan detection accuracy under the TrojAI benchmark settings. 3.A very detailed ablation study for the contribution of each item. This also leads to another question about the quality evaluation of recovered triggers. Are there any possible ways to apply the proposed method in a more realistic scene without the annotations of the trojaned model? 3.It will be better if the authors provide a more detailed implementation of baselines since the performance increase is very astonishing. This paper is well written and proposes a pretty novel trigger hunting method.<|endoftext|>This paper proposes a trojan detection method using reverse engineering techniques. This paper evaluates the proposed approach on two synthetic datasets and TrojAI benchmarks. The introduction of the diversity loss and topological loss is interesting and seems to improve the quality of generated triggers. 6.From the ablation results in Table 3, without the topological loss or the diversity loss, the results are already better than evaluated baselines. Why is this the case? The paper only briefly mentions it in the related work but does not empirically compare with it.
Reject; rating score: 3; rating score: 5; rating score: 6; In this paper, the authors propose an approach to perform co speech gesture generation. A vector quantized variational autoencoder is used to learn a codebook of different kinds of gesture sequences speaker s gesture manifold. Most of the discussion is about prior work on VAE, VQVAE, crossmodal conditional VQVAE decoding [1]. The paper is fairly confusing, and hence unclear at times. A big chunk of the paper talks about the representation learning of co speech gestures, yet there is no analysis of these representations. Here, what could happen is that the language encoder (in the third step) will now be uncertain about which code to choose from. This is in general not true as the network used here is non linear.<|endoftext|>This paper tackles with co speech gesture generation. For example, the authors claim that “….we consolidated a denoising characteristic to the variational autoencoder framework (DVAE), presenting a more flexible and robust posterior distribution approximation than the standard VAE”. However the correctness of this argument was not empirically justified. It is also an important limitation that the proposed method was tested on a dataset having only monologues. The similar comment applies for Fig.2.<|endoftext|>The authors present a method to generate co speech gestures by learning a vector quantized representation space of gestures coupled with a machine translation from natural language sentences to that representation space. 2.What reconstruction loss is used for the DVAE that works with gesture sequences? 3.In Section 7.1, the authors mention that they were limited to one person in their work.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper studies a variant of the online facility location problem where each demand point arrives with a prediction of which facility it is assigned to in an optimal solution. In line with recent work on learning augmented algorithms, the paper designs an algorithm whose performance degrades gracefully with the quality of the predictions and yet retains an almost optimal worst case guarantee. Overall the paper is a nice addition to the new area of learning augmented algorithms.<|endoftext|>This paper considers the online facility location problem with predictions. Overall, I would be okay with accepting this paper. This paper follows a recent line of work in considering online algorithms with error prone predictions.<|endoftext|>The paper considers the online facility location problem with predictions. I think the algorithm and its analysis are fairly non trivial but intuitive and enjoyable to read about, the problem being addressed is one of interest, and the lower bound/empirical results nicely complement the upper bounds.<|endoftext|>This paper presents an algorithm for online facility location with predictions. 2.The facility location problem is an important one. This is a credible paper on online facility location with predictions. But, I have concerns about the error metric, which is the most important parameter here   with the current error metric, the very realistic possibility of having a single bad prediction is also going to relegate the guarantees of the algorithm to those of having no prediction at all.<|endoftext|>The non uniform case requires a non trivial amount of analysis and may be of interest to the community. Weaknesses of the paper:While theoretically the problem is interesting   I am not convinced of the practicality of the model. This needs to be clarified. The key issue that needs to be addressed is the motivation for assuming that new facility locations can be predicted using a machine learning model. Otherwise, I think theoretically the paper is worth publishing, albeit at a more theoretical computer science (such as ESA, or APPROX ).<|endoftext|>This paper studies the classical online facility location problem in a metric space. Such predictions can potentially be acquired through historical data. I point some of them out below. They extend this approach in a non trivial way to the case with non uniform opening costs.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This manuscript developed several algorithms (e.g., AMSGrad EG, AMSGrad EG DRD) for nonconvex nonconcave min max optimization. The convergence result of AMSGrad EG DRD is shown under the one sided MVI condition. Strengths: the paper is well written. However it seems that the authors did not consider a projection in their algorithm.<|endoftext|>The paper observes the one sided convergence phenomenon of GAN s training, and proposes the one sided MVI condition suitable for this problem. Then the convergence analysis is provided for the proposed AMSGRAD EG and AMSGrad EG DRD algorithms. Cons:My main concerns of the paper focus on the motivation and the experiments. In the experiments, the authors use the Wasserstein GAN [Arjovsky et al., 2017] to conduct the experiments. This means that the used WGAN model may not satisfy Assumption 1 or Assumption 3.<|endoftext|>This paper analyzes two variants of the Adam optimizer, and proves their convergence under either the standard MVI condition or the newly proposed one sided MVI condition. The proposed one sided MVI condition is interesting, and the analysis under this condition provides new insights into the convergence of adaptive optimization algorithms on min max problems such as GANs.<|endoftext|>This paper analyzes the performance of Adam type algorithms (AMSGrad, to be specific) in nonconvex nonconcave minimax optimization. As I am not an expert in min max optimization or GANs, I cannot be very confident that the results are completely new. The authors have chosen a general nonconvex, nonconcave problem to analyze, and have shown the results under the standard MVI assumption and the one sided MVI assumption.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper presents a new form of the plug and play (PnP) half quadratic splitting algorithm with provable convergence. This builds a bridge between the recently emerged score based generative model and plug and play methods. And even more surprisingly, they show such a new formulation within the PnP framework leads to a very strong theoretical convergence guarantee under mild realistic assumptions. The proposed gradient step denoiser formulation in the PnP framework is particularly interesting. Moreover, they claim the established theorem can even be extended to non convex $f$   this is also super exciting. ## Cons.I have some questions that need clarification:   The main results are obtained based upon HQS algorithm. I m wondering whether the theoretical results are still validated on other proximal algorithms, e.g., proximal gradient descent, ADMM, etc. Could the authors provide more details on how they compute the Jacobian? How to do that with the PyTorch automaticdifferentiation tools?<|endoftext|>By replacing the proximal of regularizer with this gradient step denoiser, the authors proposed the GS PnP algorithm based on the half quadratic splitting algorithm. This paper is closely related to the recent trend of learning a regularizer functional by using deep neural networks. 2.The convergence analysis seems to follow the standard proof in the non convex optimization literature when the explicit regularizer is given. 4.Various paper has proposed backtracking steps to determine the step size ($\tau$ in GS PnP). Perhaps a comparison and discussion are required. 5.The final performance is about the same as the state of the art plug and play algorithm (DPIR). 6.Given the linkage to RED, the current paper should provide a proper review of the progress of RED as well. In general, I vote for *weak rejection* based on the current evaluation of the paper. —— After rebuttal ——I now vote for acceptance.<|endoftext|>The paper considers a novel and interesting idea: designing a deep neural network denoiser that makes Plug and play priors (PnP) convergence analysis clean and simple, motivated by PnP HQS [Zhang et al., 2017b] and regularization by denoising (RED) [Romano et al., 2017]. Existing works have proved the convergence of PnP and RED with contractive and nonexpansiven denoisers. By using such denoising step within PnP followed by the proximal of data fidelity $f$, one can guarantee convergence via traditional non convex optimization given the objective function $F(x)$. Finally, the performance and stability of the proposed method is evaluated over three image inverse problems such as deblurring, super resolution and inpainting, with satisfactory results compared to existing methods based on PnP and RED. I think the overall idea of this paper is great: designing a differentiable neural network, by construction, that equals the gradient descent step of an explicit regularizer function, the fact which provides new insights for solving an important open problem about the convergence of PnP and RED. The idea sounds simple but promising and makes it possible to be directly applied for other variant PnP/RED algorithms. This can be done directly on the testing images. 6).Having a convergent plot against PSNR (dB) would help to see the speed of improvement in imaging quality.<|endoftext|>The paper proposed a new denoiser, based on the gradient of a proposed trained regularizer, to be employed within a plug and play (PnP) framework, which is claimed to have a convergence proof compared to other existing methods (which claimed to use "unrealistic" assumptions). Experiments are provided to support the efficacy of the proposed method. This work is well written and easy to follow for the most parts. I would like to change my evaluation based on the authors  response, to accept. How is it related to your work? Does it mean the proposed work is able to work with any denoiser and for any IR tasks (e.g., compressive sensing)? Perhaps “explicit” proximal mapping. .”Nothing has been proved for the cases where “$f(x)$” is not strongly convex; however, the proof of theorem 1 is based on the strong convexity of “$f(x)$” and . they are not introduced. For instance in Table 2, how are the results obtained by other comparative methods (e.g., IRCNN and DPIR)? → “i.e.”Post rebuttal:I thank the authors for their careful responses to the reviewers’ concerns, comments and questions.
Reject; rating score: 5; rating score: 5; rating score: 5; The paper proposes a post hoc KDE based method (NUQ) for single pass uncertainty estimation. Various other methods in that direction have been proposed recently (SNGP, DDU, DUE, etc) to avoid computing an expensive model average over multiple forward passes. **Novelty**: My understanding is that the paper mostly combines results from the literature (the upper bound on the Bayes risk, the variance of the excess risk). The small scale experiments (toy classification and MNIST) only compare to deterministic networks which are well known to not provide useful uncertainty estimates. * The statement in the 3rd point at the end of the introduction that recent work “suffered from a lack of a principled method to quantify the uncertainty” is extremely vague and should be substantiated (what do you mean by principled and how is prior work not principled?) or removed.<|endoftext|>This paper proposes a nonparametric uncertainty quantification method for deep neural networks. Using a kernel based estimator of the conditional density (i.e., the predictive distribution $p(y|x)$) in the feature space of a pre trained neural network, the epistemic and the aleatoric uncertainty can be obtained separately from an upper bound of the excess risk and Bayes risk. Pros:  The paper uses the excess risk and Bayes risk to quantify the epistemic uncertainty and the aleatoric uncertainty, which is interesting. Substantial experimental results are provided on several different image datasets. 1) The method is actually only nonparametric in the feature space of a neural network. Previous work (Van Amersfoort et al., 2020;2021, Liu et al., 2020) shows that a crucial difficulty of uncertainty estimation in deterministic neural networks is feature collapse, i.e., the network maps both data samples and OOD samples to a very narrow region in the feature space, resulting in unexpected/uncalibrated predictive uncertainty. However, there are several other methods that can produce principled uncertainty. I suggest the authors provide a detailed discussion and justification about the efficiency. My second concern is about the experiments. As mentioned above, I think more methods should be compared, such as Kristiadi et al., 2020, DUQ and SNGP.<|endoftext|>This paper proposes NUQ, a kernel based estimation of epistemic uncertainty regarding a kernel based probabilistic classifier applied on the input or on the top hidden layer on on output logits of a pre trained deep net classifier. Hence I would advise against using the output logits. It does not look like the authors did a serious job of optimizing the hyper parameters of the other methods. In cases where the generative distribution is known, this should be easy to compute. The proposed approach is suspicious on theoretical grounds, in that it relies essentially on the ratio of some estimation of training uncertainty and density in representation space in order to estimate epistemic uncertainty. Finally, there is no attempt to actually compare the estimated epistemic uncertainty with the ground truth epistemic uncertainty.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The authors focus on non linear embeddings generated through neural networks. They observe a collision problem in the latent space and attempt to resolve it by introducing a regulariser based on Lipschitz continuity. Are those high dimensional tasks? In the supernova and redshift what are the dimensions of the problem? I have several questions that I would like to kindly ask the authors, hoping this can improve my assessment of the paper: 1. 6.Figure 1 is unclear: what are the original space points and what are the latent points here? Estimating the Lipschitz constant is an extremely hard task to do. This paper seems interesting but is not convincing enough for an acceptance.<|endoftext|>The paper studies the use of embeddings for high dimensional Bayesian optimization and introduces a regularization term into the model training to ensure smoothness in the latent space, so that it will be suitable for GP modeling and Bayesian optimization. I do not think the paper provides sufficient evidence to support this contribution. In particular, was BO given 100 or 400 points for its initialization so that all points get the same amount of data? As such I do not think it is ready for publication at this time.<|endoftext|>Current approaches aim to learn an embedding to optimize the objective in a low dimensional continuous latent space. This paper provides evidence that with current approaches, different data points in the input space can be mapped to same point in the latent space. ## Merits  Adapting Bayesian optimization for high dimensional search spaces is arguable one of the key challenges at the moment, and learning embeddings is one of the most promising directions. How many datapoints does the original benchmark have? Why is TPE and BO missing for the Max Area and the HDBO problem? This is somewhat contradicting the intuition provided by the authors. However, I am underwhelmed by the empirical results of the paper and not yet convinced that the proposed approach actually works reliably in practice.<|endoftext|>For the same hyperparameters used in BO exp, is it consistently observed that the collision is well suppressed? “GP are not intrinsically designed to deal with structured input” sounds a bit controversial, any reference for this? The paper focuses on an issue in latent space based Bayesian optimization and proposed a solution for it. Therefore, I think an improvement is required for the paper’s acceptance. I will be expecting that some of my concerns are answered in the discussion period.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors provide theoretical analysis for this framework, study its scalability for practical applications, and presents good empirical results. In particular, I like the idea of defining an isomorphism test that relies on WL as a subroutine (i.e.the 1 Subgraph 1 WL*) and thus "uplifting" the expressive power of WL. In particular, I think Theorem 4 is the most original since it manages to establish a connection with all the tests in the k WL hierarchy for $k \geq 3$. Like other recent papers, the proposed method manages to benefit from cutting through the k WL hierarchy by achieving more advantageous complexity tradeoffs compared to k WL based approaches. The proposed sampling takes into account how the nodes are covered by different subgraphs and the method itself is adapted to avoid potential complications at testing time when no sampling is used. It is also encouraging that the framework brings improvements irrespective of the base GNN that was tried. The context representation from Equation (6) is based on subgraphs rooted at neighbours of node v and therefore it might be based on information that is outside the subgraph of v, thus becoming misaligned with Eq(2) and the proposed subgraph WL equations. However, the evidence that this subgraph sampling strategy improves generalisation in any way is weak. It is therefore very difficult to assess the statistical significance of these scores. A stronger baseline for ZINC and MOLHIV exists: *Weisfeiler and Lehman Go Cellular: CW Networks (NeurIPS 2021)* (https://arxiv.org/abs/2106.12575). In fact, this work is also particularly relevant for this paper since it is also relying on induced subgraphs for "uplifting" existing GNNs, but it does so in a different way.<|endoftext|>The paper proposes an algorithm to overcome the expressive limits of the standard GNNs, which are upper bounded by the 1 WL. The main idea of the paper is to, for each node v, extract the subgraph induced by nodes of at most distance k to node v, and then deploy a GNN on top of each of these subgraphs. Further, the authors study the gains of the expressive power of this architecture compared to the 1 , 2 , 3 WL, and k WL. They simply consider the weaker version of the 2 WL, which is known to be equivalent to the 1 WL (see, e.g., https://arxiv.org/abs/2104.14624). This should be clearly stated in the main paper. The proof is not rigorous enough and of a very handwavey nature. **However**, it does not show that their architecture is always at least as powerful. This also has implications for Corollary 3.1. In Section 3.2, you should state the k needed for the results. However, there are some problems with the theoretical contributions, see above.<|endoftext|>The paper proposes GNN AK, a framework that can use GNN as a kernel to encode local features. It generalizes the local neighbour aggregation in Massage Passing Neural Networks (MPNN) from a star like pattern to a more flexibly defined subgraph. The paper also provides theoretical support that shows the superiority of the proposed method to 1&2 WL in terms of expressiveness. The experimental results demonstrate that the proposed method outperforms other SOTA baselines on 7 different datasets. The proposed GNN AK framework is novel for uplifting the expressiveness of GNN. It is an interesting idea to use GNN as a kernel on the induced subgraph. The design for both the GNN AK layer and subgraph sampling are reasonable. This paper didn’t explain how it gets d_{i|j}^{(l)} from distance to centroid. It is counter intuitive if a node with a larger D2C gets a higher weight during aggregation. arXiv preprint arXiv:2106.12575 (2021). The proofs and experimental results well support the proposed method.<|endoftext|>In particular, at each layer of GNN AK, the authors first extract all $|\mathcal{V}|$ ego nets of the original graph, then they apply a GNN on each one of them, and finally, they collect their outputs into node wise representations. The expressive power of different variants of their model is theoretically analysed (more expressive than 1 WL when the ego net aggregation is as expressive as 1 WL, and no less powerful than 3 WL, when the ego net aggregation is as expressive as 3 WL) and a series of design choices for a practical instantiation is provided. I am not sure I follow the claim “and the rewiring of constructing its non isomorphic counter graph has picked two edges that cannot be included by any k hop ego nets with k ≤ 4”. Is that a known fact from the original CFI paper? For example, a reference that the authors have missed, “Identity aware Graph Neural Networks”, You et al., AAAI’21, is also closely related and it goes one step further in terms of the theoretical results, showing that by identifying the root note in each ego net (implicitly present in this work – via the distance to centroid), one can also count cycles. The authors do provide a novel theoretical argument in Theorem 4, which is good, however, its practical consequences are not clear to me (I don’t understand the following sentence: “This opens a future direction of generalizing rooted subgraph to general subgraph (as in k WL) while keeping number of subgraphs in $O(|\mathcal{V}|)$.”). Can you theoretically compare ego net enabled GNNs with other modern GNNs, such as the ones that use substructures or other structural properties to improve expressivity? **Other comments**:  **Experiments**:     What version of GNN AK do you use in Tables 1 and 2?
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper proposed GraphMVP to pretrain molecular representations by using 3D information with SSL tasks. The intuition of using expensive 3D geometry information to pretrain molecular representation is convincing since (1) 3D geometry is essentially vital for molecular property and (2) it is usually unavailable for downstream tasks. Although generative and contrastive SSL are widely studied in graph representation learning, extending them into considering both 2D and 3D inputs is non trivial and well tackled by this work. One example is that for the generative SSL task this work proposes to do the reconstruction in the latent space instead of data space, where considering reconstruction is much more complicated. This insight is interesting and novel. 3.The ablation studies are extensive and well presented. The current results are mostly on small datasets (See Table 8 in supplementary) and it is not clear that if the included tasks are highly related to 3D information. More explanations of this point should be considered. More convincing experiments could improve the quality a lot.<|endoftext|>This work aims to leverage additional 3D geometric information for molecular graph representation learning and proposes a multi view pre training framework, GraphMVP. Specifically, in GraphMVP, both 2D and 3D information are used, and a combined loss function (i.e., the combination of contrastive self supervised learning loss and generative self supervised learning loss ) is adopted to enhance the quality of representations. Indeed, 3D geometric information is useful and provides a complementary view of the data, as evidenced in existing works such as protein classification. Yet, there are also some weaknesses:1. This is a little bit confusing to readers. 3.The improvements on some datasets seem to be marginal. This paper proposes integrating 3D geometric information to improve molecule graph representation learning. The idea is interesting and the experiments are comprehensive.<|endoftext|>This paper presents a pre training method of GNNs for molecular graphs by self supervised learning (SSL) on not only 2D topologies of graphs but also 3D geometries of molecules. In particular, the paper develops two SSL pretext tasks learning inter molecule and intra molecule associations in 2D and 3D. The "inter molecule" task is a graph classification on whether a 2D, 3D graph pairs come from the same molecule or not, while the "intra molecule" task is a generative task to generate 2D from 3D as well as 3D from 2D information. The idea sounds interesting, but at the same time, a big question on how to handle "conformer ensemble" seems not well addressed. 3D information is not unique for a single molecule, and this would be the main difficulty why past work didn t take into account it well. Just all of the possible 3D conformers fed into learning by data augmentation? But it is unclear how to address this ensemble problem. This point seems to remain unclear. I guess that s why (2D) GNNs are sufficient usually.<|endoftext|>The manuscript proposes a method for molecular graph representation learning by combing graph topological structure with 3D geometric information. For molecular structure, it is natural and necessary to consider 3D geometry information together with graph structure. This would be a great direction to explore. My concern is the experimental part where only molecular property prediction has been discussed. It would be necessary to illustrate the proposed graph and 3D geometry method can indeed improve reconstruction and generation for molecular data by comparing with other state of the art graph based methods. I like the proposed idea.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper has several findings taking a bias variance trade off of DG performance, that explains existing DG algorithm performance variability. Strenths:* The paper is one of the few early attempts to establish the theoretical framework of OOD/Domain Generalization. * The connections with Rademacher complexity is interesting. I strongly recommend toning down and rename it to emphasize the theoretical contribution of this work. And indeed some findings are alreadly manifested in those works. * The theoretical significance is not enough for publication. Domain generalization is unachivable without the formalization of distribution shifts but here only considers the risk over the mixtures of source domain, which is insufficient to provide out of distribution generalization guarantees.<|endoftext|>This paper presents a theory of domain generalization based on statistical learning theory (Rademacher complexity) and demonstrates a trade off between training loss and model complexity. The experimental parts report results on small models, which contradicts some existing results on larger ones. Experiments on the DomainBed benchmark show the effectiveness of the proposed method. Extensive experiments showcase the validity of the theory. One interesting observation is that IRM has the smallest model complexity and held out domain accuracy in the baselines, which contradicts existing works. [1] theoretically shows that it is easy to fit the IRMv1 objective while behaving like ERM on held out domains. Also, the held out accuracy in VLCS datasets of ERM models seems to decrease with model complexity. # Minors   Across domain accuracy and held out domain accuracy are used interchangeably. The average loss of data instances in training domains?<|endoftext|>This paper considers the problem of domain generalization (DG), wherein predictors are trained on a related set of training domains and evaluated on an unseen test domain. They go on to say that they do something more principled with a learning theoretic analysis of the problem. **Insights from Thm. Overall, despite the interesting insights, I think that this paper is not yet ready, and so I recommend that it not be accepted. In any case, Thm. * Define what is meant by "data in the wild" (page 1). **  I quite liked the experiments in Section 3.1. **  There were several places in the paper where the notation/word choice was confusing. **Softening some of the claims. I think that this is misleading, as the authors are not "solving" DomainBed. What is "dynamic" about it?<|endoftext|>No discussion on the size of n in theory section. Usually n is very small. 2.Overclaiming model complexity as the main reason explaining the DG results. I feel that it is a good paper shedding light on ill formed DG problem. The conclusion on regularization does not seem a novel idea to me, but I do like the theoretical rigor to support it. The discussion after Theorem 2, however, misses the point of what empirical domain generalization is about. But the tasks typically have 4 5 domains which makes generalization harder. The only issue is that in the DG benchmark datasets (and many real world settings with small domain shifts), ERM seems to perform better. The claim that complexity determines the OOD generalization is not supported by the experiments. All the numbers reported are lower than state of the art.
Reject; rating score: 3; rating score: 5; rating score: 8; This work presents the use of a hybrid CNN Transformer model for few shot image classification. The paper also includes a Conv based hybrid model and uses Omniglot, miniImageNet, and tieredImageNet for evaluations. Strengths: – The overall idea of the paper to increase the learning capacity looks interesting. – While respecting the effort of the author(s) on evaluation of their idea using tieredImageNet, I think we need some additional large datasets instead of Omniglot or miniImagaNet in the case of measuring the effectiveness of a model with higher capacity.<|endoftext|>This paper suggests HyperTransformer, a transformer based model that produces weights of CNN models in meta learning setup. Experimental results show that the proposed approach improved the performance of CNN models below a certain size in few shot classification and semi supervised few shot classification tasks. I think I get the idea, but it s good to be wordy for a broader audience. This paper shows that the recent transformer architecture works for meta learning setup as a hypernetwork.<|endoftext|>The paper shows an interesting way to apply "high capacity" transformers to generate the weights of a low capacity CNN model for few shot image classification. However in most practical cases I have worked with the cost of a larger backbone network is fairly minimal on modern hardware, it s still nice to have smaller models where possible and the paper demontrates that this can be achieved with comparable accuracy.
Reject; rating score: 5; rating score: 6; rating score: 6; This work empirically studies for deep networks the relationship between (1) model robustness and (2) the decision surface. A novel metric is proposed, the Populated Region Set (PRS) metric, essentially the number of regions in decision space which have at least one training sample. The authors claim the metric has a "strong relationship" to robustness, as measured by correlation, and present a number of experiments to support their claim. I think there is a lot of interesting empirical work to be done in this area. * Figure 2 is interesting: the PRS ratio clearly decays to zero for network B. What do the authors mean by this? How is it actually possible that you can compute your proposed measure?<|endoftext|>Towards that, the authors introduce a new metric, the so called Populated Region Set (PRS) whose ratio is later used to investigate the robustness of a selection of DNNs empirically. The paper is well written and clear. Although I am not knowledgeable of the related work on the interaction between decision boundaries and model s robustness, I  find the type of analysis/comparisons made (which I indicated in the summary) insightful. For instance, what happens when the number of classes is large? What do the authors think about it?<|endoftext|>The paper proposes a new metric, the size of the populated region set (PRS), as an explanation for models with similar clean accuracies reaching very different accuracies under adversarial attacks. In figure 4, 8 and 10 we see scatter plots of lots of models trained but again it is not clear how they are different. Do the results hold for l_inf? The authors give the example that it is hard to choose between models with similar clean accuracies. But in that case I don t see the advantage of using the PRS ratio instead of directly using robust accuracy.
Reject; rating score: 1; rating score: 3; rating score: 6; rating score: 6; In this paper, the authors developed a transformer based model, GeneBERT to align DNA sequences with regulatory elements. In particular, GeneBERT first applies transformers to learn representations of sequencing data and regulatory regions (e.g., open chromatin), and then aligns the representations of two modalities for identifying region aligned sequences. •	Large scale multi modal data integrationWeaknesses•	The authors need to elaborate on the details about datasets and data processing such as how to select TFs and binding motifs to construct 2D modality, how to merge 17 cell types, which cell types in fetal data were used. •	scATAC seq is sparse and noisy. Also, ENCODE TF ChIP seq data were for cell lines, which may not match the cell types that the authors used.<|endoftext|>Furthermore, the representations pre trained by our multi modal self supervised method have much smaller distances inside each cluster of cell type.” This doesn’t appear to be true to my eye. Their learning procedure is inspired by the NLP method BERT and uses several tasks from that work and its successors. However, this visualization also doesn t really help interpret the model; mutagenesis or saliency methods applied to DNA sequences would be better. The authors explore a new method for self supervised pre training before tackling several regulatory sequence analysis tasks, but are not able to deliver a clear method description or compelling empirical results. The authors’ description of the method does not contain sufficient details to understand it. Where does the ATAC seq data come in? Isn’t each position in the sequence depth 1? The authors suggest that they used a model pre trained on ImageNet. Where did the labels as disease related or not come from? Can the authors interpret what their model has learned that alternative approaches have not?<|endoftext|>The manuscript describes GeneBERT, a self supervised and multi modal pre training approach for genomic data. * The paper is well written and makes effort to explain diverse concepts from biology and machine learning to a wide audienceWeaknesses * While the formulation is indeed a novel multi modal construction as claimed by the authors, the generalization beyond genome data is not clear and hence I am not sure if this will be of broad interest to the ICLR attendees. The problem tackled by the manuscript address an important need in repressing genome data using both sequences and additional measurements.<|endoftext|>This work proposes an approach, called GeneBERT, for pre training genome data in a multi modal and self supervised manner. The main contribution of the paper is that they use transcription factor information in genomic regions which makes the model more generalizable to other cell types than the previous DNABERT that only uses the sequence information. I was expecting to see the results on many TFs. Why did the authors not show the other TF s results? It is not clear in Table 6, what are the classification tasks. Their main contribution, which is an important one, is to use the transcription factor information in the accessible genomic regions which are cell type specific.
Reject; rating score: 5; rating score: 5; rating score: 5; This paper explores the characteristics of the method, Integrated Gradients, as an attribution method, that has been proposed to explain black box models. “Baselines” in analyzing integrated gradients are discussed and the shortcomings of integrated gradients are further evaluated. The paper then proposes Integrated Certainty Gradients and shows its application on data. After considering the (un)fairness of attribution methods in this respect, the authors propose their attribution method. The authors then apply their method to data. On the application side:The paper applies their methodology to two scenarios in the main text and also provides the results of some experiments about accuracy on the MNIST dataset in the supplementary materials. The method does not show attributions of the dark squares in columns B and C of the ICG row. About the results of the supplementary materials, it is unclear how the results on the MNIST dataset can be evaluated and compared against other results in the main text. Looking at the theory and application together, the paper lacks novelty. Even though this paper approaches an interesting problem, it needs improvements and clarifications.<|endoftext|>It is interesting to exploit SHAP and BShap, two approaches based on the theory of Shapley Values, as the reference of "fair" methods. In this paper, the authors analyze the fairness of Integrated Gradient based attribution methods. Specifically, they present an "attribution transfer" phenomenon in which the Integrated Gradients are affected by some sharply fluctuated area across the integration path, thereby deviating from the  fair  attribution methods. To avoid the attribution transfer issue, they further propose Integrated Certainty Gradients method, where the integration path does not pass through the original fluctuated input space. Experiments are not comprehensive; please see the detailed comments. Various purpose designed experiments are performed to demonstrate the advantages of ICG in avoiding attribution transfer.<|endoftext|>The paper studies the unfairness of a family of integrated gradients based attribution methods, by the what they call ‘attribution transfer’. How to measure fairness. For my understanding, SHAP and IG are both post hoc attributions. How to get the damaged input. And blue and green rectangles as well. I’m also wondering is there any visual difference for ICG and IG in terms of the visualization results on the real image like they show in their paper? A typo, fourth line from the bottom of page 2, ‘any any’. This lowers my score.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; Strengths:  Comprehensive set of experiments on a tabular network and corresponding datasets using a single datasetWeaknesses:  Lack of novelty; see below  Lack of clarity and experimental details. In the end, this is an empirical work providing no novelty. Those details should be extremely clear. For one shot, also referred as single shot pruning, Lee et al.SNIP: SINGLE SHOTNETWORKPRUNING BASED ONCONNECTIONSENSITIVITY, ICLR 2019The paper does not refer to any approach in the literature. Therefore, I would expect comparisons to other approaches not based on the lottery ticket: importance based pruning; magnitude, prune while training, zero shot pruning.....  How are models with less than 2% accuracy selected? I guess that is an empirical selection, right? Clarity:  Would be more readable if the paper explains the lottery hypothesis and related pruning works and how those approaches can not be applied to tabular data; In the current form, seems like it is just plug and play: The paper states: We focus on applying this hypothesis to tabular neural networks. Why is this different to applying to other data / networks?<|endoftext|>This paper proposes two approaches to use sparse neural networks on tabular data. The topic is interesting and sometimes overlooked in the literature. There are some papers addressing this topic. You can start by reading some recent survey papers (e.g., [1]) and some concrete approaches which use sparse neural networks for tabular data (e.g., [2,3]). From Table 1, I can see that the datasets studied have quite a low number of features. Also, please add equations with the pruning criteria. The current writing style of the paper does not follow the academic writing style and it is more like a report.<|endoftext|>But as of now, I do not think the paper is well validating the method or this specific component; the paper lacks comparisons to the baseline pruning methods, with/without rewinding and/or using different norms. I wonder if this has any critical impact on the final performance. Still, having a work that formally studies pruning models for tabular datasets will be a nice starting point for future works in the direction.<|endoftext|>The authors apply a set of pruning techniques to the tabular neural networks from FastAI and examine whether the LTH still holds on the tabular datasets and their corresponding models. More importantly, it seems that we can pick a small network from the beginning (if I have understood correctly), which means we don t need pruning to get compressed networks. More experiment settings would be better. The experiments and the methods are clear in this paper, occasionally with some problems which I have mentioned above.
Reject; rating score: 6; rating score: 6; rating score: 8; An iterative process is proposed to learn a classification model. This paper extended the domain generalization on time series classification problem by modeling the distribution of sub domains within each domain and demonstrate its benefit in learning better representation. The overall technical novelty is marginal. 2.The theoretical insight is based on existing conclusion from (Sicilia et al., 2021). The problem formulation and techniques are mostly adopted from existing work, which reduce the technical novelty.<|endoftext|>Their approach ultimately results in a model for time series classification and for latent segmentation of data into discrete domains (aka user profiles). The model is framed using a min max adversarial formulation. This is an interesting paper and potentially the start to something really interesting. Strictly speaking, non stationarity is a problem with practically all real world classification problems, including images and other areas that "distributional" and "domain" work has been done. May 2021.[2] Rubio et al."Identification of hand movements from electromyographic signals using Machine Learning" 2020. Perhaps the authors can address this? However, in Fig 1b it seems that two subsequences within one series are considered separate domains?<|endoftext|>This paper focuses on one meaningful topic, targets the challenge, and proposed an interesting solution to solve it. Using the idea of domain generalization, this paper removes the influence of varying domains/distributions across patients/datasets through pseudo domain classification. Although this kind of model has been rather studied in images, but not a lot in time series. This work proposes a framework to learn a domain invariant representation of time series. The motivation and model design makes sense to me. Specifically, are the signals from different sensors (in the same segment) belong to the same domain label? If not, how to solve the problem? It s not clear to me that why the GRL learned representation is invariant to domains? It s not important for the whole paper. Implemented by neural networks?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper proposes to measure the reasoning capabilities of VQA models by training an "adversary player" that can manipulate the image and fool the evaluated models. If the evaluated VQA model produces a different output, the "adversary player" receives a positive reward and is then optimized with a reinforcement learning algorithm. Strengths:The paper proposes a general framework to evaluate the reasoning capabilities of current VQA models. Other State of the art models on CLEVR, such as MAC[1] and NS CL[2], are also likely to have consistent answers. The experimental results do not expose any new weakness of the evaluated VQA methods.<|endoftext|>CLEVR dataset is widely considered solved, especially with program synthesis based models which seem to achieve near perfect accuracy on CLEVR with a fairly small amount of training examples. However, this paper shows that it is possible to fool all classes of the CLEVR model using an adversarial player that simply re configures the scene (which should have the same answer as before). There is very little to complain about ablations, details, statistical tests, and thoroughness in experiments. S3.Interesting Results: The results are not always surprising. Overall, this is a solid paper with really interesting results. issues, but the authors have satisfactorily answered that.<|endoftext|>These settings reveal the weaknesses of visual reasoning models, and demonstrate that their apparent almost perfect performance on the standard CLEVR dataset may give imprecise impression about their “true” reasoning capabilities, as the adversarial settings may better reveal. Especially for CLEVR, there are multiple models that are e.g.based on  computing attention maps, executable programs, structured representations and other indications for their ongoing reasoning rather than just predicting an answer. * **Other datasets**: Another important way to increase the confidence that the failures happen due to reasoning and not perception, which is always a good practice in general   is to test the approach also on another dataset.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper addresses the problem of graph/subgraph similarity search in terms of the edit distance, termed GED/SED, respectively. Instead, the authors apply a neural network based approach, by designing a siamese graph neural network called NEUROSED. There is a good balance of empirical and theoretical results. Both are based on graph neural networks. Although they are not for SED, they are for subgraph isomorphism counting, the difference only lies in the loss function. If they are replaced with the same loss on SED, they can be reasonable baselines. The pair independent advantage is not unique to this method (see [a]). Moreover, it may not be that useful. In cases where each incoming query/graph is new, the model has to compute their embeddings on the fly anyway.<|endoftext|>The paper proposes a supervised model, NeuroSED, to compute Subgraph Edit Distance (SED) (and Graph Edit Distance (GED)). To this end, given two graphs, target and query graphs, NeuroSED uses a shared GNN to encourage embeddings for both graphs representing similar topological features. However, GNNs may not be that flexible, e.g., it is difficult for GNNs to identify simple substructures (Chen et al., 2020). I think there is still room to improve the paper. The model seems reasonable, and the experiments show promising results under specific settings. It is a good contribution to the SED field. Strengths  Efficiently computing SED is a vital problem. The paper is fairly well written. NeuroMatch also studies subgraph relations in terms of subgraph embeddings as in this paper. * The experiments are limited to small target and query graphs with graph diameters at most 10. Showing the performance on large graphs is necessary.<|endoftext|>This paper presents a simple and effective model for subgraph similarity search. Then, instead of using an MLP over the concatenated graph embeddings to regress the subgraph edit distance (SED) score, the paper proposes to use the l2 norm of the positive portion of the difference between two embeddings. Simple yet effective approach for estimating the SED from two graph embeddings. The paper is easy to follow and demonstrates the key insights very well. However, it should not be the only one satisfying the three properties. A discussion of [1] should also be included. Considering the strong empirical performance and the theoretical guarantee, I recommend an acceptance. My only concern is the possible limited technical innovation in Eq.(6), which desires more in depth analysis and discussion.<|endoftext|>The submission proposed to use GNN to encode graphs and calculate graph edit distance and subgraph edit distance based upon supervised learning with the generated graph embeddings. My minor concerns are:1). 2).Missing some related work, such as the following one. [1] uses random walk sampling paths and model the graph as a set of paths to measure the distance between graphs. The authors also provide proof of the properties of the proposed metric. Basically, SUBGRAPH search and matching is an important problem. The work well validated the effectiveness to use GNN for this problem. I weakly champion the acceptance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This work proposes a multi task learning architecture for neural processes termed the Multi task process (MTP). The proposed formulation was supplemented with theoretical results to show that the MTP generative model corresponds to a stochastic process. It was also useful to include the details of the specific architecture that was used in the experiments. The paper had no discussion of the possibility of negative transfer with this model. Can the authors comment on the MTP’s ability to deal with this issue? How does the computational overhead of the MTP compare to the other two methods? In my opinion, this work is original and well presented.<|endoftext|>This paper proposes a hierarchical latent variable model based on deep neural processes to model multiple functions jointly from incomplete data. Besides MSE, could the author provide some visualization results to demonstrate the difference between latent encoder with self attention and without self attention? (2) It’s unclear to me about the motivation of replacing the average pooling operation in the stochastic path with a Pooling by Multihead Attention (PMA) layer. Is it also task specific? (5) To make the results convincing, more than two previous approaches should be compared in the experiments.<|endoftext|>**Minor comments**Consider renaming to Multi task Neural Processes (MTNP), it would be much more clear. The paper is generally well written. What do you mean by intractable prior? It makes sense that the posterior $p(z|C, D)$ is intractable and approximated with $q(z|D)$, but not so much for $p(z|C_t)$ (according to the graphical model). It will not make a practical difference, but the explanation would be more clear. The derivations in A.3 of the supplement are not accurate. UPDATE: I have increased my recommendation from 5 to 6, given the clarifications and extra experiments provided in the rebuttal.<|endoftext|>This paper introduced multi task processes (MTPs), a new variant of neural processes, which jointly infers multiple heterogeneous functions to given possibly incomplete data. The authors conducted thorough experiments on several tasks to demonstrate the effectiveness of the proposed MTPs. (2) The paper lacks important baselines. To show the effectiveness of the designed hierarchical latent model in the latent encoder, it’s necessary to compare with a) MTP/STP/JTP without deterministic encoder and b) MTP without latent encoder. It does not seem to be a fair comparison. There are no theoretical guarantees on the bound defined by the approximation prior. (6) In the experiments, are results of all tasks obtained simultaneously? My main concerns are about the motivation of introducing NPs for multi task learning, the lack of important baselines, and the tightness of the bound.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper investigates the effectiveness of the higher order (second derivatives)in the context of remote cardiac measurement. The discussion about the trade off of low and high order dynamics indeed gives insights to future work. Weaknesses:The novelty of the proposed multi derivative architecture is limited since it is an updated version of the basic network proposed in (Chen & McDuff ,2018) with minor modifications. What are the pre processing steps in this work? Is this one of the reasons that the network is trained on a synthetic dataset? It is not clear why we need a large and diverse set of subjects to train the network.<|endoftext|>The use of artificial data is often key for the development of machine learning techniques in the medical field due to the scarcity of real data. I have however some doubts about the realism of the training data, do the authors believe that avatar and generated videos are realistic enough? Such an evaluation technique is important, as biomedical applications need to be assessed on their final goal and authors did will not to look at MSE of estimated PPG or derivatives, but to look at the accuracy of the estimated clinical features. I have however several concerns regarding the submission:As denoted by the authors, the use of normalized differences for the prediction of first derivative PPG has already been suggested, the innovative aspect of the paper is therefore quite limited.<|endoftext|>This metric has various clinical applications. The authors argue that by extracting second order derivatives in the trained DNN they are able to obtain improved results Strengths: This is an interesting application. Section 3 feels a bit detached from the rest of the paper. Perhaps provide there a figure illustrating what type of graph data is typically provided. How reliable are the second derivatives in such a case? Do the subjects need to be in a controlled environment for this to work? The main novelty lies in the application described.<|endoftext|>In this paper, a multi derivative convolutional attention network is used to estimate the high order derivatives of the 1D heart beating signals, with video data as input. A second order loss is used and the results are evaluated in first order(HR MAE) metrics and second order metrics (LEFT MAE). 2020.In summary, the higher order motion characteristics estimation from the video is an interesting topic while I have some concerns on the novelty and the experiment results. The meta learning has been proven to be useful for PPG estimation/cardiac motion estimation tasks.<|endoftext|>The paper will be greatly improved if the revisions suggested in this review are taken into account, especially improving the comparative results with other models and better justifying the use of synthetic data. The results are very interesting and show that by appropriately incorporating higher order dynamics, the performance of video understanding tasks can be greatly improved. Weak Points:There is a lack of clear support for the use of a synthetic dataset instead of a real one. Although the use of the attention mask is a reasonable assumption, there is a lack of comparative results of the full model with and without the attention mask. The comparison is not complicated and is suggested. The proposed architecture is interesting and works well, however there is a lack of comparison with other architectures to evidence that the proposed architecture is the best.
Reject; rating score: 1; rating score: 1; rating score: 3; I see “classification robustness” more in the literature, but this paper claims Def 2 as the “standard” without sufficient justification. The authors put “standard robustness” as “globally desirable” but “classification robustness” as not “globally desirable”, which does not look correct to me. To list a few examples (there are many more others):  * Wong, Eric, and Zico Kolter.<|endoftext|>The authors aim at creating a general and formal framework for uniting the different notions of robustness of neural networks. minor details:  The list of the approaches in the introduction does not correspond to the names introduced, which makes it very confusing between standard robustness and classification robustness.<|endoftext|>The empirical evaluation does not provide sufficient novel findings. Then some empirical studies are conducted. However, from the empirical front and theoretical front, the paper may not provide enough novel findings nor useful approaches.
Reject; rating score: 5; rating score: 6; rating score: 8; They claim to be able to handle large differences in gradient magnitude. The main contribution is based on the analysis of the individual components of the gradients instead of the individual gradients using the dominance rate. Through a series of experiments the authors show the proposed technique is competitive with other multi objective optimization approaches. Regarding the weaknesses of the paper I would point out the first claim that the method provides consistent improvements over all tasks. Multitask learning with single gradient step update for task balancing. Additional references and experiments are needed to draw conclusions regarding the actual empirical contributions of the method.<|endoftext|>This paper proposes a gradient based multi task learning method to balance multi task training by aligning the independent components of the training objective. The method is claimed to be scalable, robust to overfitting, and able to seamlessly handle multi task objectives with a large difference in gradient magnitudes. Although the performance improvements against existing methods are not that significant, the stable improvements on multiple benchmarks demonstrate the effectiveness of the proposed method.<|endoftext|>The paper presents an approach for better multi task learning in the presence of objectives with a large difference in gradient magnitudes. After rebuttal  I want to thank the authors for a complete and detailed response. ### Strengths * The authors deal with a very interesting problem, that of multi task learning in the presence of objectives with a large difference in gradient magnitudes* They propose a way of quantifying the imbalance "The rate of dominance" and use it to propose an elegant solution based on the SVD of the gradients of each loss, and an approximate version that is only computed on the shared parameters.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This can be a serious issue. Potential lack of generality to other conditional audio tasks or other domains such as image. The paper is technically correct and shows some improvements in the empirical evaluation. However, I think that moving from standard to non standard Gaussian priors is rather obvious. In addition, I am concerned about the generality of the approach, as it only focuses on two specific speech generation tasks and deriving appropriate means and variances from certain conditioning signals can be problematic or may not yield to an improvement. [Update: After authors  rebuttal I m changing my score from 5 to 6] However, results are only provided for two specific speech processing tasks. While in principle the proposed methodology could be applied to other tasks and domains, the benefit of it is not shown for those. 1.The procedure to compute instance  and time specific means and variances for the prior seems non straightforward and highly dependent on the type of conditioning signal, further complicating the generalization of the approach. In particular, regarding how hard is to adapt the approach to other tasks/domains or even conditioning signals, or how limited is the proposed idea. How do different procedures affect final performance? 1.There is at least concurrent work (https://arxiv.org/abs/2110.05948) investigating the effect of using other priors beyond the standard Gaussian. Perhaps contribution 1 could be softened in this regard.<|endoftext|>The authors show that, under restrictive conditions, the ELBO obtained with the proposed prior is smaller than that obtained with the uninformative prior. I am less excited about the methodological sections of this paper, which are not strong and precise enough to be convincing. Interesting and relevant idea, to improve the behavior of diffusion models. This paper is well written and the key idea is described in a simple way. However, the theoretical justification of the proposed method is somehow weak, and does not provide sufficiently compelling arguments on the observed behavior of the proposed method in practice. *** Post rebuttal / discussions remarks ***Thanks for the useful discussions and for the additional work to improve the paper. I have increased my ranking for the paper from 5 to 6. The presented method is discussed in an intuitive manner at first, using Fig 2. Nevertheless, after some thoughts, I think this figure can be misleading (even if it is just an illustration). However, results in Fig 4 do not show that the rate of convergence is improved in a practical endeavor.<|endoftext|>This paper presents an extension to conditional denoising diffusion models for text to speech (TTS). It would be interesting to study the influence of T on both models. Or PriorGrad with T 25. Weaknesses:  Mathematical justification of such prior is lacking. Same related work is missing. Code?The basic idea of using a better initial prior distribution to improve inference speed and training convergence is nice, although it lacks some mathematical justification. No discussion and numbers on the inference speed and real time factor (RTF), neither for vocoder or acoustic model. The experimental section is lacking too much. "The (DDPM) framework assumes the prior noise as a standard Gaussian distribution"No, it is not an assumption. It is a mathematical consequence of the definitions of the framework. Most of the math theory in the paper just shows that convergence rate and training behavior might be better for the proposed method. Those are not compared here. Experimental validation on acoustic model is weak as well. PriorGrad based on DiffWave.<|endoftext|>In this paper authors propose to use informative prior for conditional diffusion model when applied to neural  vocoding task. Relating to point above, can you clarify where is the \mu coming from in case? Noting that original paper had prior as standar normal, you have \mu also. Is there a theoretical explanation to it or is it only empirical observation? And in the same vein, why is 500k training steps better for the proposed than 1M training steps? I would like to see consistent style, for example all vectors to be bolded non caps and all matrices to be bolded and capitalized. However, I feel that modification itself is too simple to warrant an ICLR paper. The other point that lowers my score is that the final selection of noise variance is based on ad hoc technique and not justified theoretically, whereas other parts of the paper are.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes to use a set of structured keypoints as the intermediate representation for image generation, so that ideally each keypoint with control the location and semantics of a certain part. I also have concerns in terms of the baselines and ablation studies in experiments. Spatial fusion gan for image synthesis. I think the idea is good. But related works are not discussed thouroughly.<|endoftext|>This paper proposed a new way to disentangle and control the GAN synthesis process via key points positions, key points appearance and background appearance. Extensive experiments verify the effectiveness of the proposed method. The paper is well written and easy to follow. This also explains why it can be easily incorporated into SPADE. These are not highlighted in the main paper but are slightly discussed in F.2 of the appendix. I would suggest adding an in depth analysis of how these two hyperparameters influence the performance.<|endoftext|>Authors proposed the LatentKeypointGAN that detects keypoints and provides the editable capability using the keypoints. The overall training is performed in the unsupervised way without the explicit keypoint supervision. In the aspect that GAN provides the capability of generating the images with the photo realistic quality while the ability to control attributes are limited, the proposed methods contribute to the related society. > Few related works are missing: Authors proposed the interesting idea and implemented it in the effective way. However, I could remember an approach that already provide the interactive capability for GAN by representing its intermediate representation as the segmentation map. Personally, in the sense that the segmentation map is similar; yet more advanced than the 2D heat maps, authors contribution could be significantly reduced by this paper.<|endoftext|>The work proposes a novel method for **locally controllable image synthesis via keypoints**. FFHQ also has landmarks available. The method claims that existing works struggle with the disentanglement of pose and appearance, but the experimental section does not validate that their method outperforms existing approaches on that matter. For completeness, it would be good to report the keypoint detection results on FFHQ as well. The method might require as much tuning as the baseline approaches. However, there are **too few qualitative and quantitative comparisons to existing approaches** which makes it difficult to assess the quality of the proposed method in practice. **Post Rebuttal**:After the rebuttal, I lean towards accepting the paper as image fidelity improves over the baselines. This is an interesting and reasonable failure case that should be added to the limitations.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 8; The paper develops a deep ensemble based Gaussian process model and a variational inference procedure to train it. As a result, the authors obtain a method for improved uncertainty and predictions with deep ensembles. We define an NN GP Gaussian process prior. 2.We define a variational Gaussian process family, where the mean and covaraince are parameterized through the deep ensemble. The authors claim that this is "theoretically sound" and "principled" "Bayesian" method. However, the method seems to work better than the NN GP in practice, and, if the goal was to approximate NNGP, why wouldn t we just use NNGP? Is the idea that the specific family in which we approximate the posterior of the NN GP the key component here, and what we want is the closest possible deep ensemble to the NNGP posterior? The motivation isn t fully clear to me. I have a few issues with this section:1. I think your constrained in the constrained VI should not depend on the data. Restricting your model class to only the functions that get good accuracy on your train set doesn t seem like a valid Bayesian procedure. 2.I don t think there are bounds on DNN performance that are based just on the $L_2$  norm of the weights. 3.The regularization on the weights is a simple enough idea, I think it would be much more clear if you just said that you are adding an extra weight decay term without drawing parallels to constrained VI and generalization bounds. In light of [this recent blog post](https://cims.nyu.edu/~andrewgw/deepensembles/) I would like to ask the authors what exactly constitutes a Bayesian method, and why the proposed procedure is "more Bayesian" than the original deep ensembles? The recent paper [1] also proposes a way of making deep ensembles "more Bayesian", so I think it should be discussed in the paper. ## Experimental resultsI think the experimental results are the main strength of the paper. It seems like the method proposed by the authors performs well in a few practical settings, including CIFAR 10 and CIFAR 10 C. However, the performance is only marginally better than regularized deep ensembles on the in distribution CIFAR 10 benchmarks. I think the experiments could still be made stronger by including other realistic datasets, perhaps CIFAR 100. ## Questions  Do you train the ensemble from scratch using Algorithm 1, or do you pretrain the ensemble components first? However, the authors frame the main contribution of the paper as making a principled a theoretically sound version of deep ensembles. So, I vote for a weak reject.<|endoftext|>The motivation for using fully Bayesian methods over ensemble methods has been a contentious topic in recent years   while ensemble methods are prized for their relative simplicity, they lack the theoretical framework that grounds fully Bayesian approaches. In this work, the authors propose an interpretation of Deep Ensemble models (DE) under a variational Bayesian framework. In particular, the authors demonstrate how VI can be carried out in the function space, and leverage posterior regularisation on functions to incorporate prior knowledge into the model architecture. The benefits of the proposed approach are verified via an extensive evaluation covering a variety of different problem settings, whereby it appears that the DE GP consistently yields predictions having superior uncertainty calibration, and without compromising on predictive accuracy. ### Strengths 	The paper is very well structured overall and a pleasure to read. The paper also contains an additional experiment on a contextual bandit problem that I particularly appreciated, as it shows how the improved uncertainty estimates returned by this method can be beneficial when used in iterative decision making tasks. The experimental set ups are well explained in each case, and I appreciated that the results were all computed from scratch using implementations of existing methods (as opposed to simply copying in results from other papers that could possibly have been computed over slightly different dataset folds and experimental settings). ### Weaknesses 	Although I previously praised the paper for being well organised, there are several instances where the writing can be improved. There are quite a few typos and grammatical errors scattered throughout the paper which could easily have been fixed with a proper read through before submission. The introduction is particularly disappointing and I would recommend rewriting. For example, the reference to ‘graceful BNNs’ in the conclusion is quite odd. Perhaps it may be more suitable to place this section earlier in the paper such that any contributions can be more clearly referenced against earlier work? While interesting, the principal contributions of the paper are fairly incremental, and the authors themselves concede leveraging insights ‘inherited’ from other works as the foundation for some of the key contributions. Even so, this does not dent my overall opinion of the paper. The references need to be properly cleaned up   journal versions and conference proceedings should be cited instead of Arxiv editions of certain papers, while words such as Gaussian need to consistently appear as capitalized in paper titles. On the downside however, some of the writing needs to be heavily improved, while the connections to related work should also be reconsidered to further highlight the original contributions of this work.<|endoftext|>The authors propose using Deep Ensemble (DE) as basis functions to train a Gaussian Process (GP) with the primary motivation of making DEs more Bayesian. The Evidence Lower BOund (ELBO) being maximized is now optimized via variational inference (VI) over the functional space of bases (i.e.fELBO from [3] Sun et al.) However, these gains do not appear significant or well motivated. It is not clear what pain points of deep ensembles "not being Bayesian  are being  addressed and what concrete value the DE GP method brings the other forms of approximate inference against which the method is compared. In terms of empirical results, there does not seem to be significant differences between NN GP, DE GP, and DE in most cases. It could be better specified which parts of the pipeline are original work to emphasise contribution, as it currently only states which parts are borrowed. In various parts of the paper, the method is motivated by the fact that DEs "lack a proper Bayesian justification". This is confusing and the paper does not clarify how and why certain methods are considered Bayesian or not. It is stated that the objective of the paper is to make DEs more Bayesian, but also noted that doing so via DE GP may be at the cost of "trad[ing] off between theoretical soundness and flexibility in practical usage". ## Additional feedback (comments, suggestions for improvement)   Section 3.1 claims that the chosen linear, matrix value kernel is comparable to recent low rank approximations to original kernel matrices. to demonstrate this claim. It would be nice to do an ablation to better motivate why VI is necessary and important to consider in Bayesian inference. Additionally, negative log likelihood appears better than others, but there is no comparison with NN GP on CIFAR 10 for example. As uncertainty quantification is a major focus of most BNN / GP papers, additional methods and baselines should be included in the comparisons: mean field V I (motivating variational inference), AugMix (insights into the effects of data augmentations), ResNet architectures with and without batch normalization (analyze inductive biases contributed by architectures and generally how far off from most performant models). For example, it would be more exciting and interesting to use a mixture of Gaussians as basis and show that you can / cannot achieve non Gaussian and/or multimodal posteriors. Further uncertainty and robustness analysis can be justified. Since the GP has a linear kernel, the method is identical to a Bayesian linear layer after a mean only layernorm stage. ## Questions for the authors1. 3.How is expressivity of the posterior affected by the choice of prior process? There is also no clear indication of the problem being addressed and why being Bayesian matters.<|endoftext|>This paper presents a GP model with a deep kernel defined in terms of a (finite) deep ensemble (DE). A variational approximation and a regularization scheme were introduced to optimize the GP. The proposed methods were then demonstrated on several benchmark regression/classification datasets. NOVELTY & SIGNIFICANCEFirst, I am not sure why this paper is titled deep ensemble as GP posterior while the proposed method instead uses deep ensemble to parameterize the GP posterior. I was under the impression that this paper is about exposing deep ensemble as the natural posterior mean of a GP setup because it aims to provide a Bayesian justification for DE via GP but this is unfortunately not the case   one can in fact parameterize the GP with any deep network as its kernel but that is not the same as exposing them as inference results of a GP setup, thus providing a Bayesian justification. For the same reason as above, this work also appears incremental to me. It is pretty much a special case of GP with deep kernel but perhaps the point here is the DE is a fine choice for parameterization that can improve performance. As for the second contribution that involves the variational inference and regularization scheme, I have to unfortunately say the same thing that these are also too incremental as they are, as a matter of fact, vanilla inheritance from prior work with little to no adaptation: The variational expression is obviously inherited from (Sun, 2019) as the authors mentioned and it is also known even before that by the prior work of Titsias in AISTATS 09 where an ELBO of GP is derived based on a set of inducing points. The regularization is simply a L2 penalty that was well rationalized by prior work. Given the above, I think the key contribution of this work is the empirical investigation of a potential parameterization for a GP with DE, which is fine but marginal given the reported results in Table 1 (i.e.the improvement over DE and rDE is pretty much marginal)SOUNDNESSAll derivations appear correct to me. On another note, results in the contextual bandit look significant   perhaps the calibrated uncertainty will show its effect best in an exploit explore decision making scenario. But I am not sure how the authors set up the uncertainty variance for other non Bayesian baseline in this experiment? Could the authors clarify this in the rebuttal? As a more minor note, using bar plot in Fig 7 is kind of inflating the result differences. The difference between DE GP and rDE is less than 0.5% and we do not know if this is significant without showing the deviation bar   if it is similar to what was reported in Table 1 then it is not significant. This is an empirical work that aims to demonstrate the uncertainty calibration of DE GP and its improved performance over a set of DE variants.<|endoftext|>The paper presents a novel perspective on deep ensembles (DE) that aims at providing a Bayesian justification to DE. The proposed approach builds a Gaussian process (GP) using the ensemble members and performs variational inference in the functional space. Moreover, the paper introduces a regularization method that works directly in the function space. The proposed algorithm DE GP shows better results compared to DE an other well known Bayesian deep learning methods. why is "the diversity on the points far away from the training data is further fostered"? why not search over $\beta$ since it seems to change for different experiments? How sensitive is the performance with the changes of the hyperparameters $\alpha, \beta$? Minor comments:  Page 5: Require revising "given the fact that a voting is incorrect when at least one of the individuals makes a mistake"Overall, I find the contributions presented in the paper novel and important to the Bayesian deep learning researchers to know about. Such contributions open the door for more works adapting Bayesian deep learning methods.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; Which algorithm/formulation of SINDy has been considered? Ensuring that the model is interpretable and physically consistent is also crucially important. The contribution by the present authors goes in this direction. Yet, there are a lot of typos in the paper that could have been avoided using simple spellcheck. Hence, I cannot recommend this work for publication.<|endoftext|>However this is not what the paper proposes. ### WeaknessIn my opinion, despite these positive points, the article has major weaknesses in terms of theoretical and experimental soundness and clarity. 2.Furthermore, I must say that the Laurent polynomial structure of the method proposed does not seem expressive enough to model many important physical systems, which do not have a polynomial structure. Hence, I cannot recommend it for publication at its current state. **Concerning the theoretical results**5.<|endoftext|>?”, “RISE Principals for Physical Laws”, “the load files are need as”We think that this paper is not of sufficient quality to be accepted in ICLR, for at least the reasons mentioned in the Main Review section. The paper would have benefitted from a comprehensive presentation that provide the tangible elements to explain all the building blocks of this work.<|endoftext|>The article is quite descriptive and detailed. UPDATEAfter insight into other reviewers  comments and authors  responses, I tend to agree with concerns on theoretical soundness of certain aspects and need for more appropriate experimental benchmarks. I think that study described in this paper would be beneficial for the conference readers, and especially the complex systems modeling community.<|endoftext|>This paper studies the problem to learn physical equations by neural networks. Using the proposed method, it is expected that less local minima will be explored for better solution. 2.This paper has been well motivated. For example, in experimental section support vector regression (SVR) is used as baseline. The paper is well motivated and the method has been clearly described.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The paper proposes to do that based on training a "transition policy" that starts from a state produced by $a$ and attempts to match the _distribution_ of state action pairs associated with $b$. Experiments are conducted on some simulated arm manipulation and bipedal locomotion tasks with similar results to current baselines on the first group and superior results on the second group. From the position of a non expert of the domain, I found the paper well written and instructive, and was convinced by the idea of matching the transition policy and target policy distributions, but cannot really judge the novelty.<|endoftext|>In order to do so, they leverage inverse RL (leveraging adversarial learning) to train a transition policy $\pi_{ab}$ aiming at transitioning between the two policies $\pi_a, \pi_b$. It is trained by enforcing its state action occupancy to match that of the next pre trained policy $\pi_b$ via inverse RL. Empirical demonstration that their approach succeeds at tackling complex tasks by combining pre trained policies successfully, and outperforming a single policy trained with PPO/SACStrengths  The paper is well written. The approach is well justifiedWeaknesses  Some of the details in the background section slightly break the story and could be removed/moved to appendix, especially all the details about deep q learning   The number of baselines is quite marginal.<|endoftext|>The workcontributes a method to train transition policies, and a method todecide when to start executing the second policy, stopping thetransition policy. The approach is evaluated on a few simulated robotlocomotion/manipulation tasks. It would be good to see a discussion on how such issues might be  addressed or avoided even if the approach in the current form  doesn t do that. I was not sure if the experiments are the best to demonstrate or  explore strength and weaknesses of the solution. Other comments:  Alg. Experiments and results are OK, butcould be more systematic.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper studies certified robustness via randomized smoothing (RS). RS has a fundamental accuracy and robustness tradeoff. The authors aim to enhance such tradeoff through a sample wise control of robustness over the training samples. In particular, the authors investigate the correspondence between robustness and prediction confidence of smoothed classifiers and design a new loss function. Going forward, what’s the computational complexity of the proposed method? There is no result on the ImageNet. The following paper also considers model’s confidence. Please discuss with it.<|endoftext|>This paper focuses on improving the certified robustness via training a better base classifier for randomized smoothing. The empirical analysis of "hard" and "easy" training samples is novel and innovative, which backs up the proposed losses. Therefore, I do not lean towards acceptance. The authors may try to follow the proposed principle to improve the detail design of the current training approach. The authors claim that the method may be more significant on dataset with high complexity. The high certified robustness on the standard ImageNet dataset is one of the unique strengths of randomized smoothing, and evaluation on ImageNet provides a more complete landscape on the method s performance on complex and large scale datasets. Is it true? If the experiment results on ImageNet are provided, based on those results, I will re evaluate this work.<|endoftext|>This paper proposes a loss function for training the base classifier for randomized smoothed classifiers. Experiments on MNIST and CIFAR 10 show that the proposed training method is superior to existing state of the art randomized smoothed classifiers, especially when the radius r is large. How do you obtain the final certified smoothed classifier? 3.Follow the previous question, the cold start problem mentioned in Section 3.1 is also confusing to me. The authors use many terms like “the easiest noise” and “harder ones” in the paragraph above Equation (7), which are vague and hard to parse. I do not understand the colored cross mark in Figure 2 as well. In summary, the empirical results of the paper show advantages of the proposed training objective in producing better certified smoothed classifiers. However, given the current low clarity of the paper, especially when introducing the design of the proposed loss function, I suggest a weak reject for this work.<|endoftext|>This paper proposed new loss functions during the training of classifiers for certified robustness via randomized smoothing. The main idea is to prioritize samples with high confidence because it provides more additional certified radius when its confidence grows. However, the method used by the authors seems a little arbitrary to me. Minor comments:  $K$ is used as the number of classes and the binomial random variable. My current assessment of this paper is slightly below the threshold because I am not very satisfied with the high confidence part.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; Especially, two terms, global uniformity and local separation are proposed to disentangle the contrastive objective and a GLATE algorithm is proposed to adaptively adjust this parameter. # Strengths* The temperature parameter is important for understanding the behavior of contrastive objectives for graph structured data. # Weaknesses* This paper is hard to follow. For example, the authors never clearly define global uniformity and local separation. What if the structure is perturbed as well? How does the presented analysis differ from one recent work (https://arxiv.org/abs/2106.05819)? * One particular concern with the presented formulation is that, unlike visual data, negative samples involved in graph contrastive learning are not aligned with their semantic relationship (Figure 1, https://arxiv.org/abs/2110.02027), partially due to the neighborhood smoothing operation of graph neural networks. Therefore, I in person believe that simply adjusting the temperature parameter is not sufficient to account for the balance of global uniformity and local separation. Overall, I appreciate the deep insights on the temperature parameter. However, many serious problems exist in the current version.<|endoftext|>This paper proposes the importance of dynamically changing the temperature in the contrastive loss with a momentum style that will boost the graph contrastive learning. It also provides theoretical analysis on the reason to dynamically adapt the temperature. BTW, it is also suggested to conduct experiments on the graph classification task on MUTAG etc. Formula (9) is not clear. I know the authors want to express the case for SSL.<|endoftext|>The authors explore the role of the temperature in the loss function for graph contrastive learning. Thus, they develop a simple but effective algorithm GLATE to dynamically adjust the temperature value in the training phase. Strengths: The problem faced by the paper is interesting and timely and the proposed approach seems reasonable. The article is well written, the method is clearly described, and the overall quality is good. The contrastive loss function used by the authors is not universally applicable to various graph tasks. I would like to have an analysis of this, such as the effect of changing the dropping rate on temperature selection. The overall quality of the paper is good, with a clear narrative. The problems addressed are also much needed. So I recommend it for acceptance.<|endoftext|>By studying the feature of loss function’s gradients, it finds that the dynamic temperature change is beneficial to learn uniform node representations. The proposed method GLATE is simple but effective. The presentation of this paper is clear and well organized. The results on the tasks of transductive and inductive learning show GLATE’s advantage over the SOTA graph contrastive learning algorithms. Inspired by Momentum, the authors dynamically adjust the temperature’s value along with the representation’s uniformity degree in the training phase. For instance, in the inductive learning task, the comparison between baselines and GLATE is fair due to using the same choice (GraphSAGE GCN) for the encoder network. I think authors should discuss the effect of this setting and show the comparison of time costs between GLATE and baselines. This paper investigates an interesting and valuable problem, solve it via  simple, provable, and effective algorithm.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; This work focuses on training (via deep RL) task conditioned agents for a given class of workspaces. Tasks are formalized as a variant of finite temporal logic (and thus encode a regular language). A key goal of this work is to perform well on tasks that lie outside of the training distribution. The paper illustrates on two common grid like domains with the results of a number of various architectures and ablations reported. The approach taken is an adaptation of deep reinforcement learning, with the primary contribution being a slight, but seemingly important, modification of an existing architecture to create a separate embedding for (i) a task agnostic state encoding and (ii) create a task specific state encoding. In my opinion, this paper is well laid out and tackles the important problem of creating agents that can perform high level temporal tasks. The focus on simply modifying the inductive biases in the architecture (rather than dramatically changing training) is also a good venue fit.<|endoftext|>This paper lies in the space of using non Markov reward functions to model temporal task, and in addition to that attempts to generalize instruction following beyond single instruction provided in a formal language. For planning with temporal formulas, the authors extend prior work on Task temporal logic. The results demonstrate that the latent goal representation helps in generalizing at both the object level and at the task level. **Major Comments***Sound policy network design choices:* The design choices in the proposed latent goal representations appear to be sound. It appears that joint training of the goal specific information with the task embedding is beneficial to generalization. However, this is hard to judge considering the presentation of the results. *Training and testing on multiple formulas:* I like the fact that the authors have sampled multiple instructions to test on and not on a few handpicked formulas. Further I believe that the standard errors of many of the conditions overlap with each other. Without this, the claims of benefits of one over the other are tenuous. I would suggest the authors to bulletize the baseline architectures, instead of writing a long paragraph. That would make it easier to parse the important features. 2.Separate discussion of the results from the ablation studies, right now you only get to the ablations at the end of that subsection3. I believe that as written the paper is not ready for publication. I would be willing to upgrade my scores (correctness and significance) based on that. Update: The authors incorporated most of the comments, and the statistical analysis bears out the major claims.<|endoftext|>This paper introduces an neural architecture for a DRL agent tasked with learning to solve temporal logic specifications. Results show the proposed architecture outperforms existing ones when it comes to performance on instruction following in unseen environments.The proposed architecture takes in an observation and a task description, which appears to be transformed into a reward function that is then optimised. The key contribution appears to be around the inclusion of some latent state information about the environment in the reward specification phase, with the motivation that this makes it easier to interpret a human temporal logic specification of a task. At present, it lacks the clarity required to best showcase the results, and I cannot confidently say I understand this paper, because it is not explained or presented clearly enough. Specifically:  The abstract doesn t actually explain the task or method, and reads more as a collection of jargon. This needs to be reworked to more clearly specify the problem formulation, goal and proposed solution. I think there is interesting work in here, and a nice story to be told. Unfortunately, the current structure of the paper is not good enough to support this, and the paper is in need of a substantial amount of rework to improve it s clarity.<|endoftext|>The manuscript proposes  latent goal architectures  for following formal instructions in OOD scenarios. Section 3.2: This approach, especially with the definition of the symbolic module, seems to bypass many of the challenging problems in instruction following: sparse rewards (e.g., rewards only available at the end of the episode), with no explicit progress monitoring provided, noisy instructions and environment, no basis for task specific reward shaping, goals are not necessarily conditioned on the agent’s current state (i.e., leading to issues with initial heading, localisation, etc.). We call this type of deep learning model a latent goal architecture." By itself, this is not novel: there are a plethora of works in learning robot skills, and they have dramatically less supervision. Additional experiments and comparisons are needed. Throughout: The manuscript relies on the assumption that "human instructions can be separated from the sensory input". What need to we have, then, for multiple sensor modalities, when the instruction (whether natural or formal language) can completely specify the task? I have some issues with the novelty of the work presented, the accuracy of the claims made, and the somewhat lacking comparison with the related work and key ablation experiments.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; In this paper, DANN is leveraged to generate domain invariant and robust feature representation. The authors claim that the proposed method outperforms other methods when the target domain is the adversarial examples. + The paper is easy to follow and the idea is straightforward. The experiment section is not comprehensive. Only a few methods are included in the comparison. More recent SOTA methods are missing.<|endoftext|>This paper proposes DIAL to learn domain invariant representations for clean and adversarial examples to improve model robustness and clean accuracy. The main idea is to treat the problem as a domain adaptation problem by considering the data shift between adversarial and clean distributions, and then use the generative adversarial network (GAN) principle to tackle this data shift. Though showing promising performance, the idea of learning a feature extractor to minimize the distance between adversarial and clean distributions/domains has been widely studied and adopted before in the domain adaptation (DA) literature. In this paper, the author just simply introduced several DA loss terms and used the GAN framework to learn a more robust model. The experimental results are persuasive, however, the approach is too simple and not novel enough. I think the paper conducts extensive experiments to demonstrate the effectiveness of the proposed method, including some interesting ones, e.g., robustness against unseen perturbations, transfer learning (I like them). However, the novelty of this paper is insufficient, and using the domain adaptation principle and learning invariant representation has been widely studied.<|endoftext|>This paper proposes a domain invariant adversarial training (DIAL) method, which learns the feature representation that is both robust and domain invariant. This paper proposes a simple and effective adversarial learning method DIAL, which brings the idea from domain adaptation for robust representation. This paper is well written and easy to follow. The experimental results are solid and technically sound. From my point of view, the novelty of the methodology is not enough, as the domain classifier and the gradient reversal layer are the same with those methods in domain adaptation such as [1]. Unsupervised domain adaptation by backpropagation. Overall, this paper proposes a simple and effective adversarial learning method DIAL for robust representation learning. However, the novelty of the paper is not significant as similar methodology exists in domain adaptation.<|endoftext|>The paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. The proposed method is inspired by domain adaptation approaches that require a model to extract domain invariant/agnostic features from two domains. In the context of this paper, the two domains are the clean and adversarially perturbed images, and the network is required to extract domain invariant representation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Paper studies the deferentially private fine tuning of large language models and shows that privately finetuning language models can provide good utility. Although one can say that the paper is mostly empirical and focused on showing that DP finetuning works when the curse of dimensionality for DPSGD is dealt with in one way or the other, I think it adds good value and a proof to the literature. I do not have any major concerns with the paper. I like the paper and think that it will add good value to the literature and the conference.<|endoftext|>These methods are also much more memory efficient and faster than DPSGD as well. The framework and the particular methods were explained clearly and the experiments were well executed, proving the claims in the paper. Given there are huge amounts of public texts and pretrained models in the NLP community, fine tuning with privacy as proposed in this paper could be standard for learning on sensitive data. Technique novelty is a bit limited given that all the methods (Adapters, Compactors and LoRA) in the proposed framework are from prior non private fine tuning works. 4.The comparison for RGP is also a bit unfair because it was originally proposed for training with privacy from scratch. The experiment section could be strengthened with better comparison to baseline methods.<|endoftext|>This paper demonstrates the feasibility of fine tuning large language models (pretrained on public dataset) on private datasets for various downstream tasks. It proposes a meta framework in which most of the pretrained weights are held constant and only a small number of additional parameters are updated during fine tuning. While this is solid engineering work and demonstrates the feasibility of integrating DP into NLP, I am not sure if it has enough novelty to be published as a separate research paper. + The experiments are well designed and executed.<|endoftext|>In this paper, the authors propose a meta framework that applies DP SGD to NLP tasks. The technical novelty is limited. The rebuttal solved most of my concerns. Thanks for the author s efforts! The proposed meta framework can handle various fine tuning algorithms. As a DP learning algorithm, DP SGD provides a practical method to protect the privacy of the training samples of the deep learning model. 3.The authors conduct experiments on multiple NLP tasks, including NLU and NLP tasks. "Full" acts as a baseline method. How to avoid it?
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; ************************************************************************************Update: Thanks to the author for the response. I think now the paper is clear, and I have raised my score. ************************************************************************************This paper is mostly well written. In my opinion, the training invariance is very relevant in theoretically understanding the deep neural networks and the results presented in this paper are novel and interesting. However, I have questions about some statements in the paper. The author stated, after Theorem 1, "The first point of Theorem 1 admits an extremely simple proof for the linear fully connected network case in Arora et al.(2018) (Theorem 1) but has not been generalized to other architectures." Strength: Well written; Novel and significant resultsWeakness: Some statements might be wrong; Some proof needs clarification<|endoftext|>To show this invariance a decomposition of the DNN as the composition of a multilinear function and a network with {+1, 1} weights. This paper gives a nice overview of the techniques used and improves on them in comparison to previous work. The sketch of proofs are quite detailed. This a quite dense paper, with many definitions (for all architectures studied) but I found it clear and well written. Some of the assumption are quite strong, such as the sign remaining fixed and the alignment to parameters with no zero weight, but at least the authors take the time to discuss these assumptions.<|endoftext|>This paper studies the low rank phenomena in the context of *non*linear neural networks. Their assumptions are relatively mild (except maybe Assumption 1 that I discuss in my main review)They then leverage that invariance results in order to show a low rank phenomenon (thm 2) for a wide range of neural networks with positively 1 homogeneous activation functions. Overall the paper is well written in my opinion. ## WeaknessesI think this paper would benefit from a more detailed discussion of the interpretation of Theorem 2. “In fact, the following assumption, shown in the literature, is sufficient” What do you mean by shown in the literature? Also $u$ is not quantified for equation 5. The theoretical result shown is very relevant to the community and is proved under relatively weak assumptions.<|endoftext|>(3) The paper is well written and easy to follow. Weakness: (1) My main concern is that Assumption 1 and Assumption 2 are restrictive and even a bit unrealistic for some architectures. (3) Along with the last concern, another concern is that Assumption 1 gives an impression that the key difficulties are assumed away. A very interesting paper that has certain technical contributions. The assumptions may be a bit unrealistic, but the technical results can be applied in the future. I recommend acceptance and am willing to raise scores given that my concerns are adequately addressed.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This work proposed Stabilized Likelihood based Imitation Learning (SLIL) which iteratively estimates the expert state distribution by using Denoising Continuous Normalizing Flow (DCNF) and maximizes the policy learning objective in Eq.(3) that matches expert policy and state distribution. The strength of using DCNF instead of using Continuous Normalizing Flow (CNF) is well desribed and supported by the experiments evaluating the test log likelihood in Figure 9. Regarding the strength of this work, the contribution of this work over given baselines is clear, and I m satisfied with the detailed explanations and experiments, e.g., comparison with LIL, the justification of using DCNF rather than CNF, how DCNF is trained algorithmically, quantitative tests using EMD and test log likelihoods, etc. Primal Wasserstein Imitation Learning (PWIL) shows their algorithm works efficiently even when we have a small number of expert trajectories (1 or 11 trajectories), whereas this work presented empirical results assuming more than 4 expert trajectories (more than 80 expert trajectories for Humanoid). In "Imitation with Neural Density Models", energy based model is used to estimate expert support estimation, which are based on maximmum likelihood estimation similar to this work.<|endoftext|>Their objective SLIL combines (or interpolates between) generative modeling and behavioral cloning in order to avoid mode collapse of the agent policy (common issue of adversarial approaches to IL like GAIL) and to mitigate distributional shift which BC typically suffers from. A strong imitation learning algorithm consisting in estimating the expert s state distribution, and learning a policy that recovers the expert policy and that has an occupancy distribution close to the distribution induced by generative model learned via the DCNF. The ablations illustrate the robustness of the method to hyperparameter variations. $\textbf{Weaknesses}$  In the description of the proposed approach to generative modeling, coined denoising continuous normalizing flows, there are a few points I do not understand, and where their may be some incorrect statements:    It is claimed $P_{E,\sigma}(\tilde{s}|s)   P_E(s)\mathcal{N}(\tilde{s}| s, \sigma^2I)$. I believe the paper proposes an interesting approach to imitation learning, however the technical issues described in the weaknesses section makes me currently tend towards a weak reject.<|endoftext|>The paper present a novel method for imitation learning aiming to stabilize training and mode coverage compared to existing methods. The proposed method (SLIL) extends behavioral cloning by an additional objective of maximizing a state only reward that is defined by a density estimate (trained once beforehand) of the expert s state distribution. For density estimation SLIL uses a continuous normalizing flow where the expert states are corrupted by Gaussian noise, where the noise level is decreased during training. The main contributions are:1. * Novelty (1): SLIL is a hybrid between BC and Energy Based Imitation Learning (EBIL) [2], a reference certainly missing. * Much of 4.2 seems to be related work.<|endoftext|>This paper proposes a straightforward non adversarially trained imitation learning method. Performs experiments on 10 mujoco environments that illustrate that the proposed approach achieves better mode coverage than some comparable methods, that the denoising is empirically effective for generalization, that the algorithm is more robust than GAIL to variation in learning rate, among other hyperparmaeter variations. I don t see how this is true, because $\enclose{circle}{2}$ in Eq (3) is $\sum_{s \in \mathcal D_\pi} \log P_E(s)$, i.e.the state distribution is the state distribution of the policy. Why can t this dataset balance be addressed by simply normalizing the losses by the size of the datasets? Can the authors comment? Missing related work: like the submitted paper, [A] also performs likelihood based imitation learning with a normalizing flow for expert distribution matching, is not adversarially trained, and uses perturbation of the expert trajectories with gaussian noise. Discussion of the differences is important to contextualize the current paper s contributions, e.g.[A] used the distribution for planning rather than to learn a final policy  The description of the experiment corresponding to Figs 11, 12, and 13 is lacking.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper uses many techniques to achieve this including, depthwise kernels, attention over kernels, and also neural architecture search (NAS). Strengths  Usage of a depthwise kernel reduces the model size of KPConv and the computation cost involved. The paper can effectively search for a MAKPConv block that is better than a hand crafted one using NAS. The objective of building a lightweight point cloud processing model would have been better served by starting with some already lightweight architecture like [1] (explained later). Hence, the end product of the work is not very useful in the reviewer s opinion. The architecture of the proposed model MAKPConv is very complicated. This could be very useful in many applications. Some important details are also missing (see weakness).<|endoftext|>But I might change my score if the authors can address my concern or other reviewers can give more positive reviews. The improved KPConv operation. This paper is well written, in general. 2.In NK Attention, the NK Distance indicates the distance between the neighbour and the kernel point or between every two neighbours? For example, it seems that the NK Attention is similar to the h(.) 2.The OA of KPConv is 92.9\%, which is higher than the OA of the proposed method (92.6\%), while in this paper the authors do not report that result and instead report the result of their implementation. [1] https://arxiv.org/pdf/1904.08889.pdfI have two concerns,1. 2.The experiments are not enough. As a result, currently, I vote for borderline reject.<|endoftext|>This paper addresses point based methods for classification and semantic segmentation in 3D applications. Experimental study is conducted on benchmark datasets and tasks to demonstrate that the resulted deep networks can achieve higher computational efficiency and improved performance. Ablation study is also conducted to illustrate the key components in this work. 5.The ablation study on the kernel contribution via the NK Attention is helpful for illustrating the function of the proposed attention mechanism. Although the ideas of using a depthwise kernel and the attention mechanism work well in this paper, they are well known techniques in the literature. Firstly, the key differences and novelties with respect to the existing related work can be further clarified. This work is well motivated. The proposed idea on reducing computational cost is neat, and it works effectively as experimentally demonstrated.<|endoftext|>This paper aims to accelerate the inference of 3D point cloud neural networks. They then propose to reweight the weights of kernel points with attention to boost its representation power. The problem studied in this paper is very important. My primary concern of this paper is its limited technical novelty. The only thing new to me is the unified dense and sparse neural architecture representations, which, however, is more of an engineering improvement than a technical contribution. The authors have only reported the performance on the validation set of SemanticKITTI. They should also include some numbers on the final testing set.
Accept (Poster); rating score: 8; rating score: 6; rating score: 3; rating score: 10; The authors investigate the ability of adversarial training to memorize random datasets. They also discover that robust overfitting can occur in adversarial training with memorization and use temporal ensembling to mitigate it. Figure 1 is also very clear. The fact that this happens over several datasets, architectures, and threat models does indeed imply that the adversarial training algorithm has a significant impact on the convergence and not the capacity of the network. This is a strength. There are really few weaknesses with the paper.<|endoftext|>This paper presents the memorization effect/behavior in adversarial training in perspective of model capacity, convergence and generalization. They present the problem of memorization in AT by robust overfitting and propose the regularization term by temporal ensembling approach with their hypothesis, also show the effectiveness of the proposed method in systematic experiments. Lastly, they investigate the cause of robust overfitting in AT and propose a mitigation algorithm by a regularization term for avoiding the excessive memorization of adversarial examples. This paper also proposes a new hypothesis about the cause of robust overfitting, which can be intuitively understood and supported by comprehensive experiments. 2) Robust overfitting : Although it has been shown empirically that overfitting is improved through the Temporal Ensembling (TE) approach, there is a lack of novelty for the method itself. 4) In section 4.2, authors claimed that most of the approaches for noisy labels are not suitable for AT, since excluding the noisy label leads to the reduction of the training data.<|endoftext|>This paper provides comprehensive studies on several facts about the memorization of adversarial training algorithms (AT). (2) Memorization of one hot labels in AT methods can be one important factor to the robust overfitting issues. Based on these findings, the authors propose a method Temporal Ensembling (TE) approach to avoid fitting all adversarial examples with one hot labels. Moreover, the proposed method, the Temporal Ensembling (TE) approach, does not have significant novelty compared to previous methods. What is the reason for the gap between theories and the practices? 3.Moreover, in Section 4.1, the authors deploy experiments to demonstrate that there are “hard” training samples which are consistently hard to be classified. However, why the existence of hard training examples can support the claim they can result in overfitting issues? Although the Equ. Overall, the findings of this paper lack some clarifications about their significance.<|endoftext|>This paper presents a thorough investigation of the dynamics of adversarial training (AT) when a network is trained on a dataset with random labels. Finally, the authors use their novel findings to explore the reasons behind robust overfitting, and propose a new method to prevent it. This paper presents a very rigorous empirical investigation of the dynamics of memorization in adversarial training. 3.**Connection between robust overfitting and label noise**: The suggested cause for robust overfitting, i.e., introduction of label noise by the adversarial attacks, is bold, and the proposed method to overcome it seems quite effective. However, I still believe some of their arguments are a bit speculative and could be better supported by the data. I truly believe that answering any of these questions would be of great significance to the community, and therefore would make this work much more relevant.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposes a unified framework for adversarial training from the perspective of Wasserstein distributional robustness. It is proved that some standard previous adversarial training methods are special cases of the proposed framework. The paper also proposes an adversarial training method given the framework. The proposed method, while unifying previous methods, performs better than previous methods (PGD, TRADES and MART). The proofs are not carefully checked. Overall, I think the paper has a decent empirical contribution, and it also has the potential to offer a unified theoretical explanation for adversarial training.<|endoftext|>This paper applies the idea of distributional robustness for crafting adversarial examples. The methodology in the paper extends that of (Sinha et.al.2017) by introducing learnable parameters. Extensive experiments are conducted to demonstrate that the proposed method UDR when applied in conjunction with existing perturbation methods, such as PGD and TRADES, produces better results on standard datasets. This makes the attack algorithm not that realistic.<|endoftext|>To this end, this paper presents a unified framework to connect Wasserstein distributional robustness with PGD AT, TRADES, and MART methods. It is not very clear how this work differs from WRM (Sinha et al., 2017) with respect to the main conclusions of equivalence and proof techniques. This work improves WRM (Sinha et al., 2017) to cover more adversarial training methods, e.g., TRADES and MART.<|endoftext|>The paper proposes a new approach for improving adversarial training (AT) in deep neural networks (DNN). While previous approaches generated point wise adversary examples to enrich the training data set, this approach extends previous work on Wasserstein distributional robustness (WDR) and suggests a unified WDR framework to encompass previous approaches and to improve AT. ### Theoretical contributionThe paper extends the previously established connection between AT, distributional robustness and optimal transport (Blanchet & Murthy [2016 on arxiv, 2019 published]; Sinha et al.2017).However, my impression is that the main theoretical contribution for employing the work by Blanchet et al.for connecting distributional robustness with AT was made by Sinha et al.2017: They proved a relaxed version of the dual form of the risk function using a Lagrangian penalty formulation and show the way to place PGD AT in this WDR framework. The paper makes a theoretical contribution by extending the work on WDR for AT to other AT approaches. Some references are not correct, e.g., Sinha et al.2017 is not a preprint, it was actually accepted as an oral contribution to ICML 2018.<|endoftext|>In particular, three well known defenses, namely PGD AT, TRADES and MART have been the center of attention. The relation between point wise adversaries which are commonly used in AT like methods and W DRO is already known to the community. Moreover, paper is very well written. I am not completely familiar with the experimental side of this line of research, so I wait to see other reviewers  comments on that matter.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; This paper presents a method for enforcing strict orthogonality for convolutional layers. It is shown in experiments that this method is significantly faster than the recent skew orthogonal convolution (at ICML 21) method. However, the construction of orthogonal convolution in the paper requires one to first expand the kernel to the size of the feature map that it is applied to, by padding zeros. Can the authors comment on why ECO produces significantly better performance than SOC, in Table 2? The experiments only contain results with LipConvnet which are perhaps non standard and have far worse performance than standard networks, such as ResNet. This makes it less convincing as to how important the method is for practice. How is "orthogonal convolution kernel" different from "orthogonal convolution"? I also have some minor concerns on the technical approach, that hopefully can be clarified during rebuttal.<|endoftext|>This paper studies how to construct orthogonal convolutional networks in an efficient way. To this end, this paper builds the connection between the DFT transformed kernel with the common dilated convolution. During training, the forward pass can be done by a sequence of inverse DFT and dilated convolution. Pro:The authors propose a theoretically motivated way of constructing orthogonal convolutions that achieves good robust performance on several benchmarks. It has a big advantage on the testing speed over the previous state of the art, SOC. I think this is an important step in this research direction. I think this paper contributes a new idea in this field.<|endoftext|>Summarizing the paper     The paper proposes a method to construct a  convolutional layer with the orthogonal Jacobian matrix. Such a layer is 1 Lipschits: this property is an important one for building robust neural networks. The authors conduct experiments on classification tasks using CIFAR10/CIFAR100 datasets. Benefits       They show the link between singular vectors of the Jacobian of the convolutional layer and the convolutional kernel structure. The evaluation time of the proposed convolutional layer coincides with the evaluation time of the standard layer. They obtain analytical formulae that yield a nice three step approach to construct orthogonal convolution.<|endoftext|>The paper proposes an economical method to construct orthogonal convolutions using dilated convolutions. Overall, I think the idea in the paper is novel and elegant. 1) The limitations of the proposed method. a) In previous approaches (BCOP, Cayley, SOC), the convolution kernel is orthogonal for ANY resolution. The matrix P is the Fourier transform of the convolution kernel, which generally is complex valued. a) The filters must be even, i.e., the method cannot construct orthogonal convolutions that are asymmetric. b) The space of real orthogonal matrices is disconnected, and the matrix exponential can only cover one component SO(c). The paper uses truncated Taylor expansion instead of other more computation/memory efficient methods   The SOC paper adopts truncated Taylor series because there is no efficient algorithm when A is a convolution instead of a matrix. However, using the current method, the authors will have to analyze how truncation error affects the exact orthogonality of the convolution kernel. 4) Other minor issues. This is not correct.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper identifies and studies a new phenomena in the generalization of deep learning. First, it presents the observation of Generalization Disagreement Equality (GDE) between same models trained with different stochasticity, strengthening the similar observation of Nakkiran and Bansal (2020). Second, it shows theoretically that calibration of ensembles (in both the per class sense and a suitable aggregated sense) implies GDE, with accompanying experiments on their relationship. Strengths:I think the GDE phenomenon presented in this paper is quite novel, and in my opinion, this message could be quite interesting to the DL theory / phenomena community. I also think the experiments in this paper are thorough and quite well executed. The authors discussed this on Page 9 and I suggest they also make sure various earlier claims reflect this limitation, when appropriate. This paper presents GDE, an empirical phenomena about the generalization of deep learning, with novel theoretical justifications via the connection to calibration, and thorough experiments.<|endoftext|>The authors build on a striking empirical observation of Nakkiran & Bansal—namely, that if two neural networks with the same architecture are trained on independently drawn training sets, achieving generalization error $\epsilon$, then their rate of disagreement on a test set is typically nearly equal to the test error of the two networks. In this paper, it is shown that this observation also holds even if the two networks are trained on the same data, but with different random seeds (data ordering and/or initialization). It is, as the authors note, a tantalizing connection between generalization and calibration. And the authors  explanation of this observation by reduction to a calibration property is clever and convincing (though as they note it only explains it in expectation). The paper is well written. I was glad to see an attempt at an analysis of the variance of disagreement in the form of Corollary A.1.1, weak though it is. Ideally there would be more thorough experiments and more exploration of whether the experimental result is useful, but the paper is substantial enough that it is okay if such additions are left to future work.<|endoftext|>This paper builds upon and extends from Nakkiran/Bansal (2020). First, it shows empirically that models learned from two independent runs of SGD on the same training set have their prediction disagreement highly correlated (and nearly equal) to the test error of the models. It then attempts a theoretical explanation for this phenomenon. The paper also proves that the gap between the test error and the prediction disagreement is upper bounded by a calibration error. **Strength** The paper marks a significant advance in the understanding of the generalization of deep neural networks trained with SGD. The theoretical analysis is sound and the experimental results are convincing. Although the proofs are not difficult to follow, the reader is likely to be buried in their technical details without truly appreciating the fundamental reasons for which the theorems are true. The paper is also well written.<|endoftext|>This paper shows empirically that the test error of a network is approximately equal to the disagreement rate between two separate training runs of the network, measured on unlabeled test data. The authors show theoretically that this phenomenon is due to ensembles trained with SGD being well calibrated. The theoretical results are sound. This paper shows stronger results than prior work (Nakkiran & Bansal 2020) with more practical implications  being able to approximate the test error without a separately labeled dataset, given calibration. Paper is well written and easy to follow.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposed a conservative Q learning approach based on density and uncertainty estimation, D CQL. The performance of the proposed method is compared with other batch RL methods in several classical benchmarks. D CQL tries to regularize the Q value in out of distribution actions instead of simply penalizing an arbitrary distribution that is different from behavior policy. The theory in this paper is meaningless. The paper use uncertainty in the motivation and description of the algorithm choice. The specific form of uncertainty \zeta as a function of the density is not explained/justified at all.<|endoftext|>The paper shows that proposed D CQL, which uses $ \zeta   \max ( \nu , 1  \frac { \eta _ \beta (s,a) } { \eta _ \beta ( s , a _ { \mathcal { D } } ) } )$ with normalizing flow density estimator $\eta_\beta$, shows better performance than CQL in number of domains. Why can t we simply estimate $\hat{\pi}_\beta$ and use $\max (\nu, 1 \hat{\pi}_\beta$)? Overall, I believe that the contributions of the paper are only marginally significant. The results have shown that the performance has increased, but the introduced hyperparameter $\nu$ does not have a proper way of optimizing it, which is different from $\alpha$ of CQL that can be tuned to prevent the divergence of Q values. We usually expect to have a performance improvement from having another freedom on hyperparameter, and I cannot credit the performance improvement of this paper much.<|endoftext|>The authors introduce a weighting scheme into the CQL algorithm, which assigns small weights to those state action pairs close to the dataset and large weight to the OOD actions. The effect is the gap between the underestimated values of those in distribution state action pairs and their true values are closer (i.e., the lower bound is tighter). The calculation of the weighting scheme requires a density estimation, and the authors propose to use normalizing flow to learn the density function. CQL concerns an important problem in offline RL — distribution shift, and the paper focuses on improving CQL, so it is an important topic. In this case, I have to ask for stronger empirical results: baselines with other design choices and more domains. The contribution is incremental, and I doubt the significance. Note that the additional weighting scheme can be essentially thought of as a new type of \mu. I believe the proof of the theorem is a simple modification from the existing CQL work.<|endoftext|>Specifically, the proposed D CQL considers the uncertainty on OOD actions, and only supresses the Q values of actions which belong to OOD data with large probability. I think that an ablation study should be presented to verify that the underlying estimator really works in complicated environments. the empirical results in Fig.2 do not show a significant performance improvement than CQL, in fact, only in halfcheetah medium v0, the proposed D CQL can achieve a better performance than CQL in the final;  About the experiments, this paper only compares with the original algorithm, CQL, however, it s necessary to include more baselines(e.g., BAIL, BCQ, etc) to verify the effectiveness of D CQL;Some questions:1.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; Label smoothing is a popular approach to regularize modern neural networks. However, the amount of smoothing is the same across all samples in the dataset, which can be sub optimal. In this paper, the authors of the paper proposed a novel adaptive label smoothing method so that each sample gets a different amount of smoothing. Weakness   The main weakness of the paper in my opinion is the lack of novelty. The proposed method in my opinion is very similar to the previous works like [1], which proposed to use predictions from previous time stamps for self distillation, and [2], which also proposed a method for adaptive label smoothing based on predictions from previous time stamps. On top of [1] and [2], [3] and [4] are also potentially relevant prio works to potentially discuss in literature review and benchmark against. Self knowledge distillation with progressive refinement of targets, 2021. "Self distillation as instance specific label smoothing." After reading the response and other reviewers  comments, I am raising my score to 6.<|endoftext|>This paper proposes a new method that is based on label smoothing. This paper tries to extend this to a dynamic nature with different smoothness between samples and also throughout training. The main idea is to use a normalized version of the entropic level of model prediction for a certain input data point for the smoothness parameter. This is combined with self knowledge which is used as the distribution of label smoothing. Ablation study and gradient analysis provide further insights into the proposed method. Zhang and Sabuncu "Self distillation as instance specific label smoothing" (NeurIPS2020). Although already discussed in the final section, it would be interesting to see results for datasets from other domains, such as images. This paper provides a simple way to extend label smoothing to instance specific label smoothing with an adaptive smoothing parameter. I do not have further comments/questions.<|endoftext|>To deal with this issue, this paper proposes a label soothing scheme that brings dynamic nature into the smoothing parameter and the prior label distribution from the distilled knowledge. This paper proposes the adaptive \alpha computed by the entropic level of model probability distribution per sample, which leads to updating the model parameters to lower the predictive score on the ground truth target, as opposed to the effect of the cross entropy with hard targets. The hyperparameter search on \alpha in label smoothing is removed. This paper demonstrates why self knowledge distillation as a prior distribution is a form of regularization with theoretical analysis on the gradients. 2.The authors claim that “There are a number of benefits of adopting the adaptive smoothing parameter”. However, they only show that the hyperparameter search on \alpha is removed and the adaptive smoothing parameter can be connected to the gradient rescaling effect on self distillation. However, \alpha could also be changed in the training process. Aspects of the contributions exist in prior work.<|endoftext|>The paper proposes a simple yet effective way of smoothing the labels for each data point. The distribution that is used for label smoothing originates from the same but at earlier epoch of the training, i.e., a teacher student learning framework like in knowledge distillation is employed, where the teacher is a model learned in an earlier epoch. From among these earlier model candidates, the one is chosen that obtains the best evaluation metric g (Eq.5).Function g does not have to be the training loss, but can be the (possibly different) evaluation metric. The validity of this approach is theoretically supported by a gradient analysis, and experimentally corroborated by improved evaluation metrics, improved calibration, and an ablation study. And it works well in the experiments. + Instead of a simple distribution that is often used for smoothing, the paper proposes to use the distribution of the same model but from an earlier training epoch. For this reason, it could also be interesting to have experiments where the evaluation metric and training loss are the same function, e.g., log likelihood. KDD 2006.A theoretically simple yet empirically effective approach to label smoothing. +++++++++++++++++Update based on authors  response:Thanks for the clarifications. I will maintain my current score.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper extends the existing line of research of the dynamics of multiplicative weights update and similar algorithms for games. It shows that for zero sum two player games and for population games satisfying certain conditions, that the entropy increases linearly as long as the strategies are far from the distribution; this implies that the strategies concentrate near the boundary in the long run, in a sense that the paper formalizes. Strengths   Enables looking at differential entropy, and not just the volume, of a distribution of strategies for a game. Strengthens the approach of previous work by splitting the space into a boundary region and interior region and analyzing the change in probability mass between the two. Contributes a possibly novel analysis of the determinant of a Jacobian of a population game   Results are claimed to be robust to small perturbations in the game. Weaknesses   Is differential entropy (DE) the most appropriate way to index the deviation of the strategy vector from equilibrium? This paper could provide arguments for why DE makes more sense than other statistics that could be considered, e.g.determinant of covariance matrix, trace of covariance, Nash gap (max difference between a player s payoff and the payoff for their best response holding other players fixed.) Is the use of DE key to the results on non zero sum games, or could similar results have been obtained through volume analysis? This paper could make it clear which novel results are exclusive to the choice of using DE. The significance of the results, in spite of a brief reference to the "grand escape" (which is not further elaborated outside of the introduction), is not made clear to the reader. This paper could describe in greater detail what happens to make the entropy increase slow down. 2 "more probably to occur"  > "more probable to occur"   pg. 4 after eq (1) "depending on the value of qt"  > "depending on the value of pt"This paper adds some new results extending previous work that established divergence from Nash equilibrium for a class of learning algorithms for games. I was not convinced of the main argument of the paper, which claims that the advantage of using differential entropy to analyze the learning dynamics. What seemed more novel was the analysis of the Jacobian of the population game. Hence, overall I am recommending weak acceptance. If I were to be convinced that the use of differential entropy is a promising approach for the field, then I would raise my score.<|endoftext|>The paper studies the evolution of uncertainty in multi agent game dynamics. More specifically, it studies how the probability distribution over the players  cumulative payoffs evolves as players use typical online learning algorithms to play the game. The game uncertainty is quantified by the notion of Differential Entropy (DE) of such distribution, a quantity related to the Jacobian of the game dynamics. Authors show that DE increases linearly with time for a set of games including two player zero sum, coordination games, and population games, confirming the negative convergence results obtained in past works. Learning in games is a relevant and active area of research, and I believe the contributions of this paper are original and novel with respect to previous works. As mentioned by the authors, the obtained results are orthogonal w.r.t.past works that study non convergence in games, as the proposed approach quantifies the increase of unpredictability via the notion of differential entropy. In my opinion, however, the paper has also some weaknesses:The paper lacks experimental results which perhaps could better illustrate the studied phenomena (e.g., the obtained DE rates). To summarize, although the paper could improve from additional experiments and the technical contributions are limited, I have appreciated the motivations and model proposed by the authors, as well as the obtained theoretical results. Hence, my accept score.<|endoftext|>The paper studies how the uncertainty of the initial cumulative payoff vectors evolves in the process of learning in games. Looking to the differential entropy, the authors show that for a broad range of learning in game systems, including two player matrix games and one population games, the differential entropy of the distribution over the cumulative payoff vectors increases linearly with time in two learning algorithms (Multiplicative Weights Update, and Optimistic Multiplicative Weights Update). Different from the standard perspective, the paper investigates the behavior of a probability distribution over a set of initial conditions. The technical/theoretical aspects of this paper seem to be rigorous. 1.Second line below eq (1), what is q^t here? should be p^t? 2.In eq (1), the agent updates the cumulative payoff vector using the exact payoff at the current time step. I’m curious whether the main results still hold if the agent uses the empirical payoffs till to current time step? When the probability distribution over the initial conditions is discrete, which is often seen in practice, can the authors still achieve the same characterization for Shannon entropy? I’m not super familiar with the related literature of this work. The characterization on the evolution of cumulative payoff vectors seems pretty novel to me. I feel the results obtained in this work may have implications on other studies on the behavior dynamics of online learning in games, especially for those who need random initial conditions.<|endoftext|>This paper proposes a differential entropy framework to quantify the evolutionary stability of learning in games with different initial conditions. The paper finds that differential entropy of these learning in game systems increases linearly with time for Multiplicative Weights Update (MWU) or Follow  the Regularized Leader (FTRL) algorithms in zero sum games. Questions:* Fig.1 is extremely unclear, and it would be better to explain the meaning of each sub fig in the caption or main text. * I think this paper is not easy to follow, and many clarifications are needed. For example, a general problem setting before the model section would be useful. Would the same conclusions still hold when using other learning algorithms in games like gradient ascent, joint action learner? Investigating the uncertainty/stability during the learning in games with different initial conditions is an interesting problem. But from my understanding, this problem has not been well formalized in the paper, and many additional clarifications are required to make the people unfamiliar with this research problem get the paper s key points.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper proposes to add Gumbel noise to the scores to which differentiable top k via regularized optimal transport is applied. This improves theoretical guarantees for the case where the $k$th and the $k+1$th elements are equal. The paper proposes a new application for differentiable top k as well as a variation of a differentiable top k operator that improves theoretical guarantees, which they also prove. The experimental evaluation is or seems to be flawed, as it is questionable whether the comparison is fair. (which does not impact my score, as I expect this to be fixed in the final paper.) Improving and extending the comparison to related work (e.g., Mena et al.) and making the comparison explicit would improve the paper. I suggest adding and discussing these references and elaborating the choice of using the formulation of Xie et al.(At the moment, it seems like the paper implies that the choice was because this would be "the" or the only soft top k operator, which is not the case.) In the toy example in Figure 1, there is not a unique solution of the top k operation. Therefore: why is it beneficial that one specific solution is selected over a 50% / 50% mixture of both 0.6 entries? The algorithm 2 would result in a 50% / 50% mixture for the two 0.6 entries. The empirical evaluation should be more rigorous. What are the differences to the Gumbel Sinkhorn method by Mena et al.? From "However, these existing methods do not study the application in combinatorial optimization learning, and the gap between the MAP inference and the result after Gumbel re parameterization is not well characterized in (Mena et al., 2018). I appreciate the application and experiments. From the current draft, it seems like an "empirical selection" was done for your proposed method, but there is not a clear procedure to have a meaningful comparison between all methods. gradient ascend  > gradient ascentp.7 problems pf cardinality  > problems of cardinalityp.9 Directly applying Gumbel sampling to SOFT TopK seems either an effective improvement, and satisfyingresults are achieved when exploiting the Gumbel re parameterization trick and enabling gradient based optimization over the combinatorial objective score.<|endoftext|>By introducing Gumble noise into the existing Soft TopK (Xie et al., 2020) formulation, the authors claimed that the bound of the difference between approximate objective function value and the ground truth value is tightened, compared to the results in Xie et al., 2020. The authors presented Gumbel Sinkhorn TopK (GS TopK) and its application in TopK combinatorial optimization problems. Experimental results for max covering and k means clustering were presented to demonstrate the claim. 3.Both simple illustration (Figure 1) and empirical evaluations (Tables 1 and 2) for max covering and k means clustering have validated the claim that the proposed GK TopK improves Soft TopK and outperforms other greedy and combinatorial optimization solutions, with respect to the achieved approximate objective function values. This is related to the computation of objective function values in Algorithms 2 and 3 (in appendix), which basically took the best derived objective function values among the sampled transport maps for the GK TopK formulation. To access how this changes the solution quality, should the authors also provide the actual objective values with the derived combinatorial solutions? For now, it is difficult to tell whether the proposed method will have meaningful improvement in derived solution in practice. 2.It is not clear to me how significant is this proposed improvement over Soft TopK. It may address the diverging gap issue when the sorted probabilities at the boundary are the same ($x_K   x_{K+1}$). Why adding Gumble noise to the original optimal transport formulation of Soft TopK can always improve the bounds? In Figure 1, I assume that the actual optimal objective value is 2.4 while Soft TopK has the gap at 2.0. What is the actual objective value for the derived solution by Soft TopK? But $\tau$ appears in the derived bounds for both formulations. 4.Mathematical notations and the proofs have numerous issues. Here are a few examples: 1) In equation (6), $x_k$ and $x_{k+1}$ have not been defined yet. Do the authors mean by "Lemma 3" instead? Even that is the case, there appear to lack some steps to connect Lemma 3 with equation (30). There must be typos for these two same equations. For example, it is not clear what sample size #G was used in the experiments. It would be also nice to discuss more why in Figure 3, the approximate formulations always have lower bounds. The lemmas, theorems or their proofs do not seems to indicate that will always happen? There are also numerous typos, for example, "... important **hcardinality** constrained ..." in the last line of the paper on page 9. The authors presented Gumbel Sinkhorn TopK (GS TopK) to improve Soft TopK. But there are concerns on the meaningful improvement for the actual derived approximate optimal solutions.<|endoftext|>The paper proposes an unsupervised method for cardinality constrained combinatorial problems. This provides a more complete theoretical bound on the original OT problem but also strong experimental performance on two combinatorial optimization problems. ## Strengths:  Strong experimental results with baselines that include other neural approaches as well as solvers and classical algorithms. The paper builds on recent work from the literature. The approach provides improved theoretical results for soft TopK through the use Gumbel noise. The motivation and the explanation of the theoretical results are clear and well done. The work in Paulus et al.can use differentkinds of noise in their optimization program, not just Gumbel. While the paper indeed does not explicitly address cardinality constraints in the examples/experiments, it is fairly straightforward to describe such constraints with their probabilistic penalty approach. Is an asterisk missing there or am I misunderstanding something? The expectation on the lemma right below has T with both tilde and an asterisk but the sentence claims that the optimal is just tilde(T) (well, boldface T to be precise). Regarding the motivation of the paper, the authors explain ties between $x_k$ and $x_{k+1}$ will break the original bound, which is a concernbecause those are outputs of a neural network so it is a plausible scenario. Is this really an issue in practice? That is, does the Gumbel noiseend up breaking ties that appear in neural network outputs in your experiments? I would expect sampling to improve soft TopK as well, but it doesn t appear to be the case. The scope of the approach is somewhat limited as it only addresses cardinality constraints. It also seems to rely on the differentiability of the objective function. Furthermore, conceptually this is a somewhat incremental improvement over the existing soft TopK algorithm. Therefore I am concerned that the impact of this work might be limited. How much does the number of samples affect performance? This is overall a solid paper that is theoretically motivated with strong experimental results. However, the scope of the project is somewhat limited and the proposed method is incremental in nature. Furthermore, the writing could be improved and I have some doubts/questions regarding certain experimental details.<|endoftext|>This paper advances one shot solution generation methods for cardinality constrained optimization problems. This paper introduces a Gumbel randomization trick to the algorithm, prove that this removes the discontinuity in the bound, and implement their algorithm in two combinatorial problems, where they demonstrating achieving better solutions in less time than baselines. It introduces a simple trick, provides nice theory to justify the technique, and contains several experiments. I have a few comments. Specifically in Section 3.1, $\mathbf{s}$ is not introduced, making it confusing to jump into the problem. I am confused that the proposed methods beat Gurobi/SCIP. I assume this is because of the runtime limit on the solver (100 sec). Outperforming the solvers would make sense if the current methods can find *provably optimal* solutions faster than the solvers, or if the solvers cannot obtain solutions within their understanding of reasonable time (e.g., ~hours)  It is hard to determine the effectiveness of the approach by comparing raw objective function value. I am surprised that the discontinuity in the optimality gap can incur such a high sub optimality, since it seems like a small problem. I appreciate the numerical toy example, but it is still not fully clear. A bit more intuition to the reasoning would be appreciated. I emphasize that I like the paper and my main concerns are on fair comparisons. I would be happy to increase my score if the authors include an additional Gurobi/SCIP baseline where they let the solvers run for sufficiently long to solve the problems to global optimality, and then compare against the relative optimality gap from the global solution.
Reject; rating score: 3; rating score: 3; rating score: 5; They propose a new setting in open set domain adaptation, where the goal is to classify known classes into their classes as well as cluster unknown classes well. A difference from an existing open set domain adaptation is that it does only require separating unknown instances from known ones whereas this paper aims to cluster unknown instances. They combine adversarial learning to align the two domains, and the knowledge graph is introduced to generate the classifiers for the unknown classes with the employment of the graph convolution network (GCN). They provide experiments on digits dataset and show the gain over baselines. Though the high level idea of leveraging a graph in the target domain sounds reasonable, their description of the method is not clear and not convincing enough. But, why it is enough to get discriminative features for unknown samples?<|endoftext|>The paper considers the problem of open set domain adaptation where the target domain has additional group of unknown classes and domain shift with source domain. One interesting aspect of the paper is that the method utilizes a Knowledge Graph for zero shot learning on the unknown classes. Further, the method utilizes an adversarial learning approach to align the source and target domain. So, it would be interesting to include some additional motivation for the zero shot learning (on unknown classes) in real world with domain gap. w2.Limited novelty of the proposed approach: The domain adaptation module and adversarial training procedure is not novel. Second, the target dataset is unlabeled.<|endoftext|>The paper formulates a novel problem in open set domain adaptation and aims to classify the unknown classes in the target domain. In eq (4) and (5), it is confusing to use t for both the "boundary" and the subsript for the target domain. I believe the knowledge graph is important for good zero shot learning, however, this information is not mentioned in the experiments.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper provided a novel viewpoint to understand the over smoothing of deep GCNs by analyzing the asymptotic behavior of the Graph Neural Tangent Kernel (GNTK), which governs the optimization trajectory under gradient descent for wide GCNs. However, it is unclear how this would affect the theoretical results (Corollary 1). I think this approach opens up new viewpoints and directions on understanding GNNs and should be interesting to the community.<|endoftext|>They first investigate the trainability of deep GCNs through GNTK, instead of expressive power, and theoretically prove that the convergence rate of the GNTK is exponential. The paper s insights are good, but the theoretical analysis seems based on a wrong equation, so they may need to recheck the results. Graph Convolutional Networks do not have such decoupled formulations.<|endoftext|>They proposed to utilize drop edge methods to solve this problem. And the experimental result shows that their solution is better than the baseline. The experiments actually show that the residual connection doesn’t solve the problem but slows down the collapse speed during training and the distance between elements drops exponentially, which corresponds to the exponential decay of trainability. The proposed solution edge drop has been widely applied in many models. 2.The experimental setting still has some problems. The hyperparameters for the baseline have been set the same as the original paper. Because this paper reported results for the baseline are not consistent with the original paper.<|endoftext|>Specifically, the authors exploit the Graph Neural Tangent Kernel (GNTK) and use it to analyze the trainablility of GNTK in the large depth, which provides insight for the reason of the over smoothing problem. **Thus, my recommendation for this paper is "weak rejection". 2.About the experimental results:    (1).<|endoftext|>The theoretical perspective presented is interesting. However, I have the following comments:1) In Corollary 1 it is claimed that there exists some L_0 for which for all L > L_0 the GNTK at depth L is singular. In the proof of the corollary, the authors say that this follows from Theorem 1.
Reject; rating score: 5; rating score: 6; rating score: 6; The paper extends self supervised contrastive learning from one single domain to multiple domains and achieve better generation for multi domain performance. The learning objective includes 3 terms, the first 2 terms (minimize the discrepancy between data with different views and maximizing the data pair from the same domain) are derived from SimCLR and the 3rd term is designed to maximizing the discrepancy between pair of data from different domains. 2.Good results. This seems that multi domain data are crossed in the latent space. It hints that performance of a particular domain is upper bounded by SimCLR on the single domain. Is it as expected? Cons: The formulation and motivation may need further clarification.<|endoftext|>This paper proposes a method for multi domain self supervised representation learning (MDSSL). The contrastive MDSSL training objective consists of three  terms: (1) a loss  term  that  maximizes the similarity between two transformed views of a sample in the latent space (identical to SimCLR), (2) a loss term that maximizes the similarity between every pair of samples within a dataset,  and  (3)  a loss term  that minimizes the similarity between pairs of samples  across different datasets. The paper proposes a simple and sensible method for  multi domain self supervised learning that obtains good performance on for both in domain and out of domain problems. Overall, I think the paper is marginally above the acceptance threshold.<|endoftext|>The paper presents a contrastive learning approach to self supervised learning (SSL) that works for multiple domains. They also provide an approach to using multiple domains when the domain labels are not available (using clustering). Second, there is not a good theoretical justification for expression (3). The heart of the method is the contrastive learning objective of the expressions (2 4). Is it doing some kind of regularization? Can the authors provide some more justification for this change, and perhaps give a bit more of the SimCLR context as I have described here? It achieves this by … (then the thing you said). Furthermore, the paper is limited by using tiny datasets which limit how well we would expect these results to generalize in practice.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposes an auxiliary method for training BNN models. It follows the idea of contrastive mutual information maximization, which utilizes the full precision and the binary activation of BNN to form positive (binary and fp activation of the same sample) and negative (binary activation of different samples) pairs for contrastive training. * The paper demonstrates promising experimental results. The results show that the proposed method improves performance on different data sets, multiple models, and multiple tasks. The approach looks like applying the method proposed in [1] to the BNN optimization problem. * **A convincing explanation is required. How do we understand the essential meaning of maximizing mutual information between fp activation and their signs? The mutual information of P and Q is the entropy of Q, since Q is always part of P. The increase of the entropy of P cannot explain the increase of the mutual information. Only when the distribution [ 1, 1] of Q itself is closer to 50%:50%, the entropy is the largest. But in this case, mutual information maximization has the same effect as maximizing entropy(Q). I hope the author can provide reasonable proof or at least give a more convincing explanation of why maximizing mutual information of P and Q makes sense? * **AC please note that** page 5 after equation 10: “where the detailed proof is shown in the supplementary material” I am very confused, where is the author’s proof? I did not find any supplementary materials. In addition, the formula derivation in this paper has many similarities with [1]. The supplementary materials in [1] provide specific derivations of the similar formula . ..* **Real difference with other contrastive learning methods not found. ** I can t see the obvious difference from other contrastive learning methods. It seems to be the application of existing methods in the BNN training scene. Hyperparameters are pretty different from the known open source BNN methods, including the use of weight decay (the essence of BNN training is to change the sign of weight, weight decay is almost useless, so usually wd is not used), too large initial learning rate (usually 10 100 times smaller), SGD optimizer (usually adam, see AdamBNN paper for explanation), I am curious how the author can exceed the results of ReActNet by training only 100 epochs because of the latter trains at least 256 epochs. In summary, according to the above questions, I am curious whether the author considers providing code and more experimental details such as logs and models? **Minor issue*** How do you implement BATS? Since the author still didn’t share the codes as they claimed. * The ImageNet results using ResNet18 (also ResNet34) seem to be relatively weak. * Related work overlooked a lot of recent efforts on binary neural network research, e.g., ReActNet, MeliusNet, AdamBNN, etc. However, some of them are compared in the experiment section. [1] Yonglong Tian, Dilip Krishnan, Phillip Isola, Contrastive Representation Distillation,  ICLR2020 Regarding the existing problems in the paper, I recommend rejection for now, but it may be adjusted according to the rebuttal.<|endoftext|>The authors propose to make full use of the full precision latent weights in BNN training by utilizing the popular contrastive loss between samples generated by full precision activations and binary counterparts. They follow the derivations of "Contrastive Representation Distillation" to bridge the gap between mutual information maximization and the proposed loss function. The experiment results show consistent improvements over strong baseline methods on image recognition tasks. 2.The idea of applying contrastive loss to BNN training seems new. It is interesting to see that full precision latent weights can still be used in a new manner. cons.1.The main concern is the direct similarity between CMIM and CRD [1]. Please refer to the "Details Of Ethics Concerns". 2.Since the core part of this manuscript shares the same idea with CRD, the authors may overclaim the contribution of "a novel CMIM framework". In light of this, I think the novelty of this paper can be limited. However, I found no ablation study on it. Why CMIM should outperform CRD (if the authors argue that CMIM is indeed different from CRD)? 4.It is well known that KD should further improve the performance of student networks. Besides, Label Refinery [2] and Real to Binary [3] have shown that KD+BinConv leads to $+7$% Top 1 accuracy on ImageNet with XNOR Res50 and $+4.3$% Top 1 accuracy on CIFAR 100 with Res18. 5.Since CMIM achieves much higher accuracy than RBNN, it is no doubt that t SNE results will be improved. Note that the authors tend to maximize the lower bound of the mutual information. I expect some in depth analysis on the change of mutual information during training. 7.Is there any constraint on the form of $h^*(\mathbf{a}^{k,i}_B,\mathbf{a}^{k,j}_F)$? Why use Eq.(8) as the critic function to approximate the target distribution? How to determine whether $h^*(\mathbf{a}^{k,i}_B,\mathbf{a}^{k,j}_F)$ converges to the target distribution? 8.Strangely, the most related work CRD is not included in section "Difference with other contrastive learning methods." arXiv2018  * [3] Training Binary Neural Networks with Real to Binary Convolutions. ICLR2020The authors may clearly discuss the differences between CMIM and existing works. The current draft makes it hard to fully evaluate the contributions of this paper.<|endoftext|>This paper proposes to use contrastive distillation to train a binary network by maximizing the mutual information between itself (student network) and the full precision network (teacher network). Empirical results show that this new objective further improves the binarization performance on top of several recent binary networks on image classification tasks. The authors also empirically show that models trained with the proposed contrastive objective have good transfer performance. My main concern is about the novelty. A similar contrastive distillation method has already been proposed in [1]. This submission falls into the first setting shown in Figure 1(a) of [1], which compresses the model by maximizing the mutual information between the binary student network and the full precision teacher network. The derivations of contrastive mutual information maximization in equations (4) (7) are almost the same as (4) (11) in [1]. The loss function (9) in this paper also resembles that (equation (18)) of [1]. The authors should clarify the connections/differences of this submission with [1]. Minors:   What does \dot mean in equation (1)? [1] CONTRASTIVE REPRESENTATION DISTILLATION, ICLR 2020. This paper is overall structured clearly and well written. However, the novelty may be limited as it can be viewed as an application of the previous contrastive distillation method to the quantization task.<|endoftext|>This paper proposed an approach to reduce the performance gap between the binary neural networks and their real valued counterparts via maximizing the mutual information between the binary activations and the real valued activations that exist in the binary neural network. Specifically, the authors propose to treat binary activations and real valued activations as two views of the same image and use contrastive learning to pull these positive pairs while pushing other negative pairs generated from different images. The idea of treating binary activations and real valued activations as two views of the same image in self supervised learning is interesting and makes sense. The results also well support the effectiveness of the proposed method. Overall, the submission is technically sound. I have several questions as follows:1. Does inserting the MLP layers and using the proposed CMIM module slow down the training greatly? Does it look significantly different from other binary neural networks? The authors mentioned researchers in [13](CVPR 2021) propose to shift the thresholds of binary activation functions. However, learning to shift the thresholds in binary neural networks is first proposed in ReActNet (ECCV 2020). 2.The caption in Fig.4: “the effect of number of negative samples in contrastive mutual information maximization (c f)” should be “(e f)”In summary, this is an interesting paper with clear motivation and novel techniques. I will raise my score if my concern about the training time is well addressed.
Reject; rating score: 1; rating score: 3; rating score: 3; Could the authors comment on why this is not an issue? While a small handful of these would not constitute a major weakness (and, in fact, could be easily enumerated and corrected), there are enough of them to impact readability of the paper. * The clarity issues raised above suggest that the paper requires substantial revision in order to more clearly convey the contributions and algorithms to readers.<|endoftext|>The authors address the problem of exploration in reinforcement learning. What does the "width" at which it is influential on the number of pathways" mean? What is the "objective credit assignment"? The algorithm implementation or pseudo code is also not given in the paper. Page 9, contains an interesting description of the games, showing insight. Although a promising avenue, many parts of the paper are confusing, using inside jargon, and decreasing the overall clarity. Some parts of the paper are not supported by results, and the main methods used are referred to the appendix.<|endoftext|>The authors introduce a RL method for optimizing entropy over states, as opposed to actions. Overall, I found the organization and clarity of the paper to be poor. I provide some more detailed comments on how the clarity can be improved at the bottom. Are these meant to be the set of all possible state/actions, as defined by $\mathcal{S}$ and $\mathcal{A}$ in 2.1? Problem statement really needs to be split into at least 2, if not more, paragraphs.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; rating score: 5; The paper proposes a new regularization approach to introduce geometric constraint to the latent space of an autoencoder. A hierarchy for geometry preserving mappings is formulated to clarify how strong this constraint can be defined. This scale is learnt together with the manifold and the latent space representation during the training of the autoencoder. This approach was proposed also in (Chen et al., 2020) for the so called FMVAE, but here a new coordinate invariant regularization term is introduced that measure how close the decoderis to being a scaled isometry. Finally, a post processing flattening procedure is introduced to further improve the geometry properties of the latent space. There are some important points that authors should clarify: In the introduction, authors claimed that using a less stringent regularization term on isometry is more helpful, but than is not clear to identify other parts of the paper that support this claim.<|endoftext|>The paper makes two main contributions. The first is the introduction of a new regularizer term for the VAE loss, ensuring a (scaled) isometry between the learned latent space and the (typically unknown) data space. Might still have an impact on a certain community, but a potentially broader impact is not clear at this stage. The second is a post processing "flattening" step to improve the isometry constraint by directly operating on the latent space, while leaving the reconstruction error untouched. In particular, it is important to demonstrate that isometric representation learning is relevant, and that it can lead to significant improvements in many contexts. I suggest to include some sensitivity experiments showing the influence of these dimensions on the final results, together with a discussion on their effect. Similarly, I could not find a sensitivity experiment showing the influence of parameter k (degree of the conformal mapping). The mathematics are sound and accessible. How should the equidistance plot be read? The more isotropic, the better?<|endoftext|>Specifically, they add a regularization and explore the tradeoff between reconstruction and geometry preservation. The authors address an important problem in unsupervised learning, namely, manifold learning using a NN. Proceedings of the National Academy of Sciences, 2020, 117.49: 30918 30927.‏To summarize, the authors address an important problem and propose a new method for learning a low dimensional representation from high dimensional data. The method proposed in the paper extends two recent works by Chen et al.and Jang et al.They propose an Isometric Regularization that is realized based on Hutchinson’s stochastic trace estimator to enable efficient estimation of the gradients. However, the method has some limitations (namely the low dimensional embedding dimension), and the experiments are not backed up by a rigorous scheme for hyperparameter tuning. They rely on the manifold assumption and propose a new regularized to a variational autoencoder. However, I have several concerns about the proposed method. Specifically, my major comments are: The method proposed here is a variational autoencoder; why is it presented in the introduction as an autoencoder? The MSE gain compared to FMVAE is really marginal and could result from non optimal hyperparameter tuning. The equidistance ellipses are not well defined, are these computed in the ambient space and plotted in the latent space? Therefore, these ellipses should be related to a small distance in the ambient space. The scale of these ellipses is not provided. Minor comments: ”Learning the data manifold”  this term is not well defined in the paper.<|endoftext|>International conference on machine learning. The paper follows a solid theoretical foundation using (scaled) isometry between input and latent space as a principle for designing a regularizer to learn meaningful representations with autoencoders. **Weaknesses:**While I mentioned most experimental results are convincing, I still think the paper has some considerable flaws to be addressed. I see this as critical for the paper to be accepted. Second, there seems to be a conflict between the KL divergence regularization of a VAE w.r.t.changes introduced to the latent space caused by the newly introduced regularization scheme. I believe there should be a discussion on this, since knowing $P_Z$ seems to be necessary and without the KL term it is hard to say it follows a standard Gaussian. Why is it the case that IRVAE outperforms IRVAE+FM w.r.t.to P@1 by so much, but this does not happen in any other case?<|endoftext|>This paper studies the aspect of preserving geometry on the learned latent space representations. In particular, the paper looks at a hierarchy of geometry preserving mapping (isometry, conformal mapping of degree k, area preserving mapping, etc.). The comparison is done with VAE (Kingma & Welling, 2014) and FMVAE (Chen et al.2020).Experiments are shown on CelebA with 40 annotations. In the implementation H(x) is assumed to be identity and this is a strong assumption. 3) The visualization in Fig.1 does not clearly show the improvement using the isometric regularization approach shown in the paper. 4) There should be some ablation on the mixup regularization that is used in this paper. 5)  The paper does not look at sufficient strong baselines for representational learning methods. Overall, the paper addresses an important problem but there are many concerns with the experiments and the formulation.
Reject; rating score: 3; rating score: 3; rating score: 5; Details of Cons* The key concern about the paper is the question being tackled in the paper is unclear, and the lack of rigorous experimentation to support the hypothesis. * Another concern about the performance is that the improvements are minor compared to the original KD and most of the experiments are less than 1%. Cons* The research problem is not well stated. * The experimental results are not sufficient to support the claims.<|endoftext|>The analysis and proposal methods covered in this paper are interesting in terms of performance improvement. It is well known that DA differs in the degree of performance change depending on the learning configuration. Through this, the authors show that it is possible to effectively improve performance even with the DA technique, which has not been well fused in the past.<|endoftext|>Also, the related work and the experimental analyses of the paper can be further improved. W2: I am a bit concerned about the technical depth of the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Overall, I recommend accepting this paper for its solid technical contribution and convincing results. The demonstrated examples show the effectiveness of the attacks that even training with large batch size, attacks with certain knowledge about the distribution of the data could recover almost the exact training data. There could be another DP related mitigation where the model updates are first securely aggregated and then added with central DP noise before sending to the server. This paper proposed novel and effective data reconstruction attacks in federated learning.<|endoftext|>The paper provides another piece of evidence that federated learning on its own is not sufficient to protect user privacy, and explains how even with only a minor malicious change the server can exactly recover user data. This paper explores the privacy of federated learning when the server is malicious, specifically that it is allowed to control the architecture and and set certain weights in adversarial ways.<|endoftext|>This paper presents a new attack on federated learning, demonstrating that model updates shared in a federated setting can still leak user data. Attacks before were based on updated from a single data point, and hence aggregating data in an update was used as one approach against this attack. Allowing models to be changed in ways that are currently allowed with FL APIs, this paper shows that user data can still be reconstructed even with large aggregations. The experiments show the power of the attack, while demonstrating that not every image is reconstructed. ### UPDATE ###I have read over the author feedback and think the suggested changes can improve the work. Overall a nice attack that shows the keeping data distributed while only sharing model updates does not provide strong privacy guarantees.<|endoftext|>This paper provides a privacy attack scheme for federated learning, retrieving the private data from the model aggregation techniques. The idea looks interesting, but I have some concerns as below. This is an interesting paper, but it is better to include more concrete theoretic/empirical analysis on why/when the suggested scheme works.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper introduces a pipeline to generate parallel data corpus for code to code translation instrumented with unit tests, starting with monolingual data in Java programming language. It is also frequently used during inference, as a postprocessing step, and evaluation. Authors claim that the unit tests suites can be used to test program semantics in any programming languages, provided there is a way to establish the mapping between types and parameters. While this is true in most cases, have you observed and analyzed some corner cases, for instance, when an object needs to be instantiated in the body of the unit test method prior to assertions. How often does it happen, are these cases ignored? How does it handle test cases with multiple asserts? Minor:For multiple published papers the authors cite corresponding arXiv pre prints.<|endoftext|>Using this additional data, the paper shows that machine translation using state of the art transformers between programming languages significantly improves over prior data collection methods with back translation. Technically, the work touches many parts   “static” text to text transformer, executing code, checking tests passing, observing and optimising test coverage. What would have been more interesting though is how well would it work with differences in the test generation/acceptance strategies. One of the biggest limitations of the approach seems to be around how unit tests are translated between the languages. A related question is if tests could be translated with some of the baseline models such as Transcoder/DOBF and if such an idea was considered and was/wasn’t successful. I would be happy to get a confirmation of this from the authors. The idea of the paper is specific to translation between programming languages and may be of interest to a subset of the ICLR community. There are interesting ideas and overall good evaluation of the paper.<|endoftext|>This paper is aimed at using the automatic generation of unit tests as a means to perform transpilation (i.e., the translation of code from one language to another). Overall, I like this paper. The idea of using unit tests for transpilation is, although not formally guaranteed, a reasonable approach for transpilation in my opinion. Some of the language in the paper is a little off putting as it seems to be disconnected (in my opinion) from deeply understanding the space of natural language translation (unstructured ambiguous languages) vs. programming language translation (structured unambiguous languages). There are many other such examples that seem to demonstrate the authors perhaps need to do a bit deeper literature review before making such bizarre claims. Do the authors not know about the field of "transpilation" and that they are working in this space? TL;LD: it s an important emerging area. It s a reasonable (although not formal) way to perform transpilation.<|endoftext|>I have 2 concerns with this proposal:   1. An important premise of automated code translation   which the authors mention in their introduction and ethical concerns section   is about modernizing legacy languages such as COBOL, where automated test generation systems might not be available at all. The proposed method therefore, in my opinion, is bottlenecked on the availability of automated unit test generation tools, and is not applicable when they are not available. 1.Bottleneck on the availability of test generation tools thus limiting the number of languages it can be applied to. 3.Method to generate test cases for translated Python and C++ codes seems to be dependent on expected input/output pairs, and it s not clear how the proposed method would work for more complicated code segments, or for languages with no direct mapping of data types and constructs. I m not completely convinced that this is an ideal strategy that can generalize to a broader set of languages or for more complicated pieces of code. Or, how would this procedure work when there isn t a direct mapping of data types and constructs between the languages. This further reduces the applicability of this approach.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The key idea is that one learns both a symbolic as well as a neural encoder, and gradually makes the symbolic encoder more and more structured progressively increasing the complexity of it. Given such an encoder, the paper proceeds to show that the encoding from it is useful for clustering sequence data and demonstrates gains over other “unstructured” purely neural approaches. Positives+ The general problem of learning programs to represent data in unsupervised learning is novel and potentially fundamental to machine learning and AI+ The proposed method is simple and interesting, and seems to achieve nice gains over purely neural approaches+ The paper is generally well writtenNegatives  My main issue is that the neuroscience data that the paper currently mainly evaluates on is not something a general ML audience might be very familiar with, and thus, it is hard to estimate what makes the task hard or easy. Could the authors give intuitions on why regular neural encodings fail to cluster in a better manner? Are the gains from the symbolic approach something that could just be achieved by a convolutional encoder? (*)  I generally think such an encoding is potentially very useful for image data, and might be a big step towards better image representations and advances towards human like intelligence. This is an even larger concern in light of the fact that a general machine learning researcher might not have a lot of great intuitions about the neuroscience sequence data and that makes it hard to assess the impact of the work.<|endoftext|>For evaluation:1) Current evaluation seems not very convincing to me. The authors only show that with the help of symbolic program, the method could get representations with better cluster quality (program helps representation learning). But I think a more intersting perspective is to see whether the learned program itself is helpful. For example, whether it could be used to predict future trajectory (such as 3 body problem), or even help solving some high level reasoning tasks.<|endoftext|>The authors propose an unsupervised approach to train neurosymbolic encoders to obtain a programmatically interpretable representation using the dictionary of a domain speciﬁc language (DSL). The results also show that the performance can be robust across different DSL designs by domain experts. The specific comments are as follows. 3.The significance of the experimental results was unclear to me. In the basketball task, I am also concerned about the importance of offensive/defensive players detection (this is usually given). The final downstream task (not tasks) is a classification task, and the results were comparable with the previous work (Sun et al.2021b).Minor comments:4. What is x in Sec.2.2?If it is some inputs, what types of inputs? Or What DSL was used in Table 1 results? Although the concept and method are simple and new, and the paper itself is well written, the significance in the problem setting and experimental results were unclear for me.<|endoftext|>The paper is clearly written and the idea was easy to understand andfollow. I wish there was more exploration of how often are sensibleprograms learned and how sensitive this is to the choice of DSLs. Maybe something comparing the programs of experts to whatthe latent representation learned? This would also benefit from comparisons that use purely the neural componentI am also curious empirically how it s decided how many symbolicprograms should be in the latent representation? Would a similarprocess be used for real world data? This is an interesting and novel way to learn programs as symbolic encoders of the input. I have some nagging concerns about how incremental the contribution given how much this work relies on Shah et al.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6;   The paper introduces the notion of "orthogonal classifiers": classifiers that rely on orthogonal variables. It starts with the simple linear case, and adds a definition that also applies to the non linear case. The paper proposes two methods to identify a classifier orthogonal to a given one. It then describes 3 use cases: style transfer, domain adaptation, and fairness. Strong (+) and weak ( ) points  (+) The authors propose and define a new concept: orthogonal classifiers. (+) Demonstration of empirical benefits in multiple applications. So it s difficult to evaluate the novelty/advantage of the proposed method. I believe these could be fixed by the authors for the final version. The concepts of "principal classifier" and "principal variables" are used in the abstract and intro without being defined or given a reference about. Training a "full classifier" does seem trivial to me.<|endoftext|>Given a defined notion of orthogonality for random variables, this paper proposes a straight forward approach to constructing classifiers orthogonal to a given classifier. Examples of mapping problems to the orthogonal classifier setting are provided for domain adaptation and fairness as well as the newly proposed problem of controlled style transfer. The paper does an excellent job of defining orthogonal classifiers and provides a clean solution to learning these models. Providing a reasonable example of where this would be applicable would significantly increase the value demonstrated in this section. 2.Given the controlled style transfer problem is novel, it s difficult to determine whether the empirical results are significant as the baseline compared against is designed for a non controlled style transfer problem.<|endoftext|>The algorithm turns out to be very simple, only requiring access to the full classifier $P(Y|x)$ and the principal classifier $P(Y|z(x))$, where $z$ is a control variable you want to orthogonalize against. The effectiveness of the proposed classifier orthogonalization technique has been demonstrated through 3 applications: controlled style transfer, domain adaptation, and classifier fairness. Strength    The paper presents rather a simple, but principled and novel way of constructing orthogonal classifier for any non linear classifiers. The effectiveness of the proposed technique has been confirmed on various applications, including style transfer, domain adaptation, and fairness. Weakness    For domain adaptation experiments, it seems authors assume access to the marginal distribution of label variables for the test set. Is label distribution of the test set for domain adaptation experiments uniform or does it follow the distribution of the target domain used for training? For domain adaptation experiments, one baseline could be post processing the source only model, similarly to the procedure for constructing fair classifier in Section 6. Overall I find that the proposed method novel and principled. As mentioned in the review, some information could be verified through discussion, but I am leaning towards acceptance for my initial rating.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; rating score: 8; This work studies the relation between properties of the feature extractor, and the downstream performance on binary classification tasks. The authors identify two key properties, namely "local alignment" and "congregation", which can be used to derive lower bound on the test accuracy and generalization bounds. The theoretical results are verified empirically on CIFAR10 and CUB200. Strength: the paper is clearly written and easy to follow. The observations make intuitive sense. Questions:    How does the correlation between your lower bound and test loss compare with the correlation between kNN accuracy and the test loss? Empirically, how well can the lower bound predict the performance, when the learned representations are far from being clustered (for example, on representations from some lower layer of a network)? For few shot transfer, how would the performance change if we vary the number of training samples per class? The paper is clear and well justified. However, I find the results unsurprising, since local alignment and congregation together mean that the features should be well clustered, which is intuitive and well known. Personally I think the paper would need more technical contributions to be accepted. While I agree that the paper has improved after the revision and I acknowledge the contributions, it s still below the bar to me though only marginally. Given the theoretical contribution, I d expect stronger empirical results for the proposed method to be convincing. Potential improvements, as also mentioned by other reviewers, include 1) comparing with more baseline methods with a larger variety of features (e.g.expand Table 1 with results from other self supervised methods, including non contrastive ones), and 2) better justifying the criteria for model selection (i.e.the sum of the lower bound in Thm 1 and the upper bound in Thm 2) as it currently seems arbitrary.<|endoftext|>The paper asks the question “what makes a feature representation good for a target task?” In order to tackle this question, it introduces the concepts of local alignment (examples similar in feature space  > same labels), and congregation (how much examples generally embed close to each other). It uses them to produce bounds on achievable accuracy on binary classification problems which seem to be correlated with actual accuracy on real binary tasks. Minor CommentsThe paper is well written. The concepts presented are intuitive and seem useful. And their transferability to the empirical tasks chosen seems well demonstrated. As an empiricist, I defer to other reviewers when it comes to reviewing the theoretical proofs, but I can speak to the paper’s claims of practical application. My main concern is that the empirical tasks chosen are very constrained classification problems (made into binary tasks by choosing pairs of classes; no multi class fine grained classification tasks as is more realistic). I understand the desire to start with a simple settings, especially in a theoretical paper, but it makes it hard to justify the paper’s claims of testing “what makes a [good] feature representation” and “impact[ing] practical decisions”. Another question: how does one know if an embedding is not “congregated” unless there is a less congregated frame of reference? The paper alludes that “congregation” might help explain the success of contrastive learning, so it’s disappointing to see no experiments trying to demonstrate this. I also believe the paper would have been improved if the authors explicitly encouraged/discouraged local alignment/congregation when optimizing models, and showing that this changes the metrics introduced accordingly. This is a well written paper that investigates a narrow task setting which unfortunately does not justify its claims of practical applicability. This could change with more extensive experiments in realistic datasets/settings.<|endoftext|>The paper presents two properties, "local alignment" (eq 3) and "degree of congregation" (eq 4) which are claimed to be good predictors of downstream (classification) task performance. These properties are used to derive bounds on the error of downstream classifiers (under a number of assumptions, including a linear classifier with Lipshitz constraints), and are investigated empirically for pairwise classification tasks derived from the CIFAR 10 dataset, and for model selection on a few shot task. However, as defined, p_c^{\phi}(\alpha) would depend on the scaling of the feature space. For example, it is easy to see that for a Gaussian p(x) in 1 d, this quantity will depend on the standard deviation of the distribution (and it is easy to extend the analysis to a multivariate Gaussian.) Notice also that a Lipshitz constraint only makes sense relative to the scale of the feature space. Comment: I believe that the intuition behind Defn 1 is relevant to the issue of "what makes a feature representation good for a target task? However I believe the claim in eq 5 is correct. The result is then intuitive. However, I doubt that Claim 1 is of great practical use. The loss function (eq 2) is introduced because "the zero one loss ... is difficult to analyze". Thus I don t believe that this property is relevant to the zero one loss we actually care about. We start my making the above statements *conditional on x* as mentioned for Defns 1 and 2 above, giving Pr( ||phi(x)   phi(x )|| < 1/W AND y \neq y  | x)   p^{phi}_c(alpha;x) \times (1   p^{phi}_a(alpha;x)) , where the notation ;x indicates dependence on a particular x. Hence I believe there is a "bug" in this derivation for Theorem 1 as stated. Another issue is that Theorem 1 depends only on the losses obtained when ||phi(x)   phi(x )|| < 1/W AND y \neq y , so other losses will be sustained for situations where this condition does not apply, so the bound may be very loose. Also as noted above it depends critically on the loss function used in eq 2, and so may have little relevance to the zero one loss of interest. I realize that this is mainly a theory paper, but I believe the analysis of the experiments can be much improved. For the CIFAR 10 data, for each of the 45 tasks we have 18 different representations. For EACH of the 45x10 trials, one can compare the training and test losses for each of the 18 representations against the bounds. It is notable that these last two methods can perform worse than the Random baseline. Further comments: It is worth noting that the "purity" of the classes in a given ball of radius alpha around phi(x) (as in Defn 1) is something that also arises in the "RadiusNeighborsClassifier" version of k nearest neighbors, see e.g.https://scikit learn.org/stable/modules/neighbors.html#nearest neighbors classification   for this paper the radius is given by alpha. Also, importantly, it is not so clear that bounds on the loss from eq 2 will be well correlated with the 0/1 loss. (This could be investigated experimentally in sec 7.) * I like the new Fig 3 (as suggested by some of the reviewers and now implemented.) I d like to see more focus on 0/1 loss in the expts,  rather than the margin loss. (I note 0/1 loss results are promised  for the camera ready version.) While the intuition behind Definition 1 makes quite a lot of sense, there seem to be some errors in the derivation of Theorem 1.<|endoftext|>This paper proposes to study how feature representations are transferable to downstream tasks. It presents a theoretical characterization of such transfer, in terms of relatively intuitive concepts of congregation and alignment. The paper validates these theoretical observations by experiments with transfer in visual settings, between supervised tasks, or to few shot (+unlabeled data) transfer. This paper is clearly written, and the bounds are fairly intuitive and well presented. However, I think that the paper would be improved with better experimental evaluation, and more discussion of the existing literature. * Relatedly, this paper does not compare to the work of Nguyen et al.(2020) because it "assumes that representations come from classification tasks." This seems like a rather weak justification, since the present paper only evaluates representations pre trained on classification (even if they sometimes do joint coarse + fine classification, one could apply the method of Nguyen et al.to the joint labels I believe). * While it can be useful to have transfer bounds that don t depend on training, computing the probabilities used in the bounds exactly involves pairwise comparisons, i.e.$O(N^2)$ computations over the data. For large supervised datasets it seems like the transfer estimates might be better from spending an equivalent amount of compute actually training a classifier on the data and seeing how well it did. It would be useful for the paper to compare to such methods. * I would also like to see some more analysis of how important the unlabelled data is in the few shot setting, as this is fairly non standard in the meta learning literature. Literature This paper would benefit from situating further within the existing theoretical literature on transfer. This work seems in several places to overlook prior theory work that considers generalization in ways that could be (or are) applied to understanding representations and transfer. It would be useful to discuss any relationship between the bounds they express (taking the one prior task case from their work) and the present work. I haven t read this paper in detail, but superficially it seems very relevant to the present work, and it should probably be discussed. Their notion of transfer is a little different than explored in this work, since they consider simultaneous training with shared weights rather than classification of trained representations. It s also quite possible that there are even more relevant papers I ve missed; I hope the other reviewers will share some. Fine grained analysis of optimization and generalization for overparameterized two layer neural networks. The paper is clear and seems moderately useful; broader experimental evaluation and more engagement with the literature would improve it substantially.<|endoftext|>The paper addresses the question of what makes a representation suitable or "good" for a particular task. The analysis is based on the simple and intuitive concepts of local alignment and degree of congregation of the data points within a representational space and according to a binary labeling of the points. Two experiments support the theoretical claims. strengths:  The paper is very well written. The introduction makes very clear the specific challenges to be addressed in the paper and the approach taken. The description of the relationship to previous work is thorough and precise. For example, the analysis assumes the relevant task is a classification task and so the results may not be relevant for reconstruction tasks or other regression tasks. It seems to imply that a good rep is one that is already appropriately disentangled, rather than one that can be disentangled by, say, a neural network (which would violate the assumption), to do something useful (or several useful things). Pearson seems more appropriate than Spearman correlation here since the the losses and errors are on an interval rather than ordinal scale. Consider moving a basic description to the main text. You could also include a more informative plot that a bar plot, e.g.show the distribution with box & whisker or violin plot, etc. The submission seems like a clear accept. It makes a clear contribution to an important topic and is very well written and clearly described. The statistical analysis of the empirical results could be improved. I made a number of minor comments that I think can be easily addressed to improve the quality of the paper. I have no major complaints.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; I recommend reject, because several claims are wrong or not well supported, and the contribution over related work is not sufficiently discussed or demonstrated. The proposed method also has some novelty, such as learning probabilistic graphs and using contrastive loss to train the forward dynamics model. However, I have some concerns listed below. The dataset used in this paper seems a bit simplistic. This can reduce the usefulness of the learned keypoints.<|endoftext|>(2).The evaluation is limited on a newly built dataset and it will be more convincing to compare the proposed methods on the previous dataset like CLEVRER, which offers more baselines for future prediction. The reviewer does think that a revised version will make the paper much more appealing. **2.Concerns**The reviewer does have some concerns about the model. Since a bunch of significant baselines are absent and the details of the dataset are missing, the reviewer thinks the paper in the current version is not suitable for publishing on ICLR.<|endoftext|>There are three major steps in this model: extracting keypoint coordinates, inferring a probabilistic graph representation of the system, and estimating the next state of the system conditioned on the action. The interpretation is not clear; why it helps to learn actionable object centric representation? 3 As for the experiments, I do not think the dataset matches the authors  claim: hard to annotation/detection and complex scenes. 2) In the proposed dataset, I notice that there are very large `+  object, but the method only determines a few key points for each object.<|endoftext|>Strengths:  The paper is well organized  The problem motivation in the introduction is clearly explained and analysis of related work is quite thorough  I appreciate that the experiment section includes not only experiments on reconstruction and future prediction, but also a simple control task. This is important to show the applicability of the proposed method. Why doesn t it interfere with training?
Reject; rating score: 3; rating score: 5; rating score: 6; Goals: This paper introduces a new optimal transport loss based on a minibatch computation in order to alleviate some weaknesses from the original minibatch OT formulation. The formulation treats minibatches as data and seek to transport the minibatches from the source distribution to the minibatches from the target distribution. By doing so, their method prevents some undesirable connections between data. They provide small toy examples to see the differences between the proposed and the original losses. They also discuss the computational complexity of their method and discuss some practical aspects of their contributions. For instance on the theoretical side:(i) While discussing the connections between samples from the transport plan, the authors do not discuss the transported mass. (ii) Also a study similar to [1] about the possible bias of stochastic gradient is important. On the experimental side, there are many experiments, but some of them are not extensive (generative modelling and domain adaptation) which make the performances of their method unclear. I am also skeptical that the method is really competitive in practice (numerically and memory) with state of the art method due to the fact that it needs a bigger k for the loss to be computed. (ii) On domain adaptation and generative modelling, I am not convinced that the proposed method outperforms other methods in practice due to its overall complexity (k>1). (iii) On domain adaptation, there are more recent methods and I would like to see the comparison [2,3]. Related Work and Discussion: The related work is complete and well discussed. The bibliography style seems respected. Clarity: The paper is clearly written except for the experimental part. A reader which do not know about generative modelling, domain adaptation, color transfer or bayesian computation can not understand the purpose of the different experiments as the explanations are in the supplementary materials. I think this is mostly due to the high number of different experiments. Does your loss transport all samples ? I think it does, but it should be proven. This could be done on domain adaptation experiments for instance. I have read the other reviews and the different answers. I still think that the idea is appealing, but the way the paper is written and the fact that the experiments are not extensive enough prevent a publication at that time in my opinion. The proposed formulation is appealing and I like the idea. There is a big discussion regarding where the method can applied and it could be replaced by a discussion on theoretical aspects which are not mentionned. Regarding experiments, I encourage the authors to focus on two experiments (Generative modelling and domaine adaptation) and to make extensive experiments with recent baselines.<|endoftext|>An enhancement over a mini batch version of OT that provides better empirical evaluation over a large array of tasksThis paper introduces BombOT, a hierarchical approach to combining mini batches in order to enhance the approximation quality of mini batch based approaches to OT. I believe it is a nice contribution and the main strength is the comprehensive list of experiments performed, illustrating benefits on several tasks in statistics and machine learning. I have a few concerns, though1)The theoretical findings are not very strong. Showing that there is a distance being defined is a nice finding, and contrasting this with the lack of a metric structure in usual m OT suggest that this construction is in the right direction. However, practitioners are more interested in convergence properties. In this regard, theorem 2 doesn t say much, as it corresponds to a comparison between batch and population versions of the proposed method. This doesn t say much about how does this relate to the original wasserstein/sliced/etc distance. 2)Similarly, experiments mainly comparse m OT and bomb OT. We should be able to at least empirically how large batches need to be so that we will get decent approximations. As a practitioner who tried used m OT I believe the problem is super important, but the numbers provided don t say much if they are not put into perspective with respect to the true distance. The findings reporting in Fig 3 are honest, but a bit discouraging in that respect. how large need k and m need to be so we get a reasonable approximation? (related to (1)). 3)another suggestion (optional): I believe this method can be useful for aligning spaces, as in https://arxiv.org/abs/1809.00013. It would be good to have comparisons there as well, and to show whether this mini batch approximation can be reasonable (i.e.compare against the no mini batch case)Nice article.<|endoftext|>This paper proposed Batch of Mini batches Optimal Transport (BoMb OT) method, which finds the optimal coupling between mini batches in mini batch optimal transport (m OT), which is achieved by solving another OT problem over the mini batches. The authors then implemented the proposed BoMb OT method and applied it on deep generative models and deep domain adaptation, showing that BoMb OT has favourable performance over m OT. strengths:The optimal transport (OT) problem studied in this paper is important. Existing work and their advantages and disadvantages are discussed and the study is well motivated. Theorem 2 shows that the proposed BoMb OT method is sound and approximating some well defined metrics. On the other hand, the authors provided the implementation and demonstrated the usefulness of the proposed method using several different applications. As also noted the entropic regularized version of population BoMb OT would recover m OT in some cases. Does this mean the BoMb OT will also recover the same problem that m OT would have? 2.Figure 1 is confusing to me. I get the authors wanted to provide some intuitive explanations for the advantages of using BoMb OT. However, I do not understand why and how using BoMb OT can achieve the right transportation in the figure while using m OT cannot. I suggest if possible these examples can come with specific numbers such that we can calculate and verify the results, making them easier to understand and more convincing. 3.The BoMb OT has one additional OT comparing to m OT as noted in "Computational complexity of the BoMb OT" paragraph ($O(k^2(m^2+1)/\epsilon^2)$ vs. $O(k^2 m^2 /\epsilon^2)$). It looks contradiction to what is mentioned in the appendix, i.e., "The run time of the m OT nearly doubles that of the BoMb OT". Is this from a better implementation of BoMb OT. Overall, I found the problems studied in this paper interesting and important, the proposed method reasonable, theoretically sound, and well supported by experiments and implementation details.
Reject; rating score: 5; rating score: 5; rating score: 6; The paper focuses on the stochastic variant of the projective splitting (PS) algorithm. With a specific focus on monotone inclusion problems, the authors propose a novel separable algorithm featured by the ability to handle multiple constraints and non smooth regularizers.<|endoftext|>This manuscript proposes a stochastic algorithm for monotone inclusion with more than two operators, where one is Lipschitz and the rest are maximally monotone and possibly set valued. The current organization seems to be incomplete given the length of the main paper. This manuscript is ambitious in tackling the problem of multiple operators with a stochastic oracle. This can be seen as a natural generalization of stochastic gradient algorithms for more than one regularizers to the setting monotone inclusion, and can find potential applications in min max problems and so on. If the comparison is in terms of epochs (so that implementation difference can be excluded), I wonder if the outcome would be much different.<|endoftext|>The paper presents a stochastic variant of the projective splitting family of algorithms for solving monotone inclusion problems. NeurIPS 2021 (to appear). I believe that the broader audience of the ML community is not very familiar with the monotone inclusion problems.
Reject; rating score: 10; rating score: 3; rating score: 6; rating score: 6; rating score: 8; In this paper, the authors bring together recent work in the OOD detection problem and provide to the reader a sound mathematical framework to understand similarities and differences among these. The framework is based on the equivalence class of scoring function under the AUC / FPR@qTPR metrics and bayes optimality. The tools introduced in the paper allow the authors to explain why different methods perform largely similarly, when one scoring function should be preferred to others and draw conclusions regarding training with / without label data of the in distribution set. + (+) The main contribution of the paper is to unify under the same mathematical language different methods that were previously related less clearly/effectively. + ( ) The analysis framework is based on bayes optimality which is asymptotic   as the authors state in the ethics and reproducibility statement section at the end of the manuscript. But this is not an ethic nor a reproducibility matter: it is a peculiarity / design choice of this work that should be crystal clear to the reader, who will decide if this is a limitation she cares about or less. I would have liked to read it in Sec.2 or Sec.3 at most. Theoretical insights that unify different methods under the same light and allow new analysis are so valuable to the community, especially when they are so clearly exposed. The authors also show these theoretical insights are actionable, by drawing new conclusions later supported by empirical evidence. I think people working in the field will benefit from such a clear and encompassing narrative. For these reasons I will take into great consideration all other reviews and discussion for the final score.<|endoftext|>The authors show that 1) a simple baseline,  binary discriminator between in  and out of  distribution data, is competitive with state of the art OOD detection techniques and 2) the Bayes optimal scoring functions of several proposed methods for out of distribution detection are equivalent to the scoring functions of simple binary discriminators. The authors also aim to provide a better understanding of key components of various OOD detection methods such as Outlier Exposure. Strengths:The paper is trying to demonstrate that a simple baseline can out perform other more complicated methods. I appreciate such efforts and I think they provide a good check for the field in general. For example, the intro discusses why OOD detection is an important problem and then delves into related work, but fails to properly motivate the contributions by, say, discussing why the rankings induced by the Bayes optimal classifier are relevant, why we may want to revisit and better understand the differences between various OOD methods, or why we may want to consider a binary discriminator method. As another example, Section 2 presents theories and definitions related to when scoring functions are equivalent without explaining to the reader why it is necessary to understand or follow the math. 2) Table 1 actually shows that Outlier Exposure outperforms a Binary Classifier on average on the CIFAR 10 dataset? So only on one of the two datasets do we see the claimed result? 3) Experiments are not that thorough. 3) A binary discriminator requires knowing which data is OOD at training time, and having a large amount of this OOD data. In practice, we may not have access to samples from the OOD dataset to train on, or we may have only a small number of samples. I would appreciate if there was more discussion about this topic. The paper was poorly written and difficult to follow. I recommend that the authors try to make their presentation more clear and concise.<|endoftext|>The paper shows that binary discrimination between in  and out  of distributions is equivalent to several generative model based OOD detection approaches such as likelihood ratios. Moreover, the paper shows that when the binary classifier between in  and out  trained in a shared fashion with a standard classifier for in distribution classes, and using a score which integrates information from p(i|x) and p(y|x, i), the OOD detection performance achieves the state of the art. The proposed method is not completely new, but the purpose of this paper is to identify common objectives as well as the identification of the implicit scoring functions of different OOD detection methods. Also the proposed method is simple and efficient. The connection to the previous work is comprehensively discussed. I found the table results are a bit too busy to follow, with many different methods and scores. Also, the FPR@95%TPR can be very sensitive. I would recommend to report AUROC in the main text. Here is a relevant paper to the idea of training the binary discriminator between in  and out distribution together with a standard classifier on the in distribution in a shared fashion. arXiv preprint arXiv:2104.03829 (2021). https://arxiv.org/abs/2104.03829The paper provides helpful insights to connect methods for OOD detection tasks. The paper can be further polished to make it easy to follow.<|endoftext|>The paper analyzes different OOD detection methods and show that even if the formulation for many OOD methods were different, the binary discrimination is equivalent to those different types of methods when the rankings induced by Bayes optimal classifier are analyzed. "\[10] Liu et al."Energy based Out of distribution Detection, in Neurips 2020. They also claim that training binary discriminator (in dist vs out dist) in a shared fashion along with standard classifier reaches state of the art OOD performance. This uses new dataset i.e.OpenImages as $D^{ood}_{train}$ instead of TinyImages because it was retracted for ethical concerns. al (2020) "Hybrid discriminative generative training via contrastive learning. Have you explored how this train OOD dataset should be chosen? It would have been great if authors could discuss more on this and present how their method/derivation fits on any(not all) of these more recent frameworks/methods. This will help in getting more understanding of OOD methods and building novel ones further. It explains and provides some groundings to why those different methods might be performing similar ways. It includes well motivated experiments, their clean code implementation and results as well as a comprehensive appendix to support the claims. ## WeaknessThe empirical significance and novelty of third contribution seems limited to me since the claim resembles the ones made by Thulasidasan et al.[2021] and Mohseni et al.[2020].I’d request the authors to discuss more on the effect of their advancements(using unlabelled data($x_{r}^{IN}$) with BGC) and empirically discuss/show the significance of the gains. The paper provides a good survey and insights of existing OOD approaches discussing their similarities and differences. While I commend the mathematical derivation  and analysis of multiple existing OOD methods, the novelty of empirical contributions seem marginally significant to me. al.(2020).Contrastive training for improved out of distribution detection.\[8] Tack, Mo et.<|endoftext|>They show that binary discrimination between in  and (different) out distributions is equivalent to several different formulations of the OOD detection problem. They find that, when trained in a shared fashion with a standard classifier, this binarydiscriminator reaches an OOD detection performance similar to that of Outlier Exposure. * I feel that the authors achieve notable progress towards their stated goal (a "better understanding of the key components of different OOD detection methods and to identify the key properties which lead to SOTA OOD detection performance"), particularly with their first and third contributions. Weaknesses:* In my view, the authors do not make particularly clear what the problem their work is aiming to solve is or why it needs to be solved. What s missing from the existing literature, and why is it important to fill in that gap? But the introduction does not explain the need for these contributions. Their stated goal (which I quoted above) comes across as quite vague. It is therefore also difficult for me as a reviewer (with limited expertise in this area) to make a judgement call as to the importance of this paper. The authors present well supported contributions that seem interesting, but do little to explain what problem their contributions solve or why their contributions are needed.
Accept (Oral); rating score: 8; rating score: 8; rating score: 6; Experimental results prove the main conclusion of the paper, but some additional experiments are needed (see above) to further understanding the problem. The authors show that there is a strong correlation between results on open set scenario and the close set scenario (the classic problem in which the model is trained and tested on the same semantic categories). Then, they show that a baseline based on ranking the logits of a model trained on standard cross entropy training can be very competitive with more complex methods when trained with strong data augmentation and other improvements. It is interesting to see that in new and not overfitted datasets, cross entropy baseline seems to perform better than the best method for open set scenario. I see that a more complete evaluation is performed in the appendix E (Table 6), however this is not done for the new datasets (Table 3). Also, in the figure, for the simplest datasets (with performance close to 100) there is not much correlation between open set can close set performance as ARPL does not improve over cross entropy on the close set scenario. The authors propose a clear distinction between open set recognition and out of distribution. It would be more interesting to know if the same conclusions of this paper are valid also for out of distribution problems. In Fig.3 it is interesting to see that ViT seems to have a better generalization to the open set scenario on ImageNet. The paper is well written and introduces interesting results that can change the understanding of the open set recognition.<|endoftext|>This paper makes an observation that good representations for open set detection would be correlated with high closed set accuracy. This paper also proposes new benchmarks for openest detection with fine grain details, which haven’t been studied in the setting of openest detection. The core technical contribution or claim of this paper is that closed set accuracy is important for openest detection. [3],[4] highlight the fact that good representation is all you need for meta learning or out of distribution generalization which though orthogonal to open set detection, informs the research community that ‘quality of representation & performance on closed set’ is useful for auxiliary and relevant tasks of OOD, few shot learning. This paper is well written and has great empirical evaluation methodology and demonstrate SOTA compared to other methods with additional components. This paper proposes additional benchmarks for openset detection leveraging fine grained classification datasets, and this is adds a valuable dimension to openset benchmarks. Weakness:The core technical contribution(s) does not seem convincing to warranty a conference acceptance, especially given previous works making similar conclusions and contributions. This lacks sufficient commentary on why a good closed set accuracy would result in better open set detection. For completeness please consider including CIFAR100 (close set) vs CIFAR10(open set), as the number of closed set classes increase the task of openset detection becomes more challenging, this version of experiment is included in [2] which this paper includes as baseline and [1]. Though empirical evaluation is comprehensive and illustrate performance on large scale benchmarks, given existing prior works novelty of contributions seems rather limited<|endoftext|>Then based on this finding, cross entropy closed set classifier is enhanced with a few recent accuracy improvement methods, then it is transferred to a open set classifier and achieve good performance. And also experimentation is conducted to compare with other SOTA close set classifiers, and that the proposed enhanced OSR classifier based on cross entropy baseline can achieve similar accuracy. Finally a new benchmark dataset is generated. This paper is dealing with the open set recognition problem which is a challenging research problem. Based on some findings about the correlation of closed set recognizer and open set recognizer, this paper found a way to achieve a strong OSR through an enhanced close set recognizer. The weakness of this paper is that there are not more in depth analysis the behind reason for these findings, thus the value of this paper is not that significant. This paper proposed interesting finding about the correlation between closed set classifier and open set classifier, and leveraging this finding, a new SOTA open set classifier is generated from a baseline classifier, which has similar performance as  other SOTA close set classifiers. Overall it is a valuable paper, which is clearly written, and with good experimentation results to support. So overall I will recommend marginal acceptance of this paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper describes a StarGAN based approach to generating synthetic images of various defects based on the underlying training data. The proposed method considers two different types of domains, e.g., the foreground domain as defect types and the background domain as product types, and the derived synthetic images are included in the training data to improve the performance of defect classification. This work is not ready for publication yet.<|endoftext|> This paper introduces Defect Transfer GAN to transfer or generate different types of defects such as stretches or spots (foreground) and apply them onto different product images (background). The method builds upon StarGANv2 and add the cycle/content consistency loss and classification loss between foreground and background. Strength: + The paper addresses an interesting and important application.<|endoftext|>This work applies GAN for data augmentation to enhance defect classification. Concretely, based on StarGAN v2 structure, the proposed framework is able to encode the foreground and the background separately, enabling diverse defect synthesis by either transferring the defect from a reference sample or synthesizing from randomly sampled noises. The results of reference guided defect transfer are promising. **Weaknesses**  My main concern is the technical novelty. This paper just applies it to a new area. The superiority cannot be fully verified.<|endoftext|>The paper prposes a StarGAN based model to distangle the defect foreground, background, transfer the style of foreground and then synthesize the defected image for different products. The work presented in the paper can be regarded as an application of StarGAN v2, while the paper is well writen and easy to follow, the novelty and contribution of methodology is limited, particularly for ICLR 2022. The approach is also appled to augment defect images to improve the performance of defect classification. Much more defects and products need to be tested and evaluated.<|endoftext|>Authors proposed the GAN method that can translate the defect between images. While authors insist that they added few more modules such as background/foreground classifier, however this extension is rather trivial. In the application side there could be, however it was not clearly explained why such style/content separation and bg/fg classification are important for the targeted application. > The technical novelty looks weak: Most of components are similar to the StarGAN v2.
Reject; rating score: 1; rating score: 3; rating score: 3; Researchers are aware of this, and the conclusions of this paper add no value in my opinion. I recommend strongly rejecting the paper. The conclusions of the paper add no value, and the experiments are not performed well.<|endoftext|>Experimental analysis presented in the paper is very weak. This does not support the draft s claim that it works with diverse dataset types.<|endoftext|>Lack experiments. I would like to hear more from the authors on this point. This paper could meet the acceptance requirements with a clear problem formulation, more extensive experimental results with CPSs data, and theoretical analysis.
Reject; rating score: 1; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper studies the problem of explanation for similarity prediction models. Given a pair of inputs (x1, x2), the model to be explained assigns a distance (or similarity) score. The task is then to explain the model prediction on individual inputs. Two methods are proposed. In fact, the qualitative AbE examples raise more questions than they answer, and make me doubtful that the method is really working as intended. In addition, the proposed method of analogy based explanation seems novel. ## Weaknesses: Despite the strengths, I do have serious concerns about the experimental evaluation, which fails to convince me of the quality of the explanation. As a result, I could not understand the author provided explanations for this example, and I do not think it is a good opening example for the same reason.<|endoftext|>Goal: provide local explanations for black box (BB) models that assign similarity scores to two input examples. Approach: Two explanations generated: 1) feature attribution and 2) similar pair of examples that serve as analogies. Approach for 1) is:Approximate the BB model on the instance as if it was a quadratic model of the pair of inputs and learn the weighting matrix A by sampling pairs of points around the input and solving the resulting SDP for the matrix A. The paper presents a novel approach for obtaining explanations from a black box measure. The method appears sound, however, the evaluation is lacking in certain aspects. My recommendation is a borderline reject that can be improved if authors can better argue their evaluation approach.<|endoftext|>The author addresses the problem of post hoc explanation for the black box model. In this paper, the author discusses the task of explanation for two inputs, and the model provides a similarity score as output. 2017.A unified approach to interpreting model predictions. In the proposed method, the author uses feature attributes to explain the similarity between two inputs. Finally, the author proposed an analogy based explanation to select diverse analogous pairs of examples for the same similarity. Strengths:The main reason to accept this paper is empirical results, showing performance on the various methods. Can it be applied to other tasks? However, the paper misses one of the core aspects of machine learning practice: readability and reproducibility of results. 2 (2008): 256 271.<|endoftext|>The authors propose two forms of explanations for such models: feature and analogy based. Cons:* Many important design choices behind the proposed method in sections 4.1 and 4.2 are not well motivated. This makes it difficult to draw a general conclusion in favor of the proposed approach across both types of evaluation methods. Overall, I vote for rejecting the paper. I am willing to improve my current score in case the authors can address points raised in the major concerns section. Does that mean that the problem at hand can be solved with LIME and JSLIME formulation as well? * Can authors provide explanations on the effect of each of five additive components in Equation 2?<|endoftext|>This paper introduces a novel form of explanations for similarity based models. After the problem and the explanations methods are well motivated and introduced, the authors first illustrate them on selected examples. However, the STS dataset s task and the selected examples do not well support the quality of the generated explanations and the benefit of analogies based explanations. I could imagine that the MEP dataset would be more relatable. The results demonstrate the purpose and the benefits of the proposed methods. I m tending to accept this paper as it is well written and provides interesting and novel approaches to explain similarity based methods.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; I enjoyed reading this paper, it studies an important problem and proposes new methods to tackle it. I also have a few questions about the experiments. Let s assume for a moment that the anchor policies are deterministic, and consider the state occupancies of these policies. That is, the anchor policies will be at least as good as any policy along the line. The idea here is to set a constraint on how much a method will satisfy the extrinsic reward and then to maximize diversity under this constraint. Discovering a set of policies for the worst case reward.<|endoftext|>In Section 3.2, the authors claim that the proposed method does not lead to learning sub optimal policies at train time. The analysis on hyper parameter sensitivity is welcome and it is good to see that the method does not require a lot of fine tuning. Additional explanations would be especially welcome as the baselines used in this paper are based on diversity and thus follow a different intuition. This is only true if the learned policy is also deterministic which does not seem to be the case in the proposed set up.<|endoftext|>This paper considers the problem of learning policies in a training environment that adapts fast to a different test environment. The problem is interesting, and the proposed method is simple and apparently easy to tune. While this (hopefully) should not outperform the proposed method, this comparison would be useful to understand how much the continuous space of policies helps the algorithm, over simply just having a diverse set of policies. In the experiments section, it says that the hyperparameters for the comparison methods are tuned based on performance in the training environment.<|endoftext|>The work proposed a method for training robust policies in RL by optimizing convex combinations of different policies, and showed that it achieves good performance against parameter variations in various Mujoco environments. While I do find the observations interesting, I am not convinced that the method is working for the reasons that the authors have formulated. These problems can be hard to see in the Mujoco environments. First of all, the approach is not learning a "subspace" of the parameter space   in the current formulation it is clearly not closed under linear operations.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper develops a practical gradient based hyperparameter optimization method, HyperDistill, that meets the following criteria  a) scalability in hyperparameter dimension and memory constraints,   b) accuracy (hyper gradient update terms do not depend on only the last step of gradient updates)  c) applicability to the online setting. The main difficulty lies in estimating the gradient of the weights with respect to the hyperparameters. Some typos: "longer horizonS" in Section 1, "identitcal" in Section 2, "completely ignoreS" in Section 5The paper is clearly written, the theoretical justification for the paper is well presented and intuitive, and experiments confirm the value of the proposed method. My only criticism is the comparative lack of care with which the distilled dataset is analyzed; this stands out particularly in contrast to how the  rest of the technical difficulties are addressed.<|endoftext|>The paper proposes a novel hyperparameter optimization algorithm in meta learning to overcome previous limitations of only being able to see a longer horizon and not being scalable to high dimensional hyperparameters. Empirically, the authors show the advantages of the proposed approach in several benchmark datasets. * The empirical experiments are rigorous and well supports the claims made in the paper. Overall, I vote for accepting. I believe that the paper is well written, well motivated and the claims made in the paper are well justified empirically.<|endoftext|>This paper proposes a new online hyperparameter optimization algorithm. Applying meta learning for hyperparameter optimization is reasonable and interesting but suffers from the second order gradient computation. Applying knowledge distillation in second order gradient computation is novel. 2, The experiments can be extensive by applying the methods in more datasets, for instance, meta dataset. Experiments show the effectiveness of the method.<|endoftext|>The authors propose a hyperparameter optimization algorithm in meta learning, where parameters w/o being involved in the inner loop optimization are treated as hyperparameters. The proposed method is well motivated, and the paper is clearly written. My major concerns are about the experiments:1. It might be more convincing if the authors can evaluate the performance on more kinds of tasks (e.g., regression tasks). And the authors mention that the proposed method is easy to be applied on traditional hyperparameter optimization (HPO) tasks. The motivation of the proposed method is clear, but more comprehensive experiments are needed to verify the effectiveness of Hyperdistill.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposes an improved version of batch normalization that s intended to work well on small training batch sizes. Experiments are are conducted primarily on the Inception V3 network, with comparisons to BN and batch renorm. 1.The approach is rather ad hoc and not properly motivated. The main contribution is a series of techniques to modify the back propagation. The majority of the experiments are conducted on Inception V3, with the only two baselines being BN and batch renorm. First of all, I find the reliance on the computational graph terminology to be quite unnecessary, and it adds difficulty for the precise understanding of the details. I suggest the authors work on improving the methodology, adding more evaluations and improving the presentation.<|endoftext|>This paper propose a training method to deal with small batch issues of batch normalization. Please follow a formal citation style. I like the idea of this work, but obviously the authors do not elaborate or justify their idea in either theoretical or empirical aspects. I thought this claim was very interesting, and somehow reasonable, so I looked forward to seeing justification on this claim in the following part. But what I have read in section 2.2? Why not just directly demonstrate what method you apply on a neural network only consisting of BN, relu, linear(CNN) modules? The most confusing part is why the experiment results are presented in text descriptions instead of charts or tables.<|endoftext|>The authors propose a two step approach for per sample based normalization that is more accurate (they actually don t prove this) than standard BN. They do this by augmenting the computational graph (in essence the computational graph involves several samples and not just a single sample). Strength: the idea of altering the computational graph is novel, especially to encompass several samples in a single graph where the weights are  shared Weakness: The computational experiments as I read them show that the new approach is subpar. I also do not see a discussion on computational times.<|endoftext|>The paper offers an interpretation of batch normalization as a method for approximating population normalization. The paper includes some interesting insights on normalization. I understand the idea is to perform online normalization in a manner similar to SGD, but it doesn’t seem to offer any practical advantages over the existing batch normalization method. The motivation for the proposed method is weak. Although the overall insights are interesting.
Reject; rating score: 3; rating score: 3; rating score: 6; The authors propose learning control strategies in real time for agent collectives. Specifically, Q learning allows learning how light may be projected onto Volvox algae to maximally reduce their velocity. The paper presents an application of tabular Q learning to a new and interesting environment (DOME). As such I am concerned that the scope of the paper is suitable since the contribution mostly focuses on the environment rather than the learning algorithm itself. Comments that could help improve the paper:  The paper also presents a simulator. While the abstract describes the learning process for a collective, the learning actually works on a per agent level. It would help if this could be clarified earlier on. To strengthen the paper, comparisons to other competitive algorithms and baselines are needed. Consider changing writing from passive to active voiceThe paper describes the application of tabular Q learning to the DOME environment. Competitive baselines, and a comparison to state of the art are lacking as well.<|endoftext|>The given work discusses the use of Q learning to control the motion of a light responsive Volvox agent. The authors motivate the use case well, justifying the case for applying Q learning for this task. * Its (to the best of the reviewers knowledge)a novel application of Reinforcement LearningWeakness:Quality of work *  This is an area for improvement. The level of contribution could be improved. An example would be adding other baselines apart from Q learning. *  Is it possible for the speed to be 0 with continuous illumination? Is that a feasible scenario? The paper provides the application of Tabular Q based learning on a a novel application. However, the contributions, significance and empirical evaluation are limited.<|endoftext|>In this paper, the authors use machine learning to control the velocity/motion of a type of microorganisms, the Volvox algae. Is that the case? How does it change the state space? (2) Given how the authors design the state space, I think there is a more continuous way to formulate the state space: the micro agent s velocity as the state, and on off of light as the action. In this way we will have an alternative formulation. Videos will help the readers, who may not be from this field, to understand better. In summary, the authors apply Q learning to solve a real world control problem for microorganisms.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; It is claimed that two points in the meta model embedding space are nearby if they correspond to neural networks with similar dynamics. This paper introduces a new approach to studying the population of trained neural networks. However, no analysis was provided of the computational resources requires for the proposed method. Also, the results in Figs. Fig.4 is not entirely clear to me, and I am not sure what can be inferred from it.<|endoftext|>This meta model is parametrized by a set of parameters which is common to all the networks, and by other parameters which are network specific (theta). In this way, each network will correspond to a different theta, and one can analyze the global features of the set of networks by unsupervised manifold learning in the  theta space. I think that the approach is smart and potentially interesting, but the empirical evidence used to demonstrate its usefulness is not fully convincing.<|endoftext|>This paper proposes an approach to combine together multiple (pretrained) neural networks into a single model that can simulate each of the considered neural networks, both in terms of output and hidden states. However, it is not presented or evaluated as such. Instead, this work tries to pose it as a way to learn a "model manifold," and in this context the results are rather superficial and lack clear insights or benefits. The idea in this paper is interesting, and seems promising for an ensemble learning method interpolating between (or extrapolating from) a collection of neural network base model to a stronger combined meta model.<|endoftext|>The authors propose an algorithm that takes several neural networks and outputs an embedding for such networks and a meta model. The visualization it provided is meaningful and can be of value to the community. However, I have several concerns about the work at its current state. The paper has several claims on the usefulness of the proposed model. Extrapolation:The authors claim that the meta model can reach a better performance than any of its underlying models by extrapolation. The figures sometimes are far from their mention in the text. This made the paper much harder to follow than it should have.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper proposes a new method to incorporate Spelke s principals of object perception as constraints to improve the performance of an out of the box object detector. This is done via defining a hierarchical generative model which defines "metacognitive" priors over the a set of observations. Through joint inference over these metacognitive priors and new unobserved states, the method outputs better object detections. The authors show improved performance on a synthetic dataset which contains scenes rendered in a virtual environment. Therefore, I feel that this paper, in its current state, is not ready for publication. 2019.The paper presents an interesting way of enforcing consistency constraints in object detection.<|endoftext|>This paper proposes a metacognitive framework to augment object detection models to help improve them without human input. METAGEN is tested using single stage (RetineNet), two stage (FasterR CNN), and transformer (DETR) models of object detection within a dataset gathered using the synthetic ThreeDWorld environment, and is shown to improve test performance both during inference and after stopping inference on both training and new test scenes. I think the motivation behind the paper is strong and is a good fit for ICLR. The idea of being able to take a noisy object detector and learn how to denoise its outputs using a set of sensible priors, without retraining or needing to know the inner workings of the object detector itself, is a simple and attractive one to me. 2.The METAGEN model proposed in Section 3 is clearly explained and seems like a reasonable approach to impose the various desirable metacognitive priors. I think this is an interesting paper that could merit publication but I am concerned that the experimental side is lacking for the reasons mentioned above. I would say it is below the threshold for publication as I read it right now, but I look forward to hear clarifications from the authors and will take into consideration the Q&A with all the reviewers. I m blanking on a good reference that s closest to this work, but it would be nice to see some effort to make a connection here.<|endoftext|>This paper proposes a new METAGAN module which captures the relationship between objects and detections in each time slot, and it refines the detection results by eliminating the false predictions. In addition, the idea of using METAGAN to deal with object detection proposes an entire new perspective to address the detection problem, which is very interesting to me. My main concern to accept this paper is that the experiments are not convincing. The problem setting in this paper is quite different with the other papers for object detection, and the dataset for evaluation is more like a toy dataset, with simple contexts and limited images for training. It will make the proposed framework far more convincing. Springer, Cham, 2020: 431 446. If the author can validate the effectiveness of the proposed method on the challenging public benchmarks, I d like to change the score.<|endoftext|>This article proposes a metacognition model for object detection. The proposed model is trained and tested on a synthetic dataset and the experimental results show an improvement in detection performance compared to standard models. The topic of this article is a very relevant one: How to account for an AI inaccuracy for decision processes? The proposed model does not account for that. Another issue with the article is the very synthetic nature of the experimental setup. Some form of domain adaptation or fine tuning may be required here to compensate for this and make the baselines more realistic. In summary, this is an interesting research topic, but the significance of the article is limited by the rather simple model choices and experimental setup. Because of those choices, it is unclear that the proposed method would generalise to other setups.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; rating score: 8; rating score: 8; This paper develops a meta learning approach for re weighting samples for better adversarial robustness. In particular, they parameterize the weights using an additional module and learn it with the MAML objective. Novelty.** The proposed approach is a direct adaptation of the classical MAML algorithm to adversarial training, which is of limited technical novelty and hardly meets the bar of this venue. ** Unlike previous approach, the proposed BiLAW relies an additional reweighting module in the training stage. Therefore, in the (fully transparent) white box setting, I believe this additional module should be included in the evaluation stage for crafting adaptive attack. I have read the authors’ response to other reviewers, while I cannot agree with that evaluating adaptive attack is “beyond the scope of this paper”. Even if the authors argue that the reweighting module is not included in the test stage, we can still use an independently learned reweighting head for adaptive attack. MNIST/F MNIST is too old for the current research, and small CNNs are also unnecessary for CIFAR 10. On CIFAR 100, the result of BiLAW is **not shown**. Therefore, I recommend rejection.<|endoftext|>The authors borrow the idea from meta learning and design an auxiliary network to learn such weights. The results show their approach can improve robustness against certain attacks. It is interesting to combine MAML with adversarial training and formulate a bi level reweighing adversarial training framework. My biggest concern is that, from Table 2 and 3, using BiLAW alone is not competitive with others when facing more reliable attack methods like AutoAttack, especially TRADES. So when used in combination with TRADES, the gain is more likely to be brought by TRADES rather than BiLAW itself. Combined with the higher robust accuracy against PGD, it suggests that the boundary of the model trained by BiLAW may not significantly change. BiLAW is more likely to make the area near some hard samples (for PGD and its variants only) much sharper, which is virtually harmful to construct a certified robust model. Although this network will not be activated in the inferring stage, the inaccessibility of the auxiliary network to attackers may result in a performance boost. It is natural to care about the running time for BiLAW in comparison with other methods. Overall, considering the weaknesses, not good enough experimental results and the limited novelty, I am inclined to reject the paper unless the authors can provide more interesting discovery and explain a compelling reason why it cannot perform well itself.<|endoftext|>The paper proposes a novel approach (BiLAW) to reweigh training samples with the aim of improving models’ adversarial robustness. A meta learning algorithm (MAML) is used to train the reweighing model. Experiments are conducted in the MNIST, CIFAR 10 and  100 datasets to show that BiLAW outperforms previous robustness and reweighting baselines. Though the concept of reweighing training samples based on margins to improve robustness is not new, the paper presents novel approaches to reweighing by a) using multi class margins and b) meta learning how to reweight the samples from the class margins. The experiments show that BiLAW generally outperforms the current baselines. The main weaknesses of the paper are the lack of 1) theoretical discussion/justification of why the use of proposed multi class margin is better and 2) ablation studies to separately test that the two proposed components a) multi class margin and b) meta learned margin to weight mapping indeed improve the performance as claimed. Comments & Questions:What is the computational cost of BiLAW versus other reweighting baselines? Typo:Testcases > test casesDespite the weaknesses and the limited novelty from previous reweighting algorithms, I am more inclined to accept given the improved empirical results.<|endoftext|>The paper proposes a bilevel optimization procedure for adversarial training. Methodology: The proposed methodology is sound. The authors clearly explained how they adapted from prior works (MAML, Meta Weight Net, etc.). Their motivation for Def 3. and how it can be used as an input to their meta net are well explained. Experiments: The experimental results are good, especially under harder attacks (e.g.AutoAttack). Weakness (novelty, improvements decoupling):Although the proposed methodology is new in adversarial training to my best knowledge, it essentially applies idea of Meta Weight Net in the adversarial training setting. On the surface, this lacks some novelty. I think the paper can benefit from explaining how the performance improvement is decoupled: 1. the bilevel framework, 2. their encoding (Def 3.), say by replacing Def 3 as inputs to the meta net by simply the raw input data or the input data s activations. I cannot see Def 2.1 in the main paper either. I d raise my score if the author can provide more justification on their input encoding (Def 3), or an ablation study.<|endoftext|>The authors propose a bi level adversarial training mechanism to learn sample weights with the goal of training more robust neural networks. In comparison with related work which derive sample weights using various definitions of class margins, the authors parametrize the weights using a neural network. They show the efficacy of their approach by testing the robustness of their model against a variety of attacks. Further, similar bi level optimization has been used for other tasks (MAML, GAN training) for sample reweighing with a fair amount of success. How will the defense fair against an adaptive attack? 2.Sec 4.2 shows the distribution of high and low weighed training samples for BiLAW. It would be great to see similar plots for GAIRAT and MAIL to see if the weights are a property of the dataset or that of the training method. 3.The ablation studies in the appendix should be in the main paper in order to demonstrate the advantages of BiLAW over other methods such as TRADES. The paper would benefit from a more involved discussion of the ablation instead of Sec 4.2. Overall, the paper is well written. I especially like the way the authors have motivated BiLAW, and appreciate the detailed comparisons with related work. The improvements while not significant, do merit publication and further discussion.The experiments are also comprehensive and support all claims.<|endoftext|>The paper applies ideas from meta learning to adversarial training. They compare their robust accuracy against several baselines and demonstrate improved clean and robust accuracy as compared to their baselines. Is this the same or different from what the authors call "Wide Resnet 10 32"? Ultimately, I do not think that the paper absolutely needs to compare to all baselines that achieve higher robust accuracy, because the techniques may be very different, but in my opinion the paper would benefit from clarifying where it fits in the current SOTA. For example, could the SOTA be improved by combining the technique with prior methods? Given the good results this is not an issue though, in my opinion. One fact that I found quite striking in the paper is that Figure 3 a) shows that the learned weights are in fact very close to one another. This makes it all the more surprising that such small differences could really have a significant impact. It would be quite interesting to see if it is really the weights that make the difference or the altered training dynamics, i.e.that the objective is changing throughout training. If the weights really make all the difference, then they should be unaltered under this change. I would recommend using a superscript for one of them. 1  a boldface is missing in the 6th column of Table 2The paper is well motivated and has a solid evaluation showing consistent improvements on a well studied task. Even though the authors could try to push their method s performance by additionally combining it with other baselines, in my opinion the results are significant enough to warrant a publication.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper aims at improving few shot NAS by proposing a gradient guided schema to partition the supernet into sub supernets during the search. I lean toward accepting the paper because of the intuitive splitting rule proposed in the paper and the supportive results. I will consider raising the rating if the authors can address my above mentioned questions regarding the experiments. Moreover, it is shown in the ablation study that the ranking correlation in indeed improved by the proposed splitting criterion (Table 7).<|endoftext|>This paper improves the sub supernet splitting strategy in few shot NAS with a gradient matching (GM) score. I would like to raise my rating if authors properly address my concerns.<|endoftext|>2) How to partion is better? Compared with the method based on exhaustive spltting, the method proposed in this paper can achieve better results. Hopefully the authors can address my concerns in the rebuttal period.<|endoftext|>Also, the reviewer didn t see many around cost comparisons in the main paper, like time and paranum. Also, as the author said, they focused on the splitting criterion, and thus, the reviewer encourages the author to add more comprehensive studies around the graph splitting algorithm. after rebuttal The reviewer thanks the authors  efforts.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 8; The paper first establishes an in expectation first moment generalization error bound forrandomized learning algorithm with on average uniform stability, based on whichit then shows that a properly designed subbagging process leads to near tight highprobability generalization bounds over the randomness of data and algorithm. Itfurther substantializes these generic results to SGD to derive improved high probability generalization bounds for convex or non convex optimization with naturaltime decaying learning rates, which have not been possible to prove with the existing uniform stability results. However clarity can be improved in showing how far paper s results are from lower bounds.<|endoftext|>This paper establishes near optimal high probability bound for randomized algorithms with on average uniform stability. To this end, the authors use a confidence boosting technique via a subbaging process. The authors then apply the derived generalization bound to SGD and the deterministic uniformly stable algorithm. The results reveal that their bounds improve the results of SGD in (Hardt et al., 2016) and the results in (Bousquet et al,.2020) up to a $\log (N)$ term. (2.)Existing generalization bounds of randomized algorithms are often derived in expectation. (3.)I checked the proof, which seems to be solid. When the authors apply it to SGD, it occurs to me that the high probability generalization bounds are derived for a variant of SGD, not the vanilla SGD. Overall, the authors proposed an interesting idea to improve the stability based generalization bound for randomized algorithms.<|endoftext|>To be specific, the authors establish an improved high probability generalization error bounds over the randomness of data and algorithm via confidence boosting method. The main novelty is they establish the in expectation first moment generalization bound with mean (square) uniform stability for the randomized algorithm and then use subbagging technique to obtain the high confidence bounds. In particular, their results can handle time varying stepsize cases when applied to SGD. The ideas are motivated by the confidence boosting/bagging ideas in the literature. In Corollary 1 2 and 3, Consider Algorithm 1 specified to $A_{\text{SGD w}}$ is unclear to me, and $\{A_{\text{SGD w},k}\}$ is not defined. In my understanding, $A_{\text{SGD w}, k^*} (S_{ k^* } )$ is the output of SGD with subbagging process in Algorithm 1, and Algorithm 2 is seems to redundant here. Did I imiss something ? Is this the artifact of the proof in the nonconvex case? Mirror comments: Page 4 paragraph 3, $\gamma_{m,N/K}$ and $\gamma_{m^2,N/K}$ have not been defined before. Page 12, in Lemma 4, $h_i$ should be $g_i$. Overall, the paper is well written and the results are new and interesting.<|endoftext|>The paper uses the framework of confidence boosting to develop a generic procedure for amplifying the success probability in the generalization bound for a uniformly stable algorithm. 3.Finally, the paper shows that though the framework is developed for randomized algorithms it obtains tighter bounds for deterministic algorithms as well. 2.The paper is quite well written. This makes it seem a bit less specialized. Continuing from the previous point, one could argue that the results in the paper are a little bit specialized. 2.It s a bit unsatisfying that the bounds don t hold for SGD itself, but instead for the confidence boosted procedure which Alg. Do the authors believe that this is necessary? Is it possible to show the high probability results for SGD itself? Overall, this is a well written paper which uses interesting techniques and makes decent progress on an interesting problem, and hence I am in favor of acceptance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes a novel algorithm with strong empirical results. Such augmentation provides a natural solution to the quantile crossing issue, which exists for many other quantile based algorithms. The paper is well written and clearly presented. It augments the traditional quantile based algorithms with monotonic rational quadratic splines.<|endoftext|>The experimental results on continuous control environments with noise show better results in most environments. This paper proposes a new neural network design to represent quantile functions for distributional reinforcement learning, based on smooth rational quadratic splines. The method is a natural "upgrade" of NDQFN and thus its novelty is relatively low.<|endoftext|>This paper proposes to use monotonic rational quadratic splines to interpolate an inverse cdf function for distributional RL. **Novelty**: The idea of using monotonic rational quadratic splines to interpolate an inverse cdf function is novel and interesting. The paper makes a novel contribution in terms of using monotonic rational quadratic splines to interpolate an inverse cdf function for distributional RL.<|endoftext|>Originality: SPL DQN provides an alternative way of parameterizing the quantile function with monotonicity in distributional RL. This paper proposes a new variant of QR DQN with ensured monotonicity in the quantile function. While the idea of using rational quadratic splines is interesting, I do find that the proposed method needs to be better motivated:  The benefits of using monotonic rational quadratic splines as quantile approximators need to be better justified.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 10; The goal is to learn a function from x to y, with data coming from a time series (x_t) and (y_t). At the beginning no (y_t) are known, but query can be made in a active learning framework. The experiments speak for themselves. Overall while the idea is simple, and the math are not really satisfying, the experimental part erases most of my concerns. 2/ "More formally, the estimated segment for t_q is Eq.(1)"This is a choice made by the authors, and should be more clearly stated as a choice, by using “we estimate the segment for t_q”. page 4:1/ Eq.(4), what is t? I believe c, w, and s should be updated according to the gradient of the quantity appearing in Eq.(3) where there is a summation over t that make it disappears. 2/ Eq.(5) is really weird, and is based on assumptions that seems highly unrealistic. Similarly, the labels propagation from H_r is not that clear, especially if there is overlaps. 2/ In Algorithm 1, the loss ell and the query strategy should be added as input of the algorithms, as well as the stepsize lambda.<|endoftext|>This paper addresses the label propagation segment estimation problem in time series active learning, and apply plateau function to model temporal coherence. The experimental results show their effectiveness compare with baseline label propagation approaches. Does it make any difference to have a larger b? That is, does the performance of TCLP decrease as the query size increases? The experimental results are fairly good, they demonstrate TCLP s effectiveness in their experiments.<|endoftext|>The paper presents an active learning algorithm for classifying and segmenting timeseries data. When instances are labeled by users, these labels are propagated to [temporally] neighboring instances using a plateau model (if possible) so that the information is shared more efficiently across multiple instances. Pros:1.Good approach to use limited user feedback efficiently. Need some more elaboration on the theory2. It seems that computation time would increase with each round. Is there a way to lower the complexity by taking a sample of the most relevant plateau models?<|endoftext|>This paper develops a framework for time series active learning called Temporal Coherence based Label Propagation (TCLP), that uses the temporal coherence within time series to choose as few samples as possible for domain expert labeling and assigns those labels to temporally close samples. These are presented in a coherent algorithm with pseudocode that is relatively easy to follow. Additionally the empirical results are quite strong. Near the end of the experiment section, the authors mention how performance varies as a function of the parameter T in temperature scaling, and show performance increasing as T increases from 1 to 1.5 to 2.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Weakness:Since this is a theoretical paper I am basing my impression of the paper on its theoretical novelty. There is no novelty here. The Bernstein s inequality for dependent sequence was obtained in Deylon et. al.2009 and so it s not a contribution of this paper. It is not clear to me in which aspect it is generalized from Tao. If the authors could point out how it is more general that would be great. So I am voting for reject.<|endoftext|>is very interesting. I think the paper is well written, clear, and has technical novelty. This can be modeled like SCO by defining, $f_t:  F(.; \xi_t)$, but since there was no restriction on $f_t$ s, the $\xi_t$ s here can have arbitrary dependencies over time. However, I feel that the paper doesn t have many new results and is unable to generalize existing results sufficiently. In fact, it is not even satisfied for the instantaneous loss in the experiments of this paper. It would have been a significant improvement if the results of Agrawal and Duchi were extended to smooth functions with bounded variance (at the optimal), which would cover a much larger class of functions.<|endoftext|>1.Compared with existing works [1,2], the paper considers more general mixing time, that is, algebraic mixing time (fast and slow). However, some weaknesses hurt this paper. The only novel part is about the proof in geometric case but is very easy by following existing proofs in [1,2]. The journal version contains more details. I suggest the authors update the reference in the future version. [4] The generalization ability of online algorithms for dependent dataDue to existing works in this area, I do not think the current version of this paper is beyond the publication bar.<|endoftext|>* Empirical results: it will be interesting to compare the empirical convergence rate of variants of SGD with the theoretical bounds provided by the authors to see the tightness of the bounds in non asymptotic regime. This paper studies the sample complexity of a few variants of SGD in solving optimization problems over dependent data. * The paper is well organized and the message is clearly conveyed: this paper clearly conveys the message of why mini bath SGD and SGD with subsampling can provide better convergence rate compared to SGD for dependent data based on the concrete analysis of sample complexity.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 3; The paper provides a new perspective on the important problem if GAN is able to achieve distributional learning. To the best of my knowledge, it is novel to apply pseudorandom generators theory to study GAN.<|endoftext|>This is a solid work and proves an interesting result. The presentation can be improved, and in general the paper is not easy to read. This makes it hard to evaluate the strength of the bound.<|endoftext|>Overall, I think this is an interesting paper and currently, I tend to accept it if all of the concerns are well addressed. Is it meaningful in practice? The paper is interesting.<|endoftext|>Although there are many technical details, which are relatively hard to follow every bit, it is due to the rigor and complexity of the theory. The theory in the paper utilizes polynomially sized Boolean circuit theory, which is an interesting connection.<|endoftext|>For example, Lemma 2.1 is only used in the Proof of Theorem 3.6. This is an interesting and novel idea. The paper is highly theoretical.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; By analyzing the model behavior of each sample of the validation set duringtraining, they find that over half of the samples of the ImageNet validationset are either trivial or impossible for almost all analyzed models, which theyname "dichotomous data difficulty". They then show that the model agreement isalmost only caused by these trivial and impossible samples, by measuring theagreement over all models with and without the trivial examples. Finally, they show through human trial that humans can with a relatively highaccuracy (~81.36%) predict which samples are easier for neural networks topredict, showing that there is a level of difficulty to the image samples. I think the paper is very well written, and the empirical experiments areconducted thoroughly and without any obvious flaws. The insights are very significant and novel, and I feel this paper is a very important contribution. Unless I have not missed anything very obvious, this submission seems verypolished, significant and novel. Therefore, I recommend 8: accept, good paper.<|endoftext|>The authors describe the remaining 42.5% of images as "difficult", and claim that by focusing on these difficult images, it is possible to see pronounced differences between models. They also find that humans canpredict which images are “trivial” and “impossible” for CNNs at 81.4% accuracy. The authors tackle a timely and important problem. 2.I found the paper to be relatively clearly written and concise. All the authors show is that within this "difficult" set, models make different errors. To me, the claims the authors make about "difficulty" are far less supported and require significant additional work. How can we be confident that these images are "difficult"? Human uncertainty makes classification more robust. In Proceedings of the IEEE International Conference on Computer Vision. Exactly what questions were asked, what did the interface look like, how many participants were there, how were they compensated, etc. Also, why did the authors not ask annotators whether an image would fall into the 3rd "difficult" category? The authors tackle an important topic and perform fascinating experiments, but there is insufficient evidence for the claims they make and, in my view, the claims they could better make were not deeply explored.<|endoftext|>Removing these parts in the training set indeed makes models more different. Finally, the authors state that humans can easily tell what images are “trivial” and what images are “impossible”. One large part is almost trivial for all CNNs and the other small part is almost impossible. 4.The paper also shows that this consistency pattern happens for different datasets, including ImageNet, Cifar, and self constructed Gaussian vectors. It seems to me that a very straightforward hypothesis about these two parts would be that the trivial part is what’s very simple, either highly consistent to what’s in the training set, or the images with very typical object pose in the center of the images; and for the impossible part, it might be the images with ambiguous labels, atypical object pose or position. I think the human test results would support this hypothesis, but I wonder whether the authors could provide more evidence to either prove or disprove this hypothesis. If I understand the experiments correctly, these results are for models trained on ImageNet training set without the trivial or the impossible part and then tested on ImageNet validation set without the two parts. 3.It is also unclear how surprising we should be towards the consistency distribution, is this a result of an exponential distribution of the general “identification” difficulty (most images are simple, then less and less are more difficult)? The authors show the high consistency between the decisions of CNNs trained on the same dataset. It is also shown that the validation images can be split into three parts: the trivial part, the impossible part, and the part that’s between them. However, the current work lacks the test of a naïve hypothesis about these two parts: the trivial part is the easy images, and the impossible part is the difficult images. Some results are also not described clearly. I can only recommend for acceptance with confidence marginally above the threshold, but fixing these points would increase my confidence.<|endoftext|>This paper analyzes the effect of dichotomous dataset difficulty (DDD) on model predictions; it has three key findings. First, it shows that a large fraction of imagenet images are either trivial (most models classify such images correctly) or impossible (most models classify such images correctly) for several state of the art models. Second, it shows that differences between model predictions get pronounced when models are trained on "in between" images only (i.e., in the absence of DDD). Do deep neural networks learn shallow learnable examples first?. The drop in error consistency is quite drastic, so it would be good to have the individual model accuracies reported in the paper as well. **Weaknesses**  Novelty vis a vis prior work. Papers on example difficulty [R1 R4] (and references therein) already show that a non trivial fraction of data points are systematically "easy" and "hard" across different models etc. In this context, DDD is simply a different way to frame previously known results. The purpose of Figure 4 is unclear. Hard examples can also originate from "noisy" datasets that contain data points with label errors (such as imagenet) .The analysis in this paper does not take into account the effect of label errors on example difficulty (the impossible set of points). Overall, I think the weaknesses of this paper outweigh its strengths. While models with similar decision boundaries will make similar errors, it is not necessary that models that make similar errors have similar decision boundaries. While all experiments in the paper serve to identify DDD, it is not clear if this is a dataset "issue" per se. Answering these questions requires additional analysis on what it means for examples to be "trivial" or "impossible". Estimating example difficulty using variance of gradients.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 5; The paper studies the gradient subspace and finds it low rank propery. This observation motivates them to propose a new algorithm that reuses similar past gradients to save communication. They provide a theoretical analysis (with some mistakes) and conduct experiments to validate their method. I think there are more choices to set LBGs. This means we update the new LBG as the average of all LBGs. In this way, each device has an LBG set, which is updated when the space it spans can’t explain the variance of the new coming LBG. However, the current version of this paper is not proper for publication due to the mentioned concerns. 5.The equality of the projections of a vector into two different vectors, though being wrong, is frequently used in the proof of Theorem 1. In the paper, related results include Figure 5 and 6 which investigates the variation of the total number of parameters shared.<|endoftext|>This paper hypothesizes that,  **(H1)** the subspace $S$ spanned by the stochastic gradients while training through SGD is low rank, and  **(H2)** $S$ is well approximated by a subset of the actual stochastic gradients. The analysis is not particularly novel, but it is good to see how $\Delta$ appears in the convergence guarantee. The application to federated learning is definitely novel though and should become the focus of the paper. >We further reveal that LBGM can be extended to distributed training2) The distinction that the authors make between FL and distributed training is slightly confusing (even later in the paper). I don t understand why this is underplayed while discussing this paper in the related works section, where this paper is clubbed with other papers that study the hessian? It would be useful to include this in the appendix and specify how different levels of variance are specified. However, I am not convinced that the two hypotheses **(H1)** and **(H2)** are well corroborated in all the experiments. So this is an underqualified statement as well. If they were it is important to include those for a fair presentation of the work.<|endoftext|>This paper proposed two hypothesis: (1) the space spanned by gradients generated during training is low rank; and (2) the principle components can be approximated by the gradients generated during training. In the algorithm, the clients local gradients will be treated as the principle components (if there are K clients, then we have K principle components). In general, I think the proposed algorithm is quite novel and interesting. The theoretical analysis is also a nice addition to the paper. In experiments, it seems that in most cases, using LBGM will degrade the performance a lot. 2.The algorithm is more suitable for classical distributed learning rather than federated learning. I encourage the authors to change the title and related parts in the paper. This paper proposed a novel idea to reduce communication in FL. However, it seems non trivial to make this algorithm to work with partial client participation.<|endoftext|>The paper presents an efficient federated learning method which leverages the empirical observation that the gradients used in model updates are usually in a low rank subspace. A theoretical analysis of this method is given in the paper and the performance is also verified in various experiments. **Novelty:** The idea of model updates in a low rank subspace is not new. In fact, it has been mentioned in one of the pioneering papers on federated learning:  Konecny et al., Federated learning: Strategies for improving communication efficiency, 2016 (Section 2)The core idea of low rank updates in this paper is the same as the low rank structured update in the above paper by Konecny et al.While the convergence analysis in this paper may be a new result, the proof technique is relatively standard and is similar to other works on delayed/compressed SGD. For example, some detailed discussion on related work may be moved to the beginning of the appendix.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This work studies the problem of efficient pretraining of large scale models for language and vision representations, namely the issue of significant memory requirements for models with billions to trillions of parameters. Authors propose two modifications: first, to reduce the memory load and improve convergence at the initial stage of training, they suggest to train a multilayer model with shared parameters and then unshare them. When combined, the proposed methods allow the authors to train a 10 trillion parameter model on 512 GPUs. Strengths:* Overall, the paper is well written and the contributions of authors are clearly explained. Second, there is no downstream evaluation for M6 10T or comparison with other models of comparable scale: it might have been possible to reimplement some of the prior work as baselines. * Although pretraining with parameter sharing may indeed be more memory efficient, the (seemingly) inherent disadvantage of P2R is that in the second stage of training, that efficiency is lost due to the unlinking. As a result, the users of this methodology will still be forced to train the model of full size at some point, and the gains of P2R are not fully quantified. Although the parameters are shared across layers, the computational requirements of both forward and backward passes should be approximately the same.<|endoftext|>The proposed method first trains a model with weights shared across layers. In this way, the model appears to be smaller and it can fit into fewer GPUs. At some point, the authors delink the weights, and continue training the model in the conventional way. Thus, the main point should be the 10 trillion parameter model uses fewer GPU hours to get the same quality. 4.The sharing delinking paradigm is very similar to the sharing and unsharing method proposed in a previous ICLR submission: https://openreview.net/forum?id jz7tDvX6XYR . Overall, it appears to me that the experiments on very large models are not ready, given the lack of a reasonably sized dataset or downstream evaluations. Thus, it is not clear how impactful the proposed method can be.<|endoftext|>The paper further proposes Granular CPU offloading which is to offload some but not all model parameters to CPU memory to reduce GPU memory consumption. P2R seems to be a promising approach that aligns with the intuition that the structure of the model (and data) for effective learning could be dynamic throughout the training process. Does not demonstrate P2R for M6 10T, the 10T parameter model, since no downstream task results are presented for this model in the evaluation section. Similarly, the claim of 10 days pre training remains unsupported by the draft. Why have all the layers share 1 set of parameters, which not 2 or 4, or something fewer than number of layers? Although the problem is important and the proposed P2R approach is promising, the submission does not demonstrate the effectiveness of the proposal on the problem.<|endoftext|>This paper illustrates the training difficulty of extreme scale models (with a special focus on memory requirement) and provides a simple but effective solution called "Pseudo to Real", where in the "pseudo" stage the weights of the repeated layers are shared, and in the "real" stage the weights are delinked. 2.The proposed P2R method is easy to implement and can be adapted to different models (as long as there is a decent number of repeated layers, whose weights can be shared). What leads to the acceleration? There is a large volume of works on fast training of deep learning models, and the notion of "weight sharing" was also explored previously. In particular, I think it would be good to have experiments covering: a) various models. b) various downstream tasks. The M6 10T experiments warrant scalability. Does the proposed method work for them? I feel this claim is not fully supported as the M6 10T does not converge yet.<|endoftext|>The paper proposed an interesting strategy to reduce the training time for large scale language models consisting of stacking layers with identical structures. Users first trained the models with shared parameters across the layers, then relax the tie constraints so that parameters at different layers are updated differently. The paper showed some empirical evidence that the proposed strategy converged faster given a limited training time budget and demonstrated the feasibility of training a 10T model. However, it s still worth a try for practitioners in the model pretraining community. Weakness or areas of improvements. How are the downstream task performance (few shot) changed with the compute budget? pplx & one shot downstream performance. I disagreed with the author s conclusion that the worse performance is not related to the training strategy.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 3; This paper proposes a novel defense against model extraction attacks. The proposed approach slows information leakage by asking all users to answer a puzzle before receiving the response to their query (proof of work).<|endoftext|>The paper discusses ML model extraction attacks while using APIs for accessing them on thepublic networks. This paper proposed a method for dissuadingthe attacker by increasing the cost of the attack. The authors should make the privacy scoring module more clear and add more discussion about identifying malicious nodes in continuous requests<|endoftext|>This paper proposes a novel defense to prevent model stealing by requiring users to solve proof of work puzzles. This paper proposes a defense against model extraction attacks by forcing users to do a proof of work (PoW) puzzle before they receive the labels from the victim model. Thirdly, the authors create two models.<|endoftext|>I think focusing on this can lead to a better paper. Strengths  S1: This paper proposes a novel defense focusing on delaying the model stealing process. The main idea proposed in the paper against such a strong attack looks interesting, but the key technique used here is the proof of work.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper makes use of **offline algorithms** (i.e.algorithms that can view entire time series) to produce outputs which are used to train an **online algorithm** (i.e.an algorithm that can only view past values of a time series). I think the paper could be made substantially better in one of two ways:1) The authors clarify in what sense the online algorithms approximate the offline algorithms. They apply this method to synthetic and real world time series data (historical stock market data), and report the classification accurately of each of the multi task prediction problems. **Strengths**+ The paper describes their method, experimental setup, and results very clearly. + The paper presents an interesting research direction, using knowledge from offline algorithms to improve performance of online algorithms via learning. **Weaknesses**+ The primary weakness is the main claim seem incorrect. As a result it is not clear to me in what sense these online algorithms approximate the offline algorithms. + Because the primary claim is not clear, it s not clear how to evaluate the proposed method, or what baselines to compare to. + Then the main evaluation metric should be time series prediction. + It would be good to report the performance of comparable ML time series prediction algorithms trained on the same data, and with similar architectures. It would be good to see this demonstrated experimentally. While the ideas presented in this work could be very impactful, as the paper is currently written, its main claim seems incorrect which is grounds for rejection.<|endoftext|>The paper presents a novel method for approximating offline time series algorithms in an online setting. An approximate online algorithm is obtained by training a multi task classification neural network to solve these. I think the idea could be of interest to the community. That said, the paper does not provide any way to evaluate the significance of the proposed result, as there are no empirical (or theoretical) comparisons to any other methods. The proposed approach seems interesting, and I encourage the authors to resubmit after incorporating a proper evaluation by comparing to other methods on established datasets and addressing some of the other comments above (in particular the dataset issues). The paper presents two datasets (a synthetic toy dataset and a constructed dataset of historical stock market data), neither of which seem to have been used in the literature before, and trains the proposed method on these datasets but compares to no other methods. In future revisions of the paper, the authors should compare to other algorithms in this same space. A reasonable place to start is with the works discussed in the related work section. I urge the authors to also perform ablations on the method. Regarding the claim of meeting or exceeding performance on ML based stock prediction systems, there is no evidence given in this paper for this claim so it is unsubstantiated.<|endoftext|>This work claims to propose a general methodology for approximating offline algorithms in online settings, in contrast to previous methods only for particular cases. To achieve this, the author prosed a multi tasks based method to learn from the datasets created by the offline algorithms. Real world examples are discussed in the introduction and conclusion, and help to further understand the motivation. 2.The proposed approach is novel to my knowledge. I admire the idea to capture the behavior structure by multi task learning model, which is interesting to create datasets using offline algorithm for training the online counterpart. Why no baselines are presented in the experiment part. I am not an expert in this field, so I am not entirely convinced that it needs any comparison of other benchmarks. 2.Is there any theoretical guarantees or insights behind the design? 3.I personally think that the paper writing can be further enhanced. 2) Although the authors claim that the proposed method outperform the SOTA, however, the performance of the SOTA model is not present in the Table. Minor: ``We review this limitation more thoroughly in Section ? I admire the motivation, idea, and possible impact of this paper. However, I am not entirely convinced that the experimental results are convincing enough. I would like to update the score after interacting with the authors and other reviewers.<|endoftext|>That is, it tries to approximate the behavior of this offline algorithm in a setting where at time t the algorithm only has access to the input until t (whereas in the offline algorithm the algorithm can lookahead and optimize). They propose a MTL algorithm and use simulations and real world stock market data to study the effects of their approach. Strengths:+ A novel formulation and research topic. Once figured out the proposed algorithm itself is standard multi task learning. + This paper contributes to the now growing line of work on bridging classical algorithms with machine learning. The typical direction has been to use the ML model as hints to improve the online/offline algorithm. On the other hand here, the online to offline algorithm is bridged via a machine learning task. + For the most part the paper is clear and well written. Weakness  The first main weakness I find in this paper is that, it does not sufficiently motivate the problem well. In particular, the online problem and it being posed as a MTL seems very abstract to the reader. In particular, how does one interpret the class prediction for a window? May be elaborating this on a toy/standard offline algorithm before making it abstract would help the reader a great bit. Related to above, the formulation makes it seem like this applies to any offline algorithm. But it really only applies to offline algorithms that work on time segmented data.<|endoftext|>The work presents several experiments using both synthetic and real (stock market) data. The following are some questions/concerns:1. From my understanding, the ultimate goal of the whole paper is to approximate the behavior of offline algorithms in real time, as opposed to directly predicting the ground truth evolution of the time series. This seems to me that the proposed framework’s performance is primarily driven by how well the offline algorithm can fit the historical data. The paper seems to be lacking detailed discussions for this tradeoff, or, on a related note, for how one should choose the "optimal" number of decisions within a structure. 3.I am confused about the occurrence moments of predicted future actions (since the proposed algorithm is predicting X actions ahead, instead of X moment ahead). I might have missed related discussions in the paper, and it would be great if the authors can add some more emphasis. 4.I find the discussions in Section 2 General Schema quite difficult to digest at first read, and not until I went through the entire paper did I better understand how the multi task learning framework works. ), introducing the methodological framework within the context of a simple concrete example (e.g.a simplified version of the stock market example with some dummy offline algorithm) would improve the overall clarity of this section.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This paper addressed the audio visual navigation task in the environment of sound attacker. The authors formulated the problem as zero sum game between sound attacker and the agent, and provide a reasonable proof to their formulation and solution. 3.Is this work the first audio visual navigation method with attackers? The environment with the attacker is practical and interesting. The authors well formulated the action of the agent and attacker. BTW, I have some questions to convince the efficacy of the proposed method.<|endoftext|>The authors address the audio visual navigation task in this paper. Different from them, this work explores an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero sum game with the agent. Specifically, the attacker can move and change the volume and sound category to fool the agent while the agent tries to defend against the attack and navigate to the goal under the intervention. Strengths:+ The motivation is clear and the proposed sound adversarial audio visual navigation is interesting to me. + A joint training paradigm for the agent and the attacker is proposed. + Experimental results on Replica and Matterport3D can validate the superiority of the proposed method over recent approaches. It seems that the attacks are different when attacking different methods during evaluation. Overall, this is an interesting paper to explore audio visual navigation in more complex environments and the proposed sound adversarial audio visual navigation is well motivated. But, I do have some concerns about the method and results.<|endoftext|>The paper proposes an interference robust training method for audio visual navigation. Unlike existing approaches that focus on clean environments, the system is trained in simulated acoustically complex environments. A single source adversarial attacker is introduced, which determines position, noise type and volume that would make the agent to suffer most. This is claimed to improve robustness to random attacks in AVN. Results are compared to a small selection of previous works. The results are better than baselines, but the paper would benefit from more ablations.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; The paper suggests using neural networks to solve the nested optimisation problem of designing a disclosure scheme in persuasion schemes. It arguable whether a receiver, that needs to learn the commitment strategy of the sender and is aware of the sender s parallel adaptation, will miss this opportunity to manipulate the outcome. In fact, such manipulation is a serious concern for adaptive senders. In other words, if the receiver s model is in any way (near) stable, then the learned commitment strategy of the sender can potentially generate higher payoffs. Alas, this is not what Table 1 and Table 2 of the current paper show. If the overall approach of using learning to supplant unknown elements was novel   would be a publishable paper   however, there s a lot of previous work where this has been used before, but authors missed it.<|endoftext|>The authors presented an algorithm, which uses a neural network in the loop, provided theoretical analysis, and presented empirical results to prove the performance of their approach. The paper starts with a good motivation to remedy practical shortcomings of the Bayesian persuasion model, but the results presented fall largely short of getting there. The learning algorithm also has several shortcomings on its own. A paper with a good motivation but falls short in fulfilling the proposed objective. The contribution is not significant.<|endoftext|>The paper studies how to relax certain stringent assumptions made in a Bayesian persuasion framework. In particular, the paper looks to relax assumptions where the sender knows the receiver’s utility function and the receiver’s behavior is completely rational. The authors also claim that the proposed framework works on scenario where the receiver does not know the sender’s messaging scheme in advance and needs to learn to optimize his (receiver’s) own objective. The paper also provides some experiments. 2.In the paper, the authors mention in several places that the receiver is not rational. 5.“Note that this includes the no regret learning algorithms in the contextual bandit setting. Post Rebuttal  \Thank the authors for the feedback.<|endoftext|>The sender s and the receiver s strategies are modeled using neural networks, and the authors propose using a version of alternative training to simultaneously optimize the sender s strategy and predict the receiver s response. The authors analyze some theoretical properties of the approach in an idealized setting, and conduct experiments on synthetic data to evaluate the performance of their approach. When the receiver only observes individual messages (and therefore needs to learn the best response), the proposed approach significantly outperform baseline methods (which are not specifically designed for this task). ## StrengthsThe problem of Bayesian persuasion is of great theoretical and practical importance. (The model in Section 3.2 can actually model (at least some) irrational behavior so that s fine.) Overall I think the paper proposes a novel differentiable approach to Bayesian persuasion, which is then validated experimentally.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper also pointed out two limitations of prioritized experience replay, which are outdated priorities and insufficient coverage of state space, and the author proposed to use SGLD to solve the limitations. Experiments show that the proposed method leads to a good coverage of state space and improve the return of the training algorithm. The idea in the paper is interesting. However, I can t follow some discussions and am not convinced by the arguments from the author. There are many states that our current policy doesn t access, so why should we care about the TD loss for these states? 5.The importance ratio, which is an important aspect of PER, is not discussed in this paper. 6.I don t see the point of being model based. 7.In continuous control tasks, how is the max in Equation 3 computed?<|endoftext|>Some empirical validation shows that the algorithm performs well on some toy control tasks. The paper proposes to use prioritized sampling in conjunction with a model based RL algorithm and attempts to solve issues with PER using up to date priorities using SGLD and using a dynamics model to augment data to cover the space. Theorems 1 and 2 apply only to the supervised learning setting or FQI, but that is not what we do in practice. To me, this is not clear   and I think the answer is no. I don t buy the algorithm proposed. it might work well on the toy domains in the experiments, but may not still be general. The results on continuous control tasks are not convincing. I feel like the paper does not indicate (1) why we should build on PER (2) in my opinion, the algorithm is flawed and may not scale otherwise to problems where dynamics models are inaccurate (3) the empirical results are not convincing. So, I am going for a reject.<|endoftext|>The main contribution of the paper consists in using a Langevin dynamic in order to simulate the distribution of TD errors over states, in order to build minibatches in model based reinforcement learning. on Figure 3,  between PER and the distribution according to a Langevin dynamic is quite unfair. This second claim takes the form of an algorithm, building on previous work (HC Dyna) and could be a significant contribution to the community if it were better discussed. Since PER only updates TD errors in states selected in minibatches, as the authors point out, the vast majority of priorities is outdated and thus these figures are not surprising. I ll agree with the first and argue that the second has nothing to do with PER but rather with online RL in general. What s the rationale? After that, claiming that it is better because it crashes less is a very anthropomorphic interpretation whose interest I fail to see. this explains why in some (not so rare) occasions, PER does not improve on DQN. The idea of using a Langevin dynamic to draw states according to their TD error seems like an interesting basis to me but the current paper needs more work to be accepted for publication. For instance, if $v(s) \max_a Q(s,a;\theta_t)$ then the TD error is actually a TD error for the Bellman equation defined on $v$ functions.<|endoftext|>This paper proposes an alternative method for performing Prioritized Experience Replay (PER), which avoids issues of inadequate state coverage and staleness in priority scores. ### Strengths  This paper is well written. The experimental results are presented in a simple and digestable manner. The experimental setting is diverse: It contains both a gridworld environment, standard continuous control Gym tasks, and a driving task based on driving around a roundabout. In sum, their problem motivation, method motivation and description, and experimental results make the case for Dyna TD quite convincing. A key missing baseline in their RL experiments is Full PrioritizedER. This baseline is required to make sure that Dyna TD is working for the hypothesized reasons. In contrast, their method seems more accurately described as a form of PER, e.g.Langevin Dynamics PER. Given the similarity of this work to Pan et al, 2020, which introduced HC Dyna and Dyna Frequency, I believe having more in depth analysis that highlights why the new method, Dyna TD works better than Dyna Value and Dyna Frequency, is important for making this paper a more substantial, standalone contribution. The authors should make clear in the paper that their method, Dyna TD, only applies to continuous state space environments. The paper provides strong motivation and empirical results supporting their new method Dyna TD. However, given their method is a simple extension of HC Dyna and Dyna Frequency from Pan et al, 2020, where they replace the hill climbing objective with a TD error objective, it seems important to include more analysis comparing and contrasting TD Dyna to HC Dyna, as well as a deeper understanding of Dyna TD s learning dynamics. For these reasons, while I find the method itself promising, I recommend this paper as being marginally below the acceptance threshold.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper suggests that these methods contain two limitations   1. by ranking the neurons on a linguistic task, the methods conflate between ranking quality and the probe s classification quality; 2. the probing methods do not take into account whether the individual neurons are at all used by the model in the downstream linguistic tasks. Finally, the paper presents a new probing method, which does not rely on training a classifier, and shows that this simple method is able to discern among the neurons better than the preceding two methods in the literature. Perhaps the authors indicate it is the mean of the words that does not possess the attribute/label information? This suggests a capacity issue.<|endoftext|>This paper responds to several recent works focused on identifying important individual neurons for particular classifying tasks. They consider 2 existing methods which rely on an external probe to rank the neurons in a network. They also introduce a method that does not rely on a probe, instead ranking neurons according to the difference between their values across labels. The major problem in this paper is that the decisions made in evaluation and ranking are not entirely well justified. Minor:  I would have liked to see an explanation of the tasks that were used in section 2.2.<|endoftext|>This paper identifies two pitfalls of existing methods for ranking individual neuron contributions to certain linguistic attributes, and shows empirically that these pitfalls indeed exist. Additionally, it proposes a new ranking method free of these pitfalls, and shows its effectiveness.<|endoftext|>This paper describes two issues with existing probing methods within the NLP interpretability space and proposes an alternative approach which does not exhibit the same flaws. For example, the conflation between probe quality and ranking quality was not clear to me in the abstract/introduction, and perhaps an illustrative example or figure would help clarify this point.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper provides a novel explanation of benign overfitting in wide neural networks by introducing and studying a mechanism called representation mitosis. The key idea is that if the readout layer of the properly trained network is wide enough, then its neurons could split into groups (clones) that carry identical information, and differ from each other by a statistically independent noise (this mimics what happen in mitosis, thus the name for the mechanism). Empirical results are provided to demonstrate the mechanism. Strengths:   Introduce a novel, interesting phenomenon to understand feature learning in properly trained wide neural networks  Sufficient empirical evidences to support the claims  Overall a well written paperWeaknesses:   The conditions to achieve representation mitosis are not pinned down precisely. There are some heuristic claims saying that clones appear only in *well trained* regularized networks and do not appear for the experiments on ImageNet. The more interesting question is, given (some assumptions/conditions for) a dataset, a model architecture and a training algorithm, when and how precisely can the mitosis be reached? What if we do not choose it randomly but choose it according to some deterministic procedure instead? The mechanism of representation mitosis seems to be interesting and is worth exploring. I am inclined to go with a weak reject for now.<|endoftext|>**Strength**: this paper identifies an interesting phenomenon in deep neural networks and studies from various aspects. **Weakness**: the empirical evidences are a bit misaligned with the claims, and it is unclear how this observations could be used. The mitosis phenomenon is described as formation of "clones" of neurons. From this observation, it is unclear if there are one to one correspondence of copies of neurons, or it s simply the same set of information are encoded in the subspace distributedly, i.e.a (linear) combination of a subset of neurons corresponds to another subset of neurons, but there is no neuron level correspondence. It would be great if the paper could empirically identify the neurons and their "clones" to more directly support the claim of representation mitosis. 2.It is unclear what is special about the last hidden layer. 3.I find it very interesting that the representation mitosis requires using state of the art training setup and continued training after training error reaches zero. If the paper could dig deeper into those situations and identify what happens in different scenarios and the underlying reason for the discrepancy, then that would be a much more interesting paper. 4.The mitosis phenomenon was not reproduced on ImageNet. Since mitosis analysis can be applied to those pre trained models without re training. I realized some of the experiments (ImageNet) cannot be relatively cheaply done and added to the paper after reading the rebuttal. This paper identifies a potential interesting phenomenon in deep neural networks.<|endoftext|>The paper shows empirically that under certain conditions (overtraining, large width in all layers, data augmentation and regularization) neural networks (NNs) tend to learn redundant representations in the last layer. 1.Page 8: "much higher than what observed"   "we have observed"? The authors put forward some preliminary interpretation of the effect and suggest this redundancy effect could be causally responsible for the phenomenon of generalization of wide networks improving with width. The presented mitosis effect is interesting and is clearly demonstrated in certain settings. 1.Top of page 6: *"In the previous paragraphs we set forth evidence in support of thehypothesis that large chunks of the final representation of wide DNNs behave approximately like anensemble of independent measures of the full feature space. My original conclusion remains similar, but improved clarity makes it a stronger submission. In the rebuttal the authors have updated the condition set to 100% training set fit + "optimal" [what is optimal?] 1.Equation (1): what exactly are $\bf{x}$s? etc).# Original ReviewThe paper presents an interesting empirical finding, but in my opinion does not explain very well why mitosis occurs, or what are the implications of it, which is why I am leaning to reject it at this time. Further, specific ensemble sizes should be specified in the appendix. To make claims about mitosis being responsible for improved generalization with width you would need to run many more experiments, comparing generalization scaling of networks with and without mitosis. If "error infinity" is a single constant for all points on the plot, this is less important. 1.Start of page 2: *"The decay rate of −1/2 in particular implies that in this regime chunks of $w_c$ neurons can be thought as statistically independent estimators of the same features of the data, differing only by a small, uncorrelated noise"*. It would be very interesting to ablate this further, and identify which is responsible for it exactly, especially since these are two very different techniques. 1.End of same paragraph: *"Furthermore, a decay with rate −1/2 suggests that the final representation of the wide networks can be thought of as a collection of statistically independent estimates of a finite set of data features relevant for classification.<|endoftext|>This paper investigates the overparameterization problem in deep learning through a set of experiments that study the final layer hidden representations of wide and deep networks. I still am concerned about how general the mitosis effect is, since it doesn t occur in simpler models. However, with the additional clarity, I have increased my score to be at the acceptance level. I appreciate the variety of architectures and vision datasets considered, and I further appreciate the candor in Section 3.1 on Limitations. I also think that the analysis of reconstructing the wide representation from the smaller subset is a nice way of making the point about features being cloned and the information being present in the smaller subset. The first weakness is the necessity of using heavy regularization and complicated training procedures. Thus, I conclude that the "cloning" effect observed by these authors cannot be the main explanation for the way DNNs appear to defy the classical bias variance trade off. Thus, I find their claim a little hard to believe. I think this is a nice paper that demonstrates an interesting empirical effect and proposes a promising connection to the behavior of overparameterized deep neural networks. In particular, I am bothered by the fact that the effect appears only after training "using state of the art procedures" and is absent for complicated datasets (the example in the paper being ImageNet). Overall, I think this paper makes a nice set of observations and performs careful scientific experiments.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Similar ideas has been explored in existing works. The paper assumes the data are noisy and try to uncover a Koopman model with noisy data. I recommend acceptance of this paper.<|endoftext|>This paper introduces a new control framework that is based on Koopman theory. The results highlight the advantages of the approach. The proposed method is simple and easy to code, yet it achieves impressive results on challenging benchmark control tasks. Specifically, the following references are missing and should be added:1. Why SAC is not in Fig.2?Similarly, why MLP is not in Figs. The model is backed by control theory and has a guaranteed stability.<|endoftext|>Of course, this is never the case in practice, as only prediction error bounds on the parameters $(A,B)$ can be obtained, and these are distribution dependent: i.e., once we switch from the exploratory policy used to identify the parameters (such as random noise, as was used in the experiments) to the closed loop control policy, we have no guarantees that equations (6) and (10) are valid. + The paper is well written and very clearWeaknesses  The theoretical results implicitly assume that the learned Koopman embedding (encoded in equation (6)) is exact in the absence of noise across the entire state space. This is easily seen by setting $w_t 0$ in equation (10).<|endoftext|>However, there are not enough experiment varieties, and the results are close to the SAC baseline (not by large margin). In this paper, authors propose  "Deep Stochastic Koopman Operator" which can handle uncertainties in the system dynamics. The  main  weaknesses of the paper includes:(1) Can use more experiments to show a more convincing picture.
Reject; rating score: 3; rating score: 3; rating score: 5; This paper proposes a novel method incorporating decoded foreground attention and a new training scheme with encoder and decode in a loop to reconstruct image stimuli from fMRI data. Strength:The encoder decoder loop and use of foreground is relatively novel. Weakness: It is not very convincing to me that the loop enc dec model and the use of foreground attention is able to reconstruct stimuli images from brain responses better than past attempts.<|endoftext|>The solution is based on an end to end encoder decoder model under the guidance of Foreground attention to enhance the perceptual quality of reconstructed images. * Using the Loop Enc Dec method to reconstruct natural images from fMRI is not new to this field. * Cognitive insights are missing in this paper. I do not see proper evaluation metrics that are used in previous methods for the model evaluation and also not tested on other existing fMRI datasets.<|endoftext|>They also proposed a enc dec training strategy called Loop Enc Dec, which is guided by the F attention, to successfully reconstruct the visual images from the fMRI data. A higher score based on pairwise SSIM is achieved compared to previous works. My main concern is that, (if I understand the training process correctly) the label information used for training the attention decoder is from another attention map generator (the F attention labeling process), which produces artificial attention map. Fitting the F attention model to the artifical attention map does not make much sense. 3) The authors should make more clearer of the contribution of each part in their proposed model.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; In this work, the authors use evolutionary strategies to train recurrent neural networks with Hebbian plasticity rules. They test the system on two tasks, sequence prediction and a simple RL tasks that involve robot navigation. For the problems presented in this paper, the proposed approach outperforms most methods used in the comparison. How did the approach perform there? That would be a more convincing demonstration of the power of RNNs+plasticity rules. It would be great to show trajectories before and after learning. An interesting paper but currently the approach would need to be tested on more complex problems to more fully demonstrate the advantages of using an RNN+plasticity architecture.<|endoftext|>The authors propose a method, EPRNN (Evolutionary Plastic Recurrent Neural Networks) that uses Evolution Strategies to learn a Hebbian learning rule for a recurrent neural network. Weaknesses:  The proposed method is not new (the combination with recurrent neural networks may be, but it is still of very limited novelty). The language of the paper has many mistakes and is a bit informal in some parts ("researchers are increasingly obsessed", etc).<|endoftext|>The authors put together their own flavor of meta learning using off the shelf plasticity learning, evolutionary computation and recurrent network. The proposed approach particularly aims at joining plasticity learning, evolutionary computation and recurrent neural networks (more trending topics), in a blend yet unpublished. The proposed tasks are incredibly simple. I would like to start by pointing out a few words or sentences taken from the paper that the authors should necessarily address   just find them on the paper, I am confident the improvement can be evinced easily from the context. Also these are all requirements. Once again the proposed structure, identical to the previous but for adding one extra 64 neurons fully connected layer, is entirely out of scale for the problem, and unmotivated in the text. Nor in the rest of the paper. Reference to past work is incomplete (if not even biased) from a ML perspective. As a consequence the arguments on this paper are often unnecessarily complex or simply incorrect, again from the perspective of ML literature.<|endoftext|>The authors applied evolutionary algorithms to meta learn plasticity rules for recurrent neural networks. They show this approach performs better than alternative meta learning approaches on two artificial tasks (sequence prediction and wheeled robot navigation). Weakness:In my opinion, the main weakness of this work is the lack of intellectual insights or impressive empirical results. Given that there are quite a few papers in this area that involve two of the above elements (evolutionary algorithm, plasticity rules, RNNs), it is not too hard to combine the three.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper tries to generate novel object captions that meantime satisfy three aspects: fluency, fidelity, and adequacy. The major concern is about the significance of the motivation and the applied solution is not very economic by using two large scale pre training models to solve a small task. However, this method exploits two large scale models BERT and CLIP to solve this small task, which is not an economic solution.<|endoftext|>The papers presents an approach for the task of novel object captioning (NOC). Overall, I concur with the key insight from the paper which is that models for NOC should tackle fluency, fidelity and adequacy. The model utilizes BERT to sample different variations of the captions by substituting random words from the caption. This would form a fair comparison.<|endoftext|>This paper proposes a framework that combines masked language models (BERT) and image text embedding models (CLIP) for novel object captioning. The proposed model significantly improves the fluency, fidelity, and adequacy of the generated images. This paper proposed a strong baseline of novel object captioning via combining BERT and CLIP models, however, the novelty is limited.<|endoftext|>The paper proposes to improve novel object captioning i.e.describing objects/contents that are not seen during training. ### Pros:  Leveraging prior visual linguistic knowledge from pre trained BERT and CLIP models for the task of novel object captioning. They show qualitative and quantitative improvements on CiDER and SPICE scores compared to previous work and propose to measure the fluency, fidelity and adequacy of the generated captions for a more comprehensive evaluation of novel object captioning. Is this observed?
Reject; rating score: 10; rating score: 3; rating score: 5; rating score: 6; The theoretical and empirical analyses are satisfying. The improvement scheme is new (although it can be related to other approaches) and sheds new light on policy gradient optimization. The authors considered only low dimensional problems in the empirical evaluation, raising the question of whether the algorithm could be applied to more complex domains. $$This approach suffers from high variance. It is interesting to see that the $\min_q Var[p(x)/q(x)f(x)]$ leads to a _policy improvement_. In fact, the authors of POIS provided both a parameter based POIS and an action based POIS. I think that this is actually the most unclear point of the paper. 4.However, the finding $q^*$ is often not possible.<|endoftext|>This paper proposes a novel policy optimization algorithm called POPE that is based off of ideas about how to use importance sampling for variance reduction from the Monte Carlo estimation community. 5.I see a few problems with the experiments in the paper:   1. The paper raises an interesting connection between the two types of importance sampling. 3.The batch size experiments don t seem to have very clear motivation. The paper emphasizes some experiments using very small batch sizes, but it is not clear why the different between batch sizes of ~10 and ~50 matters very much. There are major issues with clarity that make it unclear how the algorithm is implemented at all and I am wary of a few parts of the experimental setup too. However, in practice the number of iterations of the inner loop is set to $ J 1$. Why might it be beneficial?<|endoftext|>The paper analyzes the connection between searching for an optimal behavior policy that minimizes variance, and policy improvement in RL. 6.Trajectory based formulation of policy optimization is easier for mathematical treatment but I would argue it has limited application in large scale practices. This leads me to wonder   is there a way to extend such formulations to step wise optimization algorithms (PPO, TRPO, IMPALA, MPO...), in which case such ideas could be made more practically appealing. 7.All toy examples seem to have a natural non negative return function. However, this paper digs into the technique of finding variance minimization behavior distribution of IS, and establishes a few interesting & novel results related to policy optimization. 1.Trajectory based formulations of policy optimization is not practical when horizons are long. I wonder how this can be implemented in practice, when the return functions are negative.<|endoftext|>The starting point of the paper is the difference of using IS in different communities, in RL community, IS is served as a **passive** tool but in Monte Carlo simulation community the behavior policy of IS can be **actively** picked and learned to reduce the variance. The minimum variance behavior distribution can also be served as an improved policy compared with the original one. Although there are several points need to be addressed by the authors, I feel the theoretical contribution is strong enough to recommend at least a borderline acceptance at this point. 2.It seems to me that in the experimental session you fix $\alpha   2$. How will $\alpha$ affect the robustness of batch sizes. 4.Most of the analysis in your algorithm is not specific to RL.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The paper tests the performances of OVD Explorer and some of its competitors on some common environments. Here are some of my questions and comments. Can the authors comment on the computational complexity of OVD Explorer? I hope these proofs could be more readable and verifiable. 5.It would be great that some theoretical guarantee (e.g.regret bound, sample complexity) can be showed for OVD Explorer6. There is no need to introduce the abbreviation OVD several times in the main text. This is an interesting paper, but it seems that some improvements are needed.<|endoftext|>The paper presents a new exploration method and shows improved performance compared to state of the art methods. This can happen e.g.when coefficient $c$ in the above bonuses is too large. In the paper, the authors write> This issue, to explore overly the state action pairs visited frequently but with high aleatoric uncertainty, is referred to as the over exploration issue.<|endoftext|>* In general the paper is well written. The proposed algorithm and the empirical results are presented in a clear way. In general the contributions of this work are significant and novel. I would have expected the empirical analysis to have been conducted on more environments, e.g., DeepMind control suite.<|endoftext|>The paper also contains extensive experiments to evaluate the performance of OVD Explorer. My major concern is about the technical soundness in paper s theoretical deriviation. From my pespective, $a$ and $a $ should be in a symmetric situation. Furthermore, the writing of this paper may also need some improvements. Although technically the theoretical motivation does not look very sound to me, the experiment results seem to be quite promising.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; I don t think I understood the paper well enough to form a strong opinion about this model. What lacks for me is a clear motivation for the following:1. 4.Also, how important is the interaction term? 5.I disagree with the approach of verifying the Main Hypothesis by showing that experimentally ETPs work better. 8.I am underwhelmed by the experimental results: Table 1 is hard to understand, so it s not obvious why ETP is better. Also, if ECE is an important metric, which I assume is, then it would be best to explain it in the text. Robustness experiments are pretty weird: I don t know how to interpet them or what was expected. Given the points above, I think that the paper lacks clarity. As I mentioned earlier, I cannot form a strong opinion before I understand the motivation for the model and the experiments.<|endoftext|>The manuscript proposes Evidential Turing Process which is a combination of Neural Turing Machines and Neural Processes. Even though I am far from an expert in the relevant literature, I find this paper very well written and was able to follow most of the presentation. It clearly introduces the relevant concepts and explains the underlying motivation for the proposed methods. I find the experimental section in the main text a bit brief. I personally find this justifiable as allows the manuscript to explain and motivate the proposed method in sufficient detail. I do want to encourage the authors to be a bit more restraint with marking the best method by bold letters in Table 1. Equation (4) misses a "(" in the subscript of the expectation valueI find the paper very well written.The proposed method appears to be theoretically well motivated and, empirically, leads to improved robustness.<|endoftext|>Strengths:    The idea is interesting and well motivated. The experimental results highlight the improvement over the baselines in terms of a series of metrics. The paper is generally well written. It would be interesting to see results on real world data beyond images, e.g.,  IMDB sentiment classification. Overall, this is an interesting work. I lean towards acceptance, especially if the comments that were raised are addressed.<|endoftext|>The reported prediction error seem unreasonably high (15+% on CIFAR10 for instance). Please clarify any misunderstanding concerning this point. There is no experimental validation for the choice of a Turing process. Despite a lot of math in the paper, the theoretical justification that is actually relevant seems rather limited. A large part of the equations appear to be somewhat decorative. I am looking forward to discussion with the authors to clarify any misunderstanding.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The paper proposes a new efficient method for denoising diffusion probabilistic models (DDPM) (generative models that optimize for the closest solution on a manifold) based on the observation that this can be seen a solving a set of differential equations on a manifold. This allows efficient pseudo numerical methods to be applied here which have many advantageous properties over classical optimization methods, including less optimization steps and guaranteed manifold solutions due to separating the gradient part from the transfer part in the optimization. The paper is overall well written, and the contribution and main ideas are explained clearly. Minor comments:  The authors claim that their implementation is in the supplementary. There are some grammatical errors throughout the text which should be proofread again.<|endoftext|>This paper introduces a framework to treat Denoising Diffusion Probabilistic Models (DDPMs) as solving differential equations on manifolds. I thank the authors for this submission. I believe there is value in this work both from a theoretical and practical perspective. In general, I am willing to accept the paper. I have two main suggestions:1. 3.I was honestly quite disappointed with the presentation at times. If you are short of space, I would rather move some of the experimental parts to a supplementary.<|endoftext|>Table 2: Error bars / confidence intervals are missing. If I understand correctly, this work builds on Probability Flows (Song et al., 2020), which leverage the existence of a deterministic process whose trajectories have the same densities as the original diffusion process. This deterministic process satisfies an ODE that depends on the score original drift and diffusion terms but also on the score function. Consequently classical numerical ODE solvers (e.g.RK) can be leveraged to sample data from the probabilistic model. Figure 3: Time has no unit. I personally find this submission interesting and significant, yet believe that clarity needs to be improved to enable readers take the most of the paper s insights. Would be necessary to give some intuition and to refer to a theoretical analysis2/ What is the precise problem? in these areas as the score is undefined (or hard to estimate)? Additionally, Section 3.1 is challenging to follow. Would perhaps be better to put less equations but spend more time explaining why and how they matter. Then, the submission shows strong empirical results on common datasets like CelebA, with faster convergences or significantly better FID (for the same number of steps) yielding SOTA. Finally, Figure 4 is quite nice as it empirically illustrate the proposed method ability to sample trajectories that like closer to the data manifold, which was the original motivation. ## WeaknessesI think that the main weakness is the writing. Citations for Runge Kutta and the Linear Multi Step methods appear to be missing. ## Additional feedback.<|endoftext|>However, Section 3 needs a lot of polishing. These are some comments and suggestions: 1. The contribution of this work is based on the assumption that $\sigma_t   0$. 3.The paragraph at the beginning of Section 3.3, supposed to give the intuition behind the introduction of the transfer part defined in (11), is not clear and should be reformulated. For example, what do you mean by ‘We find that Equation (9) has the property that if ϵ is the precise noise in $x_{t}$, then the result of $x_{t−\delta}$ is also precise, no matter how big $\delta$ is’? 4.In Algorithm 2, PLMS and PRK were not defined5. This would highlight the theoretical contributions of this work. The novelty of the methodology presented in this paper qualifies this work to be accepted to the conference conditionally to improve the clarity of Section 3.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper introduces a deep graph policy learning method that chooses tasks for each individual robot in a swarm thus addressing the problem of single task robot and single robot task (SR ST) multi robot task allocation (MRTA). The proposed method is based on the work by Kool et al (2019) with the main theoretical extensions on (1) adding constrains to the optimization objective of MRTA and (2) improving the encoder. Strengths:+ The idea of using learning methods to solve a SR ST MRTA problem is interesting. Theoretically, why does these constrains make the optimization problem more challenging to solve? However, the optimization problem introduced in Ghassemi et al (2019) includes other constraints that are ignored by the paper. Other than payload, why other constraints are removed? The paper explains that "the transition is an event based trigger", so it seems that t means the index of events (e.g., in Figure 1). In this case, when an event happens and all robots choose their tasks, how to guarantee that no multi robots select the same task? The terms generalizability and scalability are used throughout the paper. The experimental results are not clearly presented, especially the evaluation metrics. What are the task completion time of each approach for the testing scenarios in Table 1 and Table 2?<|endoftext|>This paper considers the multi robot task allocation problems. To address the limitations of existing studies, such as real world constraints, larger sized problems and generalizations, this paper proposed a learning architecture, Covariant Attention based Mechanism. This paper considers a very real world problem, and there are many details in the context. The reviewer is not very familiar with this sub area. As an application paper, it might be acceptable that the proposed method is a combination of some commonly used architectures or algorithms, such as multi head attention, encoder decoder and RL algorithms. In general, the technical novelty is not enough, although the empirical results are significant. Specifically, the reviewer is not sure what insights can this paper bring to AI community. An application paper with adequate evaluations.<|endoftext|>The paper proposes a graph learning approach for solving the multi robot task allocation (MRTA) problem. It frames the problem as a Markov Decision Process (MDP) and trains a policy with a graph neural network architecture using REINFORCE. **Strengths**The paper tackles the important problem of MRTA that is typically a computationally expensive optimization problem. The proposed approach of learning heuristics to solve it quickly seems appealing. The paper also presents a substantial evaluation comparing it to a SOTA non learning baseline. **Weaknesses**The main weakness of the paper is that it is unclear from the results if the CAM policy is learning something meaningful. Without clearly showing this, it is difficult to accept the paper at this time. **Suggestions for improvement***Overall story*The paper prescribes a very specific architecture for learning policies that solve MRTA problems. However, it does not offer much insight into why this approach is preferred over alternatives. For instance, an ablation study can be provided to show the relative importance of different components   why do we need the attention layer as opposed to a simpler aggregation step in GNN, rounds of message passing in the graph, etc. The paper would benefit from providing more insight into why one needs to apply learning for this problem, why GNN architecture is natural, why RL over other paradigms such as imitation learning of offline solvers. How much does REINFORCE improve upon this policy by using this as a warm start?<|endoftext|>This paper proposes a new method, including neural network architecture, for solving time constrained multi robot task allocation (MRTA) problems. The proposed approach models the target problem as a Markov Decision Process (MDP) over graphs and use Reinforcement Learning (RL) methods to solve the problem. The proposed learning architecture is called Covariant Attention based Mechanism (CAM). The architecture is shown to have better performance than an existing state of the art encoder decoder method regarding task completion, cost function, and scalability. Though the performance is still lower than non learning based baseline methods, i.e., BiG MRTA, the computational cost is significantly smaller than the baselines. 6.Qualitative evaluationOnly quantitative results are shown. The strength of the paper is that it proposed a new and effective method for MRTA problems. However, the architecture based on the multi head attention based encoder decoder network is interesting. In contrast with the general discussion and argumentation in the Introduction, the proposed method contains many specific formulation and network architectures (e.g., equations (1) and (2), and network architecture shown in Figures). Ablation studies are expected to clarify these points. The evaluation was conducted in terms of generalizability, scalability, completion rate, and computation time. For example, in a standard optimization problem, we can get a little worse solution by limiting the computational time (i.e., terminating the iterative algorithm). However, their theory is based on MDP. Therefore, the approach seems to be naturally applied to a probabilistic environment as well. I suggest the authors mention this point. (A graph itself is not a "problem.")<|endoftext|>This paper proposed a neural architecture for learning to solve multi robot task allocation (MRTA) problems. Case studies are presented in which the proposed CAM is compared with other learning based and non learning based methods. The references are appropriate and the problem being addressed is very well explained. 2.One of the paper contributions is the modeling of the MRTA problem as a MDP over graphs. This formulation is important because it allows to propose the covariant attention based neural architecture (CAM) to learn MRTA problems. 3.The CAM architecture is the main paper contribution. The CAM is basically an encoder decoder architecture, it uses attention mechanisms in the encoder and in the decoder, it uses information of other agents (context), and it codifies spatial information using a Graph Neural Network. 4.The reported results are convincing. 5.It could be good that authors could explain about the limitations of CAM. The proposed covariant attention based neural architecture (CAM) is a novel architecture as well as the modeling of MRTA problems as MPDs. Moreover, the use of the CAM architecture allows solving MRTA problems in a better way than using tradition methods. MRTA scales well with the number of robots/agents and with the number of tasks.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper proposes a simple but effective image text multimodal representation learning method that leverages a transformer based encoder decoder using a simple prefix langemodel as pretraining task from large scale noisy image text aligned data. As fine tuning tasks, the authors evaluate their method (SimVLM) on VQA, NLVR2, SNLI VE, CoCo caption, NoCaps, and Multi30k. With extensive experiments, this work presents promising few shot and zero shot performance results outperforming previous models. : .> , (comma)    Karpapth  > Karphathy Overall, this is a good paper but there are some issues that should be improved, in particular, reproducibility and table for readability. It is not trivial to construct 1.8B paired data even if they are weakly aligned and train them. If smaller size data can provide comparable results, the contributions of this work will be much enhanced. Table 3 can also be improved.<|endoftext|>This paper proposes a Prefix Language Modeling (PrefixLM) objective for a pretraining procedure for multiple vision language downstream tasks and zero shot evaluations. In the 2nd paragraph of Section 3.3, "we additionally add 2D relative attention for the image patches within transformer layers," which needs some elaboration for reproducibility and self contained explanation. Surprisingly, this simple pretraining approach achieves new state of the art on "a wide range of discriminative and generative vision language benchmarks," and shows "strong generalization and transfer ability" in zero shot settings. ** The most important point is to simplify a pretraining procedure. Are you mentioning that the corresponding models are pretrained on CoCo or NoCaps? Here, all SimVLM is also pretrained on both ALIGN and C4 datasets, but you don t say all your model is pretrained. However, this work is narrowed to achieve the state of the art performances while failing to show 1) better performance of the PrefixLM than MLM, 2) relationships with the other multimodal pretraining losses (e.g., ITM) 3) in the controlled experiment in terms of pretraining dataset and model architecture. Moreover, 2) this ablation study is performed for text only data, not vision language data in Section 4.4 Ablation Study. The controlled experiment still has the problems as described in W1.<|endoftext|>The paper proposes to pre train a generative language model conditioned on a visual input on billion scale web image text data. SimVLM establishes new SotA on several new tasks and shows promising zero shot capacity in certain tasks. This is a new yet promising way of transferring to new V&L tasks. I do not see if the authors check whether downstream tasks data are present in the pre training dataset. 5.Efficiency concern.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper studies few shot learning from the viewpoint of transferring feature representations. The main finding in the paper is that transferring from more complex source tasks tend to result in better performance. Accordingly, using multiple source tasks is also found to be useful. My main concern of this paper is its novelty. I understand that the focus of this paper is different (i.e., few shot, self supervised tasks),  but at high level, the findings and conclusions in the paper seem rather obvious to me. Or, is this result really surprising for few shot learning? Overall, I find this paper presents an interesting case study of feature transfer focusing on few shot learning, it lacks enough novel insights and technical contributions to warrant publication at ICLR.<|endoftext|>They analyze the way features are transferred for MAML method and 3 self supervised methods. They introduced a method combining self supervised representation training methods with voting system in order to improve performance. After initial description I was saddened that the analysis of feature representation of few shot methods involved only MAML. At least some more methods should be analyzed as then instead of studying few shot methods the paper studies MAML only (which might mean that conclusions might not generalize). The same goes for the voting system built on top of the proposed training scheme. In Table 11, only the authors method is using modified ResNetW12   this raises the question how significant that is? Interesting case study of feature representations for few shot learning. However, it lacks depth (only MAML analyzed as few shot method) and the proposed solution lacks technical novelty and convincing results.<|endoftext|>Gidaris et al.2019.Although the paper studies an interesting topic, I have some concerns about correctness of various statements in the paper, the relationship with prior work that has explored similar questions, and the need for additional experiments and analyses to aid in better understanding where the observed performance gains come from. They also propose to combine different training objectives, including the supervised classification objective and different self supervised ones in a multi task manner, and propose to do so by keeping the input the same for all objectives involved, meaning that to train the supervised loss together with the rotation, the former will also be computed on rotated images. The authors hypothesize that the reason auxiliary class training helps is to alleviate the issue of learning spurious features. This would help understand if this voting is beneficial only when the augmentations chosen are the same ones used for training. Studying the utility of different representations for few shot learning tasks is an interesting and important topic, due to the success and wide adoption of the transfer learning paradigm. I’m assuming the main difference is that a new random readout layer is initialized instead of re using MAML’s meta learned classification layer. Is there also a difference in the number of steps, e.g.logistic regression takes more steps than fast adaptation? However, the study is limited to simple datasets, and the proposed methods do not yield a consistent gain, especially on tiered ImageNet. The approach used here is reminiscent of the ‘input harmonization’ in [7].<|endoftext|>The paper provides a study on different approaches to train a backbone network used in a transfer fashion for Few Shot Learning. Representation learned from MAML, supervised classification, and self supervised tasks are considered here and their performance are compared. # Additional references  Mangla et al, "Charting the right manifold: Manifold mixup for few shot learning"; WACV 2020. # Main review The main part of the paper is dedicated to the study on the various training approaches for the transferred backbone. Author could also have cited work of Mangla et al that also addressed this point and provide some insights on the benefits of using additional self supervised task in the context of Few Shot Learning. This study part is quite long and some experiments could be omitted (e.g.training the backbone only on one self supervised task) and does not really bring some new learnings with respects to previous works. I find it as being the main contribution of this paper. Could we have more insights why the voting solution does not work for tiered ImageNet? Ablation studies is required to see what are the relative benefits of the varying tricks.<|endoftext|>This paper presents an interesting study comparing the various feature representations for few shot classification tasks (in Computer Vision) learned from the supervised classification with/without MAML, from multi task prediction, and self supervised tasks like rotation prediction and location prediction, and contrastive learning. And which proxy tasks (rotation prediction, location prediction) are helpful. * In depth analysis and experimentation with various techniques, architectures like ResNet, ConvNet, and sizes of the network help point out the limitations of those techniques and lead to the critical finding that complex tasks (combining classification, rotation prediction, and location prediction) tend to give better representations for few shot classification. * The use of transfer learning with multi task representations and novel tricks like “eliminating irrelevant features using auxiliary classes” improved the performance on 5w1s, 5w5s, 10w1s by emphasizing the unique task related features. The other trick is “voting with auxiliary task instances” where a set of rotated or cropped copies of the input image are given with the input image may not have any impact on the learned feature representations but it helped to improve the accuracy even closer to SOTA. * The description of various hyperparameters and the reasoning for constraints used in the experiments helped to better understand and compare the results to other methods. #### __Concerns:__I have some queries about the studies presented in the paper. Hopefully, the authors can address my queries/concerns (mentioned above) in the rebuttal period.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper investigates using virtual nodes in graph neural networks for link prediction. Specifically, the authors use a graph clustering algorithm to determine groups of nodes in the graph and adopt multiple virtual nodes in the graph for the link prediction senario. Experiments conducted on six datasets  provide insights and guidelines about using virtual nodes for link prediction. Weakness of the paper:  The motivation of using virtual nodes for link prediction is not clear. The authors only argue that the fact that virtual nodes have not been studied in link prediction is because the large and heterogeneous graphs in link prediction are of very different nature, but do not clearly explain why virtual nodes are important for link prediction. The authors should show a more significant performance improvement or clearly show problems that using virtual nodes only addresses.<|endoftext|>This paper analyses the roles of virtual nodes in the link prediction problem. Extensive experiments are conducted to support the claims and show that virtual nodes can improve the link prediction performance of GNN. I agree with the paper that virtual nodes lack a better understanding. 2.The paper theoretically analyses the effect of virtual nodes on influence distributions Concerns:I m particularly interested in how to decide the number of virtual nodes because it is important for practical use. Is there any practical guidance? By the way, some sentences need to be checked.<|endoftext|>By the authors’ claim, this is the first work to employ virtual nodes to improve the link prediction tasks. 2.Virtual nodes are well motivated to capture long distance / under reaching messages between nodes. The theoretical analysis is limited to regular graph for influence score and non attribute graph for expressiveness of link representation. Therefore, it seems powerful GNNs for node representation or node classification can be directly used for link representation or link prediction. But it seems that P GNN conflicts with this claim as the performance of P GNN is really bad though the authors mention some concerns about it in supplements. But there are also many concerns on both two sides which should be well addressed.<|endoftext|>This paper proposes to enhance the link prediction performance of GNNs by adding multiple virtual nodes. However, the contribution of the paper is mainly from the relatively narrow engineering perspective (a technique to improve the performance) without much conceptual or theoretical insights. Besides, the designs of different virtual nodes are a little bit ad hoc. Therefore, the studies on problem of whether virtual nodes will help link prediction seem not systematic enough. post rebuttal After rebuttal, the authors explained the rationale behind using virtual nodes for link prediction to some extend. Contribution is limited, as explained in the main review, and I think it is not enough to be accepted.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper aims to improve the reliability of LLM based code generation via two aspects: select better prompting examples based on program similarity and constrain LLM to only generate valid programs incrementally. To the best of my knowledge, the ideas presented by the paper are novel and interesting. The methods are also relatively easy to implement/reproduce and drop in existing pipelines. Intuitively, the language model probably has a better model of these languages due to more training data, such that CSD is not that needed or only marginally more beneficial than generate then test. Yet I imagine this is perhaps more important to validness than the context free layer. The methods are also relatively straightforward to reproduce and built upon by future works.<|endoftext|>6.Maybe I missed the number in the paper, but I wonder how many constraints did you implement in CEs for the three programs? The authors are encouraged to provide some analysis in that case. To resolve this problem, the authors propose 1) Target Similarity Tuning (TST) for retrieving 5 relevant examples based on program similarity and 2) Constrained Semantic Decoding (CSD) for constraining the code generation output to a set of valid programs. Could you please explain a bit more about it?<|endoftext|>SYNCHROMESH first retrieves few shot examples from a training set using Target Similarity Tuning. It then feeds the examples to a pre trained language model and samples programs using Constrained Semantic Decoding, which can constraint the output to a set of valid programs in the target language. The paper is generally well written. The proposed approach utilizes pre trained models for code generation. Can you simply use a non DL method to generate code based on the similar examples you retrieved, instead of using a pre trained model? Pros:.A new approach to code generation using pre trained models.<|endoftext|>Overall, the paper gives some interesting ideas, but it does not demonstrate these ideas lead to actual wins and doesn’t discuss its limitations. * The first one called Target Similarity Tuning (TST) is a way to pick input/output examples similar to the input text. Previous works use a pretrained models for natural language similarity (Sentence BERT is compared in the paper) that determine what examples to include in the few shot prompt and the TST proposes to also fine tune this model on code examples such that some semantic similarity in encoded   for example using the same structure of the queries [think of SQL queries to synthesize]. * The other improvement called Constrained Semantic Decoding (CSD) proposes to alter the decoder of the transformer model and to make it avoid generating programs that are impossible to complete to syntactically or semantically correct ones. The paper proposes two enhancements of this process. There is also something unclear about the comparison to generate then check   is it compared to the full Synchromesh or only to the CSD part of it?
Reject; rating score: 3; rating score: 5; rating score: 5; This paper improves on the idea of mixup by selecting samples for mixup via a model that selects suitable pairs found using knn in a batch. e.g., if the masses are some set distance apart, then use, else don t.I see meta learning as a description of the model, but not in the algorithm 1. The paper is placed as "RL" so we re missing important baselines and connections to other gradient estimators through discrete processes.<|endoftext|>The authors propose MixRL to improve upon mixup in regression settings. MixRL is used to impose a proximity constraint on the input/output pairs that are mixed during mixup based data augmentation, by predicting how many nearest neighbors to utilize, from a small set of pre specified options, based on feedback from evaluating the validation set. Consistent but small gains over mixup and manifold mixup are realized on several datasets. More generally, the lack of a local input/output kernel to prioritize more local data in a continuous manner feels like a significant limitation.<|endoftext|>To apply Mixup for regression tasks, the paper first utilizes the stricter assumption that linearity only holds within specific data or label distances for regression. Then this paper proposes a data mixing augmentation method called MixRL. The goal of MixRL is to identify which examples to mix with which nearest neighbors. MixRL employs a meta learning framework that estimates how important mixing a sample is to minimize the model loss on a validation set using policy gradient reinforcement learning. The idea is reasonable for regression tasks. The paper is well written and organized.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This work presents various improvements over the differentiable non deterministic push down automaton. Basic idea of non deterministic push down automaton is to explore all the space for state id with stack with push/replace/pop operators with dynamic programming. Consider the joint distribution of stack symbol and state, not only stack symbol, so that the controller can differentiate by the current state of the PDA. The model is not space efficient in that the stack is unbound, and thus, consumes large memory. This work proposes to limit the stack memory so that the model can run on the real World data, e.g., PTB. Experimental results on synthetic data show large improvement in terms of per symbol cross entropy. # Strength  It is a very interesting work on differentiable non deterministic PDA in terms of the theoretical view point. Although the gains are not large and computationally demanding, the proposed enhancements over prior work are very interesting in that they allow its application to the real World data, i.e., PTB. Probably manual assessments would be good enough to show some evidence. The proposed method is sound and the experiments are designed nicely to prove the improvements of this work.<|endoftext|>This work continues a recent work on nondeterministic stack RNNs (in which an RNN controls a nondeterministic pushdown automaton, which I will refer to here as an N PDA), proposing two changes to the architecture which create an RNS RNN (renormalised NS RNN) as follows:1. 2.In figure 3, there is a white line in the middle of the image for both models, suggesting that it is not learning the correct action at the point where the reversing  begins . They evaluate a memory limited RNS RNN on the Penn Treebank. A dynamic algorithm by Lang is used to maintain the PDA, I am not sure of the details quick pros   proposed changes are simple, clear, and appear to improve the model on some synthetic tasks (even significantly for one of them). quick cons   the model does not seem very good on natural language tasks. However, if my understanding of stack rnn research is correct, this is to be expected. I understand that it makes the model define a proper distribution (as opposed to weights) over the sequences, but I don t understand why this is important for these evaluations. Note: I recognise you responded to some of these questions in the previous review round, but I would like them answered in the paper too! The paper is well written, though it could do with some expansions and clarifications as elaborated in the main review. I appreciate that the authors have been very explicit in all of their constructions and all details of their experiments, I find this very valuable. In particular in equation 2 I would like some more explanation on the [i >t] inputs   it seems there are jumps over several time steps?? (Approximately, though will need rephrasing: defining and maintaining a distribution over all possible configurations of a non deterministic PDA, by computing at each step: for each state and stack top combination, a distribution over all possible next state and stack actions.And from this, each configuration s probability is the sum of the products of each sequence of transitions and state actions that get to that configuration). )<|endoftext|>The paper proposes a new stack augmented RNN, RNS RNN that includes two modifications to the Nondeterministic Stack RNN:1. RNS RNN uses unnormalized transition weights to avoid probability vanishing problems, such that gradient can be easier backpropagated to certain decisions at a previous time step. 2.RNS RNN feeds the hidden state y into the controller while reading the stack, this allows the controller has more complete information to make decisions. The proposed method solves two defects of NS RNN. The RNS RNN is based on a weighted PDA, which has a finite set of states. This setting limits the capacity and expressiveness of the proposed model, especially when modern deep learning methods usually use very large hidden states to achieve high expressiveness. 3.The formal language tasks are toyish. Overall the paper proposed useful improvements to the NS RNN.<|endoftext|>This paper aims to improve the performance of the Nondeterministic Stack RNN (NS RNN) using unnormalized positive weights instead of probabilities for stack actions and allowing the model to directly observe the state. The paper also uses the new NS RNN for a language modelling task on the Penn Treebank by introducing a memory limiting technique. Strengths  The discussion on previous stack RNNs was a strong segment of the paper as the authors connect the various approaches using common notation. Transitioning the discussion from formal languages to natural languages was a good motivation for introducing the memory limited technique. Good use of SG score metric instead of just perplexity. **[addressed by author response and paper update]**Overall, I think this work is marginally above the acceptance threshold. The improvements made to NS RNN are well motivated. While this may seem to be incremental, I think the contribution of the memory limiting approach enhances the paper s relevance by allowing for language modeling over natural language.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; the submission proposed a semi supervised learning framework that leverages the benefits of multi view learning with neural networks. There is always a tradeoff between the diversity (variance) of the base learner and its accuracy. I hope the authors could be more precise on their wording in the intro. Referred literature in this paper talked about decreasing correlations by including a penalty term, but they didn t mean that we would like to see negative correlations.<|endoftext|>A view consistent loss is designed, which is based on PGM. e.g., GAT uses multi head. 2.In multiview learning, I want to know how to model diversity? The consistency is easy to model, but the diversity is not well addressed. Even CCA can be regarded as a view consistency modeling. for what applications?<|endoftext|>This paper concerns the semi supervised problem. Different from SOTA deep semi supervised methods, the proposed DiCom employs a diversity measure on the labeled multi view data, and combines diversity with consistency based on underlying probabilistic graphical assumptions. 1.The consideration of diversity for semi supervised learning is interesting, and the reviewer considers that it is helpful for improving performance. 2.The proposed method combines diversity with consistency based on underlying probabilistic graphical assumptions is a meaningful idea. The idea of this paper is direct. The SOTA comparison method is in 2018, more comparison methods recently are expected.<|endoftext|>This paper proposed a technically sound method based on an undirected graphical model for semi supervised regression. I d like to know the main challenge and unique contribution of this work compared with [Yu et al., 2011]. It sheds light on the effects of diversity and consistency in multi view learning. The assumption of $\sigma^2_m   \sigma^2_v$ is not given here.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper presents a method on generalization of segmentation from synthetic data to real street scene data. To adapt the model pre trained with synthetic source domain data such as GTA and SYNTHIA to target domain such Cityscapes, BDD and IDD, the authors proposes Instance adaptive Batch Normalization (IaBN). TT SEG is a simple method for test time finetuning. Although the effectiveness was shown by the experiments, the novelty seems to be limited. The novelty of the proposed method is limited, since IaBN is an existing method and the method on one sample adaptation is straight forward.<|endoftext|>The first is instance adaptive bn, to combine statistics from source data with each single test sample. 2.Both the two modules are conceptually simple. 3.Extensive experiments have been performed for evaluation. Cons:1.My major concern goes to the novelty of the paper. The proposed normalization technique is incremental. Instance level normalization is a smart way for test time adaptation, which has been well studied [1][2][3]. 3.The necessity to use an independent development set for model/parameter selection needs more studies. It is also not clear to me how to select a universe development set that can fit any target domain, or should we find a unique dev set for each target domain. I am concerning that the paper may be still under the level of ICLR.<|endoftext|>This paper studies an existing problem   domain generalization for semantic segmentation of urban scenes. The technique keys of this work has two main points: instance adaptive batch normalization and testing time training using pseudo labels. Instance adaptive batch normalization aims to bias to the data distribution of individual testing examples, which is a tradeoff of previous t BN and p BN. This paper generate pseudo labels for test time training, which is not quite novel. To evaluate the effectiveness of the proposed method, this paper conducts experiments on GTA5/SYNTHIA  > Cityscapes, BDD100k and IDD, and shows better results. The weakness of this paper are as follows:  The novelty of this work is the main drawback. In instance adaptive batch normalization paragraph, the authors use \head{u}_{c}^{s}, but in Eq (4), the authors use u_{c}^{s}. Although this work shows better results than previous work.<|endoftext|>This paper contributes two techniques to improve generalization of semantic segmentation networks. The first technique is an adaptation of the test time behaviour of batch normalization, where the statistics of the sample under consideration are blended into the training time statistics of the batch norm layer. The paper additionally contributes a multi dataset evaluation procedure and shows favorable performance of the proposed techniques on this evaluation protocol. It is unclear how to improve the practicality of the approach. It would be great to see some comparisons to existing pseudo labeling approaches as I think that the setting with only a single image from the target domain artificial.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This is very likely a confounding factor in the efficacy of active learning and pruning techniques. Weaknesses: The primary weakness of the paper is the lack of convincing justification that the authors have discovered a phenomenon distinct from “collective outliers” (Karamcheti et al., 2021)   or if it is distinct, how exactly is it distinct? Questions and minor suggestions for the authors:1.<|endoftext|>investigating the negative impact of outliers on active learning for visual question answering[J]. Strengths: The experimental scale of this work is undoubtedly large and comprehensive, involving many data sets, different types of active learning algorithms. This paper conducted a large amount of empirical experiments on AL with pre trained LMs.<|endoftext|>This paper examines how active learning works with large pre trained models, with a novel explanation with empirical evidence. Strengths:   I could learn various empirical observations of how active learning works in these large pre trained models on various NLP tasks.<|endoftext|>They observe that depending on the task/dataset, AL actually does not improve over a random baseline. The idea of using convex hull of AUC is interesting, but it would it nice to confirm it experimentally. For an experimental paper, it is interesting, and might point to some new directions for AL.<|endoftext|>In addition to outliers, they show that training instability is a major issue. There are several detailed ablation analyses which I find interesting, such as the influence of question format and adversarial filtering on the datasets. Weakness:One of my major concerns is on the point of training instability.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The problem setting is good, but the main conceptual weakness is a lack of explanation for how the model is supervised to be equivariant to different data augmentations. Furthermore, empirical results seem to suggest that the proposed data augmentation does not have much of an effect on performance, which seems to undermine the motivation for this paper. Strengths:  connects data augmentation to counterfactual property generation  clearly written  What is novel about applying counterfactual data augmentation to DYNA, as opposed to standard data augmentation techniques in other areas of machine learning (e.g.computer vision) is that the goal of counterfactual data augmentation should be to learn a model that is _equivariant_ to data augmentation, whereas standard data augmentation techniques mostly focus on making the model _invariant_ to data augmentation. Based on the authors  response, it appears that the majority concerns I had raised will be addressed in the next iteration of the paper.<|endoftext|>Enhancing the diversity of training data (here, time invariant properties of the state) to improve the robustness and extrapolation capacities of an RL agent is a good idea but is standard practice known as domain randomization. However, this perspective provides no additional insight; the structural causal model it introduces does not appear to be used by the method at all. Moreover, the presentation does not cleanly separate counterfactual reasoning from intervention. Methodologically, the greatest weakness of the method, acknowledged by the authors, is that there is no way to train the model on altered data. Thus, the performance of the policy on these altered data hinges on the extent to which the model, trained without such data, happens to make accurate predictions. The causality perspective provides no novel insight.<|endoftext|>This paper aims to design a counterfactual reinforcement learning model to improve the training efficiency of the agent. To demonstrate the effectiveness of the proposed model, the authors have conducted many experiments. In general, the paper is well presented, and I can easily grasp the main idea. The major concern lies in the novelty, I think building an environment with a counterfactual technique is not new, which has been proposed before. In addition, I would like to ask how to handle the error produced by the SCM, maybe theoretical analysis can be provided to demonstrate the bound of the agent when being trained on such error involved SCM.<|endoftext|>The paper presents a method to improve the generalization of model based RL by means of interventional data augmentation. The key idea is to intervene the value of a particular variable (e.g., object property) in the learned dynamic model for episode simulations. Experimental results show that it improves (i) the generalization ablity in the OoD scenarios with respect to the intervened variable, (ii) sample efficiency in the presence of unbalanced training distribution. The paper is very well written. Weaknesses  The claimed connection between the SCM and the proposed dynamic model seems vague. The paper well demonstrates the benefits of counterfactual data augmentation for model based RL. However, the technical contribution seems limited and involves very strong assumptions.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper describes a pyramidal vision transformer that introduces cross scale patch embeddings, dynamic relative position biases and "Long Short Attention". Ablations on classification are provided hinting that the multi scale embeddings are the most important factor contributing to the overall superior results to similar, previous architectures. This is not to say that it is not useful as it clearly provides inductive bias, but it is less expressive which is the opposite of what the paper says. Hence I don t agree with the blank statement: "They fail to build the interactions among features of different scales, whereas such an ability is very vital for a lot of vision tasks." A more subtle discussion on inductive biases would be more appropriate and I would ask the authors to change that. The authors hypothesize they are due to cross scale patches. These ablations are important to also verify that it isn t the overall training setup or other subtle differences in the architecture that give the advantage.<|endoftext|>The short attention is computed in a local region. Also, a dynamic position bias is designed to make the network suitable for inputs of different resolutions. 2.Thorough experiments verify the effectiveness of the proposed techniques. And CrossFormer achieve good performance on several vision taks. Cons:1.Some comparisons and discussions with related works are missed. However, some designs are not new and some related works are missed. I would rate this as a borderline paper (marginally above the acceptance threshold).<|endoftext|>This paper proposes a novel vision transformer architecture called CrossFormer which focuses on the cross scale ability in the attention module. Experiments demonstrate that the proposed CrossFormer achieves performance improvement on image classification, object detection, instance segmentation and semantic segmentation tasks. The mixture of multi scale features enable a richer context, especially for dense prediction tasks like object detection, instance segmentation and semantic segmentation. The proposed LSDA may have some limitations in its design: Table 1 shows that each stage in CrossFormer has the same choice of (G, I) and Appendix A.1 shows that G*I is the size of the feature map. 3.This paper also proposes a dynamic position bias (DPB), which aims to replace the relative position bias (RPB) used in Swin and early works in NLP (Shaw et al).<|endoftext|>The proposed architecture follows the layout of Swin transformer but replaces the shifted windows transformer block with the proposed short distance attention and long distance attention. Comprehensive experiments are conducted on image classification, objection detection, and segmentation tasks. Comprehensive experiments are conducted, and the results showed the effectiveness of the proposed architecture. It s hard to say that the proposed dynamic positional embedding (DPE) is a contribution. I think the authors should carefully discuss the relation between them. My main concern is that the idea of the proposed components is not new in the CNN based works. The overall novelty of the paper is limited.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes spiking neural networks (SNNs) as an efficient neuromorphic alternative to dilated temporal convolutions, based on the WaveNet architecture for keyword spotting. The main idea is to model the delay periods in dilated convolutions of WaveNet as synaptic time constants in SNNs for efficient implementation with neuromorphic hardware. Then they show by experimental evaluation that WaveSense surpasses the SOTA SNN performances and come close to the SOTA performance of CNN and LSTM based methods, in keyword spotting. I think the main weakness of this paper is the limited evaluations regarding the efficiency. Authors did not address these issues.<|endoftext|>The authors propose a way to translate WaveNet into SNN based network referred to as WaveSense. The performance of spiking wavenet on three different datasets was compared with a few previous results. Several technical aspects of the proposed method were reported including data preprocessing and translation into spiking neurons and so forth. However, I indicate that the SNN considered is absolutely not an SNN given the use of multiple spikes per timestep. In this regard, strictly speaking, the network considered in the present work is not SNN. Therefore, the comparison of this work with other SNNs is not fair at all. 3.Benchmark is weak.<|endoftext|>The idea for using such an architecture for keyword spotting originates from [Coucke, Alice, et al.2018].By training the SNN using Backprop Through Time (BPTT), the proposed method achieved better performance than other SNN based methods and reached near SoA performance for the keyword spotting datasets. 2016.Coucke, Alice, et al."Efficient keyword spotting using dilated convolutions and gating." 2018.Pros1.Simulating temporal convolution in SNN by varying synaptic time constant is a neat idea. As already pointed out in the paper, it s expensive to use spike delays for neuromorphic computations. The authors gave a neat idea on how to implement temporal convolution with spiking neurons efficiently. Second, the structure and writing of the paper need considerable improvements to make it more readable. Energy efficiency is not a default property of SNN, and an SNN needs to be deployed appropriately on a neuromorphic processor to realize this advantage. When compared with the other SNN approaches, the result for the proposed method might be better due to the more complex network structure.<|endoftext|>The authors conducted experiments on several real world data sets for evaluating the efficacy of the proposed models. This paper is hard to follow due to its tinpot writing and organization. Besides, it’s necessary to give a formal introduction for the notations in the spiking neural model, such as $\tau_s$. Fig.2 does not corresponds to the context in Subsection 2.2. The points above prevent the understanding of this work.<|endoftext|>The paper proposes a spiking neural network (SNN) that takes advantage of the time constant of the neuron spiking dynamics to implement temporal convolution, instead of using synaptic delays, in order to reduce memory requirements. The experimental results show improved audio keyword recognition performance compared to the existing spike based methods. The paper demonstrates that the time constant of the leaky integrate and fire (LIF) can be effectively used to implement time delays, as required for temporal convolution. The paper provides limited technical novelty. While the presented results show that the proposed approach of temporal dilated spiking convolutional network can be trained to recognize audio keywords with competitive accuracy, the main contribution of the paper is a system that combines ideas from multiple existing works.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The authors name their approach as "meaningful learning," which, at a high level, means that we should relate the new word with existing words. The concrete technique they proposed is to use domain specific rules to generate new data that contains novel words based on the existing examples. For example, why is the arrow from "prior knowledge" to the bottom middle box the same type of arrow as the one from the bottom middle box to "new compositions?" My primary concerns about this paper are the following. 1.The learning setup contains very strong prior knowledge about the domain, which, sometimes can be wrong. 2.If we view the algorithm just as a data augmentation technique, I believe, at least for the cases studied in this paper, it has been covered by many existing techniques:  https://arxiv.org/abs/2011.09039 This paper contains results for a number of heuristic augmentation techniques. https://arxiv.org/abs/1904.09545   https://arxiv.org/abs/2010.03706 Learning to do data augmentation. https://aclanthology.org/2021.findings acl.307.pdf (replacing a subtree with a new tree that has the same tag)The authors discussed a bit about some related works in the second paragraph in Section 4, but I am still not seeing enough contribution compared with existing methods. This assumes the same kind of knowledge as the authors  proposed technique. Note that these generalization tests are all "easier" than the one we have in SCAN (e.g., jump generalization, where there is only one single example of the novel word "jump")The paper is decently written and the statements are well supported.<|endoftext|>Based on this view, they propose two data augmentation methods from either the inductive learning perspective or deductive learning perspective. They train different model variants with such augmented data. 2.The author designed extensive experiments to prove the concepts and also show the effectiveness on real data   machine translation and semantic parsing. Although the author is well motivated by some psychological concepts such as meaningful learning, semantic linking or prior knowledge, I had a hard time to draw the connection between these concepts to targeted systematic generation tasks (e.g., SCAN). I think the author should give a more concise definition of these concepts, and improve the examples in Figure 1 with better explanation. 3.It seems this paper only focuses on a specific type of generalization   lexical variants, while most of the systematic generation research focuses on compositionality. The paper seems to overclaim and not be clear at the early part of the paper. This paper provides a new perspective of systematic generalization, and gives empirical results to prove that methods based on this perspective can improve model s generalization on toy and real tasks. However, the explanation of the new perspective is not clear enough, and the proposed methods look pretty simple and only solve simple generalization problems.<|endoftext|>The paper introduces an interesting idea of improving the systematic generalization ability via meaningful learning. Through providing augmented data for inductive learning and deductive learning, the sequence to sequence model can be more generalizable to compositions of new concepts. Deductive learning and inductive learning provide two learning directions for specific to general and general to specific. 2.It designs and conducts comprehensive experiments for analyzing how the semantic linking in inductive learning and deductive learning affects the models. The technical contribution is weak. It proposes the augmentation approach using the semantic link but does not study how to better use the semantic link. I would like to see more insights into how to design the model beyond these traditional sequence to sequence models. 3.Some arguments and claims are not appropriate and make it harder to understand the approaches. The inductive learning and deductive learning story make it ambiguous on whether it is a learning strategy or data augmentation strategy. I suggest the proper tone should be augmenting the data and then explaining the learning process with augmented data in two paradigms, but not listing the inductive/deductive learning at the beginning, which makes it very confusing.<|endoftext|>This paperintroduce semantic linking for systematic generalization through the analysis of inductive and deductive learning from a meaningful learning perspective. Merits:* This paper focuses on an important problem   systematic genneralization and compositional generalization in particular. * Considering there are already many works using data augmentation to improved compositional generalizaiton [1,2], the algorithmic novelty only combining with semantic linking is quite doubleful in my opinion. But in the paper, it seems the model only use rules to generate extra parrallel data for training. Considering there are many works on compositional whether they are only exploiting dataset biases is an open question and suits the title of this work. * The claims in the paper are a bit vague like  meaningful learning  (seems like a cognitive concept but not very clear to many authors). It would be much better to have an algorithm for introducing the method.
Reject; rating score: 3; rating score: 5; rating score: 6; The paper proposes to learn training curricula by considering 3 sigmoids (representing "easy", "mid" and "hard" difficulty levels), that would define instances  weight as a function of time. The paper emphasizes that the approach can replicate some of existing curriculum heuristics (e.g.easy to hard, hard to easy etc. Finally, the approach is illustrated on 3 small to mid size datasets with largest gaps to no curriculum achieved on reduced datasets to re balance classes. Besides, since the loss function changes, the baseline and the weighted tasks optimize different objectives and it s not straight forward to compare them. 3.The requirement to pre train the baseline prior to curriculum enabled training defeats one of the main purposes of curriculum learning   saving resources in the large data regime and relieving developer from manual training design.<|endoftext|>The paper presents a curriculum learning approach applied to NLP models. The approach is evaluated on three benchmark datasets (SNLI, Alcohol, Cancer). Strengths:+ The authors conducted a braod set of experiments. + The paper is easy to follow. Weaknesses:  The main contribution of this work is to add changing weights to easy, medium, and hard training subsets, which an incremental development in my opinion. The introduction leaves the impression that weights for data shards are somehow predicted by the model at each iteration. The same applies to the supplementary. In my opinion, the weaknesses outweigh the strengths of this paper.<|endoftext|>This paper proposes a new parameterized data partitioning and weighing scheme, that partitions data into three groups {easy, medium, hard} and determines a curriculum based on relative importance of different samples. I encourage authors to address these concerns and I will be happy to bump up my score. The curriculum also provides interesting insights about the datasets and scale from smaller datasets to larger datasets. #### Strengths  Proposed curriculum method encompasses other well known curricula by parameterizing the data partition and weighing schemes. Sample weight patterns shows the relative importance of samples on different datasets, which is interesting. How will this method scale? In Section 2.3, the authors describe how the proposed framework encompasses other well known CL approaches. Authors evaluate on three datasets, not all of which are well known to the community. How significant are the improvements over other baselines? Some results show that other methods are better when it comes to full datasets.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 5; * More details and discussions are needed on the numerical experiments. * The analyses only apply to the simple two layer neural networks, for both discriminator and generator.<|endoftext|>The proposed ProCoGAN model based on the theoretic results is also novel. The main results of interpreting the two layer WGAN training problem as convex problems look solid to me.<|endoftext|>This paper aims at providing insights into optimization tractability of WGANs in the two layer discriminator case with different activation functions. I will only provide feedback for the theoretical part of the paper, as I do not have appropriate experience/exposure to the simulation side of GANs. ●	Typo: ijth column and row.<|endoftext|>In this paper, the authors analyzed the training of Wasserstein GANs with two layer neural networkdiscriminators. The first optimization related results on  WGAN  as a convex problem (or a convex concave game) with polynomial time complexity for two layer discriminators and two layer generators under various activation functions.
Reject; rating score: 5; rating score: 5; rating score: 6; This paper focuses on using generative models to improve multi task learning. The experiment is carried out on a single task, and the proposed framework is suitable for multi task. In addition, I think using Taskonomy s output as the ground truth to train with the synthesized image is not good enough. Although Taskonomy is the state of the art method, its qualitative performance is still far from being regarded as a good ground truth. The advantage of training the generative model with the multi task model is that it can better learn the synthetic images guided by the multi task outputs and the downstream tasks.<|endoftext|>Is that a bug? The paper is abundant with experiments and most of the numerical results shows good indication that the proposed method is better than existing state of the art, and ablations studies somewhat justifies the design choices of the method. Does it mean that they are better models in the low data regime? The narrative of the paper focuses on the generative model that is jointly trained with the multitask model. The performance drop without the generative model is really just marginal. According to the qualitative results shown in the main text and in the supplementary, it seems that the prediction quality is quite low for all methods.<|endoftext|>In the experiments with two dataset, the proposed model, MGM, consistently outperformed the results of the baselines and ablated methods. The scene class was known for a synthesized image, since it was generated with a class condition, which enabled training of the multi task network,M, with generated images. In addition, EM style training was proposed to train the network effectively. Although adding a self supervision network is not so novel, its effectiveness was shown by the experiments. The paper is well written, and the experiments including ones in the supplementary material are comprehensive and detailed. Cons)The proposed method needs to scene labels for all the training images. That s why MGM cannot be applied to the CityScape dataset which consists of only street scene. Did the authors try that ?
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; This paper presents a model to predict Gene Ontology (GO) term annotations for protein function. The model uses an existing method, SeqVec [2], to encode the protein sequence and a GCN on the Gene Ontology (GO) DAGs to encode the structure of term relationships. On the other side of what? [1] Zhou, Guangjie, et al."Predicting functions of maize proteins using graph convolutional network." This language is unclear. That it is *capturing* that relationship? Functions of the protein that are to be learned? This is the same adjacency weighting scheme used by DeepGOA [1]. It is not well justified to include more terms without an experiment to show the advantage of this design decision. Motivation and comparison to prior work is very limited as the paper does not properly discuss differences to prior work or motivate the decision made.<|endoftext|>The paper proposes a method to predict protein functions from Gene Ontology (GO) and protein sequences. The protein sequences are embedded with a pretrained protein language model (SeqVec) and the GO network is modelled with a graph convolutional neural network. Weakness: Overall the proposed method is very similar to DeepGOA, which the paper cited but did not benchmark with. Comment: Some ablation study on different components of the model would be helpful.<|endoftext|>The authors introduce a GCN based hierarchical model for protein function prediction. # Overall ImpressionI believe the proposed method to be interesting and the contribution to the GO embedding technique to be valuable for the PFP communitt. This article proposes an interesting architecture for PFP, but the evaluation is not very complete, and the explanation of the method is very confusing. However, I found the paper very difficult to read, and I have severalconcerns about this study, which I detail below. Notably, the authors decided not to compare with GOLabeler, the best performing methodfrom the CAFA3 competition. There are several grammatical and words that seem to be expressing the wrong idea.<|endoftext|>This paper proposes a novel PFP model that combines a pre trained Language Model (LM) and GCN based model including new node wise representations, to improve Protein Function Prediction (PFP) utilizing hierarchical features of Gene Ontology (GO) terms. ##########################################################################Pros:  The idea of this paper is very intuitive and understandable, including a language model for encoding the protein sequence and a Graph Convolutional Network (GCN) for representing Go terms. Overall, the paper is well written. The EXPERIMENTS section is well structured. ##########################################################################Cons:  The core idea of this paper is the PFP model, which combines a pre trained Language Model (LM) and GCN based model including new node wise representations. However, both LM and GCN are now very mature methods, and the combination of the two is limited novelty.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper starts with an interesting direction, to use 3D geometry to augment the 2D representation. From the technical point of view, it adopts SSL to maximize the representations of 2 views, so as to enable the 2D GNN to encode 3D geometry information, which can be beneficial for downstream tasks. This is an interesting and well motivated problem. Weaknesses:(1) The empirical result is the most important point, but it has the following problems. The empirical result does not match the motivation. If we ignore the motivation contradiction issue, then according to Method section, this work pre trains one 2D GNN and one 3D GNN, and downstream tasks include both 2D and 3D information. If the performance is about 2D downstream tasks, then why consider these tasks with 3D geometry available? This paper is an empirical work, so the empirical results are the most important. It doesn’t specify which datasets are used for the pre training baselines. (2) Some descriptions are not clear and terminologies should be specified more clearly. 1.GNN can be adopted on both 2D and 3D graphs, which is not clearly specified in the paper. 2.In Intro, `We pre train a GNN to encode implicit 3D information …`. However, as listed above, there are some key issues with the empirical performance (point 1).<|endoftext|>This paper proposes a 3D pretraining method for molecular property prediction. I am not sure whether this claim is correct. Then, it maximizes the mutual information between 3D summary vectors and the encoded representations for injecting 3D information into the representations. The pretraining phase ensures that the representations during fine tuning contain latent 3D information. The paper pre trains on three datasets, QM9, GEOM Drugs, and QMugs, and tests on both quantum mechanical properties (QM9 and GEOM Drugs) and non quantum properties (10 datasets, e.g.HIV and BACE). The model takes a 3D GNN as  teacher , while the 2D GNN learns from the 3D GNN to obtain latent 3D information. The method proposed in this paper is novel and sound. 2.The experiments are well designed. Yet some setting descriptions and results are missing (see weaknesses). Is there a specific reason for this? 3.What does PropPred mean in the paper? What do the properties of GEOM Drugs correspond to here?<|endoftext|>The authors present 3D Infomax, a graph neural network (GNN) pre training solution that leverages 3D information to generate better learned embeddings and improve performance on down stream prediction tasks where 3D information would be useful but not easily obtainable. The approach is useful for a range of downstream tasks involving molecules, including ones that are quantum mechnical, biological, and pharmacological in nature. The major strength of this work is the extensive empirical evidence supporting the claims that the proposed self supervised learning scheme (mutual information between 2D GNN and 3D GNN learned embeddings) significantly improves performance on downstream tasks as compared to other pre training strategies. In addition, the language is simple and clear, and the figures provide easily understandable graphical depictions of the method. Overall, I think this is a strong paper due to the compelling results and clarity of communication.<|endoftext|>The paper uses the 3D structures of molecules to pre train graph neural networks (GNN) representation of 2D molecules. This improves the molecular property prediction of some quantum mechanical (from 8 tested 8 were improved) and non quantum mechanical properties (from 10 tested 4 were improved). E.g.when the model is pretrained on a smaller set of elements and used for elements it has not seen this shows it learned the structure and not QM. Table 1: Is there a reason why two properties (cv and alpha) are better predicated with the pretraining of QMugs than with the same dataset (QM9)? 8 are given in the tables but often 10 are mentioned. Page 8 after table 4, last sentence of the paragraph: Incomplete and I do not understand it. There are two main advantages, firstly it can be used for a dataset where 3D information is not available and secondly it is cheaper than implementing 3D information in the representation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The manuscript introduces Bundle Networks which are neural networks designed with an explicit fiber bundle in the architecture. With this in hand one has an explicit description of the fibers of the fibers associated with a machine learning task. The model is demonstrated on both artificial data, showing that it is capable of learning the fiber bundles where these can be explicitly computed, as well as real world data sets where the fiber bundles are of interest. It would also add a lot if the task of learning the fiber structure was motivated more by e.g.an application where the fiber structure is helpful in analysing the problem. I think this is an important addition to the literature and has the possibility of being useful for future work about explainability and disentanglement.<|endoftext|>The empirical performance is promising on two synthetic datasets and two real datasets. 3) A family of deep generative models called a Bundle Network is designed. [Strengths] 1) (A new problem) The formalization of a new machine learning problem, i.e., the problem of learning fibers of a ML task. The proposed fiber learning problem is interesting and novel to me. However, the rationality of modeling the many to one relationship via the strict "product structure" seems not sufficiently explained. To avoid training a different model for each $U_i$ in $\mathcal{U}$, conditioning is used in the proposed BundleNet. The description of BundleNet seems not sufficiently clear. There are typos in this paper. This paper proposes a new problem, i.e., the fiber learning problem of a ML task, as well as an effective solution to it.<|endoftext|>It is unclear, which points come from training data and which ones come from the reconstruction. The paper introduces a new architecture for generative modeling, called Bundle Networks. Training the network then corresponds to learning the local trivialization of the fiber bundle, i.e., of the labeling function. The idea of the submission is quite interesting and I really like its presentation in the paper. It seems that the good performance is rather independent of particular hyperparameter choices and prior distributions, but mostly from the guiding idea of allowing the model to be a *local* (as opposed to *global*) trivialization, as indicated by the experiments with q 1. Bringing these ideas to machine learning is quite interesting on its own. As such the underlying concepts are mathematically well founded. So it is unsurprising, that the algorithm is able to learn this space. The method introduces new hyperparameters, in particular the fiber space $Z$, a probability distribution $\mathcal {D_Z}$ on the fiber and the number $q$ of local homeomorphisms (centroids for k means). Choosing these seems not obvious at all on non synthetic data.<|endoftext|>This paper proposes a neural network to model data (or machine learning tasks) with fiber bundle structure. Operationally, it corresponds to a covering of the label space $Y   \bigcup_{i} U_i$ by "patches" $U_i$ so that $\pi^{ 1}(U_i)$  are homeomorphic to product spaces $U_i \times Z$, with $Z$ being the fiber space. The proposed network is in essence a conditional generative model with the added twist that the conditioning is also performed on the "patch" that a label (or a regression target) belongs to (for training; inference is only performed in the "reverse" direction of fiber sampling.) The authors show interesting results in modeling familiar low dimensional bundles as well as real world datasets. I find this natural and elegant. One would need to go to the appendix, but even there you refer to some external resources. Empirically the proposed architecture seems to outperform conditional generative models. Since you re using an invertible network, it seems to me that $\text{dim}(y) + \text{dim}(z)   \text{dim}(x)$, but $x$ corresponds to points in the ambient space. I am not sure about the message: that including fiber bundle specific details in a model makes its performance worse?
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; This paper works on designing network architecture for object detection. The paper is well written and easy to follow. What is the detection head of the GiraffeDet? Ideally, Table 2 should only compare the same detector with different backbones, as the proposed contribution only changes the backbone. If the authors are not using the best detection head (or are using a better head than what is listed in Table.2), then it is necessary to show the proposed backbone can improve the best detection head. 2, the proposed GiraffeDet outperforms the better counterpart in the same block by an ~1mAP. Ideally, the authors should also compare to DCN based backbones as they are used in most detection papers. It would be interesting to see how the proposed neck works under a standard backbone. Overall the paper provides an interesting alternative to object detection architectures.<|endoftext|>This paper raises the question of whether it is effective to conventionally use deep (or heavy) networks trained on ImageNet dataset as backbone of a feature encoder for object detection. Instead, it introduces GiraffeDet, which has a deep neck network that processes multi scale representations with a shallow backbone. The argument in this paper was interesting and the approach (GiraffeDet) to addressing this argument seems to be effective for object detection. 3.Experiments supports the effectiveness of the proposed method properly. I do not have a big concern on this paper. I recommend to add an experiment comparing the proposed method to baselines when the neck networks are of equal depth.<|endoftext|>This paper proposes a new network architecture called GiraffeDet. The experimental results show the state of the art accuracy compared with other single stage object detection framework on the same FLOP level. The findings are very helpful for the community. The main contribution, in my opinion, is about the exploitation on the connections in skip layer and cross scale. But this contribution is minor.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; Another aspect that is worth investigating in my opinion is how to cope with noise in the training data, allowing the training of transformers with an approximate oracle. The authors are not proposing a novel technique for addressing the problem, but only report some experiments with different encodings for the matrices and a standard transformer architecture. In my opinion, the motivation for this approach is also lacking, since the reported experiments only consider very small problems that can be solved exactly. I wish that the paper considered instead cases that need to be approximated, or possibly prove that indeed transformers trained on smaller problems can generalise to much higher dimensions.<|endoftext|>The paper carries out a very thorough set of experiments on linear algebra calculations with transformers, using four different encodings of input matrices. Results appear to be complete, and the conclusion drawn from them are generally sound. After all, we do have algorithms for all linear algebra problems considered, and they work with 100% accuracy, perfect out of domain generalization, and faster run time. As the authors note in the discussion, at the current stage transformers have quadratic complexity in the number of tokens, which translates into $O(n^4)$ complexity for $n \times n$ input matrices, and this is asymptotically slower than the exact algorithms we have. A potentially interesting future direction (which perhaps can be advertised more by the author, to strengthen the claim that this paper is useful) is to investigate linear time transformers on tasks where the exact algorithm requires more than $O(n^2)$ time, so that perhaps transformers can be used to perform approximate computations with less time. I also have the following minor comments. This paper provides a thorough and well written investigation of the use of transformers to perform linear algebra computation.<|endoftext|>The paper also shows that some forms of out of distribution generalization are possible, and that this phenomenon is sensitive to the details of the training distribution. I found this to be an interesting paper overall. FramingI found the following claim to be problematic:  Our results on out of distribution generalization provide justification to the idea that models trained over random data can be used to solve "real world" problems . Second, in most "real world" problems, matrices are gigantic relative to the tiny context windows of dense transformers.<|endoftext|>The main findings are that transformers work surprisingly well on various matrix operations (addition, multiplication, eigenvalues, inversion, SVD, …) for small matrices (e.g.5x5), and that generalization to OOD problems is not symmetric (I.e.generalization from one distribution to another does not imply the other way round). This is a fairly comprehensive work on the idea. Thought provoking application of transformers. The models used in this paper have sometimes rather odd (small) hyper parameters. This work explores a wonderful idea: to solve linear algebra with transformer models. While these models use way more compute internally than the problem they are applied to requires to solve, it is an intriguing question whether these computations can be learned from scratch without further biases.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 3; A new self supervised learning objective is proposed to encourage the encoder to map behaviorally similar observations to similar representations without the use of reward signals. The overall model is named Cross Trajectory Representation Learning (CTRL), which achieves better generalization performance on the challenging Procgen benchmark suite. #### **Strengths**  The paper tackles a very important problem in the deep RL community. While various components that constitute CTRL were previously introduced in other works, I believe that the paper presents a novel combination of these methods and applies them in a novel problem setting. The proposed method is simple and can be easily plugged in to solve ZSG problems other than Procgen. The experiments are extensive, as the authors have compared CTRL to various baselines, including bisimulation metrics and SSL based methods. The proposed method also outperforms quite significantly these baselines in most tasks. If we utilize the reward information, however, we can avoid this behavior by forcing the encoder to not encode what is not useful to predict the reward. So the reward information in this case is actually helpful for learning a good representation. However, I want the authors to discuss in more detail about why their method works well in practice while in theory it can fail.<|endoftext|>This paper proposes a new zero shot generalization method for RL that uses a novel clustering+prediction loss to learn a cross trajectoriy encoder. The learned encoder is supposed to map behaviorally similar observations to similar representations. 2.The proposed method of training the encoder using clustering and prediction losses seems novel to me. 3.Experimental results in all 16 tasks in the Procgen benchmark show that the proposed CTRL generally achieve zero shot generalization. A lot of details are omitted from the main paper, e.g.the detailed definitions of the clustering loss and the prediction loss. Other than bigfish, bossfight and starpilot, the improvement made by CTRL compared to baselines is relatively marginal. I do not find a constraint to avoid degeneration. 2.What is the relation between this proposed clustering+prediction loss and other contrastive learning based methods such as CURL? 3.How does it compare to the Darla method[1]? The proposed method has some potential to help representation learning in RL and zero shot generalization.<|endoftext|>This paper proposes an extension of the MYOW (Azabou et al 2021) self supervised learning technique, combining it with SwAV (Caron et al 2021)’s method to perform online clustering, and adapt it for RL to assess generalisation performance on Procgen. It compares against several recent baselines (DBC, PSE, CURL, Proto RL and DIAYN), and outperforms them consistently on the tasks assessed (albeit by a small margin, depending on the reward scale of Procgen which I’m not extremely familiar with). I found the paper interesting, but rather hard to understand what it really contributed and how several decisions came about. 4.Table 1 is great and I would again like to flag that the number of baselines available are clearly to a great standard. The literature review is thorough and recent, and the overview of the algorithm in section 4 is also clear. 7.Figure 4 in the Appendix has T 3 and T 5, but they show no difference? Overall, I think this is nice paper, with good baselines and clear results, however, its presentation right now is quite lacking and hence I feel it may need some work before being ready for publication. 4.The main text talks about trajectories, but T 2 if the Appendix is to be trusted. What is your encoder? Appendix 8.2 presents it as if it is instantaneous, which is quite a limitation. MYOW explicitly used predictors everywhere and were never regressing towards the latent used to cluster (i.e.`y   f_theta(x)` in their paper.<|endoftext|>To this end, the authors propose a self supervised learning method, cross trajectory representation learning (CTRL), that captures behaviorally similarity in the trajectory representations. The CTRL objective has two components, one for online clustering and the other one for cross cluster prediction. CTRL is shown to be connected to bisimulation metrics in RL. **Originality and significance**:This paper addresses zero shot generalization, which is an important problem in RL. **Quality**:The overall quality of the paper is below the threshold for acceptance. ProcGen has two difficulty modes and the main text is not clear about which one was used in the experiment. I guess the easy mode was used based on the number of training levels. Second, I have some questions regarding the main empirical results that may undermine the empirical significance of this work. The original ProcGen paper recommended 25 million steps for the easy mode. Therefore, I don t think this paper is ready to publish yet. Although the main text says "...DAAC also exhibits good generalization performance...", DAAC in fact performs worse than PPO (6 wins, 9 loses, and 1 tie in 16 games). These results conflict with the results reported in the original paper by Raileanu and Fergus. One possible explanation is that the discrepancy is due to the lower training budget, but it then leads back to the first question of why using 8 million steps rather than the recommended 25 million steps. It verifies the importance of adapting the clusters online as the policy improves. I will be grateful if the authors can provide some clarification. Although the Sinkhorn Knopp procedure and MYOW are existing methods, the authors should provide minimum details in the *main text* so that the readers can understand what is going on.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; + The paper doesn t talk in detail about how the low level instructions are used by their proposed modular approach. The approach uses a language module that takes the high level instruction, predicts the instruction type and uses the oracle template for the instruction type to fill in the arguments (objects/receptacles) and generate a list of sub tasks to be executed on those objects and receptacles. Their approach demonstrates the ability to ground small objects on the top down semantic map and yields SoTA performance on the ALFRED benchmark. Or, are these high level goals such as "slice bread” or both? **Strengths**+ The proposed approach leads to impressive (SoTA) results on the ALFRED benchmark without using expert trajectories or low level instructions. Or, via some sequence models? But the description of the semantic search policy states that the GT location for small objects is computed every 25 steps, which seems to suggest an egocentric map. This suggests that the proposed approach is not able to make use of the low level instructions optimally. The authors should consider making this explicit in the text. This is also evidenced from the trends seen in Tab. Therefore, I feel that the paper is good/useful, but slightly below the acceptance threshold due to novelty concerns. **Some questions/clarifications/suggestions**+ Although not a critical issue at all (doesn t factor into the review), but I felt that the name of their approach FILM might get confused / overloaded with the visual reasoning model with the same name (https://arxiv.org/abs/1709.07871). + The text refers to ALFRED as an environment at one place.<|endoftext|>This paper presents FILM (Following Instructions in Language with Modular methods), a model built for the ALFRED dataset. 3) A semantic search policy which uses a CNN to map from the semantic map to subgoal object locations. Experimentally, the paper presents: 1) SOTA results on the task when only the high level instruction is given (low level instructions witheld). Despite this concern, I think this work would still be of interest to the community since it may also serve to highlight the need for improvements in high level planning/control in these types of visual language navigation tasks. I believe assuming to know the task type taxonomy is a strong assumption to make and one that makes me concerned about the method s ability to generalize to other domains (particularly those for which such a taxonomy would not be made available). Abstract: "This requires the use of expert trajectories and low level language instructions" — this statement feels a little strong to me and I would suggest qualifying it since not all existing methods require low level language instructions.<|endoftext|>Solving the task requires multiple high level actions, combined with low level actions and exploration. The submission uses precisely all these aspects to achieve a significant improvement over the next entry in the ALFRED leaderboard. On the other hand, the policy is trained only with the receptacle. I see the authors commented about them. In particular this one on the templates. >"FILM makes use of 7 highly specialized and ALFRED specific task templates. This takes advantage of the templated nature of the tasks in the ALFRED environment, somewhat trivializes the language understanding task, and will not generalize to new tasks without new template engineering." I also became more concerned about novelty. 1.Parsing the instruction. In summary, the paper proposes a modular approach for Vision Language Navigation tasks. The assumptions are heavily customized for ALFRED, but some general lessons can be used to other domains. The most interesting contribution was the level of abstraction of the semantic map and policy that allows separating the exploration and exploitation from the natural language instructions and from acting given the current situation and information in the map. So, I propose acceptance. The granularity of the semantic search policy might be hard to control.<|endoftext|>This paper presents a modular system (FILM) for egocentric instruction following in the ALFRED environment. The system does not require expert trajectories and can operate without low level instruction sequences. NLP module mapping high level natural language instructions to low level instruction sequences is very effective and maintains most of the performance of the system that has access to low level instructions. This module is inspired by previous work on egocentric navigation. The new semantic search policy provide small but consistent improvements over a version of FILM that has this module removed. However, it does make use of templated mappings from 7 high level goal types to low level instruction sequences, and these are quite specific to the tasks in ALFRED. The semantic map building module, and within sub goal deterministic policy are motivated by previous work. However their incorporation into the FILM system is novel.
Reject; rating score: 3; rating score: 5; rating score: 8; Here the authors consider the problem of k means where the distance function is given as a kernel. They show that coresets exist for this formulation, and so a brute force algorithm can be used to solve this problem. The technical framework is as follows: The authors present a known embedding from Hilbert to Euclidean space, and then the coreset of Braverman et al.satisfies the required approximation bounds. The theoretical contributions here seem very slight, more like observations, and I don t see the paper having the type of underlying innovation I would expect from ICLR. Also, I wasn t so impressed by an error of 10% using a coreset of 1000 points (although this application is somewhat outside my area of expertise). typo on page 6: "importane sampling"Nice result, but in my opinion not innovative enough for ICLR.<|endoftext|>The size of coreset is independent of the number of points and construction time is near linear in k. The authors use the coreset to get a (1+\epsilon) approximation for the kernel k means. Overall the paper is not difficult to read. However the theory results are heavily based on existing coreset techniques for classical k means and  I am not very sure if the paper has enough technical novelty for ICLR.<|endoftext|>The technique can be thought of as a generalization to the well known technique $D^2$ sampling technique of David Arthur et al.due to the incorporation of the feature map. While the paper is theoretical in nature and admits strong plausible results, I have some concerns:1. Even though the problem was not the main contribution of "A PTAS for k Means Clustering Based on Weak Coresets" by Feldman et al., it was still mentioned in this paper. Another coreset paper mentions a coreset for kernelized k means (for some type of kernels) where the coreset s size is exponential in k (see "Turning Big data into tiny data:Constant size coresets for $k$ means, PCA and projective clustering" by Feldman et al.). However, the best feature associated with your paper is the generation of coresets of smaller size with respect to any kernel function. However, Algorithm 3 requires its explicit existence which forms a bit of a problem.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; This paper presents a new approach to simultaneously preserve user level DP and prevent text extraction attacks in prediction. First, the reviewer does not understand why the proposed approach defends against the text extraction attacks without releasing the trained models? However, why next token is an MLaaS application and in which real world scenarios? Also, the connection between user level DP with text extraction attacks is weak. Second, the threat model is missing. Third, the user level DP provided in the proposed approach by partitioning users’ text data into different parts and sub parts is incorrect. Also, the assumption that if two models return similar results when they do not memorize the context of a query $x_t$ is very vague. It is unclear to me when this could happen and how we can quantify the similarity here. can be a representative measure for information leakage under data extraction attacks? However, there is room for improvement.<|endoftext|>The problem setup for the paper is when pretrained large language models are available, and they are to be fine tuned on private data for the task of next word prediction. Therefore, they introduce an ensemble based approach: partitioning the data into disjoint subsets, such that each user s data appears in only one partition. 3.Not providing any comparisons with prior existing work in NLP. If there is disagreement, the output of these models is mixed, along with the output of a publicly pretrained LLM, so as to bound the private leakage (kind of similar to PATE, but for language modeling). I applaud the authors for focusing on the problem of privacy in language modeling, however, I have some major concerns about the assumptions made and the actual applicability of the proposed method, that I will list below:1. Faulty assumptions in the experiments: The paper has defined their setup to be "finetuning" a pretrained language model. In finetuning tasks in NLP, it is often the case that training data is scarce. Therefore, they might all predict it, and agree on it, and this could leak that string. This is not  just a few missing references. I have provided a list below:1.<|endoftext|>The paper proposes a method for answering next token queries in a privacy preserving manner. As opposed to training models with DP guarantees, it adds privacy preserving guarantees during the prediction time. The approach works by getting a public model and then fine tuning it on disjoint parts of the private dataset. This way it obtains, several models. The paper is well written and experiments show that the method works in practice. The visualizations are also great. The paper studies a difficult problem as utility of next word prediction suffers a lot when trained with differential privacy. However, as written now it is not clear if the benefit in experiments is due to fine tuning or due to the new method of training/prediction proposed in the paper.<|endoftext|>Current solution to this issue is based on differential privacy, such as DP SGD. In this work, the proposed approach called SUBMIX focuses on private prediction for answering next token queries with regular language models fine tuned on the private corpus. Since this work uses non private language models for fine tuning, is it under risk of member inference attack? Sentence level privacy in the language model is a very challenge problem since LMs applied to many tasks in NLP. To this end, Submix provides a scalable and flexible approach to address the issue by focussing on next token prediction, which is popular for many downstream NLP tasks. Hence, I recommend to accept this paper.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper studies the problem of adversarial domain randomization for urban scene semantic segmentation. The style augmentation is used when training on synthetic images from Synthia and GTAV datasets. This paper demonstrates promising results when applied the trained model to real semantic segmentation benchmarks. * The idea is clearly stated and straightforward with source code included in the appendix. * At a high level, AdvStyle is a data augmentation method that generates unconstrained adversarial examples. Please elaborate on this in the rebuttal and incorporate the comparisons in the future version of the paper.<|endoftext|>This paper aims to address the domain generalization problem in semantic segmentation. Starting to synthetic source dataset, the authors propose to apply image level style transfer to bridge the gap between source and unseen real dataset. In experiments, the authors show that the proposed method can:1. achieve SOTA performance on multiple real datasets 2. generalize well with various backbones3. cover the data distributions of real dataset wellStrengths:1. The image level style transfer part is intuitive 3. My main concern is about what is really learnt in this one update? Then under this scenario, the segmentation model do not need to learn anything but stay with what it has been initialized with would be the right answer for the model.<|endoftext|>The paper introduces a domain generalization method for semantic segmentation, where one synthetic data is given as the source training dataset and the model is tested on unseen real datasets. The authors propose a simple yet effective method by introducing adversarial style augmentations. However, the authors should address the raised concerns regarding the technical clarity and experimental comparisons. ##########################################################################Although the proposed augmentation method is simple.<|endoftext|>The approach is based on the idea of adversarial style learning and the training of a model using both original and adversarial images. I believe the approach is well evaluated, making the right choices of synthetic datasets (training), real datasets (testing), semantic segmentation architectures, and baselines (e.g.ISW, which is one of the best performing approaches for domain generalization in semantic segmentation). ##########################################################################Questions during rebuttal period:  I don’t have major concerns with this paper. However, I have a recommendation/thought for the authors. Why focus just on driving scenes? It should be relatively straightforward to evaluate this approach in other types of scenes, for instance, indoor scenes, by leveraging NYU dataset + SceneNet or InteriorNet.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 5; Motivated by the structure mapping theory, this paper proposes an ad hoc solution to address the classic RAVEN problem. The authors clearly know a lot about the literature about analogical reasoning and RAVEN. Technical details are largely missing. There s even not a single equation in the entire paper. This makes it difficult to believe that the proposed method would work as the authors claimed. And if it does, it poses additional questions on how general it is to other analogical problems in general. It s hard to see if the proposed method is indeed better than prior work in terms of performance. This should be the best evidence to see if the proposed method is better. For instance, there s no explanation of what "filter" means in Figure 1. Bar plots in Figure 6 should have standard diveration or some other metrics in addition to the bar. Good literature review and motivation, but difficult to tease out the technical contributions with weak and ambiguous results.<|endoftext|>This paper tackles the problem of analogical reasoning. Then, given an incomplete sequence of two images from the target domain, the third image must be chosen from a list of four possible candidates. The engine is trained using the ground truth candidate labels. The paper presents an experiment to test systematic generalization, in which particular attributes are held out during train time. # Strengths and weaknesses  strengths:    Clear explanation of motivation    The ability to reason with analogies seems like an important step towards intelligent systems that reason in the same way that humans do    The proposed method performs well against existing work and the presented baselines  weaknesses (see detailed comments below):     The information available at training time may have made the task too easy, so that meaningful comparison with existing work cannot be made    The setup for the task seems very simple, and it doesn t seem appropriate to call it "analogy learning"     Claims about the effectiveness of NSM s components could be better supported by ablation studies    NSM achieves good performance when semantically contrasting alternatives are not available at train time. # General comments:  Sec 4: The task as it is presented in this work seems strictly easier than the task attempted by Hill 2019 or any of the baselines. In the NSM approach, this information is provided in a fully supervised manner at train time. Building this into the method seems to make the problem much easier. Thus, a fair comparison with the baselines cannot really be made, since none of the other baselines have access to the ground truth relationship at train time. Sec 4: Additionally, this approach does not scale well with increasing relationship types. It is claimed that this shows that the encoder is an essential part of the system. However, this is not necessarily the case. It could be that there is some quality that makes the encoder fail and also makes the inference engine fail. Perhaps this corresponds with a type of relationship. One way to check for this, would be to stratify results by relationship type. Sec 4: "Instead of depending on explicitly labeled candidates for mapping relational structure,"    Can you explain what this means? Only the predicted relationship matters for the Analogy Inference Engine, so why is multi task learning necessary? In Appendix A, it seems like both layouts perform equally well across all types of relationships. But if the authors work on a version of this study, in which the relationship types are latent/unobserved at train time, I think it would be a very promising step towards better analogical reasoning systems.<|endoftext|>Presenting them all in this way implies that the Visual Relationship Encoder might be somehow performing different types of systematic generalisation, when this is really not the case. The correct way to run this experiment would be to artificially provide the Inference Engine with all possible relationship labels (a different  fake  relation label per run), and then evaluate whether the Inference Engine performed better when the relationship provided matched the true underlying relationship. The architecture in the second network (whichever is chosen) receives the ‘source’ relationship and two ‘target’ scenes before trying to predict which of a set of 4 candidate  ‘target’ scenes completes the visual analogy between source and target. However the results were not thoroughly analysed, and there was a  lack of clarity in the explanation of ideas that made it hard to situate this contribution of this paper in the literature. The authors show that their model (which builds in additional architectural structure) performs better at a subset of tests than more general architectures. For example, the main paper that this work is based on seems to be somewhat misrepresented at several points in the text. “It is not feasible to build or curate datasets to always exploit the structure mapping prior”, “their key contribution was to introduce the SMT prior into the dataset…”. It is not clear what the authors mean by “Structure Mapping Theory prior” in these and other places in the text, but it does not give a good intuition for what this highly related piece of work contributes to the literature. This is a fair point to make, however here the authors are proposing a new architecture that they also train and test using the contrast method illustrated by Hill et al, so it is not as though this paper proposes an alternative to class contrasts in training. Interestingly, in the appendix the authors show that the adaptivity of their inference engine does not enable better analogy inference than either of the static engines.<|endoftext|>This paper targets the problem of abstract reasoning, with a special focus on the task of learning visual analogies. The authors made further discussion on these experimental results to support their proposals on model designs. This paper focused on learning abstract visual analogies, which is an important problem for machine learning models to be able to generalize. To illustrate the effectiveness of learned analogies, the authors chose a carefully designed benchmark that aims at solving generalization problems under the domain of RPM problems. Given that the benchmark, generalization tests, and learning setup (learning by contrasting) are defined in prior works, the major contribution of this paper should have come from model design and learning method, but the novelty of the proposed model is somewhat weak. This makes the proposed two structural layouts a bit weak as (1) it lacks semantic information when compared to modular networks (does not induce anything we could use for other similar tasks), and (2) it might not meet the need for more complex scenarios (e.g.real world analogies) as a structural mapping engine. This is somewhat supported by the fact that ResNet Parallel could be achieving similar results under certain training settings. Given these facts, the proposed model seems to be constrained on the current task and does not have strong potential as a general solution to visual analogy learning. (in this case, perhaps ResNet Parallel or WReN?) Is it because of the joint training of later modules or distribution shift in the benchmark itself? (3) There are minor errors on paper writing including typos and figure errors (e.g., the highlighting candidate in Figure.1 is incorrect). Therefore, I recommend a rejection at this time with the hope that the authors can make this submission stronger in the next version.<|endoftext|>A model called Neural Structure Mapping (NSM) is introduced to solve the task of abstract visual analogy making. The NSM model consists of a visual relationship encoder and an analogy inference engine. On the dataset proposed by Hill et. I like how the authors motivate the work in a way that can be connected to the classic structure mapping engine, as this theory is an agreed on approach in cognition for how analogy is made. Using a neural modular architecture is anything but new in the reasoning community. What s worse, there are only two possible network layouts, a much simpler design than existing cases in VQA. Therefore, I only see this work as an application of the existing modular method in a new domain, with very little novelty. al.is exactly the modular approach, despite the fact that they call it DRT instead of neural modular network. And that model was among the very first trials in such tasks. I also notice that the dataset used is an incomplete version the traditional Raven task. Besides, if you only need the relation, why bother predicting object and attribute? I m concerned on the novelty of the work and the evaluation performed. The DRT may not fall into the modular network domain though the idea looks very similar. 2.I cannot quite connect the model and the structural mapping theory despite other reviewers  opinion.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper proposed a training free solution by using only good representations to detect noisy labels. The author designed two detection methods of voting and ranking to filter instances that are likely to be corrupted. The whole method is based on the assumption that good representations can distinguish the noisy and clean labels.<|endoftext|>The authors propose a training free approach to detect samples with noisy labels by leveraging representations learned from pre trained models. The authors then argue that samples in the pre trained manifold should be closer if they share the same clean label, therefore one can use (a) local voting or (b) ranking methods to detect samples with corrupted labels. To justify the effectiveness of the proposed method in practice, it is crucial to test the algorithm on more complex and larger datasets. However, the analyses are based on a strong assumption that nearby representations should belong to the same true class. This might be true for simple CIFAR10 with a balanced training sample across 10 classes.<|endoftext|>The paper proposes a training free instancewise noise label detection method. Besides, the author only perform experiments on CIFAR10 and CIFAR100 instead of the acknowledged noisy label datasets, which is not convincing enough for me. It seems that the prerequisite of the proposed method is “given good representation”. The related work is too brief to reflect the association and difference between the proposed method and the existing methods. Followed by that,  the authors perform a local voting and global ranking based scoring system to detect the corrupted labels.<|endoftext|>This paper proposes a method to detect noisy labels given good representations (e.g., ones pre trained by contrastive learning approaches). The proposed noisy label detection method uses  the neighborhood information defined by a good set of representations in two ways: 1) checks the noisy label consensuses of nearby representations2) scores each instance by its likelihood of being clean and filters out a guaranteed percentage of instances with low scores as corrupted ones The work provide definitions for good representation and further proves a worst case error bound for the ranking based method given a  good enough  representation.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; My final major comment is on the writing. The quality of the language is rather poor, with issues in almost every sentence, to the effect that the final paper feels very rushed. The present work instead focuses on natural language generation, an inherently sequential task, which introduces new challenges. This paper provides results in the very interesting direction of private language models. Overall, presentation issues in the submitted version are to the level where I would not recommend anyone to read this until it is better polished. As a result, I commend the authors for taking steps in this direction. As I enumerated above, there are a few modifications to the basic PATE setting. It may be helpful for the presentation to keep going with the running example demonstrated in Figure 1, and how the procedure would work with this concrete instance. However, the utility is still very far from the non private utility. Of course the latter two works appeared only after the deadline and thus were impossible to compare with, so perhaps one should disregard them, but the story of both papers seem to be at odds, which is puzzling to me. It seems to say something more about the nature of the dataset. The choice of P 3 or P 4 is not clear as a privacy metric. The canary approach used in the Carlini et al Secret Sharer paper seems more meaningful to me. Comparison with prior methods seem a little mixed. But the approach and findings don t appear significant enough to warrant acceptance in ICLR right now. The paper is further hampered by the very poor presentation.<|endoftext|>There are some significant challenges in adapting PATE to this framework. Text generation or next word prediction has too many labels and a direct application of PATE doesn t work. This is debunked conclusively in the two recent papers by Li et al 2021 (https://arxiv.org/abs/2110.05679) and Yu et al 2021 (https://arxiv.org/abs/2110.06500). We also note that these are very recent and possibly concurrent with this work. (2) I am also not convinced by the experiments in Table 1. The authors compare their approach to the baseline DPSGD+$\tilde{D}^{pub}$. That is we are fine tuning the pretrained GPT2 on $\tilde{D}^{pub}$ and then fine tuning on $D^{priv}$ using DPSGD. I will be more convinced that their framework works if they compare against this benchmark. (3) Also to claim that, their method is beating DPSGD, they have to demonstrate they did a systematic search of the hyperparameter space for DPSGD. But I didn t find any evidence of this in the paper. Because I am not fully convinced the by the claims of the authors, I rate it marginally below acceptance threshold.<|endoftext|>Specifically, the authors propose SeqPATE, which applies the PATE framework to large scale language models such as GPT. 2.The paper is overall well written and the method is easy to follow. 3.The authors propose a new evaluation metric P N to indicate how many generated n grams can be found in the private set, which is an interesting and useful indicator in addition to PPL on the private set. My main concern of this paper is whether SeqPATE can achieve satisfactory utility preservation given small epsilon (say epsilon 2). From Table 1, the B 4 score of SeqPATE is different from that of the upper bound by 2 orders of magnitude, which is a bit concerning. 2.The advantage of SeqPATE vanished given a large epsilon compared with DP SGD, which makes the algorithm less scalable. It is better to provide some quantitative evaluation on SeqPATE on these settings. 4.Some experimental setups are a bit unclear. This result is a bit weird and counterintuitive.<|endoftext|>Then it applies the PATE framework to privately train a student model with several algorithmic innovations, including (1) aggregating teachers  models by averaging their output distributions; (2) reducing output space by top k/top p selection; (3) efficient knowledge distillation by only querying the teachers  models when the student model performs poorly. The paper also provides extensive experimental results and shows that it outperforms other DP learning algorithms when the \eps is small (<5). The experiments demonstrate the practicality of the algorithm. This is a timely and interesting topic and has many applications, for example, the smart compose. In terms of novelty, SeqPATE has several non trivial changes compared to the classical PATE. Weaknesses: I have a few minor comments. In practice, the choice of \eps depends on the data, so could you offer any instructions on choosing the algorithm when \eps is not very small (like between 5 10, which is also common in practice)? The writing is good, and the approach is clearly presented.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposed a method to train a once for all network, where one network can run at different resource constraints. 2.The paper is easy to follow. 2.The results seem to be not consistent with that reported in the paper. My concern about the novelty and significance of the method and results still remain. I agree that combining existing techniques may need more tuning.<|endoftext|>This manuscript aims to reduce the training cost of once for all networks. But, I find the novelty of the proposed method is a bit weak, and the generalizability of the method on other design spaces and transfer learning is unclear. 2.This paper only presents results on one design space (mobilenetv3). Although this paper presents good results, the novelty of the proposed method is a bit weak.<|endoftext|>In this manuscript, the authors propose a new framework for training the once for all (OFA) networks. The paper is overall well written with good performance. It is not clear how each component in this paper contributes to the final performance for the reduction of training time, the author should do an ablation study to justify this. The paper is overall well written, but the novelty for this paper is kind of limited. The author should prepare more experiments to justify the performance and address the technical contribution for this paper.<|endoftext|>This work proposes new methods of Once for all(OFA) network training methods, which significantly reduce the training time as compared to the existing OFA methods. Strength:  Overall this paper is well written and easy to follow. Lacking the deeper explanation of the improved results. **Limited novelty**. ** In the results part, it seems to me that the proposed methods just magically improve as compared to on the baselines. Would be nice for the authors to point out as I might miss them.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 3; The paper gives theoretical grounding on the proposed certifications and demonstrate empirically the certifications are sound. overall I am convinced this paper is solid based on its problem statement and empirical evidence, but I will defer to someone who can formally verify the math better. Overall I find the premise of the paper solid, and its empirical evidence well documented and solid. for instance, imagine we train a maze world agent on slip free environment, and want to certify some of its properties in real life where the world might be slippery?<|endoftext|>The certification is mainly based on the randomized smoothing technique. This paper is well written. [3] Zhang et al."Robust deep reinforcement learning against adversarial perturbations on state observations." The authors have provided methods to address the challenge of estimating these terms, but it also seems to be expensive. Then the certification can not serve as a strict lower bound of the cumulative reward. In many literature[3], BankHeist and RoadRunner are used in addition to Pong and Freeway.<|endoftext|>This paper introduces CROP, a framework for the certification of reinforcement learning (RL) agents against (adversarial) observation/state perturbations based on randomized smoothing (RS). Furthermore, the mathematical parts are well written and can be followed easily. ([3] is even discussed in the text.)<|endoftext|>This work proposes a certification (CROP) attesting that a policy is robust against adversarial state perturbation. "Action robust reinforcement learning and applications in continuous control." *Quality*Essentially, the two proposed certificates are based on a state perturbation to which either the Q function or the policy is applied. It is not clear which one of the two options applies. On a higher level, the fundamental RL question is not "how to certify robustness" but rather "how to attain robustness". Also, a comprehensive RL review and robustness in RL is strongly missing. If these are reward bounds, then a  factor seems to be missing  l. 113 114: I do not understand the justification for focusing on a finite horizon.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposes a method to estimate the multi dimensional intensity function of a Poisson process with a lower dimensional projection. The proposed method is motivated by perspectives of information geometry and generalized additive models in statistics. The paper is overall well written and I read it with interest. Could this be done with the proposed approach? Minor points: 1.<|endoftext|>The paper propose to apply generalized additive models (GAM) to learn the intensity of the multi dimensional Poisson processes (PP). The paper is not motivated and clear enough which leaves the reader perplexed. Motivations for introducing APP are not well explained. The presentation of APP, in Section 2.2, should be clarified. I would like to see experiments on the computation time of the approach compared to existing methods as it could be an advantage of this approach.<|endoftext|>This paper considers the problem of estimating the intensity function of a multi dimensional Poisson process. •	The paper is well written (I only found one typo!). However, given that this relationship is not quantified, statements like “Based on the Kolmogorov Arnold representation theorem, generalized additive models …” (pg 4) should be scaled back. 9: “The third order method is able to period better”The idea to decompose the function f into lower order contributions appears to be novel, and the link to GAM’s is interesting.<|endoftext|>This paper proposes a new model, called additive Poisson process, for estimating the intensity of a high dimensional Poisson process. I found the main idea of the paper novel and interesting. I believe the paper is well written and I particularly appreciated the theoretical guarantees that come with this model and inference scheme. In other words the number of lower dimensional projections in Eq 4? For the sparse case N should be 1000?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes a new semi supervised learning framework by introducing an auxiliary task that distinguishes whether the pseudo labels are truly labeled or not. Experiments on several simple benchmark datasets show that the proposed method outperforms some naive baselines. 2.The experimental section is not convincing and this is my main concern. The datasets and the baselines are too simple. State of the art SSL methods should be employed to support the claims. In particular, the uncertainty based SSL method [2] should be compared. As I have discussed above, the proposed method can implicitly be equal to existing techniques in SSL. Interesting idea, poor experiments and confusing derivation.<|endoftext|>The paper proposes a novel framework for semi supervised learning, that solves two issues of previous methods: 1) over reliance on labeled data and 2) error accumulation. It shows that jointly solving the main task together with another task (that discriminates whether the data label is real or not) leads to better performance. Strengths  The proposed framework seems to be novel. Weaknesses  Since the proposed method is only compared with the original pseudo label method, comparing with other extensions of pseudo labelling methods that are mentioned in Section 5 will make the contributions more clear. In addition to the papers mentioned in Section 5, there are a few papers that try to address the error accumulation in semi supervised learning methods that is observed in pseudo labelling. If this is correct, then why do we need M as an input in Algorithm 1 in page 6? I would like to also recommend to put the new experiments with UPS in the main paper instead of the appendix. The proposed method seems to have some nice benefits, but I feel there are a few weaknesses that should be addressed.<|endoftext|>The paper introduces Self interested Coalitional Learning (SCL), which is a novel approach to semi supervised learning. SCL combines the traditional self training approach to semi supervised learning with an auxiliary task that infers label observability. The empirical results show that, in a variety of scenarios, SCL outperforms both self training and the original model. This is an interesting paper on a topic with important practical applications: semi supervised learning. The paper would greatly benefit from an additional section that would provide an intuitive, illustrative example of how and why the proposed approach outperforms self training. Appendix B is extremely brief and not very helpful. please spell check the paper   eg, "perforamnce" on page 4  page 2:  please replace "more sufficient"  page 3: "jointly solving above two tasks"    >  "jointly solving THE above two tasks"  page 3: "there are some other works embody"  > "there are some other works THAT embody"  page 4: "are impacted the influence"  >  "are impacted BY the influence"  page 7: please replace "well learn"Overall, this paper uses a novel idea to improve the state of the art for semi supervised training.<|endoftext|>This paper proposes a new semi supervised learning method. Motivated by the error accumulation problem of typical self training paradigms, the authors propose to explicitly model the confidence of pseudo labels as an auxiliary task. Experiments demonstrate that pseudo labels are substantially more accurate with the new method and better performance of the main tasks at different label missing rates. The authors introduce a new SCL strategy to solve the problems, which can be applied to a broader class of learning problems. Cons:  Lack of experiments      The proposed method is only compared with the self learning method (with the same base learner). While this demonstrates how the model is improved with SCL, it is also necessary to compare with state of art SSL methods. Compared with original self learning method, the new method has an extra discriminator model, which are based on the same base learners as for the main tasks.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Autoregressive Diffusion Models (ARDM) are discrete diffusion models that extend D3PMs and generalize order agnostic auto regressive models (OA ARM) . The contributions are theoretically significant (generalization of ARMs) and are of practical use (parallelization of the OA ARDMs, depth upscaling). ## 1.Strengths  a. This is very clear how the method relates to D3PM.<|endoftext|>This work introduces a new model class combining elements from autoregressive and discrete diffusion models. The new approach is named Autoregressive Diffusion Models (ARDMs). The authors demonstrate that the new model has particular benefits compared to the autoregressive and discrete diffusion models in terms implementation efficiency, scalability and parallelization.<|endoftext|>The paper proposes Autoregressive Diffusion Models (ARDMs), which is a combination of two concepts: autoregressive models and (discrete) diffusion models. The proposed ARDMs generalize order agnostic ARMs [1] and discrete diffusion models [2]. No error bars are provided in the experimental result.<|endoftext|>This is a very dense paper that proposes a new autoregressive modeling (ARM) approach, which enjoys several properties. Then, one contribution of this paper is to improve the computational cost of order agnostic ARM. Instead, the reader is assumed to be familiar with the details, and the final discussion before subsection 3.1 does not help. This is important in my opinion, and not sufficiently exploited or emphasized, e.g.in the experimental section.
Reject; rating score: 3; rating score: 3; rating score: 5; This paper proposes to add an extra residual connection between a transformed word embedding and the final output layer, which easily generalizes over different recurrent architectures. The language model experiments show that their proposed model performs better than the same recurrent model without the residual connection, but the performance gain becomes marginal after optimal hyperparameter setups were used. + The presentation of the method is very clear. Sufficient details were given on the experiment setup and hyperparameter search. A very similar technique has been examined in a previous paper (https://ieeexplore.ieee.org/document/9207238), and similar experiments on PTB have also been conducted. The authors could have examined this by simply doing a version of the residual connection by directly adding the input embedding $x_t$ to the hidden state $h_t$, which will incur no extra parameters at all. + After hyperparameter tuning, the performance gain brought by the architectural change becomes very marginal. UPDATE  I have read the authors response and the other reviewers  reviews and do not think any change in my overall evaluation is justified.<|endoftext|>This work revisits the LSTM architecture. They propose to modify a recurrent architecture by adding a direct connection between the input and the output of the recurrent module, called "dual". They also consider a double layer LSTM, where the output of the recurrent module is obtained by the concatenated application of two LSTM layers. Hyper parameter tuning is reported in detail. I got a bit surprised by the improvement from the dual structure. Weakness:The used datasets like PTB or WT2 are small scale. Why there s no comparison with transformers? Not much analysis or explanation about why the dual structure is better is given. Below are some minor comments:I sometimes got confused by "dual" and "double" in the writing. Some possible typos:Sec1: "for the Penn Treebank problem " > PTB "dataset"? Comparison with transformers is not conducted, and the datasets are small scale.<|endoftext|>The idea is to insert a single additional layer into a network with one or more recurrent layers, just before the output layer (Eq.6).This is termed a "dual connection" and it combines the output of the last recurrent layer directly with the input to the network at the current step. The paper presents the results of adding this modification to both LSTM and Mogrifier LSTM networks on the Penn Treebank and WikiText 2 datasets. This is an experimental paper, and the authors have taken care to describe the setup in sufficient detail. There are a few concerns that make it difficult for me to accept this paper in its current form:1. It is unclear where the improvement is coming from. Would simpler skip connections provide a similar benefit? 3.Overall the experimental results are not strongly in favor of the proposed model. In Table 2, we find that dual layer leads to worse results without dynamic evaluation when added to "mdLSTM", but better results with dynamic evaluation. Only on the test do we see a small improvement. If 3 can also be addressed, I will increase the score further. On the state of the art of evaluation in neural language models. ————Update:I am thankful to the authors for providing additional results and analysis. However, I believe more work is needed to make sure that the increased capacity provided by the _dual_ connection can actually be useful in practice.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 3; # Post rebuttal As a result of the authors s detailed response, I update my review score to 8. # Weaknesses* There is no conclusion summarizing the main findings of the paper at least. * The domain discriminator has a single update per 1 update of feature extractor and classifier. * I am not sure if I understand the notation $q^Y(y) \propto \sigma(y)^{ \alpha}$. Does it imply that for example, we can have $q^Y(1) \propto K^{ \alpha}$ ?<|endoftext|>The result tables should be shown after the description of the methods, metrics, etc. Other settings are not demonstrated by your experiments and are not well motivated. This could have significant impact on unsupervised domain alignment and more generally on thinking about alignment problems. The paper does not include results on evaluating the support alignment explicitly. My intuition is that the objective will naturally move towards shrinking these values towards 1/2 as mentioned below in comments but maybe there is something else going on. This seems to be a fairly weak high level motivation for support alignment (at least in the introduction). Is this a correct intuition/interpretation?<|endoftext|>There are a number of works that attempt to address this problem (e.g., Binkowski 2019), but they come with a great deal of complexity. I feel like d(.,.) could have been clarified better earlier, at least to provide some examples to avoid the reader wondering about what it was until the experiment section. Binkowski 2019: Batch weight for domain adaptation with mass shiftThe paper is very clear, well motivated, the approach is simple, and problem is important.<|endoftext|>[In case, only supports are important, I wonder why SSD  needs their "weights" in its definition?] 2.It seems unclear about the motivation of using the 1d projection? The authors also illustrate the advantages of the proposed methods for domain adaptation. + Strengths:It seems that the authors propose a "new" divergence to align supports of distributions, and empirically show its advantages in domain adaption.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The authors formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). Particularly, they distinguished three sub tasks in CKL, i.e., the retention of time invariant world knowledge, the update of old knowledge, and the acquisition of new knowledge. They find that CKL demonstrates unique challenges that are not present in previous CL setups. The paper also introduced a novel metric named FUAR to measure the trade off between knowledge forgetting, update, or acquisition. Extensive experiments were conducted to benchmark the performance of various CL approaches (regularization, rehearsal, and parameter expansion methods) on different aspects of the CKL task (retention of time invariant knowledge, updating old knowledge, and acquiring new knowledge). However, in the experiments with GPT 2 (a decoder only model) in the Appendix, GPT2 MixReview (a rehearsal method) performs the best. After all, large language models take various forms and both T5 and GPT 2 are examples of large language models. Since the three methods (regularization, rehearsal, and parameter expansion) are not mutually exclusive, I was wondering if the authors have tried any combination of the approaches. For instance, the rehearsal approach could be combined with the parameter expansion method. I was wondering whether a combined approach would yield even higher performance. The paper formulated the problem of Continual Knowledge Learning and benchmarked the performance of large language models on this task with different CL methods. This work is a big contribution to the community and would invite more research into this topic.<|endoftext|>  The paper is about continuous learning for language models. They provided some findings on their continuous LM learning. Strengths   They provide a new benchmark and metric to measure the retention of time invariant knowledge, updated knowledge, and new knowledge. Weakness  In a real world scenario, how can one know in advance that the new task is truly "new"? As mentioned on page 5, it is possible that the FUAR score is very large if "no gain" and "preserve knowledge". The authors did not show experiments or analysis in such a setting, where the new task has some "knowledge overlapping" with the learned tasks. Maybe the GAP between vanilla and CKL methods will be smaller given larger models. This work is quite insightful for us to understand more about how LM continuous learning works, although I think more experiments could be beneficial as I mentioned in the weakness section. If we can make sure the numbers from this paper are reproducible and comparable, then I think it could be a good testbed for future research in this direction.<|endoftext|>The paper studies continual knowledge learning of language models, which is an interesting and important problem. Particularly, a new benchmark and a metric are introduced to quantify the retention of time invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. To establish baselines for the CKL benchmark and validate the rationality of the proposed benchmark and metric, the author conducts extensive experiments with a pre trained encoder decoder model (T5) based on various training methodologies including regularization, rehearsal, and parameter expansion methods. The paper is well organized and easy to follow. The proposed continual knowledge learning problem is quite interesting and important. The authors also conduct comprehensive experiments to verify the rationality of the proposed benchmark under the various settings. The experimental results will be more convincing if more pre trained language models (such as GPT) are included. For example, we can explore the ability of different PLMs to avoid catastrophic forgetting and to acquire new knowledge while preserving invariant knowledge. However, only a two phase setting is considered. More experiments can be explored, such as five phase or controlling the differences in the distribution of data at different phases. 3.The experimental findings in this paper are somewhat trivial. The paper studies an interesting problem. However, there are some concerns about experimental settings.<|endoftext|>This paper presents a new continual learning problem setup: continual knowledge learning (CKL) and constructs an associated benchmark resource. The authors show the empirical performance of some existing CL methods, ranging from regularization, rehearsal, and parameter expansion. And they show a few findings based on their experimental results, e.g., learning rate can be sensitive to balance the tradeoff between forgetting and learning new knowledge, and CKL methods might have transferrable performance across different LMs. I like the clear separation of the types of knowledge: time invariant (to keep), outdated (to remove), new (to inject), as well as their collected datasets for reflecting the three types of knowledge update. Also, the associated tasks in UpdatedLAMA and NewLAMA should reflect such a time series   that is, the streaming version of the current probing tasks. The current formulation described in Section 3.1 only has a single time step. It s also hard to justify whether such methods can maintain performance in general NLP tasks. The analysis of the CKL methods is not deep enough. How do we know if an arbitrary new fact conflict with the existing knowledge or not on the fly? In the current problem setup, how will such "conflict" be defined? This paper is a pilot study of an improtant problem (continual knowledge learning of LMs), but the problem formulation is overly simple and there are still many important yet missing points in both data construction and experiments.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; *The paper clarifies or proposes a new criterion to quantify generalization performance? If these are the two criteria that are going to define generalization, or are distribution shifts also being considered? Argument and ideas are developed step by step, formulated and substantiated formally, based on previous work. It looks as if the samples were mislabeled. If the equality doesn t hold, the problem definition is more about data with wrongly labeled samples; in this case, the insights from section 3 will be contingent on the number of samples for which the equality mentioned above doesn t hold (i.e., mislabeled samples). Empirical evaluation shows that the proposed method is more robust to noisy labels. **UPDATE: After discussing with the authors (see thread below) I have updated my review to reflect my view of the paper.<|endoftext|>Please try to add ImageNet results if possible, as it may help others in the field who use it as a common benchmark. I have also read the other reviews and the authors have also addressed them well too. 6.Experiments with noisy labels are well performed and provide good evidence for the theoretical ideas as well as FilterKD. Results of regular KD in addition to ESKD. Hopefully, the authors will be able to address the weaknesses I have outlined. For now, I am giving this a 6: marginal acceptance. I am happy to increase the rating based on the authors  response to the next level. I am satisfied with the additions and modifications and I am increasing my score.<|endoftext|>IIUC, the intuition from Section 3.3 is that the reason for the “zig zagging” in Figure 3 is that the label assigned to a given point changes not only based on the gradient of the loss for that point, but also on the gradients of the losses on other similar points. It first experiments with synthetic Gaussian data. In terms of novelty, clarity, and significance, I think this paper is good. However, the evidence provided is not quite sufficient to validate the claims made. The suggestion that zig zag in the learning trajectory is important to the success of distillation on real (non noisy) data is not well demonstrated, and the benefits of the proposed FilterKD technique seem marginal on real world data. The question of why distillation improves performance is an important one. The improvements from KD on these datasets are small to begin with, so they may not be particularly good benchmarks for this work.<|endoftext|>The improvement obtained by early stopping and the proposed FilterKD is rather minor. It is based on NTK theory but it is applicable only for a sufficiently wide network. Also, several "zig zag" patterns are shown in figures, but we are not sure of their definition and how often it appears in the real dataset.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; This paper proposes "learning via retracing" as an approach to learn state representations, through matching a trajectory both in the forward and in the backward direction. This paper then introduces Cycle Consistency World Model (CCWM) which is a model based RL algorithm which learns through retracing. Furthermore, it proposes a value based approach to identify "irreversible" transitions. Extensive experiments to evaluate the idea and interesting ablations and investigations described. Does this mean that all plots in Figure 3b for CCWM are trained on all transitions, with no truncation? Please clarify, as I imagined that the experimental evaluation of your method included both contributions (CCWM+adaptive truncation). In algorithm 1: I don t understand in the input, why in ${O^n_{t_n:t_n+K}, a^n_{t_n:t_n+K}}$ the $t$ has a subscript $n$, which should represent the n th sample in the batch. Also, I think there is a typo and the subscript should be to $t+T$ and not $t+K$. In figure 8 (from Appendix F), you explain how CCWM yields less accurate predictions for the irrecoverable states, but to me the first row (true trajectory for Walker), doesn t look like an irrecoverable trajectory. I like the idea and I think it s a novel approach for state representations in RL. My major concern is about the clarity of the paper, as written above. Hopefully the authors can address my concern in the rebuttal period.<|endoftext|>This paper considers state representation learning problem in deep RL. It exploits the cycle consistency supervision and develops a “learning via retracing” approach. Learning from predictive supervision from temporally forward and backward directions reveals information from both the future and past to the target state, leading to more accurate latent state inference. In particular, the paper proposes the Cycle Consistency World Model (CCWM) along with practical considerations (e.g., adaptive truncation to remove irreversible states), under the model based framework based on generative dynamics modeling (CCWM). Therefore, more thorough discussion about the novel contribution relative to PlayVirtual should be included. Why the continuity detection on action value function could detect the irreversible states? It seems that there is not much discussion on this point in Section 3.3. It is crucial to clarify it. VirtualPlay should also be included as a baseline in the experiment section in order to show which approach best exploits cycle consistency. The paper proposes CCWM, a learning via retracing method, based on cycle consistency constraint. It leads to better sample efficiency and final performance by learning better state representations. Need more clarification about its novel contribution relative to a recent work (VirtualPlay), which considers cycle consistency learning in model free RL setting.<|endoftext|>This paper proposes a self supervised approach for learning the state representation of RL tasks. The main contribution apart from prior works is to involve additional  retracing  trajectory samples in representation training, which are trained by minimizing the propensity between retraced samples and forward posterior samples. They also proposed an intuitive way to mitigate the irreversibility in RL dynamics. The strengths of the paper involve solid experiments and well structured writing. However, I think some details are not illustrated clearly, so that I have the following questions:1. I think it might fail in heavily irreversible environments. I think the performance of CCWM is comparable with Dreamer given the huge additional computational complexity. The proposed representation learning method for RL is interesting, which involves addtional retracing samples for training. However, more explanations are needed to make it clear.<|endoftext|>The paper investigates a self supervised approach for learning the state representation in RL tasks. The authors claim that this facilitates stronger representation learning and improve upon the sample efﬁciency of learning. As it is not always possible to find such a cycle consistency between two states and for counteracting the negative impacts brought by the “irreversible” transitions, the authors add a novel adaptive “truncation” mechanism. The motivation for the cycle consistency is not really discussed. How is the 2 wassertstein distance estimated from data? The "model free instantiation of learning via retracing" (appendix C) is not clear. In particular, the paper states that there is a graphical illustration "shown in Figure" but there is no reference to any figure. Section 3.3: The prediction of the irreversibility of a transition is not very clear. In the conclusion, it is written "We empirically show that CCWM yields improved performance over state of the art model based and model free methods on a number of challenging continuous control benchmarks, in terms of both the sample efﬁciency and the asymptotic performance." Can you clarify whether that is the POMDP setting? If it is not the POMDP setting, it might be worth clarifying as this is the usual denomination used in POMDP.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The paper proposes a new algorithm for learning linear SEMs using Cholesky factorization of the covariance matrix induced by the SEM. This is not the case with the algorithm proposed in Ghoshal and Honorio whose sample complexity grows as $O(\mathrm{poly}(d) \log p)$. In the experiments, the authors compare the performance of their algorithm against LISTEN (Ghoshal and Honorio 2018) only at 3000 number of samples. However given that existing algorithms from Ghoshal and Honorio 2018 and Chen et al 2019 are already polynomial time and are comparable to the proposed algorithm in terms of running time I feel this is not significant.<|endoftext|>This paper proposes a method for recovering the causal graph of additive linear models from purely observational data, under some an identifiability assumption, that seems to be related to the forward step size assumption of [1]. However, the relation to existing work is not made clear enough in my opinion. On the theoretical side, the claimed improved sample complexity compared to existing results seems unfair.<|endoftext|>This paper develops a new algorithm for learning linear structural equation models using cholesky factorization. This paper explains that the proposed algorithm is consistent in high dimensional settings and computational feasible. Strength : The paper provides thorough discussion of previous studies. (2)	The require conditions for Theorem 2.1 appear to be unrealistic. (3)	The numerical experiments do not support the theoretical findings of the paper. According to Theorem 2.1., the sample complexity does not depend on the sparsity level of a graph. This paper has a good idea of learning linear SEMs.<|endoftext|>The paper works on causal discovery in the linear Gaussian case, on which the identifiability is based on (Peters & Bühlmann, 2014). The proposed method is based on Cholesky factorization and has better efficiency/time complexity performance than the related state of art methods. Moreover, it also provides a theoretical analysis of the resulted graph, which is appreciated. The experiments can support the claims. The correctness and soundness of the method are shown by the theoretical analysis and the experimental results. My only concern is that the work is based on a quite restrictive class of SEMs, of which the extension to the nonlinear cases or the more general scenario is not so clear.<|endoftext|>In this paper the authors consider the problem of learning directed graphical models in the linear SEM setting. The authors use iterative Cholesky factorization of the covariance matrix in order to learn the causal relationship from the data. The authors restrict themselves to the linear SEM setting under the assumption that the conditional noise variance for child nodes is larger than that of the parent nodes which ensures that the problem is identifiable. Overall I believe that this is a good paper, but I need a few clarifications. What approximation are the authors talking about? Overall an efficient algorithm to learn causal relationship.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper is generally well written and well motivated. 2.I like the idea   it is quite simple and elegant   learn representations that retain information about the controllable elements of the environment. * In some cases, it could just be a matter of rewording. For example "LCER is the first representation learning algorithm that enables the pixel based SAC to outperform state based SAC on the DMControl100K benchmark". I look forward to the author s response and interacting with them to understand their approach better. * In other cases, the claims are not tested. For example, there is no experiment to show that the representations indeed "retain the controllable elements of the environment" (which is the key motivation behind the work). 2.In equation 3, is only A conditioned on $\phi(S_t)$ or both $\phi(S_{t+k})$ and $A$ are conditioned? 3.In page 3, Section 4, the authors correctly point out "For example, in navigation tasks, the position of the goalis task relevant but not controllable by the agent" as a motivation for controlling the degree of compression. However, their experiments did not have any goal conditioned environments. In that case, would it make sense to directly use equation 3 (atleast in the context of experiments considered in the paper). Could they describe why is this approximation valid? I expect it to be much higher than the baseline (given it is used k step lookaheads to compute the losses.Reporting the runtime comparison will be useful for a holistic comparison. Similarly, I expect LCER to be using much more params and would like to see a comparison of those numbers.<|endoftext|>This paper proposes a novel representation learning flamework, LCER, which enables the agent to extract task relevant representations from image based observations. ## Strengths  This paper is well structured and easy to follow. It can be simply argued that LCER introduces the CMI term into CURL objective ($J_k^1$) with some ratio $\beta>0$, and also introduces the surrogate form of CMI based on JSD (Eq.10).In addition, balancing $\beta$ seems very important hyper parameters for LCER (I guess even in standard DM Control, where $\beta 0.1$ is used.If LCER learns controllable elements from CMI, $\beta$ should be 1, but isn t). I am also a bit confused about the connection and difference between LCER and PI SAC, since both of them optimize the CMI objective as an auxiliary loss for RL. While the authors just say, "PI SAC tries to encode predictive information, while LCER focuses on encoding controllable elements", further clarification about this is required to emphasize the contribution of this paper. I think the authors may also include the results of Dreamer (Hafner et al.2020) for 100k benchmark since it generally achieves better results than PlaNet; in addition, CURL and PI SAC compared against it in their original papers (+ code is available online). In distracting environments settings, the authors may also include Contrastive Variational  Reinforcement Learning (CVRL) as a baseline, proposed by Ma et al.(2020), which achieves better results compared to Dreamer, image based SAC, and image based D4PG (+ code is available online). In addition, some important baselines (PI SAC w/ gs 2 or gs 4, Dreamer, CVRL) seem missing in the current manuscript.<|endoftext|>Their approach is based on a mutual information loss between actions sequences and final states, and they validate their approach on a series of DMControl task where both high dimensional pixel observations as well as ground truth latent state representations are available. In their empirical results they show that:  their new approach (called LCER) is able to outperform alternative approaches on a set of these tasks. On the strength of their contribution, to me it seems borderline incremental, so I would be curious to hear the opinion of the other reviewers. On some tasks, it even outperforms an SAC agent with access to the ground truth states. **Strengths:**  paper was clearly written and well motivated  approach can be applied to any task  clear mathematical exposition of the loss function used  demonstration of better sample complexity against other baselines including CURL, PlaNet, SLAC, and PI SAC. I would also have appreciated insights into how much the representations are fine tuned to the policy being used. The main motivation behind the authors  work is to have an agent learn state representations that capture controllable elements of the environment only. Evaluating transfer on different goals in the same environment would be a test of this, since an agent that truly captured the controllable features should not be adversely affected by a change in goal or reward function. The authors  main modification is really to introduce action sequences into the picture, motivated by the idea of using controllability to guide compression, and in this reviewer s view, this contribution seems borderline incremental. Indeed, the improvement over PI SAC (Table 1) seems to be within statistical error for several of the tasks (cartpole swing up, reacher easy, finger spin, and ball in cup), which makes me wonder to what extent is the improvement attributable to implementational differences instead. Section 5.4 presenting the evaluation of representation transfer was especially weak.<|endoftext|>In order to allow some non controllable elements in the representation the paper proposes a blended objective which trades off representing action controllable aspects of state with non controllable elements of state using an application specific empirically tuned hyperparameter \beta. The paper is set in a fully observable world where a state at time t, S_{t}, is projected to an instantaneous latent state Z_{t}. The idea of controllable elements is implemented by computing mutual information between the latent state projection in the future, Z_{t+k} and the actions taken by the agent I(  Z_{t+k}; A_{t:t+k 1} ). The paper then shows that this objective can be combined with a standard soft actor critic architecture to learn representations without extrinsic rewards. The proposed LCER method also compares well to an SAC agent that has access to underlying state representation of the domain (e.g., joint angles, velocities). The high level idea seems sound and practical and the implementation strategy builds on popular ideas in the literature and seems reasonable. Does not cite DIYAN (Eysenbach 2018) https://arxiv.org/abs/1802.06070 which also uses mutual info between latent state projection Z and actions A, though not for representation learning. Why is JSD more stable than MI? Is it possible to give an intuition?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; By decomposing distributable global symmetries into local symmetries, the paper proposes a new neural network structure that empirically outperforms existing MPN models. Strengths:  The observations made about symmetry in MMDPs are reasonable. The intuition behind these observations are well illustrated by giving a concrete example of a cooperative traffic light control system. While the code is not yet made public, the experiments section in the Appendix is sufficiently detailed and the results appear to be replicable. Since the paper is empirical in nature, could the authors comment more on what are other related works on applied MMDP that could be used for the same task? Can the authors show that the proposed method outperforms other algorithms, that perhaps only make use of graphs or permutation symmetries? Overall, the paper introduces an interesting observation on certain real world MMDP tasks and proposes a novel network structure that performs well under two different tasks, one with real world data and the other with simulations.<|endoftext|>This paper tries to exploit the global symmetries in the joint state action space of cooperative multi agent systems. The problem this paper considers is very interesting, but the proposed method confuses the reviewer. (2) The writing is not clear. Why is there a $|G|$ in $\phi_e$ s output in Sec.4.3.2?Can the authors use framework diagrams instead of codes in Appendix D to show the architectural details? These obscurities are very confusing, especially when some definitions in the code are not defined in this paper. The reviewer strongly recommends the authors not omit some details (although the details can be found in *MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning*). The reviewer hopes the authors could provide a running example to explain how the proposed method works, including training and executing. This paper is not well written and very confusing.<|endoftext|>The authors give a generalization of MDP Homomorphic Networks (van der Pol, 2020) to multi agent distributed settings. What is the advantage of using Symmetrizer? The method shows improved sample efficiency relative to networks which are only permutation equivariant trained with and without augmentation. The method is well motivated and well designed in that the architecture reflects and enforces both the global permutation symmetries and local rotational symmetries the authors identify in the problem. By simply assuming full communication and making a single centralized agent, we can still consider this symmetry by permutation/rotation as an MDP symmetry of a single agent MDP with symmetries. The test domains are both fairly simple and feel a bit contrived. In particular the traffic light scenario is only symmetric because the traffic lights have exactly symmetric lay out. Sec 3.2/ para 3: $e$ is not defined. City grids notwithstanding, actual road segments are usually distinguishable in the real world. Since the method is formulated to work with many groups, I’d like to see it tested with more groups. It might be good to compare to augmentation with additional samples. I believe the symmetries considered here may also be essentially considered as symmetries in the setting of van der Pol, 2020. It is thus hard to evaluate how useful the method would be in practice.<|endoftext|>By decomposing global symmetries into local transformations, this work introduces a multi agent equivariant policy network based on this factorization. Empirical results show that  on symmetric multi agent problems, distributed execution of globally symmetric policies improves data efficiency compared to non equivariant baselines. Strengths:1  The paper is clearly written and easy to follow. However, how to perform the training of policies is not given. Though it is briefly mentioned that the authors used CTDE (centralized training and decentralized execution), I would appreciate it if the authors could provide details of the backbone algorithms and details for training and execution. 2  the experimental results are not sufficient. I expect to see the comparisons to some multi agent algorithms, such as PIC[ Liu2019]. Dependent on the answers to Question 1, it is possible that more baselines need to be added. This paper is in general well written and easy to follow. The method is well motivated.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper considers a new and complex setting involving domain adaptation, federated learning, and knowledge distillation: Under the premise of protecting privacy, one needs to deploy a compact model from a source central server to target client devices and requires the model to learn new knowledge with target unlabeled client data while remembering knowledge of source data on the central server. Does the data split consider the balance of classes? However, in this submission, the large network is fixed as ResNet 50 and the compact network is fixed as ResNet 18. The authors validate the effectiveness of the proposed scheme on three domain adaptation datasets. ## SummaryFor me, the considered problem is new and complex, but not realistic. Justifications are in the main review. ## Cons1.The problem setting is not clear, which makes it not realistic. If imposing resource constraints on target clients, only considering memory footprint is not enough. In the scheme, the authors directly use SHOT (ICML 20) for adaptation from a large source model to a large target model. Besides, I think as a federated setting paper, only considering one client for most experiments is unconvincing.<|endoftext|>Generally, the proposed method consists of two major steps: the first step is to introduce lite residual connection to conduct lite residual hypothesis transfer; the second step is to combine supervised loss (based on source data) and a KL loss (based on source data and target data) to make the trained model not forget source data knowledge. Strengths:  This submission targets to a practical problem: train a model with high generalization ability on a decentralized settingWeakness:  Both steps in the proposed method are from previous works, which include: the lite residual module,  the combination of a supervised loss and a KL loss. The experimental results are not convincing enough. Based on the reported results, it is hard to demonstrate the efficacy of the proposed method. However, the technical novelty is limited and the experimental results are not convincing enough.<|endoftext|>A compact model deployed to a device may not work well if this device has a different data distribution. This work proposes to load a large pretrained model onto a device and then adapt it to the target data on the device. As directly training the full large model is too memory heavy, this work proposes to adapt the large model s knowledge by training only part of its parameters on the local device data. Strengths:   Overall a well explained methodology that is well adapted to the particular system set up. Nice to see a discussion on privacy. Clearly, the performance on the source data is worse for TO KD, but how important is that for the device? What would the performance be if I were to just train the compact model on a loss fn with source + KL divergence (without having adapated the global model to target data first)? Or has this been done in previous work? Paper solving a problem of interest with a well explained methodology but questions on results remain<|endoftext|>To solve this problem, the author uses several lite residual modules to imply hypothesis transfer, which can save a lot of memory. To maintain source knowledge during the distilling process, a source target unified distilling method is designed. 3.Train the compacted model in a distributed manner considering privacy problem. What are the advantages of using lite residual modules compared with adding another full connected features layers? This paper aims to solve a domain adaptation problem on the edge device which is a common problem today. The proposed method can help to improve the performance of a various of edge devices which I think is feasible and useful.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The theoretical insights suggest that common high level features are necessary to solve the L2DNC task, which motivates the proposed approaches based on low dimensional non linear projection of the data. Overall, I found this paper very interesting. However, CATA is less well explained. It then assigns each training point to a group defined by which of the orthogonal classifiers was most strongly activated by the observation. Meta learning inner tasks are then sampled from each group. It is unclear to me what exactly this aims to accomplish. I think the authors should explain the task sampling problem in meta learning more completely. "For $z_i   \pi_{mm}(x_i)$ and $z_i   \pi_{mm}(x_j)$" should be $z_i   \pi_{mm}(x_i)$ and $z_j   \pi_{mm}(x_j)$". The theoretical framing of the L2DNC problem is valuable, as it can help us understand what is necessary to solve this problem effectively.<|endoftext|>This paper considers the problem of novel class discovery (NCD). You can consider swapping some descriptions from Appendix to the main context. That is, using outputs with C_l dim for the labeled data while outputs with C_u dim for the unlabeled data. I did not see the trivial sampling procedure for MEDI. In addition, they reveal that NCD can be solved if known and novel classes are related. They also show a result to see how NCD fails if one key assumption is not satisfied. Some statements and implementation details are not very clear, I would like to see the response of the authors. In addition, this paper builds a connection between meta learning and NCD and presents an effective approach to address the introduced problem. It is better to move some parts in the Appendix to the main context. Is it better to consider an inequality? The pipeline of meta learning should be introduced clearly.<|endoftext|>The paper introduces meta discovery as a way to adapt meta learningstrategies to the problem of learning to discover novel classes. Italso presents a formalization of the problem that helps shading lighton the conditions under which it is learnable. The authors additionally introduce a novel sampling strategy aimed atsampling data having the same "dominant" view to help the followingclustering procedure. Learning to discover novel classes (L2DNC) is a challenging task,where the learnability conditions are not entirely clear. The need for a shared transformation/representation, that is one ofthe results of the formalization, is sensible but clearly notnovel. Results are reasonably well discussed, and include both few data (thesetting of the paper) and large data results (where performancedifference shrink, as expected) are well as ablation studies. should it be K epsilon separable or something? An intuitively sensible multi view based meta discovery algorithmshown to outperform existing alternatives in the low data regime.<|endoftext|>In this task, a known class dataset can be used to help cluster novel classes. “Novel” means that there are no overlaps between novel classes and known classes. Based on this contribution, this paper is above the acceptance borderline. 6.The condition used in Theorem 2 should be explained in natural language. 7.Directly introducing meta learning (using “few”) is not good. Besides, the connection between sampling process and Assumption (D) should be discussed deeply. Since we only need to cluster novel classes, it seems that introducing additional data is useless. In this paper, the authors answer this question and point when L2DNC is a meaningless problem. This point might affect the development of the L2DNC. Cons:1.I feel struggling when I first read this paper. The latter seems more natural. 5.From two theorems, Assumption (D) is indeed an important assumption for L2DNC problem.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper proposed a multi stage learning framework for solving FPS games, with hindsight experience replay, goal conditioned reinforcement learning, and prioritized self play. 3.The presentation of this paper is clear. The baselines compared are from 2016 & 2018. 2.The ablation study in this paper is not convincing enough to show the usefulness of different components and different stages. 3.The source code of this paper is unavailable, which makes it hard to reproduce and understand the details in the training process. But I m open change my rating after seeing the rebuttal from the authors to clarify my concerns.<|endoftext|>This paper proposes a multi stage learning framework for training high performance agents in FPS games. The paper propose a methodology called Hindsight PPO to solve goal conditioned RL, though it may have some concerns. Strengths: + The paper is well written and the logic is clear. In my opinion, there are many policy styles in FPS games, such as aggressive/defensive/wandering, etc. The paper should conduct more experiments to evaluate the strategic diversity of the agent which is the main contribution of the paper.<|endoftext|>For this reason, the method name, Diversified Strategic Control, is confusing as well. In addition, many pieces of information that would enable a complete understanding of the method and the reproducibility of the results are missing. I m not sure what the authors try to communicate? This aspect of the work is deeply lacking. The values of all the hyper parameters are not provided e.g.optimizers, losses coefficients, training parameters, etc. Why is that, and could the performance of the agents for the other goals be provided in the appendix?<|endoftext|>Visualizations make the paper much more interesting and understandable. Could be improved: Authors proposed a multi stage learning system, and it would be interesting to compare with a flat learning system with the same/similar architecture; Not only report means of different runs in Fig.6, but also report the standard error; To make the loss terms more detailed, in Formular 2 and 3, losses for RGPS and policy distil are not specified;  The implementation work is impressive, but the description of the contribution and the method is not clear enough. The authors combined several different techniques from RL field to solve FPS game, ViZDoom.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The introduction of the Almost Full Recovery regime is interesting. Legends in figures are too small to read on a printed article. To that end, the authors propose a comprehensive and visual methodology: phase diagrams. This is a solid work that will be certainly be helpful to the community, and I recommend it for publication.<|endoftext|>## Weaknesses The primary weakness of the paper seems to be in the applicability to realistic problems. For now, I d say the paper is marginally below the acceptance threshold and ask for the authors to further motivate the simplified model. The notation is a bit unclear. They focus specifically on the case where the Gram matrix for the design is block wise diagonal (2 x 2 blocks) with the off diagonals within each block given by a correlation parameter and diagonals set to 1.<|endoftext|>While the previous studies focused on the model selection consistency, this paper focused on the impassibleness of the model selection consistency based on Hamming errors. Besides, this paper is well structured, and the theoretical background of the proposed approach is well described in the paper. Specifically, I really appreciate that this paper theoretically revealed Hamming errors of each variable selection approach. However, I think variable selection approaches are especially useful in the case of n<p. So, I want to know the theoretical results in such a case.<|endoftext|>In general, the presentation of the paper is clear. The paper studied a very specific setting: "coefficients are iid drawn from a two point mixture and that the Gram matrix is block wise diagonal." Correctness:Most of the theoretical results are well supported by the proofs in the supplement material. Although the authors have done a good job to deliver their theoretical results, I feel the technical novelty is limited. This paper focused on some specific settings. I also hold a conservative opinion towards its significance.
Reject; rating score: 5; rating score: 6; rating score: 8; In this paper, the authors study differentially private empirical risk minimization (DP ERM). Specifically, they study the case where the constraint set $\mathcal{C}$ has additional geometric structure, i.e., its Gaussian width could much lower than the underlying dimension $p$, such as the $\ell_1$ norm ball. However, this paper assume there are some additional public data. Specifically, they apply Mirror Descent with the loss generated by the public data as the *mirror map*, and using DP gradients of the loss generated by the private (sensitive) data. The authors use public data as the mirror map to provide improved bounds compared with the SOTA. This idea is quite interesting and may be used to other problems. 2.They further show that their algorithm has a natural "noise stability" property. This paper provide some experimental results. Compared with the classical case ("Private empirical risk minimization beyond theworst case: The effect of the constraint set geometry") The improvement on the upper bound is limited if there is not enough data. 3.The paper did not compared with the SOTA ("Private empirical risk minimization beyond theworst case: The effect of the constraint set geometry") in experiments. I think this is necessary as this is the most close method, instead of DP SGD. Moreover, the authors left the reference [1], which also studied the problem. I tend to reject the paper.<|endoftext|>The algorithm uses the loss on public data (with a strongly convex loss function) as a “mirror map” to implement private mirror descent on the private data. It is shown to give dimension independent bounds in certain regimes. This paper proposes a new idea to use public data implicitly by using the public data loss as the mirror map in DP mirror descent. At many places, mathematical notation is either used but not defined explicitly or sometimes not consistent. A short notation section, would make its easier to read. Why can one not directly either give an algorithm with population loss bound or give an excess empirical loss bound and add the generalization error? 3.One of the biggest causes of concern for me is the Assumption 3.1, it assumes that some minimizer of the private loss is also the (unique) minimizer of the public loss. Can the authors give a real world example where the two losses are not the same but this holds? 4.Overall, the paper lacks toy examples which would help understand the theory better and clear doubts that arise when reading assumptions. I d be curious to see what are the choices of private and public loss for these cases, what is the Gaussian width and what does the guarantee look like for these values? It would be better if a formal argument is made for this.<|endoftext|>They also achieve a dimension independent bound as in some of the previous work. The idea of the paper is very simple: they use public data as the mirror map in the private mirror descent algorithm. The results are nice and for me and the approach very natural. It is an easy accept. I have few comments about the paper though. Since the paper works in the public private data setting, it would be nice to know (even theoretically) the comparison between how much public data is used by previous work and we have access to equivalent data, what is the performance of the current submission. This would put the work in a very precise light as to whether it is improving the state of the art or not. The paper mentions in the appendix that they compare SoTA with Asi et al.and they made a best effort to match their experiment set up. Asi et al.is a published work in ICML 2021 and I am surprised by the claim of the authors that the code is not publicly available   given that ICML requires code submission in the supplementary material. If it is indeed so, I would suggest the authors to reach out to the authors of Asi et al.to get their code because the hyperparameters used in this submission can be very different from what Asi et al.did and hence the claim may or may not be true. The proof is very simple and elegant, which I think is another big bonus.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; Strengths  This paper addresses an important issue of the MI based MARL framework where the joint policy converges to sub optimal. Weakness and Questions There is a lack of explanation for the collaboration criterion (section 3.2), e.g, as I understand, $s$ is a state variable whose distribution is conditioned on $\pi$. Thus, it seems that the interpretation of the proposed MI form, $I(s;\pi(\cdot|s))$, is not proper. Thus, in my opinion, the effectiveness of the proposed method comes from the further exploration and exploitation strategy rather than the collaboration strategy. To see where the performance gain comes from, it would be great if the authors compare the proposed method with MA SAC+DPCB and VM3 AC+DPCB. The proposed idea is novel, but the author should provide further explanation on collaboration criterion and further experiments to see where the performance gain comes from.<|endoftext|>Thank the authors for the detailed response. Are you referring to the cooperative version of predator prey? *I think this statement is the foundation of the correctness of the method*. This paper is related to a lot of good ideas in existing works. The authors may possibly make the experiment sections a bit more concise and shrink the conclusion section to make some space. 2.There are a lot of ablation studies conducted in the experiment section as well as the appendix, which is appreciated. The experiment section is clear and easy to follow. It could be because of the writing issue, but, at least from the current statements in the paper, the correctness cannot be rigorously justified. 1.A related work section would be encouraged. Fig.1 also looks strange as it appears in a motivation section. This is an experiment result rather than a motivation example!<|endoftext|>This paper proposes PMIC a MARL framework for improving multi agent collaboration through mutual information. The estimated mutual information bounds are then used as additional rewards for training individual policies. It would be better to compare the converged performance between baselines. 2.I am not quite following why minimizing the MI in inferior trajectories is helping improve the corporation. Generally, I would suggest more explanations on the motivation of the PMIC. I think the submission overall is good for its novelty and experiment results. Thus, I suggest accepting the paper.<|endoftext|>For the current version of paper, I can only recommend reject (but marginally below the threshold) due to the weakneses and concerns above. In my view, the idea of appropriately modelling these MI bound estimators to the actual phenomenon in the MARL problem is novel. Similar to many prior works, the tuning of the multipliers could be a potential issue. The writing of this paper is generally clear. ## WeaknessesThis paper does not have obvious weaknesses, but I have some concerns that the authors need to clarify. Can the authors give more explanations on this? Can the authors show the result for it? 4.If I understand correctly, $T\_{\omega\_{1}}$ is designed with the state and action encoders while $T\_{\omega\_{2}}$ is without. ## MinorsThere are multiple writing typos in the paper.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper proposes a novel method for comparing reward functions without policy optimization, the Dynamics Aware Reward Distance. DARD improves on the state of the art EPIC distance by using an approximate transition model to evaluate reward functions on transitions close to the training distribution, while EPIC evaluates on arbitrary transitions (which can be infeasible). The results are significant and novel, and I would recommend accepting the paper.<|endoftext|>The proposed method (DARD) builds upon EPIC [Gleave et al.] * Cons  * The empirical results could be more comprehensive. * The paper motivates the problem well by clearly identifying the issue with the prior work (EPIC) and proposes a reasonable solution that takes into account the actual dynamics of the environment. I also appreciate the authors for acknowledging the limitation of the proposed method with smooth reward functions. Although the overall method is still quite similar to EPIC, this paper proposes a reasonable solution to overcome a clear limitation of EPIC. In addition, the experiments are quite well designed, and the paper is very well written.<|endoftext|>The paper proposes a new reward pseudometric called Dynamics Aware Reward Distance (DARD) which uses approximate transition model of the environment to compare reward functions while being indifferent to reward shaping. The authors show that this can cause problems for reward functions that are technically equivalent in the feasible state transition state, but unequal on the infeasible transitions (which should not matter). The idea of comparing reward function without training policies is very appealing for Reinforcement Learning. The authors take a previous method (EPIC) and improve it by making it consider only feasible state transitions. The method assumes access to the dynamics model, and the authors state that this can be approximated (or learned) as well. The paper proposes an incremental method that improves over past work for to ignore the infeasible state transitions. I think the paper can also use a higher level use case of reward comparison where the proposed method is used to design a reward function to learn a problem that is otherwise unsuccessful.<|endoftext|>The paper proposes a new reward pseudometric (DARD), which is invariant to reward shaping and computationally efficient. Assuming access to the transition model of the environment, DARD could avoid the error due to samples out of the training distribution. The numerical results in simulated physical domains show that DARD outperforms previous methods. In particular, the experiments demonstrate that samples "out of distribution" would harm the performance of EPIC significantly. Overall, the paper is well written. My current score is 5 and I look forward to further discussion with the authors.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposes a semi relaxed Gromov Wasserstein (GW) dissimilarity for learning tasks on graphs. Overall, I think this is a good paper, and I would favor acceptance. The authors provide formulations of the standard srGW as well as entropy regularized and sparsity promoting variants, following the relevant literature. srGW being a (possibly asymmetric) divergence is really quite interesting. Following up on Sturm s work, it seems then that the space of graphs equipped with srGW is a quasimetric space, which in turn leads to various questions about its metric structure. Mémoli and Sturm proved their results for the mm space setting, and the theoretical results for the graph setting came a few years later. The authors take the well studied GW problem which involves optimization over probability matrices satisfying row and column constraints and consider a simple relaxation wherein the column constraints are removed.<|endoftext|>The problem is then given in an equivalent formulation, which is solved via Conditional Gradient. The authors propose applications with graphs, such as graph partitioning, clustering, and completion via Graph Dictionary Learning. Are there similar methods that take into account the directive graphs as well? I think that the paper would have been much better if the space used for some applications in Section 5, was used to better explain the rest of the paper, or show some other images/interpretations. The paper presents a good idea, in the form of a relaxation of GW, and its equivalent formulation, and promising results in real datasets.<|endoftext|>The experimental results for those applications on graph data have shown the efficiency of the proposed divergence. The idea of relaxing the histogram of one distribution in GW distance looks intriguing. The existing problems of learning with graphs such as graph clustering (partitioning), graph dictionary learning can be interestingly reformulated using the newly proposed divergence. As if we consider the GW barycenter of measure p with fixed supports, we will solve the same problem with computing srGW. On page 4 (third paragraph), the paper states that "A first interesting property of srGW is that since h is optimized ..., its optimal value h* can be sparse". Is this property is proved theoretically or experimentally besides the experiments with the sparse regularization? Will algorithm 2 for graph dictionary learning converge to a local solution?<|endoftext|> The paper proposes to apply a version of Gromov Wasserstein divergence to graphs. In particular, they relax the weight constraint of the second graph  and try to find a minimizer of the Gromov Wasserstein distance. The GW distance between two graphs is based on two quantities: the distance of the inner structure of graph and the weight of graph. This work proposes to relax the weight on one graph and keep the matrix  distance of that graph, and then try to find the GW distance between two graphs when the constraint on weight of one side disappears. In fact, it try to match the structure of two graphs by ignoring the weights of one graph. The first question, that is easily to have, is that what is the role and the effect of the weight of the first graph in the srGW? Does it affect the final result or not? In the paper, they stated that the interesting property of weight is the sparsity, but they should have a better theoretical result than Proposition 1, since it only deals with the case of zero  srGW. The author(s) apply the srGW to do graph partitioning and clustering. The reviewer finds that two tasks are similar. It is not clear for the reviewer how the author(s) choose the number of clusters, which is often important step in clustering method.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The author proposes use self training and negative sampling to mitigate this issue. Both the problem setting and technical contribution of this paper is problematic. Overall, I recommend a rejection to this paper.<|endoftext|>This paper proposes a self training method with negative sampling for node classification on few labeled graph data. The major contribution: data augmentation strategy (pseudo label) and negative sampling regularization, is simple and intuitively, depending on the threshold value. Weakness1   The novelty of this work is limited.<|endoftext|>The main idea of this paper is to use the reweighted self training method and the negative sampling method to stabilize GNN. The technical novelty and originality of the paper are limited.<|endoftext|>The main weakness is: the technical contribution is limited. To address the unstable problem, the paper proposes two strategies consisting of pseudo labelling based self training and negative sampling based regularization. 3.The proposed two strategies are easy to be reproduced.<|endoftext|>This paper presents a self training GCN framework. It refines the predicted results as pseudo labels and pseudo negative labels to train GCN, which is applicable to existing GCNs to stabilize the training process and enhance the training data. The paper is well organized and the motivation of this paper is clear. Besides, some comparison experiments with pseudo labeling methods are lack.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper proposed As ViT to automate the principled design of vision transformers without tedious human efforts. To my best knowledge, it is the first framework that unifies efficient search, scaling and training in ViTs. To be fair, I am not asking the authors to perform so during the rebuttal timeframe. However, the authors are expected to clarify a number of experiment and comparison issues as aforementioned. They then propose a progressive re tokenization method for efficient ViT training. 5  The authors made comprehensive comparisons with various SOTA ViTs and their NAS methods, on not only ImageNet 1K classification but also COCO detection. However, I have some reservations regarding their discussion of results, as to be laid out below.<|endoftext|>The search based auto scaling rule proposed in this paper is a good way to solve this issue. +NAS on ViTThe author uses neural architecture search on ViT like framework on the number of kernels, attention splits, expansion ratio, depth and width jointly. Swin here can be viewed as an augmented multi scale vision transformer. However, it does not show significant improvement compared with other multi scale vision transformers and my concern is that a multi scale ViT baseline for fair comparison and evaluation is needed here. I will increase my rating if the concerns are well addressed here.<|endoftext|>Instead of relying on hand designed architectures, the paper proposes to automatically search for a ViT architecture, that is both efficient & accurate. A two stage approach is followed for architecture search. It is unclear if the proposed strategy works to search a ViT architecture suitable for small scale datasets? The advantages of automatically searched ViT architectures are not immediately clear. Their major limitation for ViTs is the time they require for training, and the number of FLOPs introduced by quadratic complexity in self attention.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The paper builds on prior work on prototypical classification networks (more specifically, the work of Li et al.2018) and additionally tries to include criteria such as orthogonality to enable applications such as fair classification. Experiments show that the resulting models are able to achieve reasonable fairness accuracy tradeoffs. The paper attempts interesting problems but lacks on 2 major fronts: (1) It is not clear what the improvement over the existing work is, and if it is significant enough to merit acceptance at ICLR (2) the writing needs a lot of work to bring out motivation for different choices. Moving on to the third paragraph of 3.1, the first few lines seem to be quite similar to PCN. The same happens in 4.2. The contributions are not clear and the writing needs major work.<|endoftext|>This paper proposed a framework (that authors called the concept subspace network) using prototype based representation controlling the alignment between two subspaces for the purpose of the classifier (fair or hierarch classification). It would be helpful to understand the motivation by giving examples of major applications where both fairness and hierarchical classification should be considered. Specifically, in section 3.4: 1. For fair classification, what are the two subspaces? The motivation of the paper and the intuition of the proposed approach is not clear.<|endoftext|>The authors propose a novel model — called Concept Subspace Network (CSN) — for both hierarchical and fair classification. The idea behind the network is to use sets of prototypes to define concept subspaces in the latent space defined by the neural network itself. The way in which concept subspaces are defined is not clear to me. Where is it defined? “Random” and “Rand” in Tables 1 and 2 The paper can be accepted if some clarifications are made<|endoftext|>The present paper proposes a novel architecture for prototype based classification to support class hierarchies and fairness. However, fairness has many meanings (as the paper acknowledges in the appendix) and only some of them are related to the proposed notion of orthogonality. For fairness, this is particularly dangerous as readers may be mislead to believe that the proposed notion of orthogonality is sufficient for fairness. This constant offset gets removed by softmax, anyways.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; The core of the paper is the identification of a gap in the literature, where it has not been tried to train several critics and a single actor for multi task RL, with each critic seeing only one task to be learned. The paper proposes to do just that, and shows promising experimental results in a variety of environments. Speaking of related work, it cites the Actor Mimic paper [3], in which N critics are trained on N tasks, and then the actor is distilled to be good at all these tasks. The different with this paper, I think, is that the Actor Mimic actor does not observe a task descriptor. I have the feeling that the Actor Mimic may deserve a bit more than an entry in a citation list in the background section, as it seems quite close to the main contribution of this paper. One small remark regarding the experiments is that the authors focus on the impact of multiple critics (which is well motivated). The proposed method is simple, yet unique and performs well.<|endoftext|>This paper proposes to extend the actor critic frame to tackle multi objective (multi task) reinforcement learning. The key idea is to learn multiple critics that correspond to different reward functions. The paper applies the multi objective RL to learning different styles of completing tasks, such as aggressive or defensive style in a boxing game. The paper shows that the proposed algorithm can beat several multi task learning baselines. 3) The examples in the evaluation (e.g.Pong and the boxing game) demonstrates an useful application of the proposed method: designing AI for games. However, the claim in this paper that "seemingly no prior work explores the use of a single actor with multiple critics" is not true. It would be great to visualize the learning process with learning curves. The paper proposes a simple and yet effective algorithm for multi objective reinforcement learning. The paper is well written and the results are convincing.<|endoftext|>The paper introduces a variant of actor critic reinforcement learning algorithms where multiple critics are trained in a reduced version of multi task training where a task allows multiple reward functions. The current empirical results are ambiguous when comparing the proposed method to baseline methods, leading me to recommend rejection. Some qualitative results are shown for a fighting game trained with the multi critic approach. # Strengths1) Simple yet novel approach	  Exploring multiple critics for multiple reward functions for a single task is a natural idea that has not been done before. Or consider reporting other metrics recommended for RL evaluation (for example: https://arxiv.org/abs/2108.13264)2) Lack of baselines in the UFC domain	  The authors note that the domain is meant for demonstration only and not as a benchmark. I found the qualitative results shown most compelling for this case and so was disappointed to not have baseline comparisons (acknowledging the costs for doing these experiments). I recognize this may not be feasible, but want to indicate to the authors that this would be a strong demonstration of the work. The results show different styles but do not directly assess transitioning between styles.<|endoftext|>The work focuses on a special case of MTRL described as “multi style RL”, where the goal is to learn several distinct behaviours under the same environment dynamics. The main technical contribution is the idea of training separate critics or value heads while maintaining a single, shared actor for multiple tasks. Furthermore, this challenges the need for the explicit separation of policies for different tasks, which is a common technique in the multi task RL literature, e.g.[1,2].The experiments consider several unique, well motivated environments and tasks. Currently, the “single style” baseline is meant to fill this role, however it is difficult to compare this to the authors’ proposed approach in terms of sample efficiency since: (1) the single style is only trained on a single task vs multiple simultaneous tasks (2) single style does not observe sample efficiency gains from shared multi task training. The paper demonstrates that several distinct behaviours can be successfully learned with a novel single actor, multi critic setup. While the technique is simple and the main idea shares some motivation with previous work, I believe this will be a valuable contribution to the multi task RL literature.<|endoftext|>The method is straightforward, extending the existing deep RL method via multiple value networks. The method can not generate different styles under the same environment, or put another way, this method can not generate different styles under the same reward function. However, this paper can not generate multi style behaviors when receiving the same reward signal. The algorithm proposed in this paper is more like a multi task algorithm instead of a multi style algorithm. **update**I have read the response of the authors and increased my score. The experimental results are convincing, but the usage of this method is limited. Because this method can only be used for environments with multiple reward functions, and their approach can not generate multi style behaviors for the same task. I think it would be more reasonable for the author to reformat their paper as a multi task paper.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; Authors present a theoretical understanding for the "transition to linearity" phenomenon of wide neural networks with linear output layer under gradient descent. Authors present a very interesting and novel theoretical contribution to the understanding of contemporary deep learning architectures with increasing network width. Authors prove that the transition to linearity phenomenon when the number of sub models converge to infinity (i.e., the network becomes wider) is a result of assembling weak sub models that do not dominate the assembly from network initialization. The paper is well written and presents a novel theoretical contribution to the understanding of the recently highlighted phenomenon on how wide neural network architectures with linear output layer inherently evolve as linear models.<|endoftext|>The authors propose a new perspective where the neural network is considered as a multi level assembly model. The paper is very well written, with proper style and easy to follow. This leads to neurons within the same hidden layer being independent when the network width grows to infinity, and to show that the network is linear in an O(1) neighbourhood around the initialization. 3.The key finding of this manuscript is "[...], when the network width is large, the neurons within the same layer become essentially independent to each other in the sense that their gradient directions are orthogonal in the parameter space.".<|endoftext|>This paper aims to explain the phenomenon that neural networks with infinite width tend to be linear in the neighborhood of initial optimization points, from an assemble model perspective. The authors first proved in the simple two layer case and then extend the results to deep neural networks with L layers. Strength:The authors managed to providing a new perspective on why the wider neural networks tend to be linear, which looks interesting and also consistent with previous results. They also extend the basic results to more complex multi layer cases. As this paper is out of my expertise and purely theoretical, I hope other reviewers with more background can comment more on its novelty and significance.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper proposes an image text training method similar to CLIP. 2.The proposed method addresses the issue of many to many relationships in a technically valid way, and doesn t increase the complexity of the general approach significantly3. The authors present experiments with a good set of baselines (LS, KD, InfoNCE) as alternative methods and ablation studies on each component of their approach4. The experiments show that the proposed approach consistently outperforms alternative approaches (baselines) Weaknesses:1.<|endoftext|>Overall, I like the idea proposed in the paper to use OT to mine missing matches for language vision pretraining. The proposed method trains their visual language encoder pair with a contrastive objective (similar to CLIP) on image text pairs. The authors have motivated their approach by giving the example in Figure 1 to show that, there are many missed matches between images and texts. The proposed method demonstrated good results compared to several previous methods and also a set of variants for ablation studies.<|endoftext|>Is it possible that only a handful of examples exist like those shown in the paper? The authors showcase a fundamental problem of InfoNCE by providing interesting examples in Figure 1 and Figure 3. 5.In Table 1, it is strange that CLIP s performance using 400M images is close to the InfoNCE baselines trained on 3M images, on the GOI dataset. Is that only due to using pretrained image and text encoders?<|endoftext|>An improved InfoNCE is explored to consider the many to many relationship between unpaired images and texts. Pros: + The motivation of this paper is very clear. + Overall, the paper is well written.<|endoftext|>The authors present the potential many to many visual to text relationships that CLIP cannot handle. I doubt that InfoNCE can represent the best performance of CLIP trained on CC (3M) and WIT(5M). Strengths1)	The authors provide an interesting and solid idea based on CLIP.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The authors introduced an energy consumption attack on Neural ODE models, which is a novel application of energy consumption attacks. Two variants of the attack are introduced. Furthermore, the paper is poorly organized, and experimental analysis can be improved. It will be interesting to see if the proposed attack can be circumvented using this simple defense.<|endoftext|>a) The authors mention that  We want to add perturbation $\delta$ to x such that energy consumption is maximum . 2.The paper writing is not clear and some parts of the paper are hard to understand. The paper presentation is not clear. I suggest reject this paper. I understand this is for the universal attack. In addition, the authors mention two methods: corruption and perturbation techniques.<|endoftext|>Further, the inference in these models is adaptive. This paper investigates whether this adaptive inference can be used by an attacker to launch an energy attack on the neural ODEs. Overall, the paper definitely adds to the state of the art on neural ODEs and robust learning. It raises a very valid concern about the vulnerability of these models to energy attacks. + The paper also evaluates the transferability of NODEAttack, that is, if adversarial inputs are generated for one solver/architecture can they also increase the energy consumption for other solver/architecture.<|endoftext|>Instead of common performance/accuracy robustness, this paper considered the uncertainty of energy consumption of neural ODE models. Experimental results on MNIST and CIFAR10 demonstrate the effectiveness and transferability of the proposed attack. The imperceptibility: The generated data in the appendix seem very noisy and easy to distinguish from clean data. How well does the attack perform if the budget is limited to be small? 3.Can the authors discuss a bit on the defense of these adversarial examples?
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; This paper explores the application of simulated annealing to network pruning. * Detailed overview and motivation for pruning algorithms and simulated annealing. Weaknesses* The limited scale and scope of the experiments puts the significance of the results into question. Conducting more detailed comparison to baselines would greatly strengthen the paper. Simulated annealing is not a new technique, and the limited scale of the experiments and lack of comparisons to baselines limit the utility of this work in its current form.<|endoftext|>This paper develops a method for neural network pruning using simulated annealing. Therefore, the reviewer can recognize neither the novelty and effectiveness of the proposed method. The experimental comparison with other pruning methods is not performed.<|endoftext|>The paper proposes a simulation annealing (SA) algorithm to prune neural network by randomly dropping and adding links according to SA s acceptance rejection criteria. The SA presented in the paper is easy to understand and well studied in traditional optimization community. It has proven convergence to optimality. Applying it for network pruning is certainly an interesting idea. The experiment results demonstrate the algorithm can prune up to 80% of the links without too much accuracy deterioration. 3.Need more baseline comparison. The proposed SA method is interesting, however, I am not convinced it can scale well, and the experiments are using small networks.<|endoftext|>  This paper presents a novel simulated annealing method for pruning neural networks. The results show the effectiveness of the proposed method. The related work is very sparse. There are many recently published works considering neural architecture search (NAS) and network pruning (NP).
Accept (Oral); rating score: 6; rating score: 5; rating score: 5; rating score: 10; They show that 8 bit fixed point can represent different exponent ranges based on fractional length, thus choosing the right fractional length is critical. The two pass method for batch norm also uses float in the first pass. The results are not enough to convince me that this is a significant contribution to QAT literature. It s not clear to me what combining PACT with quantization achieves. Again, some small experiments comparing the two would be useful. However, the novelty of the proposed methods is low, and it s not clear to me what improvements they offer over existing standards. As a result I rate the paper marginally below acceptance. EDIT: The authors have addressed most of my concerns and I will raise my score to a 6. I still think that the formulas to compute the fractional length are not novel.<|endoftext|>This paper describes a novel quantization framework that involving only fix point 8 bit multiplication for DNN execution. The paper first highlights the advantages of the fixed point numeric format. After that, the paper introduces a novel approach to determine the right format for each layer during the forward propagation of the training. strengths:+ The authors consider a practical issue from the perspective of DNN hardware implementation. Without this, it is hard to convince the reviewer about the usefulness of this work. Equation 1 is based on the empirical approximation with Gaussian distribution, not the real DNN trace. Observation 1 and 2 is obvious. It is nice to have some real empirical results to demonstrate this,  but I think they should be described very briefly. Overall, I think this paper lacks some motivation for the fix point format.<|endoftext|>The paper proposes a low precision DNN inference models with 8 bit fixed point. As a reviewer mentioned, the benefits and motivation of the proposed works is not obvious and the experimental result have not shown the advantages of this approach compared to state of the art works in 8 bit integer quantization. The fraction length can be selected based on dynamic range and I think the reason behind the constant in the equation 1 is the correlation between standard division and dynamic range (DR 3ST). It is difficult to understand the advantages of this approach in compared to pervious work in 8 bit?<|endoftext|>The analysis of relative error and normal distributions gives a strong empirical and theoretical basis for the approach used in this work and provides deep insight into what the optimal fixed point data type in each layer would look like. This analysis alone will inform future research in this area. Many insights of this work are directly applicable for mixed precision hardware where most, but not all operations are done in some 8 bit data type. For that area, quantization precision is the most important unsolved problem. Overall, I disagree with the perspective of the other reviewers and I still believe this is an outstanding paper. The approach taken in this paper is very creative and the insights gained from this paper will inform research in this space for quite some time. I would be happy for this work to be selected as an oral presentation.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper studys cost sensitive hierarchical classification problems. 1, The paper claims that "Cost sensitive loss is hard to optimize since it is non smooth and non convex". what is the time cost of the proposed methods? The experiments also do not report the time cost the proposed method. This is a main major issue. 2, The distributionally robust learning approach is uesed as the optimizer. How can you use DRO for fast optimization?what is the time cost？can you report the time in experiments？can the proposed method converge？can you provide both theory and experiments support for convergency？3, The data sets used in the paper is very small and trivial. Can you test your method on imagenet dataset and larg scale NLP data sets？4.There many methods aim to devide the cost sensitive learning problem into some sub problems.<|endoftext|>The authors propose a new framework for cost sensitive hierarchical classification. First, they decompose it into level by level learning to abstain (with different abstain costs per class) sub problems. To solve these subproblems, authors apply deep distributionally robust learning (DRL) approach that directly minimizes the abstaining loss (based on Fathony et al.2018).These two elements create a method named the Layer wise Abstaining Loss Minimization method (LAM). The proposed method is compared with DARTS on two datasets and achieves attractive performance. Strengths:+ The paper has clear motivation. + The decomposition into level wise learning to abstain seems to help find the desired "performance profile"   making it easier by reducing the number of parameters. It would be nice to see also the results for some other methods and some simple baseline to get a better idea about the benefits of the proposed approach. The number of datasets is already small. I m not that familiar with some of the related work and used datasets, but I think additional comparisons are needed to assess the attractiveness of the proposed approach correctly.<|endoftext|>It shows a bijection between original cost sensitive problem and the set of layer wise abstaining losses. It is based on using the existing distributionally robust cost sensitive classification and extending to it to the hierarchical setup. The proposed methodology is demonstrated on birds and cell classification datasets, which are claimed to be large scale. It is compared to a relatively old DARTS method from 2012. Negatives   The significance of the work seems somewhat limited. The main premise of the proposed approach relies on abstention as an option. What if that is not an option, and one needs to make a hard choice as part of the problem formulation. The paper relies heavily on existing works such as Distributionally robust cost sensitive classification has been discussed in [4], and the main contribution seems to be extending to hierarchical classification with abstention option.<|endoftext|>The submission study the problem of cost sensitive hierarchical classification (CSHC) with given label taxonomy via learning to abstentions each layer within the hierarchy. 2.Using DRL framework to solve the learning to abstain problems in each layer makes optimization almost decouplable layer by layer, the rationale behind its strategy should attribute to the proved bijective correspondence between the hierarchical cost sensitive loss and the set of abstaining losses. 2.The weaknesses:2.1) Insufficient review on existing works. 2.3) Lack of the reason why to select DRL for modelling seems NOT to be given! 2.5) Insufficient experimentsWhy to just use a DARTS (2012) as a baseline, the authors should clarify the reason of just comparing the baseline. In my opinion, incorporating "learning to abstention"  to CSHC is somewhat interesting and new, in particular,  the authors provide the bijective correspondence between the hierarchical cost sensitive loss and the set of abstaining losses to make the layer by layer efficient optimization possible, while the proposed LAM achieves better performance comparing DARTS despite of insufficiency.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The main contribution of the paper is to extend the previous work of  Lyu & Li (2019) for various types of networks. There are some concerns about the technical contributions of the paper. First, the implications of the technical result are not clear, in particular for generalization ability. The technical results seems non trivial but the implications are not clear.<|endoftext|>The paper studies implicit bias of linear and Relu networks (fully connected, diagonal, convolutional etc). Owing to its non convex nature, KKT conditions of this max margin problem do not guarantee local/global optimality.<|endoftext|>This paper studies the implicit bias of gradient flow in the lens of margin maximization. 3.The counterexamples for margin maximization are simple and intuitive. But this paper lucks a key result to highlight.<|endoftext|>The topic of this paper is interesting, I think that these margin maximization questions are very important. I found the non homogeneous example interesting, though it is very simple. The paper studies an important question and is quite thorough.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; In this work, the authors introduce local augmentation that enhances the feature of each node by its local subgraph structure. Specifically, they model the data augmentation as a feature generation process: given a node’s feature, they learn the conditional distribution of its neighbors’ features to make a generative augmentation model. The proposed approach improves various GNN models in benchmark datasets. Clarification is needed. The discussion before the “Optimization of the MLE” paragraph seems redundant. a) What are the structures of networks f and g? The proposed LA GNN structure contains much more parameters, which result in a higher risk of overfitting as a consequence. The experiment of Section 4.3 seems meaningless, since the approaches with partial ideas such as “+width” or “+concatenation” are not reasonable at all. This paper proposes a new graph augmentation method that enhances the performance of various GNN models by augmenting input features.<|endoftext|>This paper presents a data augmentation methods for training graph neural networks in general. They firstly fit a generative model that learns the conditional probability of input node features. The training data are then augmented using the generative model using the proposed importance sampling method and used for training GNNs. 2.The description of the proposed methods are unnecessarily notion heavy making it time consuming to decipher what the main ideas really were. 3.Given the complexity of implementing the proposed augmentation methods, the accuracy gain is marginal comparing to the baselines with / without the complex data augmentations.<|endoftext|>This paper studies the problem of feature augmentation for training the graph neural networks. Specifically, given the central nodes features and local structure, the proposed approach estimates the distribution of the node features of neighbors. The proposed local augmentation is feature level augmentation, which uses CVAE to generate the neighboring features based on the local structure and the center node s features. 1.The unclear comparison with baselines. GraphVAE can actually generate the structure and node features by VAE (\tilde{A} and \tilde{F} in the paper). It is suggested to discuss the main difference.<|endoftext|>This paper presents an interesting method to enhance the graph representations with the proposed local augmentation techniques. The authors propose to leveraging a generator to vary the node features conditioned on the neighbor information in the subgraph. For example: Is $p_\psi(z|X_i)$ a parameterized by $\psi$ or not? In the text I found $p_\psi(z|X_i)   N(0,I)$. Probably it could be shown in Fig 3 to justify the importance of the proposed module. update after rebuttal  After carefully reading all the reviews, the authors  responses, and corresponding revisions, I think the quality of this paper is better than the original submission.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; The framework accommodates several types of risk functions that can be used to evaluate the model’s performance. While the framework addresses the problem in the most practical settings (sequential), the points of comparison with the existing frameworks were not made rigorous enough for the reader. Under the i.i.d setting, how does the performance of the proposed framework compare to Kamulete, 2021? MNIST C experiments are under the i.i.d dataset shift. 2.Similarly, when the i.i.d assumption is violated (distribution drift), can the comparison with the testing framework of Vovk et al.2021 be made? Based on my knowledge and background research conducted while reviewing this work, I think the authors have proposed a novel framework to detect dataset shifts.<|endoftext|>The work presents a framework for alerting when the risk of a deployed model exceeds pre defined levels. The work highlights an important problem of alerting when model performance differs from that in training, statistically significantly. The resolution of issues with past work, such as shifts and sequential testing, is not described well. Apart from these details, the framework and the experiments are described well. 2.What type of distribution shifts such as covariate, label shift can be addressed? The discussion on loss functions is not related back to the problem or the method.<|endoftext|>This paper proposed a sequential testing method for tracking distribution shifts for deployed models. Overall, this work is solid but still requires some improvements on the experiment. There should be a direct comparison with other baselines and non sequential methods, or at least show the behavior of a state of the art approach to demonstrate what s the advantage of sequential testing. ## Update after RebuttalWe thank the authors for their detailed responses and for updating the manuscripts.<|endoftext|>This paper proposes a sequential testing schema for identifying malignant distribution shifts that indicate a necessary update such as retraining. Builds on related work in the field with (to the best of my knowledge) a novel contribution. $\textbf{Weaknesses}$Minor grammatical/style issues. I suggest adding a period to all of them for consistency. Although the discussion of relevant distribution shifts and their effect on classifiers is really interesting, it left me wanting more.<|endoftext|>This paper develops a tool for testing online whether the performance of a model on the test data becomes significantly worse than the performance on the training data, which allows differentiating between benign and harmful shifts. The setting is quite different from existing work. The monitoring statistics are the risk functions (and their CI). I think it d be better to give some comparison with the batch detection algorithm. Thank the authors for the response!
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper studies how self supervised pre training impacts the resistance of the neural network to noisy labels. Nevertheless, the paper presents an important study the experimental setup is not comprehensive enough to support the paper s concussions, Hence, I vote to reject the paper in its current form and maintain my current score, I encourage the authors to expand the experimental study by my and other reviews comments. In addition, only symmetric synthetic noise scenario is consider, while more complicated asymmetric and real life noise cases are not considered. This paper aims to provide a theoretical understanding of why self supervised pre trained features help neural networks improve resistance under noisy labels.<|endoftext|>The authors illustrate why self supervised learning can help learning in training with noisy labels. The authors illustrate theoretically why learning good representation can help learning with noisy labels2. The novelty of the theorem 1 and 2 is limited. It is trivially true that when you have majority labels that are not noisy, you can learn an optimal classifier. This may change the conclusion in theorem 1. Does downsampling also help for the baseline CORES? At least, I would like to see the results on cifar 100. 6.Somehow the authors mention the regularizer in section 4. but do not use it in Table 1. The authors propose an interesting idea that SSL can help learning with noisy labels but due to the theoretical, experimental and clarity concerns above, I think the paper is marginally below the acceptance threshold.<|endoftext|>This paper studies the usefulness of self supervised features when encountering data with noisy labels. Some theoretical and empirical results are relating to the authors arguments. What effect does using different SSL methods have on the results, even empirically? It is not clear to me how you train h, and with what objective, in eq 3? I think the setting for fig 3 is a bit strange. It seems like for higher noise rates that the you are taking very few epochs by stopping based on test accuracy.<|endoftext|>The paper performs a theoretical analysis on one kind of noisy label learning method. The paper also presents an empirical analysis. The experiment needs to be improved. The paper performs empirical analysis only on one setup. It could make the paper better if the authors could perform analysis on other settings (e.g., asymmetric noise, etc).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; Based on this phenomenon, the authors propose the adversarial extreme value analysis (AEVA) to detect backdoors in a black box manner. Pros1.This paper is well written and easy to follow. 4.I appreciate that the adaptive attacks and potential limitations are also included in the paper. However, I still have some concerns. 3.Please analyze the effects of key hyper parameters (e.g., lambada) involved in the proposed method. I think it will help readers to better understand them and it will not reduce your contributions. A practical and novel backdoor detection with theoretical guarantees.<|endoftext|>This paper presents a novel approach for the detection of backdoored neural networks. In this regime, it is assumed that the user/defender only has access to the model through making queries and getting back the labels for those queries. Based on this observation, which the paper calls the _adversarial singularity phenomenon_, a practical black box backdoor detection is proposed. In particular, the subtle connection between backdoored networks and sparse adversarial example generation can inspire further research in this direction. This is since almost all existing methods are typically designed for the white box scenario, and as such, they have a huge advantage compared to the current method. While there are some gray areas around the proposed method (please see the strengths and weaknesses), I believe that this is a well written, thought provoking paper that can be interesting to the community and bring forward fruitful discussions. 2.Another interesting question that is not been explored is the white box performance of the algorithm.<|endoftext|>Given an image, the proposed method initiates adversarial attack on it. The method assumes the black box scenario, which is practical. 2.Some theoretical analysis is provided to establish the connection between adversary and backdoor attacks. 2.There should be some work using interpretation to detect the backdoor. Due to the close relation between adversarial attack and interpretation, I am not sure if the proposed method is still novel from this perspective. The paper proposes a good idea for using adversarial attack patterns to diagnose if backdoors exist in models. First, some adaptive studies could be conducted, analyzing the scenarios where the proposed detection method could be circumvented. This may not be very difficult since this paper assumes that the backdoor patterns are focused patches.<|endoftext|>This paper proposed an adversarial extreme value analysis (AEVA) framework to detect backdoors in black box neural networks. With linear model assumption and mean squared error loss, they showed that the mass in the adversarial perturbation would be occupied in the mask area as the backdoor sample size goes to infinity. What is the effect of the sample size used for gradient estimation on the detection accuracy? Overall, I think the paper provides a new perspective of backdoor defense, but it could be made stronger by addressing some critical aspects as listed above.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper proposes a method to generate molecular graphs with optimized properties. Molecular graphs are constructed by the iterative addition of molecular fragments in a deep reinforcement learning framework.<|endoftext|>The paper utilized many novel approaches in graph representation and reinforcement learning to resolve the timely challenge about SARS CoV 2, but the experimental results of the proposed model fail to show sufficiently powerful performance compared to existing algorithms except for better synthetic accessibility and docking scores.<|endoftext|>The novelty of the proposed method is significant, but there are some unclear parts in the description of the method, and the results do not sufficiently prove the superiority of the proposed approach (see the comments above).
Reject; rating score: 1; rating score: 3; rating score: 6; rating score: 6; rating score: 6; The authors consider neural tangent kernels (NTKs) kappa_{NT, s}^l of the neural networks of depth l associated with ReLU a and its powers a_s with a positive integer s. Theorem 1 gives relations between the RKHSs generated by the NTKs and those generated by Matern kernels. Theorem 2 gives bounds for the maximal information gain of the NTKs. Theorem 3 provides some uniform generalization bounds for kernel ridge regression with the NTKs. Lemma 1 is nice though it is an easy consequence of a recursive relation (4) given by Jacot et al.Proposition 1 follows directly from a nice asymptotic expression of eigenvalues of the integral operator associated with a Mercer kernel on [ 1, 1] which is C^\infty on ( 1, 1) but has  singularities of the same order at  1 and 1. The proof of Theorem 3 is not trivial at all and is not included in the appendix. Theorems 1 and 2 are trivial. Theorem 3 is not proved and should be wrong, to this reviewer s opinion.<|endoftext|>Later they use this eigen decay to bound the maximal information gain (MIG) of the kernels and finally present a generalization bound with respect to the number of the samples. ## Strength:This is a very well written and coherent paper. A number of comments regarding the relevance and novelty of Theorem 3 follows. Neural tangent kernel: Convergence and generalization in neural networks. Here, $f_{ntk}$ is the kernel regression predictor, using the NTK, on the same dataset, and $\hat f(x)$ is an MLP trained using gradient descent on dataset $D_n$. **Experiments**: The code for the experiments was not given by the authors, so I can t verify the correctness of the experiments. However, in the experiments the authors collect the data randomly which breaks their assumption. [8] Alberto Bietti and Francis Bach. This inconsistency is not explained in the paper. However the results for RF kernels are implicit in the other works.<|endoftext|>The paper provides a uniform generalization bound for overparameterized neural networks by assuming that the target function resides in the reproducing kernel Hilbert spaces generated by Neural Tangent (NT) kernels associated with general ReLU activation functions. The approach is based on so called maximum information gain, which is neat. The equivalence of the RKHSs generated by NT kernels and Matern kernels are discussed. To bound the generalization error, the authors provide a bound on the MIG of neural tangent kernels and random feature kernels in Theorem 2. I have some concerns as follows:1. Is it possible to consider other more general activation functions and derive similar MIG bounds? The approach based on maximum information gain is very interesting. The uniform generalization bound for overparameterized neural networks given in this paper is neat.<|endoftext|>The computation of the  asymptotics relies on an elegant use of Stein s Lemma as well as the recurrent relation between the NTK and RFK (random feature kernel) of various layers. Applications to reinforcement learning are hinted at in a discussion at the end. / References [1] Alberto Bietti and Francis Bach. 2009.This is a well written paper with a non trivial advance over the state of the art and a reasonably large number of computations, with one semi novel technique involved. The paper is well written and the results make non trivial extensions over the state of the art. Below are some more specific comments. //  after rebuttal After reading the other reviews and thee rebuttal, I would incline to slightly lower my score to (6). I notice that reviewer gYma and I both complained about the general context of "uniform generalization bounds". Contrary to reviewer gYma, I don t think the restrictions imposed make the paper unworthy of publication in itself. our..."  >  "...the corresponding eigenvalues. The proof the authors added is not that short, which indicates the paper was somewhat rushed in terms of making the deadline. I think it is still above the borderline but not by as far as I originally thought. Spectrally normalized margin bounds for neural networks.<|endoftext|>Using the above results the authors establish uniform bounds on the generalization error when learning a function from the appropriate RKHS with kernel regression. Perhaps it s not even fair to say that the paper deals with generalization bounds of overparametrized neural networks, since the main focus is on their infitely wide limit. As the genralization bounds themselves follow from previous work, the main contribution is the equivalence between the RKHS of the Matern kernels and those of NTKs associated to the powers of ReLU. The latter activations is not very common and so the applications are restricted. Some comments:  The test set $\mathcal{X}$ is never fully identified. Persumably it s the sphere, but this is never mentioned explicitly in Theorems 2 and 3, which makes it hard to appreciate the result. Perhaps it would be better to simply define it by the formula that appears at the top of page 7. The paper makes important contributions to the study of RKHS associated to the NTK.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper studies the fairness of deep learning models in classification tasks with a large number of intersectional groups. The paper has two primary contributions:1. The paper includes an empirical study that shows the inherent difficulty of this setting (i.e., due to the lack of labels), and the limitations of existing approaches. For example:     (*) The ``reweighted accuracy" with k   128 groups does account for the performance of all intersectional groups. I would recommend would be better to evaluate methods in ways that do not involve summary statistics at all (e.g., via a plot showing the distribution of group specific accuracy). I would also recommend the authors use measures of group level performance (e.g., worst case accuracy over groups, # of groups with substantial bias). This paper studies an important problem. In short, there is little work on intersectional subgroups, even less on deep learning. In this context, I see the empirical study as an opportunity to make a contribution as it can highlight previously unknown issues.<|endoftext|>This paper conducts an empirical analysis of existing bias mitigation methods on two large datasets CelebA and ImageNet where there are multiple sensitive attributes and some unavailable protected labels. The results show the existing can mitigate intersectional bias at scale but the unlabeled methods generalize poorly. This paper further proposes a knowledge distillation of independent models as regularization method (DIR) which is able to augment into other bias mitigation algorithms. 2.The claimed O(1) complexity is implausible because it assumes the availability of G group specific models. The empirical analysis is interesting but the incremental contribution concerns me the most. I would not root for this paper.<|endoftext|>This paper conducts a comprehensive comparison among existing bias reduction methods when there are multiple protected attributes. The authors also propose a knowledge distillation framework for improving fairness on the protected label scarce ImageNet dataset. Intersectional fairness is a practical and important problem. The authors conduct a comprehensive comparison among existing bias mitigation techniques, which I find valuable. I understand that training a model with the original fairness measures can be hard. It is unclear what |g| means. Is it the maximal number of subgroups? If so, then Acc_U(h)  > 0 when |g|  > \infty, which does not make any sense. (3) the intersectional bias in Eq (4) is also unclear. Interesting experimental results but the fairness metrics adopted in this paper need to be clarified<|endoftext|>This paper provides an empirical investigation of deep learning bias mitigation methods, focusing on two problems: intersectionality and missing sensitive attribute labels. The paper also proposes a new distillation based approach to resolve the runtime complexity problem that models like Domain Independent exhibit. On ImageNet their method significantly reduces bias amplification but there are no differences in group reweighed accuracy or intersectional bias. + The focus of this paper is an important problem: intersectionality in fairness. + The empirical analysis provides insight into how existing methods perform on intersectional groups and highlights fairness challenges that are particularly vexing in the context of intersectionality: runtime complexity and stability. The paper would benefit from a discussion on the hyperparameter tuning. Typically this involves a fairness accuracy tradeoff which would seem to affect the results. It may also be more appropriate to term these sensitive attributes, since protected typically evokes a legal connotation.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper addresses the problem of learning a deep learning model for dynamics forecasting which generalizes to changes in dynamics. The proposed model takes a meta learning approach and proposes to partition data into different heterogeneous domains. It consists of two components: an encoder which infers time invariant features given observed domain data and a forecaster which predicts the dynamics given these features. +* This paper addresses a new and interesting generalization problem for dynamics forecasting * It proposes a model to address different changes in the dynamics. * Evaluation is done on relevant datasets with several baselines and some ablation studies. * The applicability of the proposed approach is restricted to problems where relevant weak supervision from task parameters is available. The question of choosing relevant parameters for weak supervision is important for applying this model to other datasets, yet the definition of these parameters is unclear; how robust is the model when chosen parameters are not useful ? The performance of Wrong_enc (Table 2) tends to say that this model will then fail. * The theoretical analysis, inspired by existing work in multi task learning / domain adaptation, has some limitations and does not add much value to the paper. I have some concerns with the domain adaptation upper bound to the target error in Theorem 3.4 and Proposition 3.5. This upper bound is not minimized thus the target risk can be high i.e.the model is not guaranteed to adapt well. Moreover, the validity of the theoretical analysis is unclear as several assumptions may not be verified e.g.bounded loss in Theorem 3.1, Proposition 3.3; lipschitz continuity in Proposition 3.5. Other questions: * It would be good to better explain how the experiments include changing boundary conditions between domains. The testing scenarios only mention different initial conditions or external forces. This is the same information used by the proposed model to adapt. This paper tackles a new generalization problem for dynamics forecasting and proposes a model supported by experimental results. There are also unclarities on the ability of the model to adapt to changing boundary conditions with AdaPad, some ablation studies are missing and I have concerns on the theoretical analysis which brings limited value to the paper. After studying it, the theoretical results still have some major issues and feel disconnected from the model.<|endoftext|>Using an encoder which is trained to determine the task, the inferred latent vector is then used to adapt a forecasting network to the task at hand. Experiments on three datasets linked to fluid dynamics are then conducted to assess the proposed model. Pros :  This is an interesting problem which is quite timely given the development of the field of forecasting physical dynamics using neural networks. The proposed solution seems sound and principled. Moreover, it is well motivated and the writing was quite clear. The different additions made to the forecaster network are also quite interesting, I especially liked the AdaPad solution to deal with boundary conditions. Conducting an ablation study also considerably strengthens the paper. It would be nice to see how the model deals with other families of dynamics. Especially given the fact that the contributions of this work seem geared towards practical considerations. The setting of the experiments should be more precise and additional details should be given: how are the different datasets constructed, what supervision is there exactly regarding the different tasks, how many domains are there in each dataset and what are the differences, how is the balance between the different domains ect. This is a good work on a timely subject. The contribution is not groundbreaking but should be significant enough to warrant acceptance.<|endoftext|>The paper proposes a decomposition of such a model into an encoder that captures the innate properties of the system, and a forecaster that autoregressively makes predictions conditioned on the encoded properties. This is framed as a meta learning approach, and is shown to substantially outperform single task approaches and off the shell meta learning approaches across multiple datasets. The paper provides some theoretical analysis, and qualitative analysis of what is learned. Also, it is interesting but a bit counter intuitive that the theory section relies on results in multi task learning and domain adaptation, instead of theoretical results from the meta learning literature. Learning generalizable deep learning models across diverse settings is an important open problem. What are these loss terms and how are they different from the ones in the paper? In the ablations with no encoder, how do AdaIn and AdaPad work? U Net does seem it could be at a qualitative disadvantage compared to DyAd in terms on number of parameters, especially since U Net c is one of the more competitive baselines. It would be useful to see results for a larger U Net c, or at least some evidence that the U Net is not underfitting the training data. The fact that the encoder can be trained first and independently of the forecaster should be very useful for further rapid developments. Overall, this is very interesting and useful work. For example, the authors could train a single encoder+forecaster model across all the datasets in the paper, and analyze relationships in the learned encodings across datasets. The multi task learning and domain adaptation results are general results that are not adequately connected back to the specific model and problem the paper is considering. Yes, it is widely accepted that multi task learning and domain adaptation can work well, especially when tasks are related in some measurable way, and it can be a useful exercise to restate existing theory in the language of your framework, but what (if any) novel claims is the theory implying? Where is there potential for practical value in this theorem? Proposition 3.3 says the bound is “strictly looser” than the bound in Theorem 3.1. Adding the additional info and experiments requested could increase it further, and make this a particularly strong paper.<|endoftext|>The paper suggest a remediation for a common problem for dynamics forecasting which is the lack of generalization to other domains/tasks. The author suggest to tackle this with via a 2 component architecture, one for learning the task and one for forecasting. In empiricial experiments the authors show the practical feasibility of their approach. As a caveat: I m not an expert in the area, so my review remains on a superficial level consequently for which I apologize. I overall liked the paper quite a bit, the question discussed is relevant, the empirical evaluation is very good, the theoretical results seem as relevant as they would get and the related work discussed is crisply presented and relevant. One question I would have is that results in Table 1 are overwhelmingly good with only UNET c coming close. I m wondering how much of a gap there still is too close. Are there some complementary strengths between DyAD and ResNet that this combination can exploit better than DyAD + UNET? This is a good paper that I d like to see accepted for its combination of theoretical results, empirical results and methodological novelty.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; In short, the paper proposes a new task, that of object concept learning. To this end, the paper proposes a dataset, which starts from existing ones and extends them accordingly. Further, it proposes a baseline method that is somewhat inspired by do calculus (Pearl). This is also the authors  summary. And to do so, the do operation intervention runs over all attributes/affordances in (3) and (6)? The writing is not always very clear.<|endoftext|>The primary contribution of this paper is two folds. First, the paper presents a new crowd sourcing dataset that contains annotations for object categories, attributes, and affordances. Specifically,  is the causal graph representing the nature? This is quite tricky. I think the dataset part is a solid contribution of the paper. However, for the proposed model, I think the authors are missing important baselines and discussions with related work.<|endoftext|>This paper introduces a large annotated dataset for object concept learning. The dataset also provides causal relations between object attributes and their affordances. A deeper understanding of objects based on their physical properties and affordances can avoid such problems, and the dataset introduced in this paper is a notable step in the right direction. For a dataset that may be of wide use, it may also be good to not make its description more complex than it should be. Surprisingly, for an image dataset paper, there were very few examples of the images in the paper. The paper presentation is very dense.<|endoftext|>This paper proposes a new large scale benchmark dataset for object concept learning, which consists of recognizing attributes, affordances, and their causal raltions about objects in input images. Detailed annotations of object categories, attributes and affordances on both category and instance levels, and their causal relations (on the instance level) are provided. Is its loss part of $L_{\alpha}$? Ideally, by explicitly considering the causal relationship of $\alpha_p$ and $\beta_q$, $P_{TDE}^{\alpha_p}(\beta_q)$ should be higher than $P(\beta_q)$. Having such a large scale dataset with detailed annotations and causal relationships of visual concepts (attributes and affordances in this paper) is valuable for the entire community. The issues mentioned in the weaknesses part above are minor. They do not affect the value of the proposed benchmark dataset and baseline algorithm.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; To this end, a generalized notion based on the previous defined balance measure is introduced, based on which an integer linear programming problem is defined and is further added into an existing deep learner for fair deep clustering. The own contributions of this paper are unclear. The integer linear programming formulation is then plug into a probabilistic discriminative clustering model proposed in 2017 for deep fair discriminative clustering. The own contributions therefore remain unknown. It is a stretch to argue that the authors should not use "we" as the subjects when describing the base clustering model and others  methods which mystify their own contributions. In addition, the improvements over baselines are marginal or no improvement and flexible fairness constraint is not completely supported by the experimental results notably the ACC part. This study is not well motivated either. The drawbacks of existing studies and how these limitations are addressed by this work should have discussed to justify the necessary of this work. The rationals of selecting the base clustering model and evaluation metrics also lack of more detailed discussion. Lack of own contribution, proper motivation and sufficient justification of the proposed method.<|endoftext|>This paper proposes a novel deep fair clustering framework which addresses the problem about producing guaranteed fair predictions on clustered data with PSVs and making out of sample fair predictions for data without PSV. Weaknesses: 1. The paper said that it can be fairer in clustering without the sensitive attributes. The definition is not clear. Why do we delete these sensitive attributes and cluster the data set without sensitive attributes? 2.In the proposed algorithm, the authors first get pseudo labels y on a data set with sensitive attributes by a deep clustering model. Finally, they use \hat{y} as supervised information to train a deep model. That make me confuse: why do the authors directly add the fair constraints to the objective function or problem of the deep model? Why are the fairness constraints only used as a preprocess of pseudo labels? The present paper presents a framework to ensure the fairness of deep clustering algorithms. The paper is well organized and the motivation of this paper is clear. However, some definitions and roles about sensitive attributes are not clear. Besides, these tested datasets do not include sensitive attributes.<|endoftext|>This paper proposes a fair clustering algorithm that uses DL models to map the data into deep representations. The authors also show the equivalence between the practical fairness measure and the balance measure. The algorithms can be concluded as two steps: 1) find fair assignments $\hat y$ based on $y$;2) tune the latent representations and $y$ according to the pseudo label $\hat y$. ## Pros1) The proposed fair clustering algorithm shows the equivalence between the fairness measure defined in eqn 3 and the balance measure. 2) The experimental results seem satisfactory, especially the balance measure. 3) This paper is well organized and easy to follow. In my opinion, the major contribution is to use the fair approximation as pseudo labels. Does it mean the hard version of $Y$ as $Y$ is a continuous variable? 3) In Table 1, the balance scores of DEC and IMSAT are 0. It means that some samples from USUS are not assigned to a certain cluster (on MNIST USPS). Although these two models are non fair clustering methods, it might be inappropriate to mark the balance score as 0 simply.<|endoftext|>“Note we report both the deep model’s results and the final ILP’s results.”  What is meant here does not seem to be specified anywhere and was confusing to me (and I think resulted in my confusion elsewhere). It also presents some experiments that demonstrate the method is more effective at finding high quality fair clusterings than existing deep clustering, fair clustering, and deep fair clustering methods. I guess I know what is meant here, but there is a whole row labeled ground truth and only some of the cells in that row are blue. The paper is generally clear and well written, and the empirical results are impressive. It seems the main advantage of the proposed method over existing methods (beyond better observed performance in experiments, which is motivating in itself) is that the method guarantees a fair clustering. Unfortunately, it is not very clear how this method is able to guarantee a fair clustering, since fairness is a weighted term in the loss function, and the authors don’t seem to prove (or argue) that the method will converge to a fair clustering (I can certainly believe that it would, but perhaps it would be useful to help the reader see that this is the case), and the authors also do not address what happens when a fair clustering is not possible. I suppose the fairness guarantee involves using the output from the ILP in the last iteration as the output, but this doesn’t seem clearly specified. Minor comments:“Fairness takes two primary forms: i) group level fairness and ii) individual level fairness.” Please consider citing, I have seen other characterizations of fairness. “most of these algorithms evaluate their performance on low dimensional tabular data and mainly study the problems with binary PSV.”  Most of and mainly seem like unfortunate hedges here. Perhaps its worth pointing out which work evaluate their performance on low dimensional tabular data and study the problems with binary PSV, and which do not. “Finally, the base clustering model optimizes the clustering loss lC and lAug simultaneously” Doesn’t (Hu et al., 2017) optimize lC subject to constraints on lAug? (And isn’t this slightly different?) It seems like an overstatement to claim that by evaluating both accuracy and NMI the study is comprehensive (which means  including all or nearly all elements or aspects of something).
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes an ensemble based video compression model to capture the predictive uncertainty of intermediate predictions. (+) Experimental results suggest that the ensemble and multihead based proposed method outperforms prior works. ( ) Novelty of the paper is not clear. ( ) The ensemble based idea does not seem original as there have been several prior networks (Lakshmnarayanan et al, Agustsson et al.etc) as alluded by the authors. ( ) Should make a performance comparison with the recent paper which is readily available on archiveMissing reference Hao Chen, Bo He, Hanyu Wang, Yixuan Ren, Ser Nam Lim, Abhinav Shrivastava, NeRV: Neural Representations for Videos NeurIPS 2021It is difficult to pinpoint the novelty of the proposed compression algorithm as it stitches various ideas from various areas (ensemble, multi head, FGSM) that have already been proposed.<|endoftext|>This paper is interesting and a good try as a deep learning method for video compression. The experimental results seem strong. However, the reviewer has several concerns. 1, the motivation of multi head, i.e., the ensemble, is not clear. What is its benefit? 2, in the introduction, the authors claim that existing methods suffer from the inaccurate estimation of optical flow. In general, this paper solves a classic problem with deep learning, which is interesting. While there are several issues (see the main review section), which prevent the reviewer from scoring it high with the current status.<|endoftext|>This paper works on end to end deep learning video compression. The idea of multi head has been widely used in transformer models and it is well known that this can significantly improve performance in practice. Can the authors better contrast their idea with the multi head idea used in typical transformers? each pixel? Pls add this in the main text as this is one of the main contributions of this paper. How would the method compare with other methods when bpp is high such as> 0.15 in (a), and can the proposed approach achieve the overall better PSNR when allowing larger bpp? I am on borderline as there are specific concerns outlined above regarding contribution of multi head this simple idea, presentation of the ensemble aware loss, results when bpp is high, etc.<|endoftext|>This paper presents an uncertainty aware video compression framework with an ensemble of MV/residual decoders. To train the network, they use the ensemble aware loss function which minimizes the loss function for the k best predictions. Here are issues or concerns regarding this paper. If not, I think the authors should clarify how the underlying uncertainties are predicted and utilized to better compress the video in the entire framework. (6) Predictive uncertainties are analyzed by assuming the predictions from multi head decoders follow the mixture of Gaussian model. (8) Dependency on the optical flow estimation method. Overall, the paper was interesting because of the in depth analysis and insights. However, I have a few concerns as stated in the main review.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; In this work, the authors propose to characterize neural networks with Topological Data Analysis, more precisely with its main descriptor, the so called persistence diagram, in order to be able to compare neural networks with different numbers of layers, different numbers of neurons, or trained on different data sets. The article is well written and proposes an interesting approach, but I have some trouble figuring out how powerful and meaningful the method actually is, due to the lack of theoretical back ups and the vague, hand waving interpretations of the set of experiments. b.It is quite difficult to interpret the proposed distance matrices and values. Otherwise, it is impossible to go beyond vague comments and explanations of the results. c. I am not sure about how useful are these distances. [Post rebuttal comment] Even though I appreciate the author s responses and suggestions, I still think that the paper requires substantial improvements before publication, so I did not change my grade. Since the approach is purely experimental, I think the work is too preliminary for publication.<|endoftext|>The paper converts a given NN to a weighted directed graph and consider the flag complex on the top of it and use that object to compute the PD of the input NN. Two neural networks are then considered to be similar iff the PDs are close enough with respect to WD. I think the paper is interesting and novel. However, I have many concerns about the paper. This method suffers from this in my opinion. I think it is interesting mathematical purpose but it is not clear from a practical perspective why you want or need to do that?<|endoftext|>In this paper, the authors give a method to evaluate the closeness of a task considered by a neural network. The distance between NNs is calculated by calculating the distance between persistent homologies, and it is experimentally shown that the corresponding tasks of each NN can be determined whether they are similar or different. Strength  This paper shows that the distance between graphs calculation using TDA is more suitable for comparing tasks than the general method of calculating the distance between graphs. Weakness  As mentioned in the Related work section of this paper, Rieck et.al. Therefore, the idea of the proposed method is not particularly new. The author s perception was clear to me, but I think it still needs to be improved with additional validation, so I did not change my grade.<|endoftext|>To calculate and quantify the similarity, the authors use supported vectorized persistence summaries: Persistence landscape, Weighted silhouette, and Heat vectorizations respectively and compare their ability to measure the similarities between neural networks. Each experiment only contains one modification on hyperparameter. 2.This paper is well supported theoretically. Both the topological characterizations associated to neural networks and PH methods for measuring similarity are technically sound and significant for the problem studied. Although I think the display of experimental results could be arranged more reasonably (see weaknesses 1)## Weakness1. However, discussion about how PD discretization recognizes the different types of tasks better than 1 norm and Frobenius norm (similar to figure3), not only in the control experiments, is not clear and sufficient. 2.Very few details are given about the calculation of PD discretization, i.e., Persistence landscape, Weighted silhouette and Heat vectorizations. The motivation and the problem studied in this paper is interesting. The authors present a novel and effective approach to represent neural networks in a topological way and propose a similarity measure using PD discretization, which is supported by the experimental results.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; Due to the importance of triangle counting, a wide variety of streaming algorithms in different graph steaming models have been proposed over the years. The paper is well written, the code is provided, and has clean contributions to the problem of triangle counting, and counting cycles of length 4 in the stream.<|endoftext|>The paper proposes a one pass streaming algorithms for estimating the number of triangles in adjacency list and arbitrary order models and 4 cycle in arbitrary edge arrival order.<|endoftext|>This paper follows this line of research and applies this paradigm to the problem of cycle counting in graph streams ( specifically triangles and four cycles). 3.Is it fair to compare algorithms in terms of the number of passes? However, the authors stress that the one pass algorithm is the real contribution in these cases. Thus it is a particular case of the proposed framework.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This work revisits the population loss analysis for VAE ELBO (Dai and Wipf, 2019) and note that undesirable "asymptotic global optimas" exist, where the support of the model distribution $p_{model}(dx)$ has higher dimensions than the true data manifold. Additionally, it shows that, for linear VAE, such optimas are excluded due to the implicit bias of gradient descent, but empirically nonlinear VAEs often stuck in such optimas. It is also clear from the same source that if we lower bound $\gamma$ by a *small positive constant* $\gamma_0$, then (among the parameter sets considered in Dai and Wipf (2019) and this work) the global optima should have as small a dimension as possible, because the leading term of the loss will be $ \frac{d \hat{r}}{2}\log \gamma_0$. It would greatly enhance the paper if the authors experiment with simple fixes for this issue: e.g.clipping the decoder variance from below. 2.You may want to cite the following two papers, which improve the understanding of ELBO landscape over Dai and Wipf (2019):    1. This is interesting and demonstrates the relevance of the results in this work. + Findings about the implicit bias are interesting.<|endoftext|>The contributions are listed as follows:   For the linear case where the data is Gaussian with rank degenerate covariance, and encoder/decoder are both linear, this paper proves that VAE captures the intrinsic dimension of data distribution correctly by analyzing the objective and its gradient flow dynamics. For nonlinear cases, the paper shows a counterexample to the conjecture in (Dai & Wipf; 2019) where the support of VAE generators is a superset of that of data distribution.<|endoftext|>The authors study further on the conjectures of Dai & Wipf (2019). For the non linear case, the paper disagrees with the conjecture, and they argue that the VAE training frequently learns a higher dimensional manifold which is a superset of the ground truth manifold. Theorems and proofs are formally provided. Instead of linear VAE discussion, it would be better to discuss more on the non linear VAE case, which is more widely used. The authors only deal with toy datasets. The paper seems not ready to be published, however, the work is interesting and I m looking forward to have the paper (with additional experiment on benchmark or real world) in the revised version.<|endoftext|>This paper builds on a recent theoretical work by Dai & Wipf [1]. This work analyzes the *training* behavior of VAEs, when applied to manifold valued data. Both consider the non trivial case where the manifold’s intrinsic dimension is lower than the ambient dimension. Pros:+ Novel theoretical analysis of VAEs during training and at global optima+ Theoretical observations are corroborated by empirical resultsCons:  Theory only considers linear encoders and linear/1 hidden layer nonlinear decoders  The convergence behavior during training is only provided for linear VAEs  Work feels disjointed, writing could be clearer (e.g.Theorem 4 has very little discussion)**AFTER REBUTTAL**My understanding of the work has been improved by the rebuttal. However, I do not feel that the stance taken in the manuscript adequately represents the stance taken in the rebuttal. However, I have several concerns regarding the accuracy of the paper. In the abstract, it is implied that Dai and Wipf’s result is on the convergence properties of the VAE during training, and that the present work expands on these convergence properties. See: “Recent work by Dai and Wipf (2019) suggests that on low dimensional data, the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold.