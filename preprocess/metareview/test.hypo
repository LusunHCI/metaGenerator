The reviewers agree that this paper can be a valuable contribution to the conference, and I recommend acceptance.Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Thus, I m generally very supportive of the paper.
This paper proposes a differentiable variant of the Cross Entropy method and shows its use for a continuous control task. The problem is important and impactful. The proposed model is novel from a modelling perspective since it makes CEM part of end to end learnable models. This paper will be of interest to many readers, since it works at the interface between these. However, the reviewers were not convinced about the novelty of the proposed method, and the experiments on cheetah and walker do not compare a particularly broad set of methods. It introduces 4 hyper parameters and it is not clear how robust the method is to these. Also, why is DCEM not also sensitive to the number of
This paper introduces a new competitive/cooperative physics based environment in which different teams of agents compete in a visual concealment and search task with visibility based team based rewards (although There are no explicit incentives for agents to interact with objects in the environment). Agents trained using self play In my opinion, this is an excellent paper which main contribution is to provide experimental evidence that relevant and complex skills and strategies can emerge from multi agent RL competing scenarios. The main point of the paper is empirical RL at scale. However, there are glaring issues with some of the writing that need to be addressed before I think this work conforms to the standards of ICLR. The paper also does not give new insights
This paper proposes a new principled methodology for deriving and training RNN neural networks for prediction of permutation invariant functions. The main contribution is the empirical demonstration of the learnable nature of the obtained function unlike the learnable nature of the obtained function unlike the prior art (e.g.DeepSets) where a choice of the aggregation function severily affects the results. Authors show on simple tasks their method may outperform DeepSets, the state of the art.  The main contribution is the empirical demonstration of the learnable nature of the obtained function unlike the prior art (e.g.DeepSets) where a choice of the aggregation function severily affects the results.
This paper proposes a framework for training time filter pruning for convolutional neural networks. Specifically, they add a pruning mask for each layer and induce a sparisity loss on the mask variables during training. The pruned network is obtained by applying the learned mask to the networks.  The proposed method seems relatively easy to implement.  However, the reviewers raised several concerns about the clarity of the paper. The main concern is that the proposed method is a bit hacky and that it is not clear how it is to make it work in practice. The reviewers also pointed out that the proposed method is only valid for training from scratch where similar masks are implicitly used during training (even not
This paper introduces "projected error function regularization loss" or PER, an alternative to batch normalization. PER is based on the Wasserstein metric. The experimental results show that PER outperforms batch normalization on CIFAR 10/100 with most activation functions. The PER is presented as an objective function that minimises an upperbound on 1 Wasserstein.  The idea is clearly presented. The experimental results show that PER outperforms batch normalization on CIFAR 10/100 with most activation functions.  The idea is clearly presented. The experimental results show that PER outperforms batch normalization on CIFAR 10/100 with most activation functions.
The paper investigates the practice of using pixel level saliency maps in deep RL to “explain” agent behavior in terms of semantics of the scene. The paper is well written and clear, the literature survey is quite extensive and valuable. The main limitations the novel model aims to solve seems to be the production of "falsifiable" hypothesis in the explaination with saliency maps. However, experiments are really hard to follow and it is not clear why this is the case. In particular: visually estimating densities / correlations from scatter plots is often impossible and misleading   while the plots are nice to have, the claims regarding Figure 5, 8, 9 (b) and (
This paper builds on a recently proposed algorithm ("splitting steepest descent", Wu et al 2019) for guiding the growth of a smaller network into a larger one in architecture search. The main technical contributions are: 1) formulate linear programming to reduce energy costs, and the formulation of linear programming is straightforward. 2) address a clear scalability issue with the original work, i.e. only an additional constraint compared to the original approach of Wu et al and addressing a clear scalability issue with the original work, i.e. only an additional constraint compared to the original approach of Wu et al and addressing a clear scalability issue with the original work, i.e. only an additional
Both subprocesses in the visual perception pipeline, object detection and multiple object tracking (MOT), are considered. The authors show that by naively using existing attack approaches, the adversary needs to achieve 98% single frame attack success rate to fool the tracking system, which is too hard for existing attack algorithms. Therefore, this paper proposes a smart way of attacking MOT techniques by leveraging the properties of the tracking algorithm. The paper also presents a thorough background of MOT. The paper also provides a nice overview on the processing pipeline. The paper also provides a thorough background of MOT. The paper also provides a nice overview on the processing pipeline. Thus, it is an important step towards a more realistic attack
This paper proposes a novel approach to text to speech synthesis. The main idea behind this paper is to remove autoregressive components from the text to speech model. The authors use a wavenet VAE, which they claim can be trained without distillation (could the authors please clarify this point?)  The reviewers were concerned about the clarity of the paper and the clarity of the presentation. The reviewers were also concerned about the lack of comparisons to existing work. The authors responded to these concerns in the rebuttal, but it was not enough to change the reviewers s minds. The reviewers were also concerned about the lack of comparison to existing work. The authors responded to these concerns in the rebuttal
This paper presents a new method for learning few shot learning. The paper derives a bound for few shot performance that depends on the shot. It is shown that this makes the procedure relatively robust to the number of shots during training.  The reviewers agree that this is a valuable and novel analysis of the problem, and the problem is important since the number of shots might not be known a priori.  The reviewers also found the proof of Lemma 1 difficult to follow.  The paper is a nice reminder not to forget our linear algebra while engaged in deep learning.  The proposed method is a nice reminder not to forget our linear algebra while engaged in deep learning.
The paper introduces a fairly simple yet seemly effective method for quantizing GANs parameters. The conclusions (discriminator more sensitive than generator to quantization, quantizing both generator and discriminator helps) are sensible and interesting. The paper is clearly written and easy to follow. Weaknesses of the paper:  The related work section could be greatly improved, thereby showing the limited novelty of the proposed method (QGAN).  The related work section could be greatly improved, thereby showing the limited novelty of the proposed method (QGAN).  Some details are missing or at least lack some precision. In short, the comparison to previous works seems insufficient in my point of view.
The proposed method is simple extension of that of (Kidzinski & Hastie, 2018), by adding a term that accounts for treatment and its timing. The proposed method is simple extension of that of (Kidzinski & Hastie, 2018), by adding a term that accounts for treatment and its timing.  The reviewers agree that the proposed method is simple extension of that of (Kidzinski & Hastie, 2018), by adding a term that accounts for treatment and its timing.
The paper discusses the inductive biases in generative models that tend to assign higher likelihoods to "less complex" images. In particular, a likelihood based generative model (like Glow or PixelCNN++) trained on a particular dataset has significantly higher likelihood for data that have lower compression ratios (e.g  with PNG). The authors propose a simple approach based on the likelihood ratio between a trained model and a "prior" model (based on existing compression methods) and demonstrate that this improves unsupervised OOD detection on certain dataset pairs. Empirical results on more than 10 image datasets show that the proposed measure works better than the negative log likelihood in most cases.  The reviewers
The reviewers agree that the paper is technically sound, well written and propose an interesting modification of a previous algorithm. The algorithm is favorably compared with the state of the art on three image test sets (MNIST, CIFAR 10n ImageNet). The reviewers also agree that the paper is favorably compared with the state of the art on three image test sets.
The paper presents a new attack, called the shadow attack, that can maintain the imperceptibility of adversarial samples when out of the certified radius. The generated adversarial images are imperceptible and have a large norm to escape the certification regions. Quantitative studies on CIFAR 10 and ImageNet shows that the new attack method can generate adversarial images that have larger certified radii than natural images. Therefore, the authors claim that the method can attack certified systems. The paper is easy to follow and a lot of examples of different experiments are shown. The authors elaborate more of the results on CIFAR 10 and ImageNet.
This paper proposes an off policy reinforcement learning method in which the model parameters have been updated using the regression style loss function. The current results are not convincing enough. The reviewers agree that the main claim of this paper is a new off policy method, outperforming the previous off policy methods is a fair game. However, the main claim of this paper is a new off policy method, outperforming the previous off policy methods is a fair game. The reviewers agree that the main claim of this paper is a new off policy method, outperforming the previous off policy methods is a fair game. However, the main claim of this paper is a new off policy method, outperforming the previous off policy methods
This paper is not ready for publication in its current form. The reviewers raised several concerns about the clarity of the paper, the clarity of the analysis, and the clarity of the proposed approaches. In the current manuscript, the quantitative analysis does not immediately show the benefit or the characteristics of the proposed approaches. The authors should revise the paper and resubmit to another venue.
This paper aims at training a GAN that can generate data matches the real data distribution well especially at the boundaries of the classifiers. The authors propose a method for improving "model compatibility" in GANs. The proposed method adds to the loss of the generation procedure a term that depends on the maximum mean discrepancy between the following datasets: (1) the output of a classifier with input the real dataset, (2) the output of the same classifier with input GAN generated samples. The proposed method seems to improve the accuracy of classifiers trained on generated data. The increase in the model compatibility is very mild. The experiments only show that the proposed method got a good performance,
This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and "pair" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self attentive architectures. The paper is well written and all claims are supported. But what I see in the paper does not convince me that (*
This paper introduces a novel way to find loop invariants for aprogram. The domain is loop invariant detection, in the static program analysis space. The paper talks about neural architecture, but all I see iseffectively a curve fitting task for some template. Thisallows an invariant to be learned. This feelsdifferent from the code2inv paper where a program can be fedinto the system and the pretrained model emits the invariant. The paper talks about neural architecture, but all I see iseffectively a curve fitting task for some template. This feelsdifferent from the code2inv paper where a program can be fedinto the system and the pretrained model emits the invariant.
The paper proposes Episode Reinforcement Learning with Associative Memory (ERLAM), which maintains a graph based on the state transitions (i.e.nodes correspond to states, and edges correspond to transitions) and propagates the values through the edges in the graph in the reverse order of each trajectory. It organizes the memory as a graph in which nodes are internal representations of observed states and edges link state transitions. They show that this regularizer facilitates learning, and compare to other nonparametric approaches. Experimental results show that ERLAM significantly improves the sample efficiency in Atari benchmarks. The experimental results demonstrate that the proposed method is promising. However, the paper lacks theoretical rigor to explain
This paper proposes LORD, a novel non adversarial method of class supervised representation disentanglement. The main issue that the authors tackle is the information leakage between the representations for the class and content. Firstly, the paper makes a distinction between content and style. Second, the authors observed that the amortized models leak class information into the content representation. The reverse, leakage from class codes to content codes is achieved by adding any asymmetric noise regularization term. The experimental results indicate that LORD succeeds to disentangle class information on content codes, while it outperforms style content disentangled representations on style switching tasks (Figure 2 & 3). The authors demonstrate impressive empirical results both in terms
The paper introduced a way to modify densities such that their support agrees and that the Kullback Leibler divergence can be computed without diverging. The approach is motivated from the concern that traditional divergence such as f divergence or KL divergence may not always exist, in which the spread divergence may be a substitute. The concept of spread divergence can be valuable in many context.  The reviewers agree that this paper is an interesting contribution, but the motivation of this paper is interesting.  However, the reviewers are not convinced about the applicability of the approach.
This paper is an extension of the prior work Kurth Nelson & Redish (2009). It seems to me that the difference between this paper and Kurth Nelson & Redish (2009) is that in this paper, the approximated Q value with hyperbolic discounting function is a weighted sum over each Q values using exponential discounting factor gamma, while in Kurth Nelson & Redish (2009), the Q value is estimated by sampling one Q value based on the distribution of the gamma. Also there is no measure of standard error or statistical significance on the graphs. Therefore, I am not sure about the technical contribution of the new hyperparameter set for the experiments.
The reviewers agree that the paper is not ready for publication at ICLR. The main concern of the reviewers is that the main claim of the paper is that combining physics models with NNs is better than NNs alone, esp. that with the more accurate hybrid model, model based learning performs better or transfer learning is more straightforward. The paper would be much stronger if the it could demonstrate that with the more accurate hybrid model, model based learning performs better or transfer learning is more straightforward. The paper is not ready for publication at ICLR.
This paper tries to improve the performance of Chinese NER by developing a novel attention mechanism that leverages BERT pre trained model which considers bi directional context. Experiments on a number of tasks show that the proposed approach is effective. The proposed model outperforms sometimes the other models, often by a small margin as it is usually the case in NER experiments. But more insight on the strengths of the models should be given by conducting an ablation study. The contribution from the paper is limited, and the novelty is a bit low. The proposed algorithm is simple and effective, but the novelty is a bit low. The proposed algorithm is simple and effective, but the novelty is a bit low.
In this paper, the authors introduce an algorithm to learn a stable controller using deep NN actor critic method. Through the specific parameterization of quadratic Lyapunov function (in the latent space), the authors proposed learning a new critic function that is a value function but at the same time (almost) satisfies the Lyapunov constraints. Then the target network is trained to minimize the difference between the target and the critic. They also show in several Cartpole, Mujoco, and Repressilator experiments that this approach is more robust to perturbations (such as sinusoids), where the agent are more robust to dynamic uncertainties and disturbances. While this is an
This paper tackles the problem of scaling object oriented generative modeling of scenes to scenes with a large number of objects. The main contribution is in improving the efficiency of object detection/tracking by parallelizing the computation, without much conceptual innovation. The key components of their approach is the parallel discovery and propagation of object latents as well as the explicit modeling of the background. Both can be measured as functions of the number of objects in the scene. Most of my comments are addressed well. I thank the authors for the detailed rebuttal, as well as for the updates to the text and several new experiments in the revised version of the paper. I recommend acceptance.
The authors propose BlockBERT, a model that makes the attention matrix of Transformer models sparse by introducing block structure. This has the dual benefit of reducing memory and reducing training time. The model gains ~20% efficiency with ~20% decrease in memory use while maintaining comparable performance to the state of the art model. The paper is clear and well organized with good experiment results. However, it is not clear the savings from this architecture compared to sparse Transformers in general. No results are given at test time : is there also a memory and processing time reduction ? Even if the proposition is interesting, the impact of the paper is limited to the (flourishing) scope optimising Bert models.
This paper investigates to what degree Convolutional Neural Networks (CNNs) learn to encode positional information. The paper mainly discussed the zero padding and found it is the source of position information.  The observations and findings are interesting and helpful to the community.  The reviewers agree that this paper is interesting and well written.  The paper is well written and well motivated.  The paper is well written and well motivated.  The paper is well motivated.  The paper is well written.  The paper is well motivated.  The paper is well motivated.  The paper is well written.  The paper is well motivated.  The paper is well motivated.  The paper is well written
The paper presents a novel notion named glocal graph curvature, which offers a solution to determine the optimal curvature for embedding. The emphasis is on determining the correct dimension and curvature to minimize distortion or a threshold loss of the embedding. The authors study this setting, consider a variety of existing notions of curvature for graphs, introduce a notion of global curvature for the entire graph, and introduce a notion of global curvature for the entire graph, and now to efficiently compute it.  The reviewers agree that this paper is not ready for publication in its current form.  The paper is not ready for publication in its current form.
The authors detail PUGAN, architectural changes to models for raw waveform generation with GANs. The paper is well motivated and experiments are correct, but the quality improvements overall are a little underwhelming. The proposed PUGAN was still tested on very simple dataset (sounds of ten digit commands). However, the proposed PUGAN was still tested on very simple dataset (sounds of ten digit commands), and the quality of generated samples are only comparable to WaveGAN. The posted failure cases and some samples tend to have overlapped sounds from different digits. Also, the fact that evaluations are carried out with PUGAN 1 suggests that the progressive training does not really works well past
This paper shows the non acceleration of Nesterov SGD theoretically with a component decoupled model. Moreover, the authors introduce an additional compensation term and derive a novel optimization method, MaSS. They demonstrate that this adjustment is necessary and sufficient to benefit from the faster convergence of Nesterov gradient descent in the stochastic case. Their approach is justified by a well conducted theoretical analysis and some empirical work on toy datasets.  The reviewers agree that this paper is well written and well motivated.  However, there were some concerns about the experimental results, which were addressed by the authors in the rebuttal. However, the reviewers were not convinced that the theoretical results were sufficiently strong. 
This is a clear case of less than half baked paper. The paper is very poorly written, with many incomprehensible passages. The paper does not cite any previous research (no references) and is poorly written. The paper does not cite any previous research (no references) and is poorly written. The paper does not cite any previous research (no references) and is poorly written.Reject; rating score: 1;
This paper presents an unsupervised learning approach to solve forward and inverse problems represented by partial differential equations. A framework is proposed based on the minimisation of loss functions that enforce the boundary conditions of the PDE solution and promote its smoothness. While the paper presents an elegant solution folding forward and inverse problems into a single framework, the presentation is missing a few important details which difficult the assessment of the contribution and favour a rejection of the paper. The main issues are insufficient experimental comparisons and a lack of theoretical support for the method. The experiments in the paper is limited. It compares with only one work in the forward task, but no comparison in the inverse problem. The experimental results are presented for
This paper presents an automatically designed semantic segmentation network utilising neural architecture search. The proposed method is discovered from a search space integrating multi resolution branches, that has been recently found to be vital in manually designed segmentation models. To calibrate the balance between the goals of high accuracy and low latency, the authors propose a decoupled and fine grained latency regularization, that effectively overcomes the observed phenomenons that the searched networks are prone to “collapsing” to low latency yet poor accuracy models. When compared to related efficient architectures, the proposed method results in competitive accuracy at significantly higher frame rates. Moreover, the authors extend the proposed method to a new collaborative search (
The paper proposes an evaluation framework for Zero Shot Learning (ZSL) methods called zero shot learning from scratch (ZFS) where the model is not allowed to be pretrained on other datasets such as ImageNet. It compares supervised, unsupervised, and self supervised representation learning methods and studies the impact of locality and compositionality on ZSL performance for these methods. Two main criteria are studied to study neural networks:  compositionality (ability to be expressed as a combination of simpler parts)  locality (ability to encode only information specific to locations of interest)To provide a better understanding of their claims, the authors use MTurk annotations to construct boolean map for each local part labelled in
The authors proposed a double neural counterfactual regret minimization algorithm (DNCFR) that uses a RegretSumNetwork to approximate cumulative regret and an AvgStrategyNetwork to approximate the average strategy.  The paper introduces two neural networks, one for average regrets and one for average strategy, to (approximately) run CFR algorithm in (large) IIG.  The paper is very close to the "Deep Counterfactual Regret Minimization". I believe it should be accepted.  The reviewers agree that the paper should be accepted.  However, there are some issues with the paper. The calculation of average strategy seems wrong. The sampling probability of each information set is not equal
This paper is interesting, and the problem (modeling of diversity constraints) seems important. They have experimental results (on metric learning and image learning tasks) to show that optimization with the DPP + wasserstein  gan constraint (to ensure features lie in a bounded space) result in better quality. However, the authors do not report standard deviations for their experiments. I recommend that this paper be rejected. The authors should carefully edit the paper before publication so that their work will be properly presented.
The authors propose a multi task learning method that uses attention mechanism to identify relations between the tasks. Their major argument is that most current methods hold the assumption that the tasks are correlated with each other but they conjecture that in the real world this is not necessarily true and try to model the relationship between the input tasks at these two levels and incorporate that in the learning framework. In the proposed network, different tasks have their own encoder, which leads to a large number of model parameters especially when there are a large number of tasks. In the proposed network, different tasks have their own encoder, which leads to a large number of model parameters especially when there are a large number of tasks. In the
The authors analyze the bias in the straight through gradient estimator using the framework of harmonic analysis of boolean functions. Based on this analysis, they propose three methods to reduce the bias of the straight through estimator, resulting in a less biased estimator that is the same computational complexity as the original.   The reviewers agree that the paper is interesting, and the results are convincing. However, the reviewers are not convinced that the Fourier analysis is essential to show the bias of the estimator, and the only conclusion they draw from Fourier analysis is eq.5 (which has a straightforward proof), using the conventional notation instead of Fourier coefficients. Overall, I argue rejecting the paper in its
This paper proposes a framework for augmenting unsupervised graph embedding methods by (a) fusing feature information into the graph topology, (b) learning embeddings on a coarsened graph, and (c) refining the coarsened embeddings to obtain embeddings for the original graph nodes. The paper proposes a well designed pipeline to scale existing embedding models. While such a pipeline for scaling using a graph coarsening and refinement based approach is not new, the authors have carefully designed the pipeline to be effective and be scalable such as without any costly learning components (as in mile). The effectiveness of the proposed approach is evaluated with the node classification task on
This paper argues that text generated by existing neural language models are not as good as real text and proposes two metric functions to measure the distributional difference between real text and generated text. The proposed metrics are questionable and should be thoroughly tested on synthetic and toy datasets before deploying it for text generation. The proposed metrics are questionable and should be thoroughly tested on synthetic and toy datasets before deploying it for text generation. The proposed metrics are questionable and should be thoroughly tested on synthetic and toy datasets before deploying it for text generation. Overall, the writing also contains many grammatical errors and confusing at places. The description in the first paragraph about neural language models is not accurate. The proposed metrics are not particularly well motivated
This paper studies policy gradient where the policy is parameterized by an extremely wide neural network. Theoretical results show that, in the aforementioned setting, policy gradient and natural policy gradient converge to a stationary point at a sublinear rate and natural policy gradient s solution is globally optimal. While these results may not have immediate practical interest, the analysis is an important step in understanding the behavior of actor critic algorithms with neural networks. The paper is well written with clear derivations. The authors should provide more details about the function $u_{\hat \theta}$ defined in eq (4.4), which seems to approximate the critic function. I suggest the publication of this paper.
This paper proposes a generative modeling framework called potential flow generator. It is based on the idea that samples flowing from one distribution to another should follow a minimum travel cost path. Instead of using general vector fields, the authors apply the potential vector fields in optimal transport theory to design neural networks. This regularization is expressed as an optimal transport problem with a squared Euclidean cost.    The reviewers agree that this is an interesting idea, and the paper is well written and well motivated. However, there are a number of concerns about the paper. First, there is a lack of comparison with existing methods, such as Seguy et al. (2018) and Wasserstein natural gradient
The reviewers agree that the proposed model, RigL, is memory and computation efficient, and thus allows to train a large network in an efficient manner. As the authors state, the novelty of their method is that they use the gradients with the highest magnitudes to grow connections. The experimental validation is largely lacking, as the authors only perform experiments on ImageNet and do not compare against recent state of the art Bayesian sparsification methods (SBP, VIB, L0 regularization).  The paper does not compare against recent state of the art Bayesian sparsification methods (SBP, VIB, L0 regularization).
This paper proposes to train a GAN and an EBM jointly, and bridge them using a Stein discrepancy. Both the idea and the experiment results are interesting. However, there are a lot of gaps in the proofs of Theorems 1 and 2. There are also typos and issues elsewhere. Overall, the paper is not ready for publication.
The authors propose a new image compression method that does not require quantizing the encoded bits in an auto encoding style image compression model. Based on the encoder decoder framework, it replaces the discrete codes by continuous ones, so that the learning can be performed in an end to end way. With the construction of parameterized encoder / decoder, end to end training is conducted by popular gradient descent. The compression performance is evaluated by training the proposed model and the competing neural network based method [1] on the Clic dataset, and evaluating it on a subset of the images of the Kodak dataset. JPEG is also used as a baseline. Experiments were conducted on the Kodak dataset
This paper proposed a new algorithm for synthetic data generation under differential privacy. The paper considers synthetic data generation using deep GANs and autoencoders that can be shared for model training. The reviewers raised several concerns about the clarity of the paper and the presentation of the paper. In particular, there is a significant number of misprints and inconsistencies here and there (see more on this below). The algorithm $\mathcal{M}$ is not defined. The authors mentioned comparison with DP GAN but it is not marked in the figures the performance of DP GAN and how its results compared with DP auto GAN. The authors claimed that the proposed new evaluation metrics are novel contributions of the
The paper empirically studies the category selectivity of individual cells in hidden units of CNNs. Previous works have used different measures of selectivity (with sometimes contradictory results), and the authors investigate the degree to which these units qualify as “object detectors”. The authors find that (1) different proposed measures of selectivity are not consistent and (2) units identified as selective cannot be considered object detectors due to the high false alarm / low hit rates, analyzing a large number of selectivity measures, analyzing a large number of selectivity measures, analyzing a large number of selectivity measures, analyzing a large number of selectivity measures, analyzing a large number of selectivity measures, analyzing
This paper proposes a new method to disentangle the sentence level reward into word level ones with Adversarial Inverse Reinforcement Learning (AIRL). The proposed method achieves state of the art performance with several evaluation scores. The motivation there is to use the recovered reward on a new system with different transition dynamics. Diversity analysis and ablations are also performed to dissect the performance of the proposed approach. As reported, this refinement surprisingly improves the performance and achieves state of the art performance, while the original AIRL degraded the performance.   The reviewers agree that the proposed method achieves state of the art performance with several evaluation scores. The motivation there is to use the recovered reward on a new system
This paper describes a self supervised sentence embedding approach that incorporates a different view from plain text where some extent of linguistic knowledge is incorporated through the application of tree LSTM.  The reviewers have raised concerns about the experimental results and the claims in the paper.  The reviewers also raised concerns about the clarity of the paper and the clarity of the experiments.  Overall, I think the paper is not ready for publication at ICLR.  The authors should improve the clarity of the paper and improve the experimental results.
This paper presents a method for predicting the trajectories of objects in video (or phone) games by learning the physical parameters underlying the movements.  The problem addressed by this paper is the estimation of trajectories of moving objects thrown / launched by a user, in particular in computer games like angry birds or basketball simulation games.  The reviewers raised several concerns about the simplicity of the task (parabolic trajectories without any object/object or object/environment collisions / interactions), the interest of the task for the community (how does this generalize to other problems?), and the writing and structuring of the paper.  The paper is not well enough structured and written, many things are left unsaid
This paper proposes a linear interpolation method that could be applied to the latent space of a generative model. If the density of latent feature is normal, normal density based index is used and it could be approximated with a analytic form; while if the density is not accessible, then f is the gaussian density of certain transformation of features in the latent space. With their method, interpolating instances generated by those generative models all maintain high quality in terms of the realism index they proposed. While it is obvious that this score could be used, is it possible to make the empirical assessment? E.g.compare between two scores on an extensive amount of data. Could the authors elaborate
This paper is concerned with network multi agent RL (N MARL), where agents need to update their policy based on messages obtained only from neighboring nodes. Each agent might control a traffic light (exp 1) or a car in traffic (exp 2). The authors compare their method with CommNet (averages messages before broadcast), DIAL (small scale direct communication), etc. The main contributions are 1) a spatial discount factor that can stablize the learning process, 2) a differentiable communication protocol NeurComm. The experiments and analysis of the more general communication scheme are nice and the assumptions used make sense for the environments considered (spatial interactions and dynamics). The evaluations are comprehensive.
The paper studies a number of interesting phenomena in deep learning by characterizing the linear regions of fully connected ReLU networks. The authors derive the induced distributions of the function space parameters and show that increasing width can reduce the roughness of the initial function. They found that the value of depth in deep nets seems less about expressivity, but enable GD to find better solutions. The paper has interesting analyses, but I think the main drawback is that the clarity and presentation could be improved. In particular, while reading the paper, I found myself wanting:  More discussion of connections to relevant work.  More expository text for particular concepts.  A number of results are presented, and it would be
This is the same idea behind image augmentation which forms a crucial part of training supervised models in vision. Adding noise to for regularization is a quite common technique used in many different applications. It would surprise that this very natural idea has not been tried before. I have gone through the theoretical derivations in the paper and they look sound to me. The experiment methodology nevertheless looks sound. I am not an expert however in this setting so it is hard for me to judge the quality and significance of the benchmarks.  The experiments performed in the paper are all very small scaled. The claims made in the paper is not well supported and the experiments are not very convincing. I recommend reject.
The paper proposes a network architecture with multiple bottleneck layers, one for each label level, and skip connections. The network is a nested structure of successive classifiers. This ensures the relationship of the entropies between the label layers. The experiments show that coarse labels help learning and can improve label efficiency, i.e. don’t need all fine labels to get good classification performance.  The paper is well organized and argued, although there is a little belaboring of ideas of entropy and mutual information only to use it to buttress a point that is made and left hanging.  The empirical demonstration of contribution of skip connections is not too  The empirical demonstration of contribution of skip connections
This paper proposes a simple idea to mitigate gradient staleness in asynchronized distributed systems. Instead of scaling the staleness as in SA method, the authors propose to scale with the GAP, which is defined as the distance between current parameter and the staled parameter. In particular, when the master receives a gradient from a worker, it computes the norm of the difference between the current model parameter and the past model parameter associated with the gradient. The master then computes *gap* value based on this norm and the norm of the average gradient. This is much higher than the staleness aware method where the master stores a single scalar for each worker where the master stores a single scal
The main contribution of this work is the improvement of an end to end waveform to waveform separation models through a number of architectural changes that allow such waveform to waveform models to perform comparably with other current state of the art methods that instead operate in the spectrogram domain. They clearly outline all the architectural changes they make (GLU nonlinearities, strided upsampling, bidirectional RNN), perform thorough evaluations against strong baselines, and a complete ablation study to demonstrate the value of each component. They also report a very marginal performance improvement over an STFT based model (open unmix) The improment is 0.02 dB (table 1).
This paper presents a method to derive shaping rewards from a representation learnt with CPC. They propose learning a CPC representation from some random data and fix it. Specifically, they propose to cluster the embedding using the clusters to provide feedback to the agent by applying a positive reward when the agent enters the goal cluster. They use the resulting embedding to learn a reward function and ignore the extrinsic reward.   The reviewers had several concerns about the method as currently implemented and the empirical comparisons (specifically with the chosen competitors) which I detail below. The main difference from the previous work (i.e. CPC) is that the paper uses the learned representation for reward shaping, not for learning
This paper proposes a multi image matching method using a GNN with cyclic and geometric losses.  The reviewers agree that this is a novel and novel approach to learning feature representations, a topic of great interest to the ICLR community, rather than a new structure from motion system that has to prove its superior performance over the current state of the art. Their experiments show that this is a promising approach, but probably requires further research to achieve state of the art results.  The paper is not ready for publication at ICLR.  The reviewers agree that the paper is not ready for publication at ICLR.  The paper is not ready for publication at ICLR.
The submission proposes to reduce the memory bandwidth (and energy consumption) in CNNs by applying PCA transforms on feature vectors at all spatial locations followed by uniform quantization and variable length coding. It proposes to use lossy transform coding before sending network output to memory. The proposed method and initial results are promising. However, the novelty of the proposed method is borrowed from previous work (e.g., Goyal 2001), hence the novelty of the proposed method is in doubt. It’s harder for me to precisely assess the significance of the proposed approach, but at a high level it looks reasonable and is backed by convincing empirical evidence.
This paper presents a multi agent learning approach for learning how to set payoffs optimally for crowdsourcing contests and auctions. The underlying idea is to collect data pairs (i.e., [design, utility]), fit a model to the data, and then optimize over all the designs to figure out the best one. The authors mainly applied their method on an auction design problem, and finished a few experiments.  The main concern of the reviewers is the lack of novelty. The authors only consider 3, 4 agent auctions. However, as the authors demonstrated in Figure 1, the main idea of this approach is: collect the data, fit a model, and finally optimize the objective, which is a
The authors present an interesting exploration of initializing non normal RNNs that outperform the orthogonal counterparts.  The paper is well written and pleasant to read.  The authors do a great job in motivating the paper, and the explanation is clear and easily understandable. The novelty of the work is limited though.  The reviewers pointed out the limitation of orthogonal initialization alone for nonlinear RNNs, and proposed closed form initialization for RNNs with different activation functions. It would be worthwhile to include a comparison to that method.  Overall, I think the method is promising, but comparison with prior work is missing.  The paper is well written and pleasant to read.
This paper shows how to train an autoencoder that preserves topologicallyrelevant distance across varying length scales. The main idea proposed in this work is to use the topological signature directly as a loss for autoencoders. This is realized via an additional (i.e., in addition to reconstruction) loss term (optimized over mini batches) which requires differentiating through the PH computation. Embedding quality is evaluated with a wide variety of metrics on realand synthetic datasets highlighting the preservation of global and localtopology. There are questions here and there (see below), but I do think they can be answered. Overall, I think this is a nicely done paper, but with
This paper proposes to meta learn a curiosity module via neural architecture search. The method is evaluated by learning a curiosity module on the MiniGrid environment (with the true reward being linked to discovering new states in the environment) and evaluating it on Lunar Lander and Acrobot. A reward combination module (which combines intrinsic and extrinsic rewards) is further evaluated on continuous control tasks (Ant, Hopper) after having been meta trained on Lunar Lander. The paper is very novel   the idea of developing a domain specific language full of building blocks to represent various curiosity modules is unique and interesting. There are also some interesting results in the appendix which show the efficacy of their predictive approach to
This paper describes a simple yet effective technique for learning temporal point processes using a mixture of log normal densities whose parameters are estimated with neural networks that also adds conditional information. The paper proposes to directly model the (conditional) inter event intervals in a temporal point process, and demonstrates two different ways of parametrizing this distribution, one via normalizing flows and another via a log normal mixture model.  The reviewers agree that the paper is well written and the experiments are interesting. However, the paper could be improved by making the role of parameters like K more explicit. The paper also needs to be made more rigorous by making the role of parameters like K  more explicit.
This paper presents an interesting quantization technique that is, unusually, end to end trainable and not just an inference technique. The idea is to have a learnable threshold Delta that determines if an output activation should be computed in high or low precision, determined by the most significant bits of the value. Assuming that high activations are more important, these are computed at higher precision. According to the experiments, the method achieves better performance and computational savings as compared to other quantization method baselines. Fig.2 shows by example how the method works for an input $I$ and an output $I$.  The reviewers agree that the paper is interesting, and the experiments show substantial performance gains
This paper proposes a general framework for constructing Trojan/Backdoor attacks on deep neural networks, specifically in cases where the end user plans to perform transfer learning on the backdoored classifier. The authors argue that the proposed method can support dynamic and out scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting. The proposed method is general, and is shown to work across a variety of datasets. However, I am not convinced that the proposed approach is necessary a good way to do so, and have the following questions:1. The proposed method is general, and is shown to work across a variety of datasets. However, I am not convinced that the proposed approach is
This paper proposes to learn a latent space for the PixelCNN by first computing the Fisher score of the PixelCNN model and then projecting it onto a lower dimensional space using a sparse random matrix. The core idea is that it is possible to use (a smaller dimensional projection of) the Fisher score of the density function defined by the autoregressive model to represent data points in embedding space, and a neural decoder for mapping them back to input space. The paper proposes to get around this problem by projecting the Fisher score onto a lower dimensional space using random matrices.  The reviewers were concerned about the novelty of the paper and the comparisons to baselines. The reviewers also raised concerns about the
This paper proposes a new theory for this domain that extends generalized discrepancy theory to multi source setting. The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited. The evaluation of the proposed method is not complete. For this reason, I think it is okay but not good enough at this time.Reject.
This paper proposes an adversarial representation learning approach. Authors argue that proposed approach can simultaneously achieve accuracy parity and equalized odds. The authors show both theoretically and empirically that the proposed algorithm show better utility fairness tradeoff on balanced datasets. The experiments demonstrate utility of the algorithm for balanced datasets (without sacrificing performance to fairness) and show that while achieving equalized odds they don t hurt demographic parity. This is indeed a useful result. The reviewers agree that this paper is an interesting and useful contribution to the fairness literature. However, there is still a lot of work to be done in this area. There should be an additional condition on distribution D which is not clear at that moment in the paper.
The paper promises efficient training of spiking neuron models using back propagation. The main idea is to use neuromorphic chips that use spiking neural networks with an approximation that is basically similar to an artificial neural network. The authors say that this is important because spiking networks offer significant energy savings, yet they typically perform poorly compared to prevalent artificial neural networks (ANNs). No comparisons are to baselines are presented in the text.  The reviewers agree that this is a critical topic for the ML community, and the proposed approach seems logical and the empirical results are convincing. Unfortunately, it is not possible to assess if the proposed neuron model is working on energy efficient architecture. This point should be shown experiment
This paper presents a novel video generation method with spacio temporally consistent features. The authors introduce a spatio temporal adversarial discriminator for GAN training, that shows significant benefits over prior methods, in particular, parallel (as opposed to joint) spatial and temporal discriminators. In addition the authors introduce a self supervised objective based on cycle dependency that is crucial for producing temporally consistent videos. They provide a comprehensive ablative study of the proposed method and also show comparisons against baselines for the tasks of VSR and UVT. The methods are evaluated on two datasets and user studies. The results show that the proposed method is superior across the board compared to previous approaches from the literature. 
The authors discuss the a novel technique, called Random Distance Prediction, to learn rich features from domains where massive data are hard to produce; in particular, they focus on the two tasks of anomaly detection and clustering. The paper is well structured with background literatures, formula, as well as experiments to show the advantage of the proposed method. The paper is well written and understandable by a non specialistic audience; introduction and references are adequate, and the theoretical analysis is reasonably explained, and the theoretical analysis is reasonably explained, although the theoretical analysis is reasonably explained, and the theoretical analysis is reasonably explained, although the theoretical analysis is reasonably explained, and the theoretical analysis is reasonably explained, although the theoretical
The reviewers agree that this paper is not ready for publication at ICLR. While the idea is natural, there is a prior work [1] that has investigated the effectiveness of learned loss in gradient based meta learning, which seems pretty similar to this paper. Moreover, the presentation in this paper is hard for me to understand technical details and see the difference with existing methods. Right now, the method is only compared to ML3 with task loss, which seems not very conclusive. Besides, I wonder how important the extra information added during the meta training time is and the authors should present comparison to ML3 without the extra information. The authors should present comparison to ML3 without the extra information.
This paper provides a new method to deal with the small batch size problem of BN, called MABN. The paper proposes the improvement of batch normalization techniques for the case of small batch size. The paper addresses this issue by analyzing extra statistics in the batch normalization and introducing moving average statistics, weights centralization and a slightly modified normalization. Experiments on typical datasets demonstrate the effectiveness of the proposed trick. The theoretical analysis and guarantees are provided as well. Overall, the idea is interesting to me. However, there are several issues not addressed by the authors (the assumption of this paper  "W mean(W) and X_{input} are irrelevant"; why not using Image
This paper proposes an 8 bit floating point format for training neural networks using 8 bit floating point numbers (FP8). The goal is highly motivated: training neural networks faster, with smaller memory footprint and energy consumption. The key idea here is that for each tensor of 8 bit numbers, two 32 bit floating point statistics are recorded as well. While the method is interesting, I do not think it is practical due to the required hardware modifications. I am by no means not a hardware design expert, but I am not convinced that the gain of using 8 vs 16 bit floating point numbers outweights any extra complexity of hardware implementation. For a large tensor, this additional reduction to compute statistics may be
This paper studies the role of depth on incremental learning in several toy models for neural networks. It starts with analyzing the optimization dynamics of a toy model, and showing that it follows incremental learning, a notion defined clearly in the paper. In particular, they show that in these models, deep models require polynomially small initializations to exhibit incremental learning than shallow models. The authors contribute analysis for non asymptotically small initializations, and study an interesting role of depth in how small this initialization must be.  The reviewers agree that the paper is well written and well motivated. However, there were some concerns about the relevance of the toy model to more realistic settings, which is really
This paper presents an interesting theoretical result on providing stochastic robustness guarantees for Lipschitz constrained neural networks. However, it is not clear what the crux of the paper, the new architecture is doing. The paper is missing references to several key pieces of related work tackling similar problems. The paper could see some improvements in notation and some erroneous claims: The paper could see some improvements in notation and some erroneous claims:1. The method of Gouk et al. constrained maximization of convex functions is a hard problem. As such it is not surprising that. The method of Gouk et al. constrained maximization of convex functions is a hard problem. As such it
This paper proposes an autocurricula scheme to train a goal conditional agent in a dynamic and sparse rewarding environment. The authors propose a combination of different losses to help the setter balance its goal predictions — validity, feasibility and coverage. Given a desired distribution of goals, the learning becomes more efficient. Experimental results show that different combinations of the three types of losses can bring improvements in some scenarios. Empirical studies demonstrate the capability of the proposed model in generating task curricula across several complex goals. The paper compares this method with Goal GAN as a baseline and outperforms it on the three tasks. A minor concern is about the experiments, but the authors addressed this in the rebuttal
This paper proposes learning a branching heuristic to be used inside the SAT solver MiniSat using reinforcement learning. The agent is trained with Q learning and GNN as the function approximator: the input is presented as a bipartite graph with variables and clauses corresponding to a SAT instance in CNF form and the GNN predicts Q values for assigning each unassigned variable to True or False. The policy is trained using DQN. At each step of an episode the policy selects a variable to branch on and assigns a value to it. The episode terminates once the solver finds a satisfying assignment or proves unsatisfiability. Results on randomly generated SAT instances show that the learned policy
This paper proposes a novel Transfer Learning Method, namely Adversarial Inductive Transfer Learning (AITL), which adapts not only the input space but also the output space. The paper proposes an adversarial transfer learning network that can handle the adaptation of both the input space and the output space. The idea is interesting. However, the proposed method is not convincing from either theorical analysis or experimental results. The main contribution of the paper is in identifying a new type of problem that may be worth studying. However, the reviewer is concerned about the modeling contribution made in this paper is somewhat incremental given existing literature. It is not convincing that the method can be used in another application.
This submission provides a new theoretical framework for domain adaptation. The paper presents an in depth analysis of compression and invariance, which provides some insight. However, the reviewers have several concerns about the clarity of the analysis, the organization of the manuscript, and the experimental evaluation. The weighting aspect of the contribution is not supported by any experiment. Overall, this paper is not a thorough theoretical work because of above reasons.
This paper presents a data augmentation procedure for semi supervised learning on graph structured data. Instead of trying to incorporate an augmented dataset into the graph, this paper uses a seperate fully connected network (FCN) that shares weights with a graph neural net (GNN). The proposed method for semi supervised graph learning is based on data augmentation which is efficient and can be applied to different graph neural architectures, that is an architecture agnostic regularization technique is considered in the paper. The relative simplicity and generality of GraphMix is appealing. However, the proposed method for semi supervised graph learning is based on data augmentation which is efficient and can be applied to different graph neural architectures.
This paper proposes a graph convolutional network for graph representation learning.  The authors identify a shortcoming of existing GNN architectures for graph classification tasks   specifically, the fact that, in the featureless regime, the graph convolutional layers rely on propagating very rudimentary structural information, making it hard (or impossible) to distinguish graphs with similar local structure. More precisely, they propose a graph neural network that apply several layers of graph convolution in parallel to the node attributes and to the embedding, then takes an average of the result, which is fed to another series of graph convolution. This is combined with some form of sampling which strongly resembles median based quantization but is
The paper proposes an alternative view of obtaining generative models by viewing the generation process as a transport problem (specifically, fluid flow mass transport) between two point clouds living in high dimensional space. This paper targets training generative adversarial networks with a formulation that is motivated by mass transport of fluid flows. However, with the introduction of a transport based formulation, its respective issues may arise, that are not described in the paper. The smooth matching of point clouds does not retain too much of the initial fluid flow model. The experimental evaluation demonstrates on only two simple examples the results of the work. The experimental validation is limited and the results are not compared to other existing methods for generative models.
This paper introduces the idea of energy based model to the traditional classifier, and proposes a new framework to improve the performances of the model in multiple aspects. While much of the current work is related to normalizing flows / invertible neural networks, the authors here propose a quite simple but appealing method: A standard neural classifier is taken and the softmax is layer chopped off and replaced by an energy based model, which models the joint probability p(x,y) instead of the posterior p(y|x). A couple of details on the training procedure are missing in the experimental part. Overall, the paper provides a substantial contribution and paves the way for further work improving this joint
This paper proposes a new family of quantized neural networks, where the weights are quantized based on fixed precisions. This theory is based on minimizing the L_2 loss between the original data and quantized data. The reviewers appreciated that this quantized DNN paper has a theory. However, the approximation is loose without any guarantee. Moreover, some of the recent quantization methods are not compared. On the other hand, I checked several proofs provided by the authors for the 1 bit and 2 bit quantization cases. The proofs look good to me. However, the main criticisms are the lack of concrete evidences that those schemes are actually helpful and, on the other hand, the relative
This paper proposes a model for stochasticity for conditional image generation, building upon the previously available (DCFNet) results on composition of  convolutional filters out of the elements of the filter basis. In particular, in order to allow for further diversity in conditional signal generation, the BasiGAN proposes to model the convolutional layers as a combination of convolutional layers as a combination of basis which is stochastically sampled. This is the main contribution and difference of this paper in comparison to DCFNet (Qiu et al., ICML 2018), where the bases are not learned. The idea of introducing stochasticity by convolutional filters into the conditional
This paper proposes a new algorithm for the off policy evaluation problem in reinforcement learning. It combines the value function learning method and the stationary distribution ratio estimators. The paper provides a doubly robust method for reducing bias, since it requires separately estimating a value function. The bias of the estimator decreases as our estimate of the density ratio and value function improves.  The reviewers agree that this paper has some nice contribution to the area, by introducing a doubly robust estimator based on the density ratio, and also a new idea to achieve double robustness. Hence, I would recommend for acceptance of this paper.
This paper proposes to combine beta VAE and t SNE for anomaly detection. It combines two existing models: B VAE and t SNE. The B VAE is trained unsupervised and learns an encoder and decoder which provide both an embedding and a reconstruction. The overall anomality score for a test point is combined from 1 NN distances on t SNE plot and reconstruction error of beta VNE. Another experiment shows that t_SNE dramatically improves the performance over B VAE alone. However, the authors mention that beta VAE is trained on normal data, which means that it is not unsupervised. This means that representation learning is not meaningful and
This paper introduces two new notions of robustness for randomized classifiers, which are based on the notions of differential privacy (DP) of randomized mechanisms. Theorem 12 provides a lower bound on the L_inf norm of the noise added, and they show that Gaussian noise is close to “optimal” in terms of expected L_inf norm.  All the claims made in the paper regarding the optimality of different noise models  are specific to D_\infty and D_{MR} robustness.  All the claims made in the paper regarding the optimality of different noise models  are specific to D_\infty and D_{MR} robustness.
This paper presents a new neural network architecture called simplicial complex networks. The idea is to use algebraic topology to develop new neural network architectures based on the subdivision of a simplicialcomplex. While the paper presents an interesting idea, the reviewers were concerned about the novelty of the approach and the usefulness of the approach. The authors did not adequately compare their approach to the most common architectures in use: Fully connected nets and CNNs. The reviewers also found the discussion of storage to be somewhat surprising; is this  a property that the paper wants to stress for SCNs? The authors do not properly compare their approach to the most common architectures in use: Fully connected nets and CNNs.
This paper presents a new transformer architecture for tasks that involve a query sequence and multiple candidate sequences. The paper describes two main existing approaches for this task, namely bi encoders and cross encoders and then proposes a new formulation called poly encoders which aims to sit between the existing approaches offering high accuracy   similarly to cross encoders   and high efficiency   similarly to bi encoders. The paper is well written and although this is not related to my research I enjoying reading it. The experiments are also comprehensive. The approach proposed seems reasonable to me, and of sufficient novelty while the results presented are impressive. Therefore I recommend acceptance.
This paper introduces Conditionally Reversible Network (CrevNet) that consists of the invertible autoencoder and a reversible predictive module (RPM). The experiments on Moving MNIST, Traffic4cast, KITTI, and 2D object detection on KITTI show the improvement compare to other state of the art models. The authors show that the learned features in this paper can be used on other tasks, such as object detection. The paper is well written and the contributions are clear. The experiments on diverse tasks and datasets are provided. While the idea of the paper is good, I am not convinced of some points raised in the paper.
This paper proposes a method to compress the networks so as to accelerate the running procedure as well as save the storage. Through a statistical investigation, the authors provide a method to choose the little or big module dynamically. Specifically, they train a second "little" neural network to approximate a pre trained "big" network and use simple rules to switch between the little and the big network. By applying this method on LSTM and GRU, the authors make them more efficient. Overall, I think this paper is well written and easy to follow. The experiments is valid. However, I would like the authors to answer me two questions.  The first is about the motivation of the proposed method.
This paper studies learning the graph representation of molecules by considering the angles between atoms by considering the angles between atoms. The authors propose a new approach called directional message passing to model the angles between atoms, which is missing in existing graph neural networks for molecule representation learning, which is missing in existing graph neural networks for molecule representation learning   The proposed approach are effective on some targets. The main contribution of the paper is the use of directional embeddings of messages where angles are transformed into cosine basis for neural mappings. Since local directions are equivariant with an overall molecular rotation, the message passing architecture in this new representation remains invariant to rotations/translations. The proposed approach is
This paper proposes a novel MCMC algorithm (ATMC) that estimates and samples from the posterior distribution of neural network weights. The motivation for this approach is that applying Bayesian inference to deep learning should lead to less overfitting and better uncertainty calibrated models. The proposed approach is well explained by the authors. However, in Stochastic gradient Markov chain Monte Carlo (SG MCMC) [1], the authors used a meta learning algorithm to learn Hamiltonian dynamics with state dependent drift and diffusion which makes the system scalable to large datasets, very similar to this work. Further, the number of filters is doubled relative to ResNet 56 without explanation. In summary, the proposed approach needs to be
This paper presents Bias Resilient neural network (BR Net) that is designed to learn representations that can accurately predict the desired target while being invariant to the confounding covariates in the data. The proposed method is based on domain adversarial training strategies, especially that of (Ganin et al., 2016), where the adversarial component is modified from “loss of distinguishing between the source and target domains” to “the squared Pearson correlation between the ground truth bias covariate and its estimation from the learned representation”. In practice, the parameters of the feature extraction component are updated to solve both bias prediction and the desired classification problem in adversarial fashion. The paper provides
This paper proposes a PAC Bayesian generalization bound for Bayesian neural networks based on a previous result by Alquier et al. (2016) which connects the generalization gap to the log partition function of the same gap for the prior distribution on the learned parameters (which is identical to the ELBO bound used in Bayesian neural networks for NLL loss). The authors note that the log partition function can in general be easily unbounded for loss functions based on NLL (as in the BNN case); their result shows that if the norm of the gradient is bounded, that is enough to bound the overall generalization gap. The proposed generalization bound considers negative log likelihood loss which can
The authors propose extensions to LISTA with the goal of addressing underestimation (by introducing “gain gates”) and including momentum (by introducing “overshoot gates”). The authors provide theoretical analysis for each step of their LISTA augmentations, showing that it improves convergence rate. Thorough synthetic experiments are conducted to empirically validate the proposals as well as the theorems. The effectiveness of the gates is also proved in a real world computer vision task, photometric stereo. The reviewers found the paper to be well written and easy to follow, and the empirical results are impressive. The reviewers raised a few questions about the theoretical justification of the overshoot gate
This paper presents a new method to obtain cross lingual contextual embeddings by aligning monolingual ones through 3 steps: transform each token individually, merge them as needed to obtain a uniform granularity across languages, and reorder them. The proposed approach consists of three steps: 1. A transformation is learned that minimizes the distance between the contextual word representations of the two models of a sentence and its translation in a parallel corpus. 2. A transformation is learned that minimizes the distance between the contextual word representations of the two models of a sentence and its translation in a parallel corpus. 3. A transformation is learned that minimizes the distance between the contextual word representations of the two models
This paper proposes a new class of programs that learns to generate puzzles that are significantly harder than random counterparts, while maintaining reasonable diversity. The authors argue that this class of programs is ideal for helping learn AI systems to reason. The problem is well motivated, and the approach is sensible. The paper is fairly clear overall. However, there are some concerns about the evaluation and the baselines. In particular, more thorough analysis of how the generator is behaving, the type of curriculum it learns, and the resulting impact this has on a trainable solver all are missing. Furthermore, it would be helpful to have more discussion of the baseline methods of generating puzzles. The paper ends abruptly. Overall, I
This paper proposes a model that takes an image and a sentence as input, where the sentence is an instruction to manipulate objects in the scene, and outputs another image which shows the scene after manipulation. The model is an integration of CNN, RNN, Relation Nets, and GAN. The task itself is interesting as it aims to modify system behavior through language. The results are mostly on synthetic data, though the authors also included some results on real images toward the end. The results are mostly on synthetic data, though the authors also included some results on real images toward the end. The results are mostly on synthetic data, though the authors also included some results on real images toward the end.
The paper shows that early stopping the training on datasets with noisy labels can achieve classification performance higher than when training on clean labels (on condition that the total number of clean labels in the noisy training set is sufficiently large). The latter is often done in the literature. The “real” noise is more structured and therefore it is easier to fit and less harmful to the classification performance. The results are very interesting even though they are very intuitive and simple. The paper is well written and the idea seems to be known. However, it would have been relatively straight forward to construct synthetic datasets that would more directly evaluate the hypothesis that the true labels are learned first. The paper does not make it clear
This paper studies how to generate transferable adversarial examples for black box attacks. Two methods have been proposed,  namely Nesterov Iterative Fast Gradient Sign Method (NI FGSM) and Scale Invariant attack Method (SIM). And the second is a model augmentation method to avoid "overfitting" of the adversarial examples. Experiments are carried out to verify the scale invariant property and the Nesterov Accelerated Gradient method on both single and ensemble of models. Empirical results on ImageNet dataset demonstrate its effectiveness. The authors are expected to make more theoretical analysis. Overall, this paper is well written.
The reviewers agree that the paper makes an important contribution to the field of generalization behavior, and that the experiments are well written and well executed. The reviewers also agree that the paper is well written and the experiments are well executed. The paper should be accepted for publication at ICLR.
This paper proposes Surprise Minimizing RL (SMiRL), a conceptual framework for training a reinforcement learning agent to seek out states with high likelihood under a density model trained on visited states. The agent s policy is then conditioned on the parameters of the density model and the current size of the dataset.  The paper proposes a novel RL algorithm to minimize ‘surprise’, and the empirical results show the efficiency. However, there are several problems in the way the authors motivate their approach, and the reviewers are concerned about the clarity of the experimental results.  The paper is not ready for publication in its current form, and I recommend rejection.
The reviewers agree that this paper is not ready for publication at ICLR. The main concern is that the motivation of the proposed method is weak. The main motivation of the proposed method is weak. The motivation of the proposed method is weak. The main motivation of the proposed method is weak. The main motivation of the proposed method is weak. The main motivation of the proposed method is weak. The main motivation of the proposed method is weak. The main motivation of the proposed method is weak. The main motivation of the proposed method is weak. The main motivation of the proposed method is weak. The main motivation of the proposed method is weak. The main motivation of the proposed method is weak.
The paper aims to establish novel theoretical properties of known point based architectures for deep learning, PointNet and DeepSets. To this end, the authors prove a series of theoretical results and establish limitations of these architectures for learning from point clouds. They further provide examples of functions that can t be mutually approximated by PointNets and DeepSets. While the theorems proved in the paper are original and novel, they are a refinement of the already known results regarding approximation theorems for PointNet and DeepSets, respectively, hence only a marginal improvement in understanding these function classes. The paper lacks an experimental section. While the authors have addressed some of the concerns raised by reviewers,
This paper proposes a new graph Hierarchy representation named HAG. The HAG aiming at eliminating the redundancy during the aggregation stage In Graph Convolution networks. The idea is simple and clear. The theoretical results are nice. However, the trade off between the sampling rate and the speedup is not clear. The experimental results are also not clear. I think this paper has good theory analysis, the speed up effect is also good from the experimental result. The revised version has clarified some of my concerns.
This paper proposed Constrained Adversarial Networks (CAN), which incorporates structural constraints by augmenting a penalty term in the training object. The proposed approach relays on the knowledge compilation method, but they re very few details of it in the document. The novelty of this paper is to apply these techniques to generative models, which seem to be a bit straightforward. There is a lack of discussion in the paper on the results of each experiment. Overall the contribution of this paper does not seem to be strong enough.  The reviewers agree that the problem addressed by the authors is highly relevant and the proposed approach has the potential to be useful in practice.
This paper proposes a novel algorithm for finding mixed strategy Nash equilibria in games with continuous action spaces. The main contribution of this work is to parameterize a mixed strategy via a learned NN mapping from a simple distribution U[0,1]^d to the mixed strategy of interest. This update rule has some desirable theoretical properties, which I believe are mostly proven by Raghunathan  19. It also provides an application with a strategy approximation made with a deep neural network. The experimental finding is not surprising, as its competitors were designed to compute pure strategy Nash equilibria. While the paper generally gets the point across, much of it feels sloppy. The writing in the paper
This paper is a nice paper that combines the deep reinforcement learning and evolutionary learning techniques to neural architecture search problem. However, the reviewers were concerned about the novelty of the approach and the lack of more detailed analysis. The reviewers also pointed out that the results are not significant and the experiments are not convincing. The paper is not ready for publication at ICLR.
This paper proposes an iterative method that jointly trains the model and a scorer network that places a non uniform distribution over data sets. The distribution over the training set is parameterized by a neural network taking as arguments theStrengths:  The method is quite simple. The paper proposes a gradient method to learn the scorer network based on reinforcement learning, which is novel as to what the reviewer knows. The imagenet results seem quite strong to me.  Weaknesses:  The method is quite simple. The paper proposes a gradient method to learn the scorer network based on reinforcement learning, which is novel as to what the reviewer knows. The imagenet results seem quite strong to me.
The paper claims to study classifiers  "representation quality". However, the claimed correlation is very weakly supported by the evidence in the paper. The authors only evaluate robust accuracy on models without robust training. The paper only conducts experiments on CIFAR 10 dataset, which is not convincing enough. The measurement is also fragile under adversarial attacks, that one can feed in adversarial attacks to fool the score metrics to fool the score metrics, which is not convincing. The writing of this paper is not clear and has gramma issues.  The paper is not clear and has gramma issues. The paper only evaluates robust accuracy on models without robust training. The paper is not convincing enough.
This paper proposes an end to end deep reinforcement learning based algorithm for the 2D and 3D bin packing problems. Its main contribution is conditional query learning (CQL) which allows effective decision over mutually conditioned action spaces through policy expressed as a sequence of conditional distributions. Efficient neural architectures for modeling of such a policy is proposed. Experiments validate the effectiveness of the algorithm through comparisons with genetic algorithm and vanilla RL baselines.  The main criticism of the paper is the following: (1) The problem formulation is very unclear. (2) The proposed method works favorably with the previous RL based approach and other baselines. (3) The proposed method works favorably with the previous RL based approach
The reviewers agree that the paper is well written and the experiments are interesting. However, there are some concerns about the clarity of the experiments and the lack of a comparison between SGD with decaying step size vs SVRG with constant step size. The reviewers also pointed out the lack of a comparison between SGD with decaying step size vs SVRG with constant step size. The reviewers also pointed out the lack of a comparison between SGD with decaying step size vs SVRG with constant step size with respect to the sampling variance of SGD procedures. The reviewers also pointed out the lack of a comparison between SGD with decaying step size vs SVRG with constant step size with respect to
This paper proposed an interesting idea on how we can leverage the lead bias in summarization datasets to pretrain abstractive news summarization models on large scale unlabelled corpus in simple and effective way. The paper proposes to use this knowledge as self supervision for training summarization models. For this the author download and clean 3 years of news articles and use this to (pre )train a Tranformer model. After fine tuning the respective datasets, the gains seem significant. The real world summarizations resemble more like the ones in the DUC dataset and it would be interesting to see if the transfer results of the pretrained model on the DUC datasets.  The reviewers have some concerns about the
This paper proposes a more rigorous approach to transfer learning than the so called ‘fine tuning’ heuristic. The main results from the theory show that the distance between the final solution and its optimal are less or equal to  relative to the distance of the initial source solution to its optimum. So a near optimal solution for the source task will lead to near optimal solution for the target task. The main results from the theory show that the distance between the final solution and its optimal are less or equal to  relative to the distance of the initial source solution to its optimum. The main results from the theory show that the distance between the final solution and its optimal are less or equal to the distance
This paper proposes a corpus based approach of mining lexicon for a low resource language focusing on Amharic sentiment. This paper proposes a efficient, unsupervised way of gathering semantic lexicons that perform reasonably well on a downstream task. The proposed approach starts from a seed list of sentiment words in 3 pre determined POS classes. The proposed approach starts from a seed list of sentiment words in 3 pre determined POS classes. The proposed approach starts from a seed list of sentiment words in 3 pre determined POS classes. The proposed approach starts from a seed list of sentiment words in 3 pre determined POS classes. The proposed approach starts from a seed list of sentiment words in 3 pre determined POS classes.  
This paper proposes a context aware neural network (conCNN) that integrates context semantics into account for object detection. This paper introduces a context aware neural network (conCNN) that integrates context semantics into account for object detection. Every box proposed by the RPN is a node in the CRF, and its label is the classification label. Experiments are performed on the MS COCO object detection task. The presentation of the paper is fine in general. However, my main concern is that the experimental results (Page 8 Table 2) does not support the merits of the proposed approach. The experimental results are also a bit thin.
The paper studies the causes of the empirically poor performance in deep structures that plagues existing GNNs, and identify the suspended animation problem as the main issue. In analogy to the Residual CNN network, a residual graph network is proposed to address such issue. In the theoretical analysis, the assumption that the FC layer is identical mapping is too simplistic. The analysis differs from the actual model especially when the residual links are considered in equation (8), where the residual links are considered in equation (8), where we have a sum of FC layer output and residual connection. The analysis differs from the actual model especially when the residual links are considered in equation (8), where we have a sum
This paper considers the problem of supervised classification of time series data that are irregularly sampled and asynchronous, with a special focus on the healthcare applications in the experiments. This paper approaches the problem as a set function mapping between the time series tuples to the class label. Together with a positional embedding of the timestamps and an attention based aggregation, the paper reports improved performance of the proposed approach on a few healthcare time series with asynchronous and irregularly sampled data.  The paper is for the most part well written, and related work well characterized. The formulation is interesting and clinically relevant as well so the choice of data sets makes some sense. In addition, the interpretability shown in the experiments
This paper studies the convergence of Q Learning when a wide multi layer network in the Neural Tangent Kernel (NTK) regime is used as the function approximator. Concretely, it shows that Q learning converges with rate O(1/T) with data sampled from a single trajectory (non i.i.d.)   The reviewers agree that this paper has a lot of theoretical contributions, but there are a few major issues with the paper. The novelty is a bit unclear other than the non iid assumption. Moreover, it is unclear how to verify this condition in practice. Key technical bottlenecks that are assumed out in prior work are still assumed out in
This paper proposes a novel intrinsic reward for exploration called SFC (successor feature control), to deal with sparse reward and hard exploration task. The main idea of SFC is to provide an agent with an intrinsic reward defined to be the L2 distance between the successor features of two consecutive states (Equation 4). Empirically, the SFC+SID algorithm is evaluated on custom sparse reward navigation type environments such as VizDoom and DeepMind Lab (as well as a simple pixel based continuous control), and outperforms other intrinsically motivated RL algorithms including RND and ICM. The two policies are trained jointly and off policy. In SID, a policy for extrinsic rewards
This paper proposes to use neural networks to evaluate the mathematical expressions by designing 8 small building blocks for 8 fundamental operations, e.g., addition, subtraction, etc. The motivation of this paper is not very clear to me, i.e., why do you want to mimic the arithmetic operations using the logic networks, what is the real use case here, what is the real ML contribution of this work is in my opinion almost non existent. And the writing needs to be significantly improved.
This paper proposes to evaluate model performance when the ground truth labels were not available and noisy labels provided by multiple uncertain experts were provided instead. The proposed evaluation metric, called discrepancy ratio, is defined as the ratio between the average model annotator discrepancy and the average annotator annotator discrepancy. It can be applied to compare 1) the relative performance of different models; and 2) the relative performance of average annotators and the model. The authors demonstrated the performance of their proposal in synthetic data as well in two real world medical image datasets.  The reviewers agree that this is an interesting paper which tries to solve a practically important question. The numerical experiments are well conducted, but I am not totally convinced
This paper studies weight sharing in neural architecture search (NAS). It constructs a mini search space with 64 possible choices, and performs various comparisons and studies in an exhaustive way. Some of the observations are quite interesting, exploring the limitations of weight sharing. However, the scope is limited; everything is based on looking at 64 convnets trained on CIFAR 10, this makes it tricky to make any broad statements about weight sharing in NAS. In addition, it is based on a single architecture and a single dataset. This would have been fine if the results were supported with explanation or theoretical justification. However, the paper is entirely empirical with little if any justification of the results. In addition, it is
The paper presents a quantization method that generates per layer hybrid filter banks consisting of full precision and ternary weight filters for MobileNets. The authors focus on quantizing the MobileNets architecture to ternary values, resulting in less space and compute. This paper is generally well written with good clarity. However, The experiments only perform comparison on ImageNet dataset. I think this research is quite incremental over MobileNets and is unlikely to spur further research strains. For this kind of paper, I would like to see a more complete set of empirical results. I think a better venue for this research may be a more systems focused conference or journal.
The main idea is to train a proxy model, a smaller version of the full neural network, to choose important data points for active learning or core set selection. This paper shows that simple, almost trivial techniques can lead to significant runtime benefits for active learning and core set learning. The paper does good job of demonstrating that the proposed algorithm is effective through a comprehensive set of experiments. While the authors may be the first ones to apply this idea in the context of deep learning, they themselves note that the idea of using a smaller proxy like naive bayes for larger models like decision trees is not new. This paper is well written and was easy to follow, with a clear motivation.
This work presents an array of analytical tools to characterize linear regions of deep neural networks by analyzing the nature of linear regions in DNNs. In their presentation, the authors focus on comparing these properties between models that were either trained without regularisation, with batch normalisation, or with dropout and with different learning rates.  This paper enumerates a number of interesting findings, all of which seem to raise intriguing questions about the properties of trained networks. The paper is clearly written and is easy to follow for the most part. However, the reviewers were concerned about the clarity of the presentation and the clarity of the experiments.  The authors have addressed these concerns in the rebuttal, and the reviewers
The paper develops a continual learning method based on Gaussian Processes (GPs) applied in the way introduced by prior work as Deep Kernel Learning (DKL). Employing inducing point training for task memorization is a novel and interesting idea, which could be useful for the continual learning community. The mathematical formulation of the basic model is very elegant. Additionally, a novel approach for automatically detect task switching is introduced that exploits the Bayesian aspects of the proposed framework. The provided experiments seem reasonable and do a good job highlighting different facets of the paper. Overall, the paper is well written overall. However, the conceptual novelty of the paper is too slim compared to VCL.
This paper proposes a method of learning a hierarchical communication graph for improving collaborative multi agent reinforcement learning, particularly with large numbers of agents. The method uses a Structured Communication Network Module and Communication based Policy Module. These use a hierarchical decomposition of the multi agent system and a graph neural network that operates over the resulting abstract agent (groups). The authors evaluate on two environments, where this approach outperforms other ways to communication protocols. The initial results presented seem promising, but further work is needed to ensure the results are reproducible and repeatable. The motivation and the impact of the contributions are not very clear. The paper is not ready for publication at ICLR. The authors are encouraged to improve
The paper at hand argues that shallow feature extraction networks should be favored for the computer vision task of stereo matching, rather than the commonly used deep ResNet backbones. To that end, a model is proposed consisting of three convolutional feature extraction layers only. The paper shows that this leads to a major reduction in the number of model parameters (42% in one case) with comparable performance on the KITTI2015 data set. However, it is not clear why it requires replacing the ResNet feature extraction with a very shallow network; an alternative would be to add skip connections originating from lower levels. The related work section appears to be just a laundry list of methods. In most cases,
This paper presents a testbed framework to investigate the limitations of CNN at the classification of tiny objects and the effects of signal to noise ratio has in the task. Two artificial datasets, based on MNIST and histopathological images are introduced to conduct the experiments. The authors present an empirical study to evaluate the performance of CNN based object classifiers for situations in which the object of interest is very small relative to the size of the image. The paper attack interesting questions and links the size of the object in the image (O2I) to the training dataset size required. However, the main issue about the paper is the limited depth in the contributions, analyzing a single family of network architectures (B
The authors propose a hardware agnostic metric called effective signal norm (ESN) to measure the computational cost of convolutional neural networks. They first show empirically that this metric is correlated with the validation accuracy. The authors then show that the slimmer models with fewer parameters are better than fatter models. The paper is not very clear, and the structure is somehow confusing. The main drawback of the paper is the lack of novelty in proposed method. The paper didn t motivate properly the use of a hardware agnostic metric in the context of the quantization and pruning. Therefore, ESN is not suitable for existing hardware. Finally, the assumptions are hard to be proved.
This paper presents a new simpler routing mechanism for capsule networks and achieves good performance on real world data sets making use of this new capsule structure along with a restnet backbone. Authors improve upon dynamic routing between capsules by removing the squash function (norm normalization) and apply a layerNorm normalization instead. They report results on Cifar10 and Cifar100 and achieve similar to CNN (resnet) performance. The paper is well written. A nice analysis of the proposed routing algorithm is provided. The paper could be improved by clearing up a few ambiguities:  is the learning rate schedule the same for all three models?  is the learning rate schedule the same for all three
The paper presents algorithms for solving computational problems in a datastream model augmented with an oracle learned from data. The idea is to build on some recent work (Hsu 19) which used RNNs to predict heavy hitters in streaming data. The purpose of this paper is to analyze whether such an oracle can help streaming algorithms to obtain improved bounds. Experiments are performed on real as well as synthetic datasets using Hsu et al.’s method as an oracle.  The reviewers agree that the paper is well written and well motivated. However, there were some concerns about the clarity of the presentation and the clarity of the proofs. The reviewers were also concerned about the clarity
This paper presents an inductive matrix completion model using graph neural networks. It claims to be an inductive model and don t need any side information as it only uses the surrounding sub graph structure to give predictions. The approach (of using subgraph connectivity patterns as features) is clever, novel (at least in my knowledge) and neatly sidesteps the need for extra metadata/features. However, there is a big concern for the scalability of the model.  The reviewers have a big concern for the scalability of the model.  The authors have responded to the reviewers’ concerns and have addressed them in the rebuttal.  I recommend acceptance.
This paper presents a new technique for generating adversarial examples, by first learning the data manifold in an embedding space, then finding an adversarial example that lies on the manifold. The presented approach has been tested on toy examples regarding images (both numbers from MNIST or SVHN and images from CelebA datasets) and texts (SNLI dataset). The results show that the manifold shape is preserved while creating perturbed elements. The authors compare their approach to other attacking methods on the success rates of attacking Madry s model and Kolter & Wong s certified model.  The approach is not well motivated. Many choices in the algorithm seem to be arbitrary, and there are many approximations
This paper proposes a tree structured capsule network for program source code processing (essentially a program classification task with three datasets). The authors follow the cliché of the importance of tree structures, but show little insight into the use of capsule networks in program analysis. The paper applies the proposed architecture to three different program classification datasets, which are in three different languages. The experiments are very thin. The authors only compare their results to TreeCNN and Gated  Graph NN (GGNN). I am not excited either. For future submissions, it would be good to see a more comprehensive empirical comparison of the proposed method compared to others, and also to have more explanations about the design of the network.
This paper studies differentiable approaches to neural architecture search and convincingly points out that existing approximations to the gradient w.r.t.architecture parameters are problematic. A new approximation is proposed, and evaluated to show that degenerate architectures are not getting selected once search has converged (empirically) on standard image classification datasets. This is an interesting method, though the technique itself is not new outside the sub field of architecture search. The reviewers agree that this is an interesting and important problem. However, there are some concerns about the theoretical and empirical aspects of this paper. The theory (that the approximate gradient has an acute angle with the desired gradient) is potentially vulnerable 
This paper proposes to generate "unrestricted adversarial examples" via attribute conditional image editing via attribute conditional image editing. With the help of attribute conditional StarGAN, SemanticAdv generates adversarial examples by interpolating feature maps conditioned on attributes. They provide experiments showing the effectiveness of SemanticAdv; analysis on attributes, attack transferability, black box attack, and robustness against defenses; as well as user study with subjective. The paper is well written and technically sound with concrete experimental results. The qualitative results also look nice and the code base is open sourced. However, the reviewers had a few concerns about the clarity of the paper and the clarity of the experimental results. The main concern is that
This paper proposes to use reinforcement learning to model an agent that is reaching goal that it is intended to reach. The authors propose to then train an agent to modify the action space to make it difficult for an agent to fool the observer.  The reviewers agree that this paper is not ready for publication at ICLR.
This paper introduces the use of asymptotic constraints to findprune the search space of mathematical expressions for symbolic regression. In the paper s setting, for SR with univariate groundtruth functions f(x),  asymptotic constraints  for x > infinity and x  > 0 are given. For this situation, the paper proposes a method called NG MCTS with an RNN based generator and MCTS guided by it to consider asymptotic constraints. This neural network is then itself also used to guide aMCTS to generate mathematical expressions. Also, quantitative evaluations about extrapolative performance and detailed evaluation of the RNN generator are also reported.  
This work is tackling the problem of doing out of distribution detection in regression. The main contribution of the paper is a way of calculating the "OOD scores" that are reported in the different figures. They then use this observation to propose an algorithm for detecting out of distribution data by fitting a simple GMM to the feature space of the regression model. The paper focuses on the difference between regression and classification tasks and claims that the paper s method addresses an unmet need for OOD for regression.  However, the main contribution of the paper is a way of calculating the "OOD scores" that are reported in the different figures. They then use this observation to propose an algorithm for detecting out of distribution
The paper introduces a new Paired Associative Inference (PAI) task inspired by neuroscience and shows that most of the existing models including transformers struggle to solve this task while the proposed architecture (called MEMO) solves it better. The paper also introduces a new Paired Associative Inference (PAI) task inspired by neuroscience and shows that most of the existing models including transformers struggle to solve this task while the proposed architecture (called MEMO) solves it better. The paper also introduces a new Paired Associative Inference (PAI) task inspired by neuroscience and shows that most of the existing models including transformers struggle to solve this task while the proposed architecture (called
This paper introduces a formulation for the contextual inverse reinforcement learning (COIRL) problem and proposed three algorithms for solving the proposed problem. The main contribution of this work is the formulation of inverse reinforcement learning (for restricted spaces of context dependent reward functions) as a convex optimization problem. The authors presented their algorithms with thorough theoretical analysis. However, the authors did not analyze or show in either of their experiments how existing IRL algorithms compare with the proposed algorithms. The experimental results are not particularly useful in evaluating the proposed algorithms, as the tasks involved are relatively simple, discrete state MDPs (with continuous state features), and more importantly, no comparisons with existing IRL approaches are provided. The
This paper considers the challenging problem of learning to generate programs from natural language descriptions: the inputs are sequences of words describing a task and the outputs are corresponding programs solving the task. Both the encoder and decoder are based on the tensor product representation described in section 2. The intermediate representation (output of the encoder) is a fixed dimensional vector. The experimental results suggest that the tensor product representation is helpful for both the encoder and the decoder. However, the design of the specific neural network cannot describe the theory behind proposed binding unbinding mechanism properly. The LSTM output is defined in appendix A. Given that there are no other connections between encoder and decoder,
This paper proposes an approach to analyzing the properties of "perceptual metrics" used in deep learning image generation methods. It proposes to characterize each DNN feature in terms of two well known properties of the human visual system: a) sensitivity to changes in visual frequency and b) orientation selectivity. The authors devise an experiment similar to experiments performed on mammals to check which features are active under which types of basic patterns. The analysis is based on the proposed Perceptual Efficacy (PE) Score that measures spatial frequency and orientation selectivity of CNN features. The hypothesis put forward by the authors is that a CNN features with high PE score can be used to formulate a perceptual loss (E
The authors propose a set of model configurations on the waypoint detection, optimization techniques, a deep learning network topology and a data driven and domain knowledge based wheel detection mechanism.  The reviewers found that the paper is not ready for publication at ICLR.  The paper is not ready for publication at ICLR.  The paper is not ready for publication at ICLR.  The paper is not ready for publication at ICLR.  The paper is not ready for publication at ICLR.   The paper is not ready for publication at ICLR.  The paper is not ready for publication at ICLR.  The paper is not ready for publication
The authors study a method to help tuning the two learning rates used in the MAML training algorithm. This work theoretically discusses the relationship between the inner loop learning rate and the outer one, under a set of assumptions. Under simplifications, the paper derives some necessary conditions on \alpha and \beta for the MAML iterates to converge to local minima, and then verifies the theory by experiments on synthetic and real world data. The experiments on Omniglot and MiniImagenet are coherent with the theory, but I am not completely sure of their impact. I also have concerns about the correctness of the analysis in the single task case. I’m not a M
This paper studies self supervised learning from very few unlabeled images, down to the extreme case where only a single image is used for training. It argues that early layers of a convolutional neural network can be effectively learned from a single source image, with data augmentation. The result holds for three state of the art self supervised methods, tested with two single image training examples. BiGAN, RotNet, and DeepCluster serve as the reference self supervised methods. A performance gap exists for deeper layers, suggesting that larger datasets are required for self supervised learning of useful filters in deeper network layers. The experiments are carefully described and presented, and the paper is well written.  The reviewers
This paper proposes a system that combines MLNs and GNNs to perform inference and learning the weights of the logic formulas. The main motivation seems to be that inference in traditional MLN is computationally inefficient. The paper is cryptic about precisely why this is the case. There is some allusion in the introduction as to grounding being exponential in the number of entities and the exponent being related to the number of variables in the clauses of the MLN but this should be more clearly stated (e.g., does inference being exponential in the number of entities hold for lifted BP?). This is not a paper where the related work section should be delegated to the appendix. At the very least, the paper
This paper introduces a method for reconstructing the architecture and weights of deep ReLU network, given only the ability to query the network (observe network outputs for a sequence of inputs). The method is currently limited to ReLU networks and does not account for any parameter sharing structure, such as that found in convolutional networks. The proposed method also holds for ResNets, with slight modifications, but defers details to future work.  The reviewers found the experimental results to be too small to convince the argument of the author. The proposed method is limited to ReLU networks and does not account for any parameter sharing structure, such as that found in convolutional networks. The proposed
This paper presents 4D convolutional neural networks for video level representations. To learn long range evolution of spatio temporal representation of videos, the authors proposed V4D convolution layer. Empirical evaluation on three benchmarks against other baselines suggested the advantage of the proposed method. State of the art performance is achieved on several datasets. The paper presents an interesting idea, but there are some issues that need to be addressed before published on ICLR. Reviewers agree that the proposed framework also falls in this category, with a treatment from deep learning. It makes sense, and I expect the proposed model may benefit from its design for long range spatio temporal feature learning.
The paper is well written and easy to follow (the two exceptions/oddities are Figure 1 & Table 1, which appear one page before they are refered, which makes them initially hard to understand because they are out of context). This paper is technically not very novel, but asks interesting questions. The method is also cost effective, a few hundred dollars depending on the dataset and the number of queries used to train the attacker model. The method is also cost effective, a few hundred dollars depending on the dataset and the number of queries used to train the attacker model. The method is cost effective, a few hundred dollars depending on the dataset and the number of queries used to train the attacker model
This paper proposes Continual Neural Dirichlet Process Mixture Model (CN DPM) to solve task free continual learning. The core idea is to employ Dirichlet process mixture model to create novel experts in online fashion when task distributions change. While the Dirichlet Process Mixture (DPM) is not new, applying such nonparametric method to continual learning is new. The main algorithm itself cannot be considered to be novel. In fact, the paper claims its contribution is expansion based task free continual learning. However, this “task free characteristic” is the contribution of SVA based inference. The experimental results are impressive given the single epoch setting, in which no information about
This paper proposes a generative model for proofs in Metamath, a language for formalizing mathematics. The main purpose of this model is to generate synthetic theorems and proofs that can be used to train the neural networks of a data driven search based theorem prover. The parameters of these networks are learned from existing proofs or theorem statements. Experimental results show the usefulness of the generated synthetic theorem. The main result of the paper is that an extra 35/2720 (1.2%) of the test theorems are proven, a 6% improvement over the Holophrasm baseline of 539. It is difficult to judge how relevant of an improvement this is, and there is
This paper tackles the problem of solving a black box optimization problem where only some samples have been observed. Instead of learning only a single forward model of x  > y, this paper proposes to additionally use a mapping from y  > x. Specifically, the paper uses a GAN that transforms [y,z]  > x, where z is stochastically sampled.  The proposed idea of using an inverse network to predict x given a target y for optimisation, instead of the traditional way of optimisation (e.g. using Bayesian optimisation for the complex cases considered in the paper). However, unfortunately, this paper is too close in concept, and in my understanding lower
The paper proposes a method, LayerDrop, for pruning layers in Transformer based models. The goal is to explore the stochastic depth of transformer models during training in order to do efficient layer pruning at inference time. The key idea is simple and easy to understand: randomly dropping transformer layers during training to make the model robust to subsequent pruning. The authors showed that it is possible to have comparable performance from sub networks of smaller depth selected from one large network without additional finetuning. The reviewers agree that this is a very strong submission and strongly advocate accepting it to ICLR.
The paper is about continual learning on NLP applications like natural language instruction learning or machine translation. The authors propose to exploit "compositionality" to separate semantics and syntax so as to facilitate the problem of interest. The key idea of the proposed method is to enable the network to represent syntactic and semantic knowledge separately. This allows the neural network to leverage compositionality for knowledge transfer while alleviating catastrophic forgetting while alleviating catastrophic forgetting. The novelty of the proposed method is somewhat limited in my opinion. The writing is not good, as I cannot see clearly the backbone of the paper. But, this suggests that the datasets might be too artificial for this evaluation. More on this in point 6.
This paper proposes an approach to exploration by utilizing an intrinsic reward based on distances in a learned, evolving abstract representation space. To solve this problem, the authors leverage novelty heuristics in a lower dimensional representation of a state, for which they propose a novelty measure. They test their method by experiments conducted in two environments, where they use the same model architectures and model free methods for all types of novelty metrics, which shows the contribution of the proposed method in the results of learning. The reviewers agree that the proposed method makes sense intuitively and the proposed method makes sense intuitively. However, there are concerns about the technical contribution of the proposed method, which is too light in terms of technical contribution
This paper proposes a new convolutional operator for point cloud forecasting. The D Conv operator is included in a LSTM architecture (CloudLSTM) to enable the spatio temporal modeling of point cloud data, and can be combined in standard neural network architectures such as a Seq2Seq with attention. This work s contribution was a new "X Conv" operator, which also consumes point clouds and produces learned representations.  The reviewers agree that this paper is interesting, and the experiments are indicative that this method can improve on the current approaches. However, the reviewers are concerned that the proposed convolution operator use KNN to choose the nearest neighbors. The choice of RNN,
This paper introduces a new adversarial training approach, where a generator is used to generate the most challenging adversarial examples and the classifier is trained to correctly classify the generated adversarial examples. In this way, the robustness of the classifier is expected to be improved.  The reviewers are not convinced that the proposed method should be superior to existing attacks.  The proposed method is not a new idea, and the proposed method should be improved by improving the robustness of the classifier to by feeding adversarial examples to it is not a new idea.  The proposed method should be improved by improving the robustness of the classifier to by feeding adversarial examples to it is not a
This paper proposes meta dropout, which leverages adaptive dropout training for regularizing gradient based meta learning models, e.g., MAML and MetaSGD. The motivation is that meta learning the noise allows to learn how to best perturb examples in order to improve generlization. The proposed method is evaluated on OmniGlot and miniImageNet. It is furthermore shown that meta dropout somewhat improves the model’s robustness against an adversarial attack. This claim is supported by ample experimental evidence and comparisons against many baselines, as well as additional ablation studies w.r.t design choices of the algorithm itself.  The reviewers agree that the paper is
The authors propose a new metric for measuring the simplicity of a few shot learning benchmark and demonstrate that it is possible to achieve high performance on Omniglot and miniImageNet with their unsupervised method, resulting in a high value of this criterion, whereas the Meta Dataset is much more difficult.To validate this, the authors proposed clustering based meta learning method, called Centroid Network. The proposed CentroidNet performs well in the experiments. The paper is well written and generally very clear. The paper makes a number of contributions. However, it is unclear to what extent the proposed metric is general and predictive. The definition of CSCC is not convincing.
The paper has some major weaknesses that affect the clarity of the points being conveyed in several sections. In its current form I suggest to reject the paper and urge the authors to improve it according to the following points:1. The details of the experiment in Figure 2 are not clear. Do you use a fixed number of SGD iterations during training and a variable number of iterations (determined by a stopping criterion) during testing? The paper also demonstrates that the EfficientNet architecture can be applied to segmentation.  The paper also demonstrates that the EfficientNet architecture can be applied to segmentation.  The paper also demonstrates that the EfficientNet architecture can be applied to segmentation. 
This paper proposes a novel outlier detection approach, based on Wasserstein auto encoders. The idea being that an outlier would have a higher reconstruction error, and hence should be mapped to low probability region of the latent distribution. The novelty is in proposing a weighted reconstruction error that penalizes the mapping of data with high reconstruction errors (mostly anomalies) into high probability regions. The reviewers have partially addressed my concerns. However, the paper is not ready for publication in its current form. The main concern is that the method is not sufficiently generalizable to other datasets. The authors did not provide a comparison to other methods. The reviewers also raised some concerns about the clarity of the text.
This paper proposes a joint/compositional embedding procedure where a single instance can be mapped/embedded to multiple classes while preserving the class specific information in the embedded representations. This is done throughly jointly training embedding functions, a set union function and a query function. For example, f could be a function that maps an image to a binary representation of its classes (this could be a typical ResNet image classifier), g could be a function that does a binary OR of its two arguments and h could be a function that uses a binary AND and equality test on its two arguments. The authors look at class union and class query criteria for the composite embeddings.  
For "Impact on Convergence" in section 3.2.2, it is not clear to me what the authors are using as a metric for the degree of convergence.  The reviewers are not convinced that the paper is ready for publication at ICLR.  I recommend rejection.
This paper tries to combine FMs and GNNs to capture both sample and feature interactions. While the idea of integrating the GNN and FMs is interesting and intuitive, the reviewers are concerned about the clarity of the descriptions and the experimental evaluation. Moreover, it would be much better if the authors can carry out experiments on some widely used recommendation datasets and use standard evaluation metrics for ranking. The authors should carry out ablation study for different components of the model.
The paper proposes a regularizer for the output representation of transformer NNs, based on the singular value distribution to encourage learning of richer representations and avoid fast decay of singular values previously reported for NNs with softmax outputs. In particular, the embedding matrix is parametrized as the product of a matrix U, a diagonal matrix Sigma and a matrix V. U and V are encouraged towards orhogonality using additional penalties similar to Lagrangian augmentation. Finally, a desired singular value distribution (exponential or polynomial decay) is encouraged by adding an appropriate regularization penalty on the entries of Sigma. The authors present a generalization error bound that relates expected loss, training
This paper proposes a spectral graph neural network based on a graph kernel to predict graph evolution. The reviewers agree that the theory is interesting and the proposed method is interesting. However, I do have some concerns of papers. 1. The main drawback of the paper is the lack of experimentation with real datasets. Based on the results from four datasets they used, the efficacy of their proposed method is unclear. 2. The new graph kernels cover many existing graph kernels as well as their combination and composition as special cases. 3. The main concern is the soundness of keeping eigenvectors unchanged in the evolution. Note: I could not verify the theory in detail yet.Reject.
This paper proposes to improve the clustering ability of k means by measuring the similarity between samples and centroids by something known as Extreme Value Theory (EVT). The paper in its current form is extremely difficult to follow. There is no clear description of the algorithm. Further, after reporting the mean and comparing with more methods, the method doesn t seem to perform as well as previously reported. Finally, there is no time cost comparison in the experiments. Given the above, I am unable to evaluate the contribution of the paper. I encourage the authors to improve the paper and provide a more clear description of the algorithm. I think the above will also help decide whether section 3.2 on Extreme
In this paper, the authors proposed to extend meta learning for few shot classification to the multi domain setting. Specifically, there are three main components of the proposed method in training: 1) a base network, 2) a model pool, and 3) a selection network. The overall training process of the proposed method is:(1) Train the base embedding network f_E with the aggregated dataset from multiple domains. (2) Build a selection network through cross domain episodic training. (3) Build a model zoo based on source datasets at hand, and learned an “argmax” meta selector which takes embedding of a task as input and outputs the model selection index.
This paper proposes a method to learn to reach goals in an RL environment. The method is based on principles of imitation learning. GCSL treats each executed action as a sample from the expert policy conditioned on each of the states reached after that action is taken. For instance, beginning with an arbitrary policy that samples a sequence of state action pairs, in the next iteration, the algorithm treats the previous policy as an expert by relabeling its ending state as a goal. Experimental results demonstrate superior performance against a base (non goal conditioned) RL algorithm (TRPO), and against another approach to learning goal conditioned polices (TD3 HER), on a relatively diverse set of control problems.  
This paper first discusses some concepts related to disentanglement. The authors propose to decompose disentanglement into two distinct concepts: consistency and restrictiveness, which they don t imply each other. Then, a calculus of disentanglement is introduced to reveal the relationship between restrictiveness and consistency. The proposed concepts are applied to analyze weak supervision methods. The paper is well written and well structured.  The reviewers agree that the paper is well written and well structured.  The main weakness of the paper is that it does not propose effective methods for disentanglement in the weak supervision setting.  The reviewers agree that the paper is well written and well structured.
This paper basically built upon [1]. What kind of norm are you using? The authors only evaluate their algorithm in one environment, MazeGridWorld. It is also nice that the paper provides a good relation (with explanation) between this signal and the frequency of the value function. The paper is clearly written. I think this paper is currently one or two major revisions away from the acceptance threshold. I recommend acceptance.
This paper proposes a meta algorithm for the so called "decision based attack" problem, where a model that can be accessed only via label queries for a given input is attacked by a minimal perturbation to the input that changes the predicted label. For handling the sensibility to starting points, the authors propose a meta algorithm, which uses any iterative local update based attacks, and which maintains a set of solutions corresponding to different starting points. It is compared with three decision based attacks, including SignOPT. In the experiments, the meta algorithm uses SignOPT attack.  The reviewers agree that this paper is not ready for publication in its current form.
This paper proposes to learn sparse representation in neural networks for retrieval in large database of vectors. The novelty of this algorithm is its focus on minimizing the number of FLOPs in computing queries of instances, taking note as well of the role of the distribution of non zero values in determining the number of FLOPs. The presentation of the distribution that minimizes FLOPs is convincing, and there is easy to follow buildup into the continuous relaxation of the FLOPs minimization problem. Based on experiments, the paper claims that the proposed algorithm yields a similar or better speed vs. recall tradeoff compared to baselines. The reviewers agree that the proposed algorithm yields a similar or better speed vs.
The paper presents an empirical study of causes of parameter divergence in federated learning. Federated learning is the setting where parameter updates (e.g.gradients) are computed separately on possibly non IID subsamples of the data and then aggregated by averaging. The reviewers agree that this is an interesting topic, and that the experiments are thorough. However, there are some issues with the paper: 1. The learning rate is constant throughout all of the experiments, depending only on the optimizer, but not on the neural network architecture. 2. The authors only studies Nesterov momentum in this paper. Where was this observed? 3. The authors only studies Nesterov momentum at the
This paper proposes a way of training neural nets on analog circuit based chips, which are cursed with uncertainties. Such uncertainties are deeply rooted in the way neural nets are implemented on such chips. The authors propose an "uncertainty adaptation training scheme" (UATS) that describes the uncertainty of the neural network in the training process. The proposed algorithm is reasonable and the experimental results look encouraging. However, the paper is hard to read and there are syntactic errors as well as  issues with the grammar. Overall the quality of the presentation and the exposition in the paper is poor. The paper is not ready for publication at ICLR.
The paper explores what is more important for the early phase of training: signs of the weights or magnitude of the weights.  The authors conducted extensive and detailed experiments to study the statistics of weights and their gradients. They also show that the perturbations can be roughly be approximated by adding Gaussian noise to network weights.  This paper provides a good summary of observations and network properties that worth studying during the early stage of network’s training. The results in the paper are aligned with my intuition that the weights in the early stage are highly dependent and they share some similarities in the distribution level. And it has some interesting implications for lottery tickets.
This paper studies the effects of residual and dense net type connections on the moments of per layer gradients at random initialization. The paper studies the mean and variance of the gradient norm at each layer for vanilla feedforward, ResNet and DenseNet, respectively, at the initialization step, which is related with Hanin & Ronick 2018 studying the mean and variance of forward activations. In particular, using duality, bounds on the variance of the square norm of Jacobian (with respect to the randomness of random initialization) are derived for vanilla networks. The paper also reveals a surprising property of the gradients in general Relu networks. Whereas the variance of the gradient norm grows with depth
This paper proposes AE GAN+sr, an auto encoder based GAN for equipping neural networks with better defenses against adversarial attacks. First, the autoencoder is used to detect if an input sample is from the natural image distribution or adversarial distribution. In the latter case, Defense GAN is employed that uses gradient descent in the latent space, with encoder output as the initialization, to find a natural or non adversarial counterpart of the input sample. The autoencoder is also equipped with an adversarial loss such that the decoder produces realistic images. The authors evaluate their method on black box attacks, white box attacks, and gray box attacks on MNIST
This paper proposes a meta learning approach to learn reward functions for reinforcement learning agents. The intrinsic reward functions used in this paper map the full life time history of the agent to rewards. The meta learning algorithm and the corresponding empirical investigation are the main contributions of the paper. However, given this motivation, it would be important to see comparisons between the proposed method of learning intrinsic rewards, and other methods for fast adaptation in the literature, such as MAML, which as I understand also has many of the properties highlighted in this paper. I recommend marginal accept. However, given this motivation, it would be important to see comparisons between the proposed method of learning intrinsic rewards, and other methods for fast adaptation
This paper introduces a model that learns a slot based representation, along with a transition model to predict the evolution of these representations in a sparse fashion, all in a fully unsupervised way. This work tackles an important problem, and is very well motivated and presented in a very clear fashion. However, while the overall approach is intuitive and seems to yield desirable results, I have concerns regarding the experiments, comparisons to prior work, and the exact contributions of this work. The major problem of this manuscript, to me, is its ignorance of related work and, therefore, overclaiming at a few places. I would like to see the authors  responses regarding the missing related work.
This paper aims to define a general notion of compositionality as equivariance, build a model which is compositional in this general sense, and apply the model to SCAN. The proposed models are also new, interesting, and could be significant. However, effects requiring global equivariances like learning relationship between "twice" and "thrice", or learning relationships between different kinds of conjunctions are not handled in this work. While this paper does show that modelling effects of word substitution can be handled by the locally equivariant functions, it still cannot account for more complex generalization phenomena which are likely to be much more prevalent especially for domains dealing with natural language that are other than
This paper presents a family of parameterized composite activation functions, and a regularization technique for parameterized activation functions in general. The authors introduce a parameterized activation function to learn activation functions that have sigmoidal shapes that can be used in LSTMs. The regularization experiments use LeNet 5, which is not a compelling benchmark architecture with respect to contemporary practice. The effects of regularization techniques can be very different in different regimes of dataset and network size. This paper is simply proposing another one and showing little to no improvement. The reviewers are not familar with this stock prediction dataset, but the differences shown are often less than 1%. This paper is simply proposing another one and showing little
This paper proposes an algorithm called Sparse Momentum for learning sparse neural networks. The method redistributes the sparsity according to momentum contribution of each layer during training after each epoch. The authors compare speed up results for training in two ways: theoretical speedups which are proportional to reduction in number of FLOPS and practical speedups using dense convolutional algorithms corresponding to completely empty channels. The proposed algorithm looks interesting and seems to empirically work well. The reviewers agree that the proposed method is interesting and seems to work well. The reviewers also agree that the proposed method is not comparable with Han et al. because this work started training a sparse model while Han et al.15 trains a dense model
The authors propose a method to extract features utilizing the adjacency between patches, for better classification/regression of satellite image patches. The method is applied to tasks of estimating crowding population, and diseases density, from satellite images. The method is applied to tasks of estimating crowding population, and diseases density, from satellite images. The authors claim that this approach alleviate the need of a post classification smoothing, whereas the proposed method does not add significant overhead. The discussion is not up to the level of ICLR and offers mostly guesswork. I understand that the authors cannot compare to everything. But the authors should compare to representative baseline methods. The paper have little novelty.
The paper presents a unified approach to specify, evaluate and benchmark different ML methods. This benchmarking offers several insights on the evaluation of a given ML model, by stressing out the importance of aspects that can severely bias  the final outcome of the model (e.g., pre processing tasks, different hardware configurations or normalization of the data). To describe the workflow, authors use an image classifier on a given hardware as a running example, and play with different  preprocessing methods to measure their impact on the final  preprocessing methods to measure their impact on the final accuracy of the model.  The paper proposes a framework to evaluate machine learning models in a hardware agnostic way.  
This paper proposes a piecewise linear activation function that is learned concurrently with training, and, in the case of S APL, the activation function is forced to be symmetric. The proposed activation function is an extension of the APL activation, but is symmetric w.r.t.x axis. It also has more linear pieces (actually S pieces, where S can be arbitrarily large) than the existing activation functions like ReLU. The authors also show that neural networks with the proposed activation can be more robust to adversarial attacks. However, the gain is marginal as shown in Table 1. It is important to see comparisons with other activations such as the plain APL. 
The reviewers agree that this paper is not ready for publication in ICLR.  The main concerns of the reviewers are lack of novelty and lack of comparison with other augmentation methods, e.g., Mixup, CutMix, AutoAugment. The method description is not specific.  It would have been helpful to include more examples of the original methods CutOut and Sample pairing. Other datasets, e.g., CIFAR100 and ImageNet, should be demonstrated.
This paper presents a fractional generalized graph convolutional networks for semi supervised learning. It is based on a novel fractional filter for graph conv networks, which generalizes several previously employed graph semi supervised learning frameworks, by introducing a fractional hyperparameter (sigma in the paper), using fractional powers of the Laplace operator. Experimental results show that the proposed method (FGCN) shows the best accuracy compared to other recent graph based neural networks for all datasets except one. The relevant previous work in the area seems to be cited, and the paper appropriately embedded in the previous work. However, this idea is too incremental and applying the classification function to graph filter is very trivial and
In particular, they study local interpretable models, which are used to study interpretability at the level of one or a few data points. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e.linear); thus, if they are trained on entire datasets they will underfit. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e.non differentiable) decisions to select a subset of the dataset. The contribution of this paper is thus the use of RL for meta learning how to subsample a larger dataset in
This paper proposes warped gradient descent (WarpGrad) an optimisation framework for facilitating gradient based meta learning.  The authors propose warped gradient descent (WarpGrad) an optimisation framework for facilitating gradient based meta learning.  This is a good paper which proposes an interesting generalization of previous gradient based meta learning methods like MAML and T Net, with an impressive number of experiments, with an impressive number of experiments. It would help to develop this.Accept (Talk); rating score: 8;
This paper introduces an anchor free object detection framework that aims at simultaneously predicting the object position and the corresponding boundary. The idea is built on top of the FPN backbone, where a set of feature maps (each representing a specific scale) are used to detect object boxes in multiple scales. To achieve this, the proposed FoveaBox detector predicts category sensitive semantic maps for the object existing possibility, and  produces category agnostic bounding box for each position that is likely to contain an object. The paper shows that FoveaBox can also be used for object proposals by changing the classification target to class agnostic head. Experiments are performed on MS COCO detection benchmark.  
The problem is timely and important as it is challenging to perform CCA jointly with the task classification (see below) and hence previous work typically perform this in a pipeline   that is, first projecting the data using a pre trained CCA and then training a task classifier using the projected representation. As the authors note, this may be problematic as CCA may delete important information that is relevant for the classification, if training is not done jointly. The experimental setup is somewhat contrived to show the superiority of the proposed method. The experiments are thorough, convincing and the span a range of applications. The results demonstrate the value of the proposed approximations, and I hence recommend a weak accept
This paper applies the Go Explore algorithm to the domain of text based games and shows significant performance gains on Textworld s Coin Collector and Cooking sets of games. They achieve this by training a sequence to sequence model on trajectories collected by running the game using a previously proposed exploration method (Go Explore). While the results are promising, there might be limited novelty beyond training a sequence to sequence model on pre collected trajectories. Additionally, the authors evaluate 3 different paradigms for training agents on (1) single games, (2) jointly on multiple games, and (3) training on a train set of games and testing on a held out set of games. While the results are both impressive and
The paper proposes to use the sparse network to mitigate the task overfitting. The method is based on network pruning. Empirical results demonstrate the effectiveness of the proposed method.  The proposed method is simple and lacks technical contributions.  The paper is written in a clear way and the proposed method is simple and lacks technical contributions.  The paper is written in a clear way and the proposed method is simple and lacks technical contributions.  The paper is written in a clear way and the proposed method is simple and lacks technical contributions.  The paper is written in a clear way and the proposed method is simple and lacks technical contributions.  The paper is written in a clear way and the proposed
This paper proposes an extension of the conditional GAN objective, where the generator conditions on an attention map produced by the discriminator in addition to the input image. The proposed discriminator provides the probability of real /fake and an attention map which reflects the salience for image generation. After such a map is obtained, it is concatenated with the input image and fed iteratively to the generator. They apply their method to CycleGAN.  The reviewers were concerned about the novelty of the proposed method and the limitations of the empirical results. The reviewers were also concerned about the limitations of the empirical results. The paper should clarify what is done at test time, and clearly state the shortcomings as a
This work addresses the problem of causal inference in time dependent treatment regimes. To address the problem, the authors propose an extension of the balancing representation for causal inference framework that seeks render the current treatment independent from a representation of the history of treatment and confounders.  The reviewers agree that the proposed method works well, but there is one problem with the illustrations in Figure 1: predictions on the potential outcomes before the first treatment was given should have stayed on the same path, and then depart when different treatment was initiated (at different time) and then depart when different treatment was initiated (at different time).  The proposed method works well, but there is one problem with the illustrations in Figure 1.
The reviewers agree that this paper is a good paper, and the proposed method is well motivated. However, the novelty of the work is not presented clearly. The authors outlie a a Bayesian approach to fit the model. The topic is out of my scope. Therefore, I recommend to reject this paper.
The paper proposes a way to learn better representation for RL by employing a hindsight model based approach. While the Portal Choice experiments are informative and use Impala, it is a bit toy, and it would increase the reviewer’s confidence in the generality and robustness of the approach if improvements were also demonstrated for an actor critic method on a large environment suite.  The reviewers found this example to be more illustrative than the one in the introduction of the paper, and it seemed like it was very illustrative of an example where proposed approach will excel.  The paper would benefit significantly from a detailed explanation of how and when the proposed approach should be expected to improve on bootstrapping.
This paper proposed two constrains to tackle the problems of dead points and dead neurons. The constraints were augmented to the loss functions of DNNs as a regularizer.  The experiments are not sufficient to validate the arguments. The experiments are incomplete and likely provide misleading results. Fourth paragraph of section 4.1 mentions results using convolutional layers, but it is not clear which of the provided results are on fully connected or convolutional networks.  The experiments are not sufficient to validate the arguments. The experiments are not sufficient to validate the arguments.  The experiments are not sufficient to validate the arguments.  The experiments are not sufficient to validate the arguments.  The experiments are incomplete
This paper addresses a method of applying prototypical networks (which are popular for few shot learning problems) to few shot one classification problems where only one group of examples are available without any counter examples. The idea proposed in this paper is to introduce a null class which models the entire space of possible examples and queries are judged, compared to positive class as well as the null class. The authors provide a large enough set of comparisons to existing algorithms, and explain their approach well. The contribution of this paper is intuitive and interesting. However, the main contribution is proposing to compare against norm of the embedding rather than a prototype for random negative samples. The reviewers agree that modeling it as a 2 way with
This paper studies the training of overparametrized neural networks by gradient descent. It provides theoretical justification for a "spectral bias" that is observed in training of neural networks: a phenomenon recorded in literature (Rahaman et al.), where lower frequency components of a signal are fit faster than higher frequency ones. The authors claim that this explains the "spectral bias" observed in previous papers. Some toy experiments are provided to exhibit the spectral bias phenomenon.  The paper is not yet ready for publication in its current form.  The reviewers agree that the paper is not yet ready for publication in its current form.  The paper is not yet ready for publication in its current form.
The authors propose a novel method for positive unlabeled learning. The idea of the paper is interesting; it is well motivated and well supported with a range of experiments from NLP and computer vision tasks. In the experiments, the proposed method achieves better performance compared with state of the art methods. However, the clarity of the paper could be improved in multiple places, and the experiments could have been improved in multiple places.  The reviewers raised several concerns that do not have an impact on the score   Although the problem setting is quite different, the idea of this paper is partially similar to the importance weighting technique adopted in some recent domain adaptation methods [R1, R2]. Do you
This paper proposes to use Contrastive Predictive Coding (CPC), an unsupervised learning approach, to learn representations for further image classification. The authors show that using CPC for representation learning allows to achieve better results than other self supervised methods.  The paper is well written and the experimental results seem good. However, the novelty and technical contributions are limited. The proposed method lacks of important insights for the research community. For instance, the authors explain CPC by mentioning about masked convolutional layers that is unnecessary at this point. I understand that from engineering perspective it is crucial information, but it does not help to understand CPC. The proposed method lacks of important insights for the research community.
This paper considers robustness issues faced by Nesterov’s Acceleration used with mini batch stochastic gradients for training Deep Models. In particular, despite broad use in practice, it is unclear if standard variants of Nesterov acceleration/Heavy Ball method achieve "acceleration" in stochastic optimization. The authors propose Amortized Nesterov’s Momentum, a variant of Nesterov’s momentum that utilizes several past iterates, instead of one iterate, to provide the momentum. The goal is to have more robust iterates, faster convergence in the early stage and higher efficiency. The paper proves optimal convergence in the convex setting
The paper proposes a Neural Question Requirement module that extracts a list of condition from the question which should be met by the candidate answer in the question answering problems. The proposed model shows the effectiveness of the proposed model on SQuAD V2.0 which is constructed from Wikipedia and contains unanswerable questions generated by crowd sources. The reviewers agree that this paper is an interesting paper which focuses on the answer verification and validation, and shows the effectiveness of the proposed model. However, there are a few concerns detailed as follows:1. The proposed model is different from existing answer verifiers in that NeurQuRI pinpoints where the mismatch occurs between the question and the candidate answer in unanswer
This paper proposes a method for lossless image compression consisting of a VAE and using a bits back version of ANS. The method is based on a fully vectorized implementation ofbits back with asymetric numeral systems coding which is much faster thanprevious non vertorized implementations. These models are shown to generalize well whenthey are trained on small images (e.g. 32x32 and 64x64) and then applied tomuch larger images. The results are very impressive on a ImageNet (but maybe not so impressive on the other benchmarks). The vectorization approach can be very useful in practice and the dynamic discretization can alsobe useful as shown
This paper studies the function space regularization behavior of learning with an infinite width ReLU network with a bound on the l2 norm of weights, in arbitrary dimension, extending the univariate study of Savarese et al. (2019). The authors show that the corresponding regularization function is more or less an L1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the "R norm", which is expressed via duality through the Radon transform and powers of the Laplacian. The authors finally prove that any function in Sobolev space could be (approximately) obtained by a bounded network. 
The paper proposes a new time series model for learning a sequence of graphs. While the proposed algorithm is practically useful, I believe the submission is premature to be accepted at a conference due to (1) the lack of comparison with existing works on multi agent (reinforcement and imitation) learning and (2) the lack of novelty (It seems that the proposed method simply combines existing neural networks and applies it to multi agent behavior prediction.). In addition to them, there’s a paper on arXiv that uses GNN for MARL (https://arxiv.org/abs/1810.09202), which may be deeply related to this work as well.  
This paper puts forward a new regularization based continual learning method that explicitly regularizes the optimization trajectory by constraining in the distribution space. The authors introduce co natural gradients, which is an incremental development of natural gradient methods, note that co natural gradients use Fisher information to regularize the trajectory of the gradients which will be optimal on both tasks. This corresponds to lower forgetting of previous tasks. The experiment results have shown that the co nature gradient method help from time to time. The paper is well written, and preliminary empirical results are promising. However, the paper needs work both in the theory and experiments. I think that the performance of finetuning + co gradient is too natural,
This paper studies the properties of SGD as a function of batch size and learning rate. The authors derive empirical conclusions and perform experiments in different settings. However, the reviewers found the empirical results to be mixed up with the results of published works, making it hard to identify the contributions of this paper. The critical contribution of this work appears to be the observation that large batch size can be worse than small under same number of steps demonstrating implicit regularization of small batch size. The authors should instead focus on the discussion of generalization performance and the observation that training loss and test accuracy are independent of batch size in noise dominated regime.
This paper is not ready for publication at ICLR. The main contribution of the paper is the algorithm POLISH on page 5. However, it is not clear whether this is the objective of the paper. It is hard to understand the process without a clear step by step description. The reviewers are also not convinced by the evidence in this paper. I recommend rejection at this time.
This work proposes an alternative approach to non autoregressive translation (NAT) by predicting positions in addition to the word identities, such that the word order in the final prediction doesn t matter as long as the positions are correct. This paper uses a heuristic that the inputs positions and output positions of the decoder with close by embeddings are more likely to represent the position mapping. The length of the translation is predicted similar to Gu et al 2017, as well as smoothly copying the source sequence to decoder input. Experiments on machine translation and paraphrase generation show strong result in comparison to other non autoregressive models. The idea of delegating generation order to the latent variables seems interesting
This paper proposes an architecture search method for deep convolutional neural network models that progressively increases the number of filters per layer as well as the number of layers, and the authors refer to this general approach as boosting networks. The proposed algorithms can simultaneously grow and train a network by progressively adding both convolutional filters and layers. The algorithm for increasing the number of filters is based on split linear Bregman iteration, and the algorithm for increasing the number of layers proceeds block by block, increasing the layers per block until the accuracy does not increase. The method is motivated by SPLITLBI, and its effectiveness is verified by experiments and comparison with AutoGrow. The paper is clearly written and
This paper proposes a method to adapt a pretrained BERT model from English to another languages with a limited time/GPU budget. The proposed approach consists of, starting from a pretrained English language model, first training language specific embeddings and then fine tuning the entire pretrained model on English *and* the target language, using those embeddings. The resulting bilingual LM is evaluated for zero shot transfer learning on two tasks: XNLI and dependency parsing. The obtained representations are evaluated on downstream NLP task (natural language inference and dependency parsing) with state of the art performances.  The authors provide good details into their hyperparameter settings and about how the obtain the foreign language
This paper proposes a neural network architecture that provides an agent agent based embeddings that are used for actions that directly affect specific agent. Because it is a modification of the network structure, ASN can be combined with any type of multi agent reinforcement learning algorithm to improve its convergence speed and final performance.  The reviewers agree that this paper is interesting, and the experimental results on StarCraft II and Neural MMO show ASN significantly improves the performance of state of the art DRL approaches compared with several network architectures.  However, the reviewers are also concerned about the fact that the proposed architecture is manually split into two subsets, each of which contains the actions that affects other agents or not. The authors
The paper proposes to use the previously proposed ScatterNet Hybrid Deep Learning  (SHDL) network in a continual learning setting. This is motivated by the fact that the SHDL needs less supervised data, so keeping a small replay buffer can be enough to maintain performance while avoiding catastrophic forgetting. The authors propose to use a modular SHDL with skewed replay distributions to do continual learning and demonstrated the effectiveness of their model on the CIFAR 100 dataset.  The reviewers found the paper to be interesting and useful, but the main concern was the lack of comparison with other methods and the lack of details about the architecture and the training algorithm used. The authors did not provide a comparison with the previous literature
The authors present a framework to perform meta learning on the loss used fortraining. They propose a framework that generated a loss that can transfer to otherdatasets without any tuning required. Experiments on MNIST and Cifar10 are conducted to show the effectiveness of the proposed method. While the paper does a great job at presenting the problem and its applicationsand propose a framework that generated a loss that can transfer to otherdatasets without any tuning required. However, there are some important drawbacks of this work. The main drawback is that the method is only evaluated on one loss function that came from your EC. This means that the loss will only be evaluated in certain regimes of its inputs
This paper studies the generalization bound for GANs based on a new definition of generalization error where the distribution corresponding to the generator is assumed to be known for each generator (i.e., there is no empirical distribution for generators). In my opinion, most of the theoretical results seem follow directly from standard tools in statistical learning theory and existing results on capacity bounds of neural networks. It seems that the authors do not introduce new ideas or techniques in the analysis. The authors made comparisons with the related results in Arora et al (2017) and Chen et al (2019) for their generalization bounds while very similar results where shown before by [1] and [2].
The reviewers agree that the paper is well written and well motivated. The reviewers also agree that the paper should be accepted. The paper should be submitted to a future venue.  The paper should be submitted to a future venue.  The reviewers agree that the paper should be submitted to another venue.
This paper proposes to a novel VAE GAN hybrid which, during training, draws multiple samples from the reparameterized latent distribution for each inferred q(z|x), and only backpropagates reconstruction error for the resulting G(z) which has the lowest reconstruction. While the idea is interesting, the change in results over Rosca et. al. (Rosca et. al. [1] and Rosca et. al. (Rosca et. al. [2] and Rosca et. al. [3] is also generalized as a reversed KL based GAN in [2] [3]. The authors failed to discuss this with these previous works.
The paper propose a novel way to measure similarity between datasets, which e.g. is useful to determine if samples from a generative model resembles a test dataset. The reviewers found the paper to be well written and interesting. However, there were some concerns about the clarity of the exposition and the clarity of the experiments. The reviewers also raised concerns about the clarity of the experiments. The authors addressed these concerns in the rebuttal. The reviewers also raised concerns about the clarity of the experiments. The authors have addressed these concerns in the rebuttal. The reviewers agree that the paper is well written and the experiments are interesting.
The paper proposes cAdv and sAdv, two new unrestricted adversarial attack methods that manipulates either color or texture of an image. In order to make manipulated images photo realistic, colors to be replaced are chosen by energy values, while textures are replaced with style transfer technique. The former minimises the cross entropy as well as the loss that defines the texture differences. Experimental results show that the proposed methods are more robust on existing defense methods and more transferrable accross models. The paper also performs a user study to show that the generated examples are fairly imperceptible like the C&W attack. The paper is written clearly and organized well to understand. The proposed approaches of attacking on colour
This paper proposes metrics for evaluating concept based explanations in terms of ‘completeness’   characterized by (1) whether the set of presented concepts if sufficient to retain the predictive performance of the original model and (2) how is performance affected when all information useful to a complete set of concepts (as per (1)) is removed from features at a specific layer.   The reviewers found the paper to be interesting and relevant to the interpretable machinelearning literature. However, there were some concerns about the clarity of the experiments and the motivation of the paper. The authors could do much more to motivate theirproposals by convincing the reader that (a) existing concept based explanations
This paper presents a phrase based encoder decoder model for machine translation. The authors present a dynamic programming method for considering all possible segmentations in decoding. This submission additionally describes how an external dictionary can be incorporated using a heuristic approach.  The reviewers agree that the idea of phrase to phrase translation and the relatively simple architecture proposed in the paper is of less novelty and the idea is of less novelty. I also have a concerns about the experiments. The dataset used in this paper seems not convincing to me. Another reason is that the computational cost of the proposed model is not really clear. The authors just tested the model performance on WMT test set.  I recommend rejection.
The reviewers agree that this paper is not ready for publication at ICLR. The main concern of the reviewers is the lack of clarity in the proofs and experiments. The main observation that the authors make is that the last layer weights are updated as in the Perceptron algorithm and as long as the first layer has learned a large margin representation, the first layer weights do not change much. This would still have a margin of \Omega(1/n). The main observation that the authors make is that the last layer weights are updated as in the Perceptron algorithm and as long as the first layer has learned a large margin representation, the first layer weights do not change much. 
The reviewers were concerned about the clarity of the paper, the lack of a reference/citation, and the lack of clarity of the experimental results. The reviewers were also concerned about the choice of the specifics of the architecture, such as the number of filters, layers or the presence / absence of batch normalization (part 3.1). The reviewer would like to see how the results vary with respect to those parameters.The Frechet distance should be introduced with a citation. The magenta model requires a citation as it is not obvious what is being referred to.
This paper provides a theoretical basis called expert induced MDP (eMDP) for formulating imitation learning through RL when only partial information on the transition kernel is given. The imitation policy is trained in an environment where the responsive state features are simulated and controllable by the agent, and the unresponsive state features are replayed from the demonstrations. Based on eMDP, the paper proposes a reward function based on an integral probability metric between a state distribution of expert demonstrations under the target MDP and a state distribution of the agent under the eMDP. The paper compares the proposed method against behavior cloning, which is known to perform poorly under limited data.  The main contribution of the
This paper presents a model based approach to safety in RL, where the agent uses a transition model to plan ahead to avoid actions that can lead it to unsafe states. The imagination module is used to perform forward predictions, constructing a graph between possible states. The agent takes the baseline state as input   that can be used to define either a safe or unsafe state, that is used in the planning component. If any action would lead to a "base state" that is an unsafe state that action will not be executed and another "safe" action is selected from the policy. A practitioner might also specify safe/unsafe states as an additional source of information about the reward. This paper suffers from
The paper proposes an energy based model (learned using the transformer architecture) to learn protein conformation from data describing protein crystal structure (rather than one based on knowledge of physical and biochemical principles). To test the efficacy of their method, the authors perform the task of identifying rotamers (native side chain configurations) from crystal structures, and compare their method to the Rosetta energy function which is the current state of the art. The experimental results are comprehensive and mostly supports the claims made in the paper. The baselines that are compared to are set2set and Rosetta. The reviewers agree that the experimental results are comprehensive and mostly supports the claims made in the paper. The paper is clear
This paper proposes a modification of GANs where the latent space follows a distribution modelled by a Gaussian Mixture Model. The softmax output is considered as the mixture weights of the GMM model and controls the loss function of the generator accordingly. The overall structure is clear, although writing can be improved. However, as the authors skip the sampling step to be able to back propagate through the model, this is not significant.  The paper is hard to follow. It is not introduced.
This paper investigates the choice of noise distributions for smoothing an arbitrary classifier for defending against adversarial attacks. A new framework for certification is proposed, which allows to use different distributions compared to previous work based on Gaussian noise. From this framework, a trade off between accuracy and robustness is identified and new distributions are proposed to obtain a better trade off than with Gaussian noise. The theoretical results are interesting, showing a clear trade off between robustness and accuracy with a new lower bound and deriving better smoothing distributions. However, the experimental results are lacking, and do not support much the proposed method. The paper contains numerous grammatical errors, confusing statements, and nonstandard phrases.
This paper proposes a method to train graph neural networks on dense hardware such as TPUs. The method is motivated by an observation that connections in graphs have locality in some datasets. While I m not an expert in (G)NN acceleration on TPUs, I have experience with GNNs and approaches to accelerate CNNs in GPUs. The effectiveness of the proposed method is well supported by the experiments. The scope has to be broadened both in terms of the NN models and the hardware types. It is for these reasons that I think the paper is not appropriate for ICLR.  The reviewers agree that this paper is not suitable for ICLR.
This work summarizes the existing methods of mutual information estimation in a variational inference framework and describes the limitations in terms of bias variance tradeoffs. The authors prove that discriminative approaches that are based on the partitionfunction approach suffer from high variance where mutual information is high(Theorem 2). This high variance problem is something that has previously been observedempirically and is the main theoretical point that is being made aboutlimitations of MI estimators. They prove that their clipping approach reduces variance and thereforeintroduces a bias variance tradeoff. Further, density ratio clipping is proposed to lessen a high variance problem in estimating a partition function. The proposed SMILE estimator is natural. The comparison
This work develops a differentiable spectral projection layer to enforce spatial PDE constraints using spectral methods, to achieve the introduction of the physical constraints in the end to end network without damaging the intrinsic property of the network. The proposed strategy is very general, and can indeed be used with any PDE constraint that is a linear combination of differential operators. The experimental comparison demonstrates the superiority of the proposed method. On the negative side, the paper does not fully deliver on the promise to make physics constraints in deep networks usable in practice. In addition, the paper unfortunately contains only a single example of the proposed method. The experimental comparison is limited to a single example of the proposed method, which is not sufficient to
This paper proposes a functional form to model the dependence of generalization error on a held out test set on model and dataset size. The parameters of the function are then fit using linear regression on observed data. The authors show that the regressed function \(\epsilon(m,n)\) is able to predict the generalization error for various \(m\) and \(n\) reasonably accurately. This paper states 5 necessary criteria for any functional form for generalization error predictor that jointly considers dataset size and model size, then empirically verifies it with multiple datasets and architectures. The reviewers found the paper to be well written and interesting. The main concern raised by the reviewers was the lack of clarity
The paper presents a method for training a certified robust neural network, based on the certification method of cohen et al. It proposes a novel objective which is derived by decomposing the 0/1 certified loss into the sum of 0/1 classification error and 0/1 robustness error. Since directly maximizing this robust radius is intractable, prior work seeks to derive a lower bound which the authors term the *certified radius*. The authors propose a soft randomized smoothing loss for the *certified radius* (a lower bound for the l_2 attack radius) of the classifier. The authors provide certain theoretical guarantees and also demonstrate strong empirical results relative to two baseline approaches. 
This paper presents a novel and effective solution to the difficult task of learning reusable motor skills from unlabeled actions. The paper aims to learn middle level motor task primitives from unlabeled actions.  The reviewers agree that this paper is well written and well motivated.  However, it is not clear to me how much the accuracy gain in the latent representation transfer to the accuracy of the actual recomposed task.
This paper proposes MPO, a policy optimization method with convergence guarantees based on stochastic mirror descent that uses the average of previous gradients to update the policy parameters. The authors also proposed a variance reduced policy gradient algorithm following the variance reduction techniques in optimization. The paper has good contributions in both theoretical and algorithmic aspects to policy optimization family. The authors further proved the convergence of the proposed algorithms and some experiments are conducted to show the effectiveness of their algorithms. The empirical results are also very promising. The experiments are, however, limited and miss important baselines discussed in previous sections. Some terms like "projected gradient" and "baseline" (in the context of variance reduction) are
This paper discusses the detection of out of distribution (OOD) samples for variational autoencoders (VAE). The idea is to train the encoder such that its output variational distribution q(z|\bar{x}) is pushed away from the prior of latent z. The method proposes adding a term to the loss of the VAE that encourages the variational posterior (q) to distribute latent codes (z) for inliers and outliers differently. However, later they propose to skip a (negative) reconstruction error term for the negative data. The authors attempt to justify the objective by writing out a variational lower bound for a VAE with a mixture prior where
This paper introduces an interesting new direction in transfer learning for reinforcement learning, that is robust to the differences in the environtment dynamics. The authors hypothesize that by learning how to assign credit, structural invariants can be learned which the agent can exploit to assign credit effectively and thus learn more efficiently in new environments (be it in domain or out of domain). They hypothesize that by learning how to assign credit, structural invariants can be learned which the agent can exploit to assign credit effectively and thus learn more efficiently in new environments (be it in domain or out of domain). The reviewers agree that this paper proposes an interesting general avenue for research in transfer learning in RL. However, the
This paper proposes to use deep RL to learn an optimal policy for global parameter tuning in the parameter server setup of distributed training. The paper formalizes an RL problem by minimizing the total training time to reach a given validation accuracy.  The reviewers found the paper well written, and the results are promising. However, there were some concerns about the lack of experiments in a controlled environment, such as on a real cluster, or for training more computationally demanding models, such as an LSTM or other recurrent unit. The authors addressed these concerns in the rebuttal, and the reviewers were satisfied with the results. However, there were still some concerns about the lack of experiments in a controlled environment,
The work proposes using variational distributions to model the model the inference of latent variables and model the partition function building on NVIL, thereby providing an algorithm that would work on general MRFs for both inference and learning. The paper builds on NVIL by using two variational distributions for the NLL and how to solve the parameter estimation problem. Since the two terms in the NLL are opposite in sign, it is a minimax operation and GAN like adversial training can be used. The paper shows providing tighter results to estimate the log partition function and comparisons on the digits dataset and Anneal importance sampling. The theoretical analysis supports that the algorithm is reasonable. Experiments: The authors show the
The paper presents a novel visual planning approach which constructs explicit plans from "hallucinated" states of the environment. This image is responsible for the generalization to unseen environments, but it is a major drawback, as the image must be created beforehand. The paper is well written, the experiments are well conducted and analyzed. The paper is well written and clear. However, the paper is not ready for publication in its current form. The main weakness of the paper is that the method is evaluated on an environment, which is too simple. The paper claims to perform zero shot generalization and to adapt to changes in the environment, like the slight changes in camera motion, variations in lightning, but it
In this paper, the authors generalize the randomized smoothing type of robustness certification to handle many types of attacks beyond norm based attacks, e.g., geometric perturbation, volume change, pitch shifts on audio data. The authors provide details for a few examples from image and audio data processing, and then show experimental results on ImageNet data.  The reviewers found the novelty of the proposed generalization to be limited, and the novelty of the experimental results to be limited.  The main contribution of this paper is in that: Theorem 3.2 is valid for random smoothing using gaussian with general covariance matrix.  However, the novelty of the experimental results is limited
This paper explores the role of the alpha parameter when learning word embeddings. It turns out that alpha controls the distance between the words in the embedding transformation process. The authors provide theoretical insights on the role of alpha in relation with the original co occurrence matrix, and propose a new method to find its optimal value. Results are shown on several word similarity tasks.  The reviewers agree that the paper is well motivated and well motivated. However, there are two major issues with the paper. First, though the paper is well motivated and puts itself nicely in context of previous work, it needs a copy editor as there are many language/grammar issues some of which I highlight below. Second,
This paper introduces a new type of layer, Farkas layers, that are designed to ameliorate the "dying ReLU problem". However, the paper doesn t have convincing baselines and doesn t dig deep enough into what the Farkas layer is actually doing. The proposed Farkas layer is too simple and seems not work well. With BN, the FarkasNet does not show significant improvements than the traditional ResNet with BN on CIFAR 10, CIFAR 100 and ImageNet. Without BN, though FarkasNet shows significant improvements than ResNet. I am also not completely satisfied with the authors  explanation on why Fark
This paper proposes a fine tune technique to help BERT models to learn & capture form and content information on textual data (without any form of structural parsing needed). They key addition to the classic BERT model is the introduction of the R and S embeddings. R &S are supposed to learn the information in text that is traditionally represented as the structural positions and the content bearing  symbols in those positions. For each bert embedded token, the proposed method aims at disentangling semantic information of the word from its structural role. The proposed method aims at disentangling semantic information of the word from its structural role. The proposed method aims at disentangling semantic information of the word from
The use of von Neumann divergence as a loss for this task is perhaps novel. This divergence includes the matrix logarithm, which is perhaps computationally expensive. It is unclear why the paper decided to use von Neumann. Also, heteroscedastic regression has a long history in neural networks, dating back to at least Nix and Wiegand in 1994. Thus, it is unclear if the design choices in the paper have any strong bearing in the empirical performances. The experiments are not compelling, there are no comparisons to alternative models and the datasets used are small scale. The paper should also include and perhaps compare to their datasets. Overall, the paper makes an attempt at designing
This paper proposes an ML based method to optimize TensorFlow Graph execution. The motivation is interesting, and the proposed method is technically reasonable.  The paper is well written and the proposed method is technically reasonable.  The paper is well written and the proposed method is technically reasonable.  The paper is well written and the proposed method is technically reasonable.  The paper is well written and the proposed method is technically reasonable.
This paper proposes TrojanNet, a new threat model and corresponding attack in ML security. The Trojan horse attack is an attack scenario where the adversary trains a classifier in a way so that the learned model performs well on a main task, while after a certain permutation of the parameters specified by the adversary, the permuted model is also able to perform another secret task. The Trojan horse attacking scenario is novel and provides new insights to the research in adversarial machine learning. However, the main concern is with the validity of the threat model as it seems to assume the ability to get arbitrary software (in particular, the program that applies the permutations) onto the victim s server, at which point
The paper presents and evaluates an anomaly detection algorithm that is used for identifying anomalies in satellite data. The performance is evaluated with a real satellite data set. Though the results look interesting, I vote for rejection because the paper is not well fitted to the ICLR community.Reject; rating score: 1; rating score: 3; In this paper, a tensor based anomaly detection system for satellites is proposed. The performance is evaluated with a real satellite data set. The performance is evaluated with a real satellite data set. The results look interesting, but the paper is not well fitted to the ICLR community.
This paper proposes to use a trained GAN model as the prior distribution for Bayesian inference to quantify the uncertainty. In the first half of the paper, the general framework of the Bayes estimation is introduced. The authors use a GAN to estimate this prior distribution. Then, The authors proposed how to incorporate GAN to the Bayesian inference. There are some grammar issues in the paper. It is not clear how the HMC parameters are fixed. I have missed some references to related work on inverse problems. I do like the extension of applying the idea in physics problems. But I don’t have a full picture over this area, I’ll read the rebuttal.
This paper proposes to learn the ‘key steps’ at which to to apply an adversarial attack on a reinforcement learning agent. Instead of choosing the steps by heuristic, the authors propose to choose the key steps by augmenting the reward function with a penalty to decrease the ratio of attacks. They then proceed to compare to to prior work that use heuristics based on the policy values to select the steps to attack. The RL approach is compared with a random attack policy and two heuristic methods for attacking agents in games on the Atari benchmark.  The paper is well written, with sufficient background and related work section for the paper to be self contained. The proposed framework is an interesting
The paper proposes to learn the routing matrix in routing networks for multi task learning (MTL) using the gumbel softmax trick for binary random variables. While the end results are good, and the approach is well motivated, I am leaning to reject, because the experiments have not made clear when the method works and how it behaves. The authors do not provide any explanation for why this approach did not succeed in their settings. Additionally, there is not much theoretical discussion about what the Gumbel Softmax adds to routing networks. I would encourage the authors to conduct more experiments and comparisons.
This paper has received mixed reviews from the reviewers. The main concern is the lack of clarity in the theoretical explanation and the bloated notation. In the same paper, Han et al.showed this method can solve an HJB equation in 100 dimensions. While it serves as an explicit mean to reduced the sample complexity of methods in RL, it appears to be about avoiding premature convergence in this work. In general, the HJB equation and the FBSDEs can be much more difficult to work with. All reviewers agree that the writeup and formatting is still very much sub standard and must be improved to make this paper worth publishing. In particular, the theoretical explanation and the bloated notation should be
The paper defines the quantity of "gradient SNR" (GSNR), shows that larger GSNR leads to better generalization, and shows that SGD training of deep networks has large GSNR. The authors consider the variance and mean of the per sample gradients for each parameter of the model and define for each parameter the Gradient Signal to Noise ratio (GSNR). The GSNR of a parameter is the ratio between the mean squared of the gradient per parameter per sample (computed over the samples) and the variance of the gradient per parameter per sample (also computed over the samples). The GSNR is promising as a measure of generalization and the authors provide a
The paper introduces a new Gaussian prior objective, "D2GPo", that addresses the fact that in sequence generative models, all incorrect predictions are penalized equally by MLE, a phenomenon which the authors refer to as the negative diversity ignorance drawback. The prior penalizes incorrect generations/predictions when they are close to the reference; thus, in contrast with standard MLE alone, the training objective does not equally penalize all incorrect predictions. Experimental results show that the proposed method consistently improves the performance of the state of the art methods for neural machine translation, text summarization, storytelling, and image captioning. In all cases, simply adding the proposed prior improves over a state of
The authors present an approach to improve performance for retro synthesis of chemical targets in a seq2seq setting using transformers. Instead of just using token masking, they provide alternate proxy decompositions for a target molecule by randomly removing bond types that are likely to break and by transforming the target based on known templates. Both the pre training model and the mixture model are combining the specific domain knowledge to improve the generalization and diversity of retrosynthesis. There is not a huge algorithm novelty for the methods proposed in this paper, but they can well address the domain issues and improve the performance. Overall, I think the community will benefit from this work.
The paper provides a theoretical study of regularization capabilities of over parameterized convolutional generators trained via gradient descent, in the context of denoising with an approach similar to the "deep image prior". This paper studies the theoretical reasons why a randomly initialized decoder or autoencoder like architecture can prove useful for image denoising, by using early stopping. The paper is clearly written and the proofs are interesting. It will be of interest to the community.  The reviewers agree that the paper is clearly written and the proofs are interesting. The paper is of interest to the community.
This paper studies a puzzling question: if larger parameter counts (over parameterization) leads to better generalization (less overfitting), how does pruning parameters improve generalization? To answer this question, the authors analyzed the behaviour of pruning over training and finally attribute the pruning s effect on generalization to the instability it introduces. The authors also study some other aspects of pruning (e.g., pruning as a noise injection) and conclude the paper.  The reviewers raised several issues with the paper, including the definition of stability, the fact that the test accuracy after pruning improves if it changes, and the fact that the generalization gap is not a function of the number
This paper studies the impact of deletions of random neurons on prediction accuracy of trained architecture, with the application to failure analysis and the specific context of neuromorphic hardware.    The reviewers were concerned about the novelty of the paper and the clarity of the results.    The reviewers were also concerned about the clarity of the results.    The paper is not ready for publication at ICLR 2019.
This paper provide a NAS algorithm using Bayesian Optimization with Graph Convolutional Network predictor. The main claim of the paper is that this approach works well  for the multi objective case. However, the reviewers are concerned about the following: 1. The main claim of the paper is that this approach works well  for the multi objective case. However, no theoretical proof is provided to guarantee that the performance is getting better during while loop in Algorithm 1. 2. The main claim of the paper is that this approach works well  for the multi objective case. However, no theoretical proof is provided to guarantee that the performance is getting better during while loop in Algorithm 1. 3. The
The main contribution of this paper is that it proposed an estimator of Jacobian regularization term for neural networks to reduce the computational cost reduced by orders of magnitude, and the estimator is mathematically proved unbiased. Each of these points seem to have important contributions for the field. However, as authors are aware of, it needs to be validated experimentally. There are two concerns. 1) This paper needs to demonstrate the effectiveness of the input output Jacobian regularization over the input gradients regularization. 2) Experimental results on CIFAR10 and ImageNet show accuracy degradation on clean test data. The authors should focus on the approximating algorithm rather than the merits of Jacobian regular
The reviewers agree that the proposed method is interesting and the proposed method is scalable. It may be interesting to explore different ways of making predictions based on this decomposition based inference. However, the proposed method is not ready for publication in its current form. The authors are encouraged to improve the proposed method and resubmit to a future venue.
This paper presents a new technique (LSRA) improving Transformer for constrained scenarios (e.g., mobile settings). It combines two attention modules to provide both global and local information separately for a translation task. The authors propose their method called "Long Short Range Attention (LSRA)," which separates the self attention layers into two different purposes, where some heads focus on the local context modeling while the others capture the long distance relationship. The paper is well written and easy to follow. The experimental results look good. However, there is a large gap between the main claim and what they have done. While LSRA is included in the search space of Evolved Transformer, surprisingly, their searching algorithm
This paper proposes a so called self supervised method for learning from time series data in healthcare setting. The reviewers have raised several concerns about the proposed method, including lack of comparison with recent work on the same datasets, lack of clarity of the proposed architecture, and lack of comparison with unsupervised learning for representation learning. The authors are encouraged to address these concerns and resubmit to another venue.
This paper empirically studies the behavior of deep policy gradient algorithms during the optimization. The paper explores a critical divergence between theory and practice, emphasizing that while deep policy gradient algorithms seem to work in certain cases, they don t seem to be working foor the reasons underlying their derivations. To the best of my knowledge, the findings of this paper are new and not predictable by the current theory. The reviewers agree that this is an interesting and important paper, it emphasizes and analyzes how policy gradient methods modify their objective functions and how this leads to training differences (and often errors).
The paper proposes a Plug and Play LM model for controlled natural language generation. Their goal is to assess the degree of  steerability  rather than building a controlled generation model. This is exciting and a great research direction. However, there does not appear to be any evaluation with any existing work that performs controlled text generation. I would actually expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. There is an interesting relation to the NIPS 2019 paper: https://arxiv.org/abs/1907.04944  They also rely on gradient descent to steer a pretrained language model. I imagine the authors will emphasize
The reviewers agree that the paper is interesting and the results on the convergence seem convincing. However, there are concerns about the novelty of the approach and the applicability of the results to more practical problems. The authors should be more clear (from the abstract) on what algorithms they study. The results on the convergence seem convincing but a subtantial optimization problem remains for future work. After rebuttal   I ve read the authors s response. The reviewers agree that the results on the convergence seem convincing but a subtantial optimization problem remains for future work. The authors should be more clear (from the abstract) on what algorithms they study. After rebuttal   I ve read the authors
This paper proposes using non parametric filters like Discrete Cosine Transform (DCT) and Discrete Walsh Hadamard Transform (DWHT) which have been widely used as feature extractors in vision and image processing before deep learning became prevalent as layers especially to replace pointwise convolution (PC) layers in deep network architectures like ShuffleNet v2 and MobileNet V1. The proposed method aims to reduce the computational complexity of CNNs without degrading the performance.  The paper is well organized and easy to read although the writing and presentation can use some work.  The experiments are sufficient and I am glad to see that this simple idea works in practice. However, the rebuttal
The authors provided a novel technique about the resizable approach and the experimental results look promising. I think this is a really neat idea, and as far as I m aware it is novel. As shown in the paper [1],  one can find that the author presented they could improve both accuracy and efficiency.   However, the main problem I think is the training budget issue. The authors did not address this in the revision. I guess it is partially because I did not understand why the number of sampling should be len(L) for each mini batch (it is actually due to fair sampling).   I recommend the authors redraw all the figures for clarity.    
The paper proposes a new 2 D graph convolution method to aggregate information using both the node relation graph and the attribute graph generated using, e.g., PMI and KNN. The main contribution of their work is a type of graph convolution that combines convolutions operating on these two graphs. The basic idea is to reduce the intra class variance. The experiments seem to show that the performance is quite different. The authors provide theorems and proofs to support this claim even though it is quite intuitive that smoothing with similar neighbours preserves higher variance with respect to dissimilar neighbours. The experiments seem to show that the performance is quite different. The authors discuss related work sufficiently with one exception:
The paper proposes to explain a phenomenon that the increasing robustness for adversarial examples might lead to performance degradation on natural examples. The paper is well written with theoretically motivated experiments and detailed analysis. While the results are interesting, there are still some major concerns regarding the paper, such as the clarity of the experiments and the experimental part of the paper. It would be more convincing if the authors could perform analysis on different adversarial training methods, e.g.FGSM and even the unified gradient perturbations developed in C2. Moreover, it would also be useful to provide some intuition about how margins relate to the confidence of a classifier. In summary, it is good that a theoretical bound
This paper considers the problem of adversarial robustness. The paper shows that (Theorem 1) robust generalization error can be bounded in terms of the standard generalization error and a stability term, that does not depend on the labels. The paper suggests that we can use unlabeled data to improve the robust generalization. The authors first consider the toy model presented in Schmidt et al. 2019, and show how the labeled sample complexity in the robust setting can be lowered to match the standard setting if sufficient unlabeled data is available. The authors also collect additional images from an unlabeled and uncurated dataset (Tiny Images) and show that they can utilize them using their proposed
This paper proposes Sparse Deep Predictive Coding (SDPC) to access the impact of the inter layer feedback, which is suggested by neuro scientific evidence. The proposed model added a new term to the loss function, which represents the influence of the latter layer on the current layer. It is found that the top down term is beneficial in terms of reducing predictive error and can learn faster. The experimental results show that the top down term is beneficial in terms of reducing predictive error and can learn faster. However, the STL 10 is still a small scale dataset, and the baselines provided in the rebuttal are still limited. In terms of novelty the paper does not make a breakthrough contribution, but
This paper proposes using a new version of graph networks – multiplex graph networks – which do object representation followed by some form of graph processing and reasoning to answer "IQ test" style diagrammatic reasoning, in particular including Raven Progressive Matrices that have been previously studied (a little). The paper shows very strong results on multiple datasets, much stronger than previous results (from strong groups) on these datasets. The approach uses gated graph networks that also uses an aggregation function. This provides improved results over earlier WREN method. However, the writing quality is poor and is the primary reason for my giving it a low score. The proposed model is non interpretable. Graphs are conceptual in the proposed
This paper analyzes the invariance properties of the K FAC algorithm by reconstructing the algorithm in a coordinate free way where the neural network is viewed as a series of affine mappings alternating with nonlinear activation functions. While K FAC has been developed as approximation to the exact natural gradient update, they come up with a different Riemannian metric, definition of space, etc., such that in the end, K FAC is the exact natural gradient for that. So K FAC can be viewed as the exact natural gradient under the new metric rather than an approximation under the Fisher metric. Without empirical studies, it is not easy to see the significance of this work.
This paper proposes to compress the network weights by quantizing their values to some fixed codeword vectors. The algorithm quantizes matrices of linear operations, and, by generalization, also works on convolutional networks. The proposed method can be expected to perform well empirically, which the experiments verify, and to have potential impact. Overall, I think that the proposed algorithm is easy to apply and the draft is relatively well written. This paper is overall easy to follow. However, there are some concerns about the novelty of the proposed method, as it implicitly assumes that weight W_{ij} has a high "correlation" with weights W_{i+kN/m,j
This paper presents a new pre training procedure for image text representations called UNITER. The idea is to train the model on a huge collection of different image text datasets and the use the model for downstream tasks. The paper modifies an existing pre training procedure by conditional masking (Section 2). The evaluation on both pre training tasks and downstream tasks show that the method is working well in practice. The major limitation of this paper is why. Why the tasks used for pre training build a network that is so informative? Why the tasks used for pre training build a network that is so informative? This should be clarified in the main text of the paper.NoveltyThe novelty of the paper is
This paper deals with the problem of finding an adversarial examples when only the output of a model can be evaluated, but not its gradient. The key idea of the paper is building a Gaussian MRF (a Gaussian with a sparse inverse covariance matrix with a special band structure) to maintain a model for the gradients for predicting search directions. The numerics show effective for using fewer queries to obtain high attack accuracy. While it is certainly interesting to see that such a simple heuristic can achieve comparable performance on standard datasets over other black box attack methods in the limited query budget regime, I would expect to see some experiments that illustrate the quality of the gradient estimator more closely (not
This paper proposes a new way to learn the optimal transport (OT) cost between two datasets that can utilize subset correspondence information. The cost function is optimized via a loss function derived from side information, specifically subsets of the two datasets to be aligned that contain elements that should be matched in the optimal transport plan. The prior work does not have a mechanism to utilize such information or requires more side information such as pair wise matching. The paper show good results of the proposed method for dataset alignment on several applications. The paper could be stronger if it presented more real world examples/results and emphasized that this is an important problem to solve. The reviewers agree that this is an important problem to solve.
The paper is well written and the experiment section is extensive. The contributions of the paper are experimental. However, the goodness of the approximation is measured with (24), which the authors called "subjective error". The authors should provide more evidences to justify the approximation. Given this, I don’t think the task of comparing <f, \mu’_l, \nu’_l> with the POT results really says anything about their power in computing W. I would be glad though to hear back from the authors to see if my understanding is accurate, and adjust my evaluation from there.
The main contribution is to combine BO with dimension reduction, which leads to the effectiveness in generating black box adversarial examples in the regime of limited queries. The paper presentation and writing is high quality although the paper is a bit over length. However, the novelty is limited given the fact that there is another related paper published [1]. The authors in [1] consider using Bayesian optimization to make adversarial attack for model testing. In particular, they have considered the deep learning model. There is a big overlapping between the idea in [1] and the current paper. Such a baseline is not clear in the paper, and the comparison with (Tu et al., 2019) is not provided.
This paper presents a new method for deep learning classification of land cover and crop types using remote sensing data. The main idea is to generate, during training, two versions of the same patch by randomly dropping out channels, and to penalise the difference between the two corresponding embeddings, using the InfoNCE loss. The authors evaluate on a single data set, that seems to not have been used previously. The empirical results on a single custom dataset based on OSM is limited. To make the evaluation more solid it would be good to compare on other data sets, for example on EuroSAT [1], and with other, possibly supervised classification methods, see, e.g., [
This paper presents a method to learn an embedding space for each pixel in a video that indicates the instance id of the objects. With spatio temporal embedding loss, it is claimed to generate temporally consistent video instance segmentation. The proposed approach is simple and general and handles the problem of occluding objects in videos. The authors show that the proposed method performs nicely on tracking and segmentation task, even when there are occlusions. Overall, this paper is well written and well motivated. The reviewers agree that the method is moving towards the right direction that 3d geometry and 2d instance representation should be considered jointly under the scenario of video learning. However, there is a major
This paper studies the problem of learning disentangled representation in a hierarchical manner. Specifically, common representations are captured at root level and unique representations are learned at lower hierarchical level. The HDN is trained in a generative adversarial network (GAN) manner, with additional hierarchical classification loss enforcing the disentanglement.  The proposed approach is shown to improve generalizability and interpretability.  However, the reviewers were concerned about the lack of comparison with existing supervised/unsupervised methods on disentangled representations (e.g. see [1], [2], [3], [4], [5], [6], [7], [8], [9], [10
The paper proposes an adversarial attack on the exemplar based continual learning algorithm, A GEM. The attacker assumes to have access not only to the model but also to the episodic memory. I think that is quite a powerful assumption for the attacker and it is not very surprising that the attack would work. It is not clear whether the proposed method will also work well for other exemplar based methods like iCaRL or GEM (the simpler version than A GEM), etc. The paper claims that the investigated method is SOTA, but it s not clear this is the case, even in restricted class of similar episodic memory based models, see [1] for an independent evaluation
This paper studies the trajectory induced by applying gradient descent/gradient flow for optimizing a homogeneous model with exponential tail loss functions, including logistic and cross entropy loss in particular. This is an important direction in recent theoretical studies on deep learning as we need to understand which global minimizer the training algorithm picks to analyze the generalization behavior. The authors show that the limit points of gradient descent are KKT points of a constrained optimization problem. This is proved in two steps:  1. Assuming that gradient descent manages to find a set of network parameters that separate the data, thereafter gradient flow/descent monotonically increases the normalized margin (rather an approximation of it). While the main body of
The authors use a pre trained RGB to Depth network trained on NYU v2 to predict depth for the images of VRD and VG. The proposed model demonstrates improved results upon state of the art for visual relation prediction.  The reviewers agree that the motivation is interesting and the proposed approach is a good one. However, there were some concerns about the clarity of the experiments and the quality of the results. The reviewers were also concerned about the lack of 3D data. The authors responded to these concerns in the rebuttal. The reviewers were also concerned about the clarity of the experiments. The authors responded to these concerns in the rebuttal. The reviewers were also concerned about the clarity of the experiments.
This paper empirically presents a very interesting connection between two also very interesting phenomena (mode connectivity and lottery ticket hypothesis), while removing a previous limitation of the lottery ticket hypothesis on larger networks. The paper also focused on cases where matching subnetworks were found by IMP, but matching subnetworks can also be found by other pruning methods. Though it is unclear from the paper what are the immediate / straightforward applications, the findings do present interesting contributions. However, the existing amount of experiments sufficiently validates the existence of the connection authors put forth and hence not required. Thus, the paper does not meet the bar for acceptance.
This paper introduces a hierarchical policy structure for use in both single task and multitask reinforcement learning. A combination of different learning techniques for acquiring structure and learning with asymmetric data are used. Firstly, they assess the benefits of the hierarchical structure for single task settings in a simulated environment. The authors then assess the usefulness of such a structure in both settings on complex robotic tasks on complex robotic tasks.  The reviewers agree that the paper has some interesting experiments. However, the main concern raised by the reviewers is that the experimental results are not convincing enough to support the claim that the hierarchical structure works well in both single task and multitask settings. While the experimental results are shown to support this, further discussion
This paper studies the problem of differentially private optimization in the (strongly) convex setting. The authors focus on the gradient perturbation methods, i.e., DP GD and DP SGD, and provide the utility guarantees of DP GD and DP SGD under the so called expected curvature assumption. The expected curvature assumption better captures the properties of the optimization problem, and thus offers an explanation for the advantage of gradient perturbation based methods over objective perturbation based methods over objective perturbation and output perturbation. All the theoretical results provided in this paper are based on the expected curvature assumption (Definition 3). All the proofs are just replacing the strongly conve
The authors attempted to make the dataset represent "real world noise." The hamartia also makes their experimental findings and takeaways about "real noise" questionable, as it is not clear they are testing real noise but consequences of properties of convnet embeddings. However this paper still contributes a dataset, hence this paper still is some sort of contribution. However, with this flaw, it is not clear that it is enough for ICLR.  The paper is not ready for publication at ICLR.
This paper presents a theoretical study of ridge regression, focusing on the practical problems of correcting for the bias of the cross validation based estimate of the cross validation based estimate of the optimal regularisation parameter, and quantification of the asymptotic risk of sketching algorithms for ridge regression, both in the p / n  > gamma in (0, 1) regime (n   # data points, p   # dimensions). The whole study is complemented by a series of numerical experiments. The paper addresses an important problem and puts itself nicely in context of previous work. I am recommending this paper to be accepted for publication at ICLR.
This paper presents a theoretical contribution to deep learning based on information geometry and minimum description length to explain the generalization of deep learning from a Bayesian perspective. The authors employ Fisher Information to characterize the optimal width or the curvature around the minimum. The fact that the determinant of the Fisher Information Matrix is invariant to parametrization, under certain conditions, serves as the motivation to design an objective Bayesian prior called Jeffrey s prior. The authors test this conjecture on an artificial task using a small neural network, and investigate the sensitivity of the results to noise.  The reviewers agree that the paper is interesting, but the paper is not ready for publication at ICLR 2019. The
This paper proposes a new way to train sentence embedding models using NLI data such that very few parameters are used for classification. The authors propose entailment and contradiction operators that learn entailment and contradiction scores while training the parameters of the sentence encoders. The two similarity scores are not too novel, for example, sim_diff is the L1 distance between two vectors. The authors should have tried to an ablation type of approach in equation 1, to check if concatenation alone, element wise dot product alone or absolute difference alone or a combination of any two would work better with the scoring function. Entailment, contradiction, and neutral scores are interesting, but hardly generalize
The authors begin by establishing the existence and uniqueness of the Nash equilibrium in such a setting, and then proposes an mean field actor critic algorithm with linear function approximation. They show that under "standard" assumptions the map μ ↦ μ(π(μ)), μ) is contractive and hence, by the Banach fixed point theorem, has a unique solution, resulting in a unique Nash equilibrium of the mean field game. Second, they show that by using an actor critic method to approximate π(μ) for a given μ, this argument can be turned into an algorithm with provably convergent algorithm with provably linear convergence rate.  The reviewers were generally positive about the paper, and
This paper proposes a heuristic search method for deep reinforcement learning. Several heuristic methods are proposed for this search, and results are demonstrated in Atari domains.  The reviewers were concerned about the evaluation of the method and its practicality, as reflected by the following issues: 1. The method has many hyper parameters. 2. The method has many hyper parameters. 3. The method has many hyper parameters. 4. The major experimental results of the paper are in Table 4 of Appendix D. This is not a good effort to save space by moving the most important results into appendix. Another comment about experiments is that the paper uses pre trained DQN for the ConQur results, where only
This paper presents a new benchmark for natural language understanding called DiscEval. The benchmark focuses on datasets that more directly measure a model s understanding of the discourse structure and relations in the text. Like GLUE and SuperGLUE, datasets from existing works are collated and formated.  The reviewers agree that this is a valuable resource for the research community and that it would be a good resource for other researchers to hill climb their systems on, provided that the data format is standardized and the submission system is easy to use like GLUE.  However, there are some issues with the paper, such as the lack of clarity in the definitions of the tasks, and the lack of clarity in the
This is clearly a timely topic and I loved the motivations of the paper. However, the reviewers raised several concerns about the applicability of the approach and the assumptions on the structure of the causal graph that are only stated late in the paper. The authors have addressed most of these concerns in the rebuttal, and I recommend acceptance. However, there are still some concerns about the applicability of the approach and the experimental setting. I recommend rejection.
This paper proposes a new Reuse and Reduce with Dynamic weight Diffusion (R2D2) layer as an alternative to feed forward layers in neural networks. The main benefit of the proposed R2D2 layer is that the number of parameters can be reduced, as the existing parameters can be reused. In extensive experiments on NLI, NMT, text style transfer and subject verb agreement, feed forward layers in LSTMs and Transformers are replaced with R2D2 layers. The modified models achieve similar performance to the originals, while being more than 50% smaller. The proposed method is well explained. The experiments show that models with less parameters yield comparable performance with their larger counterparts. 
This paper proposes an imitation learning algorithm for the setting where the demonstration data consists of trajectories from sources of varying expertise. The authors define a parameterized model of the (demonstration) trajectory distribution (Equation 2), which uses the MaxEnt RL model for the optimal policy for the optimal policy, and a distribution (p_w) to model the level of expertise. Imitation learning is then reduced to maximum likelihood training under the provided demonstrations. The experiments demonstrate good performance on a set of tasks, and shows results compared against a large number of baselines. However, although the experiments demonstrate good performance on a set of tasks they fail to provide convincing evidence about the generality of the approach
The authors present an extension of variational autoencoders (VAEs), where Gaussian distribution of the latent variable is replaced by a mixture of Gaussians. The approach can be used for clustering and generation. The paper is well written and easy to read and understand. However, the results are only marginally betterthan other methods, and there are several weaknesses with the proposedarchitecture and experimental setup. The main shortcoming is that there is no discussion or experimental comparison with methods like spectral clustering and kernel spectral clustering. Given that the paper and the proposed method relates to similarity based representations it would be important to know how it compares to such methods. However, the technical
This paper is not ready for publication at ICLR. The main contribution of this paper is to show that EI can be used to handle robustness and antifragility to aleatoric noise/uncertainty in the surrogate model using [1]. The main contribution of this paper is to show that EI can be used to handle robustness and antifragility to aleatoric noise/uncertainty in the surrogate model using [1]. The main contribution of this paper is to show that EI can be used to handle robustness and antifragility to aleatoric noise/uncertainty in the surrogate model using [1]. However, the reviewers
The paper describes new norm based generalization bounds that were specifically adapted to convolutional neural networks. The authors empirically showed that there is a correlation between the generalization error of learned CNNs and the dominant term of the upper bound (i.e., the product of the parameter size and the distance from the set of initial parameters). These results improved the upper bounds that we can derive by naively applying the results of Bartlett et al.(2017) or Neushubar et al.(2017), because the dominant term of the existing upper bounds contained $l_{2, 1}$ or $l_2$ norms, which could depend on the input dimensions in the worst case
This paper presents a contextual encoding scheme for reconstruction of 3D pointclouds from 2D images. The authors propose to solve this by learning an MLP that produces the output by recurrent application, and then composing subapplications of different networks as a type of interpolation. While the fast weights approach is not totally original, its application to this problem is novel and very well suited to it. The paper is also quite well written. While most related work was covered well, I believe the authors could have a more up to date list of recent work that reconstructs triangle mesh representations from images [A C] (especially since several of these methods has an architecture that involves encoding and subsequent compos
The paper proposes a probabilistic continual learning approach which learns an optimal adaptation for arriving tasks while maintaining the performance of the past tasks while maintaining the performance of the past tasks. This has been an elusive goal of continual learning research, hence the importance of accepting this work. The paper is well written but the method is rather complex and presumably non trivial to tune. To show the effectiveness of the proposed model, the authors do experiments on several benchmarks. However, the ablation study and analysis on the model is weak and authors only show experimental observations. Also, the experiments are performed on old architectures. I suggest the authors compare with [2], which also focuses on the performance of new tasks.
This work presents a method to use active learning to control the labeling process to gather training data in the context of a semantic segmentation application. The proposed strategy contains several novelties related to the model and the application domain. The proposed algorithms seem to be highly time consuming. The proposed algorithms seem to be highly time consuming. A point that it is not clear to me is the initial training of the segmentation network. The authors deferred the state and action design entirely to the supplementary, while they are the main contributions to the paper. The proposed algorithms seem to be highly time consuming. The proposed algorithms seem to be highly time consuming. The proposed algorithms seem to be highly time consuming. The proposed algorithms
The paper studies the implicit bias of gradient based adversarial training for linear models on separable data with the exponential loss. It attempts to utilize the theoretical results for deriving insights on how adversarial training establishes robustness. Theoretical results are for the training empirical loss. These results are further illustrated by numerical experiments. The paper is also well written, and also has a nice numerical validation of the results. The paper is well written, and also has a nice numerical validation of the results.
The paper proposes an unsupervised approach to learning objects and their parts from images. The authors propose a generative model with a hierarchy of latent variables corresponding to a scene, objects, and object parts. The method is based on the "Attend, Infer, Repeat" (AIR) line of work and adds a new hierarchy level to the approach, corresponding to object parts. The method can form bounding boxes around objects and parts in the examples that are shown, and the MSE is similar to that of a VAE.  The proposed model and learning framework are closely related to previous work (AIR, SPAIR). In the proposed model, there is no constraint on the
This paper introduces a parameterized approach to generate adversarial samples by balancing the speed distortion trade off. The idea is novel, interesting and well formulated, while the intuition could be better explained. While the operating characteristic of probability of success and distortion is mainly discussed, it is unclear which argument most demonstrate the improvement in speed distortion tradeoff. The authors claim it is a fast and effective attack even with quantization. It is also interesting to discuss how the algorithm performs in classes that are linearly separable on a toy dataset. However, the results in Table 3 are not convincing or necessary.
The paper proposes an uncertainty driven acquisition for MRI reconstruction. The main contribution seems to be the combination of deep Bayesian inversion in Adler & Oktem, 2018 with an uncertainty driven sampling framework. The experimental results suggest that the technique can reduce the amount of time required to obtain good quality images from MRI scans which can potentially have a big financial impact. However, the results are rather weak. The main contribution seems to be the combination of deep Bayesian inversion in Adler & Oktem, 2018 with an uncertainty driven sampling framework. The main contribution seems to be the combination of deep Bayesian inversion in Adler & Oktem, 2018 with an uncertainty driven sampling framework. However,
The paper proposes to use an autoencoder, networkX, and node2Vec in succession to convert a Bitcoin transaction to a vector. It uses autoencoder to extract the feature of the transaction and construct a transaction graph. The proposed method lack of detail description. Given the apparent lack of any technical contribution to machine learning theory or practice, the inconclusive empirical results, and the generally unpolished writing (e.g., long run on sentence in the conclusion, vague problem definition), I do not believe this paper is suitable for publication. The authors are encouraged to provide sufficient background introduction, so that the reader can have a big picture of the problem and area.
The reviewers agree that this paper is a worthy contribution to the understanding of network pruning. The empirical study conducted by this paper is useful and complements the results previously reported in the Lotter Ticket Hypothesis (Frankle et al, 2019), so this paper seems to be more of an extension of that work. I recommend acceptance.
This paper proposes word2ket   a space efficient form of storing word embeddings through tensor products. While this results in a time cost for each word lookup, the space savings are enormous and can potentially impact several applications where the vocabulary size is too large to fit into processor memory (CPU or GPU). The experimental results seem to conflate the issues of the dimensionality of the word embeddings versus that of the higher layers. The related work and experimental sections are weak and brief, with only superficial analysis. There is  lack of careful analysis and insight into their results, as well as a careful comparisons to other work in this area. Overall the paper can be a strong contribution
This work analyzes the optimization of deep neural networks from the point of view of the different learning trajectories obtained during different learning settings as brought about by different hyperparameters in optimization. The authors conduct their analysis for networks with and without BatchNorm. To get a quantitative understanding of the different trajectories of the optimization landscape the authors monitor and analyze the A.  Hessian of the network with respect to the parameters B.  Hessian of the network with respect to the parameters A.  Hessian of the network with respect to the parameters B.  Hessian of the network with respect to the parameters A.  Hessian of the network with respect to the parameters B. 
This paper proposes a novel Hybrid RL method, namely the Risk Averse Value Expansion (RAVE), that uses an ensemble of probabilistic dynamics models to generate imaginative rollouts and to model risk aversionof risks by estimating the lower confidence bound of the ensemble. It then proposes a risk averse value expansion (RAVE) to replace the target Q function in the actor critic algorithm, which is built upon an ensemble of probabilistic models (PE) and adopt the lower confidence bound as a surrogate of the target value as in the risk sensitive RL. The experiments show that RAVE does improve over the state of the art algorithms in several different environments, with a better draw down control. 
This paper presents a method for studying the landscape of the loss function w.r.t.parameters in a neural network from the perspective of weight space symmetry.  The reviewers agree that the paper makes some interesting contributions, but the motivation of the algorithm is not clear and the contribution is not enough to warrant acceptance at this time. The paper is not ready for publication at ICLR.
The submission extends the MARL◦MAIR to the extensive Markov game case, where the decisions are made asynchronously. To  this end, the submission takes advantage of the previous game theory results, to formulate the problem, and transform the model to a MAGAIL form. Followed by Theorem1 and 2, authors define an extensive occupancy measure, a natural extension of occupancy measures in MGs, and cast a multi agent imitation learning problem into extensive occupancy measure matching problem in Theorem 3.  The reviewers agree that this is an interesting and challenging problem, and has extended the existing multi agent IRL methods to the extensive Markov game case. However, the reviewers
The paper is not clearly written. The paper misses a lot of important details. The authors skipped many recent papers. Finally, the paper is somewhat incremental. The authors make two claims that are not supported by the empirical evidence. The authors also mentioned that jointly training resulted in images with poor qualities. The authors also mentioned that jointly training resulted in images with poor qualities. The authors also mentioned that jointly training resulted in images with poor qualities. The authors also mentioned that jointly training resulted in images with poor qualities. The authors also mentioned that jointly training resulted in images with poor qualities. The authors also mentioned that jointly training resulted in images with poor qualities. The authors also mentioned that jointly training resulted in images
This work proposes a statistical framework for saliency estimation for black boxcomputer vision models, based on solving a convex program in (4). It also gives theoretical analysis on its consistency in Theorem 1, and run a few simulations to show the empirical performance of the proposed method. The method proposed seems to be novel and reasonable. However, the reviewers had some concerns about the clarity of the paper and the experimental setup. For example, the method is not sufficiently motivated. It is not clear if the problem in equation 4 can besolved by any linear programming software, for which many open source implementations exist, and if the author could remark on the empirical running time. The performance, in terms of
The authors propose a new stochastic reduced variance policy gradient estimator, which combines a baseline GPOMDP estimator with a control variate integrating past gradients by importance re weighting. The key part of the proposed algorithm for variance reduction is to have step wise importance weights to deal with the inconsistency caused by varying trajectory distribution. The authors establish the sample complexity of gradient descent using the proposed estimator, and further demonstrate its effectiveness through some simple empirical results. The empirical results presented are interesting, although I wish they were more comprehensive. In particular, there are a lot of discussions comparing this work with existing work. In Papini et al., 2018, for the experiment part, they use
This paper introduces Progressive Compressed Records (PCR) which is an on disk format for fetching and transporting training data in an attempt to reduce the overhead storage bandwidth for training large scale deep neural networks.  The reviewers had some concerns about the setting of the paper and the experimental results.  The reviewers were concerned about the clarity of the experiments, and the lack of comparison with state of the art methods.  The authors did not demonstrate the performance of their method over state of the art and compare their performance with them.  The reviewers were also concerned about the impact of the batch size when creating PCRs and when reading the image blocks, or the impact of the batch size on the training
The paper uses a mapping from nodes to an embedded space and introduces a second type of a neighborhood: a proximity in the embedded space. In the embedded space, a set of relations of nodes is defined. The method proposed in this work is novel and interesting. The reviewers agree that the method is novel and interesting. However, there is a concern about the lack of clarity in the mauscript, where there is a gain only when the correct embedding is chosen. This is partially supported by the results presented in the mauscript, where there is a gain only when the correct embedding is chosen. The reviewers agree that the method proposed in this work is novel and interesting.
This paper investigates how LSTM networks learn quantum optical experimental setup and predict characteristics of resulting quantum states. The authors train 2 separate recurrent networks (LSTMs). I believe the results are promising, but more clarity and more work on generalizing beyond the restricted regime presented would greatly improve the paper. The paper could have been a nice opening towards new applications for ICLR but it would have to be written in a much more pedestrian manner. In my opinion, this paper should be submitted to a quantum physics journal or conference rather than a machine learning conference.
The paper is well written, and the experiments are well designed to support the claim. However, the paper is below the acceptance threshold due to lack of 1) novelty, 2) significance and 3) depth of analysis. The paper chooses a single method class of model based methods to do this comparison, namely dyna style algorithms that use the model to generate new data. In comparison to prior work, the current paper has a more limited scope and significance.
This paper proposes a novel architecture for spatially structured memory. The main idea is to incorporate inductive bias/invariance derived from projective geometry arguments. The paper presents a differentiable policy network that pastes together semantic map predictions into a spatial map. This information is used to produce actions.  The paper proposes a differentiable policy network that pastes together semantic map predictions into a spatial map. The paper presents a differentiable policy network that pastes together semantic map predictions into a spatial map. This information is used to produce actions.  The main idea is to incorporate inductive bias/invariance derived from projective geometry arguments. The paper presents a differentiable policy network that
The basic idea is to meta learn a model that can adapt to different MDPs using a small amount of data. The main idea is to meta train a fast adapting model of the environment and a shared policy, both conditioned on task specific context variables. At meta testing, only the model is adapted using environment data, while the policy simply requires simulated experience. At adaptation, only the context vector is being updated whereas model parameters (theta) are fixed. Finally, the authors show experimentally that this procedure better generalizes to out of distribution tasks than similar methods.  The reviewers had some concerns about the clarity of the paper and the experimental results. For example, the specification of both environment
This paper proves the universal approximation property of interval certified ReLU networks. The paper shows that approximating a continuous function with a network such that bound propagation works well on it is feasible. The proof is by construction: the function to approximate is decomposed into a sum of functions (slices) with bounded range, which will be approximable to the correct precision. The paper shows that approximating a continuous function with a network such that bound propagation works well on it is feasible. The paper shows that approximating a continuous function with a network such that bound propagation works well on it is feasible. The paper shows that approximating a continuous function with a network such that bound propagation works well
This paper proposes a model and a training framework for question answering which requires compositional reasoning over  the input text, by building executable neural modules and training based on additional auxiliary supervision signals. These modules can be compositionally combined to perform complex reasoning. The proposed model s performance surpasses previous SOTA on several question types. The auxiliary supervision tasks appear to be essential to obtaining the results, most notably the unsupervised loss for IE. However, the comparison with MTMSN using less training data seems a bit unfair since the proposed model is given more supervision (question parse supervision and intermediate module output supervision).  The reviewers agree that the paper is very interesting and the proposed approach is a promising one.
The paper compares the true hessian and the empirical hessian of the loss function, showing the spectrum of the Empirical Hessian is generally broadened, showing the spectrum of the Empirical Hessian is generally broadened, and proposes a way to visualize the spectrum.  The reviewers have concerns about the strength of the assumptions in this paper, especially the independence of the entries in \epsilon. The authors made some discussions on generalizing the assumptions, but it is unclear from the limited discussions whether the same theoretical results could be obtained in the generalized setting. The results obtained in the paper can shed some lights on the understanding of existing optimization algorithms (e.g.,
The authors identify and address the problem of sub optimal and myopic behaviors of self imitation learning in environments with sparse rewards. The authors propose DTSIL to learn a trajectory conditioned policy to imitate diverse trajectories from the agent’s own past experience. This is claimed to drive more efficient exploration in sparse reward problems, leading to SOTA results for Montezuma s Revenge without certain common aides. Extensive experimental results demonstrated the effectiveness of the proposed DTSIL. Overall, this paper is well written with comprehensive experimental results. However, it would be nice to compare it with traditional reinforcement learning (e.g., with \epsilon greedy policy for random exploration)? The authors did not
This paper considers the neural architecture search (NAS) problem under the out of distribution (OoD) environment. As the OoD problem is not visited in the current NAS literature, this paper proposes replacements for each of the three standard components in NAS, i.e., the proxy task, the search space, and the optimization algorithm; and each replacement is built upon an ensemble of existing techniques. The authors propose to use a differentiable architecture search method which model the architectural parameters using a concrete distribution. The approach appears to work on some of their cherry picked OOD datasets. However, the reviewers were concerned about the limited novelty of the approach and the lack of comparison with standard NAS methods.
The paper proposes to do adversarial training on multiple L_p norm perturbation models simultaneously, to make the model robust against various types of attacks. The method is a based on adversarial training against a union of adversaries. The experimental results demonstrate improvement over several baselines. However, the paper is deficient in creativity and generality, so I vote for rejection. Thanks for the response.
This paper adds a very interesting and useful feature to existing autodifferentiation for training neural networks. The second order information can be backprogated just as the first order ones, which can be used to accelerate training. This idea, although according to the paper, is developed upon existing works, still, strongly attracts as the second order information is crucial for training and perhaps visualizing the landscape of neural networks. Issues of this kind have been discussed at length within the community, particularly on GitHub, and related issues with optimization of automatic differentiation code have motivated other software developments, such as Julia s Zygote package. I vote for an acceptance.
This paper proposes a pre trained language model architecture specifically used for task oriented dialogue systems. The basic idea is to alternate between the likelihood of two parties in a dialogue. The second contribution is an alternating parameterization of the model that distinguishes between agent and user utterances. The paper is concerned with reducing the amount of manual dialog state or dialog act labeling in task oriented dialog, following the lead of Eric and Manning (2017) who did not require any such explicit annotation. Experiments are done throughly by comparing to BERT and GPT 2, which reflected the cutting edge research in pre trained language model. In terms of experiment results, the authors show that their approach improves over the basic GPT
All reviewers agree that this paper is well written and the experiments are interesting. The paper is well motivated and the experiments are interesting. However, there is still a bit of room for improvement in terms of the writing of the paper. The paper appears to have way too many typos. The paper appears to have way too many typos.
This paper propose a second order approximation to the empirical loss in the PAC Bayes bound of random neural networks.  The reviewers found the paper to be difficult to follow, and the presentation to be sloppy in various places.  The reviewers also found the paper to be lacking in novelty and impact.  The paper could be improved by improving the presentation and clarity of the claims.  The paper could also be improved in terms of clarity and clarity of the experimental results.  The paper could also be improved in terms of clarity and clarity of the experimental results.  The paper could also be improved in terms of clarity and clarity of the experimental results.
This paper proposes Policy Message Passing, a Bayesian GNN which models edge message passing as a mixture distribution with corresponding coefficient generated by a learnable prior defined on graph state.  The reviewers agree that the idea is new and interesting. However, there are concerns about the clarity of the presentation, the clarity of the experiments, and the lack of concrete execution of the proposed idea. In particular, it is not clear at all how the message functions and the inference procedure functions (or called reasoning agents) are realized in practice. In addition, the choice for the variational distribution q is not mentioned. It seems that the number of parameters of PMP is much larger than GCN and GAT
The paper is well written with clear descriptions, fairly comprehensive analysis and empirical exploration, and good results, and in general I agree that learning quantization so as to minimize quantization related errors on task at hand is a good strategy. However, the paper is not ready for publication at ICLR. The reviewers agree that the paper is not ready for publication at ICLR. I recommend rejection at this time.
The paper introduces a signal propagation perspective for single shot network pruning. The authors first pointed out that a previously studied pruning criterion   connection sensitivity (CS)   is a normalized magnitude of gradients. Based on signal propagation theory, to achieve a  faithful  (with minimal amplification) CS, the gradients must be also faithful. Then by using relation of Jacobians and gradient, the authors proved that orthogonally initial weights guarantees faithful on linear networks and certain distribution property on nonlinear network can achieve layerwise dynamic isometry, which is to ensure faithful signal propagation. The distribution of singular values in the layer to layer Jacobian matrices for pruned networks becomes increasingly
This paper proposes a quantum entanglement approach for deep learning that uses entanglement entropy to derive a low dimensional attention matrix from a high dimensional matching matrix. The main goal of the paper is to show that a low dimensional attention matrix can be derived from a high dimensional matching matrix. The main contribution of this work is to study the theoretical analysis of entanglement entropy for the matching of two objects (question answering pairs), and (2) to qualitatively calculate the matching matrix. The paper is not well written and difficult to follow. The paper cites papers from physical letters and physical reviews without introducing all necessary background. Overall it is not clear what the actual contribution of this work really is.
This paper studies an interesting setting when privacy guarantee and adversarial robustness are both needed in machine learning model and proposes an algorithm to achieve such goal. The authors propose an algorithm called differentially private adversarial learning (DPAL) to achieve such goal. To apply functional mechanism, the authors use the 1st order polynomial approximation of both objective functions (by Taylor Expansion). Overall, this paper studies an interesting setting when privacy guarantee and adversarial robustness are both needed in machine learning model and proposes an algorithm to achieve such goal. The authors demonstrate the advantage of the proposed method both theoretically and experimentally. However, the current version is quite difficult to follow for people without DP background,
This paper presents a single image super resolution method. The only slight variation is that the activation function ReLU for the residual layers are arranged before the convolutional layers are arranged before the convolutional layers. This is not true,there exist online approaches such as https://arxiv.org/abs/1706.09563or distributed methods https://arxiv.org/abs/1901.09235. Finally the latex formatting suffers from many issues and typos. The paper is well written and easy to follow. However, the paper is not ready for publication.
This paper proposes a framework for learning with rejection using ideas from adversarial examples. The proposed approach is not novel. The motivated problem is not new. Both the problem setting and the technique does not have novelty. Only empirical results of the proposed methods are shown. The paper also has problems in writing. In the definition of “suspicious example” at the beginning of Sec.1, is both x and x’ defined as suspicious examples in this way? In the last equation of page 2, there is a rejection function, so minimizing this loss is a “separation based approach”. In the last equation of Page 3, there is no definition of \tilde
This paper proposes a variant of local SGD, post local SGD, for distributed training of deep neural networks. The proposed method is a modification of the local SGD which updates models distributed to several workers in a parallel way and synchronize the model parameters at every few epochs. The post local SGD is a simple extension of local SGD. The proposed method is simple and easy to implement. The authors conducted thorough experiments to investigate the performance of the proposed method. However, the authors claim that the learning rates for mini batch SGD are fine tuned (in Figure 3), which makes the empirical results questionable. The authors are encouraged to report experimental results on distributed training of large LM models
The paper is motivated by the unstable performance of Transformer in reinforcement learning, and tried several variants of Transformer to see whether some of them can stabilize the Transformer. The authors introduce two modifications to the Canonical Transformer. Their results show that transformers are able to learn in memory intensive environments, with some gating combinations surpassing LSTM. The authors have evaluated the overall performance, as well as hyperparameters, seeds, and ablations. The experimental results look good, however, I have problems in understanding the motivation, the intuition of the proposed methods, the experimental design, and the general implication to the research community that is using the Transformer in their day to day
This paper presents a detailed replication study of the BERT pre training model considering alternative design choices such as dynamic masking, removal of next sentence prediction loss, longer training time, larger batch sizes, additional training data, training on longer single document or cross document sequences etc. to demonstrate their efficacy on several benchmark datasets and tasks by achieving the new state of the art results. The proposed RoBERTa achieves/matches state of the art performance on many standard NLU downstream tasks. While the replication study is well appreciated, the novelty contribution of the paper is marginally incremental as the model structure is largely unchanged from BERT. The other techniques applied also are somewhat trivial. Very little can be deduced
The paper presents an unsupervised method for graph embedding. The authors seek to obtain graph representations by maximizing the mutual information between graph level and patch levelrepresentations. The authors naturally apply Deep Graph Infomax, a contrastive representation learning method, for the whole graph level instead of the previous node embedding learning. They also consider a semi supervised task when the Mutual Information based criterion has an additional term which quantifies a classification error, obtained when constructing a classifier based on the obtained graph representations. Experiments on both unsupervised and semi supervised experiments on popular benchmarks demonstrate the effectiveness of InfoGraph and InfoGraph*, receptively. The paper is well written and easy to follow.
The paper presents a technique for extending existing reinforcement learning algorithms to ensure safety of generated trajectories in terms of not violating given constraints. The authors proposeLyapunov based safe RL algorithms that can handleproblems with large or infinite action spaces, and return safepolicies both during training and atconvergence. The experiments are clear and well designed, showing the trade off between performance and safety. There is a novel analysis,simulations, as well as some results on real data.  The reviewers agree that the paper is well motivated and the experiments (although I have some reservations about the setup) demonstrate efficacy of the proposed method. However, it is not trivial to get from using
The reviewers were concerned about the novelty of the method, and the lack of comparison to prior work. The reviewers were also concerned about the applicability of the method to other domains, and the lack of comparison to prior work. The authors responded to these concerns in the rebuttal, but the reviewers were not convinced by the rebuttal. Overall, the paper is not ready for publication at ICLR.
The paper proposes an algorithm to generate boundary OOD positive/negative samples to train a classifier for OOD samples. Experiments are conducted on MNIST and Fashon MNIST datasets, which are used as OOD and in distribution, and vice versa. The proposed method is based on the idea from theoretical analysis, and is reasonable and valid. Overall the paper is well written and well organized. However, the reviewers raised several concerns about the experimental results, including the comparison in MNIST and Fashon MNIST variants, which cannot be seriously taken unless there is a strong theoretical background; especially, MNIST variants are too small to talk about the scalability of the method. 
The paper proposes using causal learning models for alleviating privacy attacks, i.e.membership inference attacks. The authors demonstrate how attack accuracy goes up when one dataset is used for training while one dataset is used for testing while another dataset is used for testing. They also show that optimal causal classifiers are more resistant to membership attacks. The main concern of this paper is the results are only confirmed on synthetic data, where all the 4 datasets are generated from known Bayesian networks (i.e., causal graphs). Another question is about the ‘causal models are known to be invariant to the training distribution and hence generalize well to shifts between samples from the same distribution and across different
This paper presents a new end to end imitation learning method combining language, vision, and motion. The experiments demonstrate the generalization performance of the method. However, the reviewers were concerned about the novelty of the method and the experimental setup. The reviewers were also concerned about the experimental setup and the lack of ablation study. The authors responded to these concerns in the rebuttal, but the reviewers were not convinced by this response. The reviewers were also concerned about the novelty of the method and the lack of ablation study. The authors responded to these concerns in the rebuttal, but the reviewers were not convinced by this response. The paper is not ready for publication at ICLR.
This paper suggests using the activation function k Winners Take All (k WTA) in deep neural networks to enhance the performance of adversarial defense. The reasoning given by the authors is that k WTA activation functions have many discontinuities with respect to the input space. They show that k WTA makes the network very discontinuous with respect to the input x, and thus the adversary could not get useful gradient information. They use the CIFAR10 and SVNH datasets. Their experiments show that the robust accuracies are significantly improved on all evaluated methods when they use the k WTA activation function. They also give reasonable theoretical analysis for their approach. The paper is also well written and easy
This paper proposes a simple regularization technique for domain randomization for reinforcement learning. The authors showed that the proposed method can be useful to improve the generalization ability using CartPole and Car Racing environments. Overall, the paper is well written and the ideas are novel. However, the reviewers have raised some concerns about the scope of the paper and the proposed method. The authors have addressed some of these concerns in the rebuttal. However, the reviewers are not convinced that the proposed method is sufficiently convincing. I recommend this paper to not be accepted until the following issues are addressed: 1. The proposed regularization slightly hinders the performance for the environments near l 1, g 50 (that region
This paper proposes "network deconvolution", a neural network primitive aimed at whitening the activations of each layer of the network. The method is a generalization of batch normalization that not only whitens per channel, but also removes correlations between channels and across spatial locations. Experiments show that the proposed methods improves training speed and predictive accuracy on a number of image classification models.  The reviewers agree that this paper has reasonable contributions to the learning algorithm of the deep neural network, so I recommend the AC to accept this paper.  However, there were some concerns raised by the reviewers, such as the dependence on batch size on the performance of the proposed method, and the lack of comparison
The paper proposes an alternative for masked LM pretraining that s more sample efficient called replaced token detection. Their method basically replaces certain input tokens with alternatives which are sampled from a generator and train a discriminative model to determine whether its generated or real. This method looks as only adding the discrimination task after BERT pretraining task. One inefficiency of BERT is that only 15% tokens are used for training in each example. The paper shows that ELECTRA matches the performance of RobBERTa on the popular GLUE NLP task, with just 1/4 of the training compute.  The reviewers agree that the idea is interesting and the experiments are interesting. The proposed approach is simple and
This paper proposes to mix reinforcement learning and imitation learning to boost the learning of an actor critic architecture. The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. The main advantage of the proposed method is that the reward shaping function is related to the current policy. A practical algorithm based on theoretical derivation is provided. However, the reviewers were not convinced that the proposed method is very novel and that the experiments on sparse reward tasks are sufficient to demonstrate the effectiveness of the proposed method. The motivation and related work need to be made clearer to situate this work with the other related works. In addition to not being very novel
The paper compares between SCE loss,  large margin Gaussian Mixture (L GM) loss and proposes the Max Mahalanobis center (MMC) loss as an alternative to explicitly learn more structured representations and induce high density regions in the feature space. It also attempts to analyze how the softmax cross entropy loss discourages robustness by considering the problem in terms of local density in the pre logit feature space. The authors provide many theoretical justifications, which is inspiring. Overall the paper is well written, with sufficient theoretical reasoning and experiments. I appreciate the careful analysis of the sample densities induced by each method; the N/L^2 vs. N/L result
This paper proposes locally constant network (LCN), which is implemented via the gradient of piece wise linear networks such as ReLU networks. The idea is based on the fact that DNN consisting of only linear transformations and ReLU activations is piecewise linear. The experiments conducted in the paper disclose that training LCN outperforms other methods using decision trees. Specifically, they proved that these two models are in some sense equivalent, and one can transform one model to another. Overall, the idea is clever, the presentation could be improved slightly, and the experiments raise existential questions for this kind of work. However, the paper is well written and the idea is clear.
This paper tackles the problem of RL in partially observable domains with sparse rewards. To address the sparse rewards issue, it proposes a sequence level intrinsic novelty model to guide policy learning. The sequence model is based on a dual LSTM architecture. The proposed model extends the prediction error based model by Pathak et. al. 2019 by adding a SIM (Sequence level Intrinsic exploration Module). In most existing literature, intrinsic motivation bonuses are scored based on individual states or transitions, and not over multi step trajectories. Experiments on VizDoom point navigation tasks show that the proposed model does better than baselines as rewards get sparser. Overall, the paper is easy to follow and
This paper describes a large scale ECG dataset that the authors intend to publish. I think this is an interesting dataset for the ECG machine learning community. However, the paper is not ready for publication in its current form. The reviewers agree that the paper is not ready for publication in its current form. The authors should revise the paper and resubmit to another venue.
This paper presents several models for visual recognition in the presence of image degradation (e.g., low resolution, noise, compression artifacts). In the models, an image enhancement network is placed in front of a recognition model and trained together with the recognizer to improve the recognition accuracy as well as to enhance the image quality. Authors propose a several training schemas to solve this problem and discuss a limitations of each one:    "simple" preprocessing, when the only image enhancement loss is optimized   "simple" preprocessing, when the only image enhancement loss is optimized   "RA" joint optimization of recognition and enhancement loss (supervised and unsupervised)   "
This paper presents audio visual object classification and motion prediction work on a novel dataset of 60 different objects rolling around in a bin tilted to and fro by a robot, with video and 4 channel audio recordings of the object impacts. To do this, the authors built a  tilt bot , which tilts a box and the object within to collect data (sound & vision) of object interactions. It was found that the audio contains significant object classification information. The audio was also good for predicting the trajectory of the object. The experiments are limited to a single domain, a fixed collection of objects, and there are no comparisons with published, SOTA algorithms. Overall the experiments are rather thin with only a few
The reviewers agree that the paper should be accepted. The paper is well written and the experiments are detailed an elaborate. The author has addressed the reviewer concerns in the author response. The paper should be accepted.Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 1;
This paper tackles the problem of multi modal image to image translation by pre training a style based encoder. The encoder is not expected to be invertible. Although this appears to be a simple modification, the empirical performance for generalization and reconstruction qualities prove the effectiveness of the proposal. Overall, this is an incremental work in the field of supervised I2I. The reviewers are not convinced that the proposed model is better than BicycleGAN, and the approach is somehow novel. For instance, I m interested in seeing with some toy distributions, what is the training progress (measured quantitatively) comparing the proposed method and traditional BicycleGAN.
The authors propose a scalable method based on Monte Carlo arithmetic for quantifying the sensitivity of trained neural networks to floating point rounding errors. The experiments show that a measure of sensitivity (K) is indeed a good augmentation to cross validation for model selection for the purpose of trading off accuracy and resource consumption when launching deep neural networks with floating point rounding errors. The method is a straightforward application of Monte Carlo Arithmetic (MCA) to neural networks. They demonstrate that the loss of significance metric K estimated from the process can be used for selecting networks that are more robust to quantization, and compare popular architectures (AlexNet, ResNet etc.) and compare popular architectures (AlexNet, ResNet).
This paper develops computationally efficient convex relaxations for robustness certification and adversarial attack problems given the classifier has a bounded curvature. The authors showed that, when the curvature (Hessian) of the network is bounded, improved certificate can be achieved by convex optimization. While the proposed approaches are theoretically sound, I have several concerns, mostly on the experiments. The main contribution of giving global curvature bounds of neural networks is valid. However all empirical results are only from MNIST. The paper mentions the Attack problem and proposes an attack algorithm (Algorithm 2), however I am not able to find any experiments on the attack. The paper mentions the Attack problem and proposes
The evaluation is limited to tasks of which the state space is small, and the proposed method is not compared with existing methods. There is no baseline method in the literature to be presented to compare with the proposed method. The paper does not clearly state the motivation of the proposed method. I come away from this work not fully appreciating the impact it is trying to sell me on. I would like to see better comparisons between this method and existing policy iteration methods. I m curious about the performance of the proposed method in high dimensional tasks. The paper does not clearly state the motivation of the proposed method. I m curious about the performance of the proposed method in high dimensional tasks.
This paper proposes an attention mechanism that directly reflects the phrasal correspondence by employing convolution. The proposed method is straightforward (which is good): a convolution window with size n is used to calculate representation for an n gram, which then replaces the token representation in a standard attention model. The n gram aware attention is incorporated into Transformer and shows gains in translation and language modeling tasks. On 3 WMT 14 translation tasks, the proposed approach leads to improvements between 0.7 and 1.8 BLEU with respect to Transformer Base. Experiments with machine translation and language modeling show that it outperforms the token attention counterpart. Some claims made in the paper may be too strong.
The reviewers agree that the paper is well written and well motivated. The paper should be accepted.Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; However these section are  important to understand the  rest of the paper.  The reviewers agree that this paper should be accepted.
The paper considers the problem of out of distribution (OOD) sample detection while solving a classification task. The paper is well written, and the background is introduced clearly. Experimental results on several OOD benchmarks with different backbone networks show that their method outperforms ODIN (Liang et al., 2017). In ICML, 2015. In table 2, the reported testing accuracy on CIFAR 10 using Dense BC is 92.4. However, it’s not clear to me how to avoid degenerate solutions at convergence while maintaining good testing performance with the proposed training strategy. It seems that the paper requires a lot of polishing. More about the clarity issues belowFor strong evaluation,
The main contribution of this paper is to learn a universal policy that is able to perform near optimally on test tasks with transition dynamics that were never observed during training. This is achieved by using a "probe policy" to generate short trajectories that are then used to learn a latent encoding to categorise the transition dynamics of the current task. The universal policy is then conditions on both the state and this encoding so that the learned policy can perform well on tasks with different dynamics. The idea of the main algorithm is as follows. A "probing" policy is run for a specified number of time steps. From the trajectory generated, a VAE is used to estimate the hidden parameter governing the
This paper proposes to condition the architecture on the input instances by introducing a "selection network" that learns to retain a subset of branches in the architecture during each inference pass. The method resembles sparsely gated mixture of experts [1] at a high level, but has been implemented in a way that better fits the context of architecture search (which is still technically interesting). The method resembles sparsely gated mixture of experts [1] at a high level, but has been implemented in a way that better fits the context of architecture search (which is still technically interesting). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
The paper is written well, and easy to read. The authors compare their method against Image2Latex approach (2016) that is an end to end pipeline and show that there is significant improvement compared to this approach. They find performance of their two stage model is a bit better than Deng s model on this test set. However, the proposed work does not add a significant insight in solving the problem. The reviewers did not see the explanation of "v_{att}^T" and "u_T".
The paper proposes a new method for zero shot visual transfer for RL, SADALA. The proposed method augments an existing method with a well understood attention mechanism, so the novelty of the approach is relatively low. The experiments of the paper were particularly weak. However, I d encourage the authors to try this. Reject.
This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations via linear transformations. Although experiments show that learning such representations are beneficial for low shot setting of SVHN, it is not clear whether such improvement generalizes to more realistic datasets such as ImageNet. The experiments were only done on simple image datasets. I am wondering this method can be applied to other complex datasets whose latent factors are unknown.  I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations. 
This paper proposes a new iterative pruning methods named Continuous Sparsification. It will continuously prune the current weight until it reaches the target ratio instead of iterative prune the weight to specific ratio. The paper proposes a novel objective function that can be used to jointly optimize a classification objective while at the same time encourage sparsification in a network. The lottery ticket hypothesis and associated work shows that the iterative pruning of a network can lead to a sparse network that performs with high accuracy. On the other hand, the work of Zhou et al.shows that sparse masks (dubbed "supermasks") may be learned without training the parameters of the network. In a
This paper proposes an improved DPN framework with a novel loss function, which uses the standard cross entropy loss along with a regularization term to control the sharpness of the output Dirichlet distributions from the network. The Dirichlet prior network is proposed by Malinin & Gales (2018), the new method in this paper overcomes the challenge of training the network based on the KL divergence which cannot work well for dataset with large number of classes. Instead, this paper proposes a new loss function for DPN, which consists of the commonly used cross entropy loss term and a regularization term. The regularization term is the precision of the Dirichlet from the DPN. 
The paper proposes a new intrinsic curiosity based method that deploys the methods developed in optical flow. Following this algorithm, the agents utilize the reconstruction error in the optical flow network to come up with intrinsic rewards. The authors show that this approach boosts up the behavior of the RL agents and improves the performance on a set of test environments.  While the paper is well motivated and the proposed method is interesting, there are some concerns about the motivation of the proposed method. For example, the choice of tasks where the largest improvement is shown (i.e. 5 Atari games) seems not well motivated and rather crafted for the proposed method. I understand that every game becomes hard exploration if the rewards are omitted
This paper addresses the actively studied problem of efficiently transferring policies across domains in reinforcement learning. The proposed algorithm is based on a policy adaptation mechanism, with the idea that provided that a source optimal policy of a task is available, that policy is adapted to derive the optimal policy of the target task at a low sample complexity. The idea is to "regularize" the target policy being learned to generate a trajectory distribution similar to the one induced by the source optimal policy in the respective MDP. The paper is well written and the proposed algorithm is novel and original. However, I believe there might be some technical issues with the derivations that overall make me doubt about the significance of this work. The proposed
This paper introduces a new privacy notation for the attribute attacks. It also provides an analysis of the intrinsic privacy utility tradeoff. The main weakness of the paper is the experiments, which are not sufficiently convincing. The paper is not ready for publication in its current form. The paper is not ready for publication in its current form.
The paper proposes an alternative to the truncated back propagation through time (BPTT) algorithm for training RNNs. The authors propose to learn the state of the RNNs explicitly by improving the prediction accuracy at each time step as well as predicting the "next" state of the RNN. The novel idea of the paper is therefore a modifiable state buffer for the RNN states. It maintains a buffer of the last N RNN states that can be updated. The states s_i and s_{i T}, as well as the RNN parameters are updated based on this loss function. The experiment results in Section 5 seem to be very strong. The paper is clearly written
This paper focuses on CycleGAN method to show theoretically when the exact solution space is invariant with respect to authomorphisms of the underlying probability spaces for unpaired image to image translation. The addressed issue is important, the investigation is reasonable, and the results are intuitive and plausible, with clear practical implications. The experimental results are interesting, however, they are very limited. The authors are encouraged to expand and provide more experimental evaluations. I think it is a good paper.Accept (Poster); rating score: 8; rating score: 6; rating score: 3;
This paper considers the use of a neural network s Jacobian as additional features for semi supervised, unsupervised, and transfer learning of representations. The idea is simple and the authors motivate this choice by connecting it to the recently proposed neural tangent kernel (although nothing is proven in this paper). The authors motivate this choice by connecting it to the literature on the neural tangent kernel (although nothing is proven in this paper). The ablation study could be more compelling. The discussion of the connection to theory added very little to the paper. A quick paste of PyTorch pseudo code would be sufficient here. Similarly, I’d like to see the features themselves used as a linear class
The authors study how inherent noise in the analog neural networks affects its accuracy. To mitigate this loss, the authors propose a method that combines the method of "noise injection and "knowledge distillation".  The experimental results show that the combination of distillation and noise injection outperforms pure noise injection on all networks, as well as noisy inference without retraining.   The reviewers agree that this is a very promising direction to invest, but the paper is not quite ready for ICLR.  The main concern of the reviewers is that the paper is not quite ready for ICLR.  The authors are encouraged to improve the clarity and clarity of their contribution in the final version.
The paper proposes a pure implicit likelihood approach that uses three discriminator models to estimate the KL divergences to train a generative model. The reviewers found that the paper does not present the impact of its main contribution alone. The reviewers also pointed out that the results are not equivalent, this implies that the discriminator does not reach the optimum. Moreover, the paper uses LPIPS to measure reconstruction quality   but this measure is a deep neural network. The paper uses LPIPS to measure reconstruction quality   but this measure is a deep neural network. The paper uses LPIPS to measure reconstruction quality   but this measure is a deep neural network.
The paper describes a dependency parser for Amharic text trained on the Yimam et al. 2018 treebank. The proposed method is an unlabelled arc eager transition based dependency parser followed by a dependency label classifier. The proposed method is an unlabelled arc eager transition based dependency parser followed by a dependency label classifier. The major problem of the paper is precision. The proposed method is an unlabelled arc eager transition based dependency parser followed by a dependency label classifier. The proposed method is an unlabelled arc eager transition based dependency parser followed by a dependency label classifier. The major problem of the paper is precision.
This paper proposes an end to end multi frame super resolution algorithm, that relies on a pair wise co registrations and fusing blocks (convolutional residual blocks), embedded in a encoder decoder network  HighRes net  that estimates the super resolution image. While I am not an expert on super resolution, I do see a clever algorithm, that can be for example used with different number of input views. However, I have some concerns about this paper: 1) This paper lacks many references. 2) The estimation of a single translational motion for the SR image at the end of the network after all multiple images are already fused. 3) Registration loss is important in this paper and it
The paper studies adaptation of agent policies in a simplified baseball game, which is designed as a zero sum two agent game between a batter (B) and a pitcher (P), each of which has 5 discreet actions. The authors propose a Bayesian style adaptation of the agent strategies (where each agent models the probability of the actions of the opponent by computing the posterior give a prior and evidence from the past observations), which seems to be computable analytically, from an initialization learned with counterfactual regret minimization (CFR) that approximates the Nash of the considered game. The introduced game is fully observable but stochastic, which the authors argue is a challenging setup. The authors
This paper proposes a method for aligning an input text with the frames in a video that correspond to what the text describes in a weakly supervised way. The goal is to identify a video segment within a longer video that is most relevant to an input sentence. The proposed method makes multiple comparisons while computing the attention weights over all words and frames. Then the attentive features are used to localize the sentence query in videos by calculating the similarity of words and frames. The proposed method makes multiple comparisons while computing the attention weights over all words and frames. The proposed method makes multiple comparisons while computing the attention weights over all words and frames. The proposed method makes multiple comparisons while computing the attention weights over all
This paper presents a new method to further align multilingual BERT by learning a transformation to minimize distances in a parallel corpus. The authors evaluated the proposed pre training on the contextual alignment metric and show the BERT has variable accuracy depends on the language. The proposed method improved significantly on zero shot XNLI compares to the base model. The authors also proposed to use the an regulation that prevents the learned embedding from drift too far.  The reviewers agree that this is a solid work. The authors did a good job of analyzing the bert for multi lingual. I would like to see more experiments in this space. From my point of view, the real "non contextual word retrieval"
The paper proposes a learning architecture for graph based semi supervised learning. The input graph is given with only some labeled, and the goal is to label the rest. The node labeling is based of three aspects: 1) node representation to measure the similarity between the centralized subgraph around the unlabeled node and reference node; 2) structure relation that measures the similarity between node attributes; and 3) the reachability between unlabeled query node and reference node. The authors propose to use graph convolution to learn the node representation and random work on graph to evaluate the reachability from query node to reference node. The architecture is trained on a validation set to learn features that optimize the accuracy of inference
This paper proposes a new image to image GAN based translator that uses attention and a new normalization that learns a proper ratio between instance and layer normalization. The experiments are thorough and the paper is well written. There is a thorough description of model architecture, dataset and tuning parameters in the appendix. The paper is clearly organized and feels polished. The novelty is clear and the contributions are clearly explained. Weak points include novelty and significance. The proposed method only consists of instance norm and layer norm. The proposed method only consists of instance norm and layer norm. The proposed method only consists of instance norm and layer norm. The proposed method only consists of instance norm and layer norm. The experiments are thorough
This paper discusses the important problem of model uncertainty in the output of ML models developed for clinical applications. The main arguments are the following:1. current metrics on dataset level cannot reveal uncertainty in prediction on personal level;2. when evaluated on personal level, deterministic NNs with different random initialisations can produce very different predictions (thus require consideration of model uncertainty)3. when evaluated on deep ensemble/Bayesian RNN case, deterministic NNs with different random initialisations can produce very different predictions (thus require consideration of model uncertainty)I am not sure if ICLR is the best venue for this submission, as there is quite little innovation in modelling methodology, and the empirical analysis
The reviewers agree that this paper is not ready for publication at this time. The main contribution of the paper is to show that the gap can be bounded by the variance of prediction probability. However, the empirical results are not convincing. The authors did not provide a clear explanation of why this is the case. Moreover, the authors did not provide a rebuttal to the reviewer comments.
The paper explains through a block model the impact of the complete graph regularization, intended as adding to all the entries of the adjacency matrix a constant. The paper is also well written and the results are clearly presented. Overall, this is a nice contribution to spectral graph theory and so I recommend acceptance.Accept (Spotlight); rating score: 8; rating score: 6;
This paper studies what an embodied agent that is trained using RL learns in the context of the hide and seek game. There are two agents in a simple environment (one hider and one seeker). The paper attempts to analyze how the learned representation differs with different capabilities of the agents or environment structure. The authors interpret the learned representation by using the learnt features to do classification.  The paper is well written, and the results are interesting. However, there is a lot of room for improvement in the paper. For example, it is strange that "visibilityreward" performs worst for "Awareness of Self Visibility". There are conclusions about "temporal events" (e.g
The authors study what they refer to as ecological reinforcement learning, defined as the interaction between properties of the environment and the reinforcement learning agent. They introduce environments with characteristics that reflect natural environments: non episodic learning, uninformative reward signals, and natural dynamics that cause the environment to change. These factors are shown to significantly affect the learning progress of RL agents and, unexpectedly, the agents can sometimes learn more efficiently in these more challenging conditions. While the experimental results show some light about the performance of existing methods in the proposed environment, the paper does not contain any methodological contributions. The main conclusion is the environment should be dynamic and non episodic, and the environment shaping is introduced to be effective.
This paper proposes to improve GAN by learning a function that splits the latent space in several parts and then feed each part to a different generator, this enables GAN to model distribution with disconnected support. They also show how the method is robust to the choice of number of generators used. Reviewers agree that the idea is well motivated and the proposed method is well motivated. However, there were some concerns about the experiments, such as the fact that the proposed method doesn t seem to improve that much in more realistic setting. The author point to the fact that this might be due to the fact that the metrics we used are not sensible to outliers. It also seems like the baselines you used
This paper considers the problem of text classification, especially the settings in which the number of labeled sentences is very small. The main motivation to use PARCUS is that it works better in a low resource setting than recent state of the art models for the high resource case. Furthermore, for training, PARCUS makes use of rationales. However, authors assume, annotations of rationales behind the label, i.e.highlighting tokens in a sentence which are important in deciding its label. I don t think that this kind of work aligns with the theme of learning representations. The authors should discuss the existing few shot learning mechanisms. In the mean time, It is not ready for publication
The paper presents an approach, PDP, to solve Boolean satisfiability (SAT) by decomposing it into Propagation, Decimation and Prediction, where each can be learned with a neural network. The method consists of an energy based loss function which is optimized by a three stage architecture that performs propagation, decimation, and prediction (PDP). The authors show that on uniform random 4 SAT problems, their PDP system outperforms two classical methods, a prior neural method, and performs favorably in comparison to a heavily developed industrial solver. Surprisingly, NeuroSAT cannot handle the problems studied in this work. Overall, the paper is relatively well written and well positioned with respect to
This paper presents an explicit regularization term due to dropout for matrix sensing, showing that dropout improves over SGD without dropout, and plotting generalization gaps and their bounds. The paper first focuses on matrix sensing, showing an explicit regularizer with connections to trace norm regularization and proving a generalization bound. However, both Reviewer #2 and Reviewer #3 mentioned the same issues, saying "much of the technical leverage exploited in this paper comes from earlier work...results a lot like Proposition 2 can be found in [Cavazza et al.]" (#2), and "I am surprised that [Cavazza et al.] Similarly, the generalization bounds
The authors propose in this paper a variant of Deep SVDD which brings semi supervision to this model. The solution is implemented by the encoder of a pre trained autoencoder that is further fine tuned to enforce entropy assumption on all types of training data.  The reviewers agree that the paper is well written and easy to follow (the presentation is especially pleasant to read). The experiment seems to support the authors argument.  The paper is well written and easy to follow.
This paper proposed a method to improve the image to image translation. By utilizing the CG based Synthia and embedded edge maps, it has shown some effects on the Cityscapes dataset. However, the experimental study is weak. The model side has very limited novelty. The pipeline and motivation are similar to previous work for domain adaptation for generative models and bridging domains. The only innovation is to have a middle step of converting these semantic maps to edge maps, and then from edge maps to realistic images. This is not altogether a big change, I was hoping to see it goes in a good direction. The video results look good, and this is one example of exploiting the fact that edge maps
This paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints. The proposed metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K. The weights of this weighted average and K are "hyper parameters" of the metric itself. However, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations. The definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets? My concern
The authors aim at improving the accuracy of numerical solvers (e.g. for simulations of partial differential equations) by training a neural network on simulated reference data. The neural network is used to correct the numerical solver by correcting residuals in a data driven way. The paper tackles an important problem of using NN to speed up expensive simulation computations. The paper tackles an important problem of using NN to speed up expensive simulation computations. In this paper, the authors propose the model that assists PDE solver by correcting residuals in a data driven way. The paper tackles an important problem of using NN to speed up expensive simulation computations. The paper tackles an important problem
The main idea is to cast the problem of improving generalization as the problem of minimizing the normalized information distance between the learned source code and the true source code, defined using the Kolmogorov complexity. The normalized information distance and its approximation via a compression algorithm were developed in earlier work, as noted by the authors. Built upon this framework, the method of using extended encodings of an input sample is presented, which empirically seems to lead to better generalization in the settings of adversarial attack and corruptions. The experiments show that using additional encodings improve robustness in several settings, including in the sense of adversarial robustness.  The main contribution seems to be
This paper investigates representations learned by Siamese trackers. An auxiliary detection task is proposed to induce stronger target representations in order to improve tracking performance.   Reviewers agree that the paper is well motivated and has a clear hypothesis. However, the proposed solution of just integrating an additional detection task branch within the Siamese tracking architecture is naive. The only real effect the detector shows in Figs. 6 and 8 is that the model doesn’t outperform state of the art, it attains competitive performance. The paper shows some qualitative analysis. The same quantitative analysis could also be done for existing state of the art trackers, as it does not require training. Such analysis is
This paper addresses the important problem of molecular structures generation, and more generally of efficient point cloud distributions learning in d dimensional space. The idea is simple but nonetheless developed in a smart and effective fashion. The main issue related to the manuscript is very narrow target of the experimental part, limited to the isomers of a given compound   it would have been interesting to check its potentialities in generating more different structures and distance matrices, and thus to compare its effectiveness versus alternative generative approaches. Although this paper is an application oriented paper, the comparisons with baseline methods are preferred, such as some simple and straightforward baselines, such as some simple and straightforward baselines.
This paper considers a new off policy contextual bandit method that can learn even when the logging policy has deficient support. Three approaches are explored, namely restricting the action space, reward extrapolation, and restricting the policy space. While the problem being solved is very relevant and their approach compares three different approaches to the deficient support problem, I am not sure how this work is positioned with respect to approaches solving similar problems in the reinforcement learning land. How does the proposed method compare to more recent state of the art off policy bandit approaches (Liu et al.(2019), Xie et al.(2019), Tang et al.(2019), Tang et al.(2019)) in the experiments? The work by Liu
This paper introduces an adversarial attack mechanism, Iterative Proportional Clipping (IPC), based on a differentiable MFCC feature extractor. The idea is intuitive and well presented. In contrast to other audio attacks, the authors claim that the perturbations are well masked and are much less audible. However, the changes to the signal are highly noticeable. The authors claim about perceptual properties of the method seem to be somewhat anecdotal and, while I think the method is sensible, just showing the power spectrum is not very informative. Overall, the work is interesting but I have the following concerns. The paper is an incremental research work. I recommend rejection.
This paper proposes a set of criteria to evaluate sentence similarity based on semantic similarity (something that correlates with human judgments of the same), logical equivalence and fluency. The authors train a regressor on the STS B dataset, and show that their model (which is using sentence embeddings from RoBERTa) corresponds better to the ground truth similarity labels than then scaled (but otherwise unchanged) BLEU scores. They then propose a multi dimensional evaluation criteria to evaluate sentence similarity based on semantic similarity (something that correlates with semantic similarity (something that correlates with human judgments of the same), logical equivalence and fluency. The paper proposes a set of criteria for what makes a good evaluation
This paper proposes a new query efficient black box attack algorithm using better evolution strategies. The experimental results show that the proposed method achieves state of the art attack efficiency in black box setting.  The reviewers agree that the proposed method is promising, but there is still a lot of work to be done in terms of intuition and experiments. A possible weakness in the experimental design is that the authors haven t apply any defense methodology to the classification models to be attacked. The current paper simply replaces the bandit with evolution strategies. This is also not clear to me.  After the rebuttal, the authors provided additional experiments to address the first question.  I recommend rejection.
The paper presents a case study of training a video classifier using convolutional networks, and on how the learned features related to previous, hand designed ones. The particular domain considered is of importance for biologists/medical doctors/neuroscientists: zebra fish swim bout classification. In order to identify which particular features the neural networks are paying attention to, the paper used Deep Taylor Decomposition, which allowed the authors to identify "clever hans" type phenomena (network attending to meaningless experimental setup differences that actually gave a way the ground truth classes). This allowed for the authors to mask out such features and make the network attend to more meaningful ones. The experiments are well designed and
This paper studies the problem of detecting and generating adversarial images using class conditional capsule networks. Specifically, this paper first introduced a novel method that detects adversarial examples by class conditional image reconstruction. The proposed method is shown to be more effective at detecting adversarial examples than CNNs. The visualizations of adversarial examples generated by the CapNets are more aligned with the human perception which is very insightful. The comprehensive experiments are conducted. Overall the paper is clearly written and easy to follow. Although the idea is not very novel, the paper makes enough contributions to get accepted. However, there are a few concerns about the paper: (1) The proposed method is not specific to generative models
This ICLR submission deals with a problem of whether selfish agents can learn to use an emergent communication channel, using a sender receiver game as a case study. The authors propose a new sender receiver game that can be tuned for various levels of competition which then allows them to analyze the effects of various levels of cooperation and competition. While the paper is well written and structured, the paper lacks execution and conceptual clarity, and the experimental results are not convincing. The authors have not presented real world scenarios and experiments that demonstrate the importance of selfish communication. The introduction of the circular game is suspect.  The reviewers agree that the game itself is interesting and could be a starting point for more thorough investigation.
This paper proposes a new method for overcoming catastrophic forgetting in continual learning, based on distribution based regularization using the sliced Cramer distance, i.e.Sliced Cramer Preservation (SCP). Unlike previous work on catastrophic forgetting, this paper tackles unsupervised learning scenarios as well as supervised learning. The approach makes a lot of sense, and the idea of replacing generalizing the KL to sliced Cramer distance is quite interested. I think the paper is written quite well, and the approach makes a lot of sense. I think overall this is a great paper, very informative. I recommend acceptance.
This paper proposes a modification of the usual parameterization of the encoder in VAEs, to more allow representing an embedding $z$ through an explicit basis $M_B$, which will be pushed to be orthogonal (and hence could correspond to a fully factorised disentangled representation). There is still an interesting philosophical discussion to be had about when one would like to obtain a “global basis” for the latent space (i.e. when one would prefer more local ones), or when one would prefer more local ones. The reviewers found the paper difficult to read and the theoretical claims problematic. In particular, the following points need justified and clarified. The claim of
This paper proposes a deep RNN via unfolding reweighted l1 l1 minimization, where reweighted l1 l1 minimization algorithms are applied to a video task. The method is based on the deep unfolding methods and incorporates the reweighting mechanism. This paper provides the generalization error bound for deep RNNs and shows that the proposed reweighted RNN has a lower generalization error bound. In Section 3, authors formulate Rademacher complexities for both conventional and proposed method, which shows the generalization performance of the proposed method when d increases. Overall, this paper proposes an effective reweighted RNN model based on the solver of a reweighted
This paper proposes a model for multi hop question answering, specifically in a closed domain setting where the relevant paragraphs are present within a few distractor paragraphs. The system achieves best current published result on the HotpotQA dataset. The authors experiment their model on the HotpotQA dataset and achieve the state of the art performance. Each module of the network seems to be carefully designed and the ablation results and analysis are helpful. But, it s important to experiment with some other datasets. The paper should consider testing on the open domain setting of the HotpotQA dataset. Moreover, it has been also shown recently that the distractor setting can easily be fooled and is not a great benchmark
This paper compares network pruning masks learned via different iterative pruning methods. The authors perform experiments mainly on LeNet with the MNIST dataset and analyze the observations.  The reviewers found the experiments to be too weak and the presentation to be too small to read. The experiments are only presented for LeNet and MNIST and it is non trivial whether they extend to large scale models.  The paper presents results only a single set of experiments using the MNIST dataset with the LeNet architecture.  There is little evidence anything found here will generalise to a ResNet/DenseNet on ImageNet.
The authors propose a distributional robust risk minimization method using Lipschitz regularization and give an approach to approximate the Lipschitz constant for product kernels efficiently.  The main contribution of this work is the theoretical contribution of the two main contributions of the two main contributions of this work, which are Theorems 1 and 2: Theorem 1 is a direct implication of Kantorovich duality, well known in optimal transport, and Theorem 2 is a direct implication of Kantorovich duality, well known in optimal transport.  However, the applicability of the method is still limited if it can be used only for product kernels. The Lipschitz constant of
The paper presents a novel reinforcement learning based algorithm for contextual sequence generation. The method consists in performing correlated Monte Carlo rollouts starting from each token in the generated sequence, and using the multiple rollouts to reduce gradient variance. The algorithm is evaluated on the Karel dataset for neural program synthesis and the MS COCO dataset for image captioning. The proposed algorithm is novel, and the results are promising. The paper is clearly written, and the results are promising. The paper is clearly written. The paper is clearly written, and the results are promising. The paper is clearly written, and the paper is clearly written.
This paper proposed a framework based on a mathematical tool of tropical geometry to characterize the decision boundary of neural networks. This characterisation is used to explain different phenomena of neuralnetwork training, viz. the  lottery ticket hypothesis , networkpruning, and adversarial attacks. The analysis is applied to network pruning, lottery ticket hypothesis and adversarial attacks.  The reviewers were concerned about the clarity of the paper and the application of the theoretical results of the theorem for two applications i.e., network pruning and adversarial examples generation.  The reviewers were also concerned about the lack of clarity of the theoretical results of the theorem for two applications i.e., network pruning, lottery ticket hypothesis
The paper presents DeepSphere, a method for learning over spherical data via a graphical representation and graph convolutions. The proposed architecture is a combination of existing frameworks based on the discretization of a sphere as a graph. The proposed architecture is then demonstrated on several problems as well as shown how it applies to non uniform data. The experiments show the proposed model achieves a good tradeoff between the prediction performance and the computational cost. The approach both outperforms baselines in inference time and accuracy. The experiments performed are thorough and interesting. Although the theoretical result is not strong enough, the empirical results show the proposed approach is promising. The unevenly sampled data is a nice extension showing the generality of
This paper proposes to use hypernetwork to prevent catastrophic forgetting. The authors demonstrate that their approach achieves SOTA on various (well chosen) standard CL benchmarks (notably P MNIST for CL, Split MNIST) and also does reasonably well on Split CIFAR 10/100 benchmark. The method is based on hypernetworks. For the rehearsal objective in 2, L2 penalty is used. Reviewers agree that the idea of the paper is well explained and the proposed method is well motivated. However, there was some concern about the use of Eq 2 and the comparison with the closest methods like HAT and PackNet. The authors have addressed these concerns in the rebuttal.
This paper proposes to combine amino acid sequence representation and tissue information to predict the activation of protein on specific tissue. The method consists in training a linear classifier on the output of two existing embedding methods, UniRep/SeqVec and OhmNet, respectively embedding the amino acid sequences and the tissue specific protein protein interaction networks, respectively embedding the amino acid sequences and the tissue specific protein protein interaction networks.  The reviewers agree that this is an interesting and important topic (prediction of protein function), where ML is likely to have an big impact. However, the presentation of the paper and idea is not in an acceptable form (the presentation of the paper and idea is not
This paper proposes a novel one shot pruning algorithm which improves the training of sparse networks by maximizing the norm of the gradient at initialization. Reviewers agree that the proposed method is intriguing and well motivated. However, the theoretical explanation and some of the experimental results are not convinced by the theoretical explanation and some of the experimental results (see detailed comment below). I agree with Reviewer #4 that increasing gradient norm at initialization is a promising direction on its own, which warrants acceptance. However, the experiments section of the paper misses some important sparse training baselines and needs some improvement. The writing is excellent for most of the paper.
The paper presents an interesting signal processing based extension of CNNs, where the first layer convolution is replaced by some pre defined filter banks. Since those filter banks are parameterized with a smaller number of parameters, while they have been proven to be effective in audio processing, I was convinced that this approach could produce better performance than a generic CNN with no such consideration. However, the main benefit is that the layer can be specified with a small number of parameters (+ filter configurations that are typically fixed beforehand get to be tuned to improve inductive bias). While this is interesting, I do not see any conceptual novelty. As stated in the text, many works propose to initialize the CNN with a specific filter
