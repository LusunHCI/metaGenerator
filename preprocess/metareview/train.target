An interesting analysis of the issue of short horizon bias in meta optimization that highlights a real problem in a number of existing setups. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K FAC does indeed work well, it would be a great addition to a final version of this paper. Nonetheless, I think the paper would be a interesting addition to ICLR and recommend acceptance.
The reviewers present strong concerns regarding presentation of the paper. The approach appears overly complex, some design choices are not clear and the experiments are not conducted properly. I recommend the authors to carefully go through the reviews.
The chief contribution of this paper is to show that a single set of policy parameters can be optimized in an alternating fashion while the design parameters of the body are also optimized with policy gradients and sampled. The fact that this simple approach seems to work is interesting and worthy of note. However, the paper is otherwise quite limited   other methods are not considered or compared, incomplete experimental results are given, and important limitations of the method are not addressed. As it is an interesting but preliminary work, the workshop track would be appropriate.
The manuscript proposes a simple technique for adaptive ensemble prediction. Unfortunately, several significant concerns were raised (by R2 and R3) that this AC agrees with. Both R2 and R3 asked fairly specific questions and requested follow up experiments, which have not been addressed. 
i am a big fan of this idea, but i agree with the reviewers that evaluating this idea on bAbI (which was originally created from a small set of rules and primitives) discounts quite a bit of what is being claimed here. one of the future directions mentioned by the authors ("investigating whether the proposed n gram representation is sufficient for natural languages") should have been included even with a negative result, which would ve increased the significance significantly.
This paper presents a new method for approximate Bayesian inference in neural networks.  The reviewers all found the proposed idea interesting but originally had questions about its novelty (with regard to normalizing flows) and questioned the technical rigor of the approach.  The authors did a good job of addressing the technical concerns, causing two of the reviewers to raise their scores.  However, the paper remains just borderline and none of the reviewers are willing to champion the paper as their questions about novelty and empirical evaluation remain.  The reviewers all questioned fundamental technical aspects of the paper (which were clarified in the discussion), indicating that the paper requires more careful exposition of the technical contributions.  Taking the reviewers feedback and discussion into account, running some more compelling experiments and rewriting the paper to make the technical aspects more clear would make this a much stronger submission.  Pros:   Provides an interesting idea for approximate Bayesian inference in deep networks   The paper appears correct   The approach is scalable and tractable  Cons:   The technical writing is not rigorous   The reviewers don t seem convinced by the empirical analysis   Incremental over existing (but recent) work (Luizos and Welling)
An interesting model, for an interesting problem but perhaps of limited applicability   doesn t achieve state of the art results on practical tasks. Paper has other limitations, though the authors have addressed some in rebuttals.
This paper marries the idea of Gaussian word embeddings and order embeddings, by imposing order among probabilistic word embeddings. Two reviewers vote for acceptance, and one finds the novelty of the paper incremental. The reviewer stuck to this view even after rebuttal, however, acknowledges the improvement in results. The AC read the paper, and agrees that the novelty is somewhat limited, however, the idea is still quite interesting, and the results are promising. The AC was missing more experiments on other tasks originally presented by Vendrov et al. Overall, this paper is slightly over the bar.
Dear authors,  The reviewers agreed that the theoretical part lacked novelty and that the paper should focus on its experimental part which at the moment is not strong enough to warrant publication.  Regarding the theoretical part, here are the main concerns:   Even though it is used in previous works, the continuous time approximation of stochastic gradient overlooks its practical behaviour, especially since a good rule of thumb is to use as large as stepsize as possible (without reaching divergence), as for instance mentioned in The Marginal Value of Adaptive Gradient Methods in Machine Learning by Wilson et al.   The isotropic approximation is very strong and I don t know settings where this would hold. Since it seems central to your statements, I wonder what can be deduced from the obtained results.   I do not think the Gaussian assumption is unreasonable and I am fine with it. Though there are clearly cases where this will not be true, it will probably be OK most of the time.  I encourage the authors to focus on the experimental part in a resubmission.
* presents a novel way analyzing GANs using the birthday paradox and provides a theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse * significant contribution to the discussion of whether GANs learn the target disctibution * thorough justifications
The problem addressed here is an important one: What is a good evaluation metric for generative models?  A good selection of popular metrics are analyzed for their appropriateness for model selection of GANs.  Two popular approaches are recommended: the kernel Maximum Mean Discrepancy (MMD) and the 1 Nearest Neighbour (1 NN) two sample test.  This seems reasonable, but the present work was not recommended for acceptance by 2 reviewers who raised valid concerns.  From a readability perspective, it would be nice to simply list the answer to question (1) directly in the introduction.  One must read more than a few pages to get to the answer of why the metrics that are advocated were picked.  It need not read like a mystery.  R4: "The evaluations rely on using a pre trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification"  R2: "First, it only considers a single task for which GANs are very popular. Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions."   the first point of which is also related to a concern of R4.  Given the overall high selectivity of ICLR, the present submission falls short.
This work is interested in using sentence vector representations as a method for both doing extractive summarization and as a way to better understand the structure of vector representations. While the methodological aspects utilize representation learning, the reviewers felt that the main thrust of the work would be better suited for a summarization workshop or even NLP venue, as it did not target DL based contributions. Additionally they felt that the work did not significantly engage with the long literature on the problem of summarization.
This paper received divergent reviews (7, 3, 9). The main contributions of the paper   that multi agent competition serves as a natural curriculum, opponent sampling strategies, and the characterization of emergent complex strategies   are certainly of broad interest (although the first is essentially the same observation as AlphaZero, but the different environment makes this of broader interest).  In the discussion between R2 and the authors, I am sympathetic to (a subset of) both viewpoints.  To be fair to the authors, discovery (in this case, characterization of emergent behavior) can be often difficult to quantify. R2 s initial review was unnecessary harsh and combative. The points presented by R2 as evidence of poor evaluation have clear answers by the authors. It would have been better to provide suggestions for what the authors could try, rather than raise philosophical objections that the authors cannot experimentally rebut.  On the other hand, I am disappointed that the authors were asked a reasonable, specific, quantifiable request by R2   "By the end of Section 5.2, you allude to transfer learning phenomena. It would be nice to study these transfer effects in your results with a quantitative methodology.‚Äù   and they chose to respond with informal and qualitative assessments. It doesn t matter if the results are obvious visually, why not provide quantitative evaluation when it is specifically asked?  Overall, we recommend this paper for acceptance, and ask the authors to incorporate feedback from R2. 
This paper presents a memory architecture for RL based on reservoir sampling, and is meant to be an alternative to RNNs. The reviewers consider the idea to be potentially interesting and useful, but have concerns about the mathematical justification. They also point out limitations in the experiments: in particular, use of artificial toy problems, and a lack of strong baselines. I don t think the paper is ready for ICLR publication in its current form.  
This is another paper, similar in spirit to the Wasserstein GAN and Cramer GAN, which uses ideas from optimal transport theory to define a more stable GAN architecture. It combines both a primal representation (with Sinkhorn loss) with a minibatch based energy distance between distributions. The experiments show that the OT GAN produces sharper samples than a regular GAN on various datasets. While more could probably be done to distinguish the model from WGANs and Cramer GANs, this paper seems like a worthwhile contribution to the GAN literature and merits publication. 
The paper received borderline negative scores: 5,6,4.  The authors response to R1 question about the motivations was "...thus can achieve similar classification results with much smaller network sizes. This translates into smaller memory requirements, faster computational speeds and higher expressivity." If this is really the case, then some experimental comparison to compression methods (e.g. Song Han s PhD work at Stanford) is needed to back up this.  R4 raises issues with the experimental evaluation and the AC agrees with them that they are disappointing. In general R4 makes some good suggestions for improving the paper.  The author s rebuttal also makes the general point that the paper should be accepted as it contains ideas, that these are sufficient alone: "We strongly believe that with some fine tuning it could achieve considerably better results, however we also believe that this is not the point in a first submission...". The AC disagrees with this. Ideas are cheap. *Good ideas*, i.e. those that work, as in get good performance on standard benchmarks are valuable however. The reason for having benchmarks is to give some of objective way of seeing if an idea has any merit to it. So while the reviewers and the AC accept that the paper has some interesting ideas, this is not enough for warrant acceptance. 
This paper generally presents a nice idea, and some of the modifications to searn/lols that the authors had to make to work with neural networks are possibly useful to others. Some weaknesses exist in the evaluation that everyone seems to agree on, but disagree about importance (in particular, comparison to things like BLS and Mixer on problems other than MT).  A few side comments (not really part of meta review, but included here anyway):   Treating rollin/out as a hyperparameter is not unique to this paper; this was also done by Chang et al., NIPS 2016, "A credit assignment compiler..."   One big question that goes unanswered in this paper is "why does learned rollin (or mixed rollin) not work in the MT setting." If the authors could add anything to explain this, it would be very helpful!   Goldberg & Nivre didn t really introduce the _idea_ of dynamic oracles, they simply gave it that name (e.g., in the original Searn paper, and in most of the imitation learning literature, what G&n call a "dynamic oracle" everyone else just calls an "oracle" or "expert")
The reviewers consider the paper to promising, but raise issues with the increase in the complexity of the MDP caused by the authors  parameterization of the action space, and comparisons with earlier work (Pazis and Lagoudakis).   While the authors cite this work, and say that they that they needed to make changes to PL to make it work in their setting (in addition to adding the deep networks), they do not explicitly show comparisons in the paper to any other discretization schemes.   
The paper proposes an approach to jointly learning a data clustering and latent representation.  The main selling point is that the number of clusters need not be pre specified.  However, there are other hyperparameters and it is not clear why trading # clusters for other hyperparameters is a win.  The empirical results are not strong enough to overcome these concerns.
Well written paper on a novel application of the local reprarametrisation trick to learn networks with discrete weights. The approach achieves state of the art results.  Note: I apreciate that the authors added a comparison to the Gumbel softmax continuous relaxation approach during the review period, following the suggestion of a reviewer. This additional comparison strengthens the paper.
This work extends upon recent ideas to build a complete summarization system using clever attention, copying, and RL training. Reviewers like the work but have some criticisms. Particularly in terms of its originality and potential significance  noting "It is a good incremental research, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us.". Still reviewers note the experimental results are of high quality performing excellent on several datasets and building "a strong summarization model." Furthermore the model is extensively tested including in "human readability and relevance assessments ".  The work itself is well written and clear.
The paper proposes augmenting Neural Statistician with a meta context variable that specifies the partitioning of the latent context into the per dataset and per datapoint dimensions. This idea makes a lot of sense but the reviewers found the experimental section clearly insufficient to demonstrate its effectiveness convincingly. Also introducing only the unsupervised version of the model, which looks challenging to train, but performing all the experiments with the less interesting semi supervised version makes the paper both less compelling and harder to follow.
I recommend acceptance based on the positive reviews. The paper analyzes critical points for linear neural networks and shallow ReLU networks. Getting characterization of critical points for shallow ReLU networks is a great first step.
Given that the paper proposes a new evaluation scheme for generative models, I agree with the reviewers that it is essential that the paper compare with existing metrics (even if they are imperfect). The choice of datasets was very limited as well, given the nature of the paper. I acknowledge that the authors took care to respond in detail to each of the reviews.
Thank you for submitting you paper to ICLR. ICLR. The consensus from the reviewers is that this is not quite ready for publication. In particular, the experimental results are promising, but further work is required to fully demonstrate the efficacy of the approach.
The paper reports unusally rapid convergence of the ResNet 56 model on CIFAR 10 when a single cycle of a cyclic learning rate schedule is used.  The effect is analyzed from several different perspectives. However, the reviewers were not convinced because the effect is only observed for one task, so they question the significance of the result. There was significant discussion of the paper by the reviewers and area chair before this decision was reached.  Pros: + Paper illustrates a "super convergence" phenomenon in which training of a ResNet 56 reaches an accuracy of 92.4% on CIFAR 10 in 10,000 iterations using a single cycle of a cyclic learning rate schedule, while a more standard piecewise constant schedule reaches 91.2% accuracy in 80,000 iterations. + There was partial, independent replication of the results on other tasks reported on OpenReview.  Cons:   In the paper, the effect is shown for only one architecture and one task.   In the paper, the effect is shown for only a single run.   There are no error bars to indicate which differences are significant. 
The AIRL is presented as a scalable inverse reinforcement learning algorithm. A key idea is to produce "disentangled rewards", which are invariant to changing dynamics; this is done by having the rewards depend only on the current state. There are some similarities with GAIL and the authors argue that this is effectively a concrete implementation of GAN GCL that actually works.  The results look promising to me and the portability aspect is neat and useful!  In general, the reviewers found this paper and its results interesting and I think the rebuttal addressed many of the concerns. I am happy that the reproducibility report is positive which helped me put this otherwise potentially borderline paper into the  accept  bucket.
The reviewers agree that the paper presents nice results on model based RL with an ensemble of models. The limited novelty of the methods is questioned by one reviewer and briefly by the others, but they all agree that this paper s results justify its acceptance.
The paper proposes a modification to Adam which is intended to ensure that the direction of weight update lies in the span of the historical gradients and to ensure that the effective learning rate does not decrease as the magnitudes of the weights increase.  The reviewers wanted a clearer justification of the changes made to Adam and a more extensive evaluation, and held to this opinion after reading the authors  rebuttal and revisions.  Pros: + The basic idea of treating the direction and magnitude separately in the optimization is interesting.  Cons:   Insufficient evaluation of the new method.   More justification and analysis needed for the modifications.  For example, are there circumstances under which they will fail?   The modification to Adam and batch normalized softmax idea are orthogonal to one another, making for a less coherent story.   Proposed method does not have better generalization performance than SGD.   Concern that constraining weight vectors to the unit sphere can harm generalization. 
This paper resulted in significant discussion   both between R2 and the authors, and between the AC, PCs, and other solicited experts.  The problem of language grounding (and instruction following) in virtual environments is clearly important, this work was one of the first in the recent resurgence, and the goal of understand what the agents have learned is clearly noble and important. In terms of raw recommendations, the majority reviewer recommendation is negative, but since concerns raised by R2 seemed subjective (which in principle is not a problem), out of abundance of caution, we solicited additional input. Unfortunately, we received feedback consistent with the concerns raised here:    The lack of generality of the behavior found. Even if we ignore the difficult question of why the agent prefers what it does, it s unclear how the conclusions here generalize much farther than the model and environment used; the manuscript does not provide any novel or transferable principals of the form "this kind of bias in the environment leads to this kind of bias in models with these properties".    We realize even providing that concrete a statement might be hard, but also missing are thorough comparisons to other kinds of models (e.g. non deep, as asked by R1) to establish that this is a general phenomenon.  Ultimately, there is a sense that this is too narrow an analysis, too soon. If there was one architecture for learning embodied agents in 3d environments that was clearly successful and useful, then studying its properties might be interesting (even crucial).  But the dust in this space isn t settled. Our current agents are fairly poor, and so the impact of understanding the biases of a specific model trained in a specific environment seems fairly low.  Finally   this not taken into consideration in making the decision   it is not okay to list personal homepage domains (that may reveal author identity to ACs) as conflict domains; those are meant for institutional conflicts/domains. 
Overall this paper seems to make an interesting contribution to the problem of subtask discovery, but unfortunately this only works in a tabular setting, which is quite limiting.
R1 thought the proposed method was novel and the idea interesting. However, he/she raised concerns with consistency in the experimental validation, the trade off between accuracy and running time, and the positioning/motivation, specifically the claim about interpretability. The authors responded to these concerns, and R1 upgraded their score. R2 didn‚Äôt raise major concerns or strengths. R3 questioned the novelty of the work and the experimental validations. All reviewers raised concerns with the writing. Though I think the work is interesting, issues raised about experiments and writing make me hesitant to go against the overall recommendation of the reviewers, which is just below the bar. I think this is a paper that could make a good workshop contribution. 
While there are some interesting and novel aspects in this paper, none of the reviewers recommends acceptance.
The paper studies a dropout variant, called fraternal dropout. The paper is somewhat incremental in that the proposed approach is closely related to expectation linear dropout. Having said that, fraternal dropout does improve a state of the art language model on PTB and WikiText2 by ~0.5 1.7 perplexity points. The paper is well written and appears technically sound.  Some reviewers complain that the authors could have performed a more careful hyperparameter search on the fraternal dropout model. The authors appear to have partly addressed those concerns, which frankly, I don t really agree with either. By doing only a limited hyperparameter optimization, the authors are putting their "own" method at a disadvantage. If anything, the fact that their method gets strong performance despite this disadvantage (compared to very strong baseline models) is an argument in favor of fraternal dropout.
Reviewers agree that the paper is well done and addresses an interesting problem, but uses fairly standard ML techniques. The authors have responded to rebuttals with careful revisions, and improved results. 
Unfortunately, it falls short of ICLR standards   from evaluation, novelty and clarity perspectives. The method is also not discussed in all details. 
Thank you for submitting you paper to ICLR.  The consensus from the reviewers is that this is not quite ready for publication. There is also concern about whether ICLR, with its focus on representational learning, is the right venue for this work.  One of the reviewers initially submitted an incorrect review, but this mistake has now been rectified. Apologies that this was not done sooner in order to allow you to address their concerns.
Meta score: 7  The paper combined low precision computation with different approaches to teacher student knowledge distillation.  The experimentation is good, with good experimental analysis.  Very clearly written.  The main contribution is in the different forms of teacher student training combined with low precision.  Pros:    good practical contribution    good experiments    good analysis    well written Cons:    limited originality
This paper presents a nice approach to domain adaptation that improves empirically upon previous work, while also simplifying tuning and learning. 
The authors use the Cayley transform representation of an orthogonal matrix to provide a parameterization of an RNN with orthogonal weights.   The paper is clearly written and the formulation is simple and elegant.  However,  I share the concerns of reviewer 3 about the significance of another method for parameterizing orthogonal RNN, as there has not been a lot of evidence that these have been useful on real problems (and indeed, on most of the toys used show the value of orthogonal RNN, one can get good results just by orthogonal initialization, e.g. as in Henaff et. al. as cited in this work).    This work does not compare experimentally against many of the other methods, e.g. https://arxiv.org/pdf/1612.00188.pdf,  the two Jing et. al. works cited, simple projection methods (either full projections at each step or stochastic projections as in Henaff et. al.).  It does not cite or compare against the approach in https://arxiv.org/pdf/1607.04903.pdf.  
In this paper the authors show how to allow deep neural network training on logged contextual bandit feedback. The newly introduced framework comprises a new kind of output layer and an associated training procedure. This is a solid piece of work and a significant contribution to the literature, opening up the way for applications of deep neural networks when losses based on manual feedback and labels is not possible. 
The reviewers agree that the problem being studied is important and relevant but express serious concerns. I recommend the authors to carefully go through the reviews and significantly scale up their experiments.
Meta score: 6  This is a thorough empirical paper, demonstrating the effectiveness of a relatively simple model for recommendations:  Pros:    strong experiments    always good to see simple models pushed to perform well    presumably of interest to practioners in the area  Cons:    quite oriented to the recommendation application    technical novelty is in the experimental evaluation rather than any new techniques  On balance I recommend the paper is invited to the workshop.
AnonReviewers 2 and AnonReviewer 3 rated the paper highly, with AR3 even upgrading their score.  AnonReviewer1 was less generous:  " Overall, it is a good empirical study, raising a healthy set of questions. In this regard, the paper is worth accepting. However, I am still uncomfortable with the lack of answers and given that the revision does not include the additional discussion and experiments promised in the rebuttal, I will stay with my evaluation."  The authors have promised to produce the discussion and new experiments. Given the nature of both (1: the discussion is already outline in the response and 2: the experiments are straightforward to run), I m inclined to accept the paper because it represents a solid body of empirical work.
 The reviewers have significantly different views, with one strongly negative, one strongly positive, and one borderline negative.  However, all three reviews seem to regard the NaaA framework as a very interesting and novel approach to training neural nets.  They also concur that the major issue with the paper is very confusing technical exposition regarding the motivation, math details, and how the idea works.  The authors indicate that they have significantly revised the manuscript to improve the exposition, but none of the reviewers have changed their scores.  One reviewer states that "technical details are still too heavy to easily follow."  My own take regarding the current section 3 is that it is still very challenging to parse and follow. Given this analysis, the committee recommends this for workshop.  Pros:         Interesting and novel framework for training NNs         "Adaptive DropConnect" algorithm contribution         Good empirical results in image recognition and ViZDoom domains  Cons:         Technical exposition is very challenging to parse and follow         Some author rebuttals do not inspire confidence.  For example, motivation of method due to "$100 billion market cap of Bitcoin" and in reply to unconvincing neuroscience motivation, saying "throw away the typical image of auction."
this submission demonstrates an existing loop hole (?) in rushing out new neural language models by carefully (and expensively) running hyperparameter tuning of baseline approaches. i feel this is an important contribution, but as pointed out by some reviewers, i would have liked to see whether the conclusion stands even with a more realistic data (as pointed out by some in the field quite harshly, perplexity on PTB should not be considered seriously, and i believe the same for the other two corpora used in this submission.) that said, it s an important paper in general which will work as an alarm to the current practice in the field, and i recommend it to be accepted.
This paper is an easy accept   three reviewers have above threshold scores, while one reviewer is slightly below threshold, but based on the submitted manuscript.  It appears that the paper has substantially improved based on reviewer comments.  Pros:  All reviews had positive sentiment: "very elegant and general idea" (Reviewer4); "idea is interesting and potentially very useful" (Reviewer2); "method is novel, the explanation is clear, and has good experimental results" (Reviewer3); "a good way to learn a policy for resetting while learning a policy for solving the problem.  Seems like a fairly small but well considered and executed piece of work." (Reviewer1)  Cons:  One reviewer found that testing in only three artificial tasks was a limitation.  The initial reviews noted several issues where clarification of the text and/or figures was needed.  There were also a bunch of statements where the reviewers questioned the technical correctness / accuracy of the discussion.  Most of these points appear to have been adequately addressed in the revised manuscript.
This one was really on the fence.  After some additional rounds of discussion post rebuttal with the reviewers I think the general consensus is that it s a good paper and almost there but not quite ready for acceptance at this time.  A detailed list of issues and concerns below.  PROS: 1. good idea: an additional loss term that enforces semantic constraints on the network output (like exactly 1 output element must be 1). 2. well written generally 3. a nice variety of different experiments  CONS: 1. paper organization.  The authors start with the axioms they would like a semantic loss function to obey, then provide a general definition, then show it does obey the axioms.  The general definition is intractable in a naive implementation.  The authors use boolean circuits to tractably solve the problem but this isn t discussed enough and it s unreasonable to expect readers to just give a pass on it without some more background.  I personally would prefer an organization that presented the motivation (in english) for the loss definition; then the  definition with a description of its pieces and why they are there; then a short discussion of how to implement such a loss in practice using boolean circuits (or if this is too much put it in the appendix); and a pointer to the axiomatization in an appendix.  2. related to 1, I didn t see anything which discussed the training time of this approach.  Given that the semantic loss has to be computed in a more involved way than usual, it s not clear whether it is practical.
Meta score: 7  This paper presents a novel architecture for neural network based TTS using a memory buffer architecture.  The authors have made good efforts to evaluate this system against other state of the art neural TTS systems, although this is hampered by the need for re implementation and the evident lack of optimal hyperparameters for e.g. tacotron.  TTS is hard to evaluate against existing approaches, since it requires subjective user evaluation.  But overall, despite its limtations, this is a good and interesting paper which I would like to see accepted  Pros:    novel architecture    good experimentation on multiple databases    good response to reviewer comments    good results  Cons:    some problems with the experimental comparison (baselines compared against)    writing could be clearer, and sometimes it feels like the authors are slightly overclaiming  I take  the point that this might be more suitable for a speech conference, but it seems to me that paper offers enough to the ICLR community for it to be worth accepting.  
This paper is borderline.  The reviewers agree that the method is novel and interesting, but have concerns about scalability and weakness to attacks with larger epsilon.  I will recommend accepting; but I think the paper would be well served by imagenet experiments, and hope the authors are able to include these for the final version
The paper proposes to regularize via a family of structured sparsity norms on the weights of a deep network.  A proximal algorithm is employed for optimization, and results are shown on synthetic data, MNIST, and CIFAR10.  Pros: the regularization scheme is reasonably general, the optimization is principled, the presentation is reasonable, and all three reviewers recommend acceptance.  Cons: the regularization is conceptually not terribly different from other kinds of regularization proposed in the literature.  The experiments are limited to quite simple data sets.
This work combines words and images from Tumblr to provide more fine grained sentiment analysis than just positive negative. The contribution is too slight, as a straightforward combination of existing architectures applied on an emotion classification task with conclusions that aren t well motivated and are not providing any comparison to existing related work on finer emotion classification.
This paper considers graph neural representations that use Cayley polynomials of the graph Laplacian as generators. These polynomials offer better frequency localization than Chebyshev polynomials. The authors illustrate the advantages of Cayleynets on several benchmarks, producing modest improvements.  Reviewers were mixed in the assessment of this work, highlighting on the one hand the good quality of the presentation and the theoretical background, but on the other hand skeptical about the experimental section significance. In particular, some concerns were centered about the analysis of complexity of Cayley versus the existing alternatives.  Overall, the AC believes this paper is perhaps more suited to an audience more savvy in signal processing than ICLR, which may fail to appreciate the contributions. 
The reviewers unanimously agree that this paper is worth publication at ICLR. Please address the feedback of the reviewers and discuss exactly how the potential speed up rates are computed in the appendix. I speed up rates to be different for different devices.
The paper provides a constrained mutual information objective function whose Lagrangian dual covers several existing generative models. However reviewers are not convinced of the significance or usefulness of the proposed unifying framework (at least from the way results are presented currently in the paper). Authors have not taken any steps towards revising the paper to address these concerns. Improving the presentation to bring out the significance/utility of the proposed unifying framework is needed.
The submission proposes a strategy for creating vector representations of graphs, upon which a CNN can be applied.  Although this is a useful problem to solve, there are multiple works in the existing literature for doing so.  Given that the choice between these is essentially empirical, a through comparison is necessary.  This was pointed out in the reviews, and relevant missing comparisons were given.  The authors did not provide a response to these concerns.
This work develops a methodology for exploration in deep Q learning through Thompson sampling to learn to play Atari games.  The major innovation is to perform a Bayesian linear regression on the last layer of the deep neural network mapping from frames to Q values.  This Bayesian linear regression allows for efficiently drawing (approximate) samples from the network.  A careful methodology is presented that achieves impressive results on a subset of Atari games.  The initial reviews all indicated that the results were impressive but questioned the rigor of the empirical analysis and the implementation of the baselines.  The authors have since improved the baselines and demonstrated impressive results across more games but questions over the empirical analysis remain (by AnonReviewer3 for instance) and the results still span only a small subset of the Atari suite.  The reviewers took issue with the treatment of related work, placing the contributions of this paper in relation to previous literature.  In general, this paper shows tremendous promise, but is just below borderline.  It is very close to a strong and impressive paper, but requires more careful empirical work and a better treatment of related work.  Hopefully the reviews and the discussion process will help make the paper much stronger for a future submission.  Pros:   Very impressive results on a subset of Atari games   A simple and elegant solution to achieving approximate samples from the Q network   The paper is well written and the methodology is clearly explained  Cons:   Questions remain about the rigor of the empirical analysis (comparison to baselines)   Requires more thoughtful comparison in the manuscript to related literature   The theoretical justification for the proposed methods is not strong
meta score: 5  This paper gives a thorough experimental comparison of convolutional vs recurrent networks for a variety of sequence modelling tasks.  The experimentation is thorough, but the main point of the paper,  that convolutional networks are unjustly ignored for sequence modelling, is overstated as there are several areas where convolutional networks are well explored. Pros:  clear and well written  thorough set of experiments Cons  original contribution is not strong  it is not as radical to consider convolutional networks for sequence modeling as the authors seem to suggest 
The reviewers point out that this is a well known result and is not novel.
The paper presents a modified sampling method for improving the quality of interpolated samples in deep generative models.  There is not a great amount of technical contributions in the paper, however it is written in a very clear way, makes interesting observations and analyses and shows promising results. Therefore, it should be of interest to the ICLR community.
This paper analyzes a problem with the convergence of Adam, and presents a solution. It identifies an error in the convergence proof of Adam (which also applies to related methods such as RMSProp) and gives a simple example where it fails to converge. The paper then repairs the algorithm in a way that guarantees convergence without introducing much computational or memory overhead. There ought to be a lot of interest in this paper: Adam is a widely used algorithm, but sometimes underperforms SGD on certain problems, and this could be part of the explanation. The fix is both principled and practical. Overall, this is a strong paper, and I recommend acceptance. 
The reviewers are in agreement that while the paper is interesting, both the clarity of presentation and experimental rigor could be improved. The committee feels this paper is not ready for publication at ICLR 2018 inits current form.
The authors make an empirical study of the "dimension" of a neural net optimization problem, where the "dimension" is defined by the minimal random linear parameter subspace dimension where a (near) solution to the problem is likely to be found.   I agree with reviewers that in light of the authors  revisions, the results are interesting enough to be presented at the conference.
Reviewers unanimous on rejection. Authors don t maintain anonymity. No rebuttal from authors. Poorly written
This paper proposes adding noise to the parameters of a deep network when taking actions in deep reinforcement learning to encourage exploration.  The method is simple but the authors demonstrate its effectiveness through thorough empirical analysis across a variety of reinforcement learning tasks (i.e. DQN, DDPG, and TRPO).  Overall the paper is clear, well written and the reviewers enjoyed it.  However, a common trend among the reviews was that the authors overstated their claims and contributions.  The reviewers called out some statements in particular (e.g. the discussion of ES and RL) which the authors appear to have addressed when comparing their revisions (thank you).  Overall, a clear, well written paper conveying a simple but effective idea for exploration that often works across a variety of RL tasks.  The authors also released open source code along with their paper for reproducibility (as evidenced by the reproducibility study below), which is appreciated.  Pros:   Clear and well written   Thorough experiments across deep RL domains   A simple strategy for exploration that is effective empirically  Cons:   Not a panacea for exploration (although nothing really is)   Claims are somewhat overstated   Lacks a strong justification for the method other than that it is empirically effective and intuitive
This paper was reviewed by 3 expert reviews. While they all see value in the new task and dataset, they raise concerns (templated language, unclear what exactly are the new challenges posed by this task and dataset, etc) that this AC agrees with. To be clear, the lack of a fundamentally new model is not a problem (or a requirement for every paper introducing a new task/dataset), but make a clear compelling case for why people should work on the task is a reasonable bar. We encourage the authors to incorporate reviewer feedback and invite to the workshop track. 
This paper present a functional extension to NPI, allowing the learning of simpler, more expressive programs.  Although the conference does not put explicit bounds on the length of papers, the authors pushed their luck with their initial submission (a body of 14 pages). It is clear, from the discussion and the reviews, however, that the authors have sought to substantially reduce the length of their paper while improving its clarity.  Reviewers found the method and experiments interesting, and two out of three heartily recommend it for acceptance to ICLR. I am forced to discount the score of the third reviewer, which does not align with the content of their review. I had discussed the issue of length with them, and am disappointed that they chose not to adjust their score to reflect their assessment of the paper, but rather their displeasure at the length of the paper (which, as stated above, does push the boundary a little).  Overall, I recommend accepting this paper, but warn the authors that this is a generous decision, heavily motivated by my appreciation for the work, and that they should be careful not to try such stunts in future conference in order to preserve the fairness of the submission process.
The paper is tackling an important open problem.  AnonReviewer3 identified some technical issues that led them to rate the manuscript 5 (i.e., just below the acceptance threshold). Many of these issues are resolved by the reviewer in their review, and the author response makes it clear that these fixes are indeed correct.  However, other issues that the reviewer raises are not provided with solutions.  The authors address these points, but in one case at least (regarding w_infinity), I find the new text somewhat hand waivy. Regardless, I m inclined to accept the paper because the issues seem to be straightforward. Ultimately, the authors are responsible for the correctness of the results.
The paper proposes a unique network architecture that can learn divide and conquer strategies to solve algorithmic tasks, mimicking a class of standard algorithms.  The paper is clearly written, and the experiments are diverse.  It also seems to point in the direction of a wider class of algorithm inspired neural net architectures.
The pros and cons of the paper can be summarized as follows:  Pros: * The method of combining together multiple information sources is effective * Experimental evaluation is thorough  Cons: * The method is a relatively minor contribution, combining together multiple existing methods to improve word embeddings. This also necessitates the model being at least as complicated as all the constituent models, which might be a barrier to practical applicability  As an auxiliary comment, the title and emphasis on computing embeddings "on the fly" is a bit puzzling. This is certainly not the first paper that is able to calculate word embeddings for unknown words (e.g. all the cited work on character based or dictionary based methods can do so as well). If the emphasis is calculating word embeddings just in time instead of ahead of time, then I would also expect an evaluation of the speed or memory requirements benefits of doing so. Perhaps a better title for the paper would be "integrating multiple information sources in training of word embeddings", or perhaps a more sexy paraphrase of the same.  Overall, the method seems to be solid, but the paper was pushed out by other submissions. 
Dear authors,  While the reviewers appreciated your analysis, they all expressed concerns about the significance of the paper. Indeed, given the plethora of GAN variants, it would have been good to get stronger evidence about the advantages of the Dudley GAN. Even though I agree it is difficult to provide a clean comparison between generative models because of the lack of clear objectives, the LL on one dataset and images generated is limited. For instance, it would have been nice to show robustness results as this is a clear issue with GANs.
Novel way of analyzing neural networks to predict NN attributes such as architecture, training method, batch size etc. And the method works surprisingly good on the MNIST and ImageNet.
The experiments are not sufficient to support the claim. The authors plan to improve it for future publication.
there are two separate ideas embedded in this submission; (1) language modelling (with the negative sampling objective by mikolov et al.) is a good objective to use for extracting document representation, and (2) CNN is a faster alternative to RNN s, both of which have been studied in similar contexts earlier (e.g., paragraph vectors, CNN classifiers and so on, most of which were pointed out by the reviewers already.) Unfortunately reading this manuscript does not reveal too clearly how these two ideas connect to each other (and are separate from each other) and are related to earlier approaches, which were again pointed out by the reviewers. in summary, i believe this manuscript requires more work to be accepted.
First off, this was a difficult paper to decide on. There was some vigorous discussion on the paper centering around the choices that were available to the conv nets.  The author s strongly emphasized the improvements on the PTB task.  For my part, I think the method is very compelling   sharing weights for all the models we are optimizing on seems like a great idea   and that we can make it work is even more interesting. So from this point of view, I think its a novel contribution and worth accepting.  On the other hand, I m likely to agree with some of the motivations behind the questions raised by R3. Are all the choices really necessary ? perhaps the gains came from just a couple of things like number of skip connections and channels, etc. That exploration is useful. On the flip side, I think it may be an irrelevant question   the model is able to make the correct decisions from a big set.  The authors emphasize the language modelling part, but for me, this was actually less compelling. The authors use some of the tricks from Merity in their model training (perplexity 52.8), and as a result are already using some techniques that produces better results. Further, PTB is a regularization game   and that s not really the claim of this paper. Although, one could argue that weight sharing between different models can produce an ensembling / regularization effect and those gains may show up on PTB. A much more compelling story would have been to show that this method works on a large dataset where the impact of the architecture cannot be conflated with controlling overfitting better.  As a result, this puts the paper on the fence for me; even though I very much like the idea. Polishing the paper and making a more convincing case for both the CNNs and RNNs will make this paper a solid contribution in the future.
The paper studies a defense against adversarial examples that re trains convolutional networks on adversarial examples constructed to attack pre trained networks. Whilst the proposed approach is not very original, the paper does present a solid empirical baseline for these kinds of defenses. In particular, it goes beyond the "toy" experiments that most other studies in this space perform by experimenting on ImageNet. This is important as there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to ImageNet. The importance of the baseline method studied in this paper is underlined by its frequent application in the recent NIPS competition on adversarial examples.
Training GANs to generate discrete data is a hard problem. This paper introduces a principled approach to it that uses importance sampling to estimate the gradient of the generator. The quantitative results, though minimal, appear promising and the generated samples look fine. The writing is clear, if unnecessarily heavy on mathematical notation.
A novel dual memory system inspired by brain for the important incremental learning and very good results.
The paper is well written overall. However, the algorithmic framework has limited novelty and the reviewers unanimously are unconvinced by experimental results showing marginal improvements on smallish UCI datasets.
Pros: + Clear, well written paper that tackles an interesting problem. + Interesting potential connections to other approaches in the literature such as Carreira Perpi√±√°n and Wang, 2014 and Taylor et al., 2016. + Paper shows good understanding of the literature, has serious experiments, and does not overstate the results.  Cons:   Theory only addresses gradient descent, not stochastic gradient descent.   Because the optimization process is similar to BFGS, it would make sense to have an empirical comparison against some second order method, even though the proposed algorithm is more like standard backpropagation.  This paper is a nice first step in an interesting direction, and belongs in ICLR if there is sufficient space. 
Pros:    The paper proposes interesting new ideas on evaluating generative models.    Paper provides hints at interesting links between structural prediction and adversarial learning.    Authors propose a new dataset called Thin 8 to demonstrate the new ideas and argue that it is useful in general to study generative models.    The paper is well written and the authors have made a good attempt to update the paper after reviewer comments.  Cons:   The proposed ideas are high level and the paper lack deeper analysis.   Apart from demonstrating that the parametric divergences perform better than non parametric divergences are interesting, but the reviewers think that practical importance of the results are weak in comparison to previous works. With this analysis, the committee recommends this paper for workshop.
The key motivation for the work is producing both an efficient (parallelizable / fast) and accurate reading comprehension model. At least two reviewers are not convinced that this goal is really achieved (e.g., no comparison to hierarchical modeling, performance is not as strong).   I also share concerns of R1 that, without proper ablation search and more careful architecture choice, the modeling decisions seem somewhat arbitrary.  + the goal (of achieving effective reading comprehesion models) is important   alternative parallelization techniques (e.g., hierarchical modeling) are not considered   ablation studies / more systematic architecture search are missing   it is not clear that the drop in accuracy can be justified by the potential efficiency gains (also see details in R3  > no author response to them) 
PROS: 1. Good results on CLEVER datasets 2. Writing is clear 3. The MAC unit is novel and interesting. 4. Ablation experiments are helpful  CONS: The authors overstate the degree to which they are doing "sound" and "transparent" reasoning.  In particular statements such as "Most neural networks are essentially very large correlation engines that will hone in on any statistical, potentially spurious pattern that allows them to model the observed data more accurately. In contrast, we seek to create a model structure that requires combining sound inference steps to solve a problem instance." I think are not supported.  As far as I can tell, the authors  do not show that the steps of these solutions are really doing inference in any sound way  I also found the interpretability section to be a bit unconvincing.  The reviewers and I discussed this and there was some attempt to assess what the operations were actually doing but it is not clear how the language and the image attention are linked.  I wonder whether the learned control activations are abstract and re used across problems the way that the accompanying functional solution s primitives are.  Have you looked at how similar the controls are across problems which are identical except for a different choice of attributes?  To me, one of the hallmarks of a truly "compositional" solution is one in which the pieces are re used across problems, not just that there is some sequence of explicit control activations used to solve each individual problem.
The proposed conditional variance regularizer looks interesting and the results show some promise. However, as the reviewers pointed out, the connection between the information theoretic argument provided and the final form of the regularizer is too tenuous in its current form. Since this argument is central to the paper, the authors are urged to either provide a more rigorous derivation or motivate the regularizer more directly and place more emphasis on its empirical evaluation.
This paper aims to improve on the intrinsically motivated goal exploration framework by additionally incorporating representation learning for the space of goals. The paper is well motivated and follows a significant direction of research, as agreed by all reviewers. In particular, it provides a means for learning in complex environments, where manually designed goal spaces would not be available in practice. There had been significant concerns over the presentation of the paper, but the authors put great effort in improving the manuscript according to the reviewers‚Äô suggestions, raising the average rating by 2 points after the rebuttal. 
This work presents a strong baseline model for several NLP ish tasks such as document classification, sentence classification, representation learning based NLI, and text matching. In terms of originality, reviewers found that "there is not much contribution in terms of technical novelty" but that "one might also conclude that we need more challenging dataset". There was significant discussion about whether it "sheds new lights on limitations of existing methods" or whether the results were "marginally surprising". In terms of quality, reviewers found it to be an "insightful analysis" and noted that these "SWEMs should be considered a strong baseline in future work".  There was significant discussion with the AC about the signficance of the work. In the opinion of the AC reviewers did were too quick to accept the authors novelty claims, and did not push them enough to include other baselines in their tables that were not overly deep model. In particular the AC felt that important numbers were left out of the experiment tables, for document classification that muddied the results. The response of the authors was:  "Moreover, fasttext and our SWEM variants all belong to the category of simpler methods (with parameter free compositional functions). Since our motivation is to explore the necessity of employing complicated compositional functions for various NLP tasks, we do not think it is necessary for us to make any comparisons between fasttext and SWEM."  In addition when a reviewer pointed out the lack of inclusion of FOFE embeddings, the authors noted something similar  "Besides, we totally agree that developing sentence embeddings that are both simple and efficient is a very promising research direction (FOFE is a great work along this line)."  The reviewer correctly pointed out related work that shows a model very similar to what the author s propose. In general this seems like evidence that the techniques are known, not that they are significant and novel. 
This work proposes an improved visualisation techniques for ReLU networks that compensates for filter scale symmetries/invariances, thus allowing a more meaningful comparison of low dimensional projected optimization landscapes between different network architectures.    the visualisation techniques are a small variation over previous works + extensive experiments provide nice visualisations and yield a clearer visual picture of some properties of the optimization landscape of various architectural variants  A promising research direction, which could be further improved by providing more extensive and convincing support for the significance of its contribution in comparison to prior techniques, and to its empirically derived observations, findings and claims. 
This paper is nowhere near standards for publication anywhere.
Three reviewers recommended rejection, and there was no rebuttal.
The paper proposes a new analysis of the optimization method called entropy sgd which seemingly leads to more robust neural network classifiers. This is a very important problem if successful. The reviewers are on the fence with this paper. On the one hand they appreciate the direction and theoretical contribution, while on the other they feel the assumptions are not clearly elucidated or justified. This is important for such a paper. The author responses have not helped in alleviating these concerns. As one of the reviewers points out, the writing needs a massive overhaul. I would suggest the authors clearly state their assumptions and corresponding justifications in future submissions of this work.
This paper address the increasingly studied problem of predictions over long term horizons. Despite this, and the important updates from the authors, the paper is not yeat ready and improvements identified include more control over the fair comparisons, improved clarity in exposition.
Dear authors,  Despite the desirable goal, that is to move away from regularization in parameter space toward regularization in function space, the reviewers all thought that the paper was not convincing enough, both in the choice of the particular regularization and in the experimental section.  While I appreciate that you have done a major rework of the paper, the rebuttal period should not be used for that and we can not expect the reviewers to do a complete re review of a new version.  This paper thus cannot be accepted to ICLR.
This paper is interesting since it goes to showing the role of model averaging. The clarifications made improve the paper, but the impact of the paper is still not realised: the common confusion on the retraining can be re examined, clarifications in the methodology and evaluation, and deeper contextulaisation of the wider literature.
Thank you for submitting you paper to ICLR. ICLR. The consensus from the reviewers is that this is not quite ready for publication.
The paper proposes to use multiple discriminators to stabilize the GAN training process. Additionally, the discriminators only see randomly projected real and generated samples.  Some valid concerns raised by the reviewers which makes the paper weak:    Multiple discriminators have been tried before and the authors do not clearly show experimentally / theoretically if the random projection is adding any value.   Authors compare only with DCGAN and the results are mostly subjective. How much improvement the proposed approach provides when compared to other GAN models that are developed with stability as the main goal is hence not clear.
This paper investigates emergence of language from raw pixels in a two agent setting. The paper received divergent reviews, 3,6,9. Two ACs discussed this paper, due to a strong opinion from both positive and negative reviewers. The ACs agree that the score "9" is too high: the notion of compositionality is used in many places in the paper (and even in the title), but never explicitly defined. Furthermore, the zero shot evaluation is somewhat disappointing. If the grammar extracted by the authors in sec. 3.2 did indeed indicate the compositional nature of the emergent communication, the authors should have shown that they could in fact build a message themselves, give it to the listener with an image and ask it to answer. On the other hand, "3" is also too low of a score. In this renaissance of emergent communication protocol with multi agent deep learning systems, one missing piece has been an effort toward seriously analyzing the actual properties of the emergent communication protocol.  This is one of the few papers that have tackled this aspect more carefully. The ACs decided to accept the paper. However, the authors should take the reviews and comments seriously when revising the paper for the camera ready.
The paper received borderline negative scores (6,5,5) with R1 and R2 having significant difficulty with the clarity of the paper. Although R3 was marginally positive, they pointed out that the experiments are "extremely weak". The AC look at the paper and agrees with R3 on this point. Therefore the paper cannot be accepted in its current form. The experiments and clarity need work before resubmission to another venue.  
this submission presents the positive impact of using orthogonal random features instead of unstructured random features for predictive state recurrent neural nets. there s been some sentiment by the reviewers that the contribution is rather limited, but after further discussion with another AC and PC s, we have concluded that it may be limited but a solid follow up on the previous work on predictive state RNN. 
The problem of discovering ordering in an unordered dataset is quite interesting, and the authors have outlined a few potential applications. However, the reviewer consensus is that this draft is too preliminary for acceptance. The main issues were clarity, lack of quantitative results for the order discovery experiments, and missing references. The authors have not yet addressed these issues with a new draft, and therefore the reviewers have not changed their opinions.
The general consensus is that this method provides a practical and interesting approach to unsupervised domain adaptation. One reviewer had concerns with comparing to state of the art baselines, but those have been addressed in the revision.  There were also issues concerning correctness due to a typo. Based on the responses, and on the pseudocode, it seems like there wasn t an issue with the results, just in the way the entropy objective was reported.  You may want to consider reporting the example given by reviewer 2 as a negative example where you expect the method to fail. This will be helpful for researchers using and building on your paper.
The pros and cons of the paper cited by the reviewers can be summarized as follows:  Pros:   good problem, NL2SQL is an important task given how dominant SQL is   incorporating a grammar ("sketch") is a sensible improvement.  Cons:   The dataset used makes very strong simplification assumptions (that every token is an SQL keyword or appears in the NL)   The use of a grammar in the context of semantic parsing is not novel, and no empirical comparison is made against other reasonable recent baselines that do so (e.g. Rabinovich et al. 2017).  Overall, the paper seems to do some engineering for the task of generating SQL, but without an empirical comparison to other general purpose architectures that incorporate grammars in a similar way, the results seem incomplete, and thus I cannot recommend that the paper be accepted at this time.
I fully agree with strong positive statements in the reviews.  All reviewers agree that the paper introduces a novel and elegant twist on standard RL, wherein one agent proposes a sequence of diverse tasks to a second agent so as to accelerate the second agent s learning models of the environment.  I also concur that the empirical testing of this method is quite good.  There are strong and/or promising results in five different domains (Hallway, LightKey, MountainCar, Swimmer Gather and TrainingMarines in StartCraft). This paper would make for a strong poster at ICLR.
 I am going to recommend acceptance of this paper despite being worried about the issues raised by reviewer 1.  In particular,  1:  the best possible inception score would be obtained by copying the training dataset 2:  the highest visual quality samples would be obtained by copying the training dataset 3:  perturbations (in the hidden space of a convnet) of training data might not be perturbations in l2, and so one might not find a close nearest neighbor with an l2 search 4:  it has been demonstrated in other works that perturbations of convnet features of training data (e.g. trained as auto encoders) can make convincing "new samples"; or more generally, paths between nearby samples in the hidden space of a convnet can be convincing new samples.  These together suggest the possibility that the method presented is not necessarily doing a great job as a generative model or as a density model (it may be, we just can t tell...), but it is doing a good job at hacking the metrics (inception score, visual quality).      This is not an issue with only this paper, and I do not want to punish the authors of this papers for the failings of the field; but this work, especially because of its explicit use of training examples in the memory,  nicely exposes the deficiencies in our community s methodology for evaluating GANs and other generative models.  
The reviewers were unanimous in their assessment that the paper was not ready for publication in ICLR.  Their concerns included:    lack of novelty over Niepert, Ahmed, Kutzkov, ICML 2016    The approach learns combinations of graph kernels and its expressive capacity is thus limited    The results are close to the state of the art and it is not clear whether any improvement is statistically significant.  The authors have not provided a response to these concerns.
This paper adapts (Nachum et al 2017) to continuous control via TRPO.   The work is incremental (not in the dirty sense of the word popular amongst researchers, but rather in the sense of "building atop a closely related work"), nontrivial,  and shows empirical promise.    The reviewers would like more exploration of the sensitivity of the hyper parameters.
There was substantial disagreement between reviewers on how this paper contributes to the literature; it seems (having read the paper) that the problem tackled here is clearly quite interesting, but it is hard to tease out in the current version exactly what the contribution does to extend beyond current art.
This paper analyzes mathematically why weights of trained networks can be replaced with ternary weights without much loss in accuracy. Understanding this is an important problem, as binary or ternary weights can be much more efficient on limited hardware, and we ve seen much empirical success of binarization schemes. This paper shows that the continuous angles and dot products are well approximated in the discretized network. The paper concludes with an input rotation trick to fix discretization failures in the first layer.  Overall, the contribution seems substantial, and the reviewers haven t found any significant issues. One reviewer wasn t convinced of the problem s importance, but I disagree here. I think the paper will plausibly be helpful for guiding architectural and algorithmic decisions. I recommend acceptance. 
Given the changes to the paper, the reviewers agree that the paper meets the bar for publication at ICLR. There are some concerns regarding the practical impact on CPUs and GPUs. I ask the authors to clearly discuss the impact on different hardware. One can argue if adaptive quantization techniques are helpful, then there is a chance that future hardware will support them. All of the experiments are conducted on toy datasets. Please consider including some experiments on Imagenet as well.
The experimental work in this paper leaves it just short of being suitable for acceptance. The work needs more comparisons with prior work and other approaches. The numerical ratings of the work by reviewers are just too low. 
The reviewers all outlined concerns regarding novelty and the maturity of this work. It would be helpful to clarify the relation to doubly stochastic kernel machines as opposed to random kitchen sinks, and to provide more insight into how this stochasticity helps. Finally, the approach should be tried on more difficult image datasets.
The reviewers are unanimous that this is an interesting paper, but that ultimately the empirical results are not sufficiently promising to warrant the added complexity.
I recommend acceptance based on the reviews. The paper makes novel contributions to learning one hidden layer neural networks and designing new objective function with no bad local optima.   There is one point that the paper is missing. It only mentions Janzamin et al in the passing. Janzamin et al propose using score function framework for designing alternative objective function. For the case of Gaussian input that this paper considers, the score function reduces to Hermite polynomials. Lack of discussion about this connection is weird. There should be proper acknowledgement of prior work. Also missing are some of the key papers on tensor decomposition and its analysis  I think there are enough contributions in the paper for acceptance irrespective of the above aspect.  
Dear authors,  Thank you for your submission to ICLR. Sadly, the reviewers were not convinced by the novelty of your approach nor by its experimental results. Thus, your paper cannot be accepted to ICLR.
This paper introduces a possibly useful new RL idea (though it s a incremental on Liang et al), but the evaluations don t say much about why it works (when it does), and we didn t find the target application convincing. 
The paper proposes a modification to the Transformer network, which mostly consists in changing how the attention heads are combined. The contribution is incremental, and its novelty is limited. The results demonstrate an improvement over the baseline at the cost of a more complicated training procedure with more hyper parameters, and it is possible that with similar tuning the baseline performance could be improved in a similar way.
The paper analyzes neural network with hidden layer of piecewise linear units, a single output, and a quadratic loss. The reviewers find the results incremental and not "surprising", and also complained about comparison with previous work. I think the topic is very pertinent, and definitely more relevant compared to studying multi layer linear networks. Hence, I recommend the paper be presented in the workshop track.
The paper proposes a method to generate adversaries close to the (training) data manifold using GANs rather than arbitrary adversaries. They show the effectiveness of their method in terms of human evaluation and success in fooling a deep network. The reviewers feel that this paper is for the most part well written and the contribution just about makes the mark.
Thank you for submitting you paper to ICLR. The reviewers are all in agreement that the paper is suitable for publication, each revising their score upwards in response to the revision that has made the paper stronger.  The authors may want to consider adding a discussion about whether the simple standard Gaussian prior, which is invariant under transformation by an orthogonal matrix, is a sensible one if the objective is to find disentangled representations. Alternatives, such as sparse priors, might be more sensible if a model based solution to this problem is sought.
All of the reviewers agree that the paper presents strong experimental results on continuous control benchmarks. The reviewers raised concerns regarding the analysis of the behavior of the algorithm, the possible impact of the technique, and requested more references and comparison with related work. The paper has significantly improved since the initial submission, but still not able fully satisfactory to the reviewers, partly due to the large extent of the changes needed. 
Pros: + Interesting alternative algorithm for training autoencoders  Cons:   Not a lot of practical value because DANTE does not outperform SGD in terms of time or classification performance using autoencoder features.  This is an interesting and well written paper that doesn t quite meet the threshold for ICLR acceptance. If the authors can find use cases where DANTE has demonstrable advantages over competing training algorithms, I expect the paper would be accepted. 
Observing that in contrast to classical information bottleneck, the deep variational information bottleneck (DVIB) model is not invariant to monotonic transformations of input and output marginals, the authors show how to incorporate this invariance along with sparsity in DVIB using the copula transform. The revised version of the paper addressed some of the reviewer concerns about clarity as well as the strength of the experimental section, but the authors are encouraged to improve these aspects of the paper further.
Area chair is in agreement with reviewers: this is a good experiment that successfully applies specific machine learning techniques to the particular task. However, the authors have not discussed or studied the breadth of other possible methods that could also solve the given task ... besides those mentioned by the reviewers, U Nets, and variants thereof, come to mind. Without these comparisons, the novelty and significance cannot be assessed.  Authors are encouraged to study similar works, and perform a comparison among multiple possible approaches, before submission to another venue.  
this submission proposes a learning algorithm for resnets based on their interpreration of them as a discrete approximation to a continuous time dynamical system.  all the reviewers have found the submission to be clearly written, well motivated and have proposed an interesting and effective learning algorithm for resnets.
This paper presents a novel model for generating images and natural language descriptions simultaneously. The aim is to distangle representations learned for image generation by connecting them to the paired text. The reviews praise the problem setup and the mathematical formulation. However they point out significant issues with the clarity of the presentation in particular the diagrams, citations, and optimization procedure in general. They also point out issues with the experimental setup in terms of datasets used and lack of natural images for the tasks in question.  Reviews are impressively thorough and should be of use for a future submission. 
The reviewers agree that the problem being addressed is interesting, however there are concerns with novelty and with the experimental results. An experiment beyond dealing with class imbalance would help strengthen this paper, as would experiments with other kinds of GANs.
This paper presents a new pipeline for nn compression that extends that of Han et. al, but show that it reduces parameters further, maintains higher accuracy and can be applied to methods behind classification (semantic segmentation). While the authors found the paper clearly written, excepting for some typos, and potentially useful, there were questions about originality, and significance.    Reviewers were not completely convinced the method was different enough from deep compression: "The overall pipeline including the last two stage looks quite similar to Han[1].", or that enough focus was paid to the differences inherent with classification focused work: "The paper in the title and abstract refers to segmentation as the main area of focus. However, there does not seem to be much related to it except an experiment on the CityScapes dataset."    In terms of impact, the additional benefits from pruning seem to require a significant amount of computation, and the reviewers were not convinced these were worth a small gain in compression. Furthermore, authors felt that this approach was not being applied to the most state of the art approaches to demonstrate their use.
The scores were not favorable: 5,5,2. R2 felt the motivation of the paper was inadequate. R3 raised numerous technical points, some of which were addressed in the rebuttal, but not all. R3 continues to have issue with some of the results. The AC agrees with R3 s concerns and feels that the paper cannot be accepted in its current form. 
This paper proposes an approach for predicting transcription factor (TF) binding sites and TF TF interaction.  The approach is interesting and may ultimately be valuable for the intended application.   But in its current state, the paper has insufficient technical novelty (e.g. relative to matching networks of Vinyals 2016), insufficient comparisons with prior work, and unclear benefit of the approach.  The reviewers also had some concerns about clarity.  
An interesting paper, generally well written. Though it would be nice to see that the methods and observations generalize to other datasets, it is probably too much to ask as datasets with required properties do not seem to exist.  There is a clear consensus to accept the paper.  + an interesting extension of previous work on emergent communications (e.g., referential games) + well written paper  
Pros: + The paper is very clearly written. + The proposed re embedding approach is easily implemented and can be integrated into fancier architectures.  Cons:   A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (e.g., ESIM with full training data and full word vectors).  This paper is rather close to the decision boundary. The authors had reasonable answers for some of the reviewers  concerns, but in the end the reviewers were not completely convinced. 
This is a nicely written paper proposing a reasonably interesting extension to existing work (e.g. VPN). While the Atari results are not particular convincing, they do show promise. I encourage the authors to carefully take the reviewers  comment into consideration and incorporate them to the final version.
Dear authors,  The reviewers appreciated your work and recognized the importance of theoretical work to understand the behaviour of deep nets. That said, the improvement over existing work (especially Montufar, 2017) is minor. This, combined with the limited attraction of such work, means that the paper will not be accepted.  I acknowledge the major modifications done but it is up to the reviewers to decide whether or not they agree to re review a significantly updated version.
The paper proposes a way to find why a classifier misclassified a certain instance. It tries to find pertubations in the input space to identify the appropriate reasons for the misclassification. The reviewers feel that the idea is interesting, however, it is insufficiently evaluated.  Even for the datasets they do evaluate not enough examples of success are provided. In fact, for CelebA the results are far from flattering.
The original paper was sloppy in its use of mathematical constructs such as manifolds, made assumptions that are poorly motivated (see review #2 for details), and presented an empirical evaluation is preliminary. Based on the reviews, the authors have substantially revised the paper to try and address those issues by adding new theory, etc.  Unfortunately, it is difficult to assess whether these revisions are sufficient to address the aforementioned issues without going through a second round of "full" review. I encourage the authors to use the reviewer comments to further improve the paper, and re submit to a different venue.
Initially this paper received mixed reviews. After reading the author response, R1 and and R3 recommend acceptance.  R2, who recommended rejecting the paper, did not participate in discussions, did not respond to author explanations, did not respond to AC emails, and did not submit a final recommendation. This AC does not agree with the concerns raised by R2 (e.g. I don t find this model to be unprincipled).  The concerns raised by R1 and R3 were important (especially e.g. comparisons to NMS) and the authors have done a good job adding the required experiments and providing explanations.  Please update the manuscript incorporating all feedback received here, including comparisons reported to the concurrent ICLR submission on counting. 
This paper proposes to pre train a feature embedding, using Siamese networks, for use with few shot learning for SVMs.  The idea is not very novel since there is a fairly large body of work in the general setting of pre trained features + simple predictor.  In addition, the experimental results could be stronger   there are stronger results in the literature (not cited), and better data sets for testing few shot learning.
Biological memory systems are grounded in spatial representation and spatial memory, so neural methods for spatial memory are highly interesting. The proposed method is novel, well designed and the empirical results are good on unseen environments, although the noise model may be too weak. Moreover, it would have been great to evaluate this method on real data rather than in simulation. 
Pros   Extends embeddings to use a richer representation; simple yet interesting improvement on Mikolov et al. work. Cons   All of the reviewers pointed out that the experimental evaluations needs improvement. The authors should find better ways to improve both quantitative (e.g., accuracy in analogies as in Mikolov et al., or by using the model for an external task if that‚Äôs the end goal) and qualitative (using functional similarity for the baseline) evaluations.  Given these comments, the AC recommends that the paper be rejected. 
Overall the reviewers appear to like the ideas in this paper, though this is some disagreement about novelty (I agree with the reviewer who believes that the top level search can very easily be interpreted as an MDP, making this very similar to SMDPs). The reviewers generally felt that the experimental results need to more closely compare with some existing techniques, even if they re not exactly for the same setting.
The novelty of the paper is limited and it lacks on comparisons with relevant baselines, as pointed out by the reviewers. 
An interesting application of self ensembling/temporal ensembling for visual domain adaptation that achieves state of the art on the visual domain adaptation challenge. Reviewers noted that the approach is quite engineering heavy, but I am not sure it s really much worse than making a pixel to pixel approach work well for domain adaptation.  I hope the authors follow through with their promise to add experiments to the final version (notably the minimal augmentation experiments to show just how much this domain adaptation technique is tailored towards imagenet like things).  As it stands, this paper would be a good contribution to ICLR as it shows an efficient and interesting way to solve a particular visual domain adaptation problem.
This paper presents a way to generate adversarial examples for text classification.   The method is simple   finding semantically similar words and replacing them in sentences with high language model score.  The committee identifies weaknesses in this paper that resonate with the reviews below   reviewer 1 suggests that the authors should closely compare with the work of Papernot et al, and the response to that suggestion is not satisfactory.  Addressing such concerns would make the paper stronger for a future venue.
this submission proposes an efficient parametrization of a recurrent neural net by using two transition functions (one large and one small) to reduce the amount of computation (though, without actual improvement on GPU.) the reviewers found the submission very positive.  please, do not forget to include all the result and discussion on the proposed approach s relationship to VCRNN which was presented at the same conference just a year ago.
Thank you for submitting you paper to ICLR. The consensus from the reviewers is that this is not quite ready for publication. The work is related to (although different from) Gu et al Neural Sequential Monte Carlo NIPS2015 and it would be useful to point this out in the related work section.
Thank you for submitting you paper to ICLR. The paper presents a general approach for handling inference in probabilistic graphical models that employ deep neural networks. The framework extends Jonhson et al. (2016) and Khan & Lin (2017). The reviewers are all in agreement that the paper is suitable for publication. The paper is well written and the use of examples to illustrate the applicability of the methods brings great clarity. The experiments are not the strongest suit of the paper and, although the revision has improved this aspect, I would encourage a more comprehensive evaluation of the proposed methods. Nevertheless, this is a strong paper.
Overall, the paper is missing a couple of ingredients that would put it over the bar for acceptance:    I am mystified by statements such as "RL2 no longer gets the best final performance." from one revision to another, as I have lower confidence in the results now.    More importantly, the paper is missing comparisons of the proposed methods on *already existing* benchmarks. I agree with Reviewer 1 that a paper that only compares on benchmarks introduced in the very same submission is not as strong as it could be.  In general, the idea seems interesting and compelling enough (at least on the Krazy World & maze environments) that I can recommend inviting to the workshop track.
Paper proposes adding randomization steps during inference time to CNNs in order to defend against adversarial attacks.  Pros:    Results demonstrate good performance, and the team achieve a high rank (2nd place) on a public benchmark.   The benefit of the proposed approach is that it does not require any additional training or retraining.  Cons:    The approach is very simple, common sense would tend to suggest that adding noise to images would make adversarial attempts more difficult. Though perhaps simplicity is a good thing.   Update: Paper does not cite related and relevant work, which takes a similar approach of requiring no retraining, but rather changing the inference stage: https://arxiv.org/pdf/1709.05583.pdf ‚Ä®‚Ä® Grammatical Suggestions:  This paper would benefit from polishing. For example:    Abstract: sentence 1: replace ‚Äútheir powerful ability‚Äù to ‚Äúhigh accuracy‚Äù   Abstract: sentence 3: replace ‚ÄúI.e., clean images‚Ä¶‚Äù with ‚ÄúFor example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail‚Äù   Abstract: sentence 4: replace ‚Äúutilize randomization‚Äù to ‚Äúimplement randomization at inference time‚Äù or something similar to make more clear that this procedure is not done during training.   Abstract: sentence 7: replace ‚Äúalso enjoys‚Äù with ‚Äúprovides‚Äù  Main Text: Capitalize references to figures (i.e. ‚Äúfigure 1‚Äù to ‚ÄúFigure 1‚Äù).  Introduction: Paragraph 4: Again, please replace ‚Äúrandomization‚Äù with ‚Äúrandomization at inference time‚Äù or something similar to better address reviewer concerns. 
This paper asks when SGD+M can beat adaptive methods such as Adam, and then suggests a variant of SGD+M with an adaptive controller for a single learning rate and momentum parameter.  There is are comparisons with some popular alternatives.  However, the bulk of the paper is concerned with a motivation that didn t convince any of the reviewers.
This paper studies the problem of synthesizing adversarial examples that will succeed at fooling a classification system under unknown viewpoint, lighting, etc conditions. For that purpose, the authors propose a data augmentation technique (called "EOT") that makes adversarial examples robust against a predetermined family of transformations.  Reviewers were mixed in their assessment of this work, on the one hand highlighting the potential practical applications, but on the other hand warning about weak comparisons with existing literature, as well as lack of discussion about how to improve the robustness of the deep neural net against that form of attacks. The AC thus believes this paper will greatly benefit from a further round of iteration/review, and therefore recommends rejection at this time. 
The paper got generally positive scores of 6,7,7. The reviewers found the paper to be novel but hard to understand. The AC feels the paper should be accepted but the authors should revise their paper to take into account the comments from the reviewers to improve clarity.
Though the approach is not terribly novel, it is quite effective (as confirmed on a wide range of evaluation tasks). The approach is simple and likely to be useful in applications. The paper is well written.  + simple and efficient + high quality evaluation + strong results   novelty is somewhat limited 
The paper attempts to develop a method for learning latent representations using deep predictive coding and deconvolutional networks. However, the theoretical motivation for the proposed model in relation to existing methods (such as original predictive coding, deconvolutional networks, ladder networks, etc.), as well as the empirical comparison against them is unclear. The experimental results on the CIFAR10 dataset do not provide much insight on what kind of meaningful/improved representations can be learned in comparison to existing methods, both qualitatively and quantitatively. No rebuttal was provided. 
The authors provide an alternative method to [1] for placement of ops in blocks. The results are shown to be an improvement over prior RL based placement in [1] and superior to *some* (maybe not the best) earlier methods for operations placements. The paper seems to have benefited strongly from reviewer feedback and seems like a reasonable contribution. We hope that the implementation may be made available to the community.  [1] Mirhoseini A, Pham H, Le Q V, et al. Device Placement Optimization with Reinforcement Learning[J]. arXiv preprint arXiv:1706.04972, 2017. 
This paper presents an unsupervised GAN based model for disentagling the multiple views of the data and their content.  Overall it seems that this paper was well received by the reviewers, who find it novel and significant . The consensus is that the results are promising.  There are some concerns, but the major ones listed below have been addressed in the rebuttal. Specifically:  	R3 had a concern about the experimental evaluation, which has been addressed in the rebuttal.  	R2 had a concern about a problem inherent in this setting (what is treated as ‚Äúcontent‚Äù), and the authors have clarified in the discussion the assumptions under which such methods operate.  	R1 had concerns related to how the proposed model fits in the literature. Again, the authors have addressed this concern adequately. 
This paper is well written, addresses and interesting problem, and provides an interesting solution.
meta score: 4 This paper concerns a variant to previous RNN architectures using temporal skip connections, with experimentation on the PTB language modelling task The reviewers all recommend that the paper is not ready for publication and thus should be rejected from ICLR.  The novelty of the paper and its relation to the state of the art is not clear.  The experimental validation is weak. Pros:    possibly interesting idea Cons:    weak experimental validation    weak connection to the state of the art    precise original contribution w.r.t state of the art is not clear 
The reviewers highlight a lack of technical content and poor writing. They all agree on rejection. There was no author rebuttal or pointer to a new version. 
This paper proposes an implicit model of graphs, trained adversarially using the Gumbel softmax trick.  The main idea of feeding random walks to the discriminator is interesting and novel.  However, 1) The task of generating  sibling graphs , for some sort of bootstrap analysis, isn t well motivated. 2) The method is complicated and presumably hard to tune, with two separate early stopping thresholds that need to be tuned 3) There is not even a mention of a large existing literature on generative models of graphs using variational autoencoders.
Viewing the problem of determining the validity of high dimensional discrete sequences as a sequential decision problem, the authors propose learning a Q function that indicates whether the current sequence prefix can lead to a valid sequence. The paper is fairly well written and contains several interesting ideas. The experimental results appear promising but would be considerably more informative if more baselines were included. In particular, it would be good to compare the proposed approach (both conceptually and empirically) to learning a generative model of sequences. Also, given that your method is based on learning a Q function, you need to explain its exact relationship to classic Q learning, which would also make for a good baseline.
The paper proposes a new approach for scalable training of deep topic models based on amortized inference for the local parameters and stochastic gradient MCMC for the global ones.  The key aspect of the method involves using Weibull  distributions (instead of Gammas) to model the variational posteriors over the local parameters, enabling the use of the reparameterization trick. The resulting methods perform slightly worse that the Gibbs sampling based approaches but are much faster at test time. Amortized inference has already been applied to topic models, but the use of Weibull posteriors proposed here appears novel. However, there seems to be no clear advantage to using stochastic gradient MCMC instead of vanilla SGD to infer the global parameters, so the value of this aspect of WHAI unclear.
The paper proposes to use absolute value activations, in a joint supervised + unsupervised training (classification + deep autoencoder with tied encoder/decoder weights). Pros:  + simple model and approach on ideas worth revisiting Cons:   The paper initially approached these old ideas as novel, missing much related prior work   It doesn t convincingly breathe novel insight into them.   Empirical methodology is not up to standards (non standard data split, lack of strong baselines for comparison)   Empirical validation is too limited in scope (MNIST only).
All three reviewers recommend acceptance. The authors did a good job at the rebuttal which swayed the first reviewer to increase the final rating. This is a clear accept.
The paper describes a production ready neural text to speech system. The algorithmic novelty is somewhat limited, as the fully convolutional sequence model with attention is based on the previous work. The main contribution of the paper is the description of the complete system in full detail. I would encourage the authors to expand on the evaluation part of the paper, and add more ablation studies.
This paper presents a toolbox for the exploration of layerwise parallel deep neural networks. The reviewers were consistent in their analysis of this paper: it provided an interesting class of models which warranted further investigation, and that the toolbox would be useful to those who are interested in exploring further. However, there was a lack of convincing examples, and also some concern that Theano (no longer maintained) was the only supported backend. The authors responded to say that they had subsequently incorporated TensorFlow support, they were not able to provide any more examples due to several reasons: ‚Äútime, pending IP concerns, open technical details, sufficient presentation quality, page restriction.‚Äù I agree with the consensus reached by the reviewers.
This paper attempts to decouple two factors underlying the success of GANs: the inductive bias of deep CNNs and adversarial training. It shows that, surprisingly, the second factor is not essential. R1 thought that comparisons to Generative Moment Matching Networks and Variational Autoencoders should be provided (note: this was added to a revised version of the paper). They also pointed out that the paper lacked comparisons to newer flavors of GANs. While R1 pointed out that the use of 128x128 and 64x64 images was weak, I tend to disagree as this is still common for many GAN papers. R2 was neutral to positive about the paper and thought that most importantly, the training procedure was novel. R3 also gave a neutral to positive review, claiming the paper was easy to follow and interesting. Like R1, R3 thought that a stronger claim could be made by using different datasets. In the rebuttal, the authors argued that the main point was not in proposing a state of the art generative model of images but to provide more an introspection on the success of GANs. Overall, I found the work interesting but felt that the paper could go through one more review/revision cycle. In particular, it was very long. Without a champion, this paper did not make the cut.
Thank you for submitting you paper to ICLR. Two of the reviewers are concerned that the paper‚Äôs contributions are not significant enough ‚Äîeither in terms of the theoretical or experimental contribution   to warrant publication. The authors have improved the experimental aspect to include a more comprehensive comparison, but this has not moved the reviewers. 
The paper presents self training scheme for GANs. The proposed idea is simple but reasonable, and the experimental results show promise for MNIST and CIFAR10. However, the novelty of the proposed method seems relatively small and experimental results lack comparison against other stronger baselines (e.g., state of the art semi supervised methods). Presentation needs to be improved. More comprehensive experiments on other datasets would also strengthen the future version of the paper. 
The presented work is a good attempt to expand the work of Li and Malik to the high dimensional, stochastic setting. Given the reviewer comments, I think the paper would benefit from highlighting the comparatively novel aspects, and in particular doing so earlier in the paper.  It is very important, given the nature of this work, to articulate how the hyperparameters of the learned optimizers, and of the hand engineered optimizers are chosen. It is also important to ensure that the amount of time spent on each is roughly equal in order to facilitate an apples to apples comparison.  The chosen architectures are still quite small compared to today s standards. It would be informative to see how the learned optimizers compare on realistic architectures, at least to see the performance gap.  Please clarify the objective being optimized, and it would be useful to report test error.  The approach is interesting, but does not yet meet the threshold required for acceptance.
The authors investigate various class aware GANs and provide extensive analysis of their ability to address mode collapse and sample quality issues. Based on this analysis they propose an extension called Activation Maximization GAN which tries to push each generated sample to a specific class indicated by the Discriminator. As experiments show, this leads to better sample quality & helps with mode collapse issue. The authors also analyze inception score to measure sample quality and propose a new metric better suited for this task.
All reviewers agreed that, despite the lack of novelty, the proposed method is sound and correctly linked to existing work. As the topic of automatically learning the stepsize is of great practical interest, I am glad to have this paper presented as a poster at ICLR.
Pros:   The paper proposes to use a hierarchical structure to address reconstruction issues with ALI model.   Obtaining multiple latent representations that individually achieve a different level of reconstructions is interesting.     Paper is well written and the authors made a reasonable attempt to improve the paper during the rebuttal period.   Cons:   Reviewers agree that the approach lacks novelty as similar hierarchical approaches have been proposed before.    The main goal of the paper to achieve better reconstruction in comparison to ALI without changing the latter s objective seems narrow. More analysis is needed to demonstrate that the approach out performs other approaches that directly tackle this problem in ALI.   The paper does not provide strong arguments as to why hierarchy works (limited to 2 levels in the empirical analysis presented in the paper).    Semi supervised learning as a down stream task is impressive but limited to MNSIT.  
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * The paper is a first attempt to investigate an under studied area in neural MT (and potentially other applications of sequence to sequence models as well) * This area might have a large impact; existing models such as Google Translate fail badly on the inputs described here * Experiments are very carefully designed and thorough * Experiments on not only synthetic but also natural noise add significant reliability to the results * Paper is well written and easy to follow  Cons: * There may be better architectures for this problem than the ones proposed here * Even the natural noise is not entirely natural, e.g. artificially constrained to exist within words * Paper is not a perfect fit to ICLR (although ICLR is attempting to cast a wide net, so this alone is not a critical criticism of the paper)  This paper had uniformly positive reviews and has potential for large real world impact.
The paper proposes a novel method for conditional image generation which is based on nearest neighbor matching for transferring high frequency statistics. The evaluation is carried out on several image synthesis tasks, where the technique is shown to perform better than an adversarial baseline.
The paper identifies an interesting problem in sigmoid deep nets, addressed diffferently by batchnorm, and proposes a different simple fix. It shows empirically that constraining neuron s weights to sum to zero improves training of a 100 layers sigmoid MLP. The work is currenlty limited in its theoretical contribution, and regarding the showcased practical interest of the method compared to batchnorm (it s not appplicable to RELUs and shows positive effect on optimization but not generalization).  
The idea of extending deep nets to infinite dimensional inputs is interesting but, as the reviewers noted, the execution does not have the quality we can expect from an ICLR publication. I encourage the authors to consider the meaningful comments that were made and modify the paper accordingly.
The proposed routing networks using RL to automatically learn the optimal network architecture is very interesting. Solid experimental justification and comparisons. The authors also addressed reviewers  concerns on presentation clarity in revisions.
All of the reviewers find the approach interesting, but they have reservations regarding the practical impact and empirical evaluation. The paper needs improvement both on the motivation and on the experimental results by including more baseline methods and neural architectures. 
This paper seemingly joins a cohort of ICLR submissions which attempt to port mature concepts from physics to machine learning, make a complex and non trivial theoretical contribution, and fall short on the empirical front. The one aspect that sets this apart from its peers is that the reviewers agree that the theoretical contribution of this work is clear, interesting, and highly non trivial. While the experiment sections (MNIST!) is indubitably weak, when treating this as a primarily theoretical contribution, the reviewers (in particular 6 and 3) are happy to suggest that the paper is worth reading. Taking this into account, and discounting somewhat the short (and, by their own admission, uncertain) assessment of reviewer 5, I am leaning  towards pushing for the acceptance of this paper. At very least, it would be a shame not to accept it to the workshop track, as this is by far the strongest paper of this type submitted to this conference.
This paper presents a learned inference architecture which generalizes HMC. It defines a parameterized family of MCMC transition operators which share the volume preserving structure of HMC updates, which allows the acceptance ratio to be computed efficiently. Experiments show that the learned operators are able to mix significantly faster on some simple toy examples, and evidence is presented that it can improve posterior inference for a deep latent variable model. This paper has not quite demonstrated usefulness of the method, but it is still a good proof of concept for adaptive extensions of HMC.  
The reviewers are unanimous in their opinion that the theoretical results in this paper are of limited novelty and significance. Several parts of the paper are not presented clearly enough. As such the paper is not ready for ICLR 2018 acceptance.
The paper has some interesting ideas around auto regressive policies and estimating their entropy for exploration. The use of autoregressive policies in RL is not particularly novel, and the estimate of entropy for such models is straightforward. Finally, the experiments focus on very simple tasks.
This work presents some of the first results on unsupervised neural machine translation. The group of reviewers is highly knowledgeable in machine translation, and they were generally very impressed by the results and the think it warrants a whole new area of research noting "the fact that this is possible at all is remarkable.". There were some concerns with the clarity of the details presented and how it might be reproduced, but it seems like much of this was cleared up in the discussion. The reviewers generally praise the thoroughness of the method, the experimental clarity, and use of ablations. One reviewer was less impressed, and felt more comparison should be done.
This paper explores what might be characterized as an adaptive form of ZoneOut. With the improvements and clarifications added to the paper during the rebuttal the paper could be accepted. 
The reviewer reactions to the initial manuscript were generally positive.  They considered the paper to be well written and clear, providing an original contribution to learning to cooperate in multi agent deep RL in imperfect domains.  The reviewers raised a number of specific issues to address, including improved definitions and descriptions, and proper citations of related work.  The authors have substantially  revised the manuscript to address most or all of these issues.  At this point, the only knock on this paper is that the findings seemed unsurprising from a game theoretic or deep learning point of view.  Pros: algorithmic contribution, technical quality, clarity Cons: no real surprises 
The paper addresses an interesting problem, is novel and works. While the paper improved through reviews + rebuttal, the reviewers still find the presentation lacking. 
This paper presents an analysis of using multiple generators in a GAN setup, to address the mode collapse problem. R1 was generally positive about the paper, raising the concern on how to choose the number of generators, and also whether parameter sharing was essential. The authors reported back on parameter sharing, showing its benefits yet did not have any principled method of selecting the number of generators. R2 was less positive about the paper, pointing out that mixture GANs and multiple generators have been tried before. They also raised concern with the (flawed) Inception score as the basis for comparison. R2 also pointed out that fixing the mixing proportions to uniform was an unrealistic assumption. The authors responded to these claims, clarifying the differences between this paper and the previous mixture GAN/multiple generator papers, and reporting FID scores. R3 was generally positive, also citing some novelty concerns similar to that of R2. I acknowledge the authors detailed responses to the reviews (in particular in response to R2) and I believe that the majority of concerns expressed have now been addressed. I also encourage the authors to include the FID scores in the final version of the paper.
This paper attracted strong praise from the reviewers, who felt that it was of high quality and originality.  The broad problem that is being tackled is clearly of great importance.  This paper also attracted the attention of outside experts, who were more skeptical of the claims made by the paper. The technical merits do not seem to be in question, but rather, their interpretation/application. The perception by a community as to whether an important problem has been essentially solved can affect the choices made by other reviewers when they decide what work to pursue themselves, evaluate grants, etc. It s important that claims be conservative and highlight the ways in which the present work does not fully address the broader problem of adversarial examples.  Ultimately, it has been decided that the paper will be of great interest to the community. The authors have also been entrusted with the responsibility to consider the issues raised by the outside expert (and then echoed by the AC) in their final revisions.  One final note: In their responses to the outside expert, the authors several times remark that the guarantees made in the paper are, in form, no different from standard learning theoretic claims: "This criticism, however, applies to many learning theoretic results (including those applied in deep learning)." I don t find any comfort in this statement. Learning theorists have often focused on the form of the bounds (sqrt(m) dependence and, say, independence from the # of weights) and then they resort to empirical observations of correlation to demonstrate that the value of the bound is predictive for generalization. because the bounds are often meaningless ("vacuous") when evaluated on real data sets. (There are some recent examples bucking this trend.) In a sense, learning theorists have gotten off easy. Adversarial examples, however, concern security, and so there is more at stake. The slack we might afford learning theorists is not appropriate in this new context. I would encourage the authors to clearly explain any remaining work that needs to be done to move from "good enough for learning theory" to "good enough for security". The authors promise to outline important future work / open problems for the community. I definitely encourage this.     
In principle, the idea behind the submission is sound: use a generative model (GANs in this case) to learn to generate desirable "goals" (subsets of the state space) and use that instead of uniform sampling for goals. Overall I tend to agree with Reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a high dimensional state space, which seems to be be whole raison d etre of GANs in this proposed method. The coverage experiment in Figure 5 seems like a good *illustration* of the method, but for this work to be convincing, I think we would need a more diverse set of experiments  (a la Figure 2) showing how this method performs on complicated tasks.  I encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the Assumptions being made in Section 3.3 are *violated* somehow (to actually test how the method fails in those cases).
The reviewers found the paper meaningful but noted that they were not convinced by the experiments as they stand and the presentation was dense for them.
There was some debate between the authors and an anonymous commentator on this paper.  The feeling of the commentator was that existing work (mostly from the PL community) was not compared to appropriately and, in fact, performs better than this approach.  The authors point out that their evaluation is hard to compare directly but that they disagreed with the assessment.  They modified their texts to accommodate some of the commentator s concerns; agreed to disagree on others; and promised a fuller comparison to other work in the future.  I largely agree with the authors here and think this is a good and worthwhile paper for its approach.  PROS: 1. well written 2. good ablation study 3. good evaluation including real bugs identified in real software projects 4. practical for real world usage  CONS: 1. perhaps not well compared to existing PL literature or on existing datasets from that community 2. the architecture (GGNN) is not a novel contribution
All reviewers gave "accept" ratings. it seems that everyone thinks this is interesting work.  The paper generated a large number of anonymous comments and these were addressed by the authors. 
The reviewers agree that this paper provides a sensible mechanism for producing word embeddings that exploit correlating features in the data (e.g. texts written by the same author), but point to other work doing the same thing. The lack of direct comparison in the experimental section is troublesome, although it is entirely possible the authors  were not aware of related work. Unfortunately, the lack of an author response to the reviews makes it hard to see the argument in defense of this paper, and I must recommend rejection.
The paper proposes a novel predictive model (e.g., from videos), called error encoding networks, by first learning a deterministic prediction model and then learning to minimize the residual error using latent variables. The latent variables given the sample are estimated by sampling from the prior then updating via gradient descent. The proposed method shows improved performance over the baselines. However, the qualitative results are not fully convincing, possibly because of (1) the limitation of the architecture, (2) suboptimal implementation/tuning of baselines (such as GAN and cVAE). 
The paper proposes a form of autoencoder that learns to predict the neighbors of a given input vector rather than the input itself.  The idea is nice but there are some reviewer concerns about insufficient evaluation and the effect of the curse of dimensionality.  The revised paper does address some questions and includes additional helpful experiments with different types of autoencoders.  However, the work is still a bit preliminary.  The area of auto encoder variants, and corresponding experiments on CIFAR 10 and the like, is crowded.  In order to convince the reader that a new approach makes a real contribution, it should have very thorough experiments.  Suggestions:  try to improve the CIFAR 10 numbers (they need not be state of the art but should be more credible), adding more data sets (especially high dimensional ones), and analyzing the effects of factors that are likely to be important (e.g. dimensionality, choice of distance function for neighbor search).
The paper received borderline negative reviews with scores of 5,5,6. A consistent issue was the weakness of the experiments: (i) lack of comparison to appropriate baselines, (ii) differences between published/reported numbers for DeepLab ResNet (R3) and (iii) related work, e.g. Wojna paper, as raised by R1. The AC did not find the author s responses to these issues convincing. For (ii) the gap between 73 and 79 is large and the author s explanation for the difference doesn t seem plausible. For (iii), the response promised comparisons/discussion but there were not added to the draft.  Given this, the paper cannot be accepted in it current form. The experiments should be improved before the paper is resubmitted. 
I concur with two of the reviewers: the work is somewhat incremental in terms of technical novelty (it s effectively CycleGANs for domain adaptation with a couple of effective tricks) and the need/advantage of the cycle consistency loss is not demonstrated sufficiently. The only solid ablation evidence seems to the the SVHN >MNIST experiment from post submission; I would personally like to see this kind of empirical proof extended much further (the fact that Shrivastava et al. s method doesn t work well on GTA >Cityscapes is not itself proof that cycle consistency is needed). With more empirical evidence I can see this paper being a good candidate for a computer vision conference like CVPR or ICCV.
The author s revisions addressed clarity issues and some experimental issues (e.g., including MAML results in the comparison). The work takes an original path to an important problem (transfer learning, essentially). There is a question of significance, and this is due to the fact that the empirical comparisons are still very limited. The task is an artificial one derived from MNIST. I would call this "toy" as well. On this toy task, the approach isn t that much different from MAML, which is not in of itself a problem, but it would be interested to have a less superficial discussion of the differences.  The authors mention that they didn t have time for a larger empirical study. I think one is necessary in this case because the work is purposing a new learning algorithm/framework, and the question of its potential impact/significance is an empirical one.
There are two parts to this paper (1) an efficient procedure for solving trust region subproblems in second order optimization of neural nets, and (2) evidence that the proposed trust region method leads to better generalization performance than SGD in the large batch setting. In both cases, there are some promising leads here. But it feels like two separate papers here, and I m not sure either individual contribution is well enough supported to merit publication in ICLR.  For (1), the contribution is novel and potentially useful, to the best of my knowledge. But as there s been a lot of work on trust region solvers and second order optimization of neural nets more generally, claims about computational efficiency would require comparisons against existing methods. The focus on efficiency also doesn t seem to fit with the experiments section, where the proposed method optimizes less efficiently than SGD and is instead meant to provide a regularization benefit.  For (2), it s an interesting empirical finding that the method improves generalization, but the explanation for this is very hand wavy. If second order optimization in general turned out to help with sharp minima, this would be an interesting finding indeed, but it doesn t seem to be supported by other work in the area. The training curves in Table 1 are interesting, but don t really distinguish the claims of Section 4.5 from other possible hypotheses. 
I am inclined to agree with R1 that there is an extensive literature on learning architectures now, and I have seen two others as part of my area chairing. This paper does not offer comparisons to existing methods for architecture learning other than very basic ones and that reduces the strength of the paper significantly. Further the broad exploration over 17 tasks is more overwhelming, than adding to an insight into the methods.
The paper proposes an autoencoder for sets, an interesting and timely problem.  The encoder here is based on prior related work (Vinyals et al. 2016) while the decoder uses a loss based on finding a matching between the input and output set elements.  Experiments on multiple data sets are given, but none are realistic.   The reviewers have also pointed out a number of experimental comparisons that would improve the contribution of the paper, such as considering multiple matching algorithms and more baselines.  In the end the idea is reasonable and results are encouraging, but too preliminary at this point.
The paper empirically evaluates the effectiveness of ensembles of deep networks against adversarial examples. The paper adds little to the existing literature in this area: an detailed study on "ensemble adversarial training" already exists, and the experimental evaluation in this paper is limited to MNIST and CIFAR (results on those datasets do not necessarily transfer very well to much higher dimensional datasets such as ImageNet). Moreover, the reviewers identify several shortcomings in the experimental setup of the paper. 
The paper proposes evaluation metrics for quantifying the quality of disentangled representations. There is consensus among reviewers that the paper makes a useful contribution towards this end. Authors have addressed most of reviewers  concerns in their response.
The paper received scores of 5,5,5, with the reviewers agreeing the paper was marginally below the acceptance threshold. The main issue, raised by both R2 and R3 was that connection between representation learning in deep nets and coding theory was not fully justified/made.  With no reviewer advocating acceptance, it is not possible to accept the paper unfortunately. 
The paper received mixed reviews with scores of 5 (R1), 5 (R2),  7 (R3).  All three reviewers raise concerns about the lack of comparisons to other methods. The rebuttal is not compelling on this point. There are quite a few methods that could be used for this application available (often with source code) and should be compared to, e.g. DenseNets (Huang et al.). Given that the proposed method isn t in of itself hugely novel, a thorough experimental evaluation is crucial to the justifying the approach. The AC has closely looked at the rebuttal and the paper and feels that it cannot be accepted for this reason at this time. 
This is a well written paper that aims to address an important problem. However, all the reviewers agreed that the experimental section is currently too weak for publication. They also made several good suggestions about improving the paper and the authors are encouraged to incorporate them before resubmitting.
While using self play for training a goal oriented dialogue system makes sense, the contribution of this paper compared to previous work (that the paper itself cites) seems too minor, and the limitations of using toy synthetic data further weaken the work. 
All the reviewers are agree on the significance of the topic of understanding expressivity of deep networks. This paper makes good progress in analyzing the ability of deep networks to fit multivariate polynomials. They show exponential depth advantage for general sparse polynomials.   I am very surprised that the paper misses the original contribution of Andrew Barron. He analyzes the size of the shallow neural networks needed to fit a wide class of functions including polynomials. The deep learning community likes to think that everything has been invented in the current decade.  @article{barron1994approximation,   title {Approximation and estimation bounds for artificial neural networks},   author {Barron, Andrew R},   journal {Machine Learning},   volume {14},   number {1},   pages {115 133},   year {1994},   publisher {Springer} }
The key concern from the reviewers that was not addressed is that none of the experimental results illustrate convergence vs. time instead of convergence vs. number of iterations.  While the authors point out that their method is O(ND) instead of O(KND), the reviewers really wanted to see graphs demonstrating this, given that the implicit SGD method requires an iterative solver. The revised paper is otherwise much improved from the original submission, but falls a bit short of ICLR acceptance because of the lack of a measurement of convergence vs. time.  Pros: + Promising unbiased algorithms for optimizing the log likelihood of a model using a softmax without having to repeatedly compute the normalizing factor.  Cons:   The key concern from the reviewers that was not addressed is that none of the experimental results illustrate convergence vs. time instead of convergence vs. number of iterations. 
Mixed precision application of CNNs is being explored for e.g. hardware implementations of networks trained at full precision.  Mixed precision at training time is less common.  This submission primarily concerns itself with the practical implementation details of training with mixed precision, and focuses primarily on representation of mixed precision floating point and algorithmic issues for learning.  In the end the support for the approach is primarily empirical, with the mixed precision approach giving a factor of two speedup with half the precision, while accuracies remain effectively statistically tied on the ImageNet 1k database.  Table 1 should avoid the use of bold as there is likely no statistical significance.  The reviewers appreciated the paper. The proposed approach is sensible, and appears correct.
This paper and reviews makes for a difficult call.  The reviewers appear to be in agreement that Value Propagation provides an interesting algorithmic advance over earlier work on Value Iteration networks.  AnonReviewer1 gives a strong rationale why the advance is both original and significant.  Their experiments also show very nice results with VProp and MVProp in 2 D grid worlds.  However, I also fully agree with AnonReviewer2 that testing in other domains beyond 2 D grid world is necessary.  Earlier work on VIN was also tested on a Mars Rover / continuous control domain, as well as graph based web navigation task.  The authors  rebuttal on this point comes across as weak.  In their view, they can t tackle real world domains until VProp has been proven effective in large, complex grid worlds.  I don t buy this at all   they could start initial experiments right away, which would perhaps yield some surprising results. Given this analysis, the committee recomments this paper for workshop.  Pros: significant algorithmic advance, good technical quality and writeup, nice results in 2 D grid world.  Con: Validation is only in 2 D grid world domains. 
The reviewers were generally positive about this paper with a few caveats:  PROS: 1. Important and challenging topic to analyze and any progress on unsupervised learning is interesting. 2. the paper is clear, although more formalization would help sometimes 3. The paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know. 4. A large set of experiments  CONS: 1. Some concerns about whether the claims are sufficiently justified in the experiments 2. The paper is very long and quite dense 
All reviewers agree that the contribution of this paper, a new way of training neural nets to execute Monte Carlo Tree Search, is an appealing idea.  For the most part, the reviewers found the exposition to be fairly clear, and the proposed architecture of good technical quality.  Two of the reviewers point out flaws in implementing in a single domain, 10x10 Sokoban with four boxes and four targets.  Since their training methodology uses supervised training on approximate ground truth trajectories derived from extensive plain MCTS trials, it seems unlikely that the trained DNN will be able to generalize to other geometries (beyond 10x10x4) that were not seen during training.  Sokoban also has a low branching ratio, so that these experiments do not provide any insight into how the methodology will scale at much higher branching ratios.  Pros: Good technical quality, interesting novel idea, exposition is mostly clear.  Good empirical results in one very limited domain. Cons: Single 10x10x4 Sokoban domain is too limited to derive any general conclusions.  Point for improvement: The paper compares performance of MCTSnet trials vs. plain MCTS trials based on the number of trials performed.  This is not an appropriate comparison, because the NN trials will be much more heavyweight in terms of CPU time, and there is usually a time limit to cut off MCTS trials and execute an action.  It will be much better to plot performance of MCTSnet and plain MCTS vs. CPU time used.
This paper presents new results on adversarial training, using the framework of robust optimization. Its minimax nature allows for principled methods of both training and attacking neural networks.  The reviewers were generally positive about its contributions, despite some concerns about  overclaiming . The AC recommends acceptance, and encourages the authors to also relate this work with the concurrent ICLR submission (https://openreview.net/forum?id Hk6kPgZA ) which addresses the problem using a similar approach. 
In general, the reviewers and myself find this work of some interest, though potentially somewhat incremental in terms of technical novelty compared to the work for Makhzani et al. Another bothersome aspect is the question of evaluation and understanding how well the model actually does; I am not convinced that the interpolation experiments are actually giving us a lot of insights. One interesting ablation experiment (suggested privately by one of the reviewers) would be to try AAE with Wasserstein and without a learned generator   this would disambiguate which aspects of the proposed method bring most of the benefit. As it stands, the submission is just shy of the acceptance bar, but due to its interesting results in the natural language domain, I do recommend it being presented at the workshop track.
The paper studies the use of PixelCNN density models for the detection of adversarial images, which tend to lie in low probability parts of image space. The work is novel, relevant to the ICLR community, and appears to be technically sound.  A downside of the paper is its limited empirical evaluation: there evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defense on a dataset like ImageNet.
The paper builds on earlier work by Wang et al (2015) on Visual Concepts (VCs) and explores the use of VCs for few shot learning setting for novel classes.  The work, as pointed out by two reviewers is somewhat incremental in nature, with main novelty being the demonstration of utilities of VCs for few shot learning. This would not have been a big limitation if the paper had a carefully conducted empirical evaluation providing insights on the effect of various configuration settings/hyperparameters on the performance in few shot learning, which two of the reviewers (Anon3, Anon2) state are missing. The paper falls short of the acceptance threshold in its current form.  PS: The authors posted a github link to the code on Jan 12 which may potentially compromise the anonymity of the submission (though it was after all the reviews were already in) https://openreview.net/forum?id BJ_QxP1AZ&noteId BJaIDpBEM 
This paper attempts a theoretical treatment of the influence of depth in RNNs on their ability to capture dependencies in the data. All reviewers found the theoretical contribution of the paper interesting, and while there were problems raised regarding formalisation, they appear to have been adequately addressed in the revisions to the paper. The main concern in all three reviews surrounds the evaluation, and weakness thereof. The overarching point of contention seems to be that the theory relates to a particular formulation of RNNs (RAC), causing doubts that the results lift to other architectural variants which are used to obtain state of the art results on tasks such as language modelling. It seems that the paper could be significantly improved by the provision of stronger empirical results to support the theory, or a more convincing argument as to why the results should transfer from, say, RAC to LSTMs. The authors point to two papers on the matter in their response, but it is not clear this is a substitute for experimental validation. I find the paper a bit borderline because of this, and recommend redirection to the workshop.
This paper presents a marginally interesting idea   that of an interaction tensor that compares two sentence representations word by word, and feeds the interaction tensor into a higher level feature extraction mechanism.  It produces good results on multi NLI and SNLI datasets.  There is some criticism about comparing with several baselines for multi NLI where there was a restriction of not using inter sentence comparison networks, but the authors do compare with a similar approach without that restriction and shows improvements.   However, there is no solid error analysis that shows what type of examples this interaction tensor idea captures better than other strong baselines such as ESIM. Overall, the committee feels this paper will add value to the conference.
This paper proposes an offline neural method using concrete/gumbel for learning a sparse codebook for use in NLP tasks such as sentiment analysis and MT. The method outperforms other methods using pruning and other sparse coding methods, and also produces somewhat interpretable codes. Reviewers found the paper to be simple, clear, and effective. There was particular praise for the strength of the results and the practicality of application. There were some issues, such as only being applicable to input layers, and not being able to be applied end to end. The author also did a very admirable job of responding to questions about analysis with clear and comprehensive additional experiments. 
Authors present a new multi layered capsule network architecture, implemented an EM routing procedure, and introduced "Coordinate Addition".  Capsule architectures are gaining interest because of their ability to achieve equivariance of parts, and employ a new form of pooling called "routing" (as opposed to max pooling) which groups parts that make similar predictions of the whole to which they belong, rather than relying on spatial co locality. New state of art performances are being achieved on focused datasets, for which the authors have continued the trend.  Pros:   New significant improvement to state of art performance is obtained on smallNORB, both in comparison to CNN structure as well as the most recent previous implementation of capsule network.  Cons:   Some concern arose regarding the writing of the paper and the ability to understand the material, which authors have made an effort to address.  Given the general consensus of the reviewers that this work should be accepted, the general applicability of the technology to multiple domains, and the potential impact that improvements to capsule networks may have on an early field, area chair recommends this work be accepted as a poster presentation. 
The reviewers thought that idea of trying to exploit low rank structure in the loss gradients of a feedforward network to improve training was interesting; however they expressed many concerns about the clarity of the presentation, quality of the empirical evaluation, and significance of the result (since the tests were not done on an architecture anywhere near state of the art). Because the authors did not participate in the discussion period, none of these concerns were addressed.  Pros: + Promising idea for new approaches to optimization.  Cons:   Unclear notation for the intended machine learning audience   Algorithm should be illustrated using pseudocode   Limited significance if the method is only usable with purely feedforward networks.   Limited empirical evaluation: positive results only if weights are poorly initialized. 
All reviewers acknowledge that the idea of the paper is interesting but have expressed serious concerns on empirical evaluations. The paper is not suitable for publication in its current form.
This paper turned out to be quite difficult to call.  My take on the pros/cons is:  1. The research topic, how and why humans can massively outperform DQN, is unanimously viewed as highly interesting by all participants.  2. The authors present an original human subject study, aiming to reveal whether human outperformance is due to human knowledge priors.  The study is well conceived and well executed.  I consider the study to be a contribution by itself.  3. The study provides prima facie evidence that human priors play a role in human performance, by changing the visual display so that the priors cannot be used.  4. However, the study is not definitive, as astutely argued by AnonReviewer2.  Experiments using RL agents (with presumably no human priors) yield behavior that is similar to human behavior.  So it is possible that some factor other than human prior may account for the behavior seen in the human experiments.  5. It would indeed be better, as argued by AnonReviewer2, to use some information theoretic measure to distinguish the normal game from the modified games.  6. The paper has been substantially improved and cleaned up from the original version.  7. AnonReviewer1 provided some thoughtful detailed discussion of how the authors may be overstating the conclusions that one can draw from the paper.  Bottom line: Given the procs and cons of the paper, the committee recommends this for workshop. 
This paper provides a simple technique for stabilizing GAN training, and works over a variety of GAN models.  One of the reviewers expressed concerns with the value of the theory. I think that it would be worth emphasizing that similar arguments could be made for alternating gradient descent, and simultaneous gradient descent. In this case, if possible, it would be good to highlight how the convergence of the prediction method approach differs from the alternating descent approach. Otherwise, highlight that this theory simply shows that the prediction method is not a completely crazy idea (in that it doesn t break existing theory).  Practically, I think the experiments are sufficiently interesting to show that this approach has promise. I don t see the updated results for Stacked GAN for a fixed set of epochs (20 and 40 at different learning rates). Perhaps put this below Table 1.
All reviewers recommend accepting this paper, and this AC agrees. 
This paper does not meet the bar for ICLR   neither in terms of the quality of the write up, nor in experimental design. The two confident reviewers agree to reject the paper, the weak accept comes from a less confident reviewer who did not write a good review at all. The rebuttal does not change this assessment.
Looks like a great contribution to ICLR. Continuous adaptation in nonstationary (and competitive) environments is something that an intelligent agent acting in the real world would need to solve and this paper suggests that a meta learning approach may be quite appropriate for this task.
The reviewers are unanimous in finding the work in this paper highly novel and significant.  They have provided detailed discussions to back up this assessment.  The reviewer comments surprisingly included a critique that  "the scientific content of the work has critical conceptual flaws" (!)  However, the author rebuttal persuaded the reviewers that the concerns were largely addressed.
meta score: 4  This is basically an application in which some different deep learning approaches are compared on the task of automatically identifying domain names automatically generated by malware.  The experiments are well constructed and reported.  However, the work does not have novelty beyond the application domain, and thus is not really suitable for ICLR.  Pros    good set of experiments carried out on an important task    clearly written Cons    lacks technical novelty 
The reviewers of the paper are not very enthusiastic of the new model proposed, nor are they very happy with the experiments presented.   It is unclear from both the POS tagging and dependency parsing results where they stand with respect to state of the art methods that do not use RNNs.  We understand that the idea is to compare various RNN architectures, but it is surprising that the authors do not show any comparisons with other methods in the literature.  The idea of truncating sequences beyond a certain length is also a really strange choice.  Addressing the concerns of the reviewers will lead to a much stronger paper in the future.
This paper describes a method to generate provably  optimal  adversarial examples, leveraging the so called  Reluplex  technique, which can evaluate properties of piece wise linear representations. Reviewers agreed that incorporating optimality certificates into adversarial examples is a promising direction to follow, but were also concerned about the lack of empirical justification the current paper provides and missed discussion about the relevance of choosing Lp distances. They all recommended pushing experiments to more challenging datasets before the paper can be accepted, and the AC shares the same advice. 
The main idea of the paper is to transform graph classification into image representation (via adjacency matrices). Two reviewers are positive, while one is negative. The concerns are novelty (as mentioned by R2), while the last reviewer thinks the method is too simple and unprincipled (here the AC agrees with authors that simple is not necessarily bad). Overall, none of the reviewers champions this paper. Due to many excellent submissions, unfortunately this paper cannot be accepted in present form.
The paper addresses the important question of determining the intrinsic dimensionality, but there remain several issue, which make the paper not ready at this point: unclear exposition, lack of contextualisation of existing work and seemingly limited insights. The reviewers have provided many suggestions to improve the paper which we hope will be useful to improve the paper.
This paper proposes a method for training an neural network to operate stack based mechanism in order to act as a CFG parser in order to, eventually, improve program synthesis and program induction systems. The reviewers agreed that the paper was compelling and well supported empirically, although one reviewer suggested that analysis of empirical results could stand some improvement. The reviewers were not able to achieve a clear consensus on the paper, but given that the most negative reviewer has also declared themselves the least confident in their assessment, I am happy to recommend acceptance on the basis of the median rather than mean score.
This paper proposes a theoretically motivated method for combining reinforcement learning and imitation learning. There was some disagreement amongst the reviewers, but the AC was satisfied with the authors  rebuttal.
meta score: 8  The paper explores mixing 16  and 32 bit floating point arithmetic for NN training with CNN and LSTM experiments on a variety of tasks  Pros:    addresses an important practical problem    very wide range of experimentation, reported in depth  Cons:    one might say the novelty was minor, but the novelty comes from the extensive analysis and experiments
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
The paper presents a hybrid architecture which combines WaveNet and LSTM for speeding up raw audio generation. The novelty of the method is limited, as it‚Äôs a simple combination of existing techniques. The practical impact of the approach is rather questionable since the generated audio has significantly lower MOS scores than the state of the art WaveNet model.
The paper proposes a method for accented speech generation using GANs. The reviewers have pointed out the problems in the justification of the method (e.g. the need for using policy gradients with a differentiable objective) as well as its evaluation.
The paper received scores either side of the borderline: 6 (R1), 5 (R2), 7 (R3). R1 and R3 felt the idea to be interesting, simple and effective. R2 raised a number of concerns which the rebuttal addressed satisfactorily. Therefore the AC feels the paper can be accepted.
This manuscript was reviewed by 3 expert reviewers and their evaluation is generally positive. The authors have responded to the questions asked and the reviewers are satisfied with the responses. Although the 2D environments are underwhelming (compared to 3D environments such as SUNCG, Doom, Thor, etc), one thing that distinguishes this paper from other concurrent submissions on the similar topics is the demonstration that "words learned only from a VQA style supervision condition can be successfully interpreted in an instruction following setting." 
The authors propose the use of Gaussian processes as the prior over activation functions in deep neural networks.  This is a purely mathematical paper in which the authors derive an efficient and scalable approach to their problem.  The idea of having flexible distributions over activation functions is interesting and possibly impactful.  One reviewer recommended acceptance with low confidence.  The other two found the idea interesting and compelling but confidently recommended rejection.  These reviewers are concerned that the paper is unnecessarily complex in terms of the mathematical exposition and that it repeats existing derivations without citation.  It is very important that the authors acknowledge existing literature for mathematical derivations.  Furthermore, the reviewers question the correctness of some of the statements (e.g. is the variational bound preserved?).  These reviewers agreed that the paper is incomplete without any empirical validation.  Pros:   A compelling and promising idea   The approach seems to be scalable and highly plausible  Cons:   No experiments   Significant issues with citing of related work   Significant questions about the novelty of the mathematical work
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * Solid experimental results against strong baselines on a task of great interest * Method presented is appropriate for the task * Paper is presented relatively clearly, especially after revision  Cons: * The paper is somewhat incremental. The basic idea of aggregating across multiple examples was presented in Kadlec et al. 2016, but the methodology here is different. 
The paper presents a domain specific language for RNN architecture search, which can be used in combination with learned ranking function or RL based search. While the approach is interesting and novel, the paper would benefit from an improved evaluation, as pointed out by reviewers. For example, the paper currently evaluated coreDSL+ranking for language modelling and extendedDSL+RL for machine translation. The authors should use the same evaluation protocol on all tasks, and also compare with the state of the art MT approaches.
The submission proposes a Kuiper statistic based loss function for survival clustering.  This loss function is applied to train a deep network.  Results are presented on a Friendster dataset.  This submission received borderline/mixed reviews.  The primary concerns were: justification of the Kuiper loss, lack of details of the experimental setup, writing style.  In the end, these concerns remain.  Of particular importance is the justification and experimental validation of the Kuiper statistic.  Although it seems a reasonable choice, from the authors  response to R3: "We now also report results for Kolmogorov Smirnov loss. Although the difference in performance between the two loss functions is not significant in the Friendster dataset, Kuiper loss has higher statistical power in distinguishing distribution tails [Tygert 2010]."  If this theoretical result from [Tygert 2010] is relevant, it should be possible to demonstrate this experimentally.  If such differences are irrelevant for the data of interest, the paper should perhaps be reframed with a better discussion of available statistics and literature (cf. Reviewer 2), and a more general presentation de emphasizing modeling choices that may have limited practical relevance. 
This is a high quality paper, clearly written, highly original, and clearly significant. The paper gives a complete analysis of SGD in a two layer network where the second layer does not undergo training and the data are linearly separable.  Experimental results confirm the theoretical suggestion that the second layer can be trained provided the weights don t change sign and remain bounded. The authors address the major concerns of the reviewers (namely, whether these results are indicative given the assumptions). This line of work seems very promising.
PROS: 1. Clear, interesting idea. 2. Largely convincing evaluation 3. Good writing  CONS: 1. The model used in the evaluation is a Resnet 50 and could have been more convincing with a more SOTA model. 2. There is some concern about the whether the comparison of results (fig 6c) is really apples to apples. 
Well motivated and well received by all of the expert reviewers. The AC recommends that the paper be accepted.
This submission is a continuation of a line of theoretical work that seeks to characterize optimization landscapes of neural networks by the presence or absence of spurious local minima.  As the number of critical points grows combinatorially for larger networks, it is very challenging to show such results.  The present submission extends slightly previous work by considering two hidden units and their proof technique goes beyond that of Brutzkus and Globerson, 2017, potentially leading to more interesting results if they can be extended to more complex networks.  The setting of two hidden units is quite limited   far from any practical setting.  If this were the stepping stone to proving optimality of certain optimization strategies for more complex networks, this may be of some interest, but it seems doubtful.  One indication is given in Sec. 7 / Fig. 1 in which it is shown that for even quite small numbers of hidden units, spurious local optima do occur and are reached 40% of the time for random initializations even with only 11 nodes.  
Regarding clarity, while the paper definitely needs work if it is to be resubmitted to an ML venue, different revisions would be appropriate for a physics audience. And given the above comment, any suggested changes are likely to be superfluous.
This is a very interesting paper that also seems a little underdeveloped. As noted by the reviewers, it would have been nice to see the idea applied to domains requiring function approximation to confirm that it can scale   the late addition of Freeway results is nice, but Freeway is also by far the simplest exploration problem in the Atari suite. There also seems to be a confusion between methods such as UCB, which explore/exploit, and purely exploitative methods. The case gamma_E > 0 is also less than obvious. Given the theoretical leanings of the paper, I would strongly encourage the authors to focus on deriving an RMax style bound for their approach. 
This paper proposes an approach for learning a sparsifying transform via a set of nonlinear transforms at learning time.  The presentation needs a lot of work.  The original paper was 17 pages long and very difficult to understand.  The revised paper is 12 pages long, which is still too long for the content.  The paper needs to better distinguish between the major and minor points.  It is still too difficult to judge the contribution.
This paper characterizes the induced geometry of the latent space of deep generative models. The motivation is established well, such that the paper convincingly discusses the usefulness derived from these insights. For example, the results uncover issues with the currently used methods for variance estimation in deep generative models. The technique invoked to mitigate this issue does feel somehow ad hoc, but at least it is well motivated.  One of the reviewers correctly pointed out that there is limited novelty in the theoretical/methodological aspect. However, I agree with the authors‚Äô rebuttal in that characterizing geometries on stochastic manifolds is much less studied and demonstrated, especially in the deep learning community. Therefore, I believe that this paper will be found useful by readers of the ICLR community, and will stimulate future research. 
PROS:  1. Interesting and clearly useful idea 2. The paper is clearly written. 3. This work doesn t seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know). 4. This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand.  CONS:  1. The paper has some clarity issues which the authors have promised to fix.   
meta score: 8  The paper present a distributed architecture using prioritized experience replay for deep reinforcement learning.  It is well written and the experimentation is extremely strong.  The main issue is the originality   technically, it extends previous work in a limited way;  the main contribution is practical, and this is validated by the experiments.  The experimental support is such that the paper has meaningful conclusions and will surely be of interest to people working in the field.  Thus I would say it is comfortably over the acceptance threshold.  Pros:    good motivation and literature review    strong experimentation    well written and clearly presented    details in the appendix are very helpful Cons:    possibly limited originality in terms of modelling advances 
Thank you for submitting you paper to ICLR. The revision improved the paper e.g. moving Appendix A3 to the main text has improved clarity, but, like reviewer 3, I still found section 4 hard to follow. As the authors suggest, shifting the terminology to "posterior shifting‚Äù rather than ‚Äúsharpening" would help at a high level, but the design choices should be more carefully explained. The experiments are interesting and promising. The title, although altered, still seems a misnomer given that the experimental evaluation focusses on RNNs.  Summary: There is the basis of a good paper here, but the rationale for the design choices should be more carefully explained.
The paper performs a theoretical analysis of the representation power of convolutional networks with inter layer connections. Whilst the results themselves are interesting, the current presentation of the paper stands in the way of the reading grasping and appreciating the main insights from the paper.  The authors acknowledge these issues in their rebuttal, but have not yet revised the paper to resolve them. I encourage the authors to revise the paper to address the reviewer comments, and re submit it to another venue. 
The paper studies a adversarial attacks and defenses against convolutional networks based on a minimax formulation of the problem. Whilst this is an interesting direction of research, the present paper seems preliminary. In particular, compared to several other independent ICLR submissions, the empirical evaluation is quite weak: it does not consider the strongest known gradient based attack (Carlini Wagner) as baseline and does not report results on ImageNet. The reviewers identify several issues related to Lemma 1 and to the clarity of presentation.
This paper presents an interesting idea that is related to imitation learning, safe exploration, and intrinsic motivation. However, in its current state the paper needs improvement in clarity. There are also some concerns about the number of hyperparameters involved. Finally, the experimental results are not completely convincing and should reflect existing baselines in one of the areas described above. 
There is a concern from one of the reviewers that the paper needs deeper analysis. On the other hand, applying finite horizon techniques to deep RL is relatively unexplored, and the paper does provide some interesting results in that direction. 
The reviewers agreed that the work addresses an important problem. There was disagreement as to the correctness of the arguments in the paper: one of these reviewers was eventually convinced. The other pointed out another two issue in their final post, but it seems that 1. the first is easily adopted and does not affect the correctness of the experiments and 2. the second was fixed in the second revision. Ideally these would be rechecked by the third reviewer, but ultimately the correctness of the work is the authors  responsibility.  Some related work (by McAllister) was pointed out late in the process. I encourage the authors to take this related work seriously in any revisions. It deserves more than two sentences.
The submission provides an interesting way to tackle the so called distributional shift problem in machine learning. One familiar example is unsupervised domain adaptation. The main contribution of this work is deriving a bound on the generalization error/risk for a target domain as a combo of re weighted empirical risk on the source domain and some discrepancy between the re weighted source domain and the target domain. The authors then use this to formulate an objective function.  The reviewers generally liked the paper for its theoretical results, but found the empirical evaluation somewhat lacking, as do I. Especially the unsupervised domain adaptation results are very toy ish in nature (synthetic data), whereas the literature in this field, cited by the authors, does significantly larger scale experiments. I am unsure as to how much I value I can place in the IHDP results since I am not familiar with the benchmark (and hence my lower confidence in the recommendation).  Finally, I am not very convinced that this is the appropriate venue for this work, despite containing some interesting results.
This is an interesting paper and addresses an important problem of neural networks with memory constrains. New experiments have been added that add to the paper, but the full impact of the paper is not yet realised, needing further exploration of models of current practice, wider set of experiments and analysis, and additional clarifying discussion.
This paper initially received borderline reviews. The main concern raised by all reviewers was a limited experimental evaluation (synthetic only). In rebuttal, the authors provided new results on the CelebA dataset, which turned the first reviewer positive. The AC agrees there is merit to this approach, and generally appreciates the idea of compositional concept learning.
The paper presents an approach for learning continuous valued vector representations combining multiple input feature sets of different types, in both unsupervised and supervised settings.  The revised paper is a merger of the original submission and another ICLR submission.  This meta review takes into account all of the comments on both submissions and revisions.  The merged paper is an improvement over the two separate ones.  However, the contribution over previous work is still a bit unclear.  It still does not sufficiently compare to/discuss in the context of other recent work on combining multiple feature groups.  The experiments are also quite limited.  The idea is introduced as extremely general, but the experiments focus on a small number of specific tasks, some of them non standard.
As expressed by most reviewers, the idea of the paper is interesting:  using summarization as an intermediate representation for an auto encoder.  In addition, a GAN is used on the generator output to encourage the output to look like summaries.  They just need unpaired summaries.  Even if the idea is interesting, from the committee s perspective, important baselines are missing in the experimental section:  why would one choose to use this method if it is not competitive with other baselines that have proposed work in this vein?  One reviewer brings up the point that the method is significantly worse than a supervised baseline.  Moreover, the authors mention the work of Miao and Blunsom, but could have used one of their experimental setups to show that at least in the semi supervised scenario, this work empirically performs as well or better than that baseline.
After careful consideration, I think that this paper in its current form is just under the threshold for acceptance. Please note that I did take into account the comments, including the reviews and rebuttals, noting where arguments may be inconsistent or misleading.  The paper is a promising extension of RCC, albeit too incremental. Some suggestions that may help for the future:  1) Address the sensitivity remark of reviewer 2. If the hyperparameters were tuned on RCV1 instead of MNIST, would the results across the other datasets remain consistent?  2) Train RCC or RCC DR in an end to end way to gauge the improvement of joint optimization over alternating, as this is one of the novel contributions.  3) Discuss how to automatically tune \lambda and \delta_1 and \delta_2. These may appear in the RCC paper, but it s unclear if the same derivations hold when going to the non linear case (they may in fact transfer gracefully, it s just not obvious). It would also be helpful for researchers building on DCC.
This paper presents a MIL method for medical time series data. General consensus among reviewers that work does not meet criteria for being accepted.  Specifically:  Pros:   A variety of meta learning parameters are evaluated for the task at hand.   Minor novelty of the proposed method  Cons:   Minor novelty of the proposed method   Rationale behind architectural design   Thoroughness of experimentation   Suboptimal choice of baseline methods   Lack of broad evaluation across applications for new design   Small dataset size   Significance of improvement 
This work shows how activation patterns of units reminiscent of grid and border cells emerge in RNNs trained on navigation tasks. While the ICLR audience is not mainly focused on neuroscience, the findings of the paper are quite intriguing, and grid cells are sufficiently well known and "mainstream" that this may interest many people.
This paper presents an update to the method of Franceschi 2017 to optimize regularization hyperparameters, to improve stability.  However, the theoretical story isn t so clear, and the results aren t much of an improvement.  Overall, the presentation and development of the idea needs work.
The authors provide an extension to GCNs of Kipf and Welling in order to incorporate information about higher order neighborhoods. The extension is well motivated (and  though I agree that it is not trivial modification of the K&W approach to the second order,  thanks to the authors for the clarification).  The improvements are relatively moderate.  Pros:   The approach is well motivated   The paper is clearly written Cons:   The originality and impact (as well as motivation) are questioned by the reviewers 
This paper proposes a novel application of generative adversarial networks to model neural spiking activity.  Their technical contribution, SpikeGAN, generates neural spikes that accurately match the statistics of real recorded spiking behavior from a small number of neurons.  The paper is controversial among the reviewers with a 4, a 6 and an 8.  The 6 is short and finds the idea exciting but questions the utility of the proposed approach in terms of actually studying neural spiking.  The 4 and 8 are both quite thorough reviews.  4 seems to mostly question the motivation of using a GAN over a MaxEnt model and demands empirical comparison to other approaches.  8 applauds the paper as a well executed pure application paper, applying recent innovations in machine learning to an important application with some technical innovation.  Overall the reviewers found the paper clear and easy to follow and agree that the application of GANs to neural spiking activity is novel.  In general, I find that such high variance in scores (with thorough reviews) indicate that the paper is exciting, innovative and might stir up some interesting discussion.  As such, and under the belief that ICLR is made stronger with interesting application papers, I feel inclined to accept as a poster.  Pros:   A novel application of GANs to neural spiking data   Addresses an important and highly studied application area (computational neuroscience)   Clearly written and well presented   The approach appears to model well real neural spiking activity from salamander retina  Cons:   Known pitfalls of GANs aren t really addressed in the paper (mode collapse, etc.)   The authors don t compare to state of the art models of neural spiking activity (although they compare to an accepted standard approach   MaxEnt)   Limited technical innovation over existing methods for GANs
PROS: 1. Overall, the paper is well written, clear in its exposition and technically sound. 2. With some caveats, an independent team concluded that the results were "largely reproducible" 3. The key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models. 4. The implementation seems technically sound.  CONS: 1. The results were a bit over stated (the authors promise to correct) 2. Could benefit from more comparison with other approaches (e.g. RL)
This paper combines existing models to detect topics and generate responses, and the resulting model is shown to be slightly preferred by human evaluators over baselines. This is quite incremental and the results are not impressive enough to stand on their own merit.
While the paper shows some encouraging results for scaling up SVMs using coreset methods, it has fallen short of making a fully convincing case, particularly given the amount of intense interest in this topic back in the heydey of kernel methods. When it comes to scalability, it has become the norm now to benchmark results on far larger datasets using parallelism, specialized hardware in conjunction with algorithmic speedups (e.g., using random feature methods, low rank approximations such as Nystrom and other approaches). As such the paper is unlikely to generate much interest in the ICLR community in its current form.
All three reviewers felt that the paper was just below the acceptance threshold, with scores of 5,4,5. R1 felt there were problems in the proofs, but the authors rebuttal satisfactorily addressed this. R3 and the authors had an extended discussion with the authors, but did not revise their score from its initial value (5). R4 had concerns about the experimental evaluation, that wasn t fully addressed in the rebuttal. With no reviewers advocating acceptance, the paper will have to rejected unfortunately. 
This paper explores the training of CNNs which have reduced precision activations. By widening layers, it shows less of an accuracy hit on ILSVRC 12 compared to other recent reduced precision networks. R1 was extremely positive on the paper, impressed by its readability and the quality of comparison to previous approaches (noting that results with 2 bit activations and 4 bit weights matched FP baselines). This seems very significant to me. R1 also pointed out that the technique used the same hyperparameters as the original training scheme, improving reproducibility/accessibility. R1 asked about application to MobileNets, and the authors reported some early results showing that the technique also worked with smaller network/architectures designed for low memory hardware. R2 was less positive on the paper, with the main criticism being that the overall technical contribution of the paper was limited. They also were concerned that the paper seemed to motivate based on reducing memory footprint, but the results were focused on reducing computation. R3 liked the simplicity of the idea and comprehensiveness of the results. Like R2, they thought the paper was limited novelty. In their response to R3, the authors defended the novelty of the paper. I tend to side with the authors that very few papers target quantization at no accuracy loss. Moreover, the paper targets training, which also receives much less attention in the model compression / reduced precision literature. Is the architecture really novel? No. But does the experimental work investigate an important tradeoff? Yes.
The paper nicely unifies previous results and develops the property of local openness. While interesting, I find the application to multi layer linear networks extremely limiting. There appears to be a sub field in theory now focusing on solely multi layer linear networks which is meaningless in practice. I can appreciate that this could give rise to useful proof techniques and hence, I am recommending it to the workshop track with the hope that it can foster more discussions and help researchers move away from studying multi layer linear networks.
This paper studies the interplay between adversarial examples and generalization in the uniform setting (not specific assumptions on the architecture) in a toy high dimensional setting. In particular, the authors show a fundamental tradeoff between generalization error and the average distance of adversarial examples.  Reviewers were skeptical about the possible significance of this work, but the paper underwent a major revision that greatly improved the quality of presentation. That said, the results are still preliminary since they only consider a toy dataset (concentric spheres). The AC recommends re submitting this work to the workshop track.
The reviewers agree that the paper is below threshold for acceptance in the main track (one with very low confidence), but they favor submitting the paper to the workshop track.  The paper considers policy gradient methods for two player zero sum Alternating Markov games.  They propose adversarial policy gradient (fairly obviously), wherein the critic estimates min rather than mean reward.   They also report promising empirical results in the game of Hex, with varying board sizes.  I found the paper to be well written and easy to read, possibly due to revisions in the rebuttal discussions.  The reviewers consider the contribution to be small, mainly due to the fact that the key algorithmic insights were already published decades ago.  Reintroducing them is a service to the community, but its novelty is limited.  Other critiques mentioned that results in Hex only provide limited understanding of the algorithm s behavior in general Alternating Markov games.  The lack of comparison with modern methods like AlphaGo Zero was also mentioned as a limitation.  Bottom line: The paper provides a small but useful contribution to the community, as described above, and the committee recommends it for workshop. 
Each of the reviewers had a slightly different set of issues with this paper but here is an attempt at a summary:  PROS: 1. Paper is mostly clear and well structured.  CONS: 1. Lack of novelty 2. Unsupported claims 3. Questionable methodology (using dropout confounds the goal of the experiment)  The authors did not submit a rebuttal.
There is overall consensus about the paper s lack of novelty and clarity.  Reviewer 1 has detailed comments that can be used to strengthen the paper.  Reviewer 3 suggests that this paper is very close to Anandkumar et al 2012, and it is not clear where the novelty lies.  Addressing these concerns of the reviewers will make the paper more acceptable to future venues.
The reviewers found numerous issues in the paper, including unclear problem definitions, lack of motivation, no support for desiderata, clarity issues, points in discussion appearing to be technically incorrect, restrictive setting, sloppy definitions, and uninteresting experiments.  Unfortunately, little note of positive aspects was mentioned.  The authors wrote substantial rebuttals, including an extended exchange with Reviewer2, but this had no effect in terms of score changes. Given the current state of the paper, the committee feels the paper falls short of acceptance in its current form.
This paper tackles a very important problem: evaluating natural language generation. The paper presents an overview of existing unsupervised metrics, and looks at how they correlate with human evaluation scores. This is important work and the empirical conclusions are useful to the community, but the datasets used are too limited and the authors agree it would be better to use newer bigger and more diverse datasets suggested by reviewers for drawing more general conclusions. This work would indeed be much stronger if it relied on better, more recent datasets; therefore publication as is seems premature. 
The authors present an environment for semantic navigation that is based on an existing dataset, SUNCG. Datasets/environments are important for deep RL research, and the contribution of this paper is welcome. However, this paper does not offer enough novelty in terms of approach/method and its claims are somewhat misleading, so it would probably be a better fit to publish it at a workshop. 
This paper received borderline reviews. Initially, all reviewers raised a number of concerns (clarity, small improvements, etc). Even after some back and forth discussion, concerns remain, and it s clear that while the idea has potential, another round of reviewing is needed before a decision can be reached. This would be a major revision in a journal. Unfortunately, that is not possible in a conference setting and we must recommend rejection. We recommend the authors to use the feedback to make the manuscript stronger and submit to a future venue. 
With an 8 6 6 rating all reviewers agreed that this paper is past the threshold for acceptance.  The  quality of the paper appears to have increased during the review cycle due to interactions with the reviewers. The paper addresses issues related to the quality of heterogeneous data sources. The paper does this through the framework of graph convolutional networks (GCNs). The work proposes a data quality level concept defined at each vertex in a graph based on a local variation of the vertex. The quality level is used as a regularizer constant in the objective function. Experimental work shows that this formulation is important in the context of time series prediction.  Experiments are performed on a dataset that is less prominent in the ML and ICLR community, from two commercial weather services Weather Underground and WeatherBug; however, experiments with reasonable baseline models using a "Forecasting mean absolute error (MAE)" metric seem to be well done.  The biggest weakness of this work was a lack of comparison with some more traditional time series modelling approaches. However, the authors added an auto regressive model into the baselines used for comparison. Some more details on this model would help.  I tend to agree with the author s assertion that: "there is limited work in ICLR  on data quality, but it is definitely one essential hurdle for any representation learning model to work in practice. ".  For these reasons I recommend a poster.  
 + An intriguing novel regularization method: encouraging larger norms for the feature vector input to the last softmax layer of a classifier.  + Resonably extensive experimental validation shows that it improves test accuracy to some degree.    While a motivation is given, the formal analysis of what is really going on remains very superficial and limited.  Technical note: Simply scaling the softmax layer s input would not change class rankings, so any positive effect of this regularizer on classification performance is due to it changing the learning dynamic in the upper layers as well. The paper could be much stronger if it did provide an analysis regarding how the global learning dynamic is affected in all layers, by the interaction between weight decay and the last layer s feature incay. 
This paper proposes an interesting new idea which creates an interesting discussion. 
This paper provides a method for eliminating options in multiple answer reading comprehension tasks, based on the contents of the text, in order to reduce the "answer space" a machine reading model must consider. While there s nothing wrong with this, conceptually, reviewers have questioned whether or not this is a particularly useful process to include in a machine reading pipeline, versus having agents that understand the text well enough to select the correct answer (which is, after all, the primary goal of machine reading). Some reviewers were uncomfortable with the choice of dataset, suggesting SQuAD might be a better alternative), and why I am not sure I agree with that recommendation, it would be good to see stronger positive results on more than one dataset. At the end of the day, it is the lack of convincing experimental results showing that this method yields substantial improvements over comparable baselines which does the most harm to this well written paper, and I must recommend rejection.
All reviewers agree the paper proposes an interesting setup and the main finding that "prosocial agents are able to learn to ground symbols using RL, but self interested agents are not" progresses work in this area. R3 asked a number of detail oriented questions and while they did not update their review based on the author response, I am satisfied by the answers. 
None of the reviewers are enthusiastic about the paper, primarily due to lack of proper evaluation.  The response of the authors towards this criticism is also not sufficient.  The final results are mixed which does not show very clearly that the presented associative model performs better than the sole seq2seq baseline that the authors use for comparison.  We think that addressing these immediate concerns would improve the quality of this paper.
Pros: + The proposed large batch, synchronous SGD method is able to generalize at larger batch sizes than previous approaches (e.g., Goyal et al., 2017).  Cons:   Evaluation on more than one task would make the paper more convincing.   The addition of more hyperparameters makes the proposed algorithm less appealing.   Some theoretical justifiction of the layer wise rate scaling would help.   It isn t clear that the comparison to Goyal et al., 2017 is entirely fair, because that paper also had recommendations for the implementation of batch normalization, weight decay, and a momentum correction as the learning rate is scaled up, but this submission does not address any of those.  Although the revised paper addressed many of the reviewers  concerns, they still did not feel it was quite strong enough to be accepted to ICLR. 
The paper received scores of 8 (R1), 6 (R2), 6 (R3). R1 s review is brief, and also is optimistic that these results demonstrated on ConvACs generalize to real convnets.  R2 and R3 feel this might be a potential problem. R2 advocates weak accept and given that R1 is keen on the paper, the AC feels it can be accepted.  
This paper addresses the very important problem of ensuring that sensitive training data remain private. It proposes an attack whereby the attacker can reconstruct information about the training data given only the trained classifier and an auxiliary dataset. If done well, such an attack would be a useful contribution that helps make discussion of differential privacy more complete. But as the reviewers pointed out, it s not clear from the paper whether the attack has succeeded. It works only when the auxiliary data is very similar to the training data, and it s not clear if it leaks information about the training set itself, or is just summarizing the auxiliary data. This work doesn t seem quite ready for publication, but could be a strong paper if it s convincingly demonstrated that information about the training set has been leaked.  
The paper is looking at an interesting problem, but it seems too early. The approach requires training a new language model  from scratch for each new word, rendering it completely impractical for real use. The main evaluation therefore only considers four words   "bonuses", "explained", "marketers", "strategist" (expanded to 20 during the rebuttal). This is not sufficient for ICLR.
Thank you for submitting you paper to ICLR. The reviewers agree that the paper‚Äôs development of action dependent baselines for reducing variance in policy gradient is a strong contribution and that the use of Stein s identity to provide a principled way to think about control variates is sensible. The revision clarified an number of the reviewers‚Äô questions and the resulting paper is suitable for publication in ICLR.
All of the reviewers agree that the paper clearly presents promising ideas in developing a novel actor critic algorithm. The experiments do not show a significant gain against the baselines, but they support the presented ideas. I appreciated the ablation study on dual AC.  Detailed comments: My understanding is that the x axis in Figures 1 & 2 shows the number of iterations each of which contains batch_size*1000 environment steps. It is more standard to show those plots in terms of the number of environment steps. Further, the optimal batch_size for different algorithms may be different, so using the same batch_size for all of the algorithms is not fair.
The paper studies the global convergence for policy gradient methods for linear control problems.  Multiple reviewers point out strong concerns about the novelty of the results.
This paper addresses the question of how to regularize when starting from a pre trained convolutional network in the context of transfer learning.  The authors propose to regularize toward the parameters of the pre trained model and study multiple regularizers of this type.  The experiments are thorough and convincing enough.  This regularizer has been used quite a bit for shallow models (e.g. SVMs as the authors mention, but also e.g. more general MaxEnt models).  There is at least some work on regularization toward a pre trained model also in the context of domain adaptation with deep neural networks (e.g. for speaker adaptation in speech recognition).  The only remaining novelty is the transfer learning context.  This is not a sufficiently different setting to merit a new paper on the topic.
Predicting graphs is an interesting and important direction, and there exist essentially no (effective) general purpose techniques for this problem.  The idea of predicting nodes one by one, though not entirely surprising, is interesting and the approach makes sense. Unfortunately, I (and some of reviewers) less convinced by evaluation:     For example, evaluation on syntactic parsing of natural language is very weak. First of all, the used metric   perplexity and exact match are non standard and problematic (e.g., optimizing exact match would largely correspond to ignoring longer sentences where predicting the entire tree is unrealistic).  Also the exact match scores are very low (~30% whereas 45+ were achieve by models back in 2010).    A reviewer had, I believe, valid concerns about comparison with GrammarVAE, which were not fully addressed.  Overall, I believe that it is interesting work, which regretfully cannot be published as a conference paper in its current form.  + important / under explored problem + a reasonable (though maybe not entirely surprising / original) approach   issues with evaluation  
The paper proposes a GAN model with adaptive convolution kernels. The proposed idea is reasonable, but the novelty is somewhat minor and the experimental results are limited. More comprehensive experiments (e.g., other evaluation metrics) will strengthen the future revision of paper. No rebuttal was submitted. 
All three reviewers are in agreement that this paper is not ready for ICLR in its current state. Given the pros/cons, the committee feels the paper is not ready for acceptance in its current form.
This work is proposing an approach for ensuring classification fairness through models that encapsulate deferment criteria. On the positive side, the paper provides ideas which are conceptually interesting and novel. On the other hand, the reviewers find the technical contribution to be limited and, in some cases, challenge the practicality of the method (e.g. requirement for second set of training samples). After extensive post rebuttal discussion, the consensus is that the above issues make the paper fall below the threshold for acceptance ‚Äì even if the ‚Äúout of scope‚Äù issue is not taken into account.
The submission proposes optimization with hard threshold activations.  This setting can lead to compressed networks, and is therefore an interesting setting if learning can be achieved feasibly.  This leads to a combinatorial optimization problem due to the non differentiability of the non linearity.  The submission proceeds to analyze the resulting problem and propose an algorithm for its optimization.  Results show slight improvement over a recent variant of straight through estimation (Hinton 2012, Bengio et al. 2013), called saturated straight through estimation (Hubara et al., 2016).  Although the improvements are somewhat modest, the submission is interesting for its framing of an important problem and improvement over a popular setting.
While the reviewers considered the basic idea of adding supervision intermediate to differentiable programming style architectures to be interesting and worthy of effort, they were unsure if 1: the proposed abstractions for discussing ntm and nram are well motivated/more generally applicable 2: the methods used in this work to give intermediate supervision are more generally applicable  
The pros and cons of the paper are summarized below:  Pros: * The proposed tweaks to the dynamic evaluation of Mikolov et al. 2010 are somewhat effective, and when added on top of already strong baseline models improve them substantially  Cons: * Novelty is limited. This is essentially a slightly better training scheme than the method proposed by Mikolov et al. 2010. * The fair comparison against Mikolov et al. 2010 is only shown in Table 1, where a perplexity of 78.6 turns to a perplexity of 73.5. This is a decent gain, but the great majority of this is achieved by switching the optimizer from SGD to an adaptive method, which as of 2018 is a somewhat limited contribution. The remainder of the tables in the paper do not compare with the method of Mikolov et al. * The paper title, abstract, and introduction do not mention previous work, and may give the false impression that this is the first paper to propose dynamic evaluation for neural sequence models, significantly overclaiming the paper s contribution and potentially misleading readers.  As a result, while I think that dynamic evaluation itself is useful, given the limited novelty of the proposed method and the lack of comparison to the real baseline (the simpler strategy of Mikolov et al.) in the majority of the experiments, I think this papers till falls short of the quality bar of ICLR.  Also, independent of this decision, a final note about perplexity as an evaluation measure to elaborate on the comments of reviewer 1. In general, perplexity is an evaluation measure that is useful for comparing language models of the same model class, but tends to not correlate well with model performance (e.g. ASR accuracy) across very different types of models. For example, see "Evaluation Metrics for Language Models" by Chen et al. 1998. The method of dynamic evaluation is similar to the cache based language models that existed in 1998 in that it reinforces the model to choose similar vocabulary to that it s seen before. As you can see from this paper that the quality of perplexity of an evaluation measure falls when cache based models are thrown into the mix, and one reason for this is that cache models, while helping perplexity greatly, tend to reinforce previous errors when errors do occur.
This paper proposes an approach for jointly learning a label embedding and prediction network, as a way of taking advantage of relationships between labels.  This general idea is well motivated, but the specifics of the proposed approach are not motivated or described well.  More discussion of relationship with prior work (e.g. other ways of "softening" the softmax) is needed.  The authors claim to have state of the art results, but reviewers point out that much better results exist.
This paper shows that batch normalization can be cast as approximate inference in deep neural networks.  This is an appealing result as batch normalization is used in practice in a wide variety of models.   The reviewers found the paper well written and easy to understand and were motivated by underlying idea.  However, they found the empirical analysis lacking and found that there was not enough detail in the main text to verify whether the claims were true.  The authors empirically compared to a recent method showing that dropout can be cast as approximate inference with the claim that by transitivity they were comparing to a variety of recent methods.  AnonReviewer1 casts significant doubt on the results of that work.  This is very unfortunate and not the fault of the authors of this paper.  The authors have since gone to great length to compare to Louizos and Welling, 2017.  Unfortunately, that comparison doesn t appear to be complete in the manuscript.  The main text was also lacking specific detail relating to fundamental parts of the proposed method (noted by all reviewers).  Overall, this paper seems to be tremendously promising and the underlying idea potentially very impactful.  However, given the reviews, it doesn t seem that this paper would achieve its potential impact.  The response from the authors is appreciated and goes a long way to improving the paper.  Taking the reviews into account, adding specific detail about the methodology and model (e.g. the prior) and completing careful empirical analysis will make this a strong paper that should be much more impactful.
This paper proposes a method for having a meta deep learning model generate the weights of a main model given a proposed architecture.  This allows the authors to search over the space of architectures efficiently.  The reviewers agreed that the paper was very well composed, presents an interesting and thought provoking idea and provides compelling empirical analysis.  An exploration of the failure modes of the approach is highly appreciated.  The lowest score was also of quite low confidence, so the overall score should probably be one point higher.  Pros:   Very well written and composed   "Thought provoking"   Some strong experimental results   Analysis of weaker experimental results (failure modes)  Cons:   Some weak results (also in pros, however)
Two of the reviewers liked the intent of the paper   to analyze gradient flow in residual networks and understand the tradeoffs between width and depth in such networks.  However, all reviewers flagged a number of problems in the paper, and the authors did not participate in the discussion period.  Pros: + Interesting analysis suggests wider, shallower ResNets should outperform narrower, deeper ResNets, and empirical results support the analysis.  Cons:   Independence assumption on weights is not valid after any weight updates.   The notation is not as clear as it should be.   Empirical results would be more convincing if obtained on several tasks.   The architecture analyzed in the paper is not standard, so it isn t clear how relevant it is for other practitioners.   Analysis and paper should take into account other work in this area, e.g. Veit et al., 2016 and Schoenholz et al., 2017. 
This paper presents a more complex version of the grammar VAE, which can be used to generate structured discrete objects for which a grammar is known, by adding a second  attribute grammar , inspired by Knuth.  Overall, the idea is a bit incremental, but the space is wide open and I think that structured encoder/decoders is an important direction.  The experiments seem to have been done carefully (with some help from the reviewers) and the results are convincing.
While the reviewers feel there might be some merit to this work,  they find enough ambiguities and inaccuracies that I think this paper would be better served by a resubmission.
meta score: 7  The paper introduces an online distillation technique to parallelise large scale training.  Although the basic idea is not novel, the presented experimentation indicates that the authors  have made the technique work.  Thus this paper should be of interest to practitioners.  Pros:    clearly written, the approach is well explained    good experimentation on large scale common crawl data with 128 256 GPUs    strong experimental results  Cons:    the idea itself is not novel    the range of experimentation could be wider (e.g. different numbers of GPUs) but this is expensive!  Overall the novelty is in making this approach work well in practice, and demonstrating it experimentally.
The paper proposes a new model called differential decision tree which captures the benefits of decision trees and VAEs. They evaluate the method only on the MNIST dataset. The reviewers thus rightly complain that the evaluation is thus insufficient and one also questions its technical novelty.
This work shows interesting potential applications of known machine learning techniques to the practical problem of how to devise a retina prosthesis that is the most perceptually useful. The paper suffers from a few methodological problems pointed out by the reviewers (e.g., not using the more powerful neural network encoding in the subsequent experiments of the paper), but is still interesting and inspiring in its current state.
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
This is an interesting piece of work that provides solid evidence on the topic of bootstrapping in deep reinforcement learning.
Thank you for submitting you paper to ICLR. The paper studies an interesting problem and the solution, which fuses student teacher approaches to continual learning and variational auto encoders, is interesting. The revision of the paper has improved readability. However, although the framework is flexible, it is complex and appears rather ad hoc as currently presented. Exploration of the effect of the many hyper parameters or some more supporting theoretical work / justification would help. The experimental comparisons were varied, but adding more baselines e.g. comparing to a parameter regularisation approach like EWC or synaptic intelligence applied to a standard VAE would have been enlightening.  Summary: There is the basis of a good paper here, but a comprehensive experimental evaluation of design choices or supporting theory would be useful for assessing what is a complex approach.
The paper proposes adversarial flow based neural network architecture with adversarial training for video prediction. Although the reported experimental results are promising, the paper seems below ICLR threshold due to limited novelty and issues in evaluation (e.g., mechanical turk experiment). No rebuttal was submitted.
The idea of using wavelet pooling is novel and will bring many interesting research work in this direction. But more thorough experimental justification such as those recommended by the reviewers would make the paper better. Overall, the committee feels this paper will bring value to the conference.
This paper presents a somewhat new approach to training neural nets with ternary or low precision weights.  However the Bayesian motivation doesn t translate into an elegant and self tuning method, and ends up seeming kind of complicated and ad hoc.  The results also seem somewhat toy.  The paper is fairly clearly written, however.
The proposed Bi BloSAN is a two levels  block SAN, which has both parallelization efficiency and memory efficiency. The study is thoroughly conducted and well presented.  
This work tackles an important problem of incremental learning and does so with extensive experimentation. As pointed out by two reviewers, the idea does seem novel and interesting, but the submission would require some rewriting before being potentially accepted at a venue like ICLR. I suggest focusing the paper more on the task incremental learning aspects, doing the ablation studies (and other changes) as requested by the reviewers, and having a rich appendix with details (with more discussion in the paper itself).
This paper proposes an interesting machinery around Generative Adversarial Networks to enable sampling not only from conditional observational distributions but also from interven¬≠tional distributions. This is an important contribution as this means that we can obtain samples with desired properties that may not be present in the training set; useful in applications such as ones involving fairness and also when data collection is expensive and biased. The main component called the causal controller models the label dependencies and drives the standard conditional GAN. As reviewers point out, the causal controller assumes the knowledge of the causal graph which is a limitation as this is not known a priori in many applications. Nevertheless, this is a strong paper that convincingly demonstrates a novel approach to incorporate causal structure into generative models. This should be of great interest to the community and may lead to interesting applications that exploit causality. I recommend acceptance. 
This paper proposes a way to automatically weight different tasks in a multi task setting.  The problem is a bit niche, and the paper had a lot of problems with clarity, as well as the motivation for the experimental setup and evaluation.
The paper contains an interesting way to do online multi task learning, by borrowing ideas from active learning and comparing and contrasting a number of ways on the arcade learning environment.  Like the reviewers, I have some concerns about using the target scores and I think more analysis would be needed to see just how robust this method is to the choice/distribution of target scores (the authors mention that things don t break down as long as the scores are "reasonable", but that s not a particularly empirical nor precise statement).  My inclination is to accept the paper, because of the earnest efforts made by the authors in understanding how DUA4C works. However, I do agree that the paper should have a larger focus on that: basically Section 6 should be expanded, and the experiments should be rerun in such a way that the setup for DUA4C is more "favorable" (in terms of hyper parameter optimization).  If there s any gap between any of the proposed methods and DUA4C, then this would warrant further analysis of course (since it would mean that there s an advantage to using target scores).  
This paper looks at  building new density estimation methods and new methods for tranformations and autoregressive models. The request from reviewers for comparison improves the paper. These models have seen a wide range of applications and have been highly successful, needing the added benefits shown and their potential impact to be expanded further.
This paper presents several theoretical results linking deep, wide neural networks to GPs.  It even includes illuminating experiments.  Many of the results were already developed in earlier works. However, many at ICLR may be unaware of these links, and we hope this paper will contribute to the discussion. 
Theoretical analysis and understanding of DNNs is a crucial area for ML community. This paper studies characteristics of the relu DNNs and makes several important contributions.
The authors study the problem of reducing uplink communication costs in training a ML model where the training data is distributed over many clients.   The reviewers consider the problem interesting, but have concerns about the extent of the novelty of the approach.  As the reviewers and authors agree that the paper is an empirical study, and the authors agree that the novelty is in the problem studied and the combination of approaches used, a more thorough experimental analysis would benefit the paper.
Three reviewers recommended rejection and there was no rebuttal.
 A clearly written paper. While the practical relevance that came up in the review remains, the analysis and discussion is important for a deeper understanding of the deeper connections between these two important areas of machine learning.
  The paper is overall difficult to read and would benefit from a revised presentation.   The practical relevance of the recovery conditions and algorithmic consequences of the work is not sufficiently clear or  convincing.
The paper proposes two regularizers for encouraging "clustered feature embeddings" (use of "disentangled" in the title is misleading). Reviewers have raised points about the lack of proper motivation and justification of the regularizers. There are also concerns on the experiments conducted to evaluate the method, including for hierarchical classification setting. Missing comparison with relevant baselines has also been pointed out as a weakness. I feel the work is not yet mature. 
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * Empirical results demonstrate decent improvements over other reasonable models * The method is well engineered to the task  Cons: * The paper is difficult to read due to grammar and formatting issues * Experiments are also lacking detail and potentially difficult to reproduce * Some of the experimental results are suspect in that the train/test accuracy are basically the same. Usually we would expect train to be much better in highly parameterized neural models * The content is somewhat specialized to a particular task in NLP, and perhaps of less interest to the ICLR audience as a whole (although I realize that ICLR is attempting to cast a broad net so this alone is not a reason for rejection of the paper)  In addition to the Cons cited by the reviewers above, I would also note that there is some relevant work on morphology in sequence to sequence models, e.g.: * "What do Neural Machine Translation Models Learn about Morphology?" Belinkov et al. ACL 2017.  and that it is common in sequence to sequence models to use sub word units, which allows for better handling of morphological phenomena: * "Neural Machine Translation of Rare Words with Subword Units" Sennrich et al. ACL 2016.  While the paper is not without merit, given that the cons seem to significantly outweigh the pros, I don t think that it is worthy of publication at ICLR at this time, although submission to a future conference (perhaps NLP conference) seems warranted.
Reviewers concur that the paper and the application area are interesting but that the approaches are not sufficiently novel to justify presentation at ICLR. 
The paper extends an existing work with three different frequency representations of audios and necessary network structure modifications for music style transfer. It is an interesting study but does not provide "sufficiently novel or justified contributions compared to the baseline approach of Ulyanov and Lebedev". Also the revisions can not fully address reviewer 2 s concerns.  
The paper tries to show that many of the state of the art interpretability methods are brittle and do not provide consistent stable explanations. The authors show this by perturbing (even randomly) the inputs so that the differences are imperceptible to a human observer but the interpretability methods provide completely different explanations. Although the output class is maintained before and after the perturbation it is not clear to me or the reviewers why one shouldn t have different explanations. The difference in explanations can be attributed to the fragility of the learned models (highly non smooth decision boundaries) rather than the explanation methods. This is a critical point and has to come out more clearly in the paper.
This paper proposes a new metric to evaluate the robustness of neural networks to adversarial attacks. This metric comes with theoretical guarantees and can be efficiently computed on large scale neural networks.  Reviewers were generally positive about the strengths of the paper, especially after major revisions during the rebuttal process. The AC believes this paper will contribute to the growing body of literature in robust training of neural networks.  
Well motivated and well written, with extensive results. The paper also received positive comments from all reviewers. The AC recommends that the paper be accepted.
This paper gives a scalable Laplace approximation which makes use of recently proposed Kronecker factored approximations to the Gauss Newton matrix. The approach seems sound and useful. While it is a rather natural extension of existing methods, it is well executed, and the ideas seem worth putting out there. 
The authors appear to have largely addressed the concerns of the reviewers and commenters regarding related work and experiments. The results are strong, and this will likely be a useful contribution for the graph neural network literature.
All reviewers have acknowledged that the proposed regularization is novel and also results in some empirical improvements on the reported language modeling and image classification tasks. However there are serious concerns on writing and rigor (reviewers Anon1 and Anon3) of the paper. The authors have not uploaded any revision of the paper to address these concerns.
The paper proposes to add noise to the weights of a policy network during learning in Deep RL settings and finds that this results in better performance on DQN, A3C and other algorithms that use other exploration strategies. Unfortunately, the paper does not do a thorough job of exploring the reasons and doesn t offer a comparison to other methods that have been out on arxiv for several months before the submission, in spite of reviewers and anonymous requests. Otherwise I might have supported recommending the paper for a talk. 
The authors propose to use attention over past time steps to try and solve the gradient flow problem in learning recurrent neural networks. Attention is performed over a subset of past states by a hueristic that boils to selecting best time steps.  I agree with the authors that they offer a lot of comparisons, but like the reviewers, I am inclined to find the experiments not very convincing of the arguments they are attempting to make.  The model that they propose has similarities to seq2seq in that they use attention to pass more information in the forward pass; in a sense this is a seq2seq model with the same encoder and decoder, and there are parallels to self attention. The model also has similarities to clockwork RNNs and other skip connection methods.. However, the experiments offered to not tease out these effects. It is unsurprising that a fixed size neural network is unable to do a long copy task perfectly, but an attention model can. What would have been more interesting would have been to explore if other RNN models could have done so. The experiments on pMNIST aren t really compelling as the baselines are far from SOTA (example: https://arxiv.org/pdf/1606.01305.pdf report 0.041 error rate (95.9% test acc) with LSTMs and regularization).  Text8 also shows worse results in full BPTT on LSTM.  If BPTT is consistently better than this method, it defeats the argument that gradient explosion and forgetting over long sequences is a problem for RNNs (one of the motivations offered for this attention model). 
this work adapts cycle GAN to the problem of decipherment with some success. it s still an early result, but all the reviewers have found it to be interesting and worthwhile for publication.
The paper proposes a new deep architecture based on polar transformation for improving rotational invariance. The proposed method is interesting and the experimental results strong classification performance on small/medium scale datasets (e.g., rotated MNIST and its variants with added translations and clutters, ModelNet40, etc.). It will be more impressive and impactful if the proposed method can bring performance improvement on large scale, real datasets with potentially cluttered scenes (e.g., Imagenet, Pascal VOC, MS COCO, etc.).
Pros: + Clearly written paper. + Easily implemented algorithm that appears to have excellent scaling properties and can even improve on validation error in some cases. + Thorough evaluation against the state of the art.  Cons:   No theoretical guarantees for the algorithm.  This paper belongs in ICLR if there is enough space. 
Pros: + Clearly written paper. + Good theoretical analysis of the expressivity of the proposed model. + Efficient model update is appealing. + Reviewers appreciated the addition of results on the copy and adding tasks in Appendix C.  Cons:   Evaluation was on less standard RNN tasks.  A language modeling task should have been included in the empirical evaluation because language modeling is such an important application of RNNs.  This paper is close to the decision boundary, but the reviewers strongly felt that demonstration of the method on a language modeling task was necessary for acceptance. 
Meta score: 4  The paper concerns the development of a density network for estimating uncertainty in recommender systems.  The submitted paper is not very clear and it is hard to completely understand the proposed method from the way it is presented.  This makes assessing the contribution of the paper  difficult.  Pros:    addresses an  interesting and important problem    possible novel contribution  Cons:    poorly written, hard to understand precisely what is done    difficult to compare with the state of the art, not helped by disorganised literature review    experimentation could be improved  The paper needs more work before being ready for publication.
Authors present an evaluation of end to end training connecting reconstruction network with detection network for lung nodules.  Pros:   Optimizing a mapping jointly with the task may preserve more information that is relevant to the task.  Cons:   Reconstruction network is not "needed" to generate an image   other algorithms exist for reconstructing images from raw data. Therefore, adding the reconstruction network serves to essentially add more parameters to the neural network. As a baseline, authors should compare to a detection only framework with a comparable number of parameters to the end to end system. Since this is not provided, the true benefit of end to end training cannot be assessed.    Performance improvement presented is negligible    Novelty is not clear / significant
The paper received generally positive reviews, but the reviewers also had some concerns about the evaluations.  Pros:   An improvement over HashNet, the model ties weights more systematically, and gets better accuracy. Cons:   Tying weights to compress models already tried before.   Tasks are all small and/or audio related.   Unclear how well the results will generalize for 2D convolutions.   HashNet results are preliminary; comparisons with HashNet missing for audio tasks.  Given the expert reviews, I am recommending the paper to the workshop track. 
While one reviewer did upgrade their Rating from 6 to 7, the most negative reviewer maintains: "Overall, I find this work interesting and current results surprising. However, I find it to be a preliminary work and not yet ready for publication. The paper still lacks a conclusion / a leading hypothesis / an explanation for the shown results. I find this conclusion indispensable even for a small scientific study to be published." after the rebuttal. With scores of 7 5 4 it is just not possible for the AC to recommend acceptance.
The reviewers agree this is an interesting paper with interesting ideas, but is not ready for publication in its current shape. In particular, there is a need for strong empirical results.
This an interesting new contribution to construction of random features for approximating kernel functions. While the empirical results look promising, the reviewers have raised concerns about not having insights into why the approach is more effective;  the exposition of the quadrature method is difficult to follow; and the connection between the quadrature rules and the random feature map is never explicitly stated. Some comparisons are missing (e.g., QMC methods). As such the paper will benefit from a revision and is not ready for ICLR 2018 acceptance.
The paper addresses and interesting problem, but the reviewers found that the paper is not as strong as it could be: improving the range of evaluated data (significantly improve the convincingness of the experiments, and clearly adressing any alternatives, their limitations and as baselines).
While the problem of learning word embeddings for a new domain is important, the proposed method was found to be unclearly presented and missing a number of important baselines. The reviewers found the technical contribution to be of only limited value.
This paper forms a good contribution to the active area of adversarial training.  The main issue with the original submission was presentation quality and excessive length.  The revised version is much improved.  However, it still needs some work on the writing, in large part in the transferability section but also to clean up a large number of non native formulations like missing/extra determiners and some awkward phrasing.  It should be carefully proofread by a native English speaker if possible.  Also, the citation formatting is incorrect (frequently using \citet instead of \citep).
This is a strong paper presenting a very clean proof of a result that is similar, though now incomparable to one due to Bartlett et al. These bounds (and Bartlett s) are among the most promising norm based bounds for NNs.  I would simply add that the citation of Dziugaite and Roy (2017) could be improved. There work also connects sharpness (or flatness) with generalization via the PAC Bayes framework, and moreover, there bounds are nonvacuous.  Are the bounds in this paper nonvacuous, say, on MNIST for 60,000 training data, for the network learned by SGD?  If not, how close do they get to 1.0?
The paper shows that many of the current state of the art interpretability methods are inaccurate even for linear models. Then based on their analysis of linear models they propose a technique that is thus accurate for them and also empirically provides good performance for non linear models such as DNNs.
the reviewers all found the problem to be important, the proposed approach to be interesting, but the manuscript to be preliminary. i agree with them.
This paper presents an interesting model which at the time of submission was still quite confusingly described to the reviewers. A lot of improvements have been made for which I applaud the authors. However, at this point, the original 20 babi tasks are not quite that exciting and several other models are able to fully solve them as well. I would encourage the authors to tackle harder datasets that require reasoning or multitask settings that expand beyond babi.  
Very solid paper exploring an interpretation of LSTMs. good reviewss
This paper extends work on neural architecture search by introducing a new framework for searching and experiments on new domains of NMT and QA. The results of the work are  beneficial and show improvements using this approach. However the reviewers point out significant issues with the approach itself:    There is skepticism about the use of NAS in general, particular compared to using the same computational power for other types of simpler hyperparameter search.   There is general concern about the use of such large scale brute force methods in general. Several of the reviewers expressed concerns about ever possibly being able to replicate these results.   Given the computational power required, the reviewers feel like the gains are not particularly large, for instance the Squad results not being compared to the best reported systems.  
I (and some of the reviewers) find the general motivation quite interesting (operationalizing the Gricean maxims in order to improve language generation). However, we are not  convinced that the actual model encodes these maxims in a natural and proper way.  Without this motivation, the approach can be regarded as a set of heuristics which happen to be relatively effective on a couple of datasets.  In other words, the work seems too preliminary to be accepted at the conference.  Pros:   Interesting motivation (and potential impact on follow up work)   Good results on a number of datasets Cons:   The actual approach can be regarded as a set of heuristics, not necessarily following from the maxims   More serious evaluation needed (e.g., image captioning or MT) and potential better ways of encoding the maxims  It is suitable for the workshop track, as it is likely to stimulate an interesting discussion and more convincing follow up work.  
This is a good paper with strong results via a set of simple steps for post processing off the shelf words  embeddings.  Reviewers are enthusiastic about it and the author responses are satisfactory.
The paper studies transferability of adversarial examples between model architectures, and proposes a method to improve this transferability. Whilst it covers an interesting and relevant line of research, the paper does not provide strong evidence for its main underling hypothesis: namely, that adversarial perturbations can be split in a model specified and a data specific part. The paper s presentation also warrants improvements. The authors have not provided a rebuttal.
The reviewers agree that the idea of incorporating humans in the training of generative adversarial networks is interesting and worthwhile exploring. However, they felt that the paper fell short in providing strong support for their approach. The AC agrees. The authors are encouraged to strengthen their work and resubmit to a future venue.
This paper received divergent ratings (7, 3, 3). While there is value in thorough evaluation papers, this manuscript has significant presentation issues. As all three reviewers point out, the way it is currently written, it misrepresents the claims made by Mirowski et al 2016 and over reaches in its findings. Unfortunately, we cannot make a decision on what the manuscript may look like in future once these issues are fixed, and must reject. 
The paper considers learning an NMT systems while pivoting through images. The task is formulated as a referential game. From the modeling and set up perspective it is similar to previous work in the area of emergent communication / referential games, e.g., Lazaridou et al (ICLR 17) and especially to Havrylov & Titov (NIPS 17), as similar techniques are used to handle the variable length channel (RNN encoders / decoders + the ST Gumbel Softmax estimator).  However, its multilingual version is interesting and the results are sufficiently convincing (e.g., comparison to Nakayama and Nishida, 17). The paper would more attractive for those interested in emergent communication than the NMT community, as the set up (using pivoting through images) may be perceived somewhat exotic by the NMT community. Also, the model is not attention based (unlike SoA in seq2seq / NMT), and it is not straightforward to incorporate attention (see R2 and author response).  + an interesting framing of the weakly supervised MT problem + well written + sufficiently convincing results   the set up and framework (e.g., non attention based) is questionable from practical perspective 
The paper presents a differentiable upper bound on the performance of classifier on an adversarially perturbed example (with small perturbation in the L infinity sense). The paper presents novel ideas, is well written, and appears technically sound. It will likely be of interest to the ICLR community.  The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defenses on a dataset like ImageNet.
This paper generated quite a bit of controversy among reviewers. The main claim of the paper is that Adam and related optimizers are broken because their "weight decay" regularization is not actually weight decay. It proposes to modify Adam to decay all weights the same regardless of the gradient variances.  Calling Adam s weight decay mechanism a mistake seems very far fetched to me. Neural net optimization researchers are well aware of the connection between weight decay and L2 regularization and the fact that they don t correspond in preconditioned methods. L2 regularization is basically the only justification I have heard for weight decay, and despite rejecting this interpretation, the paper does not provide an alternative justification.  Decoupling the optimization from the cost function is a well established principle. This abstraction barrier is not completely clean (e.g. gradient noise has well known regularization effects), and the experiments of this paper perhaps provide evidence that the choices may be coupled in this case. This is an interesting finding, and probably worth following up on. However, the paper seems to sweep the "decoupling optimization and cost" issue under the carpet and take for granted that the decay rate is what should be held fixed. All three reviewers found the presentation to be misleading, and I would agree with them. While there may be an interesting contribution here, I cannot endorse the paper as is. 
All the reviewers noted that the dual formulation, as presented, only applies to the logistic family of classifiers. The kernelization is of course something that *can* be done, as argued by the authors, but is not in fact approached in the submission, only in the rebuttal. The toy ish nature of the problems tackled in the submission limits the value of the presentation.  If the authors incorporate their domain adaptation results (SVHN >MNIST and others) using the kernelized approach and do the stability analysis for those cases, and obtain reasonable results on domain adaptation benchmarks (70% on SVHN >MNIST is for instance on the low side compared to the pixel transfer based GAN approaches out there!) then I think it d be a great paper.  As such, I can only recommend it as an invitation to the workshop track, as the dual formulation is interesting and potentially useful.
The authors present a centralized neural controller for multi agent reinforcement learning.   The reviewers are are not convinced that there is sufficient novelty, considering the authors setup as essentially a special case of other recent works, with added adjustments to the neural networks that are standard in the literature.  I personally am more bullish about this paper than the reviewers, as I think engineering an architecture to perform well in interesting scenarios is worth reporting.  However, the reviewers are mostly in agreement, and their reviews were neither sloppy nor factually incorrect.  So I will recommend rejection, following their judgement.  Nevertheless, I encourage the authors to continue strengthening the results and the presentation and resubmit.  
The paper studies end to end training of a multi branch convolutional network. This appears to lead to strong accuracies on the CIFAR and SVHN datasets, but it remains unclear whether or not this results transfers to ImageNet. The proposed approach is hardly novel, and lacks a systematic comparison with "regular" ensembling methods and with related mixture of experts approaches (for instance: S. Gross et al. Hard Mixtures of Experts for Large Scale Weakly Supervised Vision, 2017; Shazeer et al. Outrageously Large Neural Networks: The Sparsely Gated Mixture of Experts Layer, 2017).
Reviewers recognize that the method proposed is somewhat novel but have strong reservations on the experimental evaluation. Discussion of some relevant papers is also missing (eg, Li et al, 2017 : ALICE). The authors have not responded to the many concerns expressed by the reviewers. 
Paper explore depth wise separable convolutions for sequence to sequence models with convolutions encoders. R1 and R3 liked the paper and the results. R3 thought the presentation of the convolutional space was nice, but the experiments were hurried. Other reviewers thought the paper as a whole had dense parts and need cleaning up, but the authors seem to have only done this partially. From the reviewers comments, I m giving this a borderline accept. I would have been feeling much more comfortable with the decision if the authors had incorporated the reviewers  suggestions more thoroughly..
The reviewers are unanimous in accepting the paper.  They generally view it as introducing an original approach to online RL using bandit style selection from a fixed portfolio of off policy algorithms.  Furthermore, rigorous theoretical analysis shows that the algorithm achieves near optimal performance.  The only real knock on the paper is that they use a weak notion of regret i.e. short sighted pseudo regret.  This is considered inevitable, given the setting.
This paper was reviewed by 3 expert reviews and received largely negative reviews, with concerns about the toy ish nature of the 2D environments and limited novelty.  Since ICLR18 received multiple papers on similar topics, we took additional measures to ensure that papers were similar papers were judged under the same criteria. Specifically, we asked reviewers of (a) this paper and (b) of a concurrent submission that also studies language grounding in 2D environments to provide opinions on (b) and (a) respectively. Unfortunately, while they may be on similar topic and both working on 2D environments, we received unanimous feedback that (b) was much higher quality ("comparison with multiple baselines, better literature review, no bold claims about visual attention, etc). We realize this may be disappointing but we encourage the authors to incorporate reviewer feedback to make their manuscript stronger. 
Since this seems interesting, I suggest to accept this paper at the conference. However, there are still some serious issues with the paper, including missing references. 
The authors propose to use identity + some weights in the recurrent connections to prevent vanishing gradients. The reviewers found the experiments to have weak baselines, weakening the claims of the paper.
Pros   A novel way to incorporate LM into an end to end model, with good adaptation results.  Cons   Lacks results on public corpora or results are not close to SOTA (e.g., for Librispeech).  Given the reviews, it is clear that the experimental evaluations can be improved. But the presented approach is novel and interesting. Therefore I am recommending the paper to the workshop track.
Evaluating simple baselines for continuous control is important and nearest neighbor search methods are interesting. However, the reviewers think that the paper lacks citation and comparison to some prior work and evaluation on more challenging benchmarks.
While the reviewers agree that this is an important topic, there are numerous concerns novelty, correctness and limitations. 
This is a nice but very narrow study of domain invariance in a microscopic imaging application.  Since the problem is very general, the paper should include much more substantial context, e.g. discussion of various alternative methods (e.g. the ones cited in Sun et al. 2017).  In order to contribute to the broader ICLR community, ideally the paper would also include application to more than just the one task.
The paper proposes a VAE variant by embedding spatial information with multiple layers of latent variables. Although the paper reports state of the art results on multiple datasets, some results may be due to a bug. This has been discussed, and the author acknowledges the bug. We hope the problem can be fixed, and the paper reconsidered at another venue. 
The paper presents an adversarial learning framework for reading comprehension.  Although the idea is interesting and presents an approach that ideally would make reading comprehension approaches more robust, the results are not substantially solid (see reviewer 3 s comments) compared to other baselines to warrant acceptance.  Comments from reviewer 2 are also noteworthy where they mention that adversarial perturbations to a context around an answer can alter the facts in the context, thus destroying the actual information present there, and the rebuttal does not seem to satisfy the concern.  Addressing these issues will strengthen the paper for a potential future venue.
The contribution of this paper basically consists of using MLPs in the attention mechanism of end 2 end memory networks. Though it leads to some improvements on bAbI (which may not be so surprising   MLP attention has been shown preferable in certain scenarious), it does not seem to be a sufficient contribution. The motivation is also confusing   the work is not really that related to relation networks, which were specifically designed to deal with situations where *relations* between objects matter. The proposed architecture does not model relations.  +  improvement on bAbI over the baselines    limited novelty (MLP attention is fairly standard)    the presentation of the idea is confusing (if the claim is about relations  > other datasets need to be considered)  There is a consensus between reviewers. 
The reviewers were uniformly unimpressed with the contributions of this paper. The method is somewhat derivative and the paper is quite long and lacks clarity. Moreover, the tactic of storing autoencoder variables rather than full samples is clearly an improvement, but it still does not allow the method to scale to a truly lifelong learning setting. 
This paper has some interesting ideas that have been implemented in a rather ad hoc way; the presentation focuses perhaps too much on engineering aspects.
This paper had some quality and clarity issues and the lack of motivation for the approach was pointed out by multiple reviewers.  Just too far away from the acceptance threshold.
The paper studies factorizations of convolutional kernels. The proposed kernels lead to theoretical and practical efficiency improvements, but these improvements are very, very limited (for instance, Figure 5). It remains unclear how they compare to popular alternative approaches such as group convolutions (used in ResNeXt) or depth separable convolutions (used in MobileNet). The reviewers identify a variety of smaller issues with the manuscript.
The paper presents an interesting view of ResNets and the findings should be of broad interest. R1 did not update their score/review, but I am satisfied with the author response, and recommend this paper for acceptance. 
This work develops importance weighted autoencoder like training but with sequential Monte Carlo.  The paper is interesting, well written and the methods are very timely (there are two highly related concurrent papers    Naesseth et al. and Maddison et al.).  Initially, the reviewers shared concerns about the technical details of the paper, but the authors appear to addressed those and two reviews have been raised accordingly.   There is one outlier review (two 7s and one 3).  The 3 is the least thorough and has the lowest confidence (2) so that review is being weighted accordingly.  This appears to be a timely and interesting paper that is interesting to the community and warrants publication at ICLR.  Pros:   Well written and clear   An interesting approach   Neat technical innovations   Generative deep models are of great interest to the community (e.g. Variational Autoencoders)  Cons:   Could include a better treatment of recent related literature   Leaves a variety of open questions about specific details (i.e. from the reviews)
This paper on automatic option discovery connects recent research on successor representations with eigenoptions. This is a solidly presented, conceptual paper with results in tabular and atari environments. 
This paper attempts to connect the expressivity of neural networks with a measure of topological complexity. The authors present some empirical results on simplified datasets. All reviewers agreed that this is an intriguing line of research, but that the current manuscript is still presenting preliminary results, and that further work is needed before it can be published. 
The results in the paper are interesting, and the modifications improve the paper further. Reviewers found teh paper interesting and potentailly applicable to many models.
This work attempts to incorporate affect information from additional resources into word embeddings. This is a valuable goal, but the methods used are very similar to existing ones, and the experimental results are not quite convincing enough to make a strong enough case for accepting the paper.
This paper proposes a tree structured tensor factorisation method for parameter reduction. The reviewers felt the paper was somewhat interesting, but agreed that more detail was needed in the method description, and that the experiments were on the whole uninformative. This seems like a promising research direction which needs more empirical work, but is not ready for publication as is.
The reviewers agree this is a really interesting paper, with an interesting idea (in particular the use of regret clipping might provide a benefit over typical policy gradient methods). However, there are two major concerns: 1) clarity / exposition and more importantly 2) lack of a strong empirical motivation for the new approach (why do standard methods work just as well on these partially observable domains?).
This paper proposes improvements to WaveNet by showing that increasing connectivity provides superior models to increasing network size. The reviewers found both the mathematical treatment of the topic and the experiments to be of higher quality that most papers they reviewed, and were unanimous in recommending it for acceptance in the conference. I see no reason not to give it my strongest recommendation as well.
This paper addresses an important application in genomics, i.e. the prediction of chromatin structure from nucleotide sequences.  The authors develop a novel method for converting the nucleotide sequences to a 2D structure that allows a CNN to detect interactions between distant parts of the sequence.  The reviewers found the paper innovative, interesting and convincing.  Two reviewers gave a 7 and there was one 6.  The 6, however, indicated during rather lengthy discussion that they were willing to raise their scores if their comments were addressed.  Hopefully the authors will address these comments in the camera ready version.  Overall a solid application paper with novel insights and technical innovation.  
This paper compares autoencoder and GAN based methods for 3D point cloud representation and generation, as well as new (and welcome) metrics for quantitatively evaluating generative models.  The experiments form a good but still a bit too incomplete exploration of this topic.  More analysis is needed to calibrate the new metrics.  Qualitative analysis would be very helpful here to complement and calibrate the quantitative ones.  The writing also needs improvement for clarity and verbosity.  The author replies and revisions are very helpful, but there is still some way to go on the issues above. Overall, the committee is intersting and recommends this paper for the workshop track.
This paper introduces a GAN based framework for inferring human category representations. The reviewers agree that the idea is interesting and well motivated, and the results are promising. The technical contribution is not significant, but nevertheless the paper combines existing ideas in an interesting way. The reviewers would also like to see some more work towards the direction of investigation of the results and extraction of insights, without which the paper feels somehow incomplete.
The paper received highly diverging scores: 5 (R1) ,9 (R2), 4(R3). Both R1 and R3 complained about the comparisons to related methods. R3 suggested some kNN and GP baselines, while R1 mentioned concurrent work using deepnets for trafffic prediction.  R3 is real expert on field. R2 and R1, not so. R2 review very positive, but vacuous. Rebuttal seems to counter R1 and R3 well.  It s a close all but the AC is inclined to accept since it s an interesting application of (graph based) deepnets.
Below is a summary of the pros and cons of the proposed paper:  Pros: * Proposes a novel method to tune program synthesizers to generate correct programs and prune search space, leading to better and more efficient synthesis * Shows small but substantial gains on a standard benchmark  Cons: * Reviewers and commenters cited a few clarity issues, although these have mostly been resolved * Lack of empirical comparison with relevant previous work (e.g. Parisotto et al.) makes it hard to determine their relative merit  Overall, this seems to be a solid, well evaluated contribution and seems to me to warrant a poster presentation.  Also, just a few notes from the area chair to potentially make the final version better:  The proposed method is certainly different from the method of Parisotto et al., but it is attempting to solve the same problem: the lack of consideration of the grammar in neural program synthesis models. The relative merit is stated to be that the proposed method can be used when there is no grammar specification, but the model of Parisotto et al. also learns expansion rules from data, so no explicit grammar specification is necessary (as long as a parser exists, which is presumably necessary to perform the syntax checking that is core to the proposed method). It would have been ideal to see an empirical comparison between the two methods, but this is obviously a lot of work. It would be nice to have the method acknowledged more prominently in the description, perhaps in the introduction, however.  It is nice to see a head nod to Guu et al. s work on semantic parsing (as semantic parsing from natural language is also highly relevant). There is obviously a lot of work on generating structured representations from natural lanugage, and the following two might be particularly relevant given their focus on grammar based formalisms for code synthesis from natural language:  * "A Syntactic Neural Model for General purpose Code Generation" Yin and Neubig ACL 2017. * "Abstract Syntax Networks for Code Generation and Semantic Parsing" Rabinovich et al. ACL 2017 
Viewing language modeling as a matrix factorization problem, the authors argue that the low rank of word embeddings used by such models limits their expressivity and show that replacing the softmax in such models with a mixture of softmaxes provides an effective way of overcoming this bottleneck. This is an interesting and well executed paper that provides potentially important insight. It would be good to at least mention prior work related to the language modeling as matrix factorization perspective (e.g. Levy & Goldberg, 2014).
The reviewers agree that the proposed method is theoretically interesting, but disagree on whether it has been properly experimentally validated.   My view is that the the theoretical contribution is interesting enough to warrant inclusion in the conference, and so I will err on the side of accepting.
RDA improves on RWA, but even so, the model is inferior to the other standard RNN models. As a result R1 and R3 question the motivation for the use of this model   something the authors should motivate.
This paper presents an ensemble method for conversation systems, where a retrieval based system is ensembled with a generation based system.  The combination is done via a reranker.  Evaluation is done on one dataset containing query reply pairs with both BLEU and human evalutations.  The experimental results are good using the ensemble model.  Although this presents some novel ideas and may be useful for chatbots (not for goal oriented systems), the committee feels that the approach and the presented material does not have enough substance for publication at ICLR:  it will be interesting to evaluate this system in a goal oriented setting; many prior papers have built generation based conversation systems (1 step)   this paper does not present any comparison with those papers.  Addressing these issues may strengthen the paper for a future venue. 
The authors present a thorough exploration of large sparse models that are pruned down to a target size and show that these models can perform better than small dense models. Results are shown on a variety of datasets with as conv models and seq2seq. The authors even go so far as to release the code. I think the authors are to be thanked for their experimental contributions. However, in terms of accepting the paper for a premier machine learning conference the method holds little surprise or non obviousness. I think the paper is a good experimental contribution, and would make a good workshop paper instead but it offers little contribution by way of machine learning methods. 
This paper proposes a method for refining distributional semantic representation at the lexical level. The reviews are fairly unanimous in that they found both the initial version of the paper, which was deemed quite rushed, and the substantial revision unworthy of publication in their current state. The weakness of both the motivation and the experimental results, as well as the lack of a clear hypothesis being tested, or of an explanation as to why the proposed method should work, indicates that this work needs revision and further evaluation beyond what is possible for this conference. I unfortunately must recommend rejection.
The reviewers have found that while the task of visual domain adaptation is meaningful to explore and improve, the proposed method is not sufficiently well motivated, explained or empirically tested. 
  + Original regularizer that encourages discriminator representation entropy is shown to improve GAN training.   + good supporting empirical validation     While intuitively reasonable, no compelling theory is given to justify the approach     The regularizer used in practice is a heap of heuristic approximations (continuous relaxation of a rough approximate measure of the joint entropy of a binarized activation vector)     The writing and the mathematical exposition could be clearer and more precise
R1 and R3‚Äôs  main concern was that the work was not actually outperforming existing work and therefore its advantages were unclear. R2 brought up several questions on the experiments and asked for clarification with respect to previous work. R3 had several other detailed questions for the authors. The authors did not provide a response.
Learning identity preserving transformations from unlabeled data is definitely an important and useful direction. However the paper does not have convincing experiments to establish the effectiveness of the proposed method on real datasets which is a crucial limitation in my view, given that the paper is largely based on an earlier published work by Culpepper and Olshausen (2009). 
Pros:   The authors propose a new algorithm to train GAN based on Cramer distance arguing that this eases optimization compared to Wasserstein GAN.    Reviewers agree that the paper reads well and provides a good overview of the properties of divergence measures used for GAN training.  Cons:   It is not clear how much the central arguments about scale sensitivity, sum invariance, and unbiased sample gradients of the distances hold true in practice and generalize.   The reviewers do not agree the benefits of the new algorithm is clear from the experiments shown. Given the pros/cons ,the committee feels the paper falls short of acceptance in its current form.
Thank you for submitting you paper to ICLR. Two of the reviewers are concerned that the paper‚Äôs contributions are not significant enough ‚Äîeither in terms of the theoretical or experimental contribution   to warrant publication. The authors have improved the experimental aspect to include a more comprehensive comparison, but this has not moved the reviewers.  Summary: The approach is very promising, but more experimental work is still required to demonstrate significance. 
This work presents new results on unsupervised machine translation using a clever combination of techniques. In terms of originality, the reviewers find that the paper over claims, and promises a breakthrough, which they do not feel is justified. However there is "more than enough new content" and "preliminary" results on a new task. The experimental quality also has some issues, there is a lack of good qualitative analysis, and reviewers felt the claims about semi supervised work had issues. Still the main number is a good start, and the authors are correct to note that there is another work with similarly promising results. Of the two works, the reviewers found the other more clearly written, and with better experimental analysis, noting that they both over claim in terms of novelty. The most promising aspect of the work, will likely be the significance of this task going forward, as there is now more interest in the use of multi lingual embeddings and nmt as a benchmark task. 
This paper uses known methods for learning a differentially private models and applies it to the task of learning a language model, and find they are able to maintain accuracy results on large datasets. Reviewers found the method convincing and original saying it was "interesting and very important to the machine learning ... community", and that in terms of results it was a "very strong empirical paper, with experiments comparable to industrial scale". There were some complaints as to the clarity of the work, with requests for more clear explanations of the methods used.  
Pros: + The paper introduces a non trivial interpretation of MAML as hierarchical Bayesian learning and uses this perspective to develop a new variation of MAML that accounts for curvature information.  Cons:   Relatively small gains over MAML on mini Imagenet.   No direct comparison against the state of the art on mini Imagenet.  The reviewers agree that the interpretation of MAML as a form of hierarchical Bayesian learning is novel, non trivial, and opens up an interesting direction for future research.  The only concerns are that the empirical results on mini Imagenet do not show a particularly large improvement over MAML, and there is no direct comparison to the state of the art results on the task.  However, the value of the new perspective on meta learning outweighs these concerns. 
This work replaces the RNN layer of square with a self attention and convolution, achieving a big speed up and performance gains, particularly with data augmentation. The work is mostly clear presented, one reviewer found it "well written" although there was a complaint the work did not clear separate out the novel aspects. In terms of results the work is clearly of high quality, producing top numbers on the shared task. There were some initial complaints of only using the SQuAD dataset, but the authors have now included additional results that diversify the experiments. Perhaps the largest concern is novelty. The idea of non RNN self attention is now widely known, and there are several systems that are applying it. Reviewers felt that while this system does it well, it is maybe less novel or significant than other possible work. 
Graph neural networks (incl. GCNs) have been shown effective on a large range of tasks. However, it has been so far hard (i.e. computationally expensive or requiring the use of heuristics) to apply them to large graphs. This paper aims to address this problem and the solution is clean and elegant. The reviewers generally find it well written and interesting. There were some concerns about the comparison to GraphSAGE (an alternative approach), but these have been addressed in a subsequent revision.  + an important problem + a simple approach + convincing results + clear and well written 
This paper provides a comparison of different types of a memory augmented models and extends some of them to beyond their simple form. Reviewers found the paper to be clearly written, saying it "nice introduction to the topic" and noting that they "enjoyed reading this paper". In general though there was a feeling that the "substance of the work is limited". One reviewer complained that experiments were limited to small English datasets PTB and Wikitext 2 and asked why they didn t try "machine translation or speech recognition". (The author s note that they did try the Linzen dataset, and while the reviewers found the experiments impressive, the task itself felt artificial) . Another felt that the "multipop model" alone was not too large a contribution. The actual experiments in the work are well done, although given the fact that the models are known there was expectation of "more "in depth" analysis of the different models". Overall this is a good empirical study, which shows the limited gains achieved by these models, a nevertheless useful piece of information for those working in this area.
The authors use a memory augmented neural architecture to learn solve combinatorial optimization problems.  The reviewers consider the approach worth studying, but find the authors  experimental protocol and baselines flawed.  
The reviewers were largely agreed that the paper presented an interesting idea and has potential but needs a better empirical evaluation.  It seems that the authors largely agree and are working to improve it.  PROS: 1. Improving the speed of program synthesis is a useful problem 2. Good treatment of related work, e.g. CEGIS  CONS: 1. The approach likely does not scale 2. The architecture is underspecified making it hard to reproduce 3. Only 1 domain for evaluation
This paper incorporates attention in the PixelCNN model and shows how to use MAML to enable few shot density estimation. The paper received mixed reviews (7,6,4). After rebuttal the first reviewer updated the score to accept. The AC shares the concern of novelty with the first reviewer. However, it is also not trivial to incorporate attention and MAML in PixelCNN, thus the AC decided to accept the paper. 
This paper exposes a simple recipe to manipulate the latent space of generative models in such a way to minimize the mismatch between the prior distribution and that of the manipulated latent space. Manipulations such as linear interpolation are commonplace in the literature, and this work will be helpful to improve assessment on that front.  Reviewers found this paper interesting, yet unpolished and incomplete. In subsequent iterations, the paper has significantly improved on those fronts, however the AC believes an extra iteration will make this work even more solid. Thus, unfortunately this paper cannot be accepted at this time. 
Pros   Interesting approach to induce sparsity, trains faster than alternative approaches Cons   Fairly complex set of heuristics for pruning weights   Han et al. works well, although the authors claim it takes more time to train, which may not not hold for all training sets and doesn‚Äôt seem like a strong enough reason to choose an alternative appraoch  Given these comments, the AC recommends that the paper be rejected. 
This paper presents a nice set of results on a new RL algorithm. The main downside is the limitation to the Atari domain, but otherwise the ablation studies are nice and the results are strong.
An interesting application of graph neural networks to robotics. The body of a robot is represented as a graph, and the agent‚Äôs policy is defined using a graph neural network (GNNs/GCNs) over the graph structure.  The GNN based policy network perform on par with best methods on traditional benchmarks, but shown to be very effective for transfer scenarios: changing robot size or disabling its components.  I believe that the reviewers  concern that the original experiments focused solely on centepedes and snakes were (at least partially) addressed in the author response: they showed that their GNN based model outperforms MLPs on a dataset of 2D walkers.  Overall:   an interesting application   modeling robot morphology is an under explored direction   the paper is  well written   experiments are sufficiently convincing (esp. after addressing the concerns re diversity and robustness).  
The paper explores an increasingly important questions, especially showing the attack on existing APIs. The update to the paper has also improved it, but the paper is still not yet as impactful as it could be and needs much more comprehensive analysis to correctly appreciate its benefits and role.
The pros and cons of this paper can be summarized as follows:  Pros: * It seems that the method has very good intuitions: consideration of partial rewards, estimation of rewards from modified sequences, etc.  Cons: * The writing of the paper is scattered and not very well structured, which makes it difficult to follow exactly what the method is doing. If I were to give advice, I would flip the order of the sections to 4, 3, 2 (first describe the overall method, then describe the method for partial rewards, and finally describe the relationship with SeqGAN) * It is strange that the proposed method does not consider subsequences that do not contain y_{t+1}. This seems to go contrary to the idea of using RL or similar methods to optimize the global coherence of the generated sequence. * For some of the key elements of the paper, there are similar (widely used) methods that are not cited, and it is a bit difficult  to understand the relationship between them: ** Partial rewards: this is similar to "reward shaping" which is widely used in RL, for example in the actor critic method of Bahdanau et al. ** Making modifications of the reference into a modified reference: this is done in, for example, the scheduled sampling method of Bengio et al. ** Weighting modifications by their reward: A similar idea is presented in "Reward Augmented Maximum Likelihood for Neural Structured Prediction" by Norouzi et al.  The approach in this paper is potentially promising, as it definitely contains a lot of promising insights, but the clarity issues and fact that many of the key insights already exist in other approaches to which no empirical analysis is provided makes the contribution of the paper at the current time feel a bit weak. I am not recommending for acceptance at this time, but would certainly encourage the authors to do clean up the exposition, perhaps add a comparison to other methods such as RL with reward shaping, scheduled sampling, and RAML, and re submit to another venue.
this submission presents a novel way in which a neural machine reader could be improved. that is, by learning to reformulate a question specifically for the downstream machine reader. all the reviewers found it positive, and so do i.
the reviewers were not fully convinced of the setting under which the proposed bipolar activation function was found by the authors to be preferable, and neither am i.