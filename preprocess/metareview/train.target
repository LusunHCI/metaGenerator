All three reviewers, even after the rebuttal, agreed that the paper did not meet with bar for acceptance. A common complaint was lack of clarity being a major problem. Unfortunately, the paper cannot be accepted in its current form. The authors are encouraged to improve the presentation of their approach  and resubmit to a new venue.  
Overall, this paper got strong scores from the reviewers (2 accepts and 1 weak accept).  The paper proposes to address the responsibility problem, enabling encoding and decoding sets without worrying about permutations.  This is achieved using permutation equivariant set autoencoders and an  inverse  operation that undoes the sorting in the decoder.  The reviewers all agreed that the paper makes a meaningful contribution and should be accepted.  Some concerns regarding clarity of exposition were initially raised but were addressed during the rebuttal period.  I recommend that the paper be accepted.
This paper introduces a set of new analysis methods to try to better understand the reasons that multilingual BERT succeeds. The findings substantially bolster the hypothesis behind the original multilingual BERT work: that this kind of model discovers and uses substantial structural and semantic correspondences between languages in a fully unsupervised setting. This is a remarkable result with serious implications for representation learning work more broadly.  All three reviewers saw ways in which the paper could be expanded or improved, and one reviewer argued that the novelty and scope of the paper are below the standard for ICLR. However, I am inclined to side with the two more confident reviewers and argue for acceptance. I don t see any substantive reasons to reject the paper, the methods are novel and appropriate (even in light of the prior work that exists on this question), and the results are surprising and relevant a high profile ongoing discussion in the literature on representation learning for language.
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
The main concern raised by the reviewers is that the paper is difficult to read and potentially unclear. Therefore, the area chair read the paper, and also found it fairly dense and challenging to read. While there may be important discoveries in the paper, the paper in its current form makes it too difficult to read. Since four reviewers (including the AC) struggled to understand the paper, we believe the presentation of the paper should be improved. In particular, the claims of the paper should be better put into context.
This paper tackles the problem of detection out of distribution (OoD) samples. The proposed solution is based on a Bayesian variational autoencoder. The authors show that information theoretic measures applied on the posterior distribution over the decoder parameters can be used to detect OoD samples. The resulting approach is shown to outperform baselines in experiments conducted on three benchmarks (CIFAR 10 vs SVNH and two based on FashionMNIST).  Following the rebuttal, major concerns remained regarding the justification of the approach. The reason why relying on active learning principles should allow for OoD detection would need to be clarified, and the use of the effective sample size (ESS) would require a stronger motivation. Overall, although a theoretically informed OoD strategy is indeed interesting and relevant, reviewers were not convinced by the provided theoretical justifications. I therefore recommend to reject this paper.
This paper contributes to the recently emerging literature about applying reinforcement learning methods to combinatorial optimization problems. The authors consider TSPs and propose a search method that interleaves greedy local search with Monte Carlo Tree Search (MCTS). This approach does not contain learned function approximation for transferring knowledge across problem instances, which is usually considered the main motivation for applying RL to comb opt problems.  The reviewers state that, although the approach is a relatively straight forward combination of two existing methods, it is in principle somewhat interesting.  However, the experiments indicate a large gap to SOTA solvers for TSPs.  No rebuttal was submitted.  In absence of both SOTA results and methodological novelty, as assessed by the reviewers and my owm reading, I recommend to reject the paper in its current form.
This work presents a method for debiasing graph embeddings. The main concerns for the work were originally identified by Reviewer 3, who pointed out that the method is only capable of linear debiasing. Authors responded by updating the manuscript in several places to mention this limitation as well as adding Table 3 to the Appendix showing that SVM s with non linear kernels are still able to identify bias in the embeddings. Reviewers agreed that this addition improved the manuscript, however some reviewers still had concerns about the revised manuscript. This AC has several recommendations for improving the paper. First additional revision is needed to better address the limitations of linear debiasing, for example Table 1 still reads "MONET is successful in removing all metadata information from the topology embeddings – the links in the graph are no longer an effective predictor of political party".  Statements like this are a bit misleading, as the embeddings will still be biased with respect to a non linear classifiers (as evident by Table 3). Additionally, updating Table 1 and related experiments to measure embedding bias with respect to non linear classifiers would help clarify the limitations for perspective readers. Second, the paper should be updated to address remaining concerns that the linear debiasing assumption limits the applicability of the method. One could either discuss or demonstrate additional applications of the method that work even with the linear assumption, extend MONET so it can improve model bias with respect to non linear classifiers, or show that MONET still outperforms baselines when the non linear assumption is violated.
This paper makes the observation that, by adjusting the ratio of gradients from skip connections and residual connections in ResNet family networks in a projected gradient descent attack (that is, upweighting the contribution of the skip connection gradient), one can obtain more transferable adversarial examples. This is evaluated empirically in the single model black box transfer setting, against a wide range of models, both with and without countermeasures.  Reviewers praised the novelty and simplicity of the method, the breadth of empirical results, and the review of related work. Concerns were raised regarding a lack of variance reporting, strength of the baselines vs. numbers reported in the literature,  and the lack of consideration paid to the threat model under which an adversary employs an ensemble of source models, as well as the framing given by the original title and abstract. All of these appear to have been satisfactorily addressed, in a fine example of what ICLR s review & revision process can yield. It is therefore my pleasure to recommend acceptance.
This paper investigates the identifiability of attention distributions in the context of Transformer architectures. The main result is that, if the sentence length is long enough, difference choices of attention weights may result in the same contextual embeddings (i.e. the attention weights are not identifiable). A notion of "effective attention" is proposed that projects out the null space from attention weights.   In the discussion period, there were some doubts about the technical correctness of the identifiability result that were clarified by the authors. The attention matrix A results from a softmax transformation, therefore each of its rows is constrained to be in the probability simplex   i.e. we have A >  0 (elementwise) and A1   1. In the present version of the paper, when analyzing the null space of T (Eqs. 4 and 5) this constraint on A is not taken into account. In particular, in Eq. 5 the existence of a \tilde{A} in the null space of T is not clear at all, since for (A + \tilde{A})T   AT to hold we would need to require, besides A >  0 and A1   1, that A + \tilde{A} >  0 and A1 + \tilde{A}1   1, i.e.  \tilde{A} <  A (elementwise) \tilde{A}1   0  The present version of the paper does not make it clear that the intersection of the null space of T with these two constraints is non empty in general   which would be necessary for attention not to be identifiable, one of the main points of the paper.   The authors acknowledged this concern and provided a proof. I suggest the following simplified version of their proof:  We re looking for a vector \tilde{A} satisfying  (1) \tilde{A}’*T   0 (to be in the null space of T)  and   (2) \tilde{A}’*1   0  (3) \tilde{A} >   A  (to make sure A + \tilde{A} are in the probability simplex).  Conditions (1) and (2) are equivalent to require \tilde{A} to be in the null space of [T; 1]. It is fine to assume this null space exists for a general T (it will be a linear subspace of dimension ds   dv   1).   To take into account condition (3) here’s a simpler proof: since A is a probability vector coming from a softmax transformation (hence it is strictly > 0 elementwise), there is some epsilon > 0 such that any point in the ball centered on 0 with radius epsilon is >   A.   Since the null space of [T; 1] contains 0, any point \tilde{A} in the intersection of this null space with the epsilon ball above satisfies (1), (2), and (3). This should work for any ds   dv > 1  and as long as A is not a one hot distribution (otherwise it collapses to a single point \tilde{A}   0).  I am less convinced about the justification to use an “effective attention” which is not in the probability simplex, though (not even in the null space of [T; 1] but only null(T)). That part deserves more clarification in the paper.   I recommend acceptance of this paper provided these clarifications are provided and the proof is included in the final version.   
The work proposes a graph convolutional network based approach to multi agent reinforcement learning. This approach is designed to be able to adaptively capture changing interactions between agents. Initial reviews highlighted several limitations but these were largely addressed by the authors. The resulting paper makes a valuable contribution by proposing a well motivated approach, and by conducting extensive empirical validation and analysis that result in novel insights. I encourage the authors to take on board any remaining reviewer suggestions as they prepare the camera ready version of the paper.
This paper proposes an RL based structure search method for causal discovery. The reviewers and AC think that the idea of applying reinforcement learning to causal structure discovery is novel and intriguing. While there were initially some concerns regarding presentation of the results, these have been taken care of during the discussion period. The reviewers agree that this is a very good submission, which merits acceptance to ICLR 2020.
This paper was very well received by the reviewers with solid Accept ratings across the board. The subject matter is quite interesting    mathematical reasoning in latent space, and it was suggested by a reviewer that this could be a good candidate for an oral. The AC agrees and recommends acceptance as an oral. Some of the intuitions of what is being done in this paper could be better visualized and presented and I encourage the authors to think carefully about how to present this work if an oral presentation is granted by the PCs.
This paper tackles neural response generation with Generative Adversarial Nets (GANs), and to address the training instability problem with GANs, it proposes a local distribution oriented objective. The new objective is combined with the original objective, and used as a hybrid loss for the adversarial training of response generation models, named as LocalGAN. Authors responded with concerns about reviewer 3 s comments, and I agree with the authors explanation, so I am disregarding review 3, and am relying on my read through of the latest version of the paper. The other reviewers think the paper has good contributions, however they are not convinced about the clarity of the presentations and made many suggestions (even after the responses from the authors).  I suggest a reject, as the paper should include a clear presentation of the approach and technical formulation (as also suggested by the reviewers).
This paper addresses the problem of rotation estimation in 2D images. The method attempted to reduce the labeling need by learning in a semi supervised fashion. The approach learns a VAE where the latent code is be factored into the latent vector and the object rotation.   All reviewers agreed that this paper is not ready for acceptance. The reviewers did express promise in the direction of this work. However, there were a few main concerns. First, the focus on 2D instead of 3D orientation. The general consensus was that 3D would be more pertinent use case and that extension of the proposed approach from 2D to 3D is likely non trivial. The second issue is that minimal technical novelty. The reviewers argue that the proposed solution is a combination of existing techniques to a new problem area.   Since the work does not have sufficient technical novelty to compare against other disentanglement works and is being applied to a less relevant experimental setting, the AC does not recommend acceptance.  
The paper describes a method to train a convolutional network with large capacity, where channel gating (input conditioned) is implemented   thus, only parts of the network are used at inference time. The paper builds over previous work, with the main contribution being a "batch shaping" technique that regularizes the channel gating to follow a beta distribution, combined with L0 regularization. The paper shows that ResNet trained with this technique can achieve higher accuracy with lower theoretical MACs. Weakness of the paper is that more engineering would be required to convert the theoretical MACs into actual running time   which would further validate the practicality of the approach. 
The authors propose to use pruning to study/interpret learned CNNs. The reviewers believed the results were not surprising and/or had no practical relevance. Unlike in many cases, two of the reviewers acknowledged reading the rebuttals, but were unswayed.
This paper proposes a method to automatically generate corpora for training program synthesis systems.  The reviewers did seem to appreciate the core idea of the paper, but pointed out a number of problems with experimental design that preclude the publication of the paper at this time. The reviewers gave a number of good comments, so I hope that the authors can improve the paper for publication at a different venue in the future.
This paper proposes to reduce the number of variational parameters for mean field VI. A low rank approximation is used for this purpose. Results on a few small problems are reported.  As R3 has pointed out, the main reason to reject this paper is the lack of comparison of uncertainty estimates. I also agree that, recent Adam like optimizers do use preconditioning that can be interpreted as variances, so it is not clear why reducing this will give better results.  I agree with R2 s comments about missing the "point estimate" baseline. Also the reason for rank 1,2,3 giving better accuracies is unclear and I think the reasons provided by the authors is speculative.  I do believe that reducing the parameterization is a reasonable idea and could be useful. But it is not clear if the proposal of this paper is the right one. Due to this reason, I recommend to reject this paper. However, I highly encourage the authors to improve their paper taking these points into account.
The paper presents a training method for deep neural networks to detect out of distribution samples under perspective of Gaussian discriminant analysis.  Reviewers and AC agree that some idea is given in the previous work (although it does not focus on training), and additional ideas in the paper are not super novel. Furthermore, experimental results are weak, e.g., comparison with other deep generative classifiers are desirable, as the paper focuses on training such deep models.  Hence, I recommend rejection.
The paper proposes a technique for incorporating prior knowledge as relations between training instances.  The reviewers had a mixed set of concerns, with one common one being an insufficient comparison with / discussion of related work. Some reviewers also found the clarity lacking, but were satisfied with the revision. One reviewer found the claim of the approach being general but only tested and valid for the VQA dataset problematic.  Following the discussion, I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to another venue.
This paper proposes RTFM, a new model in the field of language conditioned policy learning. This approach is promising and important in reinforcement learning because of the difficulty to learn policies in new environments.   Reviewers appreciate the importance of the problem and the effective approach. After the author response which addressed some of the major concerns, reviewers feel more positive about the paper. They comment, though, that presentation could be clearer, and the limitations of using synthetic data should be discussed in depth.  I thank the authors for submitting this paper.
The authors present and implement a synchronous, distributed RL called Decentralized Distributed Proximal Policy Optimization. The proposed technique was validated for pointgoal visual navigation task on recently introduced Habitat challenge 2019 and got the state of art performance.  Two reviews recommend this paper for acceptance with only some minor comments, such as revising the title. The Blind Review #2 has several major concerns about the implementation details. In the rebuttal, the authors provided the source code to make the results reproducible.  Overall, the paper is well written with promising experimental results. I also recommend it for acceptance. 
This paper studies how much overparameterization is required to achieve zero training error via gradient descent in one hidden layer neural nets. In particular the paper studies the effect of margin in data on the required amount of overparameterization. While the paper does not improve in the worse case in the presence of margin the paper shows that sometimes even logarithmic width is sufficient. The reviewers all seem to agree that this is a nice paper but had a few mostly technical concerns. These concerns were sufficiently addressed in the response. Based on my own reading I also find the paper to be interesting, well written with clever proofs. So I recommend acceptance. I would like to make a suggestion that the authors do clarify in the abstract intro that this improvement can not be achieved in the worst case as a shallow reading of the manuscript may cause some confusion (that logarithmic width suffices in general).
This paper shows that DeepSets and PointNet, which are known to be universal for approximating functions, are also universal for approximating equivariant set functions. Reviewer are in agreement that this paper is interesting and makes important contributions. However, they feel the paper could be written to be more accessible.  Based on the reviews and discussions following author response, I recommend accepting this paper. I appreciate the authors for an interesting paper and look forward to seeing it at the conference.
This paper introduces a new objective for text generation with neural nets.  The main insight is that the standard likelihood objective assigns excessive probability to sequences containing repeated and frequent words.  The paper proposes an objective that penalizes these patterns.  This technique yields better text generation than alternative methods according to human evaluations.  The reviewers found the paper to be written clearly. They found the problem to be relevant and found the proposed solution method to be both novel and simple.  The experiments were carefully designed and the results were convincing.  The reviewers raised several concerns on particular details of the method.  These concerns were largely addressed by the authors in their response.  Overall, the reviewers did not find the weaknesses of the paper to be serious flaws.  This paper should be published. The paper provides a clearly presented solution for a relevant problem, along with careful experiments.   
This paper provides a novel approach for addressing ill posed inverse problems based on a formulation as a regularized estimation problem and showing that this can be optimized using the CycleGAN framework. While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR due to a critical gap between the presented theory and applications. The paper will benefit from a revision and resubmission to another venue.
The paper developed log abstract transformer, square abstract transformer and sigmoid tanh abstract transformer to certifiy robustness of neural network models for audio. The work is interesting but the scope is limited. It presented a neural network certification methods for one particular type of audio classifiers that use MFCC as input features and LSTM as the neural network layers. This thus may have limited interest to the general readers.   The paper targets to present an end to end solution to audio classifiers. Investigation on one particular type of audio classifier is far from sufficient. As the reviewers pointed out, there re large literature of work using raw waveform inputs systems. Also there re many state of the art systems are HMM/DNN and attnetion based encoder decoder models. In terms of neural network models, resent based models, transformer models etc are also important. A more thorough investigation/comparison would greatly enlarge the scope of this paper. 
All of the reviewers agree the paper has an interesting idea (using rotations of the representation as regularization). However, the reviewers also agree the empirical gains are too insignificant. While the paper shows results on CIFAR, the reviewers mentioned a few other ways to improve performance, such as more complex and unconstrained datasets. These additional experiments would make the effectiveness of proposed approach more convincing.
The paper shows that overparameterized autoencoders can be trained to memorize a small number of training samples, which can be retrieved via fixed point iteration. After rounds of discussion with the authors, the reviewers agree that the idea is interesting and overall quality of writing and experiments is reasonable, but they were skeptical regarding the significance of the finding and impact to the field and thus encourage studying the phenomenon further and resubmitting in a future conference. I thus recommend rejecting this submission for now.
Thanks to the authors for the submission and the active discussion. The paper applies deep learning to the duration of stay estimation problem in the warehouse storage application. The authors provide problem formulation and describe the pipeline of their solutions, including datasets preparation and loss functions design. The reviewers agree that this is a good application paper that showcases how deep learning can be useful for a real world problem. The release of the dataset can also be a nice contribution. A major debate during the discussion is whether this paper is in scope of ICLR given that it is mostly a straightforward application existing techniques. After several rounds of discussion, reviewers think that this should fit under the category "applications in vision, ... , computational biology, and others." Overall, this paper can be a good example of applying deep learning to real world problems. 
The paper proposes an unsupervised framework for domain adaptation in the context of person re identification to reduce the effect of noisy labels. They use refined soft labels and propose a soft softmax triplet loss to support learning with these soft labels.   All reviewers have unanimously agreed to accept the paper and appreciated the comprehensive experiments on four datasets and ablation studies which give some insights about the proposed method. I agree with the assessment of the reviewers and recommend that this paper be accepted.
The authors propose a novel approach for imitation learning in settings where demonstrations are unaligned with the task (e.g., differ in terms of state and action space). The proposed approach consists of alignment and adaptation steps and theoretical insights are provided on whether given MDPs can be aligned. Reviewers were positive about the ideas presented in the paper, and several requests for clarification were well addressed by the authors during the rebuttal phase. Key evaluation issues remained unresolved. In particular, it was unclear to what degree performance differences were purely caused by issues in alignment, and reviewers did not see sufficient evidence to support claims about performance on the full cross domain learning setting.
The paper presents a method for modeling videos with object centric structured representations. The paper is well written and clearly motivated. Using a Graph Neural Network for modeling latent physics is a sensible idea and can be beneficial for planning/control. Experimental results show improved performance over the baselines. After the rebuttal, many questions/concerns from the reviewers were addressed, and all reviewers recommend weak acceptance.
This paper introduces a new clustering method, which builds upon the work introduced by Lee et al, 2019   contextual information across different dataset samples is gathered with a transformer, and then used to predict the cluster label for a given sample. All reviewers agree the writing should be improved and clarified. The novelty is also on the low side, given the previous work by Lee et al. Experiments should be more convincing. 
The paper proposes and studies a task where the goal is to classify an image that has been intentionally degraded to reduce information content.  All the reviewers found the comparison of human and machine performance interesting and valuable. However the reviewers expressed concerns and noted the following weaknesses: the presented results are not convincing to support our understanding of the differences between human and machine perception (R1), using entropy to quantify the distortion is not well motivated and has been addressed before (R1), lack of empirical evidence (R2).  AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews are useful for improving and revising the paper. 
This paper is interested in finding salient areas in a deep learning image classification setting. The introduced method relies on masking images using Gaussian Gaussian light and shadow (GLAS) and estimating its impact on output.  As noted by all reviewers, the paper is too weak for publication in its current form:   Novelty is very low.   Experimental section not convincing enough, in particular some metrics are missing.   The writing should be improved.
The authors propose a new technique for training networks to be robust to adversarial perturbations. They do this by computing bounds on the impact of the worst case adversarial attack, but that only hold under strong assumptions on the distribution of the network weights. While these bounds are not rigorous, the authors show that they can produce networks that improve the robustness accuracy tradeoff on image classification tasks.  While the idea proposed by the authors is interesting, the reviewers had several concerns about this paper: 1) The assumptions required for the bounds to hold are unrealistic and unlikely to hold in practice, especially for convolutional neural networks. 2) The comparisons are not presented in a fair manner that allow the reader to interpret the difference between the nature of certificates computed by the authors and those computed in prior work. 3) The empirical gains are not substantial if one normalizes for the non rigorous nature of the certificates computed (given that they only hold under hard to justify assumptions).  The rebuttal phase clarified some issues in the paper, but the fundamental flaws with the approach remain unaddressed. Thus, I recommend rejection and suggest that the authors revisit the assumptions and develop more convincing arguments and/or experiments justifying them for practical deep learning scenarios.
This paper extends Adam by adding another hyperparameter that allows the second moments to be raised to a power p other than 1/2. This certainly seems worth trying. The paper is well written, and the experiments seem reasonably complete. But some of the reviewers and I feel like the contribution is a bit obvious and incremental. The "small learning rate dilemma" needs a bit more justification: since the denominator has a different scale, the learning rates for different values of p are not directly comparable. It could very well be that Adam s learning rate has to be set too small due to some outlier dimensions, but showing this would require some evidence. From the experiments, it does seem like there s some practical benefit, though it s not terribly surprising that adding an additional hyperparameter will result in improved performance. The reviewers think the theoretical analysis is a straightforward extension of prior work (though I haven t checked myself). Overall, it doesn t seem to me like the contribution is quite enough for publication at ICLR. 
The paper presents an approach to feature selection. Reviews were mixed and questions whether the paper has enough substance, novelty, the correctness of the theoretical contributions, experimental details, as well as whether the paper compares to the relevant literature.  
The paper proposes an approach for N D continuous convolution on unordered particle set and applies it to Lagrangian fluid simulation. All reviewers found the paper to be a novel and useful contribution towards the problem of N D continuous convolution on unordered particles. I recommend acceptance.  
The paper studies the effect of various hyperparameters of neural networks including architecture, width, depth, initialization, optimizer, etc. on the generalization and memorization. The paper carries out a rather through empirical study of these phenomena. The authors also rain a model to mimic identity function which allows rich visualization and easy evaluation.  The reviewers were mostly positive but expressed concern about the general picture. One reviewer also has concerns about "generality of the observed phenomenon in this paper". The authors had a thorough response which addressed many of these concerns. My view of the paper is positive. I think the authors do a great job of carrying out careful experiments. As a result I think this is a good addition to ICLR and recommend acceptance.
This submission proposes a graph sparsification mechanism that can be used when training GNNs.  Strengths:  The paper is easy to follow.  The proposed method is sound and effective.  Weaknesses:  The novelty is limited.  Given the limited novelty and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance.
The paper examines whether it is possible to train agents to follow synthetic instructions that perceives and modifies a 3D scene based on a first person viewpoint, and have the trained agents follow natural language instructions provided by humans.  The paper received two weak rejects and one weak accept.  The main concerns voiced by the reviewers are: 1. Lack of variety in natural language One of the key claims of the paper is that previous work on instruction following can only handle instructions generated from templates and cannot handle ambiguous expressions used by real people, and that the contribution of this work is that it can handle such expresssions.  However, as pointed out by R1, the language considered in this work is very simplistic in form (close to being template based) with the main variation coming from synonyms.  Even the free form natural instructions that are collected, are done so with very specific instructions that restrict diversity of language (e.g don t use colors or other properties of the object). R1 also point out that there are prior work that handles much more diverse language.  2. Limited technical novelty and questions about how much the proposed CMSA method actually contribute  3. Overclaims and lack of precision when using terminology There is concern that the task that is addressed is not actually that complex.  The environments are simple (with just 2 objects) and not that realistic.  Tackling 2 tasks is barely "multi task", and commonly, "manipulation" refers to low level grasping/picking up of objects which is not how it is used here.  While the paper has many strong elements and is mostly well written, considerable improvements still need to be made for the paper to have claims it can support.  It is currently below the bar for acceptance. The authors are encouraged to improve their paper and resubmit to an appropriate venue. 
This paper provides a fascinating hybridization approach to incorporating programs as priors over policies which are then refined using deep RL. The reviewers were, at the end of the discussion, all in favour of acceptance (with the majority strongly in favour). An excellent paper I hope to see included in the conference.
This paper describes a new method for explaining the predictions of a CNN on a particular image. The method is based on aggregating the explanations of several methods.   They also describe a new method of evaluating explanation methods which avoids manual evaluation of the explanations.  However, the most critical reviewer questions the contribution of the proposed method, which is simple.  Simple isn t always a bad thing, but I think here the reviewer has a point.  The new method for evaluating explanation methods is interesting, but the sample images given are also very simple   how does the method work when the image is cluttered?   How about when the prediction is uncertain or wrong?
The submission proposes methodology for quantizing neural networks.  The reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR.  Concerns included novelty over previous works, comparatively weak baseline comparisons, and overly restrictive assumptions.
A new method of calculating saliency maps for deep networks trained through RL (for example to play games) is presented. The method is aimed at explaining why moves were taken by showing which salient features influenced the move, and seems to work well based on experiments with Chess, Go, and several Atari games.   Reviewer 2 had a number of questions related to the performance of the method under various conditions, and these were answered satisfactorily by the reviewers.  This is a solid paper with good reasoning and results, though perhaps not super novel, as the basic idea of explaining policies with saliency is not new. It should be accepted for poster presentation. 
This paper presents a novel hierarchical reinforcement learning framework, based on learning temporal abstractions from past experience or expert demonstrations using recurrent variational autoencoders and regularising the representations.  This is certainly an interesting line of work, but there were two primary areas of concern in the reviews: the clarity of details of the approach, and the lack of comparison to baselines. While the former issue was largely dealt with in the rebuttals, the latter remained an issue for all reviewers.  For this reason, I recommend rejection of the paper in its current form.
This paper proposes a way to handle the hard negative examples (those very close to positive ones) in  NLP, using a distant supervision approach that serves as a regularization.   The paper addresses an important issue and is well written; however, reviewers pointed put several concerns, including testing the approach on the state of art neural nets, and making experiments more convincing by testing on larger problems.   
This paper uses Bayesian optimization with neural networks for neural architecture search.  One of the contributions is a path based encoding that enumerates every possible path through a cell search space. This encoding is shown to be surprisingly powerful, but it will not scale to large cell based search spaces or non cell based search spaces. The availability of code, as well as the careful attention to reproducibility is much appreciated and a factor in favor of the paper.  In the discussion, it surfaced that a comparison to existing Bayesian optimization approaches using neural networks would have been possible, while the authors initially did not think that this would be the case. The authors promised to include these comparisons in the final version, but, as was also discussed in the private discussion between reviewers and AC, this is problematic since it is not clear what these results will show. Therefore, the one reviewer who was debating about increasing their score did in the end not do so (but would be inclined to accept a future version with a clean and thorough comparison to baselines).   All reviewers stuck with their score of "weak reject", leaning to borderline. I read the paper myself and concur with this judgement. I recommend rejection of the current version, with an encouragement to submit to another venue after including a comparison to BO methods based on neural networks.
This paper proposes a method to improve word embedding by incorporating sentiment probabilities. Reviewer appreciate the interesting and simple approach and acknowledges improved results in low frequency words.  However, reviewers find that the paper is lacking in two major aspects: 1) Writing is unclear, and thus it is difficult to understand and judge the contributions of this research. 2) Perhaps because of 1, it is not convincing that the improvements are significant and directly resulting from the modeling contributions.  I thank the authors for submitting this work to ICLR, and I hope that the reviewers  comments are helpful in improving this research for future submission.
This paper uses unsupervised learning to create useful representations to improve the performance of models in predicting protein ligand binding. After reviewers had time to consider each other s comments, there was consensus that the current work is too lacking in novelty on the modeling side to warrant publication in ICLR. Additionally, current experiments are lacking comparisons with important baselines. The work in its current form may be better suited for a domain journal. 
The authors present a Siamese neural net architecture for learning similarities among field data generated by numerical simulations of partial differential equations. The goal would be to find which two field data are more similar to each. One use case mentioned is the debugging of new numerical simulators, by comparing them with existing ones.   The reviewers had mixed opinions on the paper. I agree with a negative comment of all three reviewers that the paper lacks a bit on the originality of the technique and the justification of the new loss proposed, as well as the fact that no strong explicit real world use case was given. I find this problematic especially given that similarities of solutions to PDEs is not a mainstream topic of the conference. Hence a good real world example use of the method would be more convincing.
This paper proposes a meta attack approach based on meta learning approaches to learn generalizable prior from the previously observed attack patterns. The proposed approach is able to attack targeted models with much fewer queries. After author response, all reviewers are very positive about the paper. Thus I recommend accept. 
This paper studies how self supervised objectives can improve representations for efficient RL. The reviewers are generally in agreement that the method is interesting, the paper is well written, and the results are convincing. The paper should be accepted.
The paper considers the problem of estimating the electronic structure s ground state energy of a given atomic system by means of supervised machine learning, as a fast alternative to conventional explicit methods (DFT). For this purpose, it modifies the neural message passing architecture to account for further physical properties, and it extends the empirical validation to also include unstable molecules.   Reviewers acknowledged the valuable experimental setup of this work and the significance of the results in the application domain, but were generally skeptical about the novelty of the machine learning model under study. Ultimately, and given that the main focus of this conference is on Machine Learning methodology, this AC believes this work could be more suitable in a more specialized venue in computational/quantum chemistry. 
This paper presents an auxiliary module to boost the representation power of GNNs. The new module consists of virtual supernode, attention unit, and warp gate unit. The usefulness of each component is shown in well organized experiments. This is the very borderline paper with split scores. While all reviewers basically agree that the empirical findings in the paper are interesting and could be valuable to the community, one reviewer raised concern regarding the incremental novelty of the method, which is also understood by other reviewers. The impression was not changed through authors’ response and reviewer discussion, and there is no strong opinion to champion the paper. Therefore, I’d like to recommend rejection this time.  
There is insufficient support to recommend accepting this paper.  Although the authors provided detailed responses, none of the reviewers changed their recommendation from reject.  One of the main criticisms, even after revision, concerned the quality of the experimental evaluation.  The reviewers criticized the lack of important baselines, and remained unsure about adequate hyperparameter tuning in the revision.  The technical exposition lacked a sober discussion of limitations.  The paper would be greatly strengthened by the addition of a theoretical justification of the proposed approach.  In the end, the submitted reviews should be able to help the authors strengthen this paper.
Main summary:  Sngle image super resolution network that can generate high resolution images from the corresponding C JPG images  Discussions reviewer 3: reviewer has a few issues including, claim the method is lossless, want more information about JPG revovering step reviewer 1: (not knowledgable): paper is well written and reviewer gives very few cons reviewer 2: main concerns are wrt novelty and technically sound Recommendation: the 2 more knowledgable reviwers mark this as Reject, I agree.
The reviewers all believe that this paper is not yet ready for publication. All agree that this is an important application, and an interesting approach. The methodological novelty, as well as other parts of exposition, involving related work, or further discussion of what this solution means for patients, is right now not completely convincing to reviewers. My recommendation is to work on making sure the exposition best explains the methodology, and making sure this venue is the best for the submitted line of work.
This paper proposes a new self supervised pre trained speech model that improves speech recognition performance.   The idea combines an earlier pre training approach (wav2vec) with discretization followed by BERT style masked reconstruction.  The result is a fairly complex approach, with not too much novelty but with a good amount of engineering and analysis, and ultimately very good performance.  The reviewers agree that the work deserves publication at ICLR, and the authors have addressed some of the reviewer concerns in their revision.  The complexity of the approach may mean that it is not immediately widely adopted by others, but it is a good proof of concept and may well inspire other related work.  I believe the ICLR community will find this work interesting.
Paper proposes a method for active learning on graphs. Reviewers found the presentation of the method confusing and somewhat lacking novelty in light of existing works (some of which were not compared to). After the rebuttal and revisions, reviewers minds were not changed from rejection. 
The paper is about exploration in deep reinforcement learning. The reviewers agree that this is an interesting and important topic, but the authors provide only a slim analysis and theoretical support for the proposed methods. Furthermore, the authors are encouraged to evaluate the proposed method on more than a single benchmark problem.
This paper proposes to address the high bandwidth cost when transferring data between server and user for machine learning applications. The input data is augment with channel and spatial mask so that the file transfer cost is reduced. While the reviewers agree that this is a well motivated and interesting problem to study, a number of concerns are raised, including loosely specified performance/size trade off, how this work is compared to related work, low novelty relative to a few key missing references. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.
Solid, but not novel enough to merit publication.  The reviewers agree on rejection, and despite authors  adaptation, the paper requires more work and broader experimentation for publication.
This article studies the length of one dimensional trajectories as they are mapped through the layers of a ReLU network, simplifying proof methods and generalising previous results on networks with random weights to cover different classes of weight distributions including sparse ones. It is observed that the behaviour is similar for different distributions, suggesting a type of universality. The reviewers found that the paper is well written and appreciated the clear description of the places where the proofs deviate from previous works. However, they found that the results, although adding interesting observations in the sparse setting, are qualitatively very close to previous works and possibly not substantial enough for publication in ICLR. The revision includes some experiments with trained networks and updates the title to better reflect the contribution. However, the reviewers did not find this convincing enough. The article would benefit from a deeper theory clarifying the observations that have been made so far, and more extensive experiments connecting to practice. 
   The paper presents a semi supervised data streaming approach. The proposed architecture is made of a layer wise k means structure (more specifically a epsilon means approach, where the epsilon is adaptively defined from the distortion percentile). Each layer is associated a scope (patch dimensions); each patch of the image is associated its nearest cluster center (or a new cluster is created if needed); new cluster centers are adjusted to fit the examples (Short Term Memory); clusters that have been visited sufficiently many time are frozen (Long Term Memory). Each cluster is associated a label distribution from the labelled examples. The label for each new image is obtained by a vote of the clusters and layers.  Some reviews raise some issues about the robustness of the approach, and its sensitivity w.r.t. hyper parameters. Some claims ("the distribution associated to a class may change with time") are not experimentally confirmed; it seems that in such a case, the LTM size might grow along time; a forgetting mechanism would then be needed to enforce the tractability of classification.   Some claims (the mechanism is related to how animal learn) are debatable, as noted by Rev#1; see hippocampal replay.  The area chair thinks that a main issue with the paper is that the Unsupervised Progressive Learning is considered to be a new setting ("none of the existing approaches in the literature are directly applicable to the UPL problem"), preventing the authors from comparing their results with baselines.  However, after a short bibliographic search, some related approaches exist under another name: * Incremental Semi supervised Learning on Streaming Data, Pattern Recognition 88, Li et al., 2018; * Incremental Semi Supervised Learning from Streams for Object Classification, Chiotellis et al., 2018; * Online data stream classification with incremental semi supervised learning, Loo et al., 2015.  The above approaches seem able to at least accommodate the Uniform UPL scenario. I therefore encourage the authors to consider some of the above as baselines and provide a comparative validation of STAM.
This work proposes a new regularization method for weakly supervised localization based on counting. Reviewers agree that this is an interesting topic but the experimental validation is weak (qualitative, lack of baselines), and the contribution too incremental. Therefore, we recommend rejection.
The paper extends posterior sampling to the multi agent RL setting, and develops a novel algorithm with convergence guarantees to a Nash Equilibrium strategy in two player zero sum games. Reviewers raised several questions, many of which were well addressed by the authors and which helped further clarify the approach and contribution of the paper. The paper is timely in that novel connections between Game Theory and RL are being explored in fruitful ways, and the paper provides valuable new insights and directions for future research.
This paper considers the problem of reasoning about uncertain poses of objects in images. The reviewers agree that this is an interesting direction, and that the paper has interesting technical merit. 
The author propose a method to first learn policies for intrinsically generated goal based tasks, and then leverage the learned representations to improve the learning of a new task in a generalized policy iteration framework.  The reviewers had significant issues about clarity of writing that were largely addressed in the rebuttal.  However, there were also concerns about the magnitude of the contribution (especially if it was added anything significant to the existing literature on GPI, successor features, etc), and the simplicity (and small number of) test domains.  These concerns persisted after the rebuttal and discussion.  Thus, I recommend rejection at this time.
This was a borderline paper, with both pros and cons.  In the end, it was not considered sufficiently mature to accept in its current form.  The reviewers all criticized the assumptions needed, and lamented the lack of clarity around the distinction between reinforcement learning and planning.  The paper requires a clearer contribution, based on a stronger justification of the approach and weakening of the assumptions.  The submitted comments should be able to help the authors strengthen this work.
This paper introduces an algorithm for online Bayesian learning of both streaming and non stationary data.  The algorithmic choices are heuristic but motivated by sensible principles.  The reviewers  main concerns were with novelty, but because the paper was well written and addressing an important problem they all agreed it should be accepted.
This paper proposes an algorithm for noisy labels by adopting an idea in the recent semi supervised learning algorithm.  As two problems of training noisy labels and semi supervised ones are closely related, it is not surprising to expect such results as pointed out by reviewers. However, reported thorough experimental results are strong and I think this paper can be useful for practitioners and following works.   Hence, I recommend acceptance.
This paper builds a connection between MixUp and adversarial training. It introduces untied MixUp (UMixUp), which generalizes the methods of MixUp. Then, it also shows that DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios. Though it has some valuable theoretical contributions, I agree with the reviewers that it’s important to include results on adversarial robustness, where both adversarial training and MixUp are playing an important role.
This manuscript describes a continual learning approach where individual instances consist of sequences, such as language modeling. The paper consists of a definition of a problem setting, tasks in that problem setting, baselines (not based on existing continual learning approaches, which the authors argue is to highlight the need for such techniques, but with which the reviewers took issue), and a novel architecture.  Reviews focused on the gravity of the contribution. R1 and R2, in particular, argued that the paper is written as though the problem/benchmark definition is the main contribution. R2 mentions that in spite of this, the methods section jumps directly into the candidate architecture. As mentioned above, several reviewers also took issue with the fact that existing CL techniques are not employed as baselines. The authors engaged with reviewers and promised updates, but did not take the opportunity to update their paper.  As many of the reviewers  comments remain unaddressed and the authors  updates did not materialize, I recommend rejection, and encourage the authors to incorporate the feedback they have received in a future submission.
The paper presents a framework for scalable Deep RL on really large scale architecture, which addresses several problems on multi machine training of such systems with many actors and learners running.  Large scale experiments and impovements over IMPALA are presented, leading to new SOTA results. The reviewers are very positive over this work, and I think this is an important contribution to the overall learning / RL community.
The paper investigates graph convolutional filters, and proposes an adaptation of the Fisher score to assess the quality of a convolutional filter. Formally, the defined Graph Filter Discriminant Score assesses how the filter improves the Fisher score attached to a pair of classes (considering the nodes in each class, and their embedding through the filter and the graph structure, as propositional samples), taking into account the class imbalance.  An analysis is conducted on synthetic graphs to assess how the hyper parameters (order, normalization strategy) of the filter rule the GFD score depending on the graph and class features. As could have been expected there no single killer filter.  A finite set of filters, called base filters, being defined by varying the above hyper parameters, the search space is that of a linear combination of the base filters in each layer. Three losses are considered: with and without graph filter discriminant score, and alternatively optimizing the cross entropy loss and the GFD; this last option is the best one in the experiments.  As noted by the reviewers and other public comments, the idea of incorporating LDA ideas into GNN is nice and elegant. The reservations of the reviewers are mostly related to the experimental validation: of course getting the best score on each dataset is not expected; but the set of considered problems is too limited and their diversity is limited too (as demonstrated by the very nice Fig. 5).  The area chair thus encourages the authors to pursue this very promising line of research and hopes to see a revised version backed up with more experimental evidence. 
This paper focuses on studying neural network based denoising methods. The paper makes the interesting observation that most existing denoising approaches have a tendency to overfit to knowledge of the noise level. The authors claim that simply removing the bias on the network parameters enables a variety of improvements in this regard and provide some theoretical justification for their results. The reviewers were mostly postive but raised some concerns about generalization beyond Gaussian noise and not "being very well theoretically motivated". These concerns seem to have at least partially been alleviated during the discussion period. I agree with the reviewers. I think the paper looks at an important phenomena for denoising (role of variance parameter) and is well suited to ICLR. I recommend acceptance. I suggest that the authors continue to further improve the paper based on the reviewers  comments.
The authors addressed the issues raised by the reviewers; I suggest to accept this paper.
This paper presents a novel VAE based model for multivariate spatial point process which can realize efficient inference by amortization and handle missing points via smooth intensity estimation. Authors also provide interesting theoretical analysis to connect their method to a popular VAE based collaborative filtering method. Overall, all reviewers appreciate the methodological and theoretical contributions of the paper. During the reviewer discussion, one reviewer decided to update to the score to Weak Acceptance. There are still some concerns regarding experimental validation, I think the paper provides enough theoretical contribution to the community and would like to recommend acceptance. 
The paper proposes hierarchical Bayesian optimization (HiBO) for learning control policies from a small number of environment interaction and applies it to the postural control of a humanoid. Both reviewers raised issues with the clarity of presentation, as well as contribution and overall fit to this venue. The authors’ response helped to clarify these issues only marginally. Therefore, primarily due to lack of clarity, I recommend rejecting this paper, but encourage the authors to improve the presentation as per the reviewers’ suggestions and resubmitting.
Main content:  Blind review #1 summarizes it well:  This paper first introduces a method for quantifying to what extent a dataset split exhibits compound (or, alternatively, atom) divergence, where in particular atoms refer to basic structures used by examples in the datasets, and compounds result from compositional rule application to these atoms. The paper then proposes to evaluate learners on datasets with maximal compound divergence (but minimal atom divergence) between the train and test portions, as a way of testing whether a model exhibits compositional generalization, and suggests a greedy algorithm for forming datasets with this property. In particular, the authors introduce a large automatically generated semantic parsing dataset, which allows for the construction of datasets with these train/test split divergence properties. Finally, the authors evaluate three sequence to sequence style semantic parsers on the constructed datasets, and they find that they all generalize very poorly on datasets with maximal compound divergence, and that furthermore the compound divergence appears to be anticorrelated with accuracy.     Discussion:  Blind review #1 is the most knowledgeable in this area and wrote "This is an interesting and ambitious paper tackling an important problem. It is worth noting that the claim that it is the compound divergence that controls the difficulty of generalization (rather than something else, like length) is a substantive one, and the authors do provide evidence of this."     Recommendation and justification:  This paper deserves to be accepted because it tackles an important problem that is overlooked in current work that is evaluated on datasets of questionable meaningfulness. It adds insight by focusing on the qualities of datasets that enable testing how well learning algorithms do on compositional generalization, which is crucial to intelligence.
Main content:new training regime for multi resolution slimmable networks.   Discussion: reviewer 4: believes the main contribution of mutual learning from width and resolution is a bit weak reviewer 1: incremental work, details/baselines missing in experimental section reviewer 2: (least detailed): well written with good results Recommendation: I agree with reviewer 1, 4 that the experimental section could be improved. Leaning to reject.   
Motivated by GANs, the authors study the convergence of stochastic subgradient                                                      descent on convex concave minimax games.                                                                                            They introduced an improved "anchored" SGD variant, that provably converges                                                         under milder assumptions that the base algorithm.                                                                                   It is applied to training GANs on MNIST and CIFAR 10, partially showing                                                             improvements over alternative training methods.                                                                                                                                                                                                                         A main point of criticism that the reviewers identify is the strength of the                                                        assumptions needed for the analysis.                                                                                                Furthermore, the experimental results were deemed weak as the reported scores                                                       are far away from the SOTA, and only simple baselines were compared against.  
The paper considers an important topic of the warmup in deep learning, and investigates the problem of the adaptive learning rate. While the paper is somewhat borderline, the reviewers agree that it might be useful to present it to the  ICLR community.
This work proposes a self supervised segmentation method: building upon Crawford and Pineau 2019, this work adds a Monte Carlo based training strategy to explore object proposals. Reviewers found the method interesting and clever, but shared concerns about the lack of a better comparison to Crawford and Pineau, as well as generally a lack of care in comparisons to others, which were not satisfactorily addressed by authors response. For these reasons, we recommend rejection.
This paper proposes a two stage distillation from pretrained language models, where the knowledge distillation happens in both the pre training and the fine tune stages.  Experiments show improvement on BERT, GPT and MASS.  All reviewers pointed that the novelty of the work is very limited.
In my opinion, the main strength of this work is the theoretical analysis and some observations that may be of great interest to the NLP community in terms of better analyzing the performance of RL (and "RL like") methods as optimizers. The main weakness, as pointed out by R3, the limited empirical analysis.  I would urge the authors to take R3 s advice and attempt insofar as possible to broaden the scope of the empirical analysis in the final. I believe that this is important for the paper to be able to make its case convincingly.  Nonetheless, I do think that the paper makes a significant contribution that will be of interest to the community, and should be presented at ICLR. Therefore, I would recommend for it to be accepted.
The paper proposes a using pixel adaptive convolutions to leverage semantic labels in self supervised monocular depth estimation. Although there were initial concerns of the reviewers regarding the technical details and limited experiments, the authors responded reasonably to the issues raised by the reviewers. Reviewer2, who gave a weak reject rating, did not provide any answer to the authors comments. We do not see any major flaws to reject this paper.
This paper considers adversarial attacks in continuous action model based deep reinforcement learning. An optimisation based approach is presented, and evaluated on Mujoco tasks.  There were two main concerns from the reviewers. The first was that the approach requires strong assumptions, but in the rebuttal some relaxations were demonstrated (e.g., not attacking every step). Additionally, there were issues raised with the choice of baselines, but in the discussion the reviewers did not agree on any other reasonable baselines to use.  This is a novel and interesting contribution nonetheless, which could open the field to much additional discussion, and so should be accepted.
The paper presents an unsupervised method for graph representation, building upon Loukas  method for generating a sequence of gradually coarsened graphs. The contribution is an "encoder decoder" architecture trained by variational inference, where the encoder produces the embedding of the nodes in the next graph of the sequence, and the decoder produces the structure of the next graph.   One important merit of the approach is  that this unsupervised representation can be used effectively for supervised learning, with results quite competitive to the state of the art.   However the reviewers were unconvinced by the novelty and positioning of the approach. The point of whether the approach should be viewed as variational Bayesian, or simply variational approximation was much debated between the reviewers and the authors.   The area chair encourages the authors to pursue this very promising research, and to clarify the paper; perhaps the use of "encoder decoder" generated too much misunderstanding.  Another graph NN paper you might be interested in is "Edge Contraction Pooling for Graph NNs", by Frederik Diehl.  
This paper focuses on mitigating the effect of label noise. They provide a new class of loss functions along with a new stopping criteria for this problem. The authors claim that these new losses improves the test accuracy in the presence of label corruption and helps avoid memorization. The reviewers raised concerns about (1) lack of proper comparison with many baselines (2) subpar literature review and (3) state that parts of the paper is vague. The authors partially addressed these concerns and have significantly updated the paper including comparison with some of the baselines. However, the reviewers were not fully satisfied with the new updates. I mostly agree with the reviewers. I think the paper has potential but requires a bit more work to be ready for publication and can not recommend acceptance at this time. I have to say that the authors really put a lot of effort in their response and significantly improved their submission during the discussion period. I recommend the authors follow the reviewers  suggestions to further improve the paper (e.g. comparing with other baselines) for future submissions
This work propose a compression aware training (CAT) method to allows efficient compression of  feature maps during inference. I read the paper myself. The proposed method is quite straightforward and looks incremental compared with existing approaches based on entropy regularization.   
This paper presents an end to end technique for named entity recognition, that uses pre trained models so as to avoid long training times, and evaluates it against several baselines. The paper was reviewed by three experts working in this area. R1 recommends Reject, giving the opinion that although the paper is well written and results are good, they feel the technique itself has little novelty and that the main reason the technique works well is using BERT. R2 recommends Weak Reject based on similar reasoning, that the approach consists of existing components (albeit combined in a novel way) and suggest some ablation experiments to isolate the source of the good performance. R3 recommends Weak Accept but feels it is "unsurprising" that BERT allows for faster training and higher accuracy. In their response, authors emphasize that the application of pretraining to named entity recognition is new, and that theirs is a methodological advance, not purely a practical one (as R1 suggests and other reviews imply). They also argue it is not possible to do a fair ablation study that removes BERT, but make an attempt. The reviewers chose to keep their scores after the response. Given the split decision, the AC also read the paper. It is clear the paper has significant merit and significant practical value, as the reviews indicate. However, given that three expert reviewers   all of whom are NLP researchers at top institutions   feel that the contribution of the paper is weak (in the context of the expectations of ICLR) makes it not possible for us to recommend acceptance at this time. 
The paper proposed Channel Equilibrium (CE) to overcome the over sparsity problem in CNNs using  BN+ReLU . Experiments on ImageNet and COCO show its effectiveness by introducing little computational complexity. However the reviewers pointed a number of problems in the writing and the clarity of the paper. Although the authors addressed all the se concerns in details and agreed to make revisions in the paper, it s better for the authors to submit the revised version to another opportunity.
This paper proposes an approach to type inference in dynamically typed languages using graph neural networks. The reviewers (and the area chair) love this novel and useful application of GNNs to a practical problem, the presentation, the results. Clear accept.
The paper proposes a parallelization approach for speeding up scheduled sampling, and show significant improvement over the original.  The approach is simple and a clear improvement over vanilla schedule sampling.  However, the reviewers point out that there are more recent methods to compare against or combine with, and that the paper is a bit thin on content and could have addressed this.  The proposed approach may well combine well with newer techniques, but I tend to agree that this should be tested.
The paper proposes a new objective function called ICE for metric learning.  There was a substantial discussion with the authors about this paper. The two reviewers most experienced in the field found the novelty compared to the vast existing literature lacking, and remained unconvinced after the discussion. Some reviewers also found the technical presentation and interpretations to need improvement, and this was partially addressed by a new revision.  Based on this discussion, I recommend a rejection at this time, but encourage the authors to incorporate the feedback and in particular place the work in context more fully, and resubmit to another venue.
The paper proposes two approaches to topic modeling supervised by survival analysis. The reviewers find some problems in novelty,  algorithm and experiments, which is not ready for publish.
This paper proposes an extension of Gradient Episodic Memory (GEM) namely support examples, soft gradient constraints, and positive backward transfer. The authors argue that experiments on MNIST and CIFAR show that the proposed method consistently improves over the original GEM.  All three reviewers are not convinced with experiments in the paper. R1 and R3 mentioned that the improvements over GEM appear to be small. R2 and R3 also have some concerns without results with multiple runs. R3 has questions about hyperparameter tuning. The authors also appears to be missing recent developments in this area (e.g., A GEM). The authors did not provide a rebuttal to these concerns.  I agree with the reviewers and recommend rejecting this paper.
The authors propose a conditional GAN based approach for generating faces consistent with given input speech.  The technical novelty is not large, as the approach is mainly putting together existing ideas, but the application is a fairly new one and the experiments and results are convincing.  The approach might also have broader applicability beyond this task.
The authors challenge the idea that good representation in RL lead are sufficient for learning good policies with an interesting negative result   they show that there exist MDPs which require an exponential number of samples to learn a near optimal policy even if a good but not perfect representation is given to the agent for both value based and policy based learning.  Reviewers had some minor technical questions which were clarified sufficiently by the authors, leading to a consensus of the contribution and quality of this work.  Thus, I recommend this paper for acceptance.
The paper explores in more detail the "RL as inference" viewpoint and highlights some issues with this approach, as well as ways to address these issues. The new version of the paper has effectively addressed some of the reviewers  initial concerns, resulting in an overall well written paper with interesting insights.
The paper proposes to use the mirror descent algorithm for the binary network. It is easy to read. However, novelty over ProxQuant is somehow limited. The theoretical analysis is weak, in that there is no analysis on the convergence and neither how to choose the projection for mirror mapping construction. Experimental results can also be made more convincing, by adding comparisons with bigger datasets, STOA networks, and ablation study to demonstrate why mirror descent is better than proximal gradient descent in this application.
This paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a tree structured autoencoder.  I side with the two more confident reviewers, and argue that this paper doesn t offer sufficient evidence that this method is useful in the proposed setting. I m particularly swayed by R1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed. I m also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a tree structured decoder encourages the model to generate grammatical sentences—I see no reason why this should be the case in the setting described here, and the paper doesn t seem to offer evidence to back this up.
This paper proposes to introduce perturbation biases as a counter measure against adversarial perturbations. The perturbation biases are additional bias terms that are trained by a variant of gradient ascent. Serious issues were raised in the comments. No rebuttal was provided.
The submission proposes a method for learning a graph structure and node embeddings through an iterative process. Smoothness and sparsity are both optimized in this approach. The iterative method has a stopping mechanism based on distance from a ground truth.   The concerns of the reviewers were about scalability and novelty. Since other methods have used the same costs for optimization, as well as other aspects of this approach, there is little contribution other than the iterative process. The improvement over LDS, the most similar approach, is relatively minor.   Although the paper is promising, more work is required to establish the contributions of the method. Recommendation is for rejection. 
This paper introduces a few ideas to potentially improve the performance of neural ODEs on graph networks.  However, the reviewers disagreed about the motivations for the proposed modifications.  Specifically, it s not clear that neural ODEs provide a more advantageous parameterization in this setting than standard discrete networks.  It s also not clear at all why the authors are discussion graph neural networks in particular, as all of their proposed changes would apply to all types of network.  Another major problem I had with this paper was the assertion that the running the original system backwards leads to large numerical error.  This is a plausible claim, but it was never verified.  It s extremely easy to check (e.g. by comparing the reconstructed initial state at t0 with the true original state at t0, or by comparing gradients computed by different methods).  It s also not clear if the authors enforced the constraints on their dynamics function needed to ensure that a unique solution exists in the first place.
This paper addresses the problem of differential private data generator. The paper presents a novel approach called G_PATE which builds on the existing PATE framework. The main contribution is in using a student generator with an ensemble of teacher discriminators and in proposing a new private gradient aggregation mechanism which ensures differential privacy in the information flow from discriminator to generator.  Although the idea is interesting, there are significant concerns raised by the reviewers about the experiments and analysis done in the paper which seem to be valid and have not been addressed yet in the final revision. I believe upon making significant changes to the paper, this could be a good contribution. Thus, as of now, I am recommending a Rejection.
The paper proposed an autoregressive model with a multiscale generative representation of the spectrograms to better modeling the long term dependencies in audio signals. The techniques developed in the paper are novel and interesting. The main concern is the validation of the method. The paper presented some human listening studies to compare long term structure on unconditional samples, which as also mentioned by reviewers are not particularly useful. Including justifications on the usefulness of the learned representation for any downstream task would make the work much more solid. 
This paper proposes a recurrent architecture based on a recursive gating mechanism. The reviewers leaned towards rejection on the basis of questions regarding novelty, analysis, and the experimental setting. Surprisingly, the authors chose not to engage in discussion, as all reviewers seems pretty open to having their minds changed. If none of the reviewers will champion the paper, and the authors cannot be bothered to champion their own work, I see no reason to recommend acceptance.
This paper formalizes the concept of buffer zones, and proposes a defense method based on a combination of deep neural networks and simple image transformations. The authors argue that the proposed method based on buffer zones is robust against state of the art black box attacks methods.This paper, however, falls short of (1) unjustified claims (e.g., buffer zones are widened when the models are diverse); (2) incomplete literature survey and related work; (3) similar ideas are well known in the literature, (4) unfair experimental evaluations and many others. Even after author response, it still does not gather support from the reviewers. Thus I recommend reject.
This paper presents OmniNet, an architecture based on the popular transformer for learning on data from multiple modalities and predicting on multiple tasks.  The reviewers found the paper well written, technically sound and empirically thorough.  However, overall the scores fell below the bar for acceptance and none of the reviewers felt strongly enough to  champion  the paper for acceptance.  The primary concern cited by the reviewers was a lack of strong baselines, i.e. comparison to other methods for multi task learning.  Unfortunately, as such the recommendation is to reject.  However, adding a thorough comparison to existing literature empirically and in the related work would make this a much stronger submission to a future conference.
The authors develop a strategy to learn branching strategies for branch and bound based neural network verification algorithms, based on GNNs that imitate strong branching. This allows the authors to obtain significant speedups in branch and bound based neural network verification algorithms relative to strong baselines considered in prior work.  The reviewers were in consensus and the quality of the paper and minor concerns raised in the initial reviews were adequately addressed in the rebuttal phase.   Therefore, I strongly recommend acceptance.
This paper provide an extensive set of benchmarks for Deep Model based RL algorithms.  This paper contains a large number of algorithms, environments, and empirical results. The reviewers all recognized the need for such a study to provide some clarity, insights, and common standards. The reviewers we concerned about several aspects of the implementation of the effort. (1) All the performance is based on 4 runs, smoothed curves, and default errors (often extensively overlapping). The paper cites Henderson et al, and yet does not follow the advice laid out therein. (2) The results were fairly inconclusive perhaps to be expected we didn t learn much (more on this below). (3) The paper has communication issues.  The overall approach taken was a bit perplexing. Some algorithms we given access to the dynamics. The reward functions were converted to diff. forms, and early stopping in a domain specific way was employed. This all seems like simplifying the problem in different ways so that some methods can be competitive, but it is not at all clear why. If we take the typical full rl problem and limit domain knowledge, many of these approaches cannot be applied and others will fail. Those are the results we want. One could actually view these choices are unfair to more general algorithms algorithms that need diff rewards pay no price for this assumption. This also leads to funny things, for example, like using position as the reward in mountain car (totally non standard, and invalid without discounting). The paper claims a method can solve MC, but that is unclear from the graph. The paper motivates the entire enterprise based on the claimed lack of standardization in the literature, but then proceeds to redefine classic control tasks with little discussion or explanation.   The paper has communication issues. For example, all the domains are use continuous actions (and the others in the response highlight that is their main focus), but this is never stated in the paper. The paper refers to and varies "environment length", but this was not defined in the paper and has no obvious meaning. The tasks are presumably discounted but the the value of gamma is not specified anywhere in the paper (could be there, but I searched for a while). Pages of parameter settings in the appendix with many not discussed or their ranges justified.  This paper is ambitious, but I urge the authors to perhaps limit the scope and do less, and consider a slightly broader audience in both the writing and experiment design.    
This paper combines PEARL with HAC to create a hierarchical meta RL algorithm that operates on goals at the high level and learns low level policies to reach those goals. Reviewers remarked that it’s well presented and well organized, with enough details to be mostly reproducible. In the experiments conducted, it appears to show strong results.  However there was strong consensus on two major weaknesses that render this paper unpublishable in its current form: 1) the continuous control tasks used don’t seem to require hierarchy, and 2) the baselines don’t appear to be appropriate. Reviewers remarked that a vital missing baseline is HER, and that it’s unfair to compare to PEARL, which is a more general meta RL algorithm. The authors don’t appear to have made revisions in response to these concerns.  All reviewers made useful and constructive comments, and I urge the authors to take them into consideration when revising for a future submission.
The paper studies the variance reduced TD algorithm by Konda and Prashanth (2015). The original paper provided a convergence analysis that had some technical issues. This paper provides a new convergence analysis, and shows the advantage of VRTD to vanilla TD in terms of reducing the bias and variance. Several of the five reviewers are expert in this area and all of them are positive about it. Therefore, I recommend acceptance of this work.
This paper demonstrates that per image semantic supervision, as opposed to class only supervision, can benefit zero shot learning performance in certain contexts.  Evaluations are conducted using CUB and FLOWERS fine grained zero shot data sets.  In terms of evaluation, the paper received mixed final scores (two reject, one accept).  During the rebuttal period, both reject reviewers considered the author responses, but in the end did not find the counterarguments sufficiently convincing.  For example, one reviewer maintained that in its present form, the paper appeared too shallow without additional experiments and analyses to justify the suitability of the contrastive loss used for obtaining embeddings applied to zero shot learning.  Another continued to believe post rebuttal that reference Reed et al., (2016) undercut the novelty of the proposed approach.  And consistent with these sentiments, even the reviewer who voted for acceptance alluded to the limited novelty of the proposed approach; however, the author response merely states that a future revision will clarify the novelty.  But this then requires another round of reviewing to determine whether the contribution is sufficiently new, especially given that all reviewers raised this criticism in one way or another.  Furthermore, the rebuttal also mentions the inclusion of some additional experiments, but again, we don t know how these will turn out.  Based on these considerations then, the AC did not see sufficient justification for accepting a paper with aggregate scores that are otherwise well below the norm for successful ICLR submissions.
This paper presents an attention based approach to transfer faster CNNs, which tackles the problem of jointly transferring source knowledge and pruning target CNNs.  Reviewers are unanimously positive on the paper, in terms of a well written paper with a reasonable approach that yields strong empirical performance under the resource constraint.  AC feels that the paper studies an important problem of making transfer learning faster for CNNs, however, the proposed model is a relatively straightforward combination of fine tuning and filter pruning, each having very extensive prior works. Also, AC has very critical comments for improving this paper:    The Attentive Feature Distillation (AFD) module is very similar to DELTA (Li et al. ICLR 2019) and L2T (Jang et al. ICML 2019), significantly weakening the novelty. The empirical evaluation should consider DELTA as baselines, e.g. AFS+DELTA.  I accept this paper, assuming that all comments will be well addressed in the revision.
This work investigates neural network pruning through the lens of its influence over specific exemplars (which are found to often be lower quality or mislabelled images) and how removing them greatly helps metrics. The insight from the paper is interesting, as recognized by reviewers. However, experiments do not suggest that the findings shown in the paper would generalize to more pruning methods. Nor do the authors give directions for tackling the "hard exemplar" problem. Authors  response did provide justifications and clarifications, however the core of the concern remains. Therefore, we recommend rejection.
The paper investigates the sensitivity of a QA model to perturbations in the input, by replacing content words, such as named entities and nouns, in questions to make the question not answerable by the document. Experimental analysis demonstrates while the original QA performance is not hurt, the models become significantly less vulnerable to such attacks. Reviewers all agree that the paper includes a thorough analysis, at the same time they all suggested extensions to the paper, such as comparison to earlier work, experimental results, which the authors made in the revision. However, reviewers also question the novelty of the approach, given data augmentation methods. Hence, I suggest rejecting the paper.
This paper presents a guide for setting hyperparameters when fine tuning from one domain to another. This is an important problem as many practical deep learning applications repurpose an existing model to a new setting through fine tuning.  All reviewers were positive saying that this work provides new experimental insights, especially related to setting momentum parameters. Though other works may have previously discussed the effect of momentum during fine tuning, this work presented new experiments which contributes to the overall understanding. Reviewer 3 had some concern about the generalization of the findings to other backbone architectures, but this concern was resolved during the discussion phase. The authors have provided detailed clarifications during the rebuttal and we encourage them to incorporate any remaining discussion or any new clarifications into the final draft. 
All the reviewers recommend rejecting the paper. There is no basis for acceptance.
The paper investigates the effect of convolutional information bottlenecks to generalization. The paper concludes that the width and height of the bottleneck can greatly influence generalization, whereas the number of channels has smaller effect. The paper also shows evidence against a common belief that CAEs with sufficiently large bottleneck will learn an identity map.   During the rebuttal period, there was a long discussion mainly about the sufficiency of the experimental setup and the trustworthiness of the claims made in the paper. A paper that empirically investigates an exiting method or belief should include extensive experiments of high quality in to enable general conclusions. I’m thus recommending rejection, but encourage the authors to improve the experiments and resubmitting.
Many existing approaches in multi task learning rely on intuitions about how to transfer information. This paper, instead, tries to answer what does "information transfer" even mean in this context. Such ideas have already been presented in the past, but the approach taken here is novel, rigorous and well explained.  The reviewers agreed that this is a good paper, although they wished to see the analysis conducted using more practical models.   For the camera ready version it would help to make the paper look less dense. 
The paper presents an extension of FID for conditional generation settings. While it s an important problem to address, the reviewers were concerned about the novelty and advantage of the proposed method over the existing methods. The evaluation is reported on toy datasets, and the significance is limited.
This paper proposes new target objectives for training random forests for better cross domain generalizability.   As reviewers mentioned, I think the idea of using random forests for domain adaptation is novel and interesting, while the proposed method has potential especially in the noisy settings. However, I think the paper can be much improved and is not ready to publish due to the following reviewers  comments:    This paper is not well written and has too many unclear parts in the experiments and method section. The results are not guaranteed to be reproducible given the content of the paper. Also, the organization of the paper could be improved.    The open set domain adaptation setting requires more elaboration. More carefully designed experiments should be presented.     It remains unclear how the feature extractors can be trained or fine tuned in the DNN + tree architecture. Applying trees to high dimensional features sacrifices the interpretability of the tree models, hampering the practical value of the approach.  Hence, I recommend rejection.
The paper introduces an interesting application of GNNs, but the reviewers find that the contribution is too limited and the motivation is too weak.
The authors propose a new perspective on active learning by borrowing concepts from subjective logic. In particular, they model uncertainty as a combination of dissonance and vacuity; two orthogonal forms of uncertainty that may invite additional labels for different reasons. The concepts introduced are not specific to deep learning but are generally applicable. Experiments on 2d data and a couple standard datasets are provided.  The derivation of the model is intuitive but it s not clear that it is "better" than any other intuitively derived model for active learning. With the field of active learning having such a long history, the field has moved towards a standard of expecting theoretical guarantees to distinguish a new method from the rest; this paper provides none. Instead anecdotal examples and small experiments are performed. Like other reviews, I am extremely skeptical about the use of KDE which is known to have essentially no inferential ability in high dimensions (such as in deep learning situations where presumably images are involved). It is hard not to feel as though deep learning is somewhat of a red herring in this paper.   I recommend the authors lean into understanding the method from a perspective beyond anecdotes and experiments if they wish for this method to gain traction. 
This paper provides an improved method for deep learning on point clouds.  Reviewers are unanimous that this paper is acceptable, and the AC concurs. 
Based on current unanimous reviews, the paper is accepted.
The submission proposes to improve generalization in RL environments, by addressing the scenario where the observations change even though the underlying environment dynamics do not change. The authors address this by learning an adaptation function which maps back to the original representation. The approach is empirically evaluated on the Mountain Car domain.   The reviewers were unanimously unimpressed with the experiments, the baselines, and the results. While they agree that the problem is well motivated, they requested additional evidence that the method works as described and that a simpler approach such as fine tuning would not be sufficient.   The recommendation is to reject the paper at this time.
All reviewers come to agreement that this is a solid paper worth publishing at ICLR; the authors are encouraged to incorporate additional comments suggested by reviewers.
The authors propose a framework for incorporating homogeneous linear inequality constraints on neural network activations into neural network architectures. The authors show that this enables training neural networks that are guaranteed to satisfy non trivial constraints on the neurons in a manner that is significantly more scalable than prior work, and demonstrate this experimentally on a generative modelling task.  The problem considered in the paper is certainly significant (training neural networks that are guaranteed to satisfy constraints arises in many applications) and the authors make some interesting contributions. However, the reviewers found the following issues that make it difficult to accept the paper in its present form: 1) The setting of homogeneous linear equality constraints is not well motivated and the significance of being able to impose such constraints is not clearly articulated in the paper. The authors would do well to prepare a future revision documenting use cases motivated by practical applications and add these to the paper. 2) The experimental evaluation is not sufficiently thorough: the authors evaluate their method on an artificial constraint involving a "checkerboard pattern" on MNIST. Even in this case, the training method proposed by the authors seems to suffer from some issues, and more thorough experiments need to be conducted to confirm that the training method can perform well across a variety of datasets and constraints.  Given these issues, I recommend rejection. However, I encourage the authors to revise their work on this important topic and prepare a future version including practical examples of the constraints and experiments on a variety of prediction tasks.  
This paper proposes Bayesian quantized networks and efficient algorithms for learning and prediction of these networks. The reviewers generally thought that this was a novel and interesting paper.  There were a few concerns about the clarity of parts of the paper and the experimental results. These concerns were addressed during the discussion phase, and the reviewers agree that the paper should be accepted.
All authors agree the paper is well written, and there is a good consensus on acceptance.  The last reviewer was concerned about a lack of diversity in datasets, but this was addressed in the rebuttal.
This work proposes a CNN architecture for joint depth and camera motion estimation from videos. The paper presents a differentiable formulation of the problem to allow its end to end learning, and the reviewers unanimously find the proposed approach reasonable and agree that this is a solid paper. Some of the reviewers find the method itself to be too mechanical, but they all agree that this is a well engineered solution.
The paper proposes a new, simple method for sparsifying deep neural networks.                                                       It use as temporary, pruned model to improve pruning masks via SGD, and eventually                                                  applying the SGD steps to the dense model.                                                                                          The paper is well written and shows SOTA results compared to prior work.                                                                                                                                                                                                The authors unanimously recommend to accept this work, based on simplicity of                                                       the proposed method and experimental results.                                                                                                                                                                                                                           I recommend to accept this paper, it seems to make a simple, yet effective                                                          contribution to compressing large scale models.                                                                                                                  
The reviewers reached a consensus that the paper is preliminary and has a very limited contribution. Therefore, I cannot recommend acceptance at this time.
The paper provides a proof that Transformer networks (a popular deep learning model) are universal approximators for sequence to sequence functions. The theorem relies on the idea of contextual mappings (Definition 3.1), which models the attention layers. The results provide an important starting point for understanding a very widely used architecture.  As with many theoretical papers, the reviewers provided several suggestions as to which are important parts to be presented in the main paper. The authors were very responsive during the discussion period, updating the structure of the paper significantly. This shows nice evidence supporting the need for a long discussion period for ICLR. One reviewer upgraded their score (to 8), which is not reflected in the system.  This is an excellent paper, providing much needed theoretical analysis of a popular neural architecture. Clear accept.  
This paper trains a transformer to extrapolate learning curves, and uses this in a model based RL framework to automatically tune hyperparameters. This might be a good approach, but it s hard to know because the experiments don t include direct comparisons against existing hyperparameter optimization/adaptation techniques (either the ones based on extrapolating training curves, or standard ones like BayesOpt or PBT). The presentation is also fairly informal, and it s not clear if a reader would be able to reproduce the results. Overall, I think there s significant cleanup and additional experiments needed before publication in ICLR. 
This paper proposes a noise aware knowledge graph embedding (NoiGAN) by combining KG completion and noise detection through the GANs framework. The reviewers find that the idea is interesting, but the comparison to SOTA is largely missing. The paper can be improved by addressing the reviewer comments. 
One of the reviewers pointed out similarity to existing very recent work which would require significant reframing of the current paper. Hence, this work is below the bar at the moment.
The paper proposes a hierarchical Bayesian model over multiple data sets that                                                       has both data set specific as well as shared parameters.                                                                            The data set specific parameters are further encouraged to only capture aspects                                                     that vary across data sets by an addition mutual information contribution to the                                                    training loss.                                                                                                                      The proposed method is compared to standard VAEs on multiple data sets.                                                                                                                                                                                                 The reviewers agree that the main approach of the paper is sensible. However,                                                       concerns were raised about general novelty, about the theoretical justification                                                     for the proposed loss function and about the lack of non trivial baselines.                                                          The authors  rebuttal did not manage to full address these points.                                                                                                                                                                                                      Based on the reviews and my own reading, I think this paper is slightly                                                             below acceptance threshold.
The paper proposes to compress convolutional neural networks via weight sharing across filters of each convolution layer. A fast convolution algorithm is also designed for the convolution layer with this approach. Experimental results show (i) effectiveness in CNN compression, (ii) acceleration on the tasks of image classification, object detection and neural architecture search. While the authors addressed most of reviewers  concerns, the weakness of the paper which remains is that no wall clock runtime numbers (only FLOPS) are reported   so efficiency of the approach in practice in uncertain. 
Main summary: Novel rule for scaling learning rate, known as gain ration, for which the effective batch size is increased.  Discussion:  reviewer 2: main concern is reviewer can t tell if it s better of worse than linear learning rate scaling from their experiment section. reviewer 3: novlty/contribution is a bit too low for ICLR. reviewer 1: algorthmic clarity lacking. Recommendation: all 3 reviewers recommend reject, I agree.
The authors present a hierarchical explanation model for understanding the underlying representations produced by LSTMs and Transformers.  Using human evaluation, they find that their explanations are better, which could lead to better trust of these opaque models.  The reviewers raised some issues with the derivations, but the author response addressed most of these.  
The main concern raised by the reviewers is limited experimental work, and there is no rebuttal.
The reviewers attempted to give this paper a fair assessment, but were unanimous in recommending rejection.  The technical quality of motivation was questioned, while the experimental evaluation was not found to be clear or convincing.  Hopefully the feedback provided can help the authors improve their paper.
The paper proposes an extension to the popular Generative Adversarial Imitation Learning framework that considers multi agent settings with "correlated policies", i.e., where agents  actions influence each other. The proposed approach learns opponent models to consider possible opponent actions during learning. Several questions were raised during the review phase, including clarifying questions about key components of the proposed approach and theoretical contributions, as well as concerns about related work. These were addressed by the authors and the reviewers are satisfied that the resulting paper provides a valuable contribution. I encourage the authors to continue to use the reviewers  feedback to improve the clarity of their manuscript in time for the camera ready submission.
This work proves that the weights of feed forward ReLU networks are determined, up to a specified set of symmetries, by the functions they define. Reviewers found the paper easy to read and the proof technically sound. There was some debate over the motivation for the paper, Reviewer 1 argues that there is no practical significance for the result, a point that the authors do not deny. I appreciate the concerns raised by Reviewer 1, theorists in machine learning should think carefully about the motivation for their work. However, while there is no clear practical significance of this work, I believe there is value to accepting it. Because the considered question concerns a sufficiently fundamental property of neural networks, and the proof is both easy to read and provides insights into a well studied class of models, I believe many researchers will find value in reading this paper.
The paper proposes a method of training latency limited (wait k) decoders for online machine translation. The authors investigate the impact of the value of k, and of recalculating the transformer s decoder hidden states when a new source token arrives. They significantly improve over state of the art results for German English translation on the WMT15 dataset, however there is limited novelty wrt previous approaches. The authors responded in depth to reviews and updated the paper with improvements, for which there was no reviewer response. The paper presents interesting results but IMO the approach is not novel enough to justify acceptance at ICLR.  
This paper presents a method for out of distribution detection under the condition of access to only a few positive labeled samples. The main contribution as summarized by reviewers and authors is the new proposed benchmark and problem statement.   All reviewers are in agreement that this paper is not ready for publication in its current form. The main concern is around the validity of the problem statement. The reviewers seek more clarity motivating the proposed scenario. Though the authors argue that as few shot recognition is very difficult and may benefit from strategies like active learning, it is not directly clear how out of distribution detection is the best approach. In addition, R3 seeks clarification on the similarity to existing work.   Considering the unanimous opinions of the reviewers and all author rebuttal text, the AC does not recommend acceptance of this work. We encourage the authors to focus their revisions on the explanation and motivation of this new benchmark and submit to a future venue.  
The authors propose an approach for anomaly detection in the setting where the training data includes both normal and anomalous data.  Their approach is a fairly straightforward extension of existing ideas, in which they iterate between clustering the data into normal vs. anomalous and learning an autoencoder representation of normal data that is then used to score normality of new data.  The results are promising, but the experiments are fairly limited.  The authors argue that their experimental settings follow those of prior work, but I think that for such an incremental contribution, more empirical work should be done, regardless of the limitations of particular prior work.
This paper proposes a metalearning objective to infer causal graphs from data based on masked neural networks to capture arbitrary conditional relationships. While the authors agree that the paper contains various interesting ideas, the theoretical and conceptual underpinnings of the proposed methodology are still lacking and the experiments cannot sufficiently make up for this. The method is definitely worth exploring more and a revision is likely to be accepted at another venue.
This paper proposes a flexible environment for studying never ending learning. During the discussion period, all reviewers found the paper to be borderline.  Pros:   we don t have good lifelong or never ending RL environments, and this paper seems to provide one   includes a number of interesting features such as multiple input modalities, non episodic interactions, flexible task definitions  Cons:   procedurally generated, toy environment   unclear if the environment reflects the characteristics of real world NEL problems  In the balance, I think the environments add value to the RL community, and being presented at ICLR would increase its visibility.
The paper introduces a new pooling approach "Laplacian pooling" for graph neural networks and applies this to molecular graphs. While the paper has been substantially improved from its original form, there are still various concerns regarding performance and interpretability that remain unanswered. In its current form the paper is not ready for acceptance to ICLR 2020.
The reviewers recommend rejection due to various concerns about novelty and experimental validation. The authors have not provided a response.
This paper extends adversarial imitation learning to an adaptive setting where environment dynamics change frequently. The authors propose a novel approach with pragmatic design choices to address the challenges that arise in this setting. Several questions and requests for clarification were addressed during the reviewing phase. The paper remains borderline after the rebuttal. Remaining concerns include the size of the algorithmic or conceptual contribution of the paper.
All reviewers agree that this paper is not ready for publication.
All three reviewers felt the paper should be rejected and no rebuttal was offered. So the paper is rejected.
There are several concerns with the brittleness and reproducibility of the proposed approach and experiments.
The paper develops a new method for pruning generators of GANs. It has received a mixed set of reviews. Basically, the reviewers agree that the problem is interesting and appreciate that the authors have tried some baseline approaches and verified/demonstrated that they do not work.   Where the reviewers diverge is on whether the authors have been successful with the new method. In the opinion of the first reviewer, there is little value in achieving low levels (e.g. 50%) of fine grained sparsity, while the authors have not managed to achieve good performance with filter level sparsity (as evidenced by Figure 7, Table 3 as well as figures in the appendices). The authors admit that the sparsity levels achieved with their approach cannot be turned into speed improvement without future work.  Furthermore, as pointed out by the first reviewer, the comparison with prior art, in particular with LIT method, which has been reported to successfully compress the same GAN, is missing and the results of LIT have been misrepresented. While the authors argue that their pruning is an "orthogonal technique", and can be applied on top of LIT, this is not verified in any way. In practice, combination of different compression techniques is known to be non trivial, since they aim to explain the same types of redundancies.  Overall, while this paper comes close, the problems highlighted by the first reviewer have not been resolved convincingly enough for acceptance.
The article studies benefits of over parametrization and theoretical properties at initialization in ReLU networks. The reviewers raised concerns about the work being very close to previous works and also about the validity of some assumptions and derivations. Nonetheless, some reviewers mentioned that the analysis might be a starting point in understanding other phenomena and made some suggestions. However, the authors did not provide a rebuttal nor a revision. 
The authors develop theoretical results showing that policy gradient methods converge to the globally optimal policy for a class of MDPs arising in econometrics. The authors show empirically that their methods perform on a standard benchmark.  The paper contains interesting theoretical results. However, the reviewers were concerned about some aspects: 1) The paper does not explain to a general ML audience the significance of the models considered in the paper   where do these arise in practical applications? Further, the experiments are also limited to a small MDP   while this may be a standard benchmark in econometrics, it would be good to study the algorithm s scaling properties to larger models as is standard practice in RL.  2) The implications of the assumptions made in the paper are not explained clearly, nor are the relative improvements of the authors  work relative to prior work. In particular, one reviewer was concerned that the assumptions could be trivially satisfied and the authors  rebuttal did not clarify this sufficiently.  Thus, I recommend rejection but am unsure since none of the reviewers nor I am an expert in this area.
This paper proposes a self attention based autoregressive model called Axial Transformers for images and other data organized as high dimensional tensors. The Axial Attention is applied within each axis of the data to accelerate the processing.  Most of the authors claim that main idea behind Axial Attention is widely applicable, which can be used in many core vision tasks, such as detection and classification. However, the revision fails to provide more application for Axial attention.  Overall, the idea behind this paper is interesting but more convincing experimental results are needed. 
This paper introduces a NAS algorithm based on multi agent optimization, treating each architecture choice as a bandit and using an adversarial bandit framework to address the non stationarity of the system that results from the other bandits running in parallel.  Two reviewers ranked the paper as a weak accept and one ranked it as a weak reject. The rebuttal answered some questions, and based on this the reviewers kept their ratings. The discussion between reviewers and AC did not result in a consensus. The average score was below the acceptance threshold, but since it was close I read the paper in detail myself before deciding.  Here is my personal assessment:  " Positives: 1. It is very nice to see some theory for NAS, as there isn t really any so far. The theory for MANAS itself does not appear to be very compelling, since it assumes that all but one bandit is fixed, i.e., that the problem is stationary, which it clearly isn t. But if I understand correctly, MANAS LS does not have that problem. (It would be good if the authors could make these points more explicit in future versions.)  2. The absolute numbers for the experimental results on CIFAR 10 are strong.  3. I welcome the experiments on 3 additional datasets.  Negatives: 1. The paper crucially omits a comparison to random search with weight sharing (RandomNAS WS) as introduced by Li & Talwalkar s paper "Random Search and Reproducibility for Neural Architecture Search" (https://arxiv.org/abs/1902.07638), on arXiv since February and published at UAI 2019. This method is basically MANAS without the update step, using a uniform random distribution at step 3 of the algorithm, and therefore would be the right baseline to see whether the bandits are actually learning anything. RandomNAS WS has the same memory improvements over DARTS as MANAS, so this part is not new. Similarly, there is GDAS as another recent approach with the same low memory requirement: http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Searching_for_a_Robust_Neural_Architecture_in_Four_GPU_Hours_CVPR_2019_paper.html This is my most important criticism.  2. I think there may be a typo somewhere concerning the runtimes of MANAS. It would be extremely surprising if MANAS truly takes 2.5 times longer when run with 20 cells and 500 epochs than when run with 8 cells and 50 epochs. It would make sense if MANAS gets 2.5 slower when just going from 8 to 20 cells, but when going from 50 to 500 epochs the cost should go up by another factor of 10. And the text states specifically that "for datasets other than ImageNet, we use 500 epochs during the search phase for architectures with 20 cells, 400 epochs for 14 cells, and 50 epochs for 8 cells". Therefore, I think either that text is wrong or MANAS got 10x more budget than DARTS.  3. Figure 2 shows that on Sport 8, MANAS actually does *significantly worse* when searching on 14 cells than on 8 cells (note the different scale of the y axis). It s also slightly better with 8 cells on MIT 67. I recommend that the authors discuss this in the text and offer some explanation, rather than have the text claim that 14 cells are better and the figure contradict this. Only for MANAS LS, the 14 cell version actually works better.  4. The authors are unclear about whether they compare to random search or random sampling. These are two different approaches. Random sampling (as proposed by Sciuto et al, 2019) takes a single random architecture from the search space and compares to that. Standard random search iteratively samples N random architectures and evaluates them (usually on some proxy metric), selecting and retraining the best one found that way. The number N is chosen for random search to use the same computational resources as the method being compared. The authors call their method random search but then appear to be describing random sampling.  Also, with several recent papers showcasing problems in NAS evaluation (many design decisions affect NAS performance), it would be a big plus to have code available to ensure reproducibility. Many ICLR papers are submitted with an anonymized code repository, and if possible, I would encourage the authors to do this for a future version. "  The prior rating based on the reviewers was slightly below the acceptance threshold, and my personal judgement did not push the paper above the acceptance threshold. I encourage the authors to improve the paper by addressing the reviewer s points and the points above and resubmit to a future venue. Overall, I believe this is very interesting work and am looking forward to a future version.
Based on the Bayesian approach to filtering problem, the paper proves that RNN are universal approximators for the filtering problem.  Two reviewers, however, have doubts about the novelty and difficulty to get the result. Although I do not fully agree that Reviewer3 that the proof is just "DNN can fit anything"   it is not this case, but the concerns of Reviewer2 are more strong, especially about the usage of the term "recurrent neural network". The paper is purely theoretical and does not have any numerical experiments, which probably makes it too weak for ICLR in this form. However, I encourage the authors to continue to work on the subject, since the approach looks very interesting but it still very far from practice.
The paper presents a general view of supervised learning models that are jointly trained with a model for embedding the labels (targets), which the authors dub target embedding autoencoders (TEAs).  Similar models have been studied before, but this paper unifies the idea and studies more carefully various components of it.  It provides a proof for the specific case of linear models and a set of experiments on disease trajectory prediction tasks.  The reviewer concerns were addressed well by the authors and I believe the paper is now strong.  It would be even stronger if it included more tasks (and in particular some "typical" tasks that more of the community is focusing on), and the theoretical part is to my mind not a major contribution, or at least not as large as the paper implies, because it analyzes a much simpler model than anyone is likely to use TEAs for.
The scores of the reviewers are just far to low to warrant an acceptance recommendation from the AC.
The authors presented a Federate Learning algorithm which constructs the global model layer wise by matching and averaging hidden representations. They empirically demonstrate their method outperforms existing federated learning algorithms  This paper has received largely positive reviews. Unfortunately one reviewer wrote a very short review but was generally appreciative of the work. Fortunately, R1 wrote a detailed review with very specific questions and suggestions. The authors have addresses most of the concerns of the reviewers and I have no hesitation in recommending that this paper should be accepted. I request the authors to incorporate all suggestions made by the reviewers. 
The authors propose to use numerical differentiation to approximate the Jacobian while estimating the parameters for a collection of Hidden Markov Models (HMMs). Two reviewers provided detailed and constructive comments, while unanimously rated weak rejection. Reviewer #1 likes the general idea of the work, and consider the contribution to be sound. However, he concerns the reproducibility of the work due to the niche database from e commerce applications. Reviewer #2 concerns the poor presentation, especially section 3. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.
This paper proposes an end to end approach for abstractive summarization of on line discussions. The approach is contrary to the previous work that first disentangles discussions, and the summarizes them, and aims to tackle transfer of disentanglement errors in the pipeline. The proposed method is a hierarchical encoder   hierarchical decoder architecture. Experimental results on two corpora demonstrate the benefits of the proposed approach. The reviewers are concerned about the synthetic nature of the datasets, limited novelty given the previous work, lack of clear explanation of whether disentanglement is actually needed for summarization, and simpler baselines in comparison to the state of the art. Hence, I recommend rejecting the paper.
This paper presents a synthetic oversampling method for sequence to sequence classification problems based on autoencoders and generative adversarial networks.   All reviews reject the paper for two main reasons: 1 The novelty of the paper is not enough for ICLR as the idea of utilizing GAN for data sampling is common now. 2 The experimental is not convincing as authors did not compare with other leading oversampling methods.  The rebuttal did not well answer these two questions; thus I choose to reject the paper. 
This work extends previous work (Castellini et al) with parameter sharing and  low rank approximations, for pairwise communication between agents.  However the work as presented here is still considered too incremental, in particular when compared to Castellini et al. The advances such as parameter sharing and low rank approximation are good but not enough of a contribution. Authors  efforts to address this concern did not change reviewers  judgment. Therefore, we recommend rejection.
Main content:  Blind review #2 summarizes it well:  This paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set.     Discussion:  All reviewers rejected.     Recommendation and justification:  The paper must be rejected due to its violation of blind submission (the authors reveal themselves in the Acknowledgments).  For information, blind review #2 also summarized well the following justifications for rejection:  I recommend rejection for this paper due to the following reasons:   The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors).    The experiment results aren t strong enough. And the experiments are done on only one dataset.   I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context aware.
This paper studies the problem of unsupervised inpainting occluded areas in spatiotemporal sequences and propose a GAN based framework which is able to complete the occluded areas given the stochastic model of the occlusion process. The reviewers agree that the problem is interesting, the paper is well written, and that the proposed approach is reasonable. However, after the discussion phase the critical point raised by AnonReviewer1 remains: in principle, when applying different corruptions in each step, the model is able to see the entire video over the duration of the training. This coupled with the strong assumptions on the mask distribution makes it questionable whether the approach should be considered unsupervised. Given that the results of the supervised methods significantly outperform the unsupervised ones, this issue needs to be carefully addressed to provide a clear and convincing selling point. Hence, I will recommend rejection and encourage the authors to address the remaining issues (the answers in the rebuttal are a good starting point).
There was some support for the ideas presented, but this paper was on the borderline, and ultimately not able to be accepted for publication at ICLR.  Concerns raised included level of novelty, and clarity of the exposition to an ML audience.
This paper proposes a network architecture which labels object with an identifier that it is trained to retain across subsequent instances of that same object.  After discussion, the reviewers agree that the approach is interesting, well motivated and written, and novel. However, there was unanimous concern about the experimental evaluation, so the paper does not appear to be ready for publication just yet, and I am recommending rejection.
This paper presents a number of improvements on existing approaches to neural logic programming. The reviews are generally positive: two weak accepts, one weak reject. Reviewer 2 seems wholly in favour of acceptance at the end of discussion, and did not clarify why they were sticking to their score of weak accept. The main reason Reviewer   1 sticks to 6 rather than 8 is that the work extends existing work rather than offering a "fundamental contribution", but otherwise is very positive. I personally feel that a) most work extends existing work b) there is room in our conferences for such well executed extensions (standing on the shoulders of giants etc).  Reviewer 3 is somewhat unconvinced by the nature of the evaluation. While I understand their reservations, they state that they would not be offended by the paper being accepted in spite of their reservations.  Overall, I find that the review group leans more in favour of acceptance, and an happy to recommend acceptance for the paper as it makes progress in an interesting area at the intersection of differentiable programming and logic based programming.
Thanks for the discussion, all. This paper proposes an attack strategy against federated learning. Reviewers put this in the top tier, and the authors responded appropriately to their criticisms. 
This paper proposes a method to reduce the instability issues of off policy deep reinforcement learning.  The proposed solution constructs a simple MDP from the experience in the agent s replay memory.  This graph is used to compute a lower bound for the values from the original problem. Incorporating this bound can make the learning system less prone to soft divergence.  The reviewers appreciated the motivation of the paper and the direction of this research.  However, the reviewers were not convinced that the formulation was sufficiently complete.  There were concerns that the method makes additional assumptions about the data distribution (the presence of state aggregation and the absence of repeated states in continuous spaces).  Reviewers found related work was missing.  The reviewers also found multiple aspects of the presentation unclear even after the author response.    This paper is not ready for publication as the generality of the proposed method was not sufficiently clear to the reviewers after the author response.
The submission applies architecture search to find effective architectures for video classification. The work is not terribly innovative, but the results are good. All reviewers recommend accepting the paper.
This paper was a very difficult case. All three original reviewers of the paper had never published in the area, and all of them advocated for acceptance of the paper. I, on the other hand, am an expert in the area who has published many papers, and I thought that while the paper is well written and experimental evaluation is not incorrect, the method was perhaps less relevant given current state of the art models. In addition, the somewhat non standard evaluation was perhaps causing this fact to be masked. I asked the original reviewers to consider my comments multiple times both during the rebuttal period and after, and unfortunately none of them replied.  Because of this, I elicited two additional reviews from people I knew were experts in the field. The reviews are below. I sent the PDF to the reviewers directly, and asked them to not look at the existing reviews (or my comments) when doing their review in order to make sure that they were making a fair assessment.   Long story short, Reviewer 4 essentially agreed with my concerns and pointed out a few additional clarity issues. Reviewer 5 pointed out a number of clarity issues and was also concerned with the fact that d_j has access to all other sentences (including those following the current sentence). I know that at the end of Section 2 it is noted that at test time d_j only refers to previous sentences, but if so there is also a training testing disconnect in model training, and it seems that this would hurt the model results.  Based on this, I have decided to favor the opinions of three experts (me and the two additional reviewers) over the opinions of the original three reviewers, and not recommend the paper for acceptance at this time. In order to improve the paper I would suggest the following (1) an acknowledgement of standard methods to incorporate context by processing sequences consisting of multiple sentences simultaneously, (2) a more thorough comparison with state of the art models that consider cross sentential context on standard datasets such as WikiText or PTB. I would encourage the authors to consider this as they revise their paper.  Finally, I would like to apologize to the authors that they did not get a chance to reply to the second set of reviews. As I noted above, I did try to make my best effort to encourage discussion during the rebuttal period.
The paper presents a framework named Wasserstein bounded GANs which generalizes WGAN. The paper shows that WBGAN can improve stability.  The reviewers raised several questions about the method and the experiments, but these were not addressed.  I encourage the authors to revise the draft and resubmit to a different venue.
This paper develops a method for sample selection that exploits the memorization effect. While the paper has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR in terms of presentation of the results and experimental validation. The paper will benefit from a revision and resubmission to another venue.
This paper proposes a bidirectional joint image text model using a variational hetero encoder (VHE) randomized generative adversarial network (GAN). The proposed VHE GAN model encodes an image to decode its associated text. Three reviewers have split reviews. Reviewer #3 is overall positive about this work. Reviewer #1 rated weak acceptance, while request more comparison with latest works. Reviewer  #2 rated weak reject raised concerns on the motivation of the approach, the lack of ablation and lack of comparison with the latest work. During the rebuttal, the authors provide additional comparison and ablation, which seem to address the major concerns. Given the overall positive feedback and the quality of rebuttal, the AC recommends acceptance.
Main content:  Blind review #1 summarizes it well:  This paper presents a semantic parser that operates over passages of text instead of a structured data source.  This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different).  The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations.  This is excellent work, and it should definitely be accepted.  I have a ton of questions about this method, but they are good questions.       Discussion:  The reviews all agree on a generally positive assessment, and focus on details that have been addressed, rather than major problems.     Recommendation and justification:  This paper should be accepted. Even though novelty in terms of fundamental machine learning components is minimal, but the architecture employing neural models to do symbolic work is a good contribution in a crucial direction (especially in the theme of ICLR).
This provides a simple analysis of an existing algorithm for min max optimization under some favorable assumptions.  The paper is clean and nice, though unfortunately lands just below borderline.  I urge the authors to continue their interesting work, and amongst other things address the reviewer comments, for example those on stochastic gradient descent. 
The reviewers have pointed out several major deficiencies of the paper, which the authors decided not to address.
This paper studies the transfer of representations learned by deep neural networks across various datasets and tasks when the network is pre trained on some dataset and subsequently fine tuned on the target dataset. The authors theoretically analyse two layer fully connected networks and provide an extensive empirical evaluation arguing that the loss landscape of  appropriately pre trained networks is easier to optimise (improved Lipschitzness).  Understanding the transferability of representations is an important problem and the reviewers appreciated some aspects of the extensive empirical evaluation and the initial theoretical investigation. However, we feel that the manuscript needs a major revision and that there is not enough empirical evidence to support the stated conclusions. As a result, I will recommend rejecting this paper in the current form.  Nevertheless, as the problem is extremely important I encourage the authors to improve the clarity and provide more convincing arguments towards the stated conclusions by addressing the issues raised during the discussion phase.
The reviewers have raised several important concerns about the paper that the authors decided not to address.
The reviewers appreciate the importance of the problem, and one reviewer particularly appreciated the gains in performance. However, two reviewers raised concerns about limited novelty and missing comparisons to prior work. While the rebuttal helped address these concerns, the novelty is still limited. The authors are encouraged to revise the presentation to clarify the novelty.
The reviewers have provided extensive comments, we encourage the authors to take them into account seriously in further iterations of this work.
The paper proposes an approach for finding an explainable subset of features by choosing features that simultaneously are: most important for the prediction task, and robust against adversarial perturbation. The paper provides quantitative and qualitative evidence that the proposed method works.  The paper had two reviews (both borderline), and the while the authors responded enthusiastically, the reviewers did not further engage during the discussion period.  The paper has a promising idea, but the presentation and execution in its current form have been found to be not convincing by the reviewers. Unfortunately, the submission as it stands is not yet suitable for ICLR.
The authors propose a new method for neural architecture search, except it s not exactly that because model training is separated from architecture, which is the main point of the paper. Once this network is trained, sub networks can be distilled from it and used for specific tasks.  The paper as submitted missed certain details, but after this was pointed out by reviewers the details were satisfactorily described by the authors.   The idea of the paper is original and interesting. The paper is correct and, after the revisions by authors, complete. In my view, this is sufficient for acceptance.
Following the revision and the discussion, all three reviewers agree that the paper provides an interesting contribution to the area of generative image modeling. Accept.
The reviewers attempted to provide a fair assessment of this work, albeit with varying qualifications.  Nevertheless, the depth and significance of the technical contribution was unanimously questioned, and the experimental evaluation was not considered to be convincing by any of the assessors.  The criticisms are sufficient to ask the authors to further strengthen this work before it can be considered for a top conference.
The paper proposed a new pretrained language model which can take visual information into the embeddings. Experiments showed state of the art results on three downstream tasks. The paper is well written and detailed comparisons with related work are given. There are some concerns about the clarity and novelty raised by the reviewers which is answered in details and I think the paper is acceptable.
This paper extends previous observations (Tsipars, Etmann etc) in relations between Jacobian and robustness and directly train a model that improves robustness using Jacobians that look like images. The questions regarding computation time (suggested by two reviewers, including one of the most negative reviewers) are appropriately addressed by the authors (added experiments). Reviewers agree that the idea is novel, and some conjectured why the paper’s idea is a very sensible one. We think this paper would be an interest for ICLR readers. Please address any remaining comments from the reviewers before the final copy. 
The goal of this paper is to study the dynamics of convergence of neural network training when weight normalization is used. This is an important and interesting area. The authors focus on analyzing such effect based on a recent theoretical trend which studies neural network dynamics based on the so called neural tangent kernel (NTK). The authors show an interesting phenomena of length direction decoupling. The reviewers raise various points some of which have been addressed by the authors in their response. Two main points not yet clearly addressed is (1) what is the novelty of the theoretical framework given existing literature and (2) what are the benefits of weight normalization based on this theory (e.g. generalization etc. ). The authors suggest improved convergence rate and overparameterization dependence (i.e. that with weight normalization the required width is decreased) as a possible advantage. However, as pointed out by reviewer 3 there are existing results which already obtain better results without weight normalization (the authors  response that this is only true in randomized scenarios is actually not accurate). Based on above I do not think the paper is ready for publication. That said I think this is a nice direction and well written paper. I recommend the authors revise and resubmit to a future venue. Some suggestions for improvements in case this is helpful (1) improve literature review and discussion of existing results (2) identify clear benefits to weight normalization. I doubt that improving overparameterization in existing form is one of them unless you provide a lower bound (I suspect one can eventually obtain even linear overparameterization i.e. number of parameters proportional to number of training data even in the NTK regime without weight normalization. The suggestion by the reviewer at looking at generalization might be a good direction to pursue.       
This paper provides an extensive investigation of the robustness of Deep Deterministic Policy Gradient algorithm.  Papers providing extensive and qualitative empirical studies, illustrative benchmark domains, identification of problems with existing methods, and new insights can be immensely valuable, and this paper is certainly in this direction, if not quite there yet.   The vast majority of this paper investigates one deep learning algorithm in designed domain. There is some theory but it s relegated to the appendix. There are a few issues with this approach: (1) there is no concrete evidence that this is a general issue beyond the provided example (more on that below). (2) Even in the designed domain the problem is extremely rare. (3) The study and perhaps even the issue is only shown for one particular architecture (with a whole host of unspecified meta parameter details). Why not just use SAC it works? DDPG has other issues, why is it of interest to study and fix this particular architecture? The motivation that it is the first and most popular algorithm is not well developed enough to be convincing. (4) There is really no reasoning to suggest that the particular 1D is representative or interesting in general.  The authors including Mujoco results to address #1. But the error bars overlap, its completely unclear if the baseline was tuned at all this is very problematic as the domains were variants created by the authors. If DDPG was not tuned for the variant then the plots are not representative. In general, there are basically no implementation details (how parameters were tested, how experiments were conducted)or general methodological details given in the paper. Given the evidence provided in this paper its difficult to claim this is a general and important issue.   I encourage the authors to look at John Langfords hard exploration tasks, and broaden their view of this work general learning mechanisms. 
The paper studies, theoretically and empirically, the problem when generalization error decreases as $n^{ \beta}$ where $\beta$ is not $\frac{1}{2}$. It analyses a Teacher Student problem where the Teacher generates data from a Gaussian random field. The paper provides a theorem that derives $\beta$ for Gaussian and Laplace kernels, and show empirical evidence supporting the theory using MNIST and CIFAR.  The reviews contained two low scores, both of which were not confident. A more confident reviewer provided a weak accept score, and interacted multiple times with the authors during the discussion period (which is one of the nice things about the ICLR review process). However, this reviewer also noted that ICLR may not be the best venue for this work.  Overall, while this paper shows promise, the negative review scores show that the topic may not be the best fit to the ICLR audience.
The authors present a Bayesian model for time series which are represented as discrete events in continuous time and describe methods for doing parameter inference, future event prediction and entropy rate estimation for such processes.  However, the reviewers find that the novelty of the paper is not high enough, and without sufficient acknowledgement and comparison to existing literature.
This paper proposes to extend learning to learn framework based on zeroth order optimization. Generally, the paper is well presented and easy to follow. The core idea is to incorporate another RNN to adaptively to learn the Gaussian sampling rule.  Although the method does not seem to have a strong theorical support, its effectiveness is evaluated in the well organized experiments including realistic tasks like black box adversarial attack.  All reviewers including two experts in this field admit the novelty of the methods and are positive to the acceptance. I’d like to support their opinions and recommend accepting the paper. As R#1 still finds some details unclear, please try to clarify these points in the final version of the paper.
This paper studies the problem of certified robustness to adversarial examples. It first demonstrates that many existing certified defenses can be viewed under a unified framework of regularization. Then, it proposes a new double margin based regularizer to obtain better certified robustness.  Overall, it has major technical issues and the rebuttal is not satisfying.
This paper describes a method for bounding the confidence around predictions made by deep networks. Reviewers agree that this result is of technical interest to the community, and with the added reorganization and revisions described by the authors, they and the AC agree the paper should be accepted. 
This paper proposes an novel way of expanding our VAE toolkit by tying it to adversarial robustness. It should be thus of interest to the respective communities.
The authors introduce a framework for automatically detecting diverse, self organized patterns in a continuous Game of Life environment, using compositional pattern producing networks (CPPNs) and population based Intrinsically Motivated Goal Exploration Processes (POP IMGEPs) to find the distribution of system parameters that produce diverse, interesting goal patterns.  This work is really well presented, both in the paper and on the associated website, which is interactive and features source code and demos. Reviewers agree that it’s well written and seems technically sound. I also agree with R2 that this is an under explored area and thus would add to the diversity of the program.  In terms of weaknesses, reviewers noted that it’s quite long, with a lengthy appendix, and could be a bit confusing in areas. Authors were responsive to this in the rebuttal and have trimmed it, although it’s still 29 pages. My assessment is well aligned with those of R2 and thus I’m recommending accept. In the rebuttal, the authors mentioned several interesting possible applications for this work; it’d be great if these could be included in the discussion.   Given the impressive presentation and amazing visuals, I think it could make for a fun talk. 
This paper develops a linear quadratic model predictive control approach for safe imitation learning.  The main contribution is an analytic solution for the derivative of the discrete time algebraic Riccati equation (DARE).  This allows an infinite horizon optimality objective to be used with differentiation based learning methods.  An additional contribution is the problem reformulation with a pre stabilizing controller and the support of state constraints throughout the learning process.  The method is tested on a damped spring system and a vehicle platooning problem.  The reviewers and the author response covered several topics. The reviewers appreciated the research direction and theoretical contributions of this work.  The reviewers main concern was the experimental evaluation, which was originally limited to a damped spring system.  The authors added another experiment for a substantially more complex continuous control domain.  In response to the reviewers, the authors also described how this work relates to non linear control problems.  The authors also clarified the ability of the proposed method to handle state based constraints that are not handled by earlier methods.  The reviewers were largely satisfied with these changes.  This paper should be accepted as the reviewers are satisfied that the paper has useful contributions.
The paper provides a theoretical study of what regularizations should be used in GAN training and why. The main focus is that the conditions on the discriminator that need to be enforced, to get the Lipshitz property of the corresponding function that is optimized for the generator. Quite a few theorems and propositions are provided. As noted by Reviewer3, this adds insight to well known techniques: the Reviewer1 rightfully notes that this does not lead to any practical conclusion.  Moreover, then training of GANs never goes to the optimal discriminator, that could be a weak point; rather than it proceeds in the alternating fashion, and then evolution is governed by the spectra of the local Jacobian (which is briefly mentioned). This is mentioned in future work, but it is not clear at all if the results here can be helpful (or can be generalized).  At some point of the paper it gets to "more theorems mode" which make it not so easy and motivating to read.  The theoretical results at the quantitative level are very interesting.  I have looked for a long time on Figure 1: does this support the claims? First my impression was it does not (there are better FID scores for larger learning rates). But in the end, I think it supports: the convergence for a smaller that $\gamma_0$ learning rate to the same FID indicated the convergence to the same local minima (probably). This is perfectly fine. Oscillations afterwards move us to a stochastic region, where FID oscillates. So, the theory has at least minor confirmation.   
This paper presents an approach to enforce statistical fairness notions using adversarial networks. The reviewers point out several issues of the paper, including 1) their approach does not provably enforce criteria such as demographic parity, 2) lack of novelty and 3) poor presentation.
The authors propose approaches to handle partial observability in reinforcement learning. The reviewers agree that the paper does not sufficiently justify the methods that are proposed and even the experimental performance shows that the proposed method is not always better than baselines.
The authors propose to use the information bottleneck to learn state representations for RL. They optimize the IB objective using Stein variational gradient descent and combine it with A2C and PPO. On a handful of Atari games, they show improved performance.  The reviewers primary concerns were: *Limited evaluation. The method was only evaluated on a handful of the Atari games and no comparison to other representation learning methods was done. *Using a simple Gaussian embedding function would eliminate the need for amortized SVGD. The authors should compare to that alternative to demonstrate the necessity of their approach.  The ideas explored in the paper are interesting, but given the issues with evaluation, the paper is not ready for acceptance at this time.
This paper presents a framework for navigation that leverages learning spatial affordance maps (ie what parts of a scene are navigable) via a self supervision approach in order to deal with environments with dynamics and hazards. They evaluate on procedurally generated VizDoom levels and find improvements over frontier and RL baseline agents.  Reviewers all agreed on the quality of the paper and strength of the results. Authors were highly responsive to constructive criticism and the engagement/discussion appears to have improved the paper overall. After seeing the rebuttal and revisions, I believe this paper will be a useful contribution to the field and I’m happy to recommend accept. 
The paper proposed a general framework to construct unsupervised models for representation learning of discrete structures. The reviewers feel that the approach is taken directly from graph kernels, and the novelty is not high enough. 
This paper offers an interesting and potentially useful approach to robust watermarking.  The reviewers are divided on the significance of the method.  The most senior and experienced reviewer was the most negative.  On balance, my assessment of this paper is borderline; given the number of more highly ranked papers in my pile, that means I have to assign "reject".
Main content: Authors developed graph scattering transforms (GST) with a pruning algorithm, with the aim to reduce the running time and space cost, improve robustness to perturbations on input graph signal, and encourage flexibility for domain adaption.  Discussion: reviewer 1: likes the idea, considers it to be elegant and work well. some questions regarding the proofs in the paper but it sounds like authors have addressed concerns. reviewer 2: solid paper and results, has questons on stability results, like reviewer 2. reviewer 3: likes the idea, including good sufficient theoretical analysis and algorthmic stability. concern is around complexity analysis but sounds like the authors have addressed the concerns. Recommendation: Well written solid paper with good proofs. Authors addressed any reviewer concerns and all 3 reviewres vote weak accept. This is good for poster.
This paper presents a VAE approach where the model learns representation while disentangling the location and appearance information. The reviewers found issues with the experimental evaluation of the paper, and have given many useful feedback. None of the reviewers were willing to change their score during the discussion period. with the current score, the paper does not make the cut for ICLR, and I recommend to reject this paper. 
This paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning by subscribing the target delta model to the knowledge of source pretrained model via channel pooling.  Reviewers and AC agree that this paper is well written, with simple but sound technique towards an important problem and with promising empirical performance. The main critique is that the approach can only tackle transfer learning while failing in the lifelong setting. Authors provided convincing feedbacks on this key point. Details requested by the reviewers were all well addressed in the revision.  Hence I recommend acceptance.
The paper proposes a new algorithm for solving constrained MDPs called Projection Based Constrained Policy Optimization. Compared to CPO, it projects the solution back to the feasible region after each step, which results in improvements on some of the tasks considered.   The problem addressed is relevant, as many tasks could have important constraints e.g. concerning fairness or safety.   The method is supported through theory and empirical results. It is great to have theoretical bounds on the policy improvement and constraint violation of the algorithm, although they only apply to the intractable version of the algorithm (another approximate algorithm is proposed that is used in practice). The experimental evidence is a bit mixed, with the best of the proposed projections (based on the KL approach) sometimes beating CPO but also sometimes being beaten by it, both on the obtained reward and on constraint satisfaction.   The method only considers a single constraint. I m not sure how trivial it would be to add more than one constraint. The reviewers also mention that the paper does not implement TRPO as in the original paper, as in the original paper the step size in the direction of the natural gradient is refined with a line search if the original step size (calculated using the quadratic expansion of the expected KL) does violate the original constraints. (Line search on the constraint as mentioned by the authors would be a different issue). Futhermore, the quadratic expansion of the KL is symmetric around the current policy in parameter space. This means that starting from a feasible solution the trust region should always overlap with the constraint set when feasibility is maintained, going somewhat agains the argument for PCPO as opposed to CPO brought up by the authors in the discussion with R2. I would also show this symmetry in illustrations such as Fig 1 to aid understanding.    
The paper proposes a mechanism for obtaining diverse policies for solving a task by posing it as a multi agent problem, and incentivizing the agents to be different from each other via maximizing total variation.  The reviewers agreed that this is an interesting idea, but had issues with the placement and exact motivations   precisely what kind of diversity is the work after, why, and what accordingly related approaches does it need to be compared to. Some reviewers also found the technical and exposition clarity to be lacking.  Given the consensus, I recommend rejection at this time, but encourage the authors to take the reviewers  feedback into account and resubmit to another venue.
The paper generalizes several existing results for structured linear transformations in the form of K matrices. This is an excellent paper and all reviewers confirmed that.
The paper proposes a new learning algorithm for deep neural networks that first reformulates the problem as a multi convex and then uses an alternating update to solve. The reviewers are concerned about the closeness to previous work, comparisons with related work like dlADMM, and the difficulty of the dataset. While the authors proposed the possibility of addressing some of these issues, the reviewers feel that without actually addressing them, the paper is not yet ready for publication. 
This paper explores several embedding models (Skip gram, BERT, XLNet) and describes a framework for comparing, and in the end, unifying them.  The framework is such that it actually suggests new ways of creating embeddings, and draws connections to methodology from computer vision.  One of the reviewers had several questions about the derivations in your paper and was worried about the paper s clarity.  But all of the reviewers appreciated the contributions of the paper, which joins multiple seemingly disparite models under into one theoretical framework.  The reviewers were positive about the paper, and in particular were happy to see the active response of authors to their questions and willingness to update the paper with their suggested improvements.
This paper provides an improved feature space adversarial attack.  However, the contribution is unclear in its significance, in part due to an important prior reference was omitted (song et al.)   Unfortunately the paper is borderline, and not above the bar for acceptance in the current pool.
Existing implementation of information bottleneck need access to privileged information which goes against the idea of compression. The authors propose variational bandwidth bottleneck which estimates the value of the privileged information and then stochastically decided whether to access this information or not. They provide a suitable approximation and show that their  method improves generalisation in RL while reducing access to expensive information.  These paper received only two reviews. However, both the reviews were favourable. During discussions with the AC the reviewers acknowledged that most of their concerns were addressed. R2 is still concerned that VBB does not result in improvement in terms of sample efficiency. I request the authors to adequately address this in the final version. Having said that, the paper does make other interesting contributions, hence I recommend that this paper should be accepted.
The paper introduces a novel way of learning Hamiltonian dynamics with a generative network. The Hamiltonian generative network (HGN) learns the dynamics directly from data by embedding observations in a latent space, which is then transformed into a phase space describing the system s initial (abstract) position and momentum. Using a second network, the Hamiltonian network, the position and momentum are reduced to a scalar, interpreted as the Hamiltonian of the system, which can then be used to do rollouts in the phase space using techniques known from, e.g., Hamiltonian Monte Carlo sampling. An important ingredient of the paper is the fact that no access to the derivatives of the Hamiltonian is needed.   The reviewers agree that this paper is a good contribution, and I recommend acceptance.
The authors study planning problems with sparse rewards.                                                                            They propose a tree search algorithm together with an ensemble of value                                                             functions to guide exploration in this setting.                                                                                     The value predictions from the ensemble are combined in a risk sensitive way,                                                       therefore biasing the search towards states with high uncertainty in value                                                          prediction.                                                                                                                         The approach is applied to several grid world environments.                                                                                                                                                                                                             The reviewers mostly criticized the presentation of the material, in particular                                                     that the paper provided insufficient details on the proposed                                                                        method. Furthermore, the comparison to model free RL methods was deemed somewhat                                                    lacking, as the proposed algorithm has access to the ground truth model.                                                            The authors improved the manuscript in the rebuttal.                                                                                                                                                                                                                    Based on the reviews and my own reading I think that the paper in it s current                                                      form is below acceptance threshold. However, with further improved presentation                                                     and baselines for the experiments, this has potential to be an important contribution.
This paper has been reviewed by three reviewers and received scores: 6/3/8. While two reviewers were reasonably positive, they also did not provide a very compelling reviews (e.g. one rev. just reiterated the rationale behind tensor model compression and the other admitted the paper is of limited novelty). Perhaps the shortest review (and perhaps the most telling) prompts authors to the fact that the model compression with tensor decompositions is quite common in the literature these days. One example could be T Net: Parametrizing Fully Convolutional Nets with a Single High Order Tensor by Kossaifi et al. Very likely the authors will find many more recent developments on model compression with/without tensor decomp. For a good paper in this topic, authors should carefully consider various tensor factorizations (Tucker, TT, tensor rings, t product and many more) and consider theoretical contributions and guarantees. Taking into account all pros and cons, this submissions falls marginally short of the ICLR 2020 threshold but the authors are encouraged to work on further developments.
The authors propose a method of selecting nodes to label in a graph neural network setting to reduce the loss as efficiently as possible. Building atop Sener & Savarese 2017 the authors propose an alternative distance metric and clustering algorithm. In comparison to the just mentioned work, they show that their upper bound is smaller than the previous art s upper bound. While one cannot conclude from this that their algorithm is better, at least empirically the method appears to have a advantage over state of the art.  However, reviewers were concerned about the assumptions necessary to prove the theorem, despite the modifications made by the authors after the initial round.   The work proposes a simple estimator and shows promising results but reviewers felt improvements like reducing the number of assumptions and potentially a lower bound may greatly strengthen the paper.
The authors take inspiration from regulatory fit theory and propose a new parameter for policy gradient algorithms in RL that can manage the "regulatory focus" of an agent.  They hypothesize that this can affect performance in a problem specific way, especially when trading off between broad exploration and risk.  The reviewers expressed concerns about the usefulness of the proposed algorithm in practice and a lack of thorough empirical comparisons or theoretical results.  Unfortunately, the authors did not provide a rebuttal, so no further discussion of these issues was possible; thus, I recommend to reject.
This paper presents a way of adapting an HMC based posterior inference algorithm. It s based on two approximations: replacing the entropy of the final state with the entropy of the initial state, and differentiating through the MH acceptance step. Experiments show it is able to sample from some toy distributions and achieves slightly higher log likelihood on binarized MNIST than competing approaches.  The paper is well written, and the experiments seem pretty reasonable.  I don t find the motivations for the aforementioned approximations very convincing. It s claimed that encouraging entropy of P_0 has a similar effect to encouraging entropy of P_T, but it seems easy to come up with situations where the algorithm could "cheat" by finding a high entropy P_0 which leads straight downhill to an atypically high density region. Similarly, there was some reviewer discussion about whether it s OK to differentiate through the indicator function; while we differentiate through nondifferentiable functions all the time, it makes no sense to differentiate through a discontinuous function. (This is a big part of why adaptive HMC is hard.)  This paper has some promising ideas, but overall the reviewers and I don t think this is quite ready. 
This paper presents Capacity Limited Reinforcement Learning (CLRL) which builds on methods in soft RL to enable learning in agents with limited capacity.  The reviewers raised issues that were largely around three areas: there is a lack of clear motivation for the work, and many of the insights given lack intuition; many connections to related literature are missing; and the experimental results remain unconvincing.  Although the ideas presented in the paper are interesting, more work is required for this to be accepted. Therefore at this point, this is unfortunately a rejection.
In this work, the authors focus on the high dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations.  Unfortunately, the reviewers could not reach a consensus as to whether this paper had sufficient novelty to merit acceptance at this time. Incorporating their feedback would move the paper closer towards the acceptance threshold.
The authors derive a novel, unbiased gradient estimator for discrete random variables based on sampling without replacement. They relate their estimator to existing multi sample estimators and motivate why we would expect reduced variance. Finally, they evaluate their estimator across several tasks and show that is performs well in all of them.  The reviewers agree that the revised paper is well written and well executed. There was some concern about that effectiveness of the estimator, however, the authors clarified that "it is the only estimator that performs well across different settings (high and low entropy). Therefore it is more robust and a strict improvement to any of these estimators which only have good performance in either high or low entropy settings." Reviewer 2 was still not convinced about the strength of the analysis of the estimator, and this is indeed quantifying the variance reduction theoretically would be an improvement.  Overall, the paper is a nice addition to the set of tools for computing gradients of expectations of discrete random variables. I recommend acceptance.  
The paper shows an automatic piano fingering algorithm. The idea is good. But the reviewers find that the novelty is limited and it is an incremental work. All the reivewers agree to reject.
This paper studies the problem of out of distribution (OOD) detection for semantic segmentation.  Reviewers and AC agree that the problem might be important and interesting, but the paper is not ready to publish in various aspects, e.g.,  incremental contribution and less motivated/convincing experimental setups/results.  Hence, I recommend rejection.
This paper proposes to use mixture distributions to improve uncertainty estimates in BNNs. Ensemble methods are interpreted as a Bayesian mixture posterior approximation. To reduce the computation, a modification to BBB is provided based on a concrete mixture distribution.  Both R1 and R3 have given useful feedback. It is clear that interpretation of ensemble as a Bayesian posterior is well known, and some of them also have theoretical issues. The experiment to clearly comparing proposed mixture posterior to more commonly used mixture distribution is also necessary.   Due to these reasons, I recommend to reject this paper. I encourage the authors to use reviewers feedback to improve the paper.
The authors aim to improve policy gradient methods by denoising the gradient estimate. They propose to filter the transitions used to form the gradient update based on a variance explains criterion. They evaluate their method in combination with PPO and A2C, and demonstrate improvements over the baseline methods.  Initially, reviewers were concerned about the motivation and explanation of the method. The authors revised the paper by clarifying the motivation and providing a justification based on the options framework. Furthermore, the authors included additional experiments investigating the impact of their approach on the gradient estimator, showing that with their filtering, the gradient estimator had larger magnitude.  Reviewers found the justification via the options framework to be a stretch, and I agree. The authors should explain how the options framework leads to dropping gradient terms. At the moment, the paper describes an algorithm using the options framework, however, they don t connect the policy gradients of that algorithm to their method. Furthermore, the authors should more clearly verify the claims about reducing noise in the gradient estimate. While the additional experiments on the norm are nice, the authors should go further. For example, if the claim is that the variance of the gradient estimator is reduced, then that should be verified. Finally, there are many approaches for reducing the variance of the policy gradient (Grathwohl et al. 2018, Wu et al 2018, Liu et al. 2018) and no comparisons are made to these approaches.  Given the remaining issues, I recommend rejection for this paper at this time, however, I encourage the authors to address these issues and submit to a future venue. 
The paper leverages variational auto encoders (VAEs) and disentanglement to generate data representations that hide sensitive attributes. The reviewers have identified several issues with the paper, including its false claims or statements about differential privacy, unclear privacy guarantee, and lack of related work discussion. The authors have not directly addressed these issues.
This paper on extending MLNs using NNs is borderline acceptable: one reviewer is strongly opposed, although I confess I don t really understand their response to the rebuttal or see what the issue with novelty is (a position shared by the other reviewers). I m not sure how to weigh this review, but there is not a lot of signal in favour of rejection aside from the rating.  The remaining two reviews are in favour of acceptance, with their enthusiasm only bounded by the lack of scalability of the method, something they appreciate the authors are upfront about. My view is this paper brings something new to the table which will interest the community, but doesn t oversell the result.  Given the distribution of papers in my area, this one is just a little too borderline to accept, but this is primarily a reflection of the number of high quality papers reviewed and the limited space of the conference. I have no doubt this paper will be successful at another conference, and it s a bit of a shame we were not in a position to accept it to this one.
The authors propose an agent that can act in an RL environment to verify hypotheses about it, using hypotheses formulated as triplets of pre condition, action sequence, and post condition variables. Training then proceeds in multiple stages, including a pretraining phase using a reward function that encourages the agent to learn the hypothesis triplets.   Strengths: Reviewers generally agreed it’s an important problem and interesting approach  Weaknesses: There were some points of convergence among reviewer comments: lack of connection to existing literature (ie to causal reasoning and POMDPs), and concerns about the robustness of the results (which were only reporting the max seeds).  Two reviewers also found the use of natural language to unnecessarily complicate their setup. Overall, clarity seemed to be an issue. Other comments concerned lack of comparisons, analyses, and suggestions for alternate methods of rewarding the agent (to improve understandability).    The authors deserve credit for their responsiveness to reviewer comments and for the considerable amount of additional work done in the rebuttal period. However, these efforts ultimately didn’t satisfy the reviewers enough to change their scores. Although I find that the additional experiments and revisions have significantly strengthened the paper, I don t believe it s currently ready for publication at ICLR. I urge the authors to focus on clearly presenting and integrating these new results in a future submission, which I look forward to. 
The paper addresses the question of why warm starting could result in worse generalization ability than training from scratch.  The reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources.  However, the reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR in its current form.  Concerns included that the analysis was not sufficiently focused and the experiments too small scale.  As the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state.
This paper proposes looking at the duality gap to measure performance. However, the metric is just an upperbound on the true metric of interest, and therefore its value can be ambiguous.   The reviewers found the paper to be in an unacceptable form and was clearly hastily prepared. They were also skeptical about the novelty of the result as well as the comprehensiveness of the experiments.  This paper would require extensive revisions before any potential acceptance. Reject   
There was extension discussion of the paper between the reviewers. It s clear that the reviewers appreciated the main idea in the paper, and the notion of an "online" meta critic that accelerates the RL process is definitely very appealing. However, there were unanswered questions about what the method is actually doing that make me reticent to recommend acceptance at this point. I would refer the authors to R3 and R1 for an in depth discussion of the issues, but the short summary is that it s not clear whether, if, and how the meta loss in this case actually converges, and what the meta critic is actually doing. In the absence of a theoretical understanding for what the modification does to accelerate RL, we are left with the empirical experiments, and there it is necessary to consider alternative hypotheses and perform detailed ablation analyses to understand that the method really works for the reasons stated by the authors (and not some of the alternative explanations, see e.g. R3). While there is nothing wrong with a result that is primarily empirical, it is important to analyze that the empirical gains really are happening for the reasons claimed, and to carefully study convergence and asymptotic properties of the algorithm. The comparatively diminished gains with the stronger algorithms (TD3 and especially SAC) make me more skeptical. Therefore, I would recommend that the paper not be accepted at this time, though I encourage the authors to resubmit with a more in depth experimental evaluation.
The authors present a multiple instance learning based approach that uses weak supervison (of which skills appear in any given trajectory)  to automatically segment a set of skills from demonstrations.  The reviewers had significant concerns about the significance and performance of the method, as well as the metrics used for analysis.  Most notably, neither the original paper nor the rebuttal provided a sufficient justification or fix for the lack of analysis beyond accuracy scores (as opposed to confusion matrices, precision/recall, etc), which leaves the contribution and claims of the paper unclear.  Thus, I recommend rejection at this time.
The paper proposed a method to evaluate latent variable based generative models by estimating the compression in the latents (rate) and the distortion in the resulting reconstructions. While reviewers have clearly appreciated the theoretical novelty in using AIS to get an upper bound on the rate, there are concerns on missing empirical comparison with other related metrics (precision recall) and limited practical applicability of the method due to large computational cost. Authors should consider comparing with PR metric and discuss some directions that can make the method practically as relevant as other related metrics. 
Earlier work suggests that adversarial examples exploit local features and that more robust models rely on global features. The authors propose to exploit this insight by performing data augmentation in adversarial training, by cutting and reshuffling image block. They demonstrate the idea empirically and witness interesting gains. I think the technique is an interesting contribution, but empirically and as a tool. 
The paper proposes a new algorithm called Expected Information Maximization (EIM) for learning latent variable models while computing the I projection solely based on samples. The reviewers had several questions, which the authors sufficiently answered. The reviewers agree that the paper should be accepted. The authors should carefully read the reviewer questions and comments and use them to improve their final manuscript. 
This paper aims to study the effect of data augmentation of generalization performance. The authors put forth a measure of rugosity or "roughness" based on the tangent Hessian of the function reminiscent of a classic result by Donoho et. al. The authors show that this measure changes in tandem with how much data augmentation helps. The reviewers and I concur that the rugosity measure is interesting. However, as the reviewer mention the main draw back of this paper is that this measure of rugosity when made explicit does not improve generalization. This is the main draw back of the paper. I agree with the authors that this measure is interesting in itself. However, I think in its current form the paper is not ready for prime time and recommend rejection. That said, I believe this paper has a lot of potential and recommend the authors to rewrite and carry out more careful experiments for a future submission.
All the reviewers reach a consensus to reject the current submission.   In addition, there are two assumptions in the proof which seemed never included in Theorem conditions or verified in typical cases.   1) Between Eq (16) and (17), the authors assumed the  extended restricted strong convexity’ given by the un numbered equation.   2) In Eq. (25), the authors assume the existence of \sigma making the inequality true.  However those assumptions are neither explicitly stated in theorem conditions, nor verified for typical cases in applications, e.g. even the square or logistic loss. The authors need to address these assumptions explicitly rather than using them from nowhere.
This paper addresses challenges in offline model learning, i.e., in the setting where some trajectories are given and can be used for learning a model, which in turn serves to train an RL agent or plan action sequences in simulation. A key issue in this setting is that of compounding errors: as the simulated trajectory deviates from observed data, errors build up, leading to suboptimal performance in the target domain. The paper proposes a distribution matching approach that considers trajectory sequence information and provides theoretical guarantees as well as some promising empirical results.  Several issues were raised by reviewers, including missing references, clarity issues, questions about limitations of the theoretical analysis, and limitations of the empirical validation. Many of the issues raised by reviewers were addressed by the authors during the rebuttal phase.  At the same time, several issues remain. First, the authors committed to adding results for additional tasks (initially deemed too easy or too hard to show differences). Even if the tasks show little separation between methods, these would be important data points to include as they support additional comparisons with prior and future work. The AC has to assess the paper without taking promised additional results into account. Second, questions about the results for Ant are not sufficiently addressed. The plot shows no learning. The author response mentions initialization but this is not deemed a sufficient explanation. Given the remaining questions, my assessment is that the quality and contribution of the submission are not yet ready for publication at the current stage.
Post author rebuttal the score of this paper increased. Discussions with reviewers were substantive and the AC recommends acceptance.
This paper presents a new method for detecting out of distribution (OOD) samples.  A reviewer pointed out that the paper discovers an interesting finding and the addressed problem is important. On the other hand, other reviewers pointed out theoretical/empirical justifications are limited.   In particular, I think that experimental supports why the proposed method is superior beyond the existing ones are limited. I encourages the authors to consider more scenarios of OOD detection (e.g., datasets and architectures) and more baselines as the problem of measuring the confidence of neural networks or detecting outliers have rich literature. This would guide more comprehensive understandings on the proposed method.  Hence, I recommend rejection.  
This paper is full of ideas. However, a logical argument is only as strong as its weakest link, and I believe the current paper has some weak links. For example, the attempt to tie the behavior of SGD to free energy minimization relies on unrealistic approximations. Second, the bounds based on limiting flat priors become trivial. The authors in depth response to my own review was much appreciated, especially given its last minute appearance. Unfortunately, I was not convinced by the arguments. In part, the authors argue that the logical argument they are making is not sensitive to certain issues that I raised, but this only highlights for me that the argument being made is not very precise.  I can imagine a version of this work with sharper claims, built on clearly stated assumptions/conjectures about SGD s dynamics, RATHER THAN being framed as the consequences of clearly inaccurate approximations. The behavior of diffusions can be presented as evidence that the assumptions/conjectures (that cannot be proven at the moment, but which are needed to complete the logical argument) are reasonable. However, I am also not convinced that it is trivial to do this, and so the community must have a chance to review a major revision.
The goal of verification of properties of generative models is very interesting and the contributions of this work seem to make some progress in this context. However, the current state of the paper (particularly, its presentation) makes it difficult to recommend its acceptance.
The paper proposes to extend the autoencoder loss in a deep generative model to include per latent layer loss terms.  Two variants are proposed: SAP (simple aggregation along pathway) and NAP (normalized aggregation along pathway). SAP is simply the sum of the squared norm, while NAP performs decorrelation and normalization of the magnitude.  This was viewed as novel by the reviewers, and the experiments supported the proposed approach.  In the post rebuttal phase, the inclusion of an ablation study has led to an upgrade in the reviewer recommendation.  As a result, there was a unanimous opinion that the paper is suitable for publication at ICLR.
The authors propose a hybrid model free/model based policy gradient method that attempts to reduce sample complexity without degrading asymptotic performance. They evaluate their approach on a collection of benchmark tests.  The reviewers raised concerns about limited novelty of the proposed approach and flaws in the evaluation. The authors need to compare to more baselines and ensure that the baseline algorithms are performing as previously reported. Even then, the reported improvements were small.  Given the issues raised by the reviewers, this paper is not ready for publication at ICLR.
The paper proposes a curriculum learning approach to training generative models like GANs. The reviewers had a number of questions and concerns related to specific details in the paper and experimental results. While the authors were able to address some of these concerns, the reviewers believe that further refinement is necessary before the paper is ready for publication.
The paper proposed a new synthetically generated video dataset (CATER) for benchmarking temporal reasoning. The dataset is based on the CLEVR dataset and provides videos make up of primitive actions ("rotate", "pick place", "slide", "contain") that can be combined to form for complex actions. The paper also benchmarks a variety of methods on three proposed tasks (atomic action classification, composite action classification, and  snitch  localization) and demonstrates that while it is possible to get high performance on atomic action classification, the other two task are still challenging and requires temporal modeling.    Overall, all reviewers found the paper to be well written and easy to follow, with care given to the dataset construction, as well as the task definitions and experiment setup and analysis.  The paper received strong scores from all reviewers (3 accepts).  Based on the reviewer comments, the authors further improved the paper by adding additional relevant datasets for comparison and providing missing details pointed out by the reviewers.  After the rebuttal, the reviewers remained positive.
Main content:  Blind review #3 summarizes it well:  This paper presents a technique for encoding the high level “style” of pieces of symbolic music. The music is represented as a variant of the MIDI format. The main strategy is to condition a Music Transformer architecture on this global “style embedding”.  Additionally, the Music Transformer model is also conditioned on a combination of both “style” and “melody” embeddings to try and generate music “similar” to the conditioning melody but in the style of the performance embedding.      Discussion:  The reviewers questioned the novelty. Blind review #2 wrote: "Overall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology. Firstly, I think the algorithmic novelty in the paper is fairly limited. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. al. 2019b). However, the limited algorithmic novelty is not the main concern. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. However it is not clear if this dataset will be publicly released or is only for internal experiments."  However, after revision, the same reviewer has upgraded the review to a weak accept, as the authors wrote "We emphasize that our goal is to provide users with more fine grained control over the outputs generated by a seq2seq language model. Despite its simplicity, our method is able to learn a global representation of style for a Transformer, which to the best of our knowledge is a novel contribution for music generation. Additionally, we can synthesize an arbitrary melody into the style of another performance, and we demonstrate the effectiveness of our results both quantitatively (metrics) and qualitatively (interpolations, samples, and user listening studies)."     Recommendation and justification:  This paper is borderline for the reasons above, and due to the large number of strong papers, is not accepted at this time. As one comment, this work might actually be more suitable for a more specialized conference like ISMIR, as its novel contribution is more to music applications than to fundamental machine learning approaches.
This paper proposes a multi sample variant of dropout, claiming that it accelerates training and improves generalization. CIFAR10/100, ImageNet and SVHN results are presented, along with a few ablations.  Reviewers were in agreement that the novelty of the contribution appears to be very limited, the evidence for the claims is not strong, and that the applicability of the method for achieving efficiency gains is limited to architectures that only apply dropout very late in processing, precluding applicability to models that employ dropout throughout. Importantly, comparisons to Fast Dropout (Wang 2013) seem highly relevant and are missing.  While the reviewers acknowledged some of the criticisms, virtually no arguments were offered to rebut them and no updates were made to address them. I therefore recommend rejection.
This paper focuses on avoiding overfitting in the presence of noisy labels. The authors develop a two phase method called pre stopping based on a combination of early stopping and a maximal safe set. The reviewers raised some concern about illustrating maximal safe set for all data sets and suggest comparisons with more baselines. The reviewers also indicated that the paper is missing key relevant publications. In the response the authors have done a rather through job of addressing the reviewers comments. I thank them for this. However, given the limited time some of the reviewers comments regarding adding new baselines could not be addressed. As a result I can not recommend acceptance because I think this is key to making a proper assessment. That said, I think this is an interesting with good potential if it can outperform other baselines and would recommend that the authors revise and resubmit in a future venue.
The paper proposes to learn a "virtual user" while learning a "recommender" model, to improve the performance of the recommender system. A reinforcement learning algorithm is used for address the problem the authors defined. Multiple reviewers raised several concerns regarding its technical details including the feedback signal F, but the authors have not responded to any of the concerns raised by the reviewers. The lack of authors involvement in the discussion suggest that this paper is not at the stage to be published.
This paper proposes a certified defense under the more general threat model beyond additive perturbation. The proposed defense method is based on adding noise to the classifier s outputs to limit the attacker s knowledge about the parameters, which is similar to differential privacy mechanism. The authors proved the query complexity for any attacker to generate a successful adversarial attack. The main objection of this work is (1) the assumption of the attacker and the definition of the query complexity (to recover the optimal classifier rather than generating an adversarial example successfully) is uncommon, (2) the claim is misleading, and (3) the experimental evaluation is not sufficient (only two attacks are evaluated). The authors only provided a brief response to address the reviewers’ comments/questions without submitting a revision. Unfortunately none of the reviewer is in support of this paper even after author response. 
This paper introduces T NAS, a neural architecture search (NAS) method that can quickly adapt architectures to new datasets based on gradient based meta learning. It is a combination of the NAS method DARTS and the meta learning method MAML.  All reviewers had some questions and minor criticisms that the authors replied to, and in the private discussion of reviewers and AC all reviewers were happy with the authors  answers. There was unanimous agreement that this is a solid poster.   Therefore, I recommend acceptance as a poster.
This paper proposes using the Fisher information matrix to characterize local minima of deep network loss landscapes to indicate generalizability of a local minimum. While the reviewers agree that this paper contains interesting ideas and its presentation has been substantially improved during the discussion period, there are still issues that remain unanswered, in particular between the main objective/claims and the presented evidence. The paper will benefit from a revision and resubmission to another venue.
This paper proposing a framework for augmenting classification systems with explanations was very well received by two reviewers, and on reviewer labeling themselves as "perfectly neutral". I see no reason not to recommend acceptance.
The submission presents an approach to accelerating convolutional networks. The framework is related to depthwise separable convolutions. The reviews are split. R3 expresses concerns about the experimental evaluation and results. The AC agrees with these concerns. The AC also notes that the submission is 10 pages long. Taking all factors into account, the AC recommends against accepting the paper.
This paper presents White Box Network (WBN), which allows for composing function blocks from a given set of functions to construct a target function. The main idea is to introduce a selection layer that only selects one element of the previous layer as an input to a function block.  The reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR in its current form.  There were significant concerns about the clarity in writing, and reviewers have provided detailed discussion should the authors wish to improve the paper.
This paper proposes a way to transform word vectors based on a binary attribute (e.g. male/female) based on reflection, with the property that applying the reflection operator twice, the vector for a word is left unchanged.  By identifying parameterized mirror planes for each word, the proposed method can leave neutral words left unchanged.  The paper received 3 weak accepts.  There was initially one reject, but the revisions convinced the reviewer to update their score to a weak accept.  Overall, the reviewers appreciated the idea of reflection based binary word attribute transfer.   suggestions, the authors made small improvements to the writing, added missing citations, as well as additional results for another word embedding (GloVE) and another dataset (antonyms).  One of the main remaining weakness of the work, is still the small dataset.  Although somewhat alleviated by the inclusion of the antonym dataset, this is still a weakness of the paper.    The AC agrees that the paper has an nice idea and is well presented.  However, the work is limited in scope and is likely to be of limited interest to the ICLR community and would be more appreciated in the NLP community.  The authors are encouraged to improve upon the work, and resubmit to an appropriate venue.   
Two reviewers are borderline and one recommends rejection. The main criticism is the simplicity of language, scalability to a more complex problem, and questions about experiments. Due to the lack of stronger support, the paper cannot be accepted at this point. The authors are encouraged to address the reviewer s comments and resubmit to a future conference.
This paper proposes a semi supervised method for reconstructing 3D faces from images via a disentangled representation. The method builds on previous work by Tran et al (2018, 2019). While some results presented in the paper show that this method works well, all reviewers agree that the authors should have provided more experimental evidence to convincingly demonstrate the benefits of their method. The reviewers are also unconvinced by how computationally expensive this method is or by the contributions of the unlabelled data to the performance of the proposed model. Given that the authors did not address the reviewers’ concerns, and for the reasons stated above, I recommend rejecting this paper.
The paper proposes a new method for improving generative properties of VAE model.  The reviewers unanimously agree that this paper is not ready to be published, particularly being concerned about the unclear objective and potentially misleading claims of the paper. Multiple reviewers pointed out about incorrect claims and statements without theoretical or empirical justification. The reviewers also mention that the paper does not provide new insights about VAE model as MDL interpretation of VAE it is not new.
The paper discusses a simple but apparently effective clustering technique to improve exploration. There are no theoretical results, hence the reader relies fully on the experiments to evaluate the method. Unfortunately, an in dept analysis of the results is missing making it hard to properly evaluate the strength and weaknesses. Furthermore, the authors have not provided any rebuttal to the reviewers  concerns.
This paper presents nucleus sampling, a sampling method that truncates the tail of a probability distribution and samples from a dynamic nucleus containing the majority of the probability mass. Likelihood and human evaluations show that the proposed method is a better alternative to a standard sampling method and top k sampling.  This is a well written paper and I think the proposed sampling method will be useful in language modeling. All reviewers agree that the paper addresses an important problem.   Two reviewers have concerns regarding the technical contribution of the paper (i.e., nucleus sampling is a straightforward extension of top k sampling), and whether it is enough for publications at a venue such as ICLR. R2 suggests to have a better theoretical framework for nucleus sampling. I think these are valid concerns. However, given the potential widespread application of the proposed method and the strong empirical results, I recommend to accept the paper.  Also, a minor comment, I think there is something wrong with your style file (e.g., the bottom margin appears too large compared to other submissions).
This paper presents two new architectures that model latent intermediate utilities and use non additive utility aggregation to estimate the set utility based on the computed latent utilities. These two extensions are easy to understand and seem like a simple extension to the existing RNN model architectures, so that they can be implemented easily. However, the connection to Choquet integral is not clear and no theory has been provided to make that connection. Hence, it is hard for the reader to understand why the integral is useful here.  The reviewers have also raised objection about the evaluation which does not seem to be fair to existing methods. These comments can be incorporated to make the paper more accessible and the results more appreciable. 
This paper provides an interesting insight into the fitting of variational autoencoders.  While much of the recent literature focuses on training ever more expressive models, the authors demonstrate that learning a flexible prior can provide an equally strong model.  Unfortunately one review is somewhat terse.  Among the other reviews, one reviewer found the paper very interesting and compelling but did not feel comfortable raising their score to "accept" in the discussion phase citing a lack of compelling empirical results in compared to baselines.  Both reviewers were concerned about novelty in light of Huang et al., in which a RealNVP prior is also learned in a VAE.  AnonReviewer3 also felt that the experiments were not thorough enough to back up the claims in the paper.  Unfortunately, for these reasons the recommendation is to reject.  More compelling empirical results with carefully chosen baselines to back up the claims of the paper and comparison to existing literature (Huang et al) would make this paper much stronger.  
The primary contribution of this manuscript is a conceptual and theoretical solution to the sample elicitation problem, where agents are asked to report samples. The procedure is implemented using score functions to evaluate the quality of the samples.  The reviewers and AC agree that the problem studied is timely and interesting, as there is limited work on credible sample elicitation in the literature. However, the reviewers were unconvinced about the motivation of the work, and the clarity of the conceptual results. There is also a lack of empirical evaluation. IN the opinion of the AC, this manuscript, while interesting, can be improved by significant revision for clarity and context, and revisions should ideally include some empirical evaluation.
This paper proposes a method for transferring an NLP model trained on one language a new language, without using labeled data in the new language.   Reviewers were split on their recommendations, but the reviews collectively raised a number of concerns which, together, make me uncomfortable accepting the paper. Reviewers were not convinced by the value of the experimental setting described in the paper—at least in the experiments conducted here, the claim that the model is distinctively effective depend on ruling out a large class of models arbitrarily. it would likely be valuable to find a concrete task/dataset/language combination that more closely aligns with the motivations for this work, and to evaluate whether the proposed method is genuinely the most effective practical option in that setting. Further, the reviewers raise a number of points involving baseline implementations, language families, and other issues, that collectively make me doubt that the paper is fully sound in its current form.
The authors propose using a noisy channel formulation which allows them to combine a sentence level target source translation model with a language model trained over target side document level information. They use reranking of a 50 best list generated by a standard Transformer model for forward translation and show reasonably strong results.  The reviewers were concerned about the efficiency of this approach and the limited novelty as compared to the sentence level noisy channel research Yu et al. 2017. The authors responded in depth, adding results with another baseline which includes backtranslated data. I feel that although this paper is interesting, it is not compelling enough for inclusion in ICLR. 
Two reviewers recommend acceptance. One reviewer is negative, however, does not provide reasons for rejection. The AC read the paper and agrees with the positive reviewers. in that the paper provides value for the community on an important topic of network compression.
The paper investigates calibration for regression problems. The paper identifies a shortcoming of previous work by Kuleshov et al. 2018 and proposes an alternative.   All the reviewers agreed that while this is an interesting direction, the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about motivation, clarity of the presentation and lack of in depth empirical evaluation.  I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue. 
The paper presented a unified framework for constructing likelihood based generative models for raw audio. It demonstrated tradeoffs between memory footprint, generation speech and audio fidelity. The experimental justification with objective likelihood scores and subjective mean opinion scores are matching standard baselines. The main concern of this paper is the novelty and depth of the analysis. It could be much stronger if there re thorough analysis on the benefits and limitations of the unified approach and more insights on how to make the model much better. 
This paper studies the effect of various regularization techniques for dealing with noisy labels. In particular the authors study various regularization techniques such as distance from initialization to mitigate this effect. The authors also provide theory in the NTK regime. All reviewers have positive assessment about the paper and think is clearly written with nice contributions but do raise some questions about novelty given that it mostly follows the normal NTK regime. I agree that the paper is nicely written and well motivated. I do not think the theory developed here fully captures all the nuances of practical observations in this problem. In particular, with label noise this theory suggests that test performance is not dramatically affected by label noise when using regularization or early stopping where as in practice what has been observed (and even proven in some cases) is that the performance is completely unaffected with small label noise. I think this paper is a good addition to ICLR and therefore recommend acceptance but recommend the authors to more clearly articulate the above nuances and limitations of their theory in the final manuscript.  
The authors observe that batch normalization using the statistics computed from a *test* batch significantly improves out of distribution detection with generative models.  Essentially, normalizing an OOD test batch using the test batch statistics decreases the likelihood of that batch and thus improves detection of OOD examples.  The reviewers seemed concerned with this setting and they felt that it gives a significant advantage over existing methods since they typically deal with single test example.  The reviewers thus wanted empirical comparisons to methods designed for this setting, i.e. traditional statistical tests for comparing distributions.  Despite some positive discussion, this paper unfortunately falls below the bar for acceptance.  The authors added significant experiments and hopefully adding these and additional analysis providing some insight into how the batchnorm is helping would make for a stronger submission to a future conference.
All reviewers agreed that this paper is essentially a combination of existing ideas, making it a bit incremental, but is well executed and a good contribution.  Specifically, to quote R1:  "This paper proposes a generative model architecture for molecular graph generation based on autoregressive flows. The main contribution of this paper is to combine existing techniques (auto regressive BFS ordered generation of graphs, normalizing flows, dequantization by Gaussian noise, fine tuning based on reinforcement learning for molecular property optimization, and validity constrained sampling). Most of these techniques are well established either for data generation with normalizing flows or for molecular graph generation and the novelty lies in the combination of these building blocks into a framework. ... Overall, the paper is very well written, nicely structured and addresses an important problem. The framework in its entirety is novel, but the building blocks of the proposed framework are established in prior work and the idea of using normalizing flows for graph generation has been proposed in earlier work. Nonetheless, I find the paper relevant for an ICLR audience and the quality of execution and presentation of the paper is good."
Overview: This paper introduces a method to distill a large dataset into a smaller one that allows for faster training. The main application of this technique being studied is neural architecture search, which can be sped up by quickly evaluating architectures on the generated data rather than slowly evaluating them on the original data.  Summary of discussion: During the discussion period, the authors appear to have updated the paper quite a bit, leading to the reviewers feeling more positive about it now than in the beginning. In particular, in the beginning, it appears to have been unclear that the distillation is merely used as a speedup trick, not to generate additional information out of thin air. The reviewers  scores left the paper below the decision boundary, but closely enough so that I read it myself.   My own judgement: I like the idea, which I find very novel. However, I have to push back on the authors  claims about their good performance in NAS. This has several reasons:  1. In contrast to what is claimed by the authors, the comparison to graph hypernetworks (Zhang et al) is not fair, since the authors used a different protocol: Zhang et al sampled 800 networks and reported the performance (mean +/  std) of the 10 judged to be best by the hypernetwork. In contrast, the authors of the current paper sampled 1000 networks and reported the performance of the single one judged to be best. They repeated this procedure 5 times to get mean +/  std. The best architecture of 1000 is of course more likely to be strong than the average of the top 10 of 800.  2. The comparison to random search with weight sharing (here: 3.92% error) does not appear fair. The cited paper in Table 1 is *not* the paper introducing random search + weight sharing, but the neural architecture optimization paper. The original one reported an error of 2.85% +/  0.08% with 4.3M params. That paper also has the full source code available, so the authors could have performed a true apples to apples comparison.   3. The authors  method requires an additional (one time) cost for actually creating the  fake  training data, so their runtimes should be increased by the 8h required for that.  4. The fact that the authors achieve 2.42% error doesn t mean much; that result is just based on scaling the network up to 100M params. (The network obtained by random search also achieves 2.51%.)  As it stands, I cannot judge whether the authors  approach yields strong performance for NAS. In order to allow that conclusion, the authors would have to compare to another method based on the same underlying code base and experimental protocol. Also, the authors do not make code available at this time. Their method has a lot of bells and whistles, and I do not expect that I could reproduce it. They promise code, but it is unclear what this would include: the generated training data, code for training the networks, code for the meta approach, etc? This would have been much easier to judge had the authors made the code available in anonymized fashion during the review.  Because of these reasons, in terms of making progress on NAS, the paper does not quite clear the bar for me. The authors also evaluated their method in several other scenarios, including reinforcement learning. These results appear to be very promising, but largely preliminary due to lack of time in the rebuttal phase.    Recommendation: The paper is very novel and the results appear very promising, but they are also somewhat preliminary. The reviewers  scores leave the paper just below the acceptance threshold and my own borderline judgement is not positive enough to overrule this. I believe that some more time, and one more iteration of reorganization and review, would allow this paper to ripen into a very strong paper. For a resubmission to the next venue, I would recommend to either perform an apples to apples comparison for NAS or reorganize and just use NAS as one of several equally weighted possible applications. In the current form, I believe the paper is not using its full potential.
This paper presents a novel option discovery mechanism through incrementally learning reusasble options from a small number of policies that are usable across multiple tasks.  The primary concern with this paper was with a number of issues around the experiments. Specifically, the reviewers took issue with the definition of novel tasks in the Atari context. A more robust discussion and analysis around what tasks are considered novel would be useful. Comparisons to other option discovery papers on the Atari domains is also required.  Additionally, one reviewer had concerns on the hard limit of option execution length which remain unresolved following the discussion.  While this is really promising work, it is not ready to be accepted at this stage.
This paper proposes an approach to semi supervised few shot learning. In a discussion after the rebuttal phase, the reviewers were somewhat split on this paper, appreciating the advantages of the algorithm such as increased robustness to distractors and the ability to adapt with additional iterations, but were concerned that the contributions over Ren et al were not significant. Overall, the contributions of this paper don t quite warrant publication at ICLR.
Thanks for clarifying several issues raised by the reviewers, which helped us understand the paper.  After all, we decided not to accept this paper due to the weakness of its contribution. I hope the updated comments by the reviewers help you strengthen your paper for potential future submission.
Agreement by the reviewers: although the idea is good, the paper is very hard to read and not accurately enough formulated to merit publication.    This can be repaired, and the authors should try again after a thorough revision and rewrite.
This paper attempts to improve adversarial imitation learning (GAIL) by encouraging the discriminator to focus on task dependent features.  An advantage of this paper is that it not only improves upon GAIL, but it is doing so after first demonstrating and analyzing an existing issue.   On the other hand, the presentation of the paper and breadth of experiments could be significantly improved further than the updated version. It would also be necessary to clarify whether the baseline is vanilla PG or D4PG.  A major point for discussion was the selection of the invariance set. The ablation studies and explanation provided during the rebuttal period towards this point are helpful, but somehow we still do not have the full picture to understand well how this method compares to existing literature. 
The paper received 6, 3, 1. The main criticism is the lack of quantitative evaluation/comparison. The rebuttal did not convince the last reviewer who strongly argues for a comparison. The authors are encouraged to add additional results and resubmit to a future venue.
Three reviewers have assessed this submission and were moderately positive about it . However, the reviewers have also raised a number of concerns. Initially, they complained about substandard experimentation which has been resolved to some degree after rebuttal (rev. believe more can be done in terms of unifying them, investigating backbones, attack methods, and experimental settings in light of recent papers).  A somewhat bigger criticism concerns the theoretical part:  1. Rev. remained unclear why using tensor decomposition techniques is a sound approach for designing robust network. 2. AC and rev. also noted during discussions that using low rank constraints (and other mechanisms) and i.e. encouraging smoothness (one important mechanism among many in robustness to attacks) have been extensively investigated in the literature, yet, the proposed idea makes scarce if any theoretical connection to such important theoretical tools.  Some references (not exhaustive) that may help authors further study the above aspects are: Certified Adversarial Robustness via Randomized Smoothing, Cohen et al. Local Gradients Smoothing: Defense against localized adversarial attacks, Naseer et al. Limitations of the Lipschitz constant as adefense against adversarial examples, Huster et al. Learning Low Rank Representations, Huster et al.  On balance, AC feels that despite the enthusiasm, this paper is not ready yet for the publication in ICLR as the key theory behind the proposed idea is missing. Thus, this submission falls marginally short of acceptance in ICLR 2020. However, the authors are encouraged to build up a compelling theory and resubmit to another venue (currently the paper feels like a solid workshop idea that needs to be investigated further).
The paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. This is an important problem that is of general interest in our community.  Reviewer 2 found the paper to be clear, provided a set of weaknesses relating to lack of explanations of performance and more careful ablations, along with a set of strategies to address them. Reviewer 1 recognized the importance of being useful for pretrained networks but also raised questions of explanation and theoretical motivations. Reviewer 3 was extremely supportive, used the authors  code to highlight the difference between far from distribution behaviour versus near distribution OOD examples. The authors provided detailed responses to all points raised and provided additional eidence. There was  no convergence of the review recommendations.  The review added much more clarity to the paper and it is no a better paper. The paper demonstrates all the features of a good paper, but unfortunately didn t yet reach the level for acceptance for the next conference. 
The reviewers develop a novel technique for training neural networks that are provably robust to adversarial attacks, by combining provable defenses using convex relaxations with latent adversarial attacks that lie in the gap between the convex relaxation and the true realizable set of activations at a layer of the network. The authors show that the resulting procedure is computationally efficient and able to train neural networks to attain SOTA provable robustness to adversarial attacks.  The paper is well written and clearly explains an interesting idea, backed by thorough experiments. The reviewers were in consensus on acceptance and relatively minor concerns were clearly addressed in the rebuttal phase.  Hence, I strongly recommend acceptance.
This paper seeks to adapt behavioural cloning to the case where demonstrator and learner have different dynamics (e.g. human demonstrator), by designing a state based objective. The reviewers agreed the paper makes an important and interesting contribution, but were somewhat divided about whether the experiments were sufficiently impactful. They furthermore had additional concerns regarding the clarity of the paper and presentation of the method. Through discussion, it seems that these were sufficiently addressed that the consensus has moved towards agreeing that the paper sufficiently proves the concept to warrant publication (with one reviewer dissenting).  I recommend acceptance, with the view that the authors should put a substantial amount of work into improving the presentation of the paper based on the feedback that has emerged from the discussion before the camera ready is submitted (if accepted).
The paper proposes a method to combine the decision of an ensemble of RL agents. It uses an uncertainty measure based on the TD error, and suggests a weighted average or weighted voting mechanism to combine their policy or value functions to come up with a joint decision. The reviewers raised several concerns, including whether the method works in the stochastic setting, whether it favours deterministic parts of the state space, its sensitivity to bias, and unfair comparison to a single agent setting. There is also a relevant PhD dissertation (Elliot, 2017), which the authors surprisingly refused to discuss and cite because apparently it was not published at any conference. A PhD dissertation is a citable reference, if it is relevant. If it is, a good scholarship requires proper citation.  Overall, even though the proposed method might potentially be useful, it requires further investigations. Two out of three reviewers are not positive about the paper in its current form. Therefore, I cannot recommend acceptance at this stage.  Elliott, Daniel L., The Wisdom of the crowd : reliable deep reinforcement learning through ensembles of Q functions, PhD Dissertation, Colorado State University, 2017
This paper tackles the problem of learning under data shift, i.e. when the training and testing distributions are different. The authors propose an approach to improve robustness and uncertainty of image classifiers in this situation. The technique uses synthetic samples created by mixing multiple augmented images, in addition to a Jensen Shannon Divergence consistency loss. Its evaluation is entirely based on experimental evidence.  The method is simple, easy to implement, and effective. Though this is a purely empirical paper, the experiments are extensive and convincing.   In the end, the reviewers didn t show any objections against this paper. I therefore recommend acceptance.
All three reviewers strongly recommend accepting this paper. It is clear, novel, and a significant contribution to the field. Please take their suggestions into account in a camera ready version. Thanks!
This paper proposes to use contrastive predictive coding for self supervised learning.  The proposed approach is shown empirically to be  more effective than existing self supervised learning algorithms.  While the reviewers found the experimental results encouraging, there were some questions about the contribution as a whole, in particular the lack of theoretical justification.
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
The paper shows that initializing the parameters of a deep linear network from the orthogonal group speeds up learning, whereas sampling the parameters from a Gaussian may be harmful.  The result of this paper can be interesting to the deep learning community. The main concern the reviewers raised is the huge overlap with the paper by Du & Hu (2019). It would have been nice to actually see whether the results for linear networks empirically also hold for nonlinear networks. 
The paper is well motivated by neuroscience that our brains use information from outside the receptive field of convolutive processes through top down mechanisms. However, reviewers feel that the results are not near the state of the art and the paper needs further experiments and need to scale to larger datasets. 
This paper proposes a deep clustering method based on normalized cuts.  As the general idea of deep clustering has been investigated a fair bit, the reviewers suggest a more thorough empirical validation.  Myself, I would also like further justification of many of the choices within the algorithm, the effect of changing the architecture.
The paper is well written and presents an extensive set of experiments. The architecture is a simple yet interesting attempt at learning explainable rumour detection models. Some reviewers worry about the novelty of the approach, and whether the explainability of the model is in fact properly evaluated. The authors responded to the reviews and provided detailed feedback. A major limitation of this work is that explanations are at the level of input words. This is common in interpretability (LIME, etc), but it is not clear that explanations/interpretations are best provided at this level and not, say, at the level of training instances or at a more abstract level. It is also not clear that this approach would scale to languages that are morphologically rich and/or harder to segment into words. Since modern approaches to this problem would likely include pretrained language models, it is an interesting problem to make such architectures interpretable. 
An approach to make multi task learning is presented, based on the idea of assigning tasks through the concepts of cooperation and competition.   The main idea is well motivated and explained well. The experiments demonstrate that the method is promising. However, there are a few  concerns regarding fundamental aspects, such as: how are the decisions affected by the number of parameters? Could ad hoc algorithms with human in the loop provide the same benefit, when the task set is small? More importantly, identifying task groups for multi task learning is an idea presented in prior work, e.g. [1,2,3]. This important body of prior work is not discussed at all in this paper.  [1] Han and Zhang. "Learning multi level task groups in multi task learning" [2] Bonilla et al. "Multi task Gaussian process prediction" [3] Zhang and Yang. "A Survey on Multi Task Learning" 
The paper received mixed reviews. On one hand, there is interesting novelty in relation to biological vision systems. On the other hand, there are some serious experimental issues with the machine learning model. While reviewers initially raised concerns about the motivation of the work, the rebuttal addressed those concerns. However, concerns about experiments remained. 
The authors propose a novel approach to using surrogate gradient information in ES. Unlike previous approaches, their method always finds a descent direction that is better than the surrogate gradient. This allows them to use previous gradient estimates as the surrogate gradient. They prove results for the linear case and under simplifying assumptions that it extends beyond the linear case. Finally, they evaluate on MNIST and RL tasks and show improvements over ES.  After the revisions, reviewers were concerned about:  * The strong (and potentially unrealistic) assumptions for the theorems. They felt that these assumptions trivialized the theorems. * Limited experiments demonstrating advantages in situations where other more effective methods could be used. The performance on the RL tasks shows small gains compared to a vanilla ES approach. Thus, the usefulness of the approach is not clearly demonstrated.   I think that the paper has the potential to be a strong submission if the authors can extend their experiments to more complex problems and demonstrate gains. At this time however, I recommend rejection.
This paper proposes a data augmentation method based on Generative Adversarial Networks by training several GANs on subsets of the data which are then used to synthesise new training examples in proportion to their estimated quality as measured by the Inception Score. The reviewers have raised several critical issues with the work, including motivation (it can be harder to train a generative model than a discriminative one), novelty, complexity of the proposed method, and lack of comparison to existing methods. Perhaps the most important one is the inadequate empirical evaluation. The authors didn’t address any of the raised concerns in the rebuttal. I will hence recommend the rejection of this paper.
This paper addresses the setting of imitation learning from state observations only, where the system dynamics under which the demonstrations are performed differs from the target environment. The paper proposes to circumvent this dynamics shift with an algorithm whereby the target policy is trained to imitate its own past trajectories, re ranked based on the similarity in state occupancies as judged by a WGAN critic.  The reviewers found the paper to be clearly written and enjoyable. The paper improved considerably through reviewers feedback. Notably, a behavior cloning from observations (BCO) baseline was added, which was stronger than the authors expected but still helped highlight the strength of the proposed method by comparison. R1 had a particularly productive multiple round exchange, clarifying the description of previous work, clarifying the details of the proposed procedure and strengthening the presentation of empirical evidence.  This work compellingly addresses an important problem, and in its final form is a polished piece of work. I recommend acceptance.
First, I d like to apologize once again for failing to secure a third reviewer for this paper. To compensate, I checked the paper more thoroughly than standard.  The area of online adaptation of the learning rate is of great importance and I appreciate the authors  effort in that direction. The authors carefully abundantly cite the research on gradient based hyperparameter optimization but I would have appreciated to also see past works on stochastic line search (for instance  "A stochastic line search method with convergence rate") or statistical methods ("Using Statistics to Automate Stochastic Optimization").  The issue with these methods is that, despite usually very positive claims in the paper, they are not that competitive against a carefully tuned fixed schedule and end up not being used in practice. Hence, it is critical to develop a convincing experimental section to assuage doubts. Unfortunately, the experimental section of this work is a bit lacking, as pointed by both reviewers. I would like to comment on two points specifically:   First, no plot uses wall clock time as the x axis. Since the authors state that it can be up to 4 times as slow per iteration, the gains compared to a carefully tuned schedule are unclear.   Second, the use of a single (albeit two variants) dataset also leads to skepticism. Datasets have vastly different optimization properties and, by not using a wide range of them, one can miss the true sensitivity of the proposed algorithm.  While I do not think that the paper is ready for publication, I feel like there is a clear path to an improved version that could be submitted to a later conference.
The authors propose a graph residual flow model for molecular generation.  Conceptual novelty is limited since it is simple extension and there isn t much improvement over state of art.
The paper proposes a method for learning a latent dynamics model for videos. The main idea is to learn a latent representation and model the dynamics of the latent features via residual connection motivated by ODE. The architectural choice of residual connection itself is not new as many prior works have employed "skip connections" in hidden representations but the notion of connecting this with ODE and factoring time as input into the residual function seems a new idea. The experimental results show the promise of the proposed method on moving MNIST, KTH, and BAIR datasets. The experiments on different frame rates are also nice.  In terms of weakness, the evaluation is performed on relatively simple domains (e.g., moving MNIST and KTH) with static backgrounds and the improvement on BAIR dataset (which is not considered as a difficult benchmark) in terms of FVD is not as clear. For the BAIR dataset, it s unclear how the proposed method will handle the interactions between the robot arm and background objects due to the modeling assumption (i.e., static background). In this sense, content swap results on BAIR dataset look quite anecdotal, and the significance is limited. For improvement, I would suggest adding evaluations on other challenging domains, such as Human 3.6M (where human motions are much more uncertain compared to KTH) and other Robot datasets with more complex robot object interactions. Overall, the paper proposes an interesting architecture with promising results on relatively simple datasets, but the advantage over existing SOTA methods on challenging benchmarks is unclear yet. 
Apologies for only receiving two reviews. R2 gave a WR and R3 gave an A. Given the lack of 3rd review and split nature of the scores, the AC has closely scrutinized the paper/reviews/comments/rebuttal. Thoughts:    Paper is on interesting topic.    AC agrees with R2 s concern about the evaluation not using more complex environments like Mujoco. Without evaluation on a standard benchmark, it is difficult to know objectively if the approach works.     AC agrees with authors that the DISTRAL approach forms a strong baseline.     Nevertheless, the experiments aren t super compelling either.    AC has some concerns about scaling issues w.r.t. model size & #tasks.   The paper is very borderline, but the AC sides with R2 s concerns and unfortunately feels the paper cannot be accepted without a stronger evaluation. With this, it would make a compelling paper. 
This paper introduces an unsupervised learning objective that attempts to improve the robustness of the learnt representations. This approach is empirically demonstrated on cifar10 and tiny imagenet with different network architectures including all convolutional net, wide residual net and dense net.   Two of three reviewers felt that the paper was not suitable for publication at ICLR in its current form.  Self supervision based on preserving network outputs despite data transformations is a relatively minor contribution, the framing of the approach as inspired by biological vision notwithstanding.  Several references, including at a past ICLR include: http://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf and  Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations.  In International Conference on Learning Representations (ICLR), 2018.
This manuscript proposes an approach for fair and robust training of predictive modeling   both of which are implemented using adversarial methods, i.e., an adversarial loss for fairness and an adversarial loss for robustness. The resulting model is evaluated empirically and shown to improve fairness and robustness performance.  The reviewers and AC agree that the problem studied is timely and interesting, as there is limited work on joint fairness and robustness. However, the reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. In reviews and discussion, the reviewers also noted insufficient motivation for the approach. 
Thanks for your detailed replies to the reviewers, which helped us a lot to clarify several issues. Although the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited. Given the high competition of ICLR2020, this paper is still below the bar unfortunately.
The paper proposes to model uncertainty using expected Bayes factors, and empirically show that the proposed measure correlates well with the probability that the classification is correct.  All the reviewers agreed that the idea of using Bayes factors for uncertainty estimation is an interesting approach. However, the reviewers also found the presentation a bit hard to follow. While the rebuttal addressed some of these concerns, there were still some remaining concerns (see R3 s comments).   I think this is a really promising direction of research and I appreciate the authors  efforts to revise the draft during the rebuttal (which led to some reviewers increasing the score). This is a borderline paper right now but I feel that the paper has the potential to turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue.
The submission addresses the problem of continual learning with large numbers of tasks and variable task ordering and proposes a parameter decomposition approach such that part of the parameters are task adaptive and some are task shared. The validation is on omniglot and other benchmarks.  The reviews were mixed on this paper, but most reviewers were favorably impressed with the problem setup, the scalability of the method, and the results. The baselines were limited but acceptable. The recommendation is to accept this paper, but the authors are advised to address all the points in the reviews in their final revision.
The paper presents a model combining AC GAN and StyleGAN for semi supervised learning of disentangled generative adversarial networks. It also proposes new datasets of 3d images as benchmarks. The main claim is that the proposed model can achieve strong disentanglement property by using 1 5% of the annotations on the factors of variation. The technical contribution is moderate but the architecture itself is not highly novel. While the proposed method seems to work for controlled/synthetic datasets, overall technical contribution seems incremental and it s unclear whether it can perform well on larger scale, real datasets. The experimental results on CelebA don t look convincing enough. 
VAE based sample selection for training NNs.  A well written experimental paper that is demonstrated through a number of experiments, all of which are minimal and from which generalization is not per se expected.  The absence of an underlying theory, and the absence of rigorous experimentation makes me request to extend either or, better, both.  
This paper proposes to improve the faithfulness of data to text generation models, through an attention based confidence measure and a variational approach for learning the model.  There is some reviewer disagreement on this paper.  All agree that the problem is important and ideas interesting, while some reviewers feel that the methods are insufficiently justified and/or the results unconvincing.  In addition, there is not much technical novelty here from a machine learning perspective; the contribution is to a specific task.  Overall I think this paper would fit in much better in an NLP conference/journal.
The paper proposes a new style transfer task, contextual style transfer, which hypothesises that the document context of the sentence is important, as opposed to previous work which only looked at sentence context. A major contribution of the paper is the creation of two new crowd sourced datasets, Enron Context and Reddit Context focussed on formality and offensiveness. The reviewers are skeptical that it was context that has really improved results on the style transfer tasks. The authors responded to all the reviewers but there was no further discussion. I feel like this paper has not convinced me or the reviewers of the strength of its contribution and, although interesting, I recommend for it to be rejected. 
The paper proposes combining paired attention with co attention. The reviewers have remarked that the paper is will written and that the experiments provide some new insights into this combination. Initially, some additional experiments were proposed, which were addressed by the authors in the rebuttal and the new version of the paper. However, ICLR is becoming a very competitive conference where novelty is an important criteria for acceptance, and unfortunately the paper was considered to lack the novelty to be presented at ICLR.
This paper proposes to use CNN S prior to deal with the tasks in audio processing. The motivation is weak and the presentation is not clear.  The technical contribution is trivial.
This paper provides a theoretical background for the expressive power of graph convolutional networks. The results are obviously useful, and the discussion went in the positive way. All reviewers recommend accepting, and I am with them.
The paper presents a timely method for intuitive physics simulations that expand on the HTRN model, and tested in several physicals systems with rigid and deformable objects as well as other results later in the review.   Reviewer 3 was positive about the paper, and suggested improving the exposition to make it more self contained. Reviewer 1 raised questions about the complexity of tasks and a concerns of limited advancement provided by the paper. Reviewer 2, had a similar concerns about limited clarity as to how the changes contribute to the results, and missing baselines. The authors provided detailed responses in all cases, providing some additional results with various other videos. After discussion and reviewing the additional results, the role of the stochastic elements of the model and its contributions to performance remained and the reviewers chose not to adjust their ratings.  The paper is interesting, timely and addresses important questions, but questions remain. We hope the review has provided useful information for their ongoing research. 
This paper proposes a benchmark for assessing the impact of image quality degradation (e.g. simulated fog, snow, frost) on the performance of object detection models. The authors introduce corrupted versions of popular object detection datasets, namely PASCAL C, COCO C and Cityscapes C, and an evaluation protocol which reveals that the current models are not robust to such corruptions (losing as much as 60% of the performance). The authors then show that a simple data augmentation scheme significantly improves robustness. The reviewers agree that the manuscript is well written and that the proposed benchmark reveals major drawbacks of current detection models. However, two critical issues with the paper paper remain, namely lack of novelty in light of Geirhos et al., and how to actually use this benchmark in practice. I will hence recommend the rejection of this paper in the current state. Nevertheless, we encourage the authors to address the raised shortcomings (the new experiments reported in the rebuttal are a good starting point). 
This paper presents a differentially private mechanism, called Noisy ArgMax, for privately aggregating predictions from several teacher models. There is a consensus in the discussion that the technique of adding a large constant to the largest vote breaks differential privacy. Given this technical flaw, the paper cannot be accepted.
This paper describes situations whereby data augmentation (particularly drawn from a true distribution) can lead to increased generalization error even when the model being optimized is appropriately formulated. The authors propose "X regularization" which requires that models trained on standard and augmented data produce similar predictions on unlabeled data. The paper includes a few experiments on a toy staircase regression problem as well as some ResNet experiments on CIFAR 10.  This paper received 2 recommendations for rejection, and one weak accept recommendation.  After the rebuttal phase, the author who recommended weak acceptance indicated their willingness to let the paper be rejected in light of the other reviews.  The reviewer highlighted: "I think the authors could still to better to relate their theory to practice, and expand on the discussion/presentation of X regularization."  The main open issue is that the theoretical contributions of the paper are not sufficiently linked to the proposed algorithm.
The paper considers the problem of training neural networks asynchronously, and the gap in generalization due to different local minima being accessible with different delays. The authors derive a theoretical model for the delayed gradients, which provide prescriptions for setting the learning rate and momentum.  All reviewers agreed that this a nice paper with valuable theoretical and empirical contributions.  
This paper proposes to overcome the issue of inconsistent availability of longitundinal data via the combination of leveraging principal components analysis and locality preserving projections. All three reviewers express significant reservations regarding the technical writing in the paper. As it stands, this paper is not ready for publication.  
Paper is withdrawn by authors.
The paper proposes an architecture for semantic instance segmentation learnable from coarse annotations and evaluates it on two microscopy image datasets, demonstrating its advantage over baseline. While the reviewers appreciate the details of the architecture, they note the lack of evaluation on any of popular datasets and the lack of comparisons with baselines that would be more close to state of the art. The authors do not address this criticism convincingly. It is not clear, why e.g. the Cityscapes or VOC Pascal datasets, which both have reasonably accurate annotations, cannot be used for the validation of the idea. If the focus is on the precision of the result near the boundaries, then one can always report the error near boundaries (this is a standard thing to do). Note that the performance of the baseline models is far from saturated near boundaries (i.e. the errors are larger than mistakes of annotation).  At this stage, the paper lacks convincing evaluation and comparison with prior art. Given that this is first and foremost application paper, lacking some very novel ideas (as pointed out by e.g. Rev1), better evaluation is needed for acceptance.
This paper proposes an adaptive gradient method for optimization in deep learning called AvaGrad.  The authors argue that AvaGrad greatly simplifies hyperparameter search (over e.g. ADAM) and demonstrate competitive performance on benchmark image and text problems.  In thorough reviews, thorough author response and discussion by the reviewers (which are are all appreciated) a few concerns about the work came to light and were debated.  One reviewer was compelled by the author response to raise their recommendation to weak accept.  However, none of the reviewers felt strongly enough to champion the paper for acceptance and even the reviewer assigning the highest score had reservations.  A major issue of debate was the treatment of hyperparameters, i.e. that the authors tuned hyperparameters on a smaller problem and then assumed these would extrapolate to larger problems. In a largely empirical paper this does seem to be a significant concern.  The space of adaptive optimizers for deep learning is a crowded one and thus the empirical (or theoretical) burden of proof of superiority is high.  The authors state regarding a concurrent submission: "when hyperparameters are properly tuned, echoing our results on this matter", however, it seems that the reviewers disagree that the hyperparameters are indeed properly tuned in this paper.  It s due to these remaining reservations that the recommendation is to reject.  
The paper is rejected based on unanimous reviews.
This paper presents a method for optimizing parameter matrices of deep learning objectives while enforcing orthonormality constraints.  While advantageous in certain respects, such constraints can be expensive to maintain when using existing methods.  To address this issue, an new algorithm is proposed based on the Cayley Transform and analyzed in terms of convergence.  After the discussion period two reviewers supported acceptance while one still voted for rejection.  Consequently, in recommending acceptance here for a poster, it is worth examining the significance of unresolved concerns.  First, the reject reviewer raised the valid point that the convergence proof relies on the assumption of Lipschitz continuous gradients, and yet the experiments use ReLU activation functions that do not satisfy this criteria.  In my view though, it is sometimes reasonable to derive useful theory under the assumption of Lipschitz continuous derivatives that nonetheless provides insight into the case where these derivatives may not be Lipschitz on a set of measure zero (which would be the case with ReLU activations).  So while ideally it might be nice to extend the theory to remove this assumption, the algorithm seems to work fine with ReLU activations in practice.  And this seems reasonable given the improbability of any iterate exactly hitting the measure zero points where the gradients are discontinuous.  Beyond this issue, some criticisms were mentioned in terms of how and where the timing comparisons were presented.  However, I believe that these issues can be easily remedied in a final revision.
Main content:  This paper presents negation handling approaches for Amharic sentiment classification.     Discussion:  All reviewers agree the paper is poorly written, uses outdated approaches, and requires better organization and formatting.     Recommendation and justification:  This paper after more work might be better submitted in an NLP workshop on low resource languages, rather than ICLR which is more focused on new machine learning methods.
This paper proposes a method to learn data augmentation policies using an adversarial loss. In contrast to AutoAugment where an augmentation policy generator is trained by RL (computationally expensive), the authors propose to train a policy generator and the target classifier simultaneously. This is done in an adversarial fashion by computing augmentation policies which increase the loss of the classifier. The authors show that this approach leads to roughly an order of magnitude improvement in computational cost over AutoAugment, while improving the test performance. The reviewers agree that the presentation is clear and that the proposed method is sound, and that there is a significant practical benefit of using such a technique. As most of the concerns were addressed in the discussion phase, I will recommend acceptance of this paper. We ask the authors to update the manuscript to address the remaining (minor) concerns. 
This paper presents a method to learn representations of programs via code and execution.  The paper presents an interesting method, and results on branch prediction and address pre fetching are conclusive. The only main critiques associated with this paper seemed to be (1) potential lack of interest to the ICLR community, and (2) lack of comparison to other methods that similarly improve performance using other varieties of information. I am satisfied by the authors  responses to these concerns, and believe the paper warrants acceptance.
This paper proposes PAC Bayes bounds for meta learning. The reviewers who are most knowledgeable about the subject and who read the paper most closely brought up several concerns regarding novelty (especially a description of how the proposed bounds relate to those in prior works (Pentina el al. (2014), Galanti et al. (2016) and Amit and Meir (2018))) and regarding clarity. The reviewers found theoretical analysis and proofs hard to follow. For these reasons, the paper isn t ready for publication at this time. See the reviewer s comments for details.
This paper defines a parameter tying scheme for a general feed forward network with the equivalence properties of relational data. Most reviewers raised a few concerns around the experiments, baselines, datasets used and motivation. A few pointed out that the paper is hard to read   for a person without heavy database theory literature, which includes most of ICLR readers. While this paper may read well for the folks in the domain, authors should consider revising the paper to be more inclusive so that it can be read more widely. The motivation of the problem was also another point that many reviewers have mentioned (perhaps related to the language issues above) that some noted that you may not always want equivariance in relational DB and other noted that the paper did not sufficiently demonstrate the advantage of the proposed methods. Reviewers also univocally commented on experiments   many voiced the lack of baselines (not even any simple one). Authors wrote back to defend that there is no similar method and even simple tensor factorization isn’t applicable. That makes me wonder   is there really no single simple method you can compare with? If nobody had solution for this problem, is this a problem worth solving? Reviewers also encouraged to use larger (beyond Kaggle dataset) real world datasets to strengthen the paper. All the points raised by reviewers suggests that this paper can benefit from another round of nontrivial editing before it’s ready for the show. 
Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. However, there is still room for improvement; for example, convergence to a good solution needs to be further investigated. Given the  high competition at ICLR2020, this paper is unfortunately below the bar. We hope that the reviewers  comments are useful for improving the paper for potential future publication.
Main content:  Blind review #3 summarizes it well:  This paper presents results on Dictionary Learning through l4 maximization. The authors base this paper heavily off of the formulation and algorithm in Zhai et. al. (2019) "Complete dictionary learning via l4 norm maximization over the orthogonal group". The paper draws connections between complete dictionary learning, PCA, and ICA by pointing out similarities between the objectives functions that are optimized as well as the algorithms used. The paper further presents results on dictionary learning in the presence of different types of noise (AWGN, sparse corruptions, outliers) and show that the l4 objective is robust to different types of noise. Finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied.     Discussion:  Reviews agree about the interesting work, including the connections of complete dictionary learning with classic PCA and ICA (after further clarification during the rebuttal period). Additional empirical strengthening during the rebuttal period also addressed a reviewer concern.     Recommendation and justification:  As review #3 wrote, "Overall this paper makes significant contributions by extending the work in [Zhai et. al s (2019) "Complete dictionary learning via l4 norm maximization over the orthogonal group"] to noisy dictionary learning settings".
The paper has received all negative scores. Furthermore, one of the reviewers identified an anonymity violation. This is a reject.
This work presents a theory for building scale equivariant CNNs with steerable filters. The proposed method is compared with some of the related techniques . SOTA is achieved on MNIST scale dataset and gains on STL 10 is demonstrated. The reviewers had some concern related to the method, clarity, and comparison with related works. The authors have successfully addressed most of these concerns. Overall, the reviewers are positive about this work and appreciate the generality of the presented theory and its good empirical performance. All the reviewers recommend accept.
This paper proposes a generalized way to generate sequences from undirected sequence models.  Overall, I believe a framework like this could definitely be a valuable contribution, but as Reviewer 1 and Reviewer 3 noted, the paper is a bit lacking both in theoretical analysis and strong empirical results. I don t think that this is a bad paper at all, but it feels like the paper needs a little bit of an extra push to tighten up the argumentation and/or results before warranting publication at a premier venue such as ICLR. I d suggest the authors continue to improve the paper and aim to re submit at revised version at a future conference. 
The general consensus amongst the reviewers is that this paper is not quite ready for publication, and needs to dig a little deeper in some areas.  Some reviewers thought the contributions are unclear, or unsupported.  I hope these reviews will help you as you work towards finding a home for this work.
The paper presents a structured VAE, where the model parameters depend on a local structure (such as distance in feature or local space), and it uses the meta learning framework to adjust the dependency of the model parameters to the local neighborhood.  The idea is natural, as pointed by Rev#1. It incurs an extra learning cost, as noted by Rev#1 and #2, asking for details about the extra cost. The authors  reply is (last alinea in first reply to Rev#1): we did not comment (...) because in essence, using neighborhoods in a naive way is not affordable.  The area chair would like to know the actual computational time of Local VAE compared to that of the baselines.    More details (for instance visualization) about the results on Cars3D and NORB would also be needed to better appreciate the impact of the locality structure. The fact that the optimal value (wrt Disentanglement) is rather low ($10^{ 2}$) would need be discussed, and assessed w.r.t. the standard deviation.    In summary, the paper presents a good idea. More details about its impacts on the VAE quality, and its computation costs, are needed to fully appreciate its merits. 
This paper presents a method for providing uncertainty for deep learning regressors through assigning a notion of evidence to the predictions.  This is done by putting priors on the parameters of the Gaussian outputs of the model and estimating these via an empirical Bayes like optimization.  The reviewers in general found the methodology sensible although incremental in light of Sensoy et al. and Malinin & Gales but found the experiments thorough.  A comment on the paper pointed out that the approach was very similar to something presented in the thesis of Malinin (it seems unfair to expect the authors to have been aware of this, but the thesis should be cited and not just the paper which is a different contribution).  In discussion, one reviewer raised their score from weak reject to weak accept but the highest scoring reviewer explicitly was not willing to champion the paper and raise their score to accept.  Thus the recommendation here is to reject.  Taking the reviewer feedback into account, incorporating the proposed changes and adding more careful treatment of related work would make this a much stronger submission to a future conference.
The authors present a self supervised framework for learning a hierarchical policy in reinforcement learning tasks that combines a high level planner over learned latent goals with a shared low level goal completing control policy.  The reviewers had significant concerns about both problem positioning (w.r.t. existing work) and writing clarity, as well as the fact that all comparative experiments were ablations, rather than comparisons to prior work.  While the reviewers agreed that the authors reasonably resolved issues of clarity, there was not agreement that concerns about positioning w.r.t. prior work and experimental comparisons were sufficiently resolved.  Thus, I recommend to reject this paper at this time.
This paper presents an interesting method for creating adversarial examples using a GAN.  Reviewers are concerned that ImageNet Results, while successfully evading a classifier, do not appear to be natural images.  Furthermore, the attacks are demonstrated on fairly weak baseline classifiers that are known to be easily broken.  They attack Resnet50 (without adv training), for which Lp bounded attacks empirically seem to produce more convincing images.  For MNIST, they attack Wong and Kolter’s "certifiable" defense, which is empirically much weaker than an adversarially trained network, and also weaker than more recent certifiable baselines. 
The authors construct a weighted objective that subsumes many of the existing approaches for sequence prediction, such as MLE, RAML, and entropy regularized policy optimization. By dynamically tuning the weights in the objective, they show improved performance across several tasks.  Although there were no major issues with the paper, reviewers generally felt that the technical contribution is fairly incremental and the empirical improvements are limited. Given the large number of high quality submissions this year, I am recommending rejection for this submission.
The paper presents a solution to generating molecule with three dimensional structure by learning a low dimensional manifold that preserves the geometry of local atomic neighborhoods based on Euclidean distance geometry.   The application is interesting and the proposed solution is reasonable. The authors did a good job at addressing most concerns raised in the reviews and updating the draft.   Two main concerns were left unresolved: one is the lack of novelty in the proposed model, and the other is that some arguments in the paper are not fully supported. The paper could benefit from one more round of revision before being ready for publication.   
This paper received three reviews. R1 recommends Weak Reject, and identifies a variety of concerns about the motivation, presentation, clarity and soundness of results, and experimental design (e.g. choice of metrics). In a short review, R2 recommends Weak Accept, but indicates they are not an expert in this area. R3 also recommends Weak Accept, but identifies concerns also centering around clarity and completeness of the paper as well as some specific technical questions. In their response, authors address these issues, and have a constructive back and forth conversation with R1, who remains unconvinced about significance of the empirical results and thus the conclusion of the overall paper. After the discussion period, R3 indicated that they weakly favored acceptance but agreed that the paper had significant presentation issues and would not strongly advocate for it. R1 advocated for Reject, given the concerns identified in their reviews and followup comments. Given the split decision, the AC also read the paper. While the work clearly has merit, we agree with R1 s comment that it is overall a "potentially interesting idea, but the justification and presentation/quantification of results is not good enough in the submitted paper," and feel the paper really needs a revision and another round of peer review before publication. 
All reviewers agree that the authors have done a great job identifying weaknesses with the current SOTA in super resolution.   However, there is also agreement that the proposed approach may be too simple to accurately capture a range of real camera distortions, and more comparisons to the SOTA are needed.   While this paper certainly has merits and opens the door for strong work in the future, there is not enough support to accept the paper in its current form.
This paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement.  Reviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification. While many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly. AC believes that the updated version is acceptable.  Hence I recommend acceptance.
This manuscript proposes and evaluates new metrics for measuring the quality of disentangled representations for both supervised and unsupervised settings. The contributions include conceptual definitions and empirical evaluation.  In reviews and discussion, the reviewers and AC note missing or inadequate empirical evaluation with many available methods for learning disentangled representations. On the writing, reviewers mentioned that the conciseness of the manuscript could be improved. The reviewers also mentioned incomplete references and discussion of prior work, which should be improved.
This paper presents a rigorous mathematical framework for knowledge graph embedding. The paper received 3 reviews. R1 recommends Weak Reject based on concerns about the contributions of the paper; the authors, in their response, indicate that R1 may have been confused about what the contributions were meant to be. R2 initially recommended Reject, based on concerns that the paper was overselling its claims, and on the clarity and quality of writing. After the author response, R2 raised their score to Weak Reject but still felt that their main concerns had gone unanswered, and in particular that the authors seemed unwilling to tone down their claims. R3 recommends Weak Reject, indicating that they found the paper difficult to follow and gave some specific technical concerns. The authors, in their response, express confusion about R3 s comments and suggest that R3 also did not understand the paper. However, in light of these unanimous Weak Reject reviews, we cannot recommend acceptance at this time.  We understand that the authors may feel that some reviewers did not properly understand or appreciate the contribution, but all three reviewers are researchers working at highly ranked institutions and thus are fairly representative of the attendees of ICLR; we hope that their points of confusion and concern, as reflected in their reviews, will help authors to clarify a revision of the paper for another venue.   
The paper introduces the concept of overfitting in meta learning and proposes some solutions to address this problem. Overall, this is a good paper. It would be good if the authors could relate this work to meta learning approaches, which are based on hierarchical (Bayesian) modeling for learning a task embedding.  [1] Hausman et al. (ICLR 2018): Learning an Embedding Space for Transferable Robot Skills  https://openreview.net/pdf?id rk07ZXZRb [2] Saemundsson et al. (UAI 2018): Meta Reinforcement Learning with Latent Variable Gaussian Processes http://auai.org/uai2018/proceedings/papers/235.pdf 
The author propose a method called global momentum compression for sparse communication setting, and provided some theoretical results on the convergence rate. The convergence result is interesting, but the underlying assumptions used in the analysis appear very strong. Moreover, the proposed algorithm has limited novelty as it is only a minor modification. Another main concern is that the proposed algorithm shows little performance improvement in the experiments. Moreover, more related algorithms should be included in the experimental comparison.
The submission is concerned with the catastrophic forgetting problem of continual learning, and proposes a gradient based method which uses buffers of data seen previously to integrate the angles of the gradients and thereby mitigate forgetting. Empirical results are given on several benchmarks.   The reviewers were impressed with the thorough validation and strong results, but noticed that the much simpler MEGA D baseline did almost as well. Given this, they were not convinced that the proposed approach was necessary. Although the authors provided a strong rebuttal and an additional ablation, the reviewers did not feel that their concerns were met.  My recommendation is to reject the submission at this time.
The authors develop a framework for off policy value estimation for infinite horizon RL tasks, for estimating the stationary distribution of a Markov chain. Reviewers were uniformly impressed by the work, and satisfied by the author response. Congratulations!  
The paper investigates the use of the subset scanning to the detection of anomalous patterns in the input to a neural network. The paper has received mixed reviews (one positive and two negatives). The reviewers agree that the idea is interesting, has novelty, and is worth investigating. At the same time they raise issues about the clarity and the lack of comparisons with baselines. Despite a very detailed rebuttal, both of the negative reviewers still feel that addressing their concerns through paper revision would be needed for acceptance.
This work is interesting because it s aim is to push the work in intrinsic motivation towards crisp definitions, and thus reads like an algorithmic paper rather than yet another reward heuristic and system building paper. There is some nice theory here, integration with options, and clear connections to existing work.  However, the paper is not ready for publication. There were were several issues that could not be resolved in the reviewers minds (even after the author response and extensive discussion). The primary issues were: (1) There was significant confusion around the beta sensitivity figs 6,7,8 appear misleading or at least contradictory to the message of the paper. (2) The need for x,y env states. (3) The several reviewers found the decision states unintuitive and confused the quantitative analysis focus if they given the authors primary focus is transfer performance. (4) All reviewers found the experiments lacking. Overall, the results generally don t support the claims of the paper, and there are too many missing details and odd empirical choices.    Again, there was extensive discussion because all agreed this is an interesting line of work. Taking the reviewers excellent suggestions on board will almost certainly result in an excellent paper. Keep going!
This was a difficult paper to decide, given the strong disagreement between reviewer assessments.  After the discussion it became clear that the paper tackles some well studied issues while neglecting to cite some relevant works.  The significance and novelty of the contribution was directly challenged, yet I could not see a convincing case presented to mitigate these criticisms.  The paper needs to do a better job of placing the work in the context of the existing literature, and establishing the significance and novelty of its main contributions.
The submission has two issues, identified by the reviewers; (1) the description of the proposed method was found to be confusing at times and could be improved, and (2) the proposed transitional skills were not well motivated/justified as a solution to the problem the authors propose to solve.
This paper studies the empirical performance of invertible generative models for compressive sensing, denoising and in painting. One issue in using generative models in this area has been that they hit an error floor in reconstruction due to model collapse etc i.e. one can not achieve zero error in reconstruction. The reviewers raised some concerns about novelty of the approach and thoroughness of the empirical studies. The authors response suggests that they are not claiming novelty w.r.t. to the approach but rather their use in compressive techniques. My own understanding is that this error floor is a major problem and removing its effect is a good contribution even without any novelty in the techniques. However,  I do agree that a more thorough empirical study would be more convincing. While I can not recommend acceptance given the scores I do think this paper has potential and recommend the authors to resubmit to a future venue after a through revision.
This paper tackles the problem of how to adapt a model from a source to a target domain when both data is not available simultaneously (even unlabeled) to a single learner. This is of relevance for certain privacy preserving applications where one setting would like to benefit from information learned in a related setting but due to various factors may not be willing to directly share data. The proposed solution is a transfer alignment network (TAN) which consists of two autoencoders (each trained independently on the source and the target) and an aligner which has the task of mapping the latent codes of one domain to the other.   All three reviewers expressed concerns for this submission. Of greatest concern was the experimental setting. The datasets chosen were non standard and there was no prior work to compare against directly so the results presented are difficult to contextualize. The authors have responded to this concern by specifying the existing domain adaptation benchmarks are more challenging and require more complex architectures to handle the “more complex data manifolds”. The fact that existing benchmark datasets may be more complex the the dataset explored in this work is a concern. The authors should take care to clarify whether their proposed solution may only be applicable to specific types of data. In addition, the authors claim to address a new problem setting and therefore cannot compare directly to existing work. One suggestion is if using new data, report performance of existing work under the standard setting to give readers some grounding for the privacy preserving setting. Another option would be to provide scaffold results in the standard UDA setting but with frozen feature spaces. Another option would be to ablate the choice of L2 loss for learning the transformer and instead train using an adversarial loss, L1 loss etc. There are many ways the authors could both explore a new problem statement and provide convincing experimental evidence for their solution. The AC encourages the authors to revise their manuscript, paying special attention to clarity and experimental details in order to further justify their proposed work. 
This work introduces a new neural network model that can represent hyperedges of variable size, which is experimentally shown to improve or match the state of the art on several problems.   Both reviewers were in favor of acceptance given the method s strong performance, and had their concerns resolved by the rebuttals and the discussion. I am therefore recommending acceptance. 
The authors present a method to address off policy policy evaluation in the infinite horizon case, when the available data comes from multiple unknown behavior policies.  Their solution   the estimated mixture policy   combines recent ideas from both infinite horizon OPE and regression importance sampling, a recent importance sampling based method.  At first, the reviewers were concerned about writing clarity, feasibility in the continuous case, and comparisons to contemporary methods like DualDICE.  After the rebuttal period, the reviewers agreed that all the major issues had been addressed through clarifications, rewriting, code release, and additional empirical comparisons.  Thus, I recommend to accept this paper.
This paper proposes to use PCS to replace the conventional decoder for 3D shape reconstruction. It shows competitive performance to the state of the art methods. While reviewer #3 is overall positive about this work, both reviewer #1 and #2 rated weak rejection. Reviewer #1 concerns that important details are missing, and the discussion of results is insufficient. Reviewer #3 has questions on the clarity of the presentation and comparison with SOTA methods. The authors provided response to the questions, but did not change the rating of the reviewers. The ACs agree that this work has merits. However, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.
This paper proposes to overcome some fundamental limitations of normalizing flows by introducing auxiliary continuous latent variables. While the problem this paper is trying to address is mathematically legitimate, there is no strong evidence that this is a relevant problem in practice. Moreover, the proposed solution is not entirely novel, converting the flow in a latent variable model. Overall, I believe this paper will be of minor relevance to the ICLR community.
This paper proposes a new black box adversarial attack approach which learns a low dimensional embedding using a pretrained model and then performs efficient search in the embedding space to attack target networks. The proposed approach can produce perturbation with semantic patterns that are easily transferable and improve the query efficiency in black box attacks. All reviewers are in support of the paper after author response. I am very happy to recommend accept. 
This works presents a new and interesting notion of margin for deep neural networks (that incorporates representation at all layers). It then develops generalization bounds based on the introduced margin. The reviewers pointed some concerns, including some notation issues, complexity in case of residual networks, removal of exponential dependence on depth,  and dependence on a hard to compute quantity   \kapp^{adv}. Some of these concerns were addressed by the authors. At the end, most of the reviewers find the notion of all layer margin introduced in this paper a very novel and promising idea for characterizing generalization in deep networks. Agreeing with reviewers, I recommend accept. However, I request the authors to accommodate remaining comments /concerns raised by R1 in the final version of your paper. In particular, in your response to R1 you mentioned for one case you saw improvement even with dropout, but that is not mentioned in the revision; Please include related details in the draft. 
This paper analyzes and extends learning methods based on Policy Spaced Response Oracles (PSRO) through the application of alpha rank.  In doing so, the paper explores connections with Nash equilibria, establishes convergence guarantees in multiple settings, and presents promising empirical results on (among other things) 3 to 5 player poker games.  Although this paper originally received mixed scores, after the rebuttal period all reviewers converged to a consensus. A revised version also includes new experiments from the MuJoCo soccer domain, and new poker results as well.  Overall, this paper provides a nice balance of theoretical support and practical relevance that should be of high impact to the RL community. 
This submission presents bounds on the training dynamics (including gradient evolution) for deep linear (and in some cases nonlinear) networks as a function of the width of the layers or number of convolutional layers. The work also presents experimental results that provide evidence that the bounds are tight.  Strengths: The work provides interesting insights into these training dynamics, particularly for the wide but not infinite setting, which is less studied. The work also adapts cluster graphs and Feynman diagrams to derive these bounds, which could be useful tools for researchers in this field.  Weaknesses: The validity and applicability of some of the results for nonlinear networks was not entirely clear at first but has been clarified in the revision.  The reviewer consensus was to accept this submission. 
The paper proposes a new recurrent unit which incorporates long history states to learn longer range dependencies for improved video prediction. This history term corresponds to a linear combination of previous hidden states selected through a soft attention mechanism and can be directly added to ConvLSTM equations that compute the IFO gates and the new state. The authors perform empirical validation on the challenging KTH and BAIR Push datasets and show that their architecture outperforms existing work in terms of SSIM, PSNR, and VIF. The main issue raised by the reviewers is the incremental nature of the work and issues in the empirical evaluation which do not support the main claims in the paper. After the rebuttal and discussion phase the reviewers agree that these issues were not adequately resolved and the work doesn’t meet the acceptance bar. I will hence recommend the rejection of this paper. Nevertheless, we encourage the authors improve the manuscript by addressing the remaining issues in the empirical evaluation.
This paper proposes an analysis of regularization for policy optimization. While the multiple effects of regularization are well known in the statistics and optimization community, it is less the case in the RL community. This makes the novelty of the paper difficult to judge as it depends on the familiarity of RL researchers with the two aforementioned communities.  Besides the novelty aspect, which is debatable, reviewers had doubts on the significance of the results, and in particular on the metrics chosen (based on the rank). While defining a "best" algorithm is notoriously difficult, and could be considered outside of the scope of this paper, the fact is that the conclusions reached are still sensitive to that difficulty.  I thus regret to reject this paper as I feel not much more work is necessary to provide a compelling story. I encourage the authors to extend their choice of metrics to be more convincing in their conclusions.
This paper aims to study the effect of curvature correction techniques on training dynamics. The focus is on understanding how natural gradient based methods affect training dynamics of deep linear networks. The main conclusion of the analysis is that it does not fundamentally affect the path of convergence but rather accelerates convergence. They also show that layer correction techniques alone do not suffice. In the discussion the reviewers raised concerns about extrapolating too much based on linear networks and also lack of a cohesive literature review. One reviewer also mentioned that there is not enough technical detail. These issues were partially addressed in the response. I think the topic of the paper is interesting and timely. However, I concur with Reviewer #2 that there are still lots of missing detail and the connection with the nonlinear case is not clear (however the latter is not strictly necessary in my opinion if the rest of the paper is better written). As a result I think the paper in its current form is not ready for publication. 
The authors present a system agnostic interpretable method based on the idea of that provides a brief ( compressed) but comprehensive ( informative) explanation. Their system is build upon the idea of VIB. The authors compare against 3 state of the art interpretable machine learning methods and the evaluation is terms of interpretability ( human understandable) and fidelity ( accuracy of approximating black box model). Overall, all reviewers agreed that the topic of model interpretability is an important one and the novel connection between IB and interpretable data summaries is a very natural one.    This manuscript has generated a lot of discussion among the reviewers during the rebuttal and there are a number of concerns that are currently preventing me from recommending this paper for acceptance. The first concern relates to the lack of comparison against attention methods (I agree with the authors that this is a model specific solution whereas they propose a model agnostic one), however attention is currently the elephant in room and the first thing someone thinks of when thinking of interpretability. As such, the authors should have presented such a comparison. The second concern relates to the human evaluation protocol which could be significantly improved  (Why 100 samples from all models but 200 for VIBI? Given the small set of results, are these model differences significant? Similarly, assuming that we have multiple annotations per sample, what is the variance in the annotations?).  This paper is currently borderline and given reviewers  concerns and the limited space in the conference program I cannot recommend acceptance of this paper. 
This paper proposes the use of gradient of the loss evaluated at the example with respect to the model parameters as the feature representation of that example. The authors performed an empirical analysis on anomaly detection benchmarks to demonstrate the practical benefits of the proposed method. While the reviewers find the idea interesting, the consensus is that the proposed method lacks justification, and that the main claims were not substantiated. While the reviewers proposed several key points of improvement, the raised issues were not addressed in the rebuttal. I will hence recommend rejection of this paper. 
The authors propose a way to recover latent factors implicitly constructed by a neural net with black box access to the nets output. This can be useful for identifying possible adversarial attacks. The majority of reviewers agrees that this is a solid technical and experimental contribution.
There is no author response for this paper. The paper addresses the affective analysis of video sequences in terms of continual emotions of valence and arousal. The authors propose a multi modal approach (combining modalities such as audio, pose estimation, basic emotions and scene analysis) and a multi scale temporal feature extractor (to capture short and long temporal context via LSTMs) to tackle the problem. All the reviewers and AC agreed that the paper lacks (1) novelty, as the proposed approach is a combination of the existing well studied techniques without explanations why and when this could be advantageous beyond the considered task, (2) clarity and motivation   see R2’s and R3’s concerns and suggestions on how to improve. We hope the reviews are useful for improving the paper. 
The main concern raised by reviewers is the limited experiments, which are on simple tasks and missing some baselines to state of the art methods. While the overall approach is interesting, the reviewers found the empirical evidence to be fairly unconvincing. 
The authors propose a simple but effective method for feature crossing using interpretation inconsistency (as defined by the authors).  I think this is a good work and the authors as well as the reviewers participated well in the discussions. However, there is still disagreement about the positioning of the paper. In particular, all the reviewers  felt that additional baselines should be tried. While the authors have strongly rebutted the necessity of these baselines the reviewers are not convinced about it. Given the strong reservations of the all the 3 reviewers at this point I cannot recommend the acceptance of this paper. I strongly suggest that in subsequent submissions the authors should position their work better and perhaps compare with some of the related works recommended by the reviewers.
This submission proposes a method for providing visual explanations for why two images match by highlighting image regions that most contribute to similarity.  Reviewers agreed that the problem is interesting but were divided on the degree of novelty of the proposed approach.  AC shares R1’s concern that localization accuracy is not satisfactory as a quantitative measure of the quality of the explanations. In particular, it pre supposes what the explanations ought to be, i.e. that a good explanation means good localization. A small user study would be more convincing. A more convincing evaluation would also include a study of explanation of image pairs with different degrees of similarity (e.g. images that are dissimilar as well as images with the same object).  AC also shares R2’s concern about the validity of the model diagnosis application. This discussion also relies on the assumption that better localization of the whole object means a better explanation. Further, the highlighted regions in Figure 5 are very similar. Once again, a user study would help to indicate whether these results really do improve explainability.  Reviewers also had concerns about missing details and, while the authors did improve this, key details are still missing. For example, the localization method that was used was only referenced but should be described in the paper itself.  Given that several concerns remain, AC recommends rejection. 
This paper presents theoretical results showing the conditional generative models cannot be robust. The paper also provide counter examples and some empirical evidence showing that the theory is reflected in practice. Some reviewers doubt how much of the theory holds in reality, but still they think that this paper could be a useful for the community. After the rebuttal period, R2 increased their score and it seems that with the current score the paper can be accepted.
This paper aims to theoretically understand the the benefit of attention mechanisms. The reviewers agreed that better understanding of attention mechanisms is an important direction. However, the paper studies a weaker form of attention which does not correspond well to the attention models using in the literature. The paper should better motivate why the theoretical results for this restrained model would carry over to more realistic mechanisms.
This paper considers an interesting theoretical question. However, it would add to the strength of the paper if it was able to meaningfully connect the considered model as well as derived methodology to the challenges and performance that arise in practice. 
This paper deals with multi agent hierarchical reinforcement learning. A discrete set of pre specified low level skills are modulated by a conditioning vector and trained in a fashion reminiscent of Diversity Is All You Need, and then combined via a meta policy which coordinates multiple agents in pursuit of a goal. The idea is that fine control over primitive skills is beneficial for achieving coordinated high level behaviour.  The paper improved considerably in its completeness and in the addition of baselines, notably DIAYN without discrete, mutually exclusive skills. Reviewers agreed that the problem is interesting and the method, despite involving a degree of hand crafting, showed promise for informing future directions.   On the basis that this work addresses an interesting problem setting with a compelling set of experiments, I recommend acceptance.
The authors provide bounds on the expected generalization error for noisy gradient methods (such as SGLD). They do so using the information theoretic framework initiated by Russo and Zou, where the expected generalization error is controlled by the mutual information between the weights and the training data. The work builds on the approach pioneered by Pensia, Jog, and Loh, who proposed to bound the mutual information for noisy gradient methods in a step wise fashion.  The main innovation of this work is that they do not implicitly condition on the minibatch sequence when bounding the mutual information. Instead, this uncertainty manifests as a mixture of gaussians. Essentially they avoid the looseness implied by an application of Jensen s inequality that they have shown was unnecessary.  I think this is an interesting contribution and worth publishing. It contributes to a rapidly progressing literature on generalization bounds for SGLD that are becoming increasingly tight.  I have one strong request that I will make of the authors, and I ll be quite disappointed if it is not executed faithfully.  1. The stepsize constraint and its violation in the experimental work is currently buried in the appendix. This fact must be brought into the main paper and made transparent to readers, otherwise it will pervert empirical comparisons and mask progress.  2. In fact, I would like the authors to re run their experiments in a way that guarantees that the bounds are applicable. One approach is outline by the authors: the Lipschitz constant can be replaced by a max_i bound on the running squared gradient norms, and then gradient clipping can be used to guarantee that the step size constraint is met.  The authors might compare step sizes, allowing them to use less severe gradient clipping. The point of this exercise is to verify that the learning dynamics don t change when the bound conditions are met. If they change, it may upset the empirical phenomena they are trying to study. If this change does upset the empirical findings, then the authors should present both, and clearly explain that the bound is not strictly speaking known to be valid in one of the cases. It will be a good open problem.     
This paper proposes the unknown aware deep neural network (UDN), which can discover out of distribution samples for CNN classifiers. Experiments show that the proposed method has an improved rejection accuracy while maintaining a good classification accuracy on the test set. Three reviewers have split reviews. Reviewer #2 provides positive review for this work, while indicating that he is not an expert in image classification. Reviewer #1 agrees that the topic is interesting, yet the experiment is not so convincing, especially with limited and simple databases. Reviewer #3 shared the similar concern that the experiments are not sufficient. Further, R3 felt that the main idea is not well explained. The ACs concur these major concerns and agree that the paper can not be accepted at its current state.
This paper focuses on studying the double descent phenomenon in a one layer neural network training in an asymptotic regime where various dimensions go to infinity together with fixed ratios. The authors provide precise asymptotic characterization of the risk and use it to study various phenomena. In particular they characterize the role of various scales of the initialization and their effects. The reviewers all agree that this is an interesting paper with nice contributions. I concur with this assessment.  I think this is a solid paper with very precise and concise theory. I recommend acceptance.
The paper proposes a "compressive transformer", an extension of the transformer, that keeps a compressed long term memory in addition to the fixed sized memory.  Both memories can be queried using attention weights.  Unlike TransfomerXL that discards the oldest memories, the authors propose to "compress" those memories.  The main contribution of this work is that that it introduces a model that can handle extremely long sequences. The authors also introduces a new language modeling dataset based on text from Project Gutenberg that has much longer sequences of words than existing datasets.  They provide comprehensive experiments comparing against different compression strategies and compares against previous methods, showing that this method is able to result in lower word level perplexity. In addition, the authors also present evaluations on speech, and image sequences for RL.  Initially the paper received weak positive responses from the reviewers. The reviewers pointed out some clarity issues with details of the method and figures and some questions about design decisions. After rebuttal, all of the reviewers expressed that they were very satisfied with the authors responses and increased their scores (for a final of 2 accepts and 1 weak accept).  The authors have provided a thorough and well written paper, with comprehensive and convincing experiments. In addition, the ability to model long range sequences and dependencies is an important problem and the AC agrees that this paper makes a solid contribution in tackling that problem.  Thus, acceptance is recommended.
This paper proposes a network quantization method which is based on kernel level quantization. The extension from layer level to kernel level is straightforward, and so the novelty is somewhat limited given its similarity with HAQ. Nevertheless, experimental results demonstrate its efficiency in real applications. The paper can be improved by clarifying some experimental details, and have further discussions on its relationship with HAQ.
Main content: Paper proposes a fast network adaptation (FNA) method, which takes a pre trained image classification network, and produces a network for the task of object detection/semantic segmentation  Summary of discussion: reviewer1: interesting paper with good results, specifically without the need to do pre training on Imagenet. Cons are better comparisons to existing methods and run on more datasets.  reviewer2:  interesting idea on adapting source network network via parameter re mapping that offers good results in both performance and training time. reviewer3: novel method overall, though some concerns on the concrete parameter remapping scheme. Results are impressive Recommendation: Interesting idea and good results. Paper could be improved with better comparison to existing techniques. Overall recommend weak accept.
The paper is develops a self training framework for graph convolutional networks where we have partially labeled graphs with a limited amount of labeled nodes. The reviewers found the paper interesting. One reviewer notes the ability to better exploit available information and raised questions of computational costs. Another reviewer felt the difference from previous work was limited, but that the good results speak for themselves. The final reviewer raised concerns on novelty and limited improvement in results. The authors provided detailed responses to these queries, providing additional results.  The paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time.
This work studies parameter quantization using binary codes and proposes an encryption algorithm/architecture to compress quantized weights and achieve fractional numbers of bits per weight, and to perform decryption using XOR gates. The authors conduct experiments on datasets including ImageNet to evaluate their scheme. Much of the concern from reviewers relates to baseline comparison and details around that. Specifically, R1 believes that the submission could have a bigger impact if authors could conduct more thorough experiments, e.g. compressing more widely used and challenging architecture of ResNet 50, or trying tasks such as image detection (Mask R CNN). The authors  responded to that and mentioned their choice of the current experimental setting is to facilitate comparison with previous works (baselines), which use similar experimental settings. Nevertheless, the baseline methods could have been attempted by the authors on broader tasks, or more widely used architectures could have been investigated by authors on the baseline methods. As a result, R1 was not convinced. To ensure the paper receives the attention it deserves, I recommend considering a more thorough evaluation of the proposed method against baseline methods.
This paper studies the use of a graph neural network for drug to drug interaction (DDI) prediction task (an instance of a link prediction task with drugs as vertices and interaction as edges). In particular, the authors apply structured prediction energy networks (SPEN) and model the dependency structure of the labels by minimising an energy function. The authors empirically validate the proposed approach against feedforward GNNs on two DDI prediction tasks. The reviewers feel that understanding drug drug interactions is an important task and that the work is well motivated. However, the reviewers argued that the proposed methodology is not novel enough to merit publication at ICLR and that some conclusions are not supported by the empirical analysis. For the former, the benefits of the semi supervised design need to be clearly and concisely presented. For the latter, providing a more convincing practical benefit would greatly improve the manuscript. As such, I will recommend the rejection of this paper at the current state.
The paper proposes an adaptive sampling mechanism for zeroth order optimization that samples perturbed points from a mixture distribution with asymptotic convergence guarantees. The reviewers raised issues regarding the clarity of presentation, potential problems with the proofs, and simplicity of the experimental setup. The authors did not provide a response. Overall, the reviewers agree that the quality of the paper is not sufficient for publishing, and therefore I recommend rejection.
The paper proposes a method to handle Mahalanobis metric learning thorough linear programming.  All reviewers were unclear on what novelty of the approach is compared to existing work.  I recommend rejection at this time, but encourage the authors to incorporate reviewers  feedback (in particular placing the work in better context and clarifying the motivations) and resubmitting elsewhere.  
This paper considers the question of how to quantize deep neural networks, for processors operating on low precision integers.  The authors propose a methodology and have evaluated it thoroughly. The reviewers all agree that this question is important in practice, though there was disagreement about how novel a contribution this paper is specifically, and on its clarity. The clarity questions were resolved on rebuttal, so I lean to accepting the paper.
This paper presents and evaluates a technique for unsupervised object part discovery in 3d   i.e. grouping points of a point cloud into coherent parts for an object that has not been seen before. The paper received 3 reviews from experts working in this area. R1 recommended Weak Accept, and identified some specific technical questions for the authors to address in the response (which the authors provided and R1 seemed satisfied). R2 recommends Weak Reject, and indicates an overall positive view of the paper but felt the experimental results were somewhat weak and posed several specific questions to the reviewers. The authors  response convincingly addressed these questions. R3 recommends Accept, but suggests some additional qualitative examples and ablation studies. The author response again addresses these. Overall, the reviews indicate that this is a good paper with some specific questions and concerns that can be addressed; the AC thus recommends a (Weak) Accept based on the reviews and author responses.
The paper proposes a hierarchical diversity promoting regularizer for neural networks. Experiments are shown with this regularizer applied to the last fully connected layer of the network, in addition to L2 and energy regularizers on other layers. Reviewers found the paper well motivated but had concerns on writing/readability of the paper and that it provides only marginal improvements over existing simple regularizers such as L2. I would encourage the authors to look for scenarios where the proposed regularizer can show clear improvements and resubmit to a future venue. 
This paper is good, with relatively positive support from the reviewers. However, there were also several legitimate issues raised, for example regarding the semantics of a negative answer and associated explanations. Though this paper cannot be accepted at this time, we hope the feedback here can help improve a future version, as all reviewers agree this is a valuable line of work.
The authors propose a framework for improving the robustness of neural networks to adversarial perturbations via optimal control techniques (Lyapunov Stability and the Pontryagin Maximum Principle, in particular). By considering a continuous time limit of the training process, the authors use the PMP to derive udpate rules for the neural network weights that would result in a robust network. While the approach is interesting, the paper has some serious deficiencies that make it unacceptable for publication in its current form:  1. Quality of empirical evaluation: The authors only report final numbers on CIFAR 10 for a fixed set of adversarial attacks. It has been observed repeatedly in the adversarial robustness literature that adversarial evaluation of neural networks has to be done carefully to guard against possible underestimation of the worst case attack. In particular, the specific details of the adversarial attacks used (number of steps, number of initializations, performance under larger perturbation radii) that are necessary to trust the results are not given (see https://arxiv.org/pdf/1902.06705.pdf for example).  2. Unclear novelty: The authors do not sufficiently explain the novelty in their approach relative to prior work (particular prior work that has used optimal control ideas in this context).  3. Computational cost: The authors do not give sufficient details to judge the computational overhead of their method to judge how much more expensive it is to train with their approach relative to standard or adversarial training.  While one reviewer voted for a weak accept, the other reviewers were in consensus on rejection. The authors did not respond during the rebuttal phase and hence the reviews were unchanged.  In summary, I vote for rejection. However, I think this paper has potentially interesting ideas that should be carefully developed and evaluated in a future revision.
The work considers sparse and short blind deconvolution problem, which is to inverse a convolution of a sparse source (such as spikes at cell locations in microscopy) with a short (of limited spatial size) kernel or point spread function, not known in advance. This is posed as a bilinear lasso optimization problem. The work applies a non linear optimization method with some practical improvements (such as data driven initialization, momentum, homotopy continuation).  The paper extends the work by Kuo et al. (2019) by providing a practical algorithm for solving those inverse problems. A focus of the paper is to solve the bilinear lasso instead of the approximate bilinear lasso, because this approximation is poor for coherent problems. Having read the rebuttal and the paper, I believe the authors addressed the issues raised by Reviewer #2 in a sufficient way.  small things:   it would be good to define $\iota$ (zero padding operator) in (1)   it would be good to define $p, p_0$ just below (3). They seem to be appearing out of the blue without any direct relation to anything mentioned prior in section 2.   it would be good to cite some older/historic references for various optimization methods , e.g. [1] below.    [1] Richter & deCarlo  Continuation methods: Theory and applications IEEE Transactions on Systems, Man, and Cybernetics, 1983 https://ieeexplore.ieee.org/abstract/document/6313131
The paper discusses smooth market games and demonstrate the merit of the approach.    The reviewers agree on the quality of the paper, and the comments have been addressed well by the authors. 
This paper studies over parameterization for unsupervised learning. The paper does a series of empirical studies on this topic. Among other things the authors observe that larger models can increase the number latent variables recovered when fitting larger variational inference models. The reviewers raised some concern about the simplicity of the models studied and also lack of some theoretical justification. One reviewer also suggests that more experiments and ablation studies on more general models will further help clarify the role over parameterized model for latent generative models. I agree with the reviewers that this paper is "compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods". I disagree with the reviewers that theoretical study is required as I think a good empirical paper with clear conjectures is as important. I do agree with the reviewers however that for empirical paper I think the empirical studies would have to be a bit more thorough with more clear conjectures. In summary, I think the paper is nice and raises a lot of interesting questions but can be improved with more through studies and conjectures. I would have liked to have the paper accepted but based on the reviewer scores and other papers in my batch I can not recommend acceptance at this time. I strongly recommend the authors to revise and resubmit. I really think this is a nice paper and has a lot of potential and can have impact with appropriate revision.
This paper presents a language model for Amharic using HMMs and incorporating POS tags. The paper is very short and lacks essential parts such as describing the exact model and the experimental design and results. The reviewers all rejected this paper, and there was no author rebuttal. This paper is clearly not appropriate for publication at ICLR. 
This paper introduces a probabilistic data subsampling scheme that can be optimized end to end.  The experimental evaluation is a bit weak, focusing mostly on toy scale problems, and I would have liked to see a discussion of bias in the Gumbel max gradient estimator.    It s also not clear how the free hyperparameters for this method were chosen, which makes me suspect they were tuned on the test set.  However, the overall idea is sensible, and the area seems under explored.
This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. However, the main concern is that the paper is very similar to a recent work by the authors, which is not cited.
This work examines how internal consistency objectives can help emergent communication, namely through possibly improving ability to refer to unseen referents and to generalize across communicative roles. Experimental results support the second hypothesis but not the first. Reviewers agree that this is an exciting object of study, but had reservations about the rationale for the first hypothesis (which was ultimately disproven), and for how the second hypothesis was investigated (lack of ablations to tease apart which part was most responsible for improvement, unsatisfactory framing). These concerns were not fully addressed by the response. While the paper is very promising and the direction quite interesting, this cannot in its current form be recommended for acceptance. We encourage authors to carefully examine reviewers  suggestions to improve their work for submission to another venue.
The paper proposes a solution based on self attention RNN to addressing the missing value in spatiotemporal data.   I myself read through the paper, followed by a discussion with the reviewers. We agree that the model is reasonable, and the results are promising. However, there is still some room for improvement: 1. The self attention mechanism is not new. The specific way proposed in the paper is an interesting tweak of existing models, but not brand new per se. Most importantly, it is unclear if the proposed way is the optimal one and where the performance improvement comes from. As the reviewer suggested, more thorough empirical analysis should be performed for deeper insights of the model.   2. The datasets were adopted from existing work, but most of them do not have such complex models as the one proposed in the paper. Therefore, the suggestion for bigger datasets is valid.   Given the considerations above, we agree that while the paper has a lot of good materials, the current version is not ready yet. Addressing the issues above could lead to a good publication in the future. 
This paper proposes a method for neural architecture search in embedding space. This is an interesting idea, but its novelty is limited due to its similarity to the NAO approach. Also, the empirical evaluation is too limited; comparisons should have been performed to NAO and other contemporary NAS methods, such as DARTS.   Due the factors above, all reviewers gave rejecting scores (3,3,1). The rebuttal did not remove the main issues, resulting in the reviewers sticking to their scores. I therefore recommend rejection.
Thanks for the submission. This paper leverages the stability of differential privacy for the problems of anomaly and backdoor attack detection. The reviewers agree that this application of differential privacy is novel. The theory of the paper appears to be a bit weak (with very strong assumptions on the private learner), although it reflects the basic underlying idea of the detection technique. The paper also provides some empirical evaluation of the technique.
This paper proposes a method to do zero shot ICD coding, which involves assigning natural language labels (ICD codes) to input text. This is an important practical problem in healthcare, and it is not straightforward to solve, because many ICD codes have none or very few training examples due to the long distribution tail. The authors adapt a GAN based technique previously used in vision to solve this problem. All of the reviewers agree that the paper is well written and well executed, and that the results are good. However, the reviewers have expressed concerns about the novelty of the GAN adaptation step, and left this paper very much borderline based on the scores it received. Due to the capacity restrictions I therefore have to recommend rejection, however I hope that the  authors resubmit elsewhere. 
This paper explores the idea of using meta learning for acquisition functions. It is an interesting and novel research direction with promising results.   The paper could be strengthened by adding more insights about the new acquisition function and performing more comparisons e.g. to Chen et al. 2017. But in any case, the current form of the paper should already be of high interest to the community 
The paper makes its contribution by deriving an accelerated gradient flow for the Wasserstein distances. It is technically strong and demonstrates it applicability using examples fo Gaussian distributions and logistic regression.  Reviewer 3 provided a deep technical assessment, pointing out the relevance to our ML community since these ideas are not yet widespread, but had concerns about the clarity of the paper. Reviewer 2 had similar concerns about clarity, and was also positive about its relevance to the ML community. The authors provided details responses to the technical questions posed by the reviewers. The AC believes that such work is a good fit for the conference. The reviewers felts that this paper does not yet achieve the aim of making this work more widespread and needs more focus on communication.  This is a strong paper and the authors are encouraged to address the accessibility questions. We hope the review offers useful points of feedback for their future work.
This paper introduces a new RNN architecture which uses a small network to decide which cells get updated at each time step, with the goal of reducing computational cost.  The idea makes sense, although it requires the use of a heuristic gradient estimator because of the non differentiability of the update gate.  The main problem with this paper in my view is that the reduction in FLOPS was not demonstrated to correspond to a reduction in wallclock time, and I don t expect it would, since the sparse updates are different for each example in each batch, and only affect one hidden unit at a time.  The only discussion of this problem is "we compute the FLOPs for each method as a surrogate for wall clock time, which is hardware dependent and often fluctuates dramatically in practice."  Because this method reduces predictive accuracy, the reduction in FLOPS should be worth it!  Minor criticism: 1) Figure 1 is confusing, showing not the proposed architecture in general but instead the connections remaining after computing the sparse updates. 
This paper proposes a response generation approach that aims to tackle the generic response problem. The approach is learning a latent semantic space by maximizing the correlation between features extracted from prompts and responses. The reviewers were concerned about the lack of comparison with previous papers tackling the same problem, and did not change their decision (i.e., were not convinced) even after the rebuttal. Hence, I suggest a reject for this paper.
This paper studies the properties of adversarial training in the large scale setting. The reviewers found the properties identified by the paper to be of interest to the ICLR community   in particular the robustness community. We encourage the authors to release their models to help jumpstart future work building on this study.
This paper proposes an out of distribution detection (OOD) method without assuming OOD in validation.  As reviewers mentioned, I think the idea is interesting and the proposed method has potential. However, I think the paper can be much improved and is not ready to publish due to the followings given reviewers  comments:  (a) The prior work also has some experiments without OOD in validation, i.e., use adversarial examples (AE) instead in validation. Hence, the main motivation of this paper becomes weak unless the authors justify enough why AE is dangerous to use in validation.   (b) The performance of their replication of the prior method is far lower than reported. I understand that sometimes it is not easy to reproduce the prior results. In this case, one can put the numbers in the original paper. Or, one can provide detailed analysis why the prior method should fail in some cases.  (c) The authors follow exactly same experimental settings in the prior works. But, the reported score of the prior method is already very high in the settings, and the gain can be marginal. Namely, the considered settings are more or less "easy problems". Hence, additional harder interesting OOD settings, e.g., motivated by autonomous driving, would strength the paper.  Hence, I recommend rejection.
This paper proposes using first order logic to rule out superficial information for improved natural language inference. While the topic is of interest, reviewers find that the paper misses much of the previous literature on semantics which is highly relevant.   I thank the authors for submitting this paper to ICLR. Please take the reviewers  comments, especially recommended references, to improve the paper for future submission.
This submission has been assessed by three reviewers and scored 3/6/1. The reviewers also have not increased their scores after the rebuttal. Two reviewers pointed to poor experimental results that do not fully support what is claimed in contributions and conclusions. Theoretical support for the reconstruction criterion was considered weak. Finally, the paer is pointed to be a special case of (Zhang 2019). While the paper has some merits, all reviewers had a large number of unresolved criticism. Thus, this paper cannot be accepted by ICLR2020. 
This paper proposes to mitigate mode collapse in GANs by encouraging distribution matching in the latent space. Reviewers 1 and 3 expressed concerns that the methodology is too incremental in the context of the existing literature (VEEGAN, VAE GAN, AAE). This, combined with the lack of up to date baselines, makes it difficult to access the significance of the proposed modifications. The quality and precision of the writing can also be improved to meet the standards of publication at a top tier conference.  
Reviewers found the problem statement having merit, but found the solution not completely justifiable. Bandit algorithms often come with theoretical justification because the feedback is such that the algorithm could be performing horribly without giving any indication of performance loss. With neural networks this is obviously challenging given the lack of supervised learning guarantees, but reviewers remain skeptical and prefer not to speculate based on empirical results. 
The paper proposes a way to use kernel method for multi view generation. The points are mapped into a common subspace (with CNN feature extractor and kernel on top), and then a generation procedure from a latent point is given.  I found the paper not easy to ready and follow; the idea of using CNN + kernel methods have been around for some years (for example, see "Impostor networks" by Lebedev et. al), and explicit feature map shows that kernel is just an additional layer to the network. Overall, the approach is straightforward, the generation can be quite slow and the benefits are not clear. The reviewers are mildly negative, so I think this time this paper can not be accepted.
The authors propose a novel algorithm for batch RL with offline data. The method is simple and outperforms a recently proposed algorithm, BCQ, on Mujoco benchmark tasks.  The main points that have not been addressed after the author rebuttal are: * Lack of rigor and incorrectness of theoretical statements. Furthermore, there is little analysis of the method beyond the performance results. * Non standard assumptions/choices in the algorithm without justification (e.g., concatenating episodes). * Numerous sloppy statements / assumptions that are not justified. * No comparison to BEAR, making it challenging to evaluate their state of the art claims. The reviewers also point out several limitations of the proposed method. Adding a brief discussion of these limitations would strengthen the paper.  The method is interesting and simple, so I believe that the paper has the potential to be a strong submission if the authors incorporate the reviewers suggestions in a future submission. However, at this time, the paper falls below the acceptance bar.
The authors demonstrate that starting from the 3rd epoch, freezing a large fraction of the weights (based on gradient information), but not entire layers, results in slight drops in performance.  Given existing literature, the reviewers did not find this surprising, even though freezing only some of a layers weights has not been explicitly analyzed before. Although this is an interesting observation, the authors did not explain why this finding is important and it is unclear what the impact of such a finding will be. The authors are encouraged to expand on the implications of their finding and theoretical basis for it. Furthermore, reviewers raised concerns about the extensiveness of the empirical evaluation.  This paper falls below the bar for ICLR, so I recommend rejection.
The paper proposes a tree search based policy optimization methods for continuous action state spaces. The paper does not have a theoretical guarantee, but has empirical results.  Reviewers brought up issues such as lack of using other policy optimizations methods (SAC, RERPI, etc.), sample inefficiency, and unclear difference with some other similar papers. Even though the authors have provided a rebuttal to address these issues, all the reviewers remain negative. So I can only recommend rejection at this stage.
This paper analyzes a mechanism of the implicit regularization caused by nonlinearity of ReLU activation, and suggests that the learned DNNs interpolate almost linearly between data points, which leads to the low complexity solutions in the over parameterized regime. The main objections include (1) some claims in this paper are not appropriate; (2) lack of proper comparison with prior work; and many other issues in the presentation. I agree with the reviewers’ evaluation and encourage the authors to improve this paper and resubmit to future conference. 
The paper provides some insight why model based RL might be more efficient than model free methods. It provides an example that even though the dynamics is simple, the value function is quite complicated (it is in a fractal). Even though the particular example might be novel and the construction interesting, this relation between dynamics and value function is not surprising, and perhaps part of the folklore. The paper also suggests a model based RL methods and provides some empirical results.  The reviewers find the paper interesting, but they expressed several concerns about the relevance of the particular example, the relation of the theory to empirical results, etc. The authors provided a rebuttal, but the reviewers were not convinced. Given that we have two Weak Rejects and the reviewer who is Weak Accept is not completely convinced, unfortunately I can only recommend rejection of this paper at this stage.
The paper proposes an aggregation algorithm for federated learning that is robust against label flipping, backdoor, and Gaussian noise attacks. The reviewers agree that the paper presents an interesting and novel method, however the reviewers also agree that the theory was difficult to understand and that the success of the methodology may be highly dependent on design choices and difficult to tune hyperparameters. 
This paper proposes a channel pruning approach based one shot neural architecture search (NAS). As agreed by all reviewers, it has limited novelty, and the method can be viewed as a straightforward combination of NAS and pruning. Experimental results are not convincing. The proposed method is not better than STOA on the accuracy or number of parameters. The setup is not fair, as the proposed method uses autoaugment while the other baselines do not. The authors should also compare with related methods such as Bayesnas, and other pruning techniques. Finally, the paper is poorly written, and many related works are missing.
Transformer models have proven to be quite successful when applied to a variety of ML tasks such as NLP.  However, the computational and memory requirements can at times be prohibitive, such as when dealing with long sequences.  This paper proposes locality sensitive hashing to reduce the sequence length complexity, as well as reversible residual layers to reduce storage requirements.  Experimental results confirm that the performance of Transformer models can be preserved even with these new efficiencies in place, and hence, this paper will likely have significant impact within the community.    Some relatively minor points notwithstanding, all reviewers voted for acceptance which is my recommendation as well.  Note that this paper was also vetted by several detailed external commenters.  In all cases the authors provided reasonable feedback, and the final revision of the work will surely be even stronger.
The paper presented an adaptive stochastic gradient descent method with layer wise normalization and decoupled weight decay and justified it on a variety of tasks. The main concern for this paper is the novelty is not sufficient. The method is a combination of LARS and AdamW with slight modifications. Although the paper has good empirically evaluations, theoretical convergence proof would make the paper more convincing. 
The paper discusses audio source separation with complex NNs.  The approach is good and may increase an area of research.  But the experimental section is very weak and needs to be improved to merit publication.
The reviewers found this paper on improving NLG using a graph to sequence architecture interesting and the results impressive. While I would personally have preferred to see further evaluation of this model on another NLG task, I think it would be overstepping in my role as AC to go against the reviewer consensus. The paper is clearly acceptable.
The authors present an approach to multi task learning. Reviews are mixed. The main worries seem to be computational feasibility and lack of comparison with existing work. Clearly, one advantage to Cross stitch networks over the proposed approach is that their approach learns sharing parameters in an end to end fashion and scales more efficiently to more tasks. Note: The authors mention SluiceNets in their discussion, but I think it would be appropriate to directly compare against this architecture   or DARTS [https://arxiv.org/abs/1806.09055], maybe   since the offline RSA computations only seem worth it if better than *anything* you can do end to end. I would encourage the authors to map out this space and situate their proposed method properly in the landscape of existing work. I also think it would be interesting to think of their approach as an ensemble learning approach and look at work in this space on using correlations between representations to learn what and how to combine. Finally, some work has suggested that benefits from MTL are a result of easier optimization, e.g., [3]; if that is true, will you not potentially miss out on good task combinations with your approach?  Other related work:  [0] https://www.aclweb.org/anthology/C18 1175/ [1] https://www.aclweb.org/anthology/P19 1299/ [2] https://www.aclweb.org/anthology/N19 1355.pdf   a somewhat similar two stage approach [3] https://www.aclweb.org/anthology/E17 2026/
The paper proposes a tensor decomposition method that interpolates between Tucker and CP decompositions. The authors also propose an optimization algorithms (AdaImp) and argue that it has superior performance against AdaGrad in this tesnor decomposition task. The approach is evaluated on some NLP tasks. The reviewers raised some concerns related to clarity, novelty, and strength of experiments. As part of addressing reviewers concerns, the authors reported their own results on MurP and Tucker (instead of quoting results from reference papers). While the reviewers greatly appreciated these experiments as well as authors  response to their questions and feedback, the concerns largely remained unresolved. In particular, R2 found the gain achieved by AdaImp not significantly large compared to Adagrad. In addition, R2 found very limited evaluation on how AdaImp outperforms Adagrad (thus little evidence to support that claim). Finally, AdaImp lacks any theoretical analysis (unlike Adagrad).
This paper considers deep reinforcement learning skill transfer and composition, through an attention model that weighs the contributions of several base policies conditioned on the task and state, and uses this to output an action. The method is evaluated on several Mujoco tasks.  There were two main areas of concern. The first was around issues with using equivalent primitives and training times for comparison methods. The second was around the general motivation of the paper, and also the motivation for using a BiRNN. These issues were resolved in a comprehensive discussion, leaving this as an interesting paper that should be accepted.
The reviewers were unanimous that this submission is not ready for publication at ICLR in its current form.   Concerns raised include a significant lack of clarity, and the paper not being self contained.
The paper addresses individual fairness scenario (treating similar users similarly) and proposes a new definition of algorithmic fairness that is based on the idea of robustness, i.e. by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased. All reviewers and AC agree that this work is clearly of interest to ICLR, however the reviewers have noted the following potential weaknesses: (1) presentation clarity   see R3’s detailed suggestions e.g. comparison to Dwork et al, see R2’s comments on how to improve, (2) empirical evaluations   see R1’s question about using more complex models, see R3’s question on the usefulness of the word embeddings.  Pleased to report that based on the author respond with extra experiments and explanations, R3 has raised the score to weak accept. All reviewers and AC agree that the most crucial concerns have been addressed in the rebuttal, and the paper could be accepted   congratulations to the authors! The authors are strongly urged to improve presentation clarity and to include the supporting empirical evidence when preparing the final revision.
This paper tackles the problem of regret minimization in a multi agent bandit problem, where distributed learning bandit algorithms collaborate in order to minimize their total regret. More specifically, the work focuses on efficient communication protocols and the regret corresponds to the communication cost. The goal is therefore to design protocols with little communication cost. The authors first establish lower bounds on the communication cost, and then introduce an algorithm with provable near optimal regret.  The only concern with the paper is that ICLR may not be the appropriate venue given that this work lacks representation learning contributions. However, all reviewers being otherwise positive about the quality and contributions of this work, I would recommend acceptance.
The main idea proposed by the work is interesting. The reviewers had several concerns about applicability and the extent of the empirical work. The authors responded to all the comments, added more experiments, and as reviewer 2 noted, the method is interesting because of its ability to handle local noise. Despite the author s helpful responses, the ratings were not increased, and it is still hard to assess the exact extent of how the proposed approach improves over state of the art.   Because some concerns remained, and due to a large number of stronger papers, this paper was not accepted at this time.
This paper offers a possibly novel approach to regularizing policy learning to make it suitable for large scale divergence in the underlying domain.  Unfortunately all the reviewers are unanimous that the paper is not acceptable in present form.  Insufficient clarity regarding the contribution relative to several references, some of which were missing from the submitted version, is perhaps the most significant issue in the view of the AC.
In this paper, the authors draw upon online convex optimization in order to derive a different interpretation of Adam Type algorithms, allowing them to identify the functionality of each part of Adam. Based on these  observations, the authors derive a new Adam Type algorithm,  AdamAL and test it in 2 computer vision datasets using 3 CNN architectures. The main concern shared by all reviewers is the lack of novelty but also rigor both on the experimental and theoretical justification provided by the authors. After having read carefully the reviews and main points of the paper, I will side with the reviewers, thus not recommending acceptance of this paper. 
This paper proposes EXAID, a method to detect adversarial attacks by building on the advances in explainability (particularly SHAP), where activity map like explanations are used to justify and validate decisions. Though it may have some valuable ideas, the execution is not satisfying, with various issues raised in comments. No rebuttal was provided.
This paper has a few interesting contributions: (a) a bound for un compressed networks in terms of the compressed network (this is in contrast to some prior work, which only gives bounds on the compressed network); (b) the use of local Rademacher complexity to try to squeeze as much as possible out of the connection; (c) an application of the bound to a specific interesting favorable condition, namely low rank structure.  As a minor suggestion, I d like to recommend that the authors go ahead and use their allowed 10th body page!
This paper is concerned with warm starting Bayesian optimization (i.e. starting with a better surrogate model) through transfer learning among related problems.   While the key motivation for warm starting BO is certainly important (although not novel), there are important shortcomings in the way the method is developed and demonstrated. Firstly, the reviewers questioned design decisions, such as why combine NNs and GPs in this particular way or why the posterior variance of the hybrid model is not calculated. Moreover, there are issues with the experimental methodology that do not allow extraction of confident conclusions (e.g. repeating the experiments for different initial points is highly desirable). Finally, there are presentation issues. The authors replied only to some of these concerns, but ultimately the shortcomings seem to persist and hint towards a paper that needs more work. 
This work combines style transfer approaches either in a serial or parallel fashion, and shows that the combination of methods is more powerful than isolated methods. The novelty in this work is extremely limited and not offset by insightful analysis or very thorough experiments, given that most results are qualitative. Authors have not provided a public response. Therefore, we recommend rejection.
This paper investigates an existing method for fitting sparse neural networks, and provides a novel proof of global convergence.  Overall, this seems like a sensible, if marginal, contribution.  However, there were serious red flags regarding the care which which the scholarship was done which make me deem the current submission unsuitable for publication.  In particular, two points raised by R4, which were not addressed even after the rebuttal:  1) "One important issue with the paper is that it blurs the distinction between prior work and the new contribution. For example, the subsection on Split Linearized Bregman Iteration in the "Methodology" section does not contain anything new compared to [1], and this is not clear enough to the reader."  2) "The newly written conclusion is still incorrect, stating again that Split LBI achieves SOTA performance on ImageNet."  I believe that R3 s high score is due to not noticing these unsupported or misleading claims.  
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.  Concerns raised include lack of sufficient motivation for the approach, and problems with clarity of the exposition.
The authors propose a novel distance metric learning approach. Reviews were mixed, and while the discussion was interesting to follow, some issues, including novelty, comparison with existing approaches, and impact, remain unresolved, and overall, the paper does not seem quite ready for publication. 
Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. This paper potentially discusses an interesting problem, and the concern raised by Review #2 was addressed in the revised paper. However,  given the  high competition at ICLR2020, this paper is unfortunately below the bar. We hope that the reviewers  comments are useful for improving the paper for potential future publication.  The 
This paper proposes a meta RL algorithm that learns an objective function whose gradients can be used to efficiently train a learner on entirely new tasks from those seen during meta training. Building off policy gradient based meta RL methods is challenging, and had not been previously demonstrated. Further, the demonstrated generalization capabilities are a substantial improvement in capabilities over prior meta learning methods. There are a couple related works that are quite relevant (and somewhat similar in methodology) and overlooked   see [1,2]. Further, we strongly encourage the authors to run the method on multiple meta training environments and to report results with more seeds, as promised. The contributions are significant and should be seen by the ICLR community. Hence, I recommend an oral presentation.  [1] Yu et al. One Shot Imitation from Observing Humans via Domain Adaptive Meta Learning [2] Sung et al. Meta critic networks
The work proposes a modification to existing architectures applied to predict taxonomic labels from metagenomic sequences. Reviewers agreed that the problem was well motivated, but that current experiments lack comparisons with existing standard baselines in the area. I recommend the authors update their work to included the additional experiments suggested by the reviewers.
This paper proposes to extract a character from a video, manually control the character, and render into the background in real time.  The rendered video can have arbitrary background and capture both the dynamics and appearance of the person. All three reviewers praises the visual quality of the synthesized video and the paper is well written with extensive details. Some concerns are raised. For example, despite an excellent engineering effort, there is few things the reader would scientifically learn from this paper. Additional ablation study on each component would also help the better understanding of the approach. Given the level of efforts, the quality of the results and the reviewers’ comments, the ACs recommend acceptance as a poster.
This paper uses a variant of parallel tempering to tune the subset of neural net hyperparameters which control the amount of noise and/or rate of diffusion (e.g. learning rate, batch size). It s certainly an appealing idea to run multiple chains in parallel and periodically propose swaps between them. However, I m not persuaded about the details. The argumentation in the paper is fairly informal, and it uses ideas from optimization and MCMC somewhat interchangeably. Since the individual chains aren t sampling from any known stationary distribution, it s not clear to me what MH based swaps will achieve.   The authors are upset with one of the reviews and think it misrepresents their paper. However, I find myself agreeing with most of the reviewer s points. Furthermore, as a general principle, the availability of code doesn t by itself make a paper reproducible. One should be able to reproduce it without the code, and one shouldn t need to refer to the code for important details about the algorithm.  Another limitation (pointed out by various reviewers) is that there aren t any comparisons against prior work on hyperparameter optimization. Overall, I think there are some promising and appealing ideas in this submission, but it needs to be cleaned up before it s ready for publication at ICLR. 
This paper proposes using RL to solve PDEs, with application to solving conservation laws. It is quite borderline, with one reviewer weakly recommending acceptance, one finding the paper interesting but the application not sufficiently novel, and one confessing they have not understood the paper.  I concur with R2 this is a difficult subject matter, but the other reviewers seem satisfied with the clarity of the presentation. R3 seems to believe the paper sufficiently proves the concept to warrant publication. I confess I do not understand R1 s argument for lack of novelty, despite my pushing for further detail. I see this as a novel application of RL methods, and R1 admits this will be seen as novel for a PDE conference. I am in favour of a certain degree of interdisciplinarity at ICLR, and believe this paper could bring a bit of subject matter diversity to the programme. However, due to the number of high quality submissions in my area, I m afraid this one must be rejected due to limited space. The authors are encouraged to submit to another ML conference after addressing (or having addressed) some of the action items from the more sympathetic reviewers.
The authors argue that directly optimizing the IS proposal distribution as in RWS is preferable to optimizing the IWAE multi sample objective. They formalize this with an adaptive IS framework, AISLE, that generalizes RWS, IWAE STL and IWAE DREG.   Generally reviewers found the paper to be well written and the connections drawn in this paper interesting. However, all reviewers raised concerns about the lack of experiments (Reviewer 3 suggested several experiments that could be done to clarify remaining questions) and practical takeaways.   The authors responded by explaining that "the main "practical" takeaway from our work is the following: If one is interested in the bias reduction potential offered by IWAEs over plain VAEs then the adaptive importance sampling framework appears to be a better starting point for designing new algorithms than the specific multi sample objective used by IWAE. This is because the former retains all of the benefits of the latter without inheriting its drawbacks." I did not find this argument convincing as a primary advantage of variational approaches over WS is that the variational approach optimizes a unified objective. At least in principle, this is a serious drawback of the WS approaches. Experiments and/or a discussion of this is warranted.  This paper is borderline, and unfortunately, due to the high number of quality submissions this year, I have to recommend rejection at this point. 
This paper presents a neural topic model with the goal of improving topic discovery with a PLSA loss. Reviewers point out major limitations including the following:  1) Empirical comparison is done only with LDA when there are many newer models that perform much better. 2) Related work section is incomplete, especially for the newer models. 3) Writing is unclear in many parts of the paper.  For these reasons, I recommend that the authors make major improvements to the paper before resubmitting to another venue.
This paper addresses the tasks of video generation and prediction and shows impressive results on the datasets such as Kinetics 600. There is a reviewer disagreement on this paper. AC can confirm that all three reviewers have read the rebuttal and have contributed to a long discussion. The reviewers have raised the following concerns that were viewed as critical issues when making the final decision: R1 and R3 expressed the concerns regarding limited technical novelty of the proposed approach in light of the prior works, e.g. MoCoGAN and TGANv2. R3 suggests, that the proposed method shows advantage that might be due to the large computational resources available to train the model. Providing a comparison of the proposed model and the relevant baselines on the Kinetics dataset is desirable to access the benefits of the proposed approach (R1).  AC also agrees with the R2 about the potential impact this work could have in the community. However, given that the reviewers have raised important concerns and have given suggestions, the paper needs too many revisions for acceptance at this time. We hope the reviews are useful for improving and revising the paper.
This article studies the effects of BN on robustness. The article presents a series of experiments on various datasets with noise, PGD adversarial attacks, and various corruption benchmarks, that show a drop in robustness when using BN. It is suggested that a main cause of vulnerability is the tiling angle of the decision boundary, which is illustrated in a toy example.  The reviewers found the contribution interesting and that the effect will impact many DNNs. However, they the did not find the arguments for the tiling explanation convincing enough, and suggested more theory and experimental illustration of this explanation would be important. In the rebuttal the authors maintain that the main contribution is to link BN and adversarial vulnerability and consider their explanation reasonable. In the initial discussion the reviewers also mentioned that the experiments were not convincing enough and that the phenomenon could be an effect of gradient masking, and that more experiments with other attack strategies would be important to clarify this. In response, the revision included various experiments, including some with various initial learning schedules. The revision clarified some of these issues. However, the reviewers still found that the reason behind the effect requires more explanations. In summary, this article makes an important observation that is already generating a vivid discussion and will likely have an impact, but the reviewers were not convinced by the explanations provided for these observations.  
This paper proposes a graphon based search space for neural architecture search. Unfortunately, the paper as currently stands and the small effect sizes in the experimental results raise questions about the merits of actually employing such a search space for the specific task of NAS. The reviewers expressed concerns that the results do not convincingly support graphon being a superior search space as claimed in the paper.  
This paper offers an improved attack on 3 D point clouds. Unfortunately the clarity of the contribution is unclear and on balance insufficient for acceptance.
Navigation is learned in a two stage process, where the (recurrent) network is first pre trained in a task agnostic stage and then fine tuned using Q learning. The analysis of the learned network confirms that what has been learned in the task agnostic pre training stage takes the form of attractors.  The reviewers generally liked this work, but complained about lack of comparison studies / baselines. The authors then carried out such studies and did a major update of the paper.  Given that the extensive update of the paper seems to have addressed the reviewers  complaints, I think this paper can be accepted.
The paper proposes a training method for generative adversarial network that avoids solving a zero sum game between the generator and the critic, hence leading to more stable optimization problems. It is similar to MMD GAN, in which MMD is computed on a projected low dim space, but the projection is trained to match the density ratio between the observed and the latent space. The reviewers raised several questions. Most of them have been addressed after several rounds of discussions. Overall, they are all positive about this paper, so I recommend acceptance. I encourage the authors to incorporate those discussions in their revised paper.
This paper studies the training of over parameterized two layer neural networks by considering high order Taylor approximation, and randomizing the network to remove the first order term in the network’s Taylor expansion. This enables the neural network training go beyond the recently so called neural tangent kernel (NTK) regime. The authors also established the optimization landscape, generalization error and expressive power results under the proposed analysis framework. They showed that when learning polynomials, the proposed randomized networks with quadratic Taylor approximation outperform standard NTK by a factor of the input dimension. This is a very nice work, and provides a new perspective on NTK and beyond. All reviewers are in support of accepting this paper. 
The paper explores the setting of *just* using data augmentation without an additional regularization term included.  The submission claims that comparatively good performance can be achieved with data augmentation alone.  The reviewers unanimously felt that the submission was not suitable for publication at ICLR.  The reasons included skepticism that augmentation without regularization is a useful setting to explore, as well as concerns about the experiments used to support the conclusions in the paper.  In particular, there were concerns that the experiments do not match best practice and that the error rates were too high.  Finally, there were concerns about the clarity of definitions of "implicit" and "explicit" regularization.
Main content: Paper is about training low precision networks to a high accuracy.  Discussion: reviewer 2: impressive results, main questions are around some clarity in the experiments tried, but sounds like authors addressed most of this in rebuttal. reviewer 1: well written paper, but authors think some technical details could be clarified.  reviewer 3:  well written but experimental section could be improved. Recommendation: all reviewers are in consensus, well written paper but some experiments/technical details could be improved. i vote poster.
This paper proposes to build an  imitative model  to improve the performance for imitation learning. The main idea is to combine the model based RL type of work to the imitation learning approach. The model is trained using a probabilistic method and can help the agent imitate goals that were previously not easy to achieve with previous works.  Reviewers 2 and 3 strongly agree that the paper should be accepted. R3 has increased their score after the rebuttal, and the authors  response helped in this case. Based on reviewers score, I recommend to accept this paper.
The paper presents a semi supervised learning approach to handle semantic classification (pixel level classification). The approach extends Hung et al. 18, using a confidence map generated by an auxiliary network, aimed to improve the identification of small objects.  The reviews state that the paper novelty is limited compared to the state of the art; the reviewers made several suggestions to improve the processing pipeline (including all images, including the confidence weights).  The reviews also state that the paper needs be carefully polished.   The area chair hopes that the suggestions about the contents and writing of the paper will help to prepare an improved version of the paper.  
Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. However, the novelty of this paper is rather marginal and given the  high competition at ICLR2020, this paper is unfortunately below the bar. We hope that the reviewers  comments are useful for improving the paper for potential future publication. 
The paper consider the problem of program induction from a small dataset of input output pairs; the small amount of available data results a large set of valid candidate programs. The authors propose to train an neural oracle by unsupervised learning on the given data, and synthesizing new pairs to augment the given data, therefore reducing the set of admissible programs. This is reminiscent of data augmentation schemes, eg elastic transforms for image data.  The reviewers appreciate the simplicity and effectiveness of this approach, as demonstrated on an android UI dataset. The authors successfully addressed most negative points raised by the reviewers in the rebuttal, except the lack of experimental validating on other datasets.  I recommend to accept this paper, based on reviews and my own reading. I think the manuscript could be further improved by more explicitly discussing  (early in the paper) the intuition why the authors think this approach is sensible: The additional information for more successfully infering the correct program has to come from somewhere; as no new information is eg given by a human oracle, it was injected by the choice of prior over neural oracles. It is essential that the paper discuss this. 
The authors present a deep model for probabilistic clustering and extend it to handle time series data.   The proposed method beats existing deep models on two datasets and  the representations learned in the process are also interpretable.  Unfortunately, despite detailed responses by the authors, the reviewers felt that some of their main concerns were not addressed. For example, the authors and the reviewers are still not converging on whether SOM VAE uses a VAE or an autoencoder. Further, the discussion about the advantages of VAE over AE is still not very convincing. Currently the work is positioned as a variational clustering method but the reviewers feel that it is a clustering method which uses a VAE (yes, I understand that this difference is subtle but needs to be clarified).   The reviewers read the responses of the author and during discussions with the AC suggested that there were still not convinced about some of their initial questions. Given this, at this point I would prefer going by the consensus of the reviewers and recommend that this paper cannot be accepted.
The submission proposes an architecture to learn a similarity metric for graph matching. The architecture uses node graph information in order to learn a more expressive, multi level similarity score. The hierarchical approach is empirically validated on a limited set of graphs for which pairwise matching information is available and is shown to outperform other methods for classification and regression tasks.  The reviewers were divided in their scores for this paper, but all noted that the approach was somewhat incremental and empirically motivated, without adequate analysis, theoretical justification, or extensive benchmark validation.   Although the approach has value, more work is needed to support the method fully. Recommendation is to reject at this time.
This paper proposes to deal with task heterogeneity in meta learning by extracting cross task relations and constructing a meta knowledge graph, which can then quickly adapt to new tasks. The authors present a comprehensive set of experiments, which show consistent performance gains over baseline methods, on a 2D regression task and a series of few shot classification tasks. They further conducted some ablation studies and additional analyses/visualization to aid interpretation.  Two of the reviewers were very positive, indicating that they found the paper well written, motivated, novel, and thorough, assessments that I also share. The authors were very responsive to reviewer comments and implemented all actionable revisions, as far as I can see. The paper looks to be in great shape. I’m therefore recommending acceptance.  
This paper proposes a new framework for improved nearest neighbor search by learning a space partition of the data, allowing for better scalability in distributed settings and overall better performance over existing benchmarks.  The two reviewers who were most confident were both positive about the contributions and the revisions. The one reviewer who recommended reject was concerned about the metric used and whether comparison with baselines was fair. In my opinion, the authors seem to have been very receptive to reviewer comments and answered these issues to my satisfaction. After author and reviewer engagement, both R1 and myself are satisfied with the addition of the new baselines and think the authors have sufficiently addressed the major concerns. For the final version of the paper, I’d urge the authors to take seriously R4’s comment regarding clarity and add algorithmic details as per their suggestion. 
The paper proposed an attention forcing algorithm that guides the sequence to sequence model training to make it more stable. But as pointed out by the reviewers, the proposed method requires alignment which is normally unavailable. The solution to address that is using another teacher forcing model, which can be expensive.   The major concern about this paper is the experimental justification is not sufficient: * lack of evaluations of the proposed method on different tasks; * lack of experiments on understanding how it interact with existing techniques such as scheduled sampling etc; * lack of comparisons to related existing supervised attention mechanisms.  
This paper discusses new methods to perform adversarial attacks on salience maps.  In its current form, this paper in its current form has unfortunately has not convinced several of the reviewers/commenters of the motivation behind proposing such a method. I tend to share the same opinion. I would encourage the authors to re think the motivation of the work, and if there are indeed solid use cases to express them explicitly in the next version of the paper.
The reviewers equivocally reject the paper, which is mostly experimental and the results of which are limited.  The authors do not react to the reviewers  comments.
This paper proposes a new mechanism to visualize the latent space of a neural network. The idea is simple and the paper includes several experiments to test the effectiveness of the method. However, the method bears similarity to previous work and the evaluation does not sufficiently show quantitative improvements over other introspection techniques. The reviewers found this was a substantial problem and for this reason the paper is not ready for publication. The paper should improve its discussion of prior work and better establish its place in this regard.
All reviewers assessed this paper as a weak reject. The AC recommends rejection.
This paper explores the application of the lottery ticket hypothesis to NLP and RL problems for better initialisations of deep networks and reduced model sizes. This is evaluated in a variety of settings, including continuous control and ATARI games for RL, and LSTMs and Transformers for NLP, showing very positive results.  The main issue raised by the reviewers was the lack of algorithmic novelty in the paper. Despite this, I believe the paper to present an important contribution that could stimulate much additional research. The paper is well written and the results are rigorous and interesting. For these reasons I recommend acceptance.
The paper proposes a hybrid weighs representation method in deep networks. The authors propose to utilize the extra state in 2 bit ternary representation to encode large weight values. The idea is simple and straightforward. The main concern is on the experimental results. The use of mixed bit width for neural network quantization is not new, but the authors only compare with basic quantization method in the original submission. In the revised version of the paper, the proposed method performs significantly worse than recent quantization methods such as PACT and QIL. Moreover, writing can be improved, and parts of the paper need to be clarified.
The authors present a method that optimizes a differentiable neural computer with evolutionary search, and which can transfer abstract strategies to novel problems.  The reviewers all agreed that the approach is interesting, though were concerned about the magnitude of the contribution / novelty compared to existing work, clarity of contributions, impact of pretraining, and simplicity of examples.  While the reviewers felt that the authors resolved the many of their concerns in the rebuttal, there was remaining concern about the significance of the contribution.  Thus, I recommend this paper for rejection at this time.
This paper proposes a WTA models for binary projection.  While there are notable partial contributions, there is disagreement among the reviewers.   I am most persuaded by the concern expressed that the experiments are not done on datasets that are large enough to be state of the art compared to other random projection investigations.
This paper proposes a CNN based text classification model that uses words, characters, and labels as its input. It also presents an attention block to replace the pooling operation that is typically used in a CNN. The proposed method is evaluated on six benchmark classification datasets, achieving reasonably good results.  While the proposed method performs reasonably well compared to baselines in the papers, all reviewers pointed out that there is no discussion or comparison with existing SotA based on pretrained models (e.g., BERT, XLNet), which would strengthen the main claim of the paper. All three reviewers also suggested that the writing of the paper could be improved. The authors did not respond to these reviews, so there was little discussion needed to arrive at a consensus.  I agree with all reviewers and recommend to reject the paper.
This paper introduces a probabilistic generative model which mixes a variational autoencoder (VAE) with an energy based model (EBM). As mentioned by all reviewers (i) the motivation of the model is not well justified (ii) experimental results are not convincing enough. In addition (iii) handling sets is not specific to the proposed approach, and thus claims regarding sets should be revised. 
This paper investigates ways of using pretrained transformer models like BERT for classification tasks on documents that are longer than a standard transformer can feasibly encode.   This seems like a reasonable research goal, and none of the reviewers raised any concerns that seriously questioned the claims of the paper. However, neither of the more confident reviewers were convinced by the experiments in the paper (even after some private discussion) that the methods presented here represent a useful contribution.   This is not an area that I (the area chair) know well, but it seems as though there aren t any easy fixes to suggest: Additional discussion of the choice of evaluation data (or new data), further ablations, and general refinement of the writing could help.
The paper shows the relationship between node embeddings and structural graph representations. By careful definition of what structural node representation means, and what node embedding means, using the permutation group, the authors show in Theorem 2 that node embeddings cannot represent any extra information that is not already in the structural representation. The paper then provide empirical experiments on three tasks, and show in a fourth task an illustration of the theoretical results.  The reviewers of the paper scored the paper highly, but with low confidence. I read the paper myself (unfortunately not with a lot of time), with the aim of increasing the confidence of the resulting decision. The main gap in the paper is between the phrases "structural node representation" and "node embedding", and their theoretical definitions. The analogy of distribution and its samples follows unsurprisingly from the definitions (8 and 12), but the interpretation of those definitions as the corresponding English phrases is not obvious by only looking at the definitions. There also seems to be a sleight of hand going on with the most expressive representations (Definitions 9 and 11), which is used to make the conditional independence statement of Theorem 2. The authors should clarify in the final version whether the existence of such a representation can be shown, or even better a constructive way to get it from data.  Given the significance of the theoretical results, the authors should improve the introduction of the two main concepts by:   relating them to prior work (one way is to move Section 5 towards the front)   explaining in greater detail why Definitions 8 and 12 correspond to the two concepts. For example expanding the part of the proof of Corollary 1 about SVD, to make clear what Definition 12 means.   a corresponding simple example of Definition 8 to relate to a classical method.  The paper provides a nice connection between two disparate concepts. Unfortunately, the connection uses graph invariance and equivariance, which is unfamiliar to many of the ICLR audience. On balance, I believe that the authors can improve the presentation such that a reader can understand the implications of the connection without being an expert in graph isomorphism. As such, I am recommending an accept.  
This paper tackles hard exploration RL problems. The idea is to learn separate exploration and exploitation strategies using the same network (representation). The exploration is driven by intrinsic rewards, which are generated using an episodic memory and a lifelong novelty modules. Several experiments (simple and Atari domains) show that the proposed approach compares favourably with the baselines.  The work is novel both in terms of the episodic curiosity metric and its integration with the life long curiosity metric, and the results are convincing. All reviewers being positive about this paper, I therefore recommend acceptance.
This paper is an empirical studies of methods to stabilize offline (ie, batch) RL methods where the dataset is available up front and not collected during learning. This can be an important setting in e.g. safety critical or production systems, where learned policies should not be applied on the real system until their performance and safety is verified. Since policies leave the area where training data is present, in such settings poor performance or divergence might result, unless divergence from the reference policy is regularized. This paper studies various methods to perform such regularization.   The reviewers are all very happy about the thoroughness of the empirical work. The work only studies existing methods (and combination thereof), so the novelty is limited by design. The paper was also considered well written and easy to follow. The results were very similar between the considered regularizers, which somehow limits the usefulness of the paper as practical guideline (although at least now we know that perhaps we do not need to spend a lot of time choosing the best between these). Bigger differences were observed between "value penalties" versus "policy regularization". This seems to correspond to theoretical observations by Neu et al (https://arxiv.org/abs/1705.07798, 2017), which is not cited in the manuscript. Although unpublished, I think that work is highly relevant for the current manuscript, and I d strongly recommend the authors to consider its content. Some minor comments about the paper are given below.  On the balance, the strong point of the paper is the empirical thoroughness and clarity, whereas novelty, significance, and theoretical analysis are weaker points. Due to the high selectivity of ICLR, I unfortunately have to recommend rejection for this manuscript.  I have some minor comments about the contents of the paper:   The manuscript contains the line:  "Under this definition, such a behavior policy πb is always well defined even if the dataset was collected by multiple, distinct behavior policies". Wouldn t simply defining the behavior as a mixture of the underlying behavior policies (when known) work equally well?   The paper mentions several earlier works that regularize policies update using the KL from a reference policy (or to a reference policy). The paper of Peters is cited in this context, although there the constraint is actually on the KL divergence between state action distributions, resulting in a different type of regularization.
This paper presents a new reinforcement learning based approach to device placement for operations in computational graphs and demonstrates improvements for large scale standard models.  The paper is borderline with all reviewers appreciating the paper even the reviewer with the lowest score. The reviewer with the lowest score is basing the score on minor reservation regarding lack of detail in explaining the experiments.  Based upon the average score rejection is recommended. The reviewers  comments can help improve the paper and it is definitely recommended to submit it to the next conference.  
This paper proposes question answering as a general paradigm to decode and understand the representations that agents develop, with application to two recent approaches to predictive modeling. During rebuttal, some critical issues still exist, e.g., as Reviewer#3 pointed out, the submission in its current form lacks experimental analysis of the proposed conditional probes, especially the trade offs on the reliability of the representation analysis when performed with a conditional probe as well as a clear motivation for the need of a language interface. The authors are encouraged to incorporate the refined motivation and add more comprehensive experimental evaluation for a possible resubmission.
The authors present a new training procedure for generative models where the target and generated distributions are first mapped to a latent space and the divergence between then is minimised in this latent space. The authors achieve state of the art results on two datasets.  All reviewers agreed that the idea was vert interesting and has a lot of potential. Unfortunately, in the initial version of the paper the main section (section 3) was not very clear with confusing notation and statements. I thank the authors for taking this feedback positively and significantly revising the writeup. However, even after revising the writeup some of the ideas are still not clear. In particular, during discussions between the AC and reviewers it was pointed out that the training procedure is still not convincing. It was not clear whether the heuristic combination of the deterministic PGA parts of the objective (3) with the likelihood/VAE based terms (9) and (12,13), was conceptually very sound. Unfortunately, most of the initial discussions with the authors revolved around clarity and once we crossed the "clarity" barrier there wasn t enough time to discuss the other technical details of the paper. As a result, even though the paper seems interesting, the initial lack of clarity went against the paper.   In summary, based on the reviewer comments, I recommend that the paper cannot be accepted.  
This paper seeks to understand the effect of learning rate decay in neural net training. This is an important question in the field and this paper also proposes to show why previous explanations were not correct. However, the reviewers found that the paper did not explain the experimental setup enough to be reproducible. Furthermore, there are significant problems with the novelty of the work due to its overlap with works such as (Nakiran et al., 2019), (Li et al. 2019) or (Jastrzębski et al. 2017).
This article studies convergence of WGAN training using SGD and generators of the form $\phi(Ax)$, with results on convergence with polynomial time and sample complexity under the assumption that the target distribution can be expressed by this type of generator. This expands previous work that considered linear generators. An important point of discussion was the choice of the discriminator as a linear or quadratic function. The authors  responses clarified some of the initial criticism, and the scores improved slightly. Following the discussion, the reviewers agreed that the problem being studied is a difficult one and that the paper makes some important contributions. However, they still found that the considered settings are very restrictive, maintaining that quadratic discriminators would work only for the very simple type of generators and targets under consideration. Although the article makes important advances towards understanding convergence of WGAN training with nonlinear models, the relevance of the contribution could be greatly enhanced by addressing / discussing the plausibility or implications of the analysis in a practical setting, in the best case scenario addressing a more practical type of neural networks. 
This paper discusses the (lack of) correlation between the image semantics and the likelihood assigned by flow based models, and implications for out of distribution (OOD) detection.  The reviewers raised several important questions: 1) precise definition of OOD: definition of semantics vs typicality (cf. definition in Nalisnick et al. 2019 pointed by R1) There was a nice discussion between authors and the reviewers. At a high level, there was some agreement in the end, but lack of precise definition may cause confusion. I think adding a precise definition will add more clarity and improve the paper.  2) novelty: similar observations have been made in earlier papers cf. Nalisnick et al. 2018. R3 also pointed a recent paper by Ren et al. 2019 which showed that likelihood can be dominated by background pixels. Older work has shown that the likelihood and sample quality are not necessarily correlated. The reviewers appreciate that this paper provides additional evidence, but weren t convinced that the new observations in this paper qualified for a full paper.  3) experiments on more datasets  Overall, while this paper explores an interesting direction, it s not ready for publication as is. I encourage the authors to revise the paper based on the feedback and submit to a different venue.
The reviewers had a hard time fully identifying the intended contribution behind this paper, and raised concerns that suggest that the experimental results are not sufficient to justify any substantial contribution with the level of certainty that would warrant publication at a top venue. The authors have not responded, and the concerns are serious, so I have no choice but to reject this paper despite its potentially valuable topic.
This paper proposes a new method to answering queries using incomplete knowledge bases. The approach relies on learning embeddings of the vertices of the knowledge graph. The reviewers unanimously found that the method was well motivated and found the method convincingly outperforms previous work.
This paper investigates how two means of learning natural language   supervised learning from labeled data and reward maximizing self play   can be combined. The paper empirically investigates this question, showing in two grounded visual language games that supervision followed by self play works better than the reverse.   The reviewers found this paper interesting and well executed, though not especially novel. The last is a reasonable criticism but in this case I think a little beside the point. In any case, since all the reviewers are in agreement I recommend acceptance.
Paper https://arxiv.org/abs/1802.10026 (Garipov et. al, NeurIPS 2018) shows that one can find curves between two independently trained solutions along which the loss is relatively constant. The authors of this ICLR submission claim as a key contribution that they show the weights along the path correspond to different models that make different predictions ("Note that prior work on loss landscapes has focused on mode connectivity and low loss tunnels, but has not explicitly focused on how diverse the functions from different modes are, beyond an initial exploration in Fort & Jastrzebski (2019)"). Much of the disagreement between two of the reviewers and the authors is whether this point had already been shown in 1802.10026.  It is in fact very clear that 1802.10026 shows that different points on the curve correspond to diverse functions. Figure 2 (right) of this paper shows the test error of an _ensemble_ of predictions made by the network for the parameters at one end of the curve, and the network described by \phi_\theta(t) at some point t along the curve: since the error goes down and changes significantly as t varies, the functions corresponding to different parameter settings along these curves must be diverse. This functional diversity is also made explicit multiple times in 1802.10026, which clearly says that this result shows that the curves contain meaningfully different representations.  In response to R3, the authors incorrectly claim that  "Figure 2 in Garipov et al. only plots loss and accuracy, and does not measure function space similarity, between different initializations, or along the tunnel at all. Just by looking at accuracy and loss values, there is no way to infer how similar the predictions of the two functions are." But Figure 2 (right) is actually showing the test error of an average of predictions of networks with parameters at different points along the curve, how it changes as one moves along the curve, and the improved accuracy of the ensemble over using one of the endpoints. If the functions associated with different parameters along the curve were the same, averaging their predictions would not help performance.   Moreover, Figure 6 (bottom left, dashed lines) in the appendix of 1802.10026 shows the improvement in performance in ensembling points along the curve over ensembling independently trained networks. Section A6 (Appendix) also describes ensembling along the curve in some detail, with several quantitative results. There is no sense in ensembling models along the curve if they were the same model.  These results unequivocally demonstrate that the points on the curve have functional diversity, and this connection is made explicit multiple times in 1802.10026 with the claim of meaningfully different representations: “This result also demonstrates that these curves do not exist only due to degenerate parametrizations of the network (such as rescaling on either side of a ReLU); instead, points along the curve correspond to meaningfully different representations of the data that can be ensembled for improved performance.”  Additionally, other published work has built on this observation, such as 1907.07504 (UAI 2019), which performs Bayesian model averaging over the mode connecting subspace, relying on diversity of functions in this space; that work also visualizes the different functions arising in this space.   It is incorrect to attribute these findings to Fort & Jastrzebski (2019) or the current submission.  It is a positive contribution to build on prior work, but what is prior work and what is new should be accurately characterized, and currently is not, even after the discussion phase where multiple reviewers raised the same concern. Reviewers appreciated the broader investigation of diversity and its effect on ensembling, and the more detailed study regarding connecting curves. In addition to the concerns about inaccurate claims regarding prior work and novelty (which included aspects of the mode connectivity work but also other works), several reviewers also felt that the time accuracy trade offs of deep ensembles relative to standard approaches were not clearly presented, and comparisons were lacking. It would be simple and informative to do an experiment showing a runtime accuracy trade off curve for deep ensembles alongside FGE and various Bayesian deep learning methods and mc dropout. It s also possible to use for example parallel MCMC chains to explore multiple quite different modes like deep ensembles but for Bayesian deep learning. For the paper to be accepted, it would need significant revisions, correcting the accuracy of claims, and providing such experiments.
This submission proposes a black box method for certifying the robustness of smoothed classifiers in the presence of adversarial perturbations. This work goes beyond previous works in certifying robustness for arbitrary smoothing measures.  Strengths:  Sound formulation and theoretical justification to tackle an important problem.  Weaknesses  Experimental comparison was at times not fair.  The presentation and writing could be improved.  These two weaknesses were sufficiently addressed during the discussion. All reviewers recommend acceptance.
The paper proposes a novel method to calibrate a knowledge graph embedding method when ground truth negatives are not available. Essentially, the method relies on generating corrupted triples as negative examples to be used by known approaches (Platt scaling and isotonic regression).   This is claimed as the first approach of probability calibration for knowledge graph embedding models, which is considered to be very relevant for practitioners working on knowledge graph embedding (although this is a narrow audience). The paper does not propose a wholly novel method for probability calibration. Instead, the value in experimental insights provided.  Some reviewers would have liked to see a more in depth analysis, but reviewers appreciated the thoroughness of the results in the clear articulation of the findings and the fact that multiple datasets and models are studied.   There was an animated discussion about this paper, but the paper seems a useful contribution to the ICLR community and I would like to recommend acceptance. 
The authors introduce an approach to learn a random forest model and a representation simultaneously. The basic idea is to modify the representation so that subsequent trees in the random forest are less correlated.  The authors evaluate the technique empirically and show some modest gains. While the reviews were mixed, the approach is quite different from the usual approaches published at ICLR  and so I think it s worth highlighting this work. 
This submission has been assessed by three reviewers who scored it 3/1/3, and they have remained unconvinced after the rebuttal. The main issues voiced are the difficult readability of the paper, cryptic at times due to a mix of physical and DL notations, and a lack of sufficient experimentation to support all claims. The reviewers acknowledge the authors  efforts to resolve the main issues but find these efforts insufficient. Thus, this paper cannot be accepted to ICLR2020.
This paper extends state of the art semi supervised learning techniques (i.e., MixMatch) to collect new data adaptively and studies the benefit of getting new labels versus adding more unlabeled data. Active learning is incorporated in a natural and simple (albeit, unsurprising) way and the experiments are convincing that this approach has merit.  While the approach works, reviewers were concerned about the novelty of the combination given that its somewhat obvious and straightforward to accomplish. Reviewers were also concerned that the space of both semi supervised learning algorithms and active learning algorithms was not sufficiently exhaustively studied. As one reviewer points out: neither of these ideas are new or particular to deep learning.  Due to lack of novelty, this paper is not suited for a top tier conference. 
This paper proposed an extension of the Monte Carlos Tree Search to find the optimal policy. The method combines A* and MCTS algorithms to prioritize the state to be explored. Compare with traditional MCTS based on UCT, A* MCTS seem to perform better.  One concern of the reviewers is the paper s presentation, which is hard to follow. The second concern is the strong restriction of assumption, which make the setting too simple and unrealistic. The rebuttal did not fully address these problems.  This paper needs further polish to meet the standard of ICLR. 
The authors develop a new technique for training neural networks to be provably robust to adversarial attacks. The technique relies on constructing a polyhedral envelope on the feasible set of activations and using this to derive a lower bound on the maximum certified radius. By training with this as a regularizer, the authors are able to train neural networks that achieve strong provable robustness to adversarial attacks.  The paper makes a number of interesting contributions that the reviewers appreciated. However, two of the reviewers had some concerns with the significance of the contributions made: 1) The contributions of the paper are not clearly defined relative to prior work on bound propagation (Fast Lin/KW/CROWN). In particular, the authors simply use the linear approximation derived in these prior works to obtain a bound on the radius to be certified. The authors claim faster convergence based on this, but this does not seem like a very significant contribution.  2) The improvements on the state of the art are marginal.  These were discussed in detail during the rebuttal phase and the two reviewers with concerns about the paper decided to maintain their score after reading the rebuttals, as the fundamental issues above were not   Given these concerns, I believe this paper is borderline   it has some interesting contributions, but the overall novelty on the technical side and strength of empirical results is not very high.
This work proposes a robust variant of GAN, in which the generator and discriminator compete with each other in a worst case setting within a small Wasserstein ball. Unfortunately, the reviewers have raised some critical concerns in terms of theoretical analysis and empirical support. The authors did not submit rebuttals in time. We encourage the authors to improve the work based on reviewer s comments. 
The paper proposes a new method for testing whether new data comes from the same distribution as training data without having an a priori density model of the training data. This is done by looking at the intersection of typical sets of an ensemble of learned models.   On the theoretical side, the paper was received positively by all reviewers. The theoretical results were deemed strong, and the ideas in the paper were considered novel. The problem setting was considered relevant, and seen as a good proposal to deal with the shortcoming of models on out of distribution data.   However, the lack of empirical results on at least somewhat realistic datasets (e.g. MNIST) was commented on by all reviewers. The authors only present a toy experiment. The authors have explained their decision, but I agree with R1 that it would be appropriate in such situations to present the toy experiment next to a more realistic dataset. This also means that the effectiveness of the proposed method in real settings is as of yet unclear. Although the provided toy example was considered clear and illuminating, the clarity of the text could still be improved.  Although the reviewers had a spread in their final score, I think they would all agree that the direction this paper takes is very exciting, but that the current version of the paper is somewhat premature. Thus, unfortunately, I have to recommend rejection at this point.   
This paper proposes convergence results for zeroth order optimization.  One of the main complaints was that ZO has limited use in ML. I appreciate the authors  response that there are cases where gradients are not easily available, especially for black box attacks.  However, I find the limited applicability an issue for ICLR and I encourage the authors to find a conference that is more suited to that work.
This paper studies two layer graph convolutional networks and two layer multi layer perceptions and develops quantitative results of their effect in signal processing settings. The paper received 3 reviews by experts working in this area. R1 recommends Weak Accept, indicating that the paper provides some useful insight (e.g. into when graph neural networks are or are not appropriate for particular problems) and poses some specific technical questions. In follow up discussions after the author response, R1 and authors agree that there are some over claims in the paper but that these could be addressed with some toning down of claims and additional discussion. R2 recommends Weak Accept but raises several concerns about the technical contribution of the paper, indicating that some of the conclusions were already known or are unsurprising. R2 concludes "I vote for weak accept, but I am fine if it is rejected." R3 recommends Reject, also questioning the significance of the technical contribution and whether some of the conclusions are well supported by experiments, as well as some minor concerns about clarity of writing. In their thoughtful responses, authors acknowledge these concerns.  Given the split decision, the AC also read the paper. While it is clear it has significant merit, the concerns about significance of the contribution and support for conclusions (as acknowledged by authors) are important, and the AC feels a revision of the paper and another round of peer review is really needed to flesh these issues out. 
This paper presents an extension of MPNN which leverages the random color augmentation to improve the representation power of MPNN. The experimental results shows the effectiveness of colorization. A majority of the reviewers were particularly concerned about lacking permutation invariance in the approach as well as the large variance issue in practice, and their opinion stays the same after the rebuttal. The reviewers unanimously expressed their concerns on the large variance issue during the discussion period. Overall, the reviewers believe that the authors has not addressed their concerns sufficiently.
This paper proposes a deep network architecture for learning to predict depth from images with sparsely depth labeled pixels.   This paper was subject to some discussion, since the authors felt that the approach was interesting and the problem well motivated. Some of the concerns about experimental evaluation (especially from R1) were resolved due the author s rebuttal, but ultimately the reviewers felt the paper was not yet ready for publication. 
This paper experimentally analyzes the double descent phenomenon for deep models. While, as the reviewers have mentioned, this phenomenon has been observed for some time, some of its specificities still elude us. As a consequence, I am happy to see this paper presented to ICLR.  That being said, given the original lack of proper references as well as the recent public announcements about this paper giving it visibility, I want to make it absolutely clear that this paper is accepted with the assumption that proper credit will be given to past work and that efforts will be made to draw connections between all these works.
The paper proposes a new problem setup as "online continual compression". The proposed idea is a combination of existing techniques and very simple, though interesting. Parts of the algorithm are not clear, and the hierarchy is not well motivated. Experimental results seem promising but not convincing enough, since it is on a very special setting, the LiDAR experiment is missing quantitative evaluation, and different tasks might introduce different difficulties in this online learning setting. The ablation study is well designed but not discussed enough.
The paper introduces a neat idea that an SGD update can be written as a solution of the linear least squares problem with a given backpropagated output; this is generalized to a larger batch size, giving a sort of "block" gradient type update. Some notes that the columns of $O_t$ have to be scaled are made, but not clear why. The paper then goes into the experiments, and then gets back to the fast approximation of DGB. It really looks like bad organization of the paper, which was noted.  The reviewers agree that the actual computational improvements are marginal, and all recommend rejection. As a recommendation, I would suggest to restructure the paper for a more coherent view, and also the improvements in Top 1 are not very stimulating. The general view is interesting, but it is not clear what insight it brings.
The paper derives results for nonnegative matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star convex towards randomized planted solutions.  Overall, the paper is relatively well written and fairly clear.  The reviewers agree that the theoretical contribution of the paper could be improved (tighten bounds) and that the experiments can be improved as well. In the context of other papers submitted to ICLR I therefore recommend to reject the paper.  
This paper gave a general L2O convergence theory called Learned Safeguarded KM (LSKM).  The reviewers found flaws both in theory and in experiments.  While all the reviewers have read the authors  rebuttal and gave detailed replies, they all agree to reject this paper.  I agree also.
This paper tackles the challenge of incentivising selfish agents towards a collaborative goal. In doing so, the authors propose several new modules.   The reviewers commented on experiments being extremely thorough. One reviewer commented on a lack of ablation study of the 3 contributions, which was promptly provided by the authors. The proposed method is also supported by theoretical derivations. The contributions appear to be quite novel, significantly improving performance of the studied SMGs.  One reviewer mentioned the clarity being compromised by too much material being in the appendix, which has been addressed by the authors moving some main pieces of content to the main text.   Two reviewer commented on the relevance being lower because of the problem not being widely studied in RL. I would disagree with the reviewers on this aspect, it is great to have new problem brought to light and have fresh and novel results, rather than having yet another paper work on Atari. I also think that the authors in their rebuttal made the practical relevance of their problem setting sufficiently clear with several practical examples. 
All reviewers unanimously accept the paper.
The reviewers generally agreed that the application and method are interesting and relevant, and the paper should be accepted.  I would encourage the authors to carefully go through the reviewers  suggestions and address them in the final.
This paper proposes to add constraints to the RL problem within a variational method. The hope is to specify a safe vs non safe states. The reviewers were not convinced that this paper makes the cut for ICLR. Moreover, there was no rebuttal from the authors, so it didn t give the reviewer a chance to reconsider their opinion. Based on the current ratings, I recommend to reject this paper. 
This paper presents sparse attention mechanisms for image captioning. In addition to recent sparsemax based method, authors proposed to extend it by incorporating structural constraints in 2D images, which is called TVMAX. The proposed methods are shown to improve the quality of captioning, particularly in terms of fewer erroneous repetitions, and obtain better human evaluation scores.  Through reviewer discussion, one reviewer updated the score to rejection. A major concern raised by the reviewers is that the motivation of introducing sparse attention is not clear, and the reason why it improves the quality (particularly, why it can reduce repetition) is not convincing. While we understand it is plausible for long sequences as in text domain, we are not convinced that it is really necessary for image captioning problems. Although authors seem to have some ideas, we cannot see how they will be reflected in the paper so I’d like to recommend rejection. I recommend authors to polish the paper with a clearer description of the motivation and high level analysis of the method as well as testing on other visual tasks to show its generality.  
The submission proposes a variant of a Transformer architecture that does not use positional embeddings to model local structural patterns but instead adds a recurrent layer before each attention layer to maintain local context. The approach is empirically verified on a number of domains.  The reviewers had concerns with the paper, most notably that the architectural modification is not sufficiently novel or significant to warrant publication, that appropriate ablations and baselines were not done to convincingly show the benefit of the approach, that the speed tradeoff was not adequately discussed, and that the results were not compared to actual SOTA results.  For these reasons, the recommendation is to reject the paper.
This paper provides an approach to improve the differentially private SGD method by leveraging a differentially private version of the lottery mechanism, which reduces the number of parameters in the gradient update (and the dimension of the noise vectors). While this combination appears to be interesting, there is a non trivial technical issue raised by Reviewer 3 on the sensitivity analysis in the paper. (R3 brought up this issue even after the rebuttal.) This issue needs to be resolved or clarified for the paper to be published.
This paper provides an active learning approach to improve the performance of an existing differentially private classifier with public labeled data. Where the paper provides a new approach, there is a consensus among the reviewers that the paper does not provide a strong enough contribution for acceptance. The authors can potentially improve the submission by including a more comprehensive comparison with the PATE framework and improving its overall presentation.
The authors design a GAN based text to speech synthesis model that performs competitively with state of the art synthesizers.  The reviewers and I agree that this appears to be the first really successful effort at GAN based synthesis.  Additional positives are that the model is designed to be highly parallelisable, and that the authors also propose several automatic measures of performance in addition to reporting human mean opinion scores.  The automatic measures correlate well (though far from perfectly) with human judgments, and in any case are a nice contribution to the area of evaluation of generative models.  It would be even more convincing if the authors presented human A/B forced choice test results (in addition to the mean opinion scores), which are often included in speech synthesis evaluation, but this is a minor quibble.
As Reviewer 2 pointed out in his/her response to the authors  rebuttal, this paper (at least in current state) has significant shortcomings that need to be addressed before this paper merits acceptance.
This paper presents an approach to improving the calculation of embeddings for nearest neighbor search with respect to edit distance.  Reading the reviews, it seems that the paper is greatly improved over its previous version, but still has significant clarity issues. Given that these issues remain even after one major revision, I would suggest that the paper not be accepted for this ICLR, but that the authors carefully revise the paper for clarity and submit to a following submission opportunity. It may help to share the paper with others who are not familiar with the research until they can read it once and understand the method well.  I have quoted Reviewer 3 below in the author discussion, where there are some additional clarity issues that may help being resolved:     Some specifics are clear now with their new edition.  * The [relationship between] cgk  & cgk not as clear as it could be. For example the algorithms are designed for bits. So one should assume that they are applying it on the bits of the characters. But this should be clarified in the manuscript. * Also still backpropagating through f  is not clear to me. * And in the text for inference they still say: "We randomly select 100 queries and use the remainder of the dataset as the base set" which should be "the remainder excluding the training set" or "including?".
This paper takes results related to the convergence and implicit regularization of stochastic mirror descent, as previously applied within overparameterized linear models, and extends them to the nonlinear case.  Among other things, conditions are derived for guaranteeing convergence to a global minimizer that is (nearly) closest to the initialization with respect to a divergence that depends upon the mirror potential.  Overall the paper is well written and likely at least somewhat accessible even for non experts in this field.  That being said, two reviewers voted to reject while one chose accept; however, during the rebuttal period the accept reviewer expressed a somewhat borderline sentiment.  As for the reviewers that voted to reject, a common criticism was the perceived similarity with reference (Azizan and Hassibi, 2019), as well as unsettled concerns about the reasonableness of the assumptions involved (e.g., Assumption 1).  With respect to the former, among other similarities the proof technique from both papers relies heavily on Lemma 6.  It was then felt that this undercut the novelty somewhat.     Beyond this though, even the accept reviewer raised an unsettled issue regarding the ease of finding an initialization point close to the manifold that nonetheless satisfies the conditions of Assumption 1.  In other words, as networks become more complex such that points are closer to the manifold of optimal solutions, further non convexity could be introduced such that the non negativity of the stated divergence becomes more difficult to achieve.  While the author response to this point is reasonable, it feels a bit like thoughtful speculation forged in the crunch time of a short rebuttal period, and possibly subject to change upon further reflection.  In this regard a less time constrained revision could be beneficial (including updates to address the other points mentioned above), and I am confident that this work can be positively received at another venue in the near future.
This paper presents experimental evidence that learning with privacy requires optimization of the model settings (architectures and initializations) that are not identical to those used when learning without privacy. While acknowledging potential usefulness of this work for practitioners, the reviewers expressed several important concerns such as (1) lack of SOTA baseline comparisons, (2) lack of clarity of the empirical evaluation protocols, (3) large models (that are widely used in practice) have not been studied in the paper, (4) low technical novelty. The authors have successfully addressed some of the concerns regarding (1) and (2). However (3) and (4) make it difficult to assess the benefits of the proposed approach for the community and were viewed by AC as critical issues. We hope the detailed reviews are useful for improving and revising the paper. 
Although the reviewers appreciated the novelty of this work, they unanimously recommended rejection.  The current version of the paper exhibits weak presentation quality and lacks sufficient technical depth.  The experimental evaluation was not found to be sufficiently convincing by any of the reviewers.  The submitted comments should help the authors improve their paper.
The reviewers were unanimous that this submission is not ready for publication at ICLR in its present form.  Concerns raised included lack of relevant baselines, and lack of sufficient justification of the novelty and impact of the approach.
This paper presents a method for improving optimization in multi task learning settings by minimizing the interference of gradients belonging to different tasks.   While the idea is simple and well motivated, the reviewers felt that the problem is still not studied adequately. The proofs are useful, but there is still a gap when it comes to practicality.  The rebuttal clarified some of the concerns, but still there is a feeling that (a) the main assumptions for the method need to be demonstrated in a more convincing way, e.g. by boosting the experiments as suggested with other MTL methods (b) by placing the paper better in the current literature and minimizing the gap between proofs/underlying assumptions and practical usefulness.  
The paper is interested in multi task learning. It introduces a new architecture which condition the model in a particular manner: images features and task ID features are fed to a top down network which generates task specific weights, which are then used in a bottom up network to produce final labels. The paper is experimental, and the contribution rather incremental, considering existing work in the area. Experimental section is currently not convincing enough, given marginal improvements over existing approaches   multiple runs as well as confidence intervals would help in that respect. 
The authors provide a framework for improving robustness (if the model of the dynamics is perturbed) into the RL methods, and provide nice experimental results, especially in the updated version. I am happy to see that the discussion for this paper went in a totally positive and constructive way which lead to a) constructive criticism of the reviewers b) significant changes in the paper c) corresponding better scores by the reviewer. Good work and obvious accept.
The paper proposes a recurrent and autorgressive architecture to model temporal knowledge graphs and perform multi time step inference in the form of future link prediction. However, the reviewers feel that the papers are more of a straight application of current techniques. Furthermore, a better presentation of the experimental section will also help improve the paper.  
The proposed paper presents low rank compression method for DNNs. This topic has been around for a while, so the contribution is limited. Lebedev et. al paper in ICLR 2015 used CP factorization to compress neural networks for Imagenet classification; in 2019, the idea has to be really novel in order to be presented on CIFAR datasets. The latency is not analyzed.  So, I agree with reviewers.
This paper studies the relationship between attention networks such as Transformers and convolutional networks. The paper shows that a special case of attention can be cast as convolution. However this link depends on using relative positional embeddings and generalization to other encodings are not given in the paper. The reviewers found the results correct, but we caution that the writing should better reflect the caveats of the approach.
The AC has carefully looked at the paper/comments/discussion in order to arrive at this meta review.  Looking over the paper, the FGL layer is an interesting idea, but its utility is only evaluated in a limited setting (fMRI data), rather that other types of images/data. Also, the approach seems to work on some of the fMRI datasets, on others the performance is on par with the baselines.   Overall, the paper is borderline but the AC believes the paper would be a good contribution to the conference.
Catastrophic forgetting in neural networks is a real problem, and this paper suggests a mechanism for avoiding this using a k nearest neighbor mechanism in the final layer. The reason is that the layers below the last layer should not change significantly when very different data is introduced.   While the idea is interesting none of the reviewers is entirely convinced about the execution and empirical tests, which had partially inconclusive. The reviewers had a number of questions, which were only partially satisfactorily answered. While some of the reviewers had less familiarity with the specific research topic, the seemingly most knowledgeable reviewer does not think the paper is ready for publication.  On balance, I think the paper cannot be accepted in its current state. The idea is interesting, but needs more work.
The paper provides a theoretical analysis of graph neural networks, as the number of layers goes to infinity. For the graph convolutional network, they relate the expressive power of the network with the graph spectra. In particular for Erdos Renyi graphs, they show that very deep graphs lose information, and propose a new weight normalization scheme based on this insight.  The authors responded well to reviewer comments. It is nice to see that the open review nature has also resulted in a new connection. Unfortunately one of the reviewers did not engage further in the discussion with respect to the author rebuttals.  Overall, the paper provides a nice theoretical analysis of a widely used graph neural network architecture, and characterises its behaviour on a popular class of graphs. The fact that the theory provides a new approach for weight normalization is a bonus.
All reviewers agree that this research is novel and well carried out, so this is a clear accept. Please ensure that the final version reflect the reviewer comments and the new information provided during the rebuttal
Reviewers put this paper in the lower half and question the theoretical motivation and the experimental design. On the other hand, this seems like an alternative general framework for solving large scale multi task learning problems. In the future, I would encourage the authors to evaluate on multi task benchmarks such as SuperGLUE, decaNLP and C4. Note: It seems there s more similarities with Ruder et al. (2019) [0] than the paper suggests.   [0] https://arxiv.org/abs/1705.08142
The authors consider the problem of predicting DNA folding patterns.                                                                They use a range of simple, linear models and find that a bi LSTM architecture                                                      yielded best performance.                                                                                                                                                                                                                                               This paper is below acceptance.                                                                                                     Reviewers pointed out strong similarity to previously published work.                                                               Furthermore the manuscript lacked in clarity, leaving uncertain eg details about                                                    experimental details. 
The paper proposed using stochastic AUC for dealing with imbalanced data. This paper provides useful insights and experiments on this important problem. I recommend acceptance.
This paper proposes a new distillation based method for using large pretrained models like BERT to produce much *smaller* fine tuned target task models.   This paper is low borderline: It has merit and meets our basic standards, but owing to capacity limitations we had to give preference to papers we see as having a higher potential impact. Reviewers had some concerns about experimental design, but those seem to have been fully resolved after discussion. Reviewers were not convinced, even after some discussion, that the method and results were sufficiently novel and effective to have a substantial impact on the state of practice in this area.
The paper presents a model for learning spiking representations. The basic model is a a deep autoencoder trained end to end with a biophysical generative model and results are presented on EMG and sEMG data, with the aim to motivate further research in self supervised learning.  The reviewers raised several points about the paper. Reviewer 1 raised concerns about lack of context on surrounding work, clarity of the model itself and motivating the loss. Reviewer 2 pointed out strengths of the paper in its simplicity and the importance of this problem, but also raised concerns about the papers clarity, again motivations on the loss function and sensibility of design choices. The authors responded to the feedback from reviewer 1, but overall the reviewer did not think their scores should be changed.  The paper in its current form is not yet ready for acceptance, and we hope there has been useful feedback from the reviewing process for their future research.
The paper presents  CuBERT (Code Understanding BERT), which is BERT inspired pretraining/finetuning setup, for source code contextual embedding. The embedding results are tested on classification tasks to demonstrate the effectiveness of CuBERT.   This is an interesting application paper that extends existing models to source code analysis. The authors did a good job at motivating the applications, describing the proposed models and discussing the experiments. The authors also agree to share all the datasets and source code so that the experiment results can be replicated and compared with by other researchers.   One major concern is the lack of strong baselines. All reviewers are concerned about this issue. The paper could lead to a good publication in the future if the issues can be addressed. 
The submission proposes a method for adversarial imitation learning that combines two previous approaches   GAIL and RED   by simply multiplying their reward functions. The claim is that this adaptation allows for better learning   both handling reward bias and improving training stability.   The reviewers were divided in their assessment of the paper, criticizing the empirical results and the claims made by the authors. In particular, the primary claims of handling reward bias and reducing variance seem to be not well justified, including results which show that training stability only substantially improves when SAIL b, which uses reward clipping, is used.   Although the paper is promising, the recommendation is for a reject at this time. The authors are encouraged to clarify their claims and supporting experiments and to validate their method on more challenging domains.
This paper uses the interpolation property to design a new optimization algorithm for deep learning, which computes an adaptive learning rate in closed form at each iteration. The authors also analyzed the convergence rate of the proposed algorithm in the stochastic convex optimization setting. Experiments on several benchmark neural networks and datasets verify the effectiveness of the proposed algorithm. This is a borderline paper and has been carefully discussed. The main objection of the reviewers include: (1) The interplay between regularization and the interpolation property is not clear; and (2) the proposed algorithm  is no better than SGD in any of the benchmarks except one, where SGD s learning rate is set to be a constant. After the author response, this paper still does not gather sufficient support. So I encourage the authors to improve this paper and resubmit it to future conference.
While there was some support for this paper, there was not enough support to accept it for publication at ICLR.  The following concern is characteristic of the concerns raised by the reviewers: "The "main contribution of this paper is hard to discern, but the ideas presented are interesting." Other reviewers said it was "hard to read" and not ready for publication. 
The paper presents a deep learning approach for tasks such as symbolic integration and solving differential equations.   The reviewers were positive and the paper has had extensive discussion, which we hope has been positive for the authors.   We look forward to seeing the engagement with this work at the conference.
The paper proposes a method to produce embeddings of discrete objects, jointly learning a small set of anchor embeddings and a sparse transformation from anchor objects to all the others. While the paper is well written, and proposes an interesting solution, the contribution seems rather incremental (as noted by several reviewers), considering the existing literature in the area.  Also, after discussions the usefulness of the method remains a bit unclear   it seems some engineering (related to sparse operations) is still required to validate the viability of the approach. 
The authors attempt to unify graph convolutional networks and label propagation and propose a model that unifies them. The reviewers liked the idea but felt that more extensive experiments are needed. The impact of labels needs to be specially studied more in depth.
This paper demonstrates that for deep RL problems one can construct adversarial examples where the examples don t really need to be even better than the best opponent. Surprisingly, sometimes, the adversarial opponent is less capable than normal opponents which the victim plays successfully against, yet they can disrupt the policies. The authors present a physically realistic threat model and demonstrate that  adversarial policies can exist in this threat model.  The reviewers agree with this paper presents results (proof of concept) that is "timely" and the RL community will benefit from this result. Based on reviewers comment, I recommend to accept this paper. 
This paper presents a NAS method that avoids having to retrain models from scratch and targets a range of model sizes at once. The work builds on Yu & Huang (2019) and studies a combination of many different techniques. Several baselines use a weaker training method, and no code is made available, raising doubts concerning reproducibility.  The reviewers asked various questions, but for several of these questions (e.g., running experiments on MNIST and CIFAR) the authors did not answer satisfactorily. Therefore, the reviewer asking these questions also refuses to change his/her rating.   Overall, as AnonReviewer #1 points out, the paper is very empirical. This is not necessarily a bad thing if the experiments yield a lot of insight, but this insight also appears limited. Therefore, I agree with the reviewers and recommend rejection.
This paper proposes and evaluates using graph convolutional networks for semi supervised learning of probability distributions (histograms). The paper was reviewed by three experts, all of whom gave a Weak Reject rating. The reviewers acknowledged the strengths of the paper, but also had several important concerns including quality of writing and significance of the contribution, in addition to several more specific technical questions. The authors submitted a response that addressed these concerns to some extent. However, in post rebuttal discussions, the reviewers chose not to change their ratings, feeling that quality of writing still needed to be improved and that overall a significant revision and another round of peer review would be needed. In light of these reviews, we are not able to recommend accepting the paper, but hope the authors will find the suggestions of the reviewers helpful in preparing a revision for another venue. 
There was some support for this paper, but it was on the borderline and significant concerns were raised. It did not compare to the exiting related literature on communications, compression, and coding. There were significant issues with clarity.
This paper proposes an analysis of signSGD in some special cases. SignGD has been shown to be of interest, whether because of its similarity to Adam or in quasi convex settings.  The complaint shared by reviewers was the strength of the conditions. SGC is really strong, I have yet to see increasing mini batch sizes to be used in practice (although there are quite a  few papers mentioning this technique to get a convergence rate) and the strength of the other two are harder to assess. With that said, the improvement compared to existing work such as Karimireddy et. al. 2019 is unclear.  I encourage the authors to address the comment of the reviewers and to submit an improved version to a later, or perhaps to a more theoretical, convergence.
The paper proposes and validates a simple idea of training a neural network for a parametric family of losses, using a popular AdaIN mechanism. Following the rebuttal and the revision, all three reviewers recommend acceptance (though weakly). There is a valid concern about the overlap with an ICLR19 workshop paper with essentially the same idea, however the submission is broader in scope and validates the idea on several applications.
This paper presents a dataset, created using a combination of existing resources, crowdsourcing, and model based filtering, that aims to tests models  understanding of typical progressions of events in everyday situations. The dataset represents a challenge for a range of state of the art models for NLP and commonsense reasoning, and also can be used productively as a training task in transfer learning.  After some discussion, reviewers came to a consensus that this represents an interesting contribution and a potentially valuable resource. There were some concerns—not fully resolved—about the implications of using model based filtering during data creation, but these were not so serious as to invalidate the primary contributions of the paper.  While the thematic fit with ICLR is a bit weak—the primary contribution of the paper appears to be a dataset and task definition, rather than anything specific to representation learning—there are relevant secondary contributions, and I think that this work will be practically of interest to a reasonable fraction of the ICLR audience. 
The authors propose an intriguing way to designing competitive online algorithms. However, the state of the paper and the provided evidence of the success of the proposed methodology is too preliminary to merit acceptance.
The paper proposes a meta learning algorithm to learn the divergence measure of variational inference as well as the initialization of the variational parameters (which reduces optimization steps of VI). Improved performance by the learned divergence against hand designed ones are empirically shown on: Gaussian mixture approximation, Bayesian neural regression, and p VAE based recommender. Reviewers initally raised some concerns on hyperparameters selection, weakness of experiments, and motivation for the proposed scheme. The authors responded by adding additional experiments (MNIST) as well as some new sections in their appendix about details of their method or the baselines. The reviewers greatly appreciated the response and commonly believed that the revised version is significantly improved over the initial draft  and the improvements of the draft. As a result of that, some reviewers increased their scores. However, some of their concerns did not resolve. In particular, R1 questions the impact of the work and importance of learning divergence measure (referring to GAN or VQ VAE for obtaining realistic samples). Also R1 finds evaluation based on MNIST unsatisfactory, as it is commonly considered as a toy dataset. To motivate the method, it is suggested that the authors think about real applications which can highlight the benefits of their method in practice. Similar concerns are shared by R2 after authors  response. In particular, R2 is not convinced about motivation and the necessity of using meta learning for learning the divergence. I suggest authors improve on issues around motivation and support the impact of their scheme in a more practical setting.
Gradient clipping is increasingly popular and it s nice to see a paper theoretically exploring its nice performance. All reviewers appreciated the work and the results.  Please make sure to incorporate all of their comments for the final version.
The paper pursues an interesting approach, but requires additional maturation.  The experienced reviewers raise several concerns about the current version of the paper.  The significance of the contribution was questioned.  The paper missed key opportunities to evaluate and justify critical aspects of the proposed approach, via targeted ablation and baseline studies.  The quality and clarity of the technical exposition was also criticized.  The comments submitted by the reviewers should help the authors strengthen the paper. 
The paper extracts feature interactions in recommender systems and studies the effect of these interactions on the recommendations. While the focus is on recommender systems the authors claim that the ideas can be generalised to other domains also.   All reviewers found the empirical results and analysis thereof to be very interesting and useful. This paper saw a healthy discussion between the authors and reviewers and all reviewers agreed that this paper makes a useful contribution. I recommend that the authors address all the concerns of the reviewers in the final version of the paper. 
The paper proposed what is termed Relational State Space Model (R SSM) that can be used for modeling interacting time series data. The model essentially consists of a set of (nonlinear) state space models whose states are jointly evolved in a way that take into account a known interaction structure between them (the relational part, even though technically it is just a coupling structure   the term relational structure in the past has been used for models with objects and classes, for example see the difference between "coupled HMM" vs "relational HMM"). The authors also proposed a graph normalizing flow operation to model the joint state evolution. The main weakness of the paper is in the complexity of the model. However, from a modeling point of view, R SSM seems suitable in situation when the interaction structure is known, and this is demonstrated in the experimental results when comparing against the baselines. 
This paper presents a generalization bound for RNNs based on matrix 1 norm and Fisher Rao norm. As the initial bound relies on non signularity of input covariance, which may not always hold in practice, the authors present additional analysis by noise injection to ensure covariance is positive definite. Through the resulted bound, the paper discusses how weight decay and gradient clipping in the training can help generalization. There were some concerns raised by reviewers, including  rigorous report of the experiment results,  claims on generalization in IMDB experiment,  claims of no explicit dependence on the size of networks, and the relationship of small eigenvalues in input covariance to high frequency features. The authors responded to these and also revised their draft to address most of these concerns (in particular, authors added a new section in the appendix that includes additional experimental results). Reviewers were mainly satisfied with the responses and the revision, and they all recommend accept. 
This paper presents a new benchmark for architecture search. Reviewers put this paper in the top tier. I encourage the authors to also cite https://openreview.net/forum?id SJx9ngStPH in their final version. 
This paper proposes a load balanced hashing called AHash that balances the load of hashing bins to avoid empty bins that appear in some minwise hashing methods. Reviewers found the work interesting and well motivated. Authors addressed some clarity issues in their rebuttal. However the impact appeared quite limited, and the experimental validation limited to few realistic experiments that did not alleviate this concern. We thus recommend rejection.
This paper proposed to improve the quality of underwater images, specifically color distortion and haze effect, by an unsupervised generative adversarial network (GAN). An end to end autoencoder network is used to demonstrate its effectiveness in comparing to existing works, while maintaining scene content structural similarity. Three reviewers unanimously rated weak rejection. The major concerns include unclear difference with respect to the existing works, incremental contribution, low quality of figures, low quality of writing, etc. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.
Two reviewers are negative on this paper while the other reviewer is slightly positive. Overall, the paper does not make the bar of ICLR. A reject is recommended.
This paper proposes a model based policy optimization approach that uses both a policy and model to plan online at test time. The paper includes significant contributions and strong results in comparison to a number of prior works, and is quite relevant to the ICLR community. There are a couple of related works that are missing [1,2] that combine learned policies and learned models, but generally the discussion of prior work is thorough. Overall, the paper is clearly above the bar for acceptance.  [1] https://arxiv.org/pdf/1703.04070.pdf [2] https://arxiv.org/pdf/1904.05538.pdf
The paper studies the role of entropy in maximum entropy RL, particularly in soft actor critic, and proposes an action normalization scheme that leads to a new algorithm, called Streamlined Off Policy (SOP), that does not maximize entropy, but retains or exceeds the performance of SAC. Independently from SOP, the paper also introduces Emphasizing Recent Experience (ERE) that samples minibatches from the replay buffer by prioritizing the most recent samples. After rounds of discussion and a revised version with added experiments, the reviewers viewed ERE as the main contribution, while had doubts regarding the claimed benefits of SOP. However, the paper is currently structured around SOP, and the effectiveness of ERE, which can be applied to any off policy algorithm, is not properly studied. Therefore, I recommend rejection, but encourage the authors to revisit the work with an emphasis on ERE.
This paper studies learning with noisy labels by integrating the idea of curriculum learning.  All reviewers and AC are happy with novelty, clear write up and experimental results.  I recommend acceptance.  
The paper proposes a text normalisation model for Amharic text. The model uses word classification, followed by a character based GRU attentive encoder decoder model. The paper is very short and does not present reproducible experiments. It also does not conform to the style guidelines of the conference. There has been no discussion of this paper beyond the initial reviews, all of which reject it with a score of 1. It is not ready to publish and the authors should consider a more NLP focussed venue for future research of this kind.  
The authors propose to enforce interpretability and controllability on latent variables, like affect and speaking rate, in a speech synthesis model by training in a semi supervised way, with a small amount of labeled data with the variables of interest labeled.   The idea is sensible and the results are very encouraging, and the authors have addressed the initial concerns brought up by the reviewers.
This work proposes a VAE based model for learning transformations of sequential data (the main here intuition is to have the model learn changes between frames without learning features that are constant within a time sequence). All reviewers agreed that this is a very interesting submission, but have all challenged the novelty and rigor of this paper, asking for more experimental evidence supporting the strengths of the model. After having read the paper, I agree with the reviewers and I currently see this one as a weak submission without potentially comparing against other models or showing whether the representations learned from the proposed model lead in downstream improvements in a task that uses this representations.
This paper proposed to use an autoencoder based approach for anomaly localization. The method shows promising on inpainting task compared with traditional auto encoder.  First two reviewers recommend this paper for acceptance. The last review has some concerns about the experimental design and whether VAE is a suitable baseline. The authors provide reasonable explanation in rebuttal while the reviewer did not give further comments.  Overall, the paper proposes a promising approach for anomaly localization; thus, I recommend it for acceptance. 
This manuscript analyzes the convergence of federated learning wit hstragellers, and provides convergence rates. The proof techniques involve bounding the effects of the non identical distribution due to stragglers and related issues. The manuscript also includes a thorough empirical evaluation. Overall, the reviewers were quite positive about the manuscript, with a few details that should be improved. 
This is a mostly theoretical paper concerning online and stochastic optimization for convex loss functions that are not Lipschitz continuous. The authors propose a method for replacing the Lipschitz continuity condition with a more general Riemann Lipschitz continuity condition, under which they are able to provide regret bounds for the online mirror descent algorithm, as well as extending to the stochastic setting. They follow up by evaluating their algorithm on Poisson inverse problems.   The reviewers all agree that this is a well written paper that makes a clear contribution. To the best of our knowledge, the theory and derivations are correct, and the authors were highly responsive to reviewers’ (minor) comments. I’m therefore happy to recommend acceptance.
This paper presents a new generative modeling approach to transform between data domains via a neuron editing technique. The authors address the scenario of source to target domain translation that can be applied to a new source domain. While the reviewers acknowledged that the idea of neuron editing is interesting, they have raised several concerns that were viewed by AC as critical issues: (1) given the progress that have been made in the field, an empirical comparison with SOTA GANs models is required to assess the benefits/competitiveness of the proposed approach   see R1’s comments, also [StarGAN by Choi et al, CVPR 2018], (2) the literature review is incomplete and requires a major revision   see R1’s and R3’s suggestions, also [CYCADA by Hoffman et al, ICML 2018], (3) presentation clarity   see R1’s and R2’s comments. AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews are useful for improving and revising the paper. 
This paper presents a model for building sentence embeddings using a generative transformer model that encoders separately semantic aspects (that are common across languages)  and language specific aspects. The authors evaluate their embeddings in a non parametric way (i.e., on STS tasks by measuring cosine similarity) and find their method to outperform other sentence embeddings methods. The main concern that both reviewers (and myself) have about this work relates to its evaluation part. While the authors present a set of very interesting difficult evaluation and probing splits aiming at quantifying the linguistic behaviour of their model, it is unsatisfying the fact that the authors do not evaluate their model extensively in standard classification embedding benchmarks (e.g., as in GLUE). The authors comment: “[their model in producing embeddings] it isn’t as strong when using classification for final predictions. This indicates that the embeddings learned by our approach may be most useful when no downstream training is possible”. If this is true, why is it the case and isn’t it quite restrictive? I think this work is interesting with a nice analysis but the current empirical results are borderline  (yes, the model is better on STS, but this is quite limited of an idea compared to using these embeddings as features in a classification tasks). As such, I do not recommend this paper for acceptance but I do hope that authors will keep improving their method and will make it work in more general problems involving classification tasks.
The paper combines graph convolutional networks with noisy label learning. The reviewers feel that novelty in the work is limited and there is a need for further experiments and  extensions. 
The paper addresses the problem of learning disentangled representations in supervised and unsupervised settings.   In general, the problem of representation learning in of course a core problem in ICLR. However, in the set up described by the authors, R2 commented on the the set up for supervised being a bit unnatural in as detailed labels need to be given (somewhat confusingly, the labels are called control variates in the paper).  Several reviewers commented on the novelty of the paper being on the low side, with R2 commenting the contribution being fairly small, and R3 noting similarities to stackgan.  There were also some comments on quality, and clarity. On the topic of technical quality, R2 did note that the authors present extensive results, but R3 mentions that the case for the disentanglement improving is not sufficiently supported. In terms of clarity, there was some initial confusing about e.g. the inference procedure, though the authors addressed these issues in the discussion. 
The paper proposes an approach for unsupervised learning of keypoint landmarks from images and videos by decomposing them into the foreground and static background. The technical approach builds upon related prior works such as Lorenz et al. 2019 and Jakab et al. 2018 by extending them with foreground/background separation. The proposed method works well for static background achieving strong pose prediction results. The weaknesses of the paper are that (1) the proposed method is a fairly reasonable but incremental extension of existing techniques; (2) it relies on a strong assumption on the property of static backgrounds; (3) video prediction results are of limited significance and scope. In particular, the proposed method may work for simple data like KTH but is very limited for modeling videos as it is not well suited to handle moving backgrounds, interactions between objects (e.g., robot arm in the foreground and objects in the background), and stochasticity. 
While considerable effort went into improving the paper during the author response period, the concerns outlined by reviewer 2 remain and the aggregate score across reviewers reflects this issue. The AC recommends rejection with strong encouragement to resubmit this work to another high quality venue upon further revision to the work.
This paper proposes to apply regularizers such as weight decay or weight noise only periodically, rather than every epoch. It investigates how the "non regularization period", or period between regularization steps, interacts with other hyperparameters.   Overall, the writing feels somewhat scattered, and it is hard to identify a clear argument for why the NRP should help. Certainly one could save computation this way, but regularizers like weight decay or weight noise incur only a small computational cost anyway. One explicit claim from the paper is that a higher NRP allows larger regularization. There s a sense in which this is demonstrated, though not a very interesting sense: Figure 4 shows that the weight decay strength should be adjusted proportionally to the NRP. But varying the parameters in this way simply results in an unbiased (but noisier) estimate of gradients of exactly the same regularization penalty, so I don t think there s much surprising here.  Similarly, Section 3 argues that a higher NRP allows for larger stochastic perturbations, which makes it easier to escape local optima. But this isn t demonstrated experimentally, nor does it seem obvious that stochasticity will help find a better local optimum.  Overall, I think this paper needs substantial cleanup before it s ready to be published at a venue such as ICLR. 
The paper aims to generate molecules with desired properties using a variant of supervised variational auto encoders. Disentanglement is encouraged among the style factors.  The reviewers point out that the idea is nice, but authors avoid quantitative comparison with SotA Graph based generative models.  Especially, the JT VAE is acknowledged as a strong baseline widely in the community and is a VAE based model, it is important to do these comparisons. 
Main content:  Blind review #1 summarizes it well:  Recently many language GAN papers have been published to overcome the so called exposure bias, and demonstrated improvements  in natural language generation in terms of sample quality, some works propose to assess the generation in terms of diversity, however, quality and diversity are two conflicting measures that are hard to meet. This paper is a groundbreaking work that proposes receiver operating curve or Pareto optimality for quality and diversity measures, and shows that simple temperature sweeping in MLE generates the best quality diversity curves than all language GAN models through comprehensive experiments. It points out a good target that language GANs should aims at.      Discussion:  The main reservation was the originality of the idea of using temperature sweep in the softmax. However, it turns out this idea came from the authors in the first place, which they have not been able to state directly due to the anonymity requirement. Per the program chair s instruction to direct this to the area chair, I think this has been handled correctly.     Recommendation and justification:  This paper should be accepted. It provides readers with insight in that it illuminates a misconception of how important exposure bias has been assumed to be, and provides a less expensive MLE based way to train than GAN counterparts.
This paper uses GAN for data augmentation to improve the performance of knowledge distillation.  Reviewers and AC commonly think the paper suffers from limited novelty and insufficient experimental supports/details.  Hence, I recommend rejection.
This paper has been assessed by three reviewers scoring it as follows: 6, 3, 8. The submission however attracted some criticism post rebuttal from the reviewers e.g., why concatenating teacher to student is better than the use l2 loss or how the choice of transf. layers has been made (ad hoc). Similarly, other major criticism includes lack of proper referencing to parts of work that have been in fact developed earlier in preceding papers. On balance, this paper falls short of the expectations of ICLR 2020, thus it cannot be accepted at this time. The authors are encouraged to work through major comments and resolve them for a future submission.
The paper introduces a distributed algorithm for training deep nets in clusters with high latency (i.e. very remote) nodes. While the motivation and clarity are the strengths of the paper, the reviewers have some concerns regarding novelty and insufficient theoretical analysis. 
While the reviewers appreciated the ideas presented in the paper and their novelty, there were major concerns raised about the experimental evaluation. Due to the serious doubts that the reviewers raised about the effectiveness of the proposed approach, I do not think that the paper is quite ready for publication at this time, though I would encourage the authors to revise and resubmit the work at the next opportunity.
This paper proposes a CNN that is invariant to input transformation, by making two modifications on top of the TI pooling architecture: the input dependent convolutional filters, and a decoder network to ensure fully transformation invariant. Reviewer #1 concerns the limited novelty, unconvincing experimental results. Reviewer #2 praises the paper being well written, but is not convinced by the significance of the contributions. The authors respond to Reviewer #2 but did not change the rating. Reviewer #3 especially concerns that the paper is not well positioned with respect to the related prior work.  Given these concerns and overall negative rating (two weak reject and one reject), the AC recommends reject.
This paper proposes a modification to GCNs that generalizes the aggregation step to multiple levels of neighbors, that in theory, the new class of models have better discriminative power. The main criticism raised is that there is lack of sufficient evidence to distinguish this works theoretical contribution from that of Xu et al. Two reviewers also pointed out the concerns around experiment results and suggested to includes more recent state of the art SOTA results. While authors disagree that the contributions of their work is incremental, reviewers concerns are good samples of the general readers of this paper— general readers may also read this paper as incremental. We highly encourage authors to take another cycle of edits to better distinguish their work from others before future submissions.  
This paper offers an innovative approach to adjusting style transfer parameters. The reviewers were consistent, and all recommend acceptance.  I concur.  
This paper tackles the interesting problem of meta learning in problem spaces where training "tasks" are scarce.  Two criticisms that seems to shared across reviewers are that (i) it is debatable how "novel" the space of meta learning with "few" tasks is, especially since there aren t established standard for how many training tasks should be available, and (ii) the paper could use more comparisons with baseline methods and ablations to understand the contributions.  As an AC, I down weight criticism (i) because I don t feel the paper has to be creating a new problem definition; it s acceptable to make advances within an existing space.  However, criticism (ii) seems to remain.  After conferring with reviewers it seems that the rebuttal was not strong enough to significantly alter the reviewer s opinions on this issue, and so the paper does not have enough support to justify acceptance.  The paper certainly addresses interesting issues, and I look forward to seeing a revised/improved version at another venue. 
While the reviewers agreed that the problem of learning robust policies is an important one, there were a number of major concerns raised about the paper, and as a result I would recommend that the paper not be accepted at this time. The important points are: (1) limited novelty in light of prior work in this area (see R2 and R3); (2) a number of missing comparisons (see R2). There is also a bit of confusion in the reviews, which I think stems from a somewhat unclear statement in the paper of the problem formulation. While there is nothing wrong with assuming access to a parameterized simulator and studying robustness under parametric variation, this is of course a much stronger assumption than some prior work on robust reinforcement learning. Clarity on this point is crucial, and there are a large number of prior methods that can likely do well in this setting (e.g., based on system ID, etc.).
This paper proposes to represent the distribution w.r.t. which neural architecture search (NAS) samples architectures through a variational autoencoder, rather than through a fully factorized distribution (as previous work did).   In the discussion, a few things improved (causing one reviewer to increase his/her score from 1 to 3), but it became clear that the empirical evaluation has issues, with a different search space being used for the method than for the baselines. There was unanimous agreement for rejection. I agree with this judgement and thus recommend rejection.
This paper received two weak and one strong reject from the reviewers.  The major issues cited were 1) a lack of strong enough baselines or empirical results, 2) Novelty with respect to "Certified adversarial robustness via randomized smoothing" and 3) a limitation to Gaussian noise perturbations.  Unfortunately, as a result the reviewers agreed that this work was not ready for acceptance.  Adding stronger empirical results and a careful treatment of related work would make this a much stronger paper for a future submission.
This paper proposes a neural network model for predicting multi aspect sentiment and generating masks that can justify the predictions. The positive aspects of the paper include improved results over the state of the art.  Reviewers found the technical novelty limited, and the experiments short of being fully convincing. After the author rebuttal, there were discussions between the reviewers and the AC, and the reviewers still thought the paper is not fully convincing given these limitations.  I thank the authors for their submission and detailed responses to the reviewers and hope to see this research in a future venue.
Unfortunately the paper is confusingly written, and there is only agreement by all reviewers on the rejection of the paper.  Indeed, if all reviewers and the area chair do not interpret the paper well, the authors  best response would be to rewrite the papers rather than disagree with all reviewers.  In the area chair s opinion, the current form the paper does not merit publication.  The authors are advised to address the reviewers  concerns, rework the paper, and submit to a conference again.
The authors take a closer look at widely held beliefs about neural networks. Using a mix of analysis and experiment, they shed some light on the ways these assumptions break down. The paper contributes to our understanding of various phenomena and their connection to generalization, and should be a useful paper for theoreticians searching for predictive theories.
This paper tackles the problem of transferring learning between tasks when performing Bayesian hyperparameter optimization. In this setting, tasks can correspond to different datasets or different metrics. The proposed approach uses Gaussian copulas to synchronize the different scales of the considered tasks and uses Thompson Sampling from the resulting Gaussian Copula Process for selecting next hyperparameters.  The main weakness of the paper resides in the concerns raised about the experiments. First, the results are hard to interpret, leading to a misunderstanding of performances. Moreover, the considered baselines may not be adapted (they may be trivial). This might be due to a misunderstanding of the paper, which would align with the third major concern, that is the lack of clarity. These points could be addressed in a future version of the work, but it would need to be reviewed again and therefore would be too late for the current camera ready.  Hence, I recommend rejecting this paper.
The paper proposes a new problem setting of predicate zero shot learning for visual relation recognition for the setting when some of the predicates are missing, and a model that is able to address it.  All reviewers agreed that the problem setting is interesting and important, but had reservations about the proposed model. In particular, the reviewers were concerned that it is too simple of a step from existing methods. One reviewer also pointed towards potential comparisons with other zero shot methods.  Following that discussion, I recommend rejection at this time but highly encourage the authors to take the feedback into account and resubmit to another venue.
The paper focuses on characterizing the expressiveness of graph neural networks. The reviewers were satisfied that the authors answered their questions suffciiently and uniformly agree that this is a strong paper that should be accepted.
The paper proposes a representation learning objective that makes it  amenable to planning,   The initial submission contained clear holes, such as missing related work and only containing very simplistic baselines. The authors have substantially updated the paper based on this feedback, resulting in a clear improvement.  Nevertheless, while the new version is a good step in the right direction, there is some additional work needed to fully address the reviewers  complaints. For example, the improved baselines are only evaluated in the most simple domain, while the more complex domains still only contain simplistic baselines that are destined to fail. There are also some unaddressed questions regarding the correctness of Eq. 4. Finally, the substantial rewrites have given the paper a less than polished feel.  In short, while the work is interesting, it still needs a few iterations before it s ready for publication.
This paper presents a novel black box adversarial attack algorithm, which exploits a sign based rather than magnitude based, gradient estimator for black box optimization. It also adaptively constructs queries to estimate the gradient. The proposed approach outperforms many state of the art black box attack methods in terms of  query complexity. There is a unanimous agreement to accept this paper.
What is investigated is what kind of representations are formed by embodied agents; it is argued that these are different than from non embodied arguments. This is an interesting question related to foundational AI and Alife questions, such as the symbol grounding problem. Unfortunately, the empirical investigations are insufficient. In particular, there is no comparison with a non embodied control condition. The reviewers point this out, and the authors propose a different control condition, which unfortunately is not sufficient to test the hypothesis.  This paper should be rejected in its current form, but the question is interesting and hopefully the authors will do the missing experiments and submit a new version of the paper.
This paper presents an encoder decoder based approach to construct a compressed latent space representation of each molecule. Then a second neural network segments the output and assigns an atomic number. Unlike previous works using 1D or 2D representations, the proposed method focuses on the 3D representations.  The reviewers have several major concerns. Firstly, the novelty of the paper seems to be limited as the proposed method mainly use the existing techniques. Secondly, there is no clear baseline to compare with. Finally, there is no clear quantitative results to measure the proposed method. The rebuttal did not well address these problems.  Overall, this paper did not meet the standard of ICLR and I choose to reject the paper. 
This paper describes a method for learning compact RL policies suitable for mobile robotic applications with limited storage.  The proposed pipeline is a scalable combination of efficient neural architecture search (ENAS) and evolution strategies (ES).  Empirical evaluations are conducted on various OpenAI Gym and quadruped locomotion tasks, producing policies with as little as 10s of weight parameters, and significantly increased compression reward trade offs are obtained relative to some existing compact policies.  Although reviewers appreciated certain aspects of this paper, after the rebuttal period there was no strong support for acceptance and several unsettled points were expressed.  For example, multiple reviewers felt that additional baseline comparisons were warranted to better calibrate performance, e.g., random coloring, wider range of generic compression methods, classic architecture search methods, etc.  Moreover, one reviewer remained concerned that the scope of this work was limited to very tiny model sizes whereby, at least in many cases, running the uncompressed model might be adequate.
Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera ready submission.
This paper proposed a semi supervised few shot learning method, on top of Prototypical Networks, wherein a regularization term that involves a random walk from a prototype to unlabeled samples and back to the same prototype.  SotA results were obtained in several experiments by using this method.  All reviewers agreed that the novelty of the paper is not such high compared with Haeusser et al. (2017) and the analysis and the experiments could be improved.
All reviewers recommend reject, and there is no rebuttal.
The paper studies how the size of the initialization of neural network weights affects whether the resulting training puts the network in a "kernel regime" or a "rich regime". Using a two layer model they show, theoretically and practically, the transition between kernel and rich regimes. Further experiments are provided for more complex settings.  The scores of the reviewers were widely spread, with a high score (8) from a low confidence reviewer with a very short review. While the authors responded to the reviewer comments, two of the reviewers (importantly including the one recommending reject) did not further engage.  Overall, the paper studies an important problem, and provides insight into how weight initialization size can affect the final network. Unfortunately, there are many strong submissions to ICLR this year, and the submission in its current state is not yet suitable for publication.
The authors present a combination of few shot learning with one class classification model of problems. The authors use the existing MAML algorithm and build upon it to present a learning algorithm for the problem. As pointed out by the reviewers, the technical contributions of the paper are quite minimal and after the author response period the reviewers have not changed their minds. However, the authors have significantly changed the paper from its initial submission and as of now it needs to be reviewed again. I recommend authors to resubmit their paper to another conference. As of now, I recommend rejection.
This paper investigates a promising direction on the important topic of interpretability; the reviewers find a variety of issues with the work, and I urge the authors to refine and extend their investigations.
The paper combines several recent optimizer tricks to provide empirical evidence that goes against the common belief that adaptive methods result in larger generalization errors. The contribution of this paper is rather small: no new strategies are introduced and no new theory is presented. The paper makes a good workshop paper, but does not meet the bar for publication at ICLR. 
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
All reviewers rated this paper as a weak reject. The author response was just not enough to sway any of the reviewers to revise their assessment. The AC recommends rejection.
The paper describes an approach for learning context dependent entity representations that encodes fine grained entity types. The paper includes some good empirical results and observations, but the proposed approach is very simple but lacks technical novelty needed to top ML conference; the clarify of the presentation can also be improved. 
This paper considers the information plane analysis of DNNs. Estimating mutual information is required in such analysis which is difficult task for high dimensional problems. This paper proposes a new "matrix–based Renyi’s entropy coupled with ´tensor kernels over convolutional layers" to solve this problem. The methods seems to be related to an existing approach but derived using a different "starting point". Overall, the method is able to show improvements in high dimensional case.  Both R1 and R3 have been critical of the approach. R3 is not convinced that the method would work for high dimensional case and also that no simulation studies were provided. In the revised version the authors added a new experiment to show this. R3 s another comment makes an interesting point regarding "the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities mutual information seems like a leap that s not justified in the paper." I could not find an answer in the rebuttal regarding this.  R1 has also commented that the contribution is incremental in light of existing work. The authors mostly agree with this, but insist that the method is derived differently.  Overall, I think this is a reasonable paper with some minor issues. I think this can use another review cycle where the paper can be improved with additional results and to take care of some of the doubts that reviewers  had this time.   For now, I recommend to reject this paper, but encourage the authors to resubmit at another venue after revision.
(I acknowledge reading authors  recent note on decaNLP.)  This paper proposes a span extraction approach (SpExBERT) to unify question answering, text classification and regression. Paper includes a significant number of experiments (including low resource and multi tasking experiments) on multiple benchmarks. The reviewers are concerned about lack of support on author s claims from the experimental results due to seemingly insignificant improvements and lack of analysis regarding the results. Hence, I suggest rejecting the paper.
The paper proposes a very simple but thoroughly evaluated and investigated idea for improving generalization in GCNs. Though the reviews are mixed, and in the post rebuttal discussion the two negative reviewers stuck to their ratings, the area chair feels that there are no strong grounds for rejection in the negative reviews. Accept.
The paper introduces a non stationary bandit strategy for adapting the exploration rate in Deep RL algorithms. They consider exploration algorithms with a tunable parameter (e.g. the epsilon probability in epsilon greedy) and attempt to adjust this parameter in an online fashion using a proxy to the learning progress. The proposed approach is empirically compared with using fixed exploration parameters and adjusting the parameter using a bandit strategy that doesn t model the learning process.  Unfortunately, the proposed approach is not theoretically grounded and the experiments lack comparison with good baselines in order to be convincing. A comparison with other, provably efficient, non stationary bandit algorithms such as exponential weight methods (Besbes et al 2014) or Thompson sampling (Raj & Kalyani 2017), which are cited in the paper, is missing. Moreover, given the whole set of results and how they are presented, the improvement due to the proposed method is not clear. In light of these concerns I recommend to reject this paper.
Two reviewers are negative on this paper while the other one is slightly positive. Overall, the paper does not make the bar of ICLR and thus a reject is recommended.
The idea of integrating causality into an auto encoder is interesting and very timely. While the reviewers find this paper to contain some interesting ideas, the technical contributions and mathematical rigor, scope of the method, and the presentation of results would need to be significantly improved in order for this work to reach the quality bar of ICLR.
This paper studies the problem of mode collapse in GANs. The authors present new metrics to judge the model s diversity of the generated faces. The authors present two black box approaches to increasing the model diversity. The benefit of using a black box approach is that the method does not require access to the weights of the model and hence it is more easily usable than white box approaches. However, there are significant evaluation problems and lack of theoretical and empirical motivation on why the methods proposed by the paper are good. The reviewers have not changed their score after having read the response and there is still some gaps in evaluation which can be improved in the paper. Thus, I m recommending a Rejection.
This paper proposes methodology to train binary neural networks.  The reviewers and authors engaged in a constructive discussion. All the reviewers like the contributions of the paper.  Acceptance is therefore recommended.
This paper provides a fresh application of tools from causality theory to investigate modularity and disentanglement in learned deep generative models. It also goes one step further towards making these models more transparent by studying their internal components. While there is still margin for improving the experiments, I believe this paper is a timely contribution to the ICLR/ML community. This paper has high variance in the reviewer scores. But I believe the authors did a good job with the revision and rebuttal. I recommend acceptance.
This paper introduces MELEE, a meta learning procedure for contextual bandits. In particular, MELEE learns how to explore by training on datasets with full information about what every reward each action would obtain (e.g., using classification datasets). The idea is strongly related to imitation learning, and a regret bound is demonstrated for the procedure that comes from that literature. Experiments are performed.   Perhaps due to the generality in which the algorithm was presented, reviewers found some parts of the work unintuitive and difficult to follow. The work may greatly benefit from having an explicit running example for F and pi and how it evolves during training. Some reviewers were not impressed by the experimental results relative to epsilon greedy. Yes, epsilon greedy is a strong baseline, but MELEE introduces significant technical debt and data infrastructure so it seems fair to expect a sizable bump over epsilon greedy or else why is it worth it?  Perhaps with revisions and experiments within a domain that justify its complexity, this paper may be suitable at another venue. But it is not deemed acceptable at this time, Reject.  
The paper introduces a generative approach to reconstruct 3D images for cryo electron microscopy (cryo EM).  All reviewers really liked the paper, appreciate the challenging problem tackled and the proposed solution.  Acceptance is therefore recommended. 
This paper investigates gradient sparsification using top k for distributed training. Starting with empirical studies, the authors propose a distribution for the gradient values, which is used to derive bounds on the top k sparsification. The top k approach is further improved using a procedure that is easier to parallelize.  The reviewers and AC agree that the problem studied is timely and interesting. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the rigor and novelty of the results, and perhaps issues with unstated assumptions. In reviews and discussion, the reviewers also noted issues with clarity of the presentation, some of which were corrected after rebuttal. In the opinion of the AC, the manuscript is not appropriate for publication in its current state. 
This paper was assessed by three reviewers who scored it as 6/1/6. The main criticism included somewhat weak experiments due to the manual tuning of bandwidth, the use of old (and perhaps mostly solved/not challenging) datasets such as Mnist and Cifar10, lack of ablation studies. The other issue voiced in the review is that the proposed method is very close to a MMD GAN with a kernel plus random features. Taking into account all positives and negatives, we regret to conclude that this submission falls short of the quality required by ICLR2020, thus it cannot be accepted at this time.  
The authors introduce the idea of using Wasserstein distances over latent "behavioral spaces" to measure the similarity between two polices, for use in RL algorithms.  Depending on the choice of behavioral embedding, this method produces different regularizers for policy optimization, in some cases recovering known algorithms such as TRPO.  This approach generalizes ideas of similarity used in many common algorithms like TRPO, making these ideas widely applicable to many policy optimization approaches.  The reviewers all agree that the core idea is interesting and would likely be useful to the community.  However, a primary concern that was not sufficiently resolved during the rebuttal period was the experimental evaluation   both the ability of the experiments to be replicated, as well as whether they provide sufficient insight into how/why the algorithm performs.  Thus, I recommend rejection of this paper at this time.
This paper proposes a novel method for considering translations in both directions within the framework of generative neural machine translation, significantly improving accuracy.  All three reviewers appreciated the paper, although they noted that the gains were somewhat small for the increased complexity of the model. Nonetheless, the baselines presented are already quite competitive, so improvements on these datasets are likely to never be extremely large.  Overall, I found this to be a quite nice paper, and strongly recommend acceptance, perhaps as an oral presentation.
The paper presented a detailed discussion on the implementation of a library emulating Atari games on GPU for efficient reinforcement learning. The analysis is very thoroughly done. The major concern is whether this paper is a good fit to this conference. The developed library would be useful to researchers and the discussion is interesting with respect to system design and implementation, but the technical depth seems not sufficient.
This submission proposes an interesting experiment/modification of CNNs. However, it looks like this contribution overlaps significantly with prior work (that the authors initially missed) and the comparison in the (revised) manuscript seem to not clearly delineate and acknowledge the similarities and differences.  I suggest the authors improve this aspect and try submitting this work to next venue. 
The authors leverage advances in semi supervised learning and data augmentation to propose a method for active learning. The AL method is based on the principle that a model should consistently label across perturbation/augmentations of examples, and thus propose to choose samples for active learning based on how much the estimated label distribution changes based on different perturbations of a given example. The method is intuitive and the experiments provide some evidence of efficacy. However, during discussion there was a lingering question of novelty that eventually swayed the group to reject this paper. 
This paper proposes an interpretable machine learning method, ProtoAttend, that bases decisions on few relevant "prototypes." The proposed method uses an attention mechanism (possibly sparse, via sparsemax) that relates the encoded representations to samples in order to determine prototypes. The resulting model enables similarity based interpretability, confidence estimation by quantifying the mismatch across prototype labels, and can be used for distribution mismatch detection.   While the proposed model is interesting, the reviewers raised several concerns regarding the choice of prototypes and the evaluation of human interoperation. The paper would benefit from more experiments besides the provided user studies to check if the provided prototypes can help human users correctly guess the model prediction. I encourage the authors to address these suggestions in a future resubmission.
This paper proposes a tool to visualizing the behaviour of deep RL agents, for example to observe the behaviour of an agent in critical scenarios. The idea is to learn a generative model of the environment and use it to artificially generate novel states in order to induce specific agent actions. States can then be generated such as to optimize a given target function, for example states where the agent takes a specific actions or states which are high/low reward. They evaluate the proposed visualization on Atari games and on a driving simulation environment, where the authors use their approach, to investigate the behaviour of different deep RL agents such as DQN.  The paper is very controversial. On the one hand, as far as we know, this is the first approach that explicitly generates states that are meant to induce specific agent behaviour, although one could relate this to adversarial samples generation. Interpretability in deep RL is a known problem and this work could bring an interesting tool to the community. However, the proposed approach lacks theoretical foundations, thus feels quite ad hoc, and results are limited to a qualitative, visual, evaluation. At the same time, one could say that the approach is not more ad hoc than other gradient saliency visualization approaches, and one could argue that the lack of theoretical soundness is due to the difficulty of defining good measures of interpretability and that apply well to image based environments.  Nonetheless, this paper is a step in the good direction in a field that could really benefit from it. 
The paper proposes a gradient rescaling method to make deep neural network training more robust to label noise. The intuition of focusing more on easier examples is not particularly new, but empirical results are promising. On the weak side, no theoretical justification is provided, and the method introduces extra hyperparameters that need to be tuned. Finally, more discussions on recent SOTA methods (e.g., Lee et al. 2019) as well as further comprehensive evaluations on various cases, such as asymmetric label noise, semantic label noise, and open set label noise, would be needed to justify and demonstrate the effectiveness of the proposed method. 
The authors develop a certified defense for label flipping attacks (where an adversary can flip labels of a small number of training set samples) based on the randomized smoothing technique developed for certified defenses to adversarial perturbations of the input. The framework applies to least squares classifiers acting on pretrained features learned by a deep network. The authors show that the resulting framework can obtain significant improvements in certified accuracy against targeted label flipping attacks for each test example.  While the paper makes some interesting contributions, the reviewers had the following shared concerns regarding the paper: 1) Reality of threat model: The threat model assumes that the adversary has access to the model and all of the training data (so as to choose which labels to flip), which is very unlikely in practice.  2) Limitation to least squares on pre trained features: The only practical instantiation of the framework presented in the paper is on least squares classifiers acting on pre trained features learned by a deep network.  In the rebuttal phase, the authors clarified some of the more minor concerns raised by the reviewers, but the above concerns remained.  Overall, I feel that this paper is borderline   If the authors extend the applicability of the framework (for example relaxing the restriction on pre training the deep features) and motivating the threat model more strongly, this could be an interesting paper.
This work proposes a new architecture for abstract visual reasoning called "Attention Relation Network" (ARNe), based on Transformer style soft attention and relation networks, which the authors show to improve on the "Wild Relation Network" (WReN). The authors test their network on the PGM dataset, and demonstrate a non trivial improvement over previously reported baselines.   The paper is well written and makes an interesting contribution, but the reviewers expressed some criticisms, including technical novelty, unfinished experiments (and lack of experimental details), and somewhat weak experimental results, which suggest that the proposed ARNe model does not work well when training with weaker supervision without meta targets. Even though the authors addressed some concerns in their revised version (namely, they added new experiments in the extrapolation split of PGM and experiments on the new RAVEN dataset), I feel the paper is not yet ready for publication at ICLR.  
In this paper the authors view meta learning under a general, less studied viewpoint, which does not make the typical assumption that task segmentation is provided. In this context, change point analysis is used as a tool to complement meta learning in this expanded domain.    The expansion of meta learning in this more general and often more practical context is significant and the paper is generally well written. However, considering this particular (non)segmentation setting is not an entirely novel idea; for example the reviewers have already pointed out [1] (which the authors agreed to discuss), but also [2] is another relevant work. The authors are highly encouraged to incorporate results, or at least a discussion, with respect to at least [2]. It seems likely that inferring boundaries could be more powerful, but it is important to better motivate this for a final paper.    Moreover, the paper could be strengthened by significantly expanding the discussion about practical usefulness of the approach. R3 provides a suggestion towards this direction, that is, to explore the performance in a situation where task segmentation is truly unavailable.    [1] Rahaf et el. "Task Free Continual Learning".  [2] Riemer et al. "Learning to learn without forgetting by maximizing transfer and minimizing interference".   
The paper proposes a GAN approach for unsupervised learning of 3d object shapes from natural images. The key idea is a two stage generative process where the 3d shape is first generated and then rendered to pixel level images. While the experimental results are promising, the experimental results are mostly focused on faces (that are well aligned and share roughly similar 3d structures across the dataset). Results on other categories are preliminary and limited, so it s unclear how well the proposed method will work for more general domains. In addition, comparison to the existing baselines (e.g., HoloGAN; Pix2Scene; Rezende et al., 2016) is missing. Overall, further improvements are needed to be acceptable for ICLR.    Extra note: Missing citation to a relevant work Wang and Gupta, Generative Image Modeling using Style and Structure Adversarial Networks https://arxiv.org/abs/1603.05631
The paper introduces a novel and effective approach to policy optimization.  The overall contribution is sufficient to merit acceptance.  Nevertheless, the authors should improve the presentation and experimental evaluation in line with the reviewer criticisms.  The criticisms of AnonReviewer2 in particular should not be neglected.  Regarding the theory, I agree with AnonReviewer3 that the UNOP assumption is too limiting.  The paper would be much stronger if this assumption could be significantly weakened, or better justified.
This paper proposes a framework which qualifies how well given neural architectures can perform on reasoning tasks. From this, they show a number of interesting empirical results, including the ability of graph neural network architectures for learn dynamic programming.  This substantial theoretical and empirical study impressed the reviewers, who strongly lean towards acceptance. My view is that this is exactly the sort of work we should be show casing at the conference, both in terms of focus, and of quality. I am happy to recommend this for acceptance.
This paper proposes a curriculum based reinforcement learning approach to improve theorem proving towards longer proofs. While the authors are tackling an important problem, and their method appears to work on the environment it was tested in, the reviewers found the experimental section too narrow and not convincing enough. In particular, the authors are encouraged to apply their methods to more complex domains beyond Robinson arithmetic. It would also be helpful to get a more in depth analysis of the role of the curriculum. The discussion period did not lead to improvements in the reviewers’ scores, hence I recommend that this paper is rejected at this time.  
This is an interesting contribution that sheds some light on a well studied but still poorly understood problem. I think it might be of interest to the community.
The presented paper gives a differentiable product quantization framework to compress embedding and support the claim by experiments (the supporting materials are as large as the paper itself). Reviewers agreed that the idea is simple is interesting, and also nice and positive discussion appeared. However, the main limiting factor is the small novelty over Chen 2018b, and I agree with that. Also, the comparison with low rank is rather formal: of course it would be of full rank , as the authors claim in the answer, but looking at singular values is needed to make this claim. Also, one can use low rank tensor factorization to compress embeddings, and this can be compared.  To summarize, I think the contribution is not enough to be accepted.
The submission presents an approach to estimating physical parameters from video. The approach is sensible and is presented fairly well. The main criticism is that the approach is only demonstrated in simplistic "toy" settings. Nevertheless, the reviewers recommend (weakly) accepting the paper and the AC concurs.
The paper addresses a video generation setting where both initial and goal state are provided as a basis for long term prediction. The authors propose two types of models, sequential and hierarchical, and obtain interesting insights into the performance of these two models. Reviewers raised concerns about evaluation metrics, empirical comparisons, and the relationship of the proposed model to prior work.  While many of the initial concerns have been addressed by the authors, reviewers remain concerned about two issues in particular. First, the proposed model is similar to previous approaches with sequential latent variable models, and it is unclear how such existing models would compare if applied in this setting. Second, there are remaining concerns on whether the model may learn degenerate solutions. I quote from the discussion here, as I am not sure this will be visible to authors [about Figure 12]: "now the two examples with two samples they show have the same door in the middle frame which makes me doubt the method learn[s] anything meaningful in terms of the agent walking through the door but just go to the middle of the screen every time."
This paper received all negative reviewers, and the scores were kept after the rebuttal. The authors are encouraged to submit their work to a computer vision conference where this kind of work may be more appreciated. Furthermore, including stronger baselines such as Acuna et al is recommended.
This paper studies the robustness of NeuralODE, as well as propose a new variant. The results suggest that the neuralODE can be used as a building block to build robust deep networks. The reviewers agree that this is a good paper for ICLR, and based on their recommendation I suggest to accept this paper.
The paper proposes a combination of a delay embedding as well as an autoencoder to perform representation learning. The proposed algorithm shows competitive performance with deep image prior, which is a convnet structure. The paper claims that the new approach is interpretable and provides explainable insight into image priors.  The discussion period was used constructively, with the authors addressing reviewer comments, and the reviewers acknowledging this an updating their scores.  Overall, the proposed architecture is good, but the structure and presentation of the paper is still not up to the standards of ICLR. The current presentation seems to over claim interpretability, without sufficient theoretical or empirical evidence. 
This paper tackles an important problem: understanding if different NN solutions are similar or different. In the current form, however, the main motivation for the approach, and what the empirical results tell us, remains unclear. I read the paper after the updates and after reading reviews and author responses, and still had difficulty understanding the goals and outcomes of the experiments (such as what exactly is being reported as test accuracy and what is meant by: "High test accuracy means that assumptions are reasonable."). We highly recommend that the authors revisit the description of the motivation and approach based on comments from reviewers; further explain what is reported as test accuracy in the experiments; and more clearly highlight the insights obtain from the experiments. 
This paper proposes using non Euclidean spaces for GCNs, leveraging the gyrovector space formalism. The model allows products of constant curvature, both positive and negative, generalizing hyperbolic embeddings.   Reviewers got mixed impressions on this paper. Whereas some found its methodology compelling and its empirical evaluation satisfactory, it was generally perceived that this paper will greatly benefit from another round of reviewing. In particular, the authors should improve readability of the main text and provide a more thorough discussion on related recent (and concurrent) work. 
The authors consider distilling posterior expectations for Bayesian neural networks. While reviewers found the material interesting, and the responses thoughtful, there were questions about the practical utility of the work. Evaluations of classification favour NLL (and typically do not show accuracy), and regression (which was considered in the original Bayesian Dark Knowledge paper) is not considered. In general, it is difficult to assess and interpret how the approach is working, and in what application regime it would be a gold standard, e.g., with respect to downstream tasks. The authors are encouraged to continue with this work, taking reviewer comments into account in a final version.
This paper presents a unified probabilistic approach for deep continual learning by combining generative and discriminative models into one framework that solves the following problems: catastrophic forgetting, and identifying out of distribution and open set examples. The method termed, OCDVAE in the paper achieves closer or better to SOTA results in different evaluation tasks.   The reviewers had several concerns about the presentation of the paper and some errors in the equations, all of which seem to have been fixed in the latest upload made by the authors. Blind review #3 was delayed as the original reviewer refused to review the paper and this review was then obtained by someone else after the new upload of the paper, so this review looks at the new version of the paper. I would recommend the authors to incorporate suggestions provided by reviewer #3 in the final version of the paper including expanding on the related work section.   However, as of now I recommend to reject the paper.
The submission is a detailed and extensive examination of overfitting in vision and language navigation domains. The authors evaluate several methods across multiple environments, using different splits of the environment data into training, validation seen, and validation unseen. The authors also present an approach using semantic features which is shown to have little or no gap between training and validation performance.   The reviewers had mixed reviews and there was substantial discussion about the merits of the paper. However, a significant issue was observed and confirmed with the authors, relating to tuning the semantic features and agent model on the unseen validation data. This is an important flaw, since the other methods were not tuned in this way, and there was no  test  performance given in the paper. For this reason, the recommendation is to reject the paper. The authors are encouraged to fairly compare all models and resubmit their paper at another venue.
This paper studies the problem of defending deep neural network approaches for image classification from physically realizable attacks. It first demonstrates that adversarial training with PGD attacks and randomized smoothing exhibit limited effectiveness against three of the highest profile physical attacks. Then, it proposes a new abstract adversarial model, where an adversary places a small adversarially crafted rectangle in an image, and develops two approaches for efficiently computing the resulting adversarial examples. Empirical results show the effectiveness. Overall, a good paper. The rebuttal is convincing.
This paper studies the loss landscape of neural networks by taking into consideration the symmetries arising from the parametrisation. Specifically, given two models $\theta_1$, $\theta_2$, it attempts to connect $\theta_1$ with the equivalence of class of $\theta_2$ generated by weight permutations.  Reviewers found several strengths in this work, from its intuitive and simple idea to the quality of the experimental setup. However, they also found important shortcomings in the current manuscript, chief among them the lack of significance of the results. As a result, this paper unfortunately cannot be accepted in its current form. The chairs encourage the authors to revise their work by taking the reviewer feedback into consideration. 
The paper proposes an algorithm for zero shot generalization in RL via learning a scoring a function from.  The reviewers had mixed feelings, and many were not from the area. A shared theme was doubts about the significance of the experimental setting, and also the generality of the approach.  As this is my field, I read the paper, and recommend rejection at this time. The proposed method is quite laborious and requires quite a bit of assumptions on the environments to work, as well as fine tuning parameters for each considered task (number of regions, etc). I also agree that the evaluation is not convincing   stronger baselines need to be considered and the experiments to better address the zero shot transfer aspect that the paper is motivated by. I encourage the authors to take the review feedback into account and submit a future version to another venue.
This paper proposes a new way of comparing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images, i.e. replacing the conventional test set based evaluation methods with a more flexible mechanism. The main proposal is to build a test set adaptively in a manner that captures how classifiers disagree, as measured by the wordnet tree. As noted by R2, this work has the potential to be of interest to a broad audience and can motivate many subsequent works.   While the reviewers acknowledged the importance of this work, they raised several concerns: (1) the proposed approach is immature to be considered for benchmarking yet (R1,R4), (2) selecting k and studying its influence on the performance ( R1, R3, R4), (3) the proposed approach requires data annotation which might not be straightforward   (R3, R4).  The authors provided a detailed rebuttal addressing the reviewer concerns.  There is reviewer disagreement on this paper. The comments from R3 were valuable for the discussion, but at the same time too brief to be adequately addressed by the authors. The comments from emergency reviewer were helpful in making the decision. AC decided to recommend acceptance of the paper seeing its valuable contributions towards re thinking the evaluation of current SOTA models. 
This paper proposes adding a Dirichlet distribution as a wrapper on top of a black box classifier in order to better capture uncertainty in the predictions.  This paper received four reviews in total with scores (1,1,1,6).  The reviewer who gave the weak accept found the paper well written, easy to follow and intuitive.  The other reviewers, however, were primarily concerned about the empirical evaluation of the method.  They found the baselines too weak and weren t convinced that the method would work well in practice.  The reviewers also cited a lack of comparison to existing literature for their scores.  One reviewer noted that while the method addresses aleatoric uncertainty, it doesn t provide any mechanism for epistemic uncertainty, which would be necessary for the applications motivating the work.    The authors did not provide a response and thus there was no discussion. 
This paper proposes an idea of using a pre trained language model on a potentially smaller set of text, and interpolating it with a k nearest neighbor model over a large datastore. The authors provide extensive evaluation and insightful results. Two reviewers vote for accepting the paper, and one reviewer is negative. After considering the points made by reviewers, the AC decided that the paper carries value for the community and should be accepted.
This paper presents mixout, a regularization method that stochastically mixes parameters of a pretrained language model and a target language model. Experiments on GLUE show that the proposed technique improves the stability and accuracy of finetuning a pretrained BERT on several downstream tasks.  The paper is well written and the proposed idea is applicable in many settings. The authors have addressed reviewers concerns  during the rebuttal period and all reviewers are now in agreement that this paper should be accepted.  I think this paper would be a good addition to ICLR and recommend to accept it. 
The paper proposes a simple and effective way to stabilize training by adding consistency term to discriminator. Given the stochastic augmentation procedure $T(x)$ the loss is just a penalty on $D$. The main unsolved question why it help to make discriminator "smoother" in the consistency case for a standard GAN (since typically, no constraints are enforced). Nevertheless, at the moment this a working heuristics that gives new SOTA, and that is the main strength. The reviewer all agree to accept, and so do I.
This paper addresses the problem of unsupervised domain adaptation and proposes explicit modeling of the source and target feature distributions to aid in cross domain alignment.   The reviewers all recommended rejection of this work. Though they all understood the paper’s position of explicit feature distribution modeling, there was a lack of understanding as to why this explicit modeling should be superior to the common implicit modeling done in related literature. As some reviewers raised concern that the empirical performance of the proposed approach was marginally better than competing methods, this experimental evidence alone was not sufficient justification of the explicit modeling. There was also a secondary concern about whether the two proposed loss functions were simultaneously necessary.   Overall, after reading the reviewers and authors comments, the AC recommends this paper not be accepted. 
The paper presents an extension of flow based invertible generative models to a conditional setting. The key idea is fairly simple modification of the original architecture, but authors also propose techniques for down sampling with Haar wavelets. The experimental results on class conditional MNIST generation and colorization are promising. However, in terms of weakness, the technical novelty seems somewhat limited although it s a reasonable extension. In addition, the experimental results lack evaluation on general conditional image generation tasks with more widely used benchmarks (e.g., class conditional generation setting for real images, such as CIFAR and ImageNet; attribute conditional or image to image translation settings; etc.). In other words, colorization seems like a niche task. The baselines compared are not the strongest models. For example, the diversity of  cGANs can be significantly improved by simple plug in modifications (e.g., DSGAN) to any existing GAN architectures, and those methods were demonstrated on broader benchmarks. So I view the experimental validation somewhat limited in scope and significance. While this work presents a reasonable extension of conditional invertible generative models with promising results, I believe that more work needs to be done to be publishable at a top tier conference.  Diversity Sensitive Conditional Generative Adversarial Networks https://arxiv.org/abs/1901.09024  Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis https://arxiv.org/abs/1903.05628 * exactly the same idea as DSGAN above. 
The method proposed and explored here is to introduce small spatial distortions, with the goal of making them undetectable by humans but affecting the classification of the images. As reviewers point out, very similar methods have been tested before. The methods are also only tested on a few low resolution datasets.   The reviewers are unanimous in their judgement that the method is not novel enough, and the authors  rebuttals have not convinced the reviewers or me about the opposite.
The paper proposes to study what information is encoded in different layers of StyleGAN.  The authors do so by training classifiers for different layers of latent codes and investigating whether changing the latent code changes the generated output in the expected fashion.  The paper received borderline reviews with two weak accepts and one weak reject.  Initially, the reviewers were more negative (with one reject, one weak reject, and one weak accept).  After the rebuttal, the authors addressed most of the reviewer questions/concerns.    Overall, the reviewers thought the results were interesting and appreciated the care the authors took in their investigations.  The main concern of the reviewers is that the analysis is limited to only StyleGAN.  It would be more interesting and informative if the authors applied their methodology to different GANs.  Then they can analyze whether the methodology and findings holds for other types of GANs as well. R1 notes that given the wide interest in StyleGAN like models, the work maybe of interest to the community despite the limited investigation.  The reviewers also point out the writing can be improved to be more precise.  The AC agrees that the paper is mostly well written and well presented.  However, there are limitations in what is achieved in the paper and it would be of limited interest to the community.  The AC recommends that the authors consider improving their work, potentially broadening their investigation to other GAN architectures, and resubmit to an appropriate venue.
The paper introduces a new method for 3d point cloud generation based upon auto encoders and GANs.  Two reviewers voted for accept and one reviewer for outright reject. Both authors and reviewers posted thorough responses. Based upon these it is judged best to not accept the paper in the present. The authors should take the feedback into account in a an updated version of the paper.  Rejection is recommended.  
The paper introduces a novel way of doing IRL based on learning constraints. The topic of IRL is an important one in RL and the approach introduced is interesting and forms a fundamental contribution that could lead to relevant follow up work.
This submission proposes a secondary objective when learning language models like BERT that improves the ability of such models to learn entity centric information. This additional objective involves predicting whether an entity has been replaced. Replacement entities are mined using wikidata.  Strengths:  The proposed method is simple and shows significant performance improvements for various tasks including fact completion and question answering.  Weaknesses:  The experimental settings and data splits were not always clear. This was sufficiently addressed in a revised version.  The paper could have probed performance on tasks involving less common entities.  The reviewer consensus was to accept this submission. 
This paper investigates using "curiosity" to improve representation learning. This paper is not ready for publication. The main issues was the reviewers found the paper did not support the claim contributions in terms of (1) evaluating the new representations and improvement due to the representation, and (2) the novelty of the method compared to the long literature in this area. In general the reviewers found the empirical evidence unconvincing, and the too many missing details.  The results in this paper have many issues: claims of performance based on three runs; undefined error measures; bolding entries in tables which appear not significantly better without explanation; unclear/informal meta parameter tuning.   Finally, there are some terminology issues in this paper. I suggest an excellent paper on the topic: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858647/  
The paper proposed an optimization based defense against model stealing attacks.  A criticism of the paper is that the method is computationally expensive, and was not demonstrated on more complex problems like ImageNet.  While this criticism is valid, other reviewers seem less concerned by this because the SOTA in this area is currently focused on smaller problems.  After considering the rebuttal, there is enough reviewer support for this paper to be accepted.
This work proposes using spectral element methods to speed up training of ODE Networks for system identification. The authors utilize truncated series of Legendre polynomials to analyze the dynamics and then conduct experiments that shows their proposed scheme achieves an order of magnitude improvement in training speed compared to baseline methods. Reviewers raised some concerns (e.g. empirical comparison against adjoint methods in the multi agent example) or asked for clarifications (e.g. details of time sampling of the data). The authors adequately addressed most of these concerns via rebuttal response as well as revising the initial submission. At the end, all reviewers recommended for accept based on contributions of this work on improving training speed of ODE Networks. R4 hopes that some of the additional concerns that are not yet reflected in the current revision, be addressed in the camera ready version. 
This paper carries out extensive experiments on Neural Tangent Kernel (NTK)  kernel methods based on infinitely wide neural nets on small data tasks. I recommend acceptance.
This paper is board line but in the end below the standards for ICLR. Firstly this paper could use significant polishing. The text has significant grammar and style issues: incorrect words, phrases and tenses; incomplete sentences; entire sections of the paper containing only lists, etc. The paper is in need of significant editing.  This of course is not enough to merit rejection, but there are concerns about the contribution of the new method, experiment details, and the topic of study. The results are reported from either a single run or unknown number of runs of the learning system, which is not acceptable even if the we suspect the variance is low. The proposed approach relies on pre training a feature extractor which in many ways side steps the forgetting/interference problem rather than what we really need: new algorithms that processes the training data in ways the mitigate interference by learning representations. In general the reviewers found it very difficult to access the fairness of the comparisons dues do differences between how different methods make use of stored data and pre training. The reviewers highlighted the similarity between the propose approach and recent work in angle of generative modeling / out of distribution (OOD) detection which suggests that the proposed approach has limited utility (as detailed by R1) and that OOD baselines were missing. Finally, the CL problem formulation explored here, where task identifiers are available during training and data is i.i.d, is of limited utility. Its hard to imagine how approaches that learn individual networks for each task could scale to more realistic problem formulations.  All reviewers agreed the paper s experiments were borderline and the paper has substantial issues. There are too many revisions to be done.
This paper presents an analysis of the kind of knowledge captured by pre trained word embeddings. The authors show various kinds of properties like relation between entities and their description, mapping high level commands to discrete commands etc. The problem with the paper is that almost all of the properties shown in this work has already been established in existing literature. In fact, the methods presented here are the baseline algorithms to the identification of different properties presented in the paper.  The term common sense which is used often in the paper is mischaracterized. In NLP literature, common sense is something that is implicitly understood by humans but which is not really captured by language. For example, going to a movie means you need parking is something that is well understood by humans but is not implied by the language of going to the movie. The phenomenon described by the authors is general language processing.  Towards the end the evaluation criteria for embedding proposed is also a well established concept, its just that these metrics are not part of the training mechanism as yet. So if the contribution was on showing how those metrics can be integrated in training the embeddings, that would be a great contribution.  I agree with the reviewer s critics and recommend a rejection as of now.
The authors study neural networks with binary weights or activations, and the so called "differentiable surrogates" used to train them. They present an analysis that unifies previously proposed surrogates and they study critical initialization of weights to facilitate trainability.  The reviewers agree that the main topic of the paper is important (in particular initialization heuristics of neural networks), however they found the presentation of the content lacking in clarity as well as in clearly emphasizing the main contributions.  The authors imporved the readability of the manuscript in the rebuttal.  This paper seems to be at acceptance threshold and 2 of 3 reviewers indicated low confidence. Not being familiar with this line of work, I recommend acceptance following the average review score.
(1) the authors emphasize the theoretical contribution and claims the bound are tighter. However, they did not directly compare with any certified robust methods, or previous bounds to support the argument.  HM, not sure, need to check this  (2) The empirical results look suboptimal. The authors did not convince me why they sampled 1000 images for test for a small CIFAR 10 dataset. The proposed method is 10% less robust comparing to Madry s in table 1.   Seems ok, understand authors response  1) The theoretical analysis are not terribly new, which is just a straightforward application of first order Taylor expansion. This idea could be traced back to the very first paper on adversarial examples FGSM (Goodfellow et al 2014). True  2) The novelty of the paper is to replace exact gradient (w.r.t input) by their finite difference and use it as a regularization. However, there is a misalignment between the theory and the proposed algorithm. The theory only encourages input gradient regularization, regardless to how it is evaluated, and previous studies have shown that this is not a very effective way to improve robustness. According to the experiments, the main empirical improvement comes from the finite difference implementation but the benefit of finite difference is not justified/discussed by the theory. Therefore, the empirical improvement are not supported by the theory. Authors have briefly respond to this issue in the discussion but I believe a more rigorous analysis is needed.   This seems okay based on author response  3) Moreover, the empirical performance does not achieve state of the art result. Indeed, there is a non negligible  gap (12%) between the obtained performance and some well known baseline. Thus the empirical contribution is also limited.  Yea, for some cases
The paper proposes to use greedy core set sampling to improve GAN training with large batches. Although the problem is clear and the solution works, reviewers have raised several concerns. One concern is that the technical novelty is limited; another (in the first version) that even a simpler version of gradient accumulation can solve the  main task (rather that computing core sets). In the end, some discussion was done, with quite a few additions and experiments done by the authors. The final concern that seemingly was not addressed: the gradient accumulation seems to give the same numbers as large batches, thus you can  mimic  large batch sizes with smaller ones and gradient accumulation, making the main claim of the paper questionable. The achievement of SOTA is good, but it is not clear wether it is due to the proposed technique, or rather smart tuning of a larger set of hyperparameters. Thus, I would agree with the concern of Reviewer1.
Thanks for the detailed replies to the reviewers, which significantly helped us understand your paper better. However, after all, we decided not to accept your paper due to weak justification and limited experimental validation. Writing should also be improved significantly. We hope that the feedback from the reviewers help you improve your paper for potential future submission. 
This paper proposes a dedicated deep models for analysis of multiplexed ion beam imaging by time of flight (MIBI TOF).  The reviewers appreciated the contributions of the paper but not quite enough to make the cut.  Rejection is recommended. 
The paper presents a method for visual robot navigation in simulated environments. The proposed method combines several modules, such as mapper, global policy, planner, local policy for point goal navigation. The overall approach is reasonable and the pipeline can be modularly trained. The experimental results on navigation tasks show strong performance, especially in generalization settings. 
This paper explores the emergence of language in environments that demand agents communicate, focusing on the compositionality of language, and the cultural transmission of language.  Reviewer 1 has several suggestions about new experiments that are possible. The AC does think there is value in many of the suggested experiments, if not to run, then just to acknowledge their possibility and leave for future work. The reviewers also point to some previous work that is very similar.  E.g. "Ease of Teaching and Language Structure from Emergent Communication", Funshan Li et al
Since there were only two official reviews submitted, I reviewed the paper to form a third viewpoint.  I agree with reviewer 2 on the following points, which support rejection of the paper: 1) Only CIFAR is evaluated without Penn Treebank; 2) The "faster convergence" is not empirically justified by better final accuracy with same amount of search cost; and 3) The advantage of the proposed ACSA over SBMD is not clearly demonstrated in the paper.  The scores of the two official reviews are insufficient for acceptance, and an additional review did not overturn this view.
In this paper dense layers in deep neural networks representing policies are replaced by tensor regression layers, also by a scattering layer, and second order optimization is considered. The paper does not have a single consistent message, and combines different techniques for unclear reason. Important related work is not cited. The presentation was found unclear by the reviewers. 
The reviewers have issues with the lack of enough experimental results as well as with novelty of the solution proposed. I recommend rejection.
This paper proposes to use the grey level co occurrence matrix method (GLCM) for both the performance evaluation metric and an auxiliary loss function for single image super resolution. Experiments are conducted on X ray images of rock samples. Three reviewers provide comments. Two reviewers rated reject while one rated weak reject. The major concerns include the lack of clear and detailed description, low novelty, limited experiment on only one database, unconvincing improvement over the prior work, etc. The authors agree that the limited experiment on one database does not demonstrate the generalization capability of the proposed method. The AC agrees with the reviewers’ comments, and recommend rejection.
The reviewers were generally in agreement that the paper presents a valuable contribution and should be accepted for publication. However, I would strongly encourage the authors to carefully read over the reviews and address the suggestions and concerns insofar as possible for the final.
This paper introduces the concept of gradient confusion to show how the neural network architecture affects the speed of training. The reviewers  opinion on this paper varies widely, also after the discussion phase.  The main disagreement is on the significance of this work, and whether the concept of gradient confusion adds something meaningful to the existing literature with respect to understanding deep networks. The strong disagreement on this paper suggest that the paper is not quite ready yet for ICLR, but that the authors should make another iteration on the paper to strengthen the case for its significance. 
The article studies the behaviour of binary and full precision ReLU networks towards explaining differences in performance and suggests a random bias initialisation strategy. The reviewers agree that, while closing the gap between binary networks and full precision networks is an interesting problem, the article cannot be accepted in its current form. They point out that more extensive theoretical analysis and experiments would be important, as well as improving the writing. The authors did not provide a rebuttal nor a revision. 
The work proposes to learn neural networks using a homotopy based continuation method. Reviewers found the idea interesting, but the manuscript poorly written, and lacking in experimental results. With no response from the authors, I recommend rejecting the paper.
This paper proposes an extension of MPO for on policy reinforcement learning. The proposed method achieved promising results in a relatively hyper parameter insensitive manner.  One concern of the reviewers is the lack of comparison with previous works, such as original MPO, which has been partially addressed by the authors in rebuttal. In addition, Blind Review #3 has some concerns with the fairness of the experimental comparison, though other reviews accept the comparison using standardized benchmark.  Overall, the paper proposes a promising extension of MPO; thus, I recommend it for acceptance. 
This paper presents an ODE based latent variable model, argues that extra unobserved dimensions are necessary in general, and that deterministic encodings are also insufficient in general.  Instead, they optimize the latent representation during training.  They include small scale experiments showing that their framework beats alternatives.  In my mind, the argument about fixed mappings being inadequate is a fair one, but it misses the fact that the variational inference framework already has several ways to address this shortcoming: 1) The recognition network outputs a distribution over latent values, which in itself does not address this issue, but provides regularization benefits. 2) The recognition network is just a strategy for speeding up inference.  There s no reason you can t just do variational inference or MCMC for inference instead (which is similar to your approach), or do semi amortized variational inference.  Basically, this paper could have been somewhat convincing as a general exploration of approximate inference strategies in the latent ODE model.  Instead, it provides a lot of philosophical arguments and a small amount of empirical evidence that a particular encoder is insufficient when doing MAP inference.  It also seems like a problem that hyperparameters were copied from Chen et al 2018, but are used in a MAP setting instead of a VAE setting.  Finally, it s not clear how hyperparameters such as the size of the latent dimensions were chosen.
This article proposes a regularisation scheme to learn classifiers that take into account similarity of labels, and presents a series of experiments. The reviewers found the approach plausible, the paper well written, and the experiments sufficient. At the same time, they expressed concerns, mentioning that the technical contribution is limited (in particular, the Wasserstein distance has been used before in estimation of conditional distributions and in multi label learning), and that it would be important to put more efforts into learning the metric. The author responses clarified a few points and agreed that learning the metric is an interesting problem. There were also concerns about the competitiveness of the approach, which were addressed in part in the authors  responses, albeit not fully convincing all of the reviewers. This article proposes an interesting technique for a relevant type of problems, and demonstrates that it can be competitive with extensive experiments. ``Although this is a reasonably good article, it is not good enough, given the very high acceptance bar for this year s ICLR.  
This paper addresses an important and relevant problem in reinforcement learning: learning from off policy data, taking into account the offsets in the visitation distribution of states. This has the promise of lowering variance even with long horizon roll outs. Existing methods have required access to the behavior policy (or have required data from the stationary distribution). The novel proposed approach instead uses an alternative method, based on the fixed point of the "backward flow" operator, to calculate the importance ratios required for policy evaluation in discrete and continuous environments.   In the initial version of the submission, several concerns were expressed regarding both the quality of the paper and clarity. The authors have updated the paper to address these concerns to the satisfaction of the reviewers, who are now unanimously in favor of acceptance. 
This is a very interesting paper which discusses practical issues and solutions around deploying RL on real physical robotic systems, specifically involving questions on the use of raw sensory data, crafting reward functions, and not having resets at the end of episodes.  Many of the issues raised in the reviews and discussion were concerned with experimental details and settings, as well as relation to different areas of related work. These were all sufficiently handled in the rebuttal, and all reviewers were in favour of acceptance.
This paper examined a pure exploration method for efficient action value estimates in tabular reinforcement learning.  The paper is on the theoretical properties of value estimates in the large sample regime.  The method is shown to outperform baseline algorithms for this task in tabular reinforcement learning.  The reviewers were divided on the merits of this work.  The use of the central limit theorem was viewed as elegant, and the results were thought to be potentially useful.  However, the reviewers several limitations.  They found the assumption of a communicating MDP to be overly restrictive (reviewer 1).  The algorithm may be computationally inefficient (reviewer 2).  The nature of "exploration" in this work is not the conventional meaning in reinforcement learning (reviewer 3).  The paper is not yet ready for publication at ICLR.  The theoretical results do not clearly convey insights for reinforcement learning with function approximation, and the reviewers are also not in agreement that the current results are applicable to a general MDP setting.
The paper empirically investigates the behaviour of graph neural networks, as a function of topology, structural noise, and coupling between nodal attributes and structure. While the paper is interesting, reviewers in general felt that the presentation lacked clarity and aspects of the experiments were hard to interpret. The authors are encouraged to continue with this work, accounting for reviewer comments in subsequent versions. 
The paper proposes a set of conditions that enable a mapping from word embeddings to relation embeddings in knowledge graphs. Then, using recent results about pointwise mutual information word embeddings, the paper provides insights to the latent space of relations, enabling a categorization of relations of entities in a knowledge graph. Empirical experiments on recent knowledge graph models (TransE, DistMult, TuckER and MuRE) are interpreted in light of the predictions coming from the proposed set of conditions.  The authors responded to reviewer comments well, providing significant updates during the discussion period. Unfortunately, the reviewers did not engage further after their original reviews, and so it is hard to tell whether they agreed that the changes resolved all their questions.  Overall, the paper provides much needed analysis for understanding of the latent space of relations on knowledge graphs. Unfortunately, the original submission did not clearly present the ideas, and it is unclear whether the updated version addresses all the concerns. The paper in its current state is therefore not yet suitable for publication at ICLR.
This paper augments transformer encoder decoder networks architecture with k nearest neighbors to fetch knowledge or information related to the previous conversation, and demonstrates improvements through manual and automated evaluation. Reviewers note the fact that the approach is simple and clean and results in significant improvements, however, the approach is incremental over the previous work (including https://arxiv.org/pdf/1708.07863.pdf). Furthermore, although the authors improved the article in the light of reviewer suggestions (i.e., rushed analysis, not so clear descriptions) and some reviewers increased their scores, none of them actually marked the paper as an accept or a strong accept.
A defense against of adversarial attacks is presented, which builds mostly on combining known methods in a novel way. While the novelty is somewhat limited, this would be fine if the results were unequivocally good and other parts of the problematic. However, reviewers were not entirely convinced by the results, and had a number of minor complaints with various parts of the paper.  In sum, this paper is not currently at a stage where it can be accepted.
The paper proposes an approach for forecasting diverse object trajectories using determinantal point processes (DPP). Past trajectory is mapped to a latent code and a conditional VAE is used to generate the future trajectories. Instead of using log likelihood of DPP, the propose method optimizes expected cardinality as a measure for diversity. While there are some concerns about the core method being incremental in novelty over some existing DPP based methods, the context of the paper is different from these papers (ie, diverse trajectories in continuous space) and reviewers have appreciated the empirical improvements over the baselines, in particular over DPP NLL and DPP MAP in latent space. 
This paper proposes a two stage adversarial training approach for learning a disentangled representation of style and content of anime images. Unlike the previous style transfer work, here style is defined as the identity of a particular anime artist, rather than a set of uninterpretable style features. This allows the trained network to generate new anime images which have a particular content and are drawn in the style of a particular artist. While the approach works well, the reviewers voiced concerns about the method (overly complicated and somewhat incremental) and the quality of the experimental section (lack of good baselines and quantitative comparisons at least in terms of the disentanglement quality). It was also mentioned that releasing the code and the dataset would strengthen the appeal of the paper. While the authors have addressed some of the reviewers’ concerns, unfortunately it was not enough to persuade the reviewers to change their marks. Hence, I have to recommend a rejection.
In this paper, the authors propose a novel approach for learning the structure of a directed acyclic graph from observational data that allows to flexibly model nonlinear relationships between variables using neural networks. While the reviewers initially had concerns with respect to the positioning of the paper and various questions regarding theoretical results and experiments, these concerns have been addressed satisfactorily during the discussion period.  The paper is now acceptable for publication in ICLR 2020. 
The reviewers wondered about the practical application of this method, given that the performance was lower.  The reviewers were also surprised by some of your claims and wanted you to explore them more deeply.    On the positive side, the reviewers found your experiments to be very thorough.  You also performed additional experiments during the rebuttal period.  We hope that those experiments will help you to build a better paper as you work towards publishing this work.
This paper proposes a new gradient based stochastic optimization algorithm by adapting theory for proximal algorithms to the non convex setting.   The majority of reviewers voted for accept. The authors are encouraged to revise with respect to reviewer comments.
The paper proposes a neural network architecture that uses a hypernetwork (RNN or feedforward) to generate weights for a network (variational RNN), that models sequential data. An empirical comparison of a large number of configurations on synthetic and real world data show the promise of this method.  The authors have been very responsive during the discussion period, and generated many new results to address some reviewer concerns. Apart from one reviewer, the others did not engage in further discussion in response to the authors updating their paper.  The paper provides a tweak to the hypernetwork idea for modeling sequential data. There are many strong submissions at ICLR this year on RNNs, and the submission in its current state unfortunately does not pass the threshold.
This paper shows a case study of an adversarial attack on a copyright detection system. The paper implements a music identification method with a simple convolutional neural network, and shows that it is possible to fool such CNN with an adversarial learning. After the discussion period, two among three reviewers incline to the rejection of the paper. Although the majority of the reviewers agree that this is an interesting problem with an important application, they also find many of their concerns remain unaddressed. These include the generality of the finding as the current paper is more like a proof of concept that black/white box attack can work for copyright system. The reviewers are also concerned that the technique solution/finding is not novel as it is very similar to prior work in other domains (e.g., image classification). One reviewer was particularly concerned about that the user study is missing, making it difficult to judge whether the quality of the modified audio is reasonable or not.
This manuscript proposes an approach for estimating cross correlations between model outputs, related to deep CCA. Authors note that the procedure improves results when applied to supervised learning problems.  The reviewers have pointed out the close connection to previous work on deep CCA, and the author(s) have agreed. The reviewers agree that the paper has promise if properly expanded both theoretically and empirically.
The paper examines the problem of unsupervised domain translation. It poses the problem in a rigorous way for the first time and examines the shortcomings of existing CycleGAN based methods. Then the authors propose to consider the problem through the lens of Optimal Transport theory and formulate a practical algorithm.  The reviewers agree that the paper addresses an important problem, brings clarity to existing methods, and proposes an interesting approach / algorithm, and is well written. However, there was a shared concern about whether the new approach just moves the complexity elsewhere (into the design of the cost function). The authors claim to have addressed in the rebuttal by adding an extra experiment, but the reviewers remained unconvinced.  Based on the reviewer discussion, I recommend rejection at this time, but look forward to seeing the revised paper at a future venue.
The paper propose a new quantization friendly network training algorithm called GQ (or DQ) net. The paper is well written, and the proposed idea is interesting. Empirical results are also good. However, the major performance improvement comes from the combination of different incremental improvements. Some of these additional steps do seem orthogonal to the proposed idea. Also, it is not clear how robust the method is to the various hyperparameters / schedules. For example, it seems that some of the suggested training options are conflicting each other. More in depth discussions and analysis on the setting of the regularization parameter and schedule for the loss term blending parameters will be useful.
This paper examines how different distributions of the layer wise number of CNN filters, as partitioned into a set of fixed templates, impacts the performance of various baseline deep architectures.  Testing is conducting from the viewpoint of balancing accuracy with various resource metrics such as number of parameters, memory footprint, etc.  In the end, reviewer scores were partitioned as two accepts and two rejects.  However, the actual comments indicate that both nominal accept reviewers expressed borderline opinions regarding this work (e.g., one preferred a score of 4 or 5 if available, while the other explicitly stated that the paper was borderline acceptance worthy).  Consequently in aggregate there was no strong support for acceptance and non dismissable sentiment towards rejection.  For example, consistent with reviewer comments, a primary concern with this paper is that the novelty and technical contribution is rather limited, and hence, to warrant acceptance the empirical component should be especially compelling.  However, all the experiments are limited to cifar10/cifar100 data, with the exception of a couple extra tests on tiny ImageNet added after the rebuttal.  But these latter experiments are not so convincing since the base architecture has the best accuracy on VGG, and only on a single MobileNet test do we actually see clear cut improvement.  Moreover, these new results appear to be based on just a single trial per data set (this important detail is unclear), and judging from Figure 2 of the revision, MobileNet results on cifar data can have very high variance blurring the distinction between methods.  It is therefore hard to draw firm conclusions at this point, and these two additional tiny ImageNet tests notwithstanding, we don t really know how to differentiate phenomena that are intrinsic to cifar data from other potentially relevant factors.  Overall then, my view is that far more testing with different data types is warranted to strengthen the conclusions of this paper and compensate for the modest technical contribution.  Note also that training with all of these different filter templates is likely no less computationally expensive than some state of the art pruning or related compression methods, and therefore it would be worth comparing head to head with such approaches.  This is especially true given that in many scenarios, test time computational resources are more critical than marginal differences in training time, etc.
This submission presents a theoretical study of phase transitions in IB: adjusting the IB parameter leads to step wise behaviour of the prediction.  Quoting R3: “The core result is given by theorem 1: the phase transition betas necessarily satisfy an equation, where the LHS is expressed in terms of an optimal perturbation of the encoding function X >Z.” This paper received a borderline review and two votes for weak accept.  The main comment for the borderline review was about the rigor of a proof and the use of << symbols.  The authors have updated the proof using limits as requested, addressing this primary concern.  On the balance, the paper makes a strong contribution to understanding an important learning setting and a contribution to theoretical understanding of the behavior of information bottleneck predictors.  
The reviewers are unanimous in their opinion that this paper offers a novel approach to secure edge learning.  I concur.  Reviewers mention clarity, but I find the latest paper clear enough.
This paper proposes a reinforcement learning approach to clustering time series data. The reviewers had several questions related to clarity and concerns related to the novelty of the method, the connection to RL, and experimental results. While the authors were able to address some of these questions and concerns in the rebuttal, the reviewers believe that the paper is not quite ready for publication.
The authors propose a framework for estimating "global robustness" of a neural network, defined as the expected value of "local robustness" (robustness to small perturbations) over the data distribution. The authors prove the the local robustness metric is measurable and that under this condition, derive a statistically efficient estimator. The authors use gradient based attacks to approximate local robustness in practice and report extensive experimental results across several datasets.  While the paper does make some interesting contributions, the reviewers were concerned about the following issues: 1) The measurability result, while technically important, is not surprising and does not add much insight algorithmically or statistically into the problem at hand. Outside of this, the paper does not make any significant technical contributions. 2) The paper is poorly organized and does not clearly articulate the main contributions and significance of these relative to prior work. 3) The fact that the local robustness metric is approximated via gradient based attacks makes the final results void of any guarantees, since there are no guarantees that gradient based attacks compute the worst case adversarial perturbation. This calls into question the main contribution claim of the paper on computing global robustness guarantees.  While some of the technical aspects of the reveiwers  concerns were clarified during the discussion phase, this was not sufficient to address the fundamental issues raised above.  Hence, I recommend rejection.
This paper proposes a method for efficiently training neural networks combined with blackbox implementations of exact combinatorial solvers.  Reviewers and AC agree that it is a well written paper with a novel idea supported by good experimental results. Experimental results are of small scale and can be further improved, but the authors acknowledged this aspect well.  Hence, I recommend acceptance.
This paper studies the properties of Differentiable Architecture Search, and in particular when it fails, and then proposes modifications that improve its performance for several tasks. The reviews were all very supportive with three Accept opinions, and authors have addressed their comments and suggestions. Given the unanimous reviews, this appears to be a clear Accept. 
In this paper, the authors propose the incremental RNN, a novel recurrent neural network architecture that resolves the exploding/vanishing gradient problem. While the reviewers initially had various concerns, the paper has been substantially improved during the discussion period and all questions by the reviewers have been resolved. The main idea of the paper is elegant, the theoretical results interesting, and the empirical evaluation extensive. The reviewers and the AC recommend acceptance of this paper to ICLR 2020.
The paper proposes a method to prune edges in proximity graphs for faster similarity search. The method works by making the graph edges annealable and optimizing over the weights. The paper tackles an important and practically relevant problem as also acknowledged by the reviewers. However there are some concerns about empirical results, in particular about missing comparisons with tree structure based algorithms (perhaps with product quantization for high dimensional data), and about modest empirical improvement on two of the three datasets used in the paper, which leaves room for convincing empirical justification of the method. Authors are encouraged to take the reviewers  comments into account and resubmit to a future venue.   
This paper suggests stabilizing the training of GANs using ideas from control theory. The reviewers all noted that the approach was well motivated and seemed convinced that that the problem was a worthwhile one. However, there were universal concerns about the comparisons with baselines and performance over previous works on Stabilizing GAN training and the authors were not able to properly address them.
The paper introduces an ensemble of RL agents that share knowledge amongst themselves. Because there are no theoretical results, the experiments have to carry the paper.  The reviewers had rather different views on the significance of these experiments and whether they are sufficient to convincingly validate the learning framework introduced. Overall, because of the high bar for ICLR acceptance, this paper falls just below the threshold.  
This paper proposes Conv TT LSTM for long term video prediction. The proposed method saves memory and computation by low rank tensor representations via tensor decomposition and is evaluated in Moving MNIST and KTH datasets.  All reviews argue that the novelty of the paper does not meet the standard of ICLR. In the rebuttal, the authors polish the experiment design, which fails to change any reviewer’s decision.  Overall, the paper is not good enough for ICLR.
This submission addresses the problem of few shot classification. The proposed solution centers around metric based models with a core argument that prior work may lead to learned embeddings which are overfit to the few labeled examples available during learning. Thus, when measuring cross domain performance, the specialization of the original classifier to the initial domain will be apparent through degraded test time (new domain) performance. The authors therefore, study the problem of domain generalization in the few shot learning scenario. The main algorithmic contribution is the introduction of a feature wise transformation layer.   All reviewers suggest to accept this paper. Reviewer 3 says this problem statement is especially novel. Reviewer 1 and 2 had concerns over lack of comparisons with recent state of the art methods. The authors responded with some additional results during the rebuttal phase, which should be included in the final draft.   Overall the AC recommends acceptance, based on the positive comments and the fact that this paper addresses a sufficiently new problem statement. 
Nice start but unfortunately not ripe.  The issues remarked by the reviewers were only partly addressed, and an improved version of the paper should be submitted at a future venue.
This work has a lot of promise; however, the author response was not sufficient to address the concerns expressed by reviewer 1, leading to an aggregate rating that is just not sufficient to justify an acceptance recommendation. The AC recommends rejection.
The authors propose a method to train a neural network that is robust to visual distortions of the input image. The reviewers agree that the paper lacks justification of the proposed method and experimental evidence of its performance.
The consensus among all reviewers was to reject this paper, and the authors did not provide a rebuttal.
This paper proposes a new method to learning heuristics for quantified boolean formulas through RL. The focus is on a method called backtracking search algorithm. The paper proposes a new representation of formulas to scale the predictions of this method.  The reviewers have an overall positive response to this paper. R1 and R2 both agree that the paper should be accepted, and have given some minor feedback to improve the paper. R3 initially was critical of the paper, but the rebuttal helped to clarify their doubt. They still have one more comment and I encourage the authors to address this in the final version of the paper.  R3 meant to increase their score but somehow this is not reflected in the current score. Based on their comments though, I am assuming the scores to be 6,8,6 which makes the cut for ICLR. Therefore, I recommend to accept this paper.
This paper proposes an unsupervised text style transfer model which combines a language model prior with an encoder decoder transducer. They use a deep generative model which hypothesises a latent sequence which generates the observed sequences. It is trained on non parallel data and they report good results on unsupervised sentiment transfer, formality transfer, word decipherment, author imitation, and machine translation. The authors responded in depth to reviewer comments, and the reviewers took this into consideration. This is a well written paper, with an elegant model and I would like to see it accepted at ICLR. 
This paper studies the landscape of linear networks and its critical point. The authors utilize geometric properties of determinantal varieties to derive interesting results on the landscape of linear networks. The reviewers raised some concerns about the fact that many of the results stated here can already be achieved using other techniques and therefore had some concerns about the novelty of these results. The authors provided a detailed response addressing these concerns. One reviewer however still had some concerns about the novelty. My own understanding of the paper is that while some of these results can be obtained using other approaches the proof techniques (brining ideas from algebraic geometry) is novel and could be rather useful. While at this point it is not clear that the techniques generalize to the nonlinear case I think algebraic geometry perspective have a good potential and provide some diversity in the theoretical techniques. As a result I recommend acceptance if possible.
This paper proposes a neural architecture search method that uses balanced sampling of architectures from the one shot model and drops operators whose importance drops below a certain weight.  The reviewers agreed that the paper s approach is intuitive, but main points of criticism were:   Lack of good baselines   Potentially unfair comparison, not using the same training pipeline   Lack of available code and thus of reproducibility. (The authors promised code in response, which is much appreciated. If the open sourcing process has completed in time for the next version of the paper, I encourage the authors to include an anonymized version of the code in the submission to avoid this criticism.)  The reviewers appreciated the authors  rebuttal, but it did not suffice for them to change their ratings. I agree with the reviewers that this work may be a solid contribution, but that additional evaluation is needed to demonstrate this. I therefore recommend rejection and encourage resubmission to a different venue after addressing the issues pointed out by the reviewers.
The authors demonstrate how neural networks can be used to learn vectorial representations of a set of items given only triplet comparisons among those items.  The reviewers had some concerns regarding the scale of the experiments and strength of the conclusions:  empirically, it seemed like there should be more truly large scale experiments considering that this is a selling point; there should have been more analysis and/or discussion of why/how the neural networks help; and the claim that deep networks are approximately solving an NP hard problem seemed unimportant as they are routinely used for this purpose in ML problems.  With a combination of improved experiments and revised discussion/analysis, I believe a revised version of this paper could make a good submission to a future conference.
A method is introduced to estimate the hidden state in imperfect information in multiplayer games, in particular Bridge. This is interesting, but the paper falls short in various ways. Several reviewers complained about the readability of the paper, and also about the quality and presentation of the interesting results.   It seems that this paper represents an interesting idea, but is not yet ready for publication.
Thanks for an interesting discussion. The paper introduces a sound question generation technique for QA. Reviewers are moderately positive, with low confidence. Some issues remain unresolved, though: While the UniLM comparison is currently not apples to apples, for example, nothing prevents the authors from using their method to pretrain UniLM. Currently, QA results are low ish, and it is hard to accept a paper based solely on BLEU scores (questionable metric) for question generation (the task is but a means to an end). Moreover, the authors do not really discuss how their method relates to previous work (see Review 2 and the related work cited there; there s more, e.g., [0]). I also find it a little problematic that the paper completely ignores all work prior to 2017: The NLP community started organizing workshops on question generation in 2010. [1]
This work presents a simple technique for improving the latent space geometry of text autoencoders. The strengths of the paper lie in the simplicity of the method, and results show that the technique improves over the considered baselines. However, some reviewers expressed concerns over the presented theory for why input noise helps, and did not address concerns that the theory was useful. The paper should be improved if Section 4 were instead rewritten to focus on providing intuition, either with empirical analysis, results on a toy task, or clear but high level discussion of why the method helps. The current theorem statements seem either unnecessary or make strong assumptions that don t hold in practice. As a result, Section 4 in its current form is not in service to the reader s understanding why the simple method works.  Finally, further improvements to the paper could be made with comparisons to additional baselines from prior work as suggested by reviewers.
main summary: sparse time LSTM  discussions; reviewer 4: technical description of the proposed method insufficient, reviewer 2, 3: same paper sent to ICLR 2019 and rejected recommendation: rejected, based on all reviewers comments
This paper proposes to optimize the code optimal code in DNN compilers using adaptive sampling and reinforcement learning. This method achieves  significant speedup in compilation time and execution time. The authors made strong efforts in addressing the problems raised by the reviewers, and promised to make the code publicly available, which is of particular importance for works of this nature.    
The paper focuses on learning speech representations with contrastive predictive coding (CPC). As noted by reviewers, (i) novelty is too low (mostly making the model bidirectional) for ICLR (ii) comparison with existing work is missing.
This paper proposed a very general idea called Atomic Compression Networks (ACNs) to construct neural networks. The idea looks simple and effective.  However, the reason why it works is not well explained.  The experiments are not sufficient enough to convince the reviewers.
The paper proposes a method to speed up training of deep nets by re weighting samples based on their distance to the decision boundary. However, they paper seems hastily written and the method is not backed by sufficient experimental evidence.
This paper suggests using RNN and policy gradient methods for improving symbolic regression. The reviewers could not reach a consensus, and due to concerns about the clarity of the paper and the extensiveness of the experimental results, the paper does not appear to currently meet the level of publication.   Also, while not mentioned in the reviews, there appears to be some work on symbolic regression aided by deep learning, (see for example, https://twhughes.github.io/pdfs/cs221_final.pdf, which was found by searching "symbolic regression deep learning") I would thus also recommend the authors do a more thorough literature search for future revisions. 
This paper presents an energy efficient architecture for quantized deep neural networks based on decomposable multiplication using MACs. Although the proposed approach is shown to be somehow effective, two reviewers pointed out that the very similar idea was already proposed in the previous work, BitBlade [1]. As the authors did not submit a rebuttal to defend this critical point, I’d like to recommend rejection. I recommend authors to discuss and clarify the difference from [1] in the future version of the paper.   [1] Sungju Ryu, Hyungjun Kim, Wooseok Yi, Jae Joon Kim. BitBlade: Area and Energy Efficient Precision Scalable Neural Network Accelerator with Bitwise Summation. DAC 2019 
This paper analyzes neural recording data taken from rodents performing a continual learning task using demixed principal component analysis, and aims to find representations for behaviorally relevant variables. They compare these features with those of a deep RL agent.  I am a big fan of papers like this that try to bridge between neuroscience and machine learning. It seems to have a great motivation and there are some interesting results presented. However the reviewers pointed out many issues that lead me to believe this work is not quite ready for publication. In particular, not considering space when analyzing hippocampal rodent data, as R2 points out, seems to be a major oversight. In addition, the sample size is incredibly small (5 rats, only 1 of which was used for the continual learning simulation). This seems to me like more of an exploratory, pilot study than a full experiment that is ready for publication, and therefore I am unfortunately recommending reject.  Reviewer comments were very thorough and on point. Sounds like the authors are already working on the next version of the paper with these points in mind, so I look forward to it. 
This paper examines the correspondence between topological similarity of languages (correlation between the message space and object space) and ability to learn quickly in a situation of emergent communication between agents.  While this paper is not without issues, it does seem to present a nice contribution that all of the reviewers appreciated to some extent. I think it will spark further discussions in this area, and thus can recommend it for acceptance.
This paper tackles hard exploration RL problems using learning from demonstrations. The idea is to combine the existing R2D2 algorithms with imitation learning from human demonstrations. Experiments are conducted on a new set of challenging tasks, highlighting limitations of strong current baseline while highlighting the strength of the proposed approach.  The contribution is two folds: the proposed algorithm which clear outperforms previous SOTA agents and the set of benchmarks. All reviewers being positive about this paper, I therefore recommend acceptance.
The paper presents a new variation of neural (re) rendering of objects, that uses a set of two deep ConvNets to model non Lambertian effects associated with an object. The paper has received mostly positive reviews. The reviewers agree that the contribution is well described, valid and valuable. The method is validated against strong baselines including Hedman et al., though Reviewer4 rightfully points out that the comparison might have been more thorough.   One additional concern not raised by the reviewers is the lack of comparison with [Thies et al. 2019], which is briefly mentioned but not discussed. The authors are encouraged to provide a corresponding comparison (as well as additional comparisons with Hedman et al) and discuss pros and cons w.r.t. [Thies et al] in the final version.
The main contribution is a Bayesian neural net algorithm which saves computation at test time using a vector quantization approximation. The reviewers are on the fence about the paper. I find the exposition somewhat hard to follow. In terms of evaluation, they demonstrate similar performance to various BNN architectures which require Monte Carlo sampling. But there have been lots of BNN algorithms that don t require sampling (e.g. PBP, Bayesian dark knowledge, MacKay s delta approximation), so it seems important to compare to these. I think there may be promising ideas here, but the paper needs a bit more work before it is to be published at a venue such as ICLR. 
The work this paper presents is interesting, but it is not quite ready yet for publication at ICLR. Specifically, the motivation of particular choices could be better, such as summing over quantiles, as indicated by Reviewer 1. The inherent trade off between safety and speed of adaptation and how this relates to the proposed method could also use a clearer exposition.
This paper presents a certified defense method for adversarial patch attacks. The proposed approach provides certifiable guarantees to the attacks, and the reviewers particularly find its experiments results interesting and promising. The added new experiments during the rebuttal phase strengthened the paper. There still is a remaining concern that is novelty is limited as this paper could be viewed as the application of the original IBP to patch attacks, but the reviewers believe in that its empirical results are important.
This is a very interesting paper on unsupervised skill learning based on the predictability of skill effects, with the incorporation of these ideas into model based RL.  This is a clear accept, based on the clarity of the ideas presented and the writing, as well as the thorough and convincing experiments.
Main content:  Blind review #3 summarized it well, as follows:  This paper studies knowledge distillation in the context of non autoregressive translation. In particular, it is well known that in order to make NAT competitive with AT, one needs to train the NAT system on a distilled dataset from the teacher model. Using initial experiments on EN >ES/FR/DE, the authors argue that this necessity arises from the overly multimodal nature of the output distribution, and that the AT teacher model produces a less multimodal distribution that is easier to model with NAT.   Based on this, the authors propose two quantities that estimate the complexity (conditional entropy) and faithfulness (cross entropy vs real data), and derive approximations to these based on independence assumptions and an alignment model.  The translations from the teacher output are indeed found to be less complex, thereby facilitating easier training for the NAT student model.     Discussion:  Questions were mostly about how robust the results were on other language pairs and random starting points. Authors addressed questions reasonably.  One low review came from a reviewer who admitted not knowing the field, and I agree with the other two reviewers.     Recommendation and justification:  I think papers that offer empirically support for scientific insight (giving an "a ha!" reaction), rather than massive engineering efforts to beat the state of the art, are very worthwhile in scientific conferences. This paper meets that criteria for acceptance.
The paper proposes a fast training method for extreme classification problems where number of classes is very large. The method improves the negative sampling (method which uses uniform distribution to sample the negatives) by using an adversarial auxiliary model to sample negatives in a non uniform manner. This has logarithmic computational cost and minimizes the variance in the gradients. There were some concerns about missing empirical comparisons with methods that use sampled softmax approach for extreme classification. While these comparisons will certainly add further value to the paper, the improvement over widely used method of negative sampling and a formal analysis of improvement from hard negatives is a valuable contribution in itself that will be of interest to the community. Authors should include the experiments on small datasets to quantify the approximation gap due to negative sampling compared to full softmax, as promised.
This paper focuses on studying the impact of initialization and activation functions on the Neural Tangent Kernel (NTK) type analysis. The authors claim to make a connection between NTK and edge of chaos analysis. The reviewers had some concern about (1) impact of smooth activations "any NTK based training method for DNNs should use a Smooth Activation Function from the class S and the network should be initialized on the EOC" (2) proofs of residual networks (3) and why mixing NTK with EOC is interesting. Some of these concerns were addressed in the response. I do share the reviewer concerns about (2). The authors need to give a clear proof. I think this combination of NTK and EOC could be interesting but needs to be better motivated. As a result I do not recommend publication.
The authors propose an invertible flow based model for molecular graph generation. The reviewers like the idea but have several concerns: in particular, overfitting in the model, need for more experiments and missing related work. It is important for authors to address them in a future submission
This manuscript proposes a strategy for fitting predictive models on data separated across nodes, with respect to both samples and features.  The reviewers and AC agree that the problem studied is timely and interesting, and were impressed by the size and scope of the evaluation dataset (particularly for a medical application). However, reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. On the conceptual end, the AC also suggests that the authors look into closely related work on split learning (https://splitlearning.github.io/) which has also been applied to medical data settings.
The authors develop a spectral analysis on the boolean cube for the neural "conjugate kernel" (CK) and "tangent kernel" (NTK). The analysis sheds light into inductive biases of neural networks, such as whether they are biased to simple functions.    This work contains rigorous analysis and theory which is useful for further discussions. However, the theory and insights do not feel complete. One important drawback is that the analysis is limited by the boolean cube setting; this also means that it is more difficult to link theory to practical scenarios. This has been discussed a lot during the rebuttal and among reviewers. Empirical validation has attempted to deal with these concerns, but it would be useful to have this validation coming from theory, or at least have further relevant theoretical insights. This could happen by further building on the theorem provided in the rebuttal for eigenvalue behavior when d is large.
The authors proposed a simple and effective approach to parallel training based on stochastic weight averaging. Moreover, the authors have carefully addressed the reviewer comments in the discussion period, particularly the relation to local SGD, to the satisfaction of reviewers. Local SGD mimics sequential SGD with noise induced by lack of synchronization, whereas SWAP averages multiple samples from a stationary distribution, and synchronizes at the end. Please clarify these points and carefully account for reviewer comments in the final version. Overall, the proposed approach will make an excellent addition to the program, both elegant and practically useful.
This paper proposes an approximate inference approach for decoding in autoregressive models, based on the method of auxiliary coordinates,  which uses iterative factor graph approximations of the model.  The approach leads to nice improvements in performance on a text infilling task.  The reviewers were generally positive about this paper, though there was a concern that more baselines are needed and discussion was very limited following the author responses.  I tend to agree with the authors that their results are convincing on the infilling task.  The impact of the paper is a bit limited by the lack of experiments on more standard decoding tasks, which, as the authors point out, would be challenging as their approach is computationally demanding.  Overall I believe this would be an interesting contribution to the ICLR community.
The authors propose a new mini batch selection method for training deep NNs. Rather than random sampling, selection is based on a sliding window of past model predictions for each sample and uncertainty about those samples. Results are presented on MNIST and CIFAR.  The reviewers agreed that this is an interesting idea which was clearly presented, but had concerns about the strength of the experimental results, which showed only a modest benefit on relatively simple datasets. In the rebuttal period, the authors added an ablation study and additional results on Tiny ImageNet. However, the results on the new dataset seem very marginal, and R1 did not feel that all of their concerns were addressed. I’m inclined to agree that more work is required to prove the generalizability of this approach before it’s suitable for acceptance. 
The paper presents an off policy actor critic scheme where i) a buffer storing the trajectories from several agents is used (off policy replay) and mixed with the on line data from the current agent; ii) a trust region estimator is used to select trajectories that are sufficiently close to the current policy (e.g. in the sense of a KL divergence).  As noted by the reviews, the results are impressive.   Quite a few concerns still remain: * After Fig. 1 (revised version), what matters is the shared replay, where the agent actually benefits from the experience of 9 other different agents; this implies that the population based training observes 9x more frames than the no shared version, and the question whether the comparison is fair is raised; * the trust region estimator might reduce the data seen by the agent, leading it to overfit the past (Fig. 3, left); * the influence of the $b$ hyper parameter (the trust threshold) is not discussed. In standard trust region based optimization methods, the trust region is gradually narrowed, suggesting that parameter $b$ here should evolve along time.   
The paper considers a special case of decision making processes with                                                                non Markovian reward functions, where conditioned on an unobserved task label                                                       the reward function becomes Markovian.                                                                                              A semi supervised loss for learning trajectory embeddings is proposed.                                                              The approach is tested on a multi task grid world environment and ablation                                                          studies are performed.                                                                                                                                                                                                                                                  The reviewers mainly criticize the experiments in the paper. The environments                                                       studied are quite simple, leaving it uncertain if the approach still works in                                                       more complex settings.                                                                                                              Apart from ablation results, no baselines were presented although the setting is                                                    similar to continual learning / multi task learning (with unobserved task label)                                                    where prior work does exist.                                                                                                        Furthermore, the writing was found to be partially lacking in clarity, although                                                     the authors addressed this in the rebuttal.                                                                                                                                                                                                                             The paper is somewhat below acceptance threshold, judging from reviews and my own                                                   reading, mostly due to lack of convincing experiments. Furthermore, the general setting                                             considered in this paper seems quite specific, and therefore of limited impact.
The paper is rejected based on unanimous reviews.
This paper proposes and evaluates a formulation of graph convolutional networks for multi relation graphs. The paper was reviewed by three experts working in this area and received three Weak Accept decisions. The reviewers identified some concerns, including novelty with respect to existing work and specific details of the experimental setup and results that were not clear. The authors have addressed most of these concerns in their response, including adding a table that explicitly explains the contribution with respect to existing work and clarifying the missing details. Given the unanimous Weak Accept decision, the ACs also recommend Accept as a poster.
In this work, the authors interpret the Transformer as a numerical ODE modelling multi particle convection. Guided by this connection, the authors take the Transformer that uses a feed forward net over attentions, and create a variant of transformer which instead uses an FFN attention FFN layer, thus the name macaron net. The authors present experiments in the GLUE dataset and in two MT datasets, and they overall report improved performance using their variant of Transformer. Thus, the main selling point of the paper is how seeing Transformer under his new light can potentially improve results through the construction of better models. The main criticisms from the authors is that  this story is not entirely convincing because the proposed variant departs a bit from the theory (R1 and comment about the Strang Marchuk splitting) and the papers does not consider an evaluation of accuracy of Macaron in solving the underlying set of ODEs (comment from R3). As such, I cannot recommend acceptance of this paper   I believe another set of revisions would increase the impact of this paper.
The paper introduces a variant of AMSGrad ("Optimistic AMSGrad"), which integrates an estimate of the future gradient into the optimization problem. While the method is interesting, reviewers agree that novelty is on the low side. The motivation of the approach should also be clarified. The experimental section should be made stronger; in particular, reporting convincing wall clock running time advantages is critical for validating the viability of the proposed approach.   
The paper proposes a neurally inspired model that is a variant of conv LSTM called V1net. The reviewers had trouble gleaning the main contributions of the work. Given that it is hard to obtain state of art results in neurally inspired architectures, the bar is much higher to demonstrate that there is value in pursuing these architectures. There are not enough convincing results in the paper to show this. I recommend rejection.
The paper makes a reasonable contribution to generative modeling for unsupervised scene decomposition.  The revision and rebuttal addressed the primary criticisms concerning the qualitative comparison and clarity, which caused some of the reviewers to increase their rating.  I think the authors have adequately addressed the reviewer concerns.  The final version of the paper should still strive to improve clarity, and strengthen the evaluation and ablation studies.
The authors propose a method for multi task learning with time series data. The reviewers found the paper interesting, but the majority found the description of the method in the paper confusing and several technical details missing. Moreover, the reviewers were not convinced that the technique used for uncertainty quantification of the features at each stage of the time series is well founded.
Main content: Proposes a deep RL unified framework to manage the trade off between static pruning to decrease storage requirements and network flexibility for dynamic pruning to decrease runtime costs Summary of discussion: reviewer1: Reviewer likes the proposed DRL approach, but writing and algorithmic details are lacking reviewer2: Pruning methods are certainly imortant, but there are details missing wrt the algorithm in the paper.  reviewer3: Presents a novel RL algorithm, showing good results on CIFAR10 and ISLVRC2012. Algorithmic details and parameters are not clearly explained.  Recommendation: All reviewers liked the work but the writing/algorithmic details are lacking. I recommend Reject. 
The authors addressed the issues raised by the reviewers, so I suggest the acceptance of this paper.
This paper proposes a learning framework for spiking neural networks that exploits the sparsity of the gradient during backpropagation to reduce the computational cost of training. The method is evaluated against prior works that use full precision gradients and shown comparable performance. Overall, the contribution of the paper is solid, and after a constructive rebuttal cycle, all reviewers reached a consensus of weak accept. Therefore, I recommend accepting this submission.
The paper seems technically correct and has some novelty, but the relevance of the paper is questionable. Considering the selectiveness of ICLR, I cannot recommend the paper for acceptance at this point.   In more detail: the authors propose a technique for estimating density rations between a target distribution of real samples and a distribution of samples generated by the model, without storing samples. The method seems to be technically well executed and verified. However, there was major concerns among multiple reviewers that the addressed problem does not seem relevant to the ICLR community. The question addressed seemed artificial, and it was not considered realistic (by R2 and also by R1 in the confidential discussion). R3 also expressed doubts at the usefulness of the method.   Furthermore, some doubts were expressed regarding clarity (although opinions were mixed on that) and on the justification of the modification of the VAE objective to the continual setting. 
The paper identifies the limitation of graph neural networks and proposed new variants of graph neural works. However, the reviewers feel that the theory of the paper have some problems:  1. A major concern is that the theoretical analyses in this paper are limited to graphs sampled from the SBM model. It is unclear how these analyses can be generalized to real graphs.  2. The robustness definition is inconsistent.  Furthermore, more extensive experiments on more datasets will also be helpful. 
The paper proposes two methods for interactive panoptic segmentation (a combination of semantic and instance segmentation) that leverages scribbles as supervision during inference. Reviewers had concerns about the novelty of the paper as it applies existing algorithms for this task and limited empirical comparison with other methods. Reviewers also suggested that ICLR may not be a good fit for the paper and I encourage the authors to consider submitting to a vision oriented conference. 
The paper analyses the importance of different DNN modules for generalization performance, explaining why certain architectures may be much better performing than others. All reviewers agree that this is an interesting paper with a novel and important contribution. 
This paper offers a new approach to cross modal embodied learning that aims to overcome limited vocabulary and other issues.  Reviews are mixed.  I concur with the two reviewers who say the work is interesting but the contribution is not sufficiently clear for acceptance at this time.
Main content:  Blind review #2 summarizes it well:  The aim of this work is to improve interpretability in time series prediction. To do so, they propose to use a relatively post hoc procedure which learns a sparse representation informed by gradients of the prediction objective under a trained model. In particular, given a trained next step classifier, they propose to train a sparse autoencoder with a combined objective of reconstruction and classification performance (while keeping the classifier fixed), so as to expose which features are useful for time series prediction.  Sparsity, and sparse auto encoders, have been widely used for the end of interpretability. In this sense, the crux of the approach is very well motivated by the literature.     Discussion:  All reviews had difficulties understanding the significance and novelty, which appears to have in large part arisen from the original submission not having sufficiently contextualized the motivation and strengths of the approach (especially for readers not already specialized in this exact subarea).     Recommendation and justification:  The reviews are uniformly low, probably due to the above factors, and while the authors  revisions during the rebuttal period have improved the objections, there are so many strong submissions that it would be difficult to justify override the very low reviewer scores.
The authors propose a novel distributed reinforcement learning algorithm that includes 3 new components: a target network for the policy for stability, a circular buffer, and truncated importance sampling. The authors demonstrate that this improves performance while decreasing wall clock training time.  Initially, reviewers were concerned about the fairness of hyper parameter tuning, the baseline implementation of algorithms, and the limited set of experiments done on the Atari games. After the author response, reviewers were satisfied with all 3 of those issues.  I may have missed it, but I did not see that code was being released with this paper. I think it would greatly increase the impact of the paper at the authors release source code, so I strongly encourage them to do so.  Generally, all the reviewers were in consensus that this is an interesting paper and I recommend acceptance.
All reviewers agreed that this submission is still premature to be accepted to ICLR2020. We hope the review comments are useful for improving your paper for potential future submission.
This paper aims to study the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The purpose is to understand the regime where the width and depth go to infinity together with a fixed ratio. The paper does not have a lot of numerical experiments to test the mathematical conclusions. In the discussion the reviewers concurred that the paper is interesting and has nice results but raised important points regarding the fact that only the diagonal elements are studied. This I think is the major limitation of this paper. Another issue raised was lack of experimental work validating the theory. Despite the limitations discussed above, overall I think this is an interesting and important area as it sheds light on how to move beyond the NTK regime. I also think studying this limit is very important to better understanding of neural network training. I recommend acceptance to ICLR.
The authors explore the use of flow based models for video prediction. The idea is interesting. The paper is well written. It is a good paper worthwhile presenting in ICLR.  For final version, we suggest that the authors can significantly improve the experiments: (1) report results on human motion datasets; (2) include the results by the FVD metric.  
This work addresses the problem of detecting an adversarial attack. This is a challenging problem as the detection mechanism itself is also vulnerable to attack. The paper proposes asymmetrical adversarial training as a robust solution. This approach partitions the feature space according to the output of the robust classifier and trains an adversarial example detector per partition. The paper demonstrates improvements over state of the art detection techniques.   All three reviewers recommend acceptance of this work. Some positive points include the paper being well written with strong experimental evidence. One potential difficulty with the proposed approach is the additional computational cost associated with a per class adversarial attack detector. The authors have responded to this concern by claiming that the straightforward version of their approach is K times slower (10 in the case of 10 classes), but their integrated version is 2x slower as they only run the detector associated with the example specific class prediction. We encourage the authors to include a discussion on computational cost in the final version. In addition, there was a community comment about black box testing which will be of relevance to many in the community. The authors have already provided additional experiments to address this question as well as code to reproduce the new experiment.   Overall, the paper addresses an important problem with a two step solution of training a robust model and detecting potentially perturbed samples per class. This is a novel solution with comprehensive experiments and therefore recommend acceptance.  
This paper addresses the problem of many to many cross domain mapping tasks with a double variational auto encoder architecture, making use of the normalizing flow based priors.  Reviewers and AC unanimously agree that it is a well written paper with a solid approach to a complicated real problem supported by good experimental results. There are still some concerns with confusing notations, and with human study to further validate their approach, which should be addressed in a future version.  I recommend acceptance.
The paper explores an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) that is better than random initialization and the approach is tested on various MNIST and TIMIT data sets with positive results.  Reviewer 3 raised concerns about the breadth of experiments and novelty. Reviewer 2 recognized that the model performs well on its MNIST baselines and had concerns about applicability to larger settings. Reviewer 1 acknowledges a very well written paper, but again raises concerns about the thoroughness of the experiments. The authors responded to all three reviewers, responding that the tasks were chosen to match existing work and that the approach is complementary to LSTMs to solve different tasks. Overall the reviewers did not re adjust their ratings.  There remains questions on scalability and generality, which makes the paper not yet ready for acceptance. We hope that the reviews support the authors further research.
The paper develops linear over parameterization methods to improve training of small neural network models. This is compared to training from scratch and other knowledge distillation methods.   Reviewer 1 found the paper to be clear with good analysis, and raised concerns on generality and extensiveness of experimental work. Reviewer 2 raised concerns about the correctness of the approach and laid out several other possibilities. The authors conducted several other experiments and responded to all the feedback from the reviewers, although there was no final consensus on the scores.  The review process has made this a better paper and it is of interest to the community. The paper demonstrates all the features of a good paper, but due to a large number of strong papers, was not accepted at this time.
This paper presents a transfer learning framework in neural topic modeling. Authors claim and reviewers agree that this view of transfer learning in the realm of topic modeling is novel.  However, after much deliberation and discussion among the reviewers, we conclude that this paper does not contribute sufficient novelty in terms of the method. Also, reviewers find the experiments and results not sufficiently convincing.  I sincerely thank the authors for submitting to ICLR and hope to see a revised paper in a future venue.
This paper proposes a differentiable inductive logic programming method in the vein of recent work on the topic, with efficiency focussed improvements. Thanks the very detailed comments and discussion with the reviewers, my view is that the paper is acceptable to ICLR. I am mindful of the reasons for reluctance from reviewer #3 — while these are not enough to reject the paper, I would strongly, *STRONGLY* advise the authors to consider adding a short section providing comparison to traditional ILP methods and NLM in their camera ready.
The paper proposes to combine a VAE model with the Optimal Transport to approximate some components of the model. The authors evaluate their approach on semi supervised problems and claim to obtain very competitive results compared to literature. Unfortunately, the paper would benefit substantially from revisions to make it easier to follow. For this reason, the paper is not ready for publication in this venue at this time.
This paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of VAEs to use deterministic autoencoders with an explicit regularization term on the latent representations. While the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation. The authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in ICLR 2020. 
This paper presents a method to compress DNNs by quantization. The core idea is to use NAS techniques to adaptively set quantization bits at each layer. The proposed method is shown to achieved good results on the standard benchmarks.  Through our final discussion, one reviewer agreed to raise the score from ‘Reject’ to ‘Weak Reject’,  but still on negative side. Another reviewer was not satisfied with the author’s rebuttal, particularly regarding the appropriateness of training strategy and evaluation. Moreover, as reviewers pointed out, there were so many unclear writings and explanations in the original manuscript. Although we admit that authors made great effort to address the comments, the revision seems too major and need to go through another complete peer reviewing. As there was no strong opinion to push this paper, I’d like to recommend rejection. 
The paper explores the idea of using implicit human feedback, gathered via EEG, to assist deep reinforcement learning. This is an interesting and at least somewhat novel idea. However, it is not clear that there is a good argument why it should work, or at least work well. The experiments carried are more exploratory than anything else, and it is not clear that much can be learned from the results. It s a proof of concept more than anything else, of the type that would work well for a workshop paper. More systematic empirical work would be needed for a good conference paper.  The authors did not provide a rebuttal to reviewers, but rather agreed with their comments and that the paper needs more work. In light of this, the paper should be rejected and we wish the authors best of luck with a new version of the paper. 
The paper investigates how to improve the performance of dropout and proposes an omnibus dropout strategy to reduce the correlation between the individual models.  All the reviewers felt that the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about novelty of the method relative to existing methods, significance of performance improvements and clarity of the presentation.   I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue. 
The paper in its current form was just not well enough received by the reviewers to warrant an acceptance rating. It seems this work may have promise and the authors are encouraged to continue with this line of work. 
The paper presents a new semi supervised boosting approach.   As reviewers pointed out and AC acknowledge, the paper is not ready to publish in various aspects: (a) limited novelty/contribution, (b) reproducibility issue and (c) arguable assumptions.  Hence, I recommend rejection.
This paper proposes a new algorithmic approach to reduced variance in off policy, policy gradient updates.  Multiple reviewers were concerned with both the soundness of the proposed approach, and the cost of using rollouts. In particular, the interaction between the target policy and the behavior policy, and how they are swapped was unclear, where the algorithms in the paper do not match the code provided.  The results show apparent reduction in variance across runs compared with TD3: clear improvements in two domains, minor improvements , and/or an increase in variance in others. In some domains there was decrease in mean performance. The reviewers wanted comparisons with other baseline methods (even in terms of variance across runs).  It is difficult to evaluate the results in this paper, as the performance is averaged over only 5 runs, and runs which result in "failure" are discarded from analysis. The authors explain this was done in the original TD3 code, and one can sympathise in following common practices in the literature. However, the consensus of the reviewers and the AC was that this choice was not well defended, obscures a key difficulty of the learning problem, and makes algorithms look considerably stronger then they actually are. This is particularly confounding in a paper about improving the robustness of learning algorithms. This is not acceptable empirical practice and we strongly encourage the authors to discontinue this.  The reviewers gave nice suggestions including changing the pitch of the paper, and including results in noisy tasks. To reduce the burden of doing more scientific experiments, we suggest the authors start with small or even designed problems to carefully study robustness of learning and the potential improvements due to their algorithm. After this is done in a statistically significant way, it would be natural to move to more demonstration style scaled up results.
The paper suggests an RL based approach to design a data valuation estimator. The reviewers agree that the proposed method is new and promising, but they also raised concerns about the empirical evaluations, including not comparing with other approaches of data valuation and limited ablation study. The authors provided a rebuttal to address these concerns. It improves the evaluation of one of the reviewers, but it is difficult to recommend acceptance given that we did not have a champion for this paper and the overall score is not high enough.
Perturbation based methods often produce artefacts that make the perturbed samples less realistic. This paper proposes to corrects this through use of an inpainter.  Authors claim that this results in more plausible perturbed samples and produces methods more robust to hyperparameter settings.  Reviewers found the work intuitive and well motivated, well written, and the experiments comprehensive. However they also had concerns about minimal novelty and unfair experimental comparisons, as well as inconclusive results. Authors response have not sufficiently addressed these concerns. Therefore, we recommend rejection.
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
The paper proposed an regret based approach to speed up counterfactural regret minimization. The reviewers find the proposed approach interesting. However, the method require large memory. More experimental comparisons and comparisons pointed out by reviewers and public comments will help improve the paper. 
This paper proposes an ensemble based active learning approach to select a subset of training data that yields the same or better performance. The proposed method is rather heuristic and lacks novel technical contribution that we expect for top ML conferences. No theoretical justification is provided to argue why the proposed method works. Additional studied are needed to fully convincingly demonstrate the benefit of the proposed method in terms computational cost. 
This is certainly a boarderline paper. The reviewers agreed this paper provides a good explanation and empirical justification of why popular normalization schemes don t help in DRL. The paper then proposes a simple scheme and demonstrates how it improves learning in several domains. The main concerns are the nature of these gains and how broadly useful the new approach is. In many cases there appear to be somewhat clear wins in the middle of the learning curves, but by the end of each experiment the errorbars overlap. The most clear results are those with TD3. There are some oddities here: using half SD error bars and smoothing both underline the concern about significance.   The reviewers requested more experiments and the authors provided three more domains: two in which their method appears better. These are not widely used benchmarks and it was hard to compare the performance of the baselines with fan et al (different setup) to evaluate the claims. The paper nicely provides lots of insight and empirical wisdom in the appendix, explaining how they got the algorithms to perform well.    
This paper studies the information theoretic complexity for emergent languages when pairs of neural networks are trained to solve a two player communication game. One of the primary claims of the paper was that under common training protocols, networks were biased towards low entropy solutions. During the discussion period, one reviewer shared an ipython notebook investigating the experiments shown in Figure 1. There it was discovered that low entropy solutions were only obtained for networks which were themselves initialized at low entropy configurations. When networks are initialized at high entropy configurations, the converged solution would remain high entropy. This experiment raises questions about the validity of the claim that there was "pressure" towards low entropy solutions to the task. Therefore, a more careful analysis of the phenomenon is required. 
This paper presents an approach combining multi agent with hierarchical RL in a custom made simulated humanoid robotics setting.  Although it is an interesting premise and has a compelling motivation (multi agent, real world interaction, humanoid robotics), the reviewers had some trouble pinpointing what the significant contributions are. Partly this is due to lack of clarity in the presentation, such as with overlong sections (eg 5.2), unclear descriptions, mistakes in the text, etc. Reviewers also remarked that this paper might be trying to do too much, without performing the necessary experiments/comparisons and analyses needed to interpret the contributions of each component.   This work is definitely promising and has the potential to make a nice contribution, given some additional care (experiments, analyses) and rewriting/polishing. As it is, it’s probably a bit premature for publication at ICLR. 
This paper addresses the challenge of time complexity in aggregating neighbourhood information in GCNs. As we aggregate information from larger hops (deeper neighbourhoods) the number of nodes can increases exponentially thereby increasing time complexity. To overcome this the authors propose a sampling method which samples nodes layer by layer based on bidirectional diffusion between layers. They demonstrate the effectiveness of their approach on 3 large benchmarks.  While the ideas presented in the paper were interesting the reviewers raised some concerns which I have summarised fellow:  1) Novelty: The reviewers felt that the techniques presented were not very novel and is very similar to one existing work as pointed out by R4 2) Writing: The writing needs to be improved. The authors have already made an attempt towards this but it could be improved further 3) Comparisons with baselines: R4 has raised some concerns  the settings/configurations used for the baseline methods. In particular, the results for the baseline methods are lower than those reported in the original papers. I have read the author s rebuttal for this but I am not completely convinced about it. I would suggest that the authors address this issue in subsequent submissions  Based on the above reasons I recommend that the paper cannot be accepted.    
Main summary:  Design an effective and economical model which spots keywords about pests and disease from community radio data in Luganda and English.  Discussions: all reviewers vote on rejecting the paper, due to lack of generalizability, training and evaluation discussion need work Recommendation: Reject
The main concern raised by reviewers is limited novelty, poor presentation, and limited experiments. All the reviewers appreciate the difficulty and importance of the problem. The rebuttal helped clarify novelty, but the other concerns remain.
This paper studies the trade off between the model size and quantization levels in quantized CNNs by varying different channel width multipliers. The paper is well  motivated and draws interesting observations but can be improved in terms of evaluation. It is a borderline case and rejection is made due to the high competition. 
The majority of reviewers suggest that this paper is not yet ready for publication. The idea presented in the paper is interesting, but there are concerns about what experiments are done, what papers are cited, and how polished the paper is. This all suggests that the paper could benefit from a bit more time to thoughtfully go through some of the criticisms, and make sure that everything reviewers suggest is covered.
The authors propose a multi scale architecture for generative flows that can learn which dimensions to pass through more flow layers based on a heuristic that judges the contribution to the likelihood. The authors compare the technique to some other flow based approaches. The reviewers asked for more experiments, which the authors delivered. However, the reviewers noted that a comparison to the SOTA for CIFAR in this setting was missing. Several reviewers raised their scores, but none were willing to argue for acceptance. 
This paper incorporates phrases within the transformer architecture.  The underlying idea is interesting, but the reviewers have raised serious concerns with both clarity and the trustworthiness of the experimental evaluation, and thus I cannot recommend acceptance at this time.
This paper proposes Restricted AutoEncoders (REAs) for unsupervised feature selection, and applies and evaluates it in applications in biology. The paper was reviewed by three experts. R1 recommends Weak Reject, identifying some specific technical concerns as well as questions about missing and unclear experimental details. R2 recommends Reject, with concerns about limited novelty and unconvincing experimental results. R3 recommends Weak Accept saying that the overall idea is good, but also feels the contribution is "severely undermined" by a recently published paper that proposes a very similar approach. Given that that paper (at ECMLPKDD 2019) was presented just one week before the deadline for ICLR, we would not have expected the authors to cite the paper. Nevertheless, given the concerns expressed by the other reviewers and the lack of an author response to help clarify the novelty, technical concerns, and missing details, we are not able to recommend acceptance. We believe the paper does have significant merit and hope that the reviewer comments will help authors in preparing a revision for another venue.
The paper proposes an attention mechanism for equivariant neural networks towards the goal of attending to co occurring features. It instantiates the approach with rotation and reflection transformations, and reports results on rotated MNIST and CIFAR 10. All reviewers have found the idea of using self attention on top of equivariant feature maps technically novel and sound. There were some concerns about readability which the authors should try to address in the final version. 
This paper report empirical implications of privacy ‘leaks’ in language models. Reviewers generally agree that the results look promising and interesting, but the paper isn’t fully developed yet. A few pointed out that framing the paper better to better indicate broader implications of the observed symptoms would greatly improve the paper. Another pointed out better placing this work in the context of other related work. Overall, this paper could use another cycle of polishing/enhancing the results.  
The author proposes a object oriented probabilistic generative model of 3D scenes.  The model is based on the GQN with the key innovation being that there is a separate 3D representation per object (vs a single one for the entire scene).  A scene volume map is used to prevent two objects from occupying the same space. The authors show that using this model, it s possible to learn the scene representation in an unsupervised manner (without the 3D ground truth).   The submission has received relatively low scores with one weak accept and 3 weak rejects.  All reviewers found the initial submission to be unclear and poorly written (with 1 reject and 3 weak rejects initially).  The initial submission also failed to acknowledge prior work on object based representations in the 3D vision community.  Based on the reviewer feedback, the authors greatly improved the paper by reworking the notation and the description of the model, and included a discussion of related work from 3D vision.  Overall, the exposition of the paper was substantially improved.  Some of the reviewers recognize the improvement, and lifted their scores.    However, the work still have some issues: 1. The experimental section is still weak The reviewers (especially those from an computer vision background) questioned the lack of baseline comparisons and ablation studies, which the authors (in their rebuttal) felt to be unnecessary. It is this AC s opinion that comparisons against alternatives and ablations is critical for scientific rigor, and high quality work aims not to just propose new models, but also to demonstrate via experimental analysis how the model compares to previous models, and what parts of the model is necessary, coming up with new metrics, baselines, and evaluation when needed.  It is the AC s opinion that the authors should attempt to compare against other methods/baselines when appropriate.  For instance, perhaps it would make sense to compare the proposed model against IODINE and MONet.  Upon closer examination of the experimental results, the AC also finds that the description of the object detection quality to be not very precise.  Is the evaluation in 2D or 3D?  The filtering of predictions that are too far away from any ground truth also seems unscientific.    2. The objects and arrangements considered in this paper is very simplistic.    3. The writing is still poor and need improvement. The paper needs an editing pass as the paper was substantially rewritten.  There are still grammar/typos, and unresolved references to Table ?? (page 8,9).   After considering the author responses and the reviewer feedback, the AC believe this work shows great promise but still need improvement.  The authors have tackled a challenging and exciting problem, and have provided a very interesting model.  The work can be strengthened by improving the experiments, analysis, and the writing.  The AC recommend the authors further iterate on the paper and resubmit.  As the revised paper was significantly different from the initial submission, an additional review cycle will also help ensure that the revised paper is properly fully evaluated.  The current reviewers are to be commended for taking the time and effort to look over the revision. 
This paper argues that incorporating unsupervised/semi supervised learning into the training process can dramatically increase the performance of models. In particular, its incorporation can result in performance gains that dwarf the gains obtained by collecting data actively alone. The experiments effectively demonstrate this phenomenon.   The paper is written with a tone that implicitly assumes that "active learning for deep learning is effective" and therefore it is a surprise and a challenge to the status quo that using unlabelled data in intelligent ways alone gets such a boost. On the contrary, reviewers found that active learning not working very well for deep learning is a well known state of affairs. This is not surprising because the most effective theoretically justifiable active learning algorithms rely on finite capacity assumptions about the model class, which deep learning disobeys.   Thus, the reviewers found the conclusions to lack novelty as the power of semi supervised and unsupervised learning is well known. Reject. 
Two reviewers are negative on this paper while the other one is slightly positive. Overall, this paper does not make the bar of ICLR. A reject is recommended.
Thank you very much for the detailed feedback to the reviewers, which helped us better understand your paper. Thanks also for revising the manuscript significantly; many parts were indeed revised.  However, due to the major revision, we find more points to be further discussed, which requires another round of reviews/rebuttals. For this reason, we decided not to accept this paper. We hope that the reviewers  comments are useful for improving the paper for potential future publication. 
The paper shows empirical evidence that the the optimal action value function Q* often has a low rank structure. It uses ideas from the matrix estimation/completion literature to provide a modification of value iteration that benefits from such a low rank structure. The reviewers are all positive about this paper. They find the idea novel and the writing clear. There have been some questions about the relation of this concept of rank to other definitions and usage of rank in the RL literature. The authors’ rebuttal seem to be satisfactory to the reviewers. Given these, I recommend acceptance of this paper.
This paper proposes a method for semi supervised semantic segmentation through consistency (with respect to various perturbations) regularization. While the reviewers believe that this paper contains interesting ideas and that it has been substantially improved from its original form, it is not yet ready for acceptance to ICLR 2020. With a little bit of polish, this paper is likely to be accepted at another venue.
Two reviewers are positive about this paper while the other reviewer is negative. The low scoring reviewer did not respond to discussions. I also read the paper and found it interesting. Thus an accept is recommended.
The general consensus amongst the reviewers is that this paper is not quite ready for publication. The reviewers raised several issues with your paper, which I hope will help you as you work towards finding a home for this work.
This paper proposes a framework for privacy preserving training of neural networks within a Trusted Execution Environment (TEE) such as Intel SGX. The reviewers found that this is a valuable research directions, but found that there were significant flaws in the experimental setup that need to be addressed. In particular, the paper does not run all the experiments in the same setup, which leads to the use of scaling factor in some cases. The reviewers found that this made it difficult to make sense of the results. The writing of this paper should be streamlined, along with the experiments before resubmission.
The authors propose two measures of calibration that don t simply rely on the top prediction. The reviewers gave a lot of useful feedback. Unfortunately, the authors didn t respond.
All the reviewers found the paper to contain an interesting idea with insightful experiments. The rebuttal further improved confidence of the reviewers. The paper is accepted.
This paper proposes a novel stochastic gradient Markov chain Monte Carlo method incorporating a cyclical step size schedule (cyclical SG MCMC).  The authors argue that this step size schedule allows the sampler to cross modes (when the step size is large) and locally explore modes (when the step size is smaller).  SG MCMC is a very promising method for Bayesian deep learning as it is both scalable and easily to incorporate into existing models.  However, the stochastic setting often leads to the sampler getting stuck in a local mode due to a requirement of a small step size (which itself is often due to leaving out the Metropolis Hastings accept / reject step).   The cyclic learning rate intuitively helps the sampler escape local modes.  This property is demonstrated on synthetic problems in comparison to existing SG MCMC baselines.  The authors demonstrate improved negative log likelihood on larger scale deep learning benchmarks, which is appreciated as the related literature often restricts experiments to small scale problems.  The reviewers all found the paper compelling and argued for acceptance and thus the recommendation is to accept.  Some questions remain for future work.  E.g. all experiments were performed using a very low temperature, which implies that the methods are not sampling from the true Bayesian posterior.  Why is such a low temperature needed for reasonable performance?  In any case a very nice paper.
The authors propose to learn space aware 3D feature abstractions of the world given 2.5D input, by minimizing 3D and 2D view contrastive prediction objectives. The work builds upon Tung et al. (2019) but extends it by removing some of the limitations, making it thus more general. To do so, they learn an inverse graphics network which takes as input 2.5D video and maps to a 3D feature maps of the scene. The authors present experiments on both real and simulation datasets  and their proposed approach is tested on feature learning, 3D moving object detection, and 3D motion estimation with good performance. All reviewers agree that this is an important problem in computer vision and the papers provides a working solution. The authors have done a good job with comparisons and make a clear case about their superiority of their model (large datasets, multiple tasks). Moreover, the rebuttal period has been quite productive, with the authors incorporating reviewers  comments in the manuscript, resulting thus in a stronger submission. Based in reviewer s comment and my own assessment, I think this paper should get accepted, as the experiments are solid with good results that the CV audience of ICLR would find relevant.  
The paper received scores of WR (R1) WR (R2) WA (R3), although R3 stated that they were borderline. The main issues were (i) lack of novelty and (ii) insufficient experiments. The AC has closely look at the reviews/comments/rebuttal and examined the paper. Unfortunately, the AC feels that with no one strongly advocating for acceptance, the paper cannot be accepted at this time. The authors should use the feedback from reviewers to improve their paper. 
This paper presents a dataset to evaluate the quality of embeddings learnt for source code. The dataset consists of three different subtasks: relatedness, similarity, and contextual similarity. The main contribution of the paper is the construction of these datasets which should be useful to the community. However, there are valid concerns raised about the size of the datasets (which is pretty small) and the baselines used to evaluate the embeddings   there should be a baselines using a contextual embeddings model like BERT which could have been fine tuned on the source code data. If these comments are addressed, the paper can be a good contribution in an NLP conference. As of now, I recommend a Rejection.
Both reviewers (we apologize for the lack of a 3rd review) did not feel the paper should be accepted. The rebuttal offered did not change the reviewer scores. So the paper cannot be accepted unfortunately. But the authors should use the feedback to improve their paper and resubmit.
This paper presents a capsule network to handle 3d point clouds which is equivariant to SO(3) rotations. It also provides the theoretical analysis to connect the dynamic routing approach to the Generalized Weiszfeld Iterations. The equivariant property of the method is demonstrated on classification and orientation estimation tasks of 3D shapes. While the technical contribution of the method is sound, the main concern raised by the reviewers was the lack of details in the presentation of methodology and results. Although the authors have made substantial efforts to update the paper, some reviewers were still not convinced and thus the scores remained the same. The paper was on the very borderline, but because of the limited capacity, I regret that I have to recommend rejection.  Invariances and equivariances are indeed important topics in representation learning, for which the capsule network is known as one of the promising approaches but still not well investigated compared to other standard architectures. I encourage authors to resubmit the paper taking in the reviewers  comments.
The reviewers generally expressed considerable reservation about the novelty of the proposed method. After reading the reviews in detail and looking at the paper, I m inclined to agree that the contribution is rather incremental. Using normalizing flows for representing policies in RL has been studied in a number of prior works, including with soft actor critic, and I think the novelty in this work is limited in relation to prior work. Therefore, I cannot recommend acceptance at this time.
This paper is consistently supported by all three reviewers during initial review and discussions. Thus an accept is recommended.
The paper proposes a novel method for embedding sequences of states and actions into a latent representation that enables efficient estimation of empowerment for an RL system. They use empowerment as intrinsic reward for safe exploration. While the reviewers agree that this paper has promise, they also agree that it is not quite ready for publication in its current state. In particular, the paper is lacking a theoretical justification for the proposed approach, the definition of empowerment used by the authors raised questions, and the manuscript would benefit from more clear and detailed description of the method. For these reasons I recommend rejection.
The reviewers have provided thorough reviews of your work. I encourage you to read them carefully should you decide to resubmit it to a later conference.
All reviewers gave this paper a score of 1. The AC recommends rejection.
This work proposes a GAN architecture that aims to align the latent representations of the generator with different interpretable degrees of freedom of the underlying data (e.g., size, pose).  Reviewers found this paper well motivated and the proposed method to be technically sound. However, they cast some doubts about the novelty of the approach, specifically with respect to DMWGAN and MADGAN. The AC shares these concerns and concludes that this paper will greatly benefit from an additional reviewing cycle that addresses the remaining concerns.  
This paper presents an ensemble method for reinforcement learning.  The method trains an ensemble of transition and reward models.  Each element of this ensemble has a different view of the data (for example, ablated observation pixels) and a different latent space for its models.  A single (collective) policy is then trained, by learning from trajectories generated from each of the models in the ensemble.  The collective policy makes direct use of the latent spaces and models in the ensemble by means of a translator that maps one latent space into all the other latent spaces, and an aggregator that combines all the model outputs.  The method is evaluated on the CarRacing and VizDoom environments.    The reviewers raised several concerns about the paper. The evaluations were not convincing with artificially weak baselines and only worked well in one of the two tested environments (reviewer 2). The paper does not adequately connect to related work on model based RL (reviewer 1 and 2). The paper does not motivate its artificial setting (reviewer 2 and 1).  The paper s presentation lacks clarity from using non standard terminology and notation without adequate explanation (reviewer 1 and 3).  Technical aspects of the translator component were also unclear to multiple reviewers (reviewers 1, 2 and 3).  The authors found the review comments to be helpful for future work, but provided no additional clarifications.  The paper is not ready for publication.
This paper extends the degree to which ReLU networks can be provably resistant to a broader class of adversarial attacks using a MMR Universal regularization scheme.  In particular, the first provably robust model in terms of lp norm perturbations is developed, where robustness holds with respect to *any* p greater than or equal to one (as opposed to prior work that may only apply to specific lp norm perturbations).  While I support accepting this paper based on the strong reviews and significant technical contribution, one potential drawback is the lack of empirical tests with a broader cohort of representative CNN architectures (as pointed out by R1).  In this regard, the rebuttal promises that additional experiments with larger models will be added in the future to the final version, but obviously such results cannot be used to evaluate performance at this time.
Main content:  Blind review #1 summarizes it well:  his paper claims to be the first to tackle unconditional singing voice generation. It is noted that previous singing voice generation approaches leverage explicit pitch information (either of an accompaniment via a score or for the voice itself), and/or specified lyrics the voice should sing. The authors first create their own dataset of singing voice data with accompaniments, then use a GAN to generate singing voice waveforms in three different settings: 1) Free singer   only noise as input, completely unconditional singing sampling 2) Accompanied singer   Providing the accompaniment *waveform* (not symbolic data like a score   the model needs to learn how to transcribe to use this information) as a condition for the singing voice 3) Solo singer   The same setting as 1 but the model first generates an accompaniment then, from that, generates singing voice     Discussion:  The reviews generally point out that while a lot of new work has been done, this paper bites off too much at once: it tackles many different open problems, in a generative art domain where evaluation is subjective.     Recommendation and justification:  This paper is a weak reject, not because it is uninteresting or bad work, but because the ambitious scope is really too large for a single conference paper. In a more specialized conference like ISMIR, it would still have a good chance. The authors should break it down into conference sized chunks, and address more of the reviewer comments in each chunk.
This paper presents a quantization scheme with the advantage of high computational efﬁciency. The experimental results show that the proposed scheme outperforms SOTA methods and is competitive with the full precision models. The reviewers initially raised some concerns including baseline ResNet performance,  detailed comparison of the quantization size, and comparison with ResNet50. Authors addressed these concerns in the rebuttal and revised the draft to accommodate the requested items. The reviewers appreciated the revision and find it highly improved. Their overall recommendation is toward accept, which I also support.
The main contribution of this work is introducing the uncertainty aware value function prediction into model based RL, which can be used to balance the risk and return empirically.   The reviewers generally agree that this paper addresses an interesting problem, but there are some concerns that remain (see reviewer comments).   I also want to highlight that in terms of empirical results, it is insufficient to present results for 3 different random seeds. To highlight any kind of robustness, I suggest *at least* 10 20 different random seeds; otherwise the findings can/will be misleading. 
The paper received 3, 3, 6. All reviewers agree that the method is technically interesting. The main concern shared by the reviewers are the experiments which are somewhat underwhelming. The AC believes that this is a solid technical paper that needs a little bit more work. The authors are encouraged to strengthen their evaluation and resubmit to a future conference.
This paper presents to integrate the codes based on multiple hashing functions with Transformer networks to reduce vocabulary sizes in input and output spaces. Compared to non hashed models, it enables training more complex and powerful models with the same number of overall parameters, thus leads to better performance.  Although the technical contribution is limited considering hash based approach itself is rather well known and straightforward, all reviewers agree that some findings in the experiments are interesting. On the cons side, two reviewers were concerned about unclear presentation regarding the details of the method. More importantly, the proposed method is only evaluated on non standard tasks without comparison to other previous methods. Considering that the main contribution of the paper is in empirical side, I agree it is necessary to evaluate the method on more standard benchmarking tasks in NLP where there should be many other state of the art methods of model compression. For these reasons, I’d like to recommend rejection. 
This paper explores the practice of using lower dimensional embeddings to perform Bayesian optimization on high dimensional problems.  The authors identify several issues with performing such an optimization on a lower dimensional projection and propose solutions leading to better empirical performance of the optimization routine.  Overall the reviewers found the work well written and enjoyable.  However, the reviewers were concerned primarily about the connection to existing literature (R2) and the empirical analysis (R1, R3).  The authors claim that their method outperforms state of the art on a range of problems but the reviewers did not feel there was sufficient empirical evidence to back up this claim.   Unfortunately, as such the paper is not quite ready for publication.  The authors claim to have significantly expanded the experiments in the response period, however, which will likely make it much stronger for a future submission. 
The paper considers the setting of constrained MDPs and proposes using backward value functions to keep track of the constraints.  All reviewers agreed that the idea of backward value functions is interesting, but there were a few technical concerns raised, and the reviewers remained unconvinced after the rebuttal. In particular, there were doubts whether the method actually makes sense for the considered problem (the backward VF averaging constraints over all trajectories, instead of only considering the current one), and a concern about insufficient baseline comparisons.  I recommend rejection at this time, but encourage the authors to take the feedback into account, make the paper more crisp, and resubmit to a future venue.
A new software framework fo Deep RL is introduced. This is a useful work for the community, but it is not a research work. I agree with Reviewer4 that somehow it is not a right venue: other papers need to have technical contributions, SOTA, and here   it is difficult but it is another type of work   accurate technical implementation and commenting. I do not feel right to have as it a paper on ICLR. 
This paper addresses the problem of estimating treatment responses involving a continuous dosage parameter.  The basic idea is to learn a GAN model capable of generating synthetic dose response curves for each training sample, which then facilitates the supervised training of an inference model that estimates these curves for new cases.  For this purpose, specialized architectures are also proposed for the GAN, which involves a multi task generator network and a hierarchical discriminator network.  Empirical results demonstrate improvement over existing methods.  While there is always a chance that reviewers may underappreciate certain aspects of a submission, the fact that there was a unanimous decision to reject this work indicates that the contribution must be better marketed to the ML community.  For example, after the rebuttal one reviewer remained unconvinced regarding explanations for why the proposed method is likely to learn the full potential outcome distribution.  Among other things, another reviewer felt that both the proposed DRGAN model, and the GANITE framework upon which it is based, were not necessarily working as advertised in the present context.
This paper addresses a promising method for order learning and applies the new ideas of multiple chain learning and anchor selection to age estimation and aesthetic regression. The decision regarding instance class is made by comparing it with anchor instances in the same chain and maximizing the consistency among the comparison results. In a multi chain setting, each chain may correspond to a higher level attribute class, for example, gender or ethnic group. Supervised and unsupervised learning of multiple ordered chains is proposed.  As rightly acknowledged by R4: “What more promising is the unsupervised chains, which could automatically search for a more optimal multi chain division scheme than the pre defined data division.” All three reviewers and AC agree that the proposed approach is interesting and shows promising results. There are several potential weaknesses and suggestions to further strengthen this work: (1) more quantitative results are needed for assessing the benefits of this approach (R3, R4)   see R3’s request to complete the results for FG Net, to include the results for CLAP2016 and a comparison with the SOTA method BridgeNet. Pleased to report that the authors have revised the manuscript and have included performance of the arithmetic scheme as well as the geometric scheme for FG Net. Also the authors have provided some initial evaluations of BridgeNet and promised to report the final results as well as the results for CLAP2016 in the final version.  (2) R3 and R4 have expressed concerns regarding using the geometric ratio of the class distances in age estimation and that the improvement may be caused by the data distribution that favours it (R4) or because the baseline methods are not fine tuned in the same manner (R3). The authors have partially addressed this concern in the rebuttal.  There is a large body of work in computer vision that is focused on relative comparison of samples based on attributes (e.g. age) that is not clearly articulated in the discussions / baseline comparisons (1CH)   see the seminal work [Relative attributes by Parikh and Grauman, ICCV2011] and the follow up works.  Considering the author response, the AC decided that the most crucial concerns have been addressed in the revision and that the paper could be accepted, but the authors are strongly urged to include additional results that were promised in the rebuttal for the final revision.  
The paper aims to model fake news by drawing tools from multi agent reinforcement learning. After the discussion period, there is a consensus among the reviewers that the paper lacks novel technical contributions. The reviewers also acknowledge that paper also doesn t quite deliver a practical solution as claimed by the authors.
This paper does extensive experiments to understand the lottery ticket hypothesis. The lottery ticket hypothesis is that there exist sparse sub networks inside dense large models that achieve as good accuracy as the original model. The reviewers have issues with the novelty and significance of these experiments. They felt that it didn t shed new scientific light. They felt that epochs needed to do early detection was still expensive. I recommend doing further studies and submitting it to another venue.
All reviewers voted to accept this paper. The AC recommends acceptance.
This paper uses neural amortized inference for clustering processes to automatically tune the number of clusters based on the observed data. The main contribution of the paper is the design of the posterior parametrization based on the DeepSet method.  The reviewers feel that the paper has limited novelty since it mainly follows from existing methodologies. Also, experiments are limited and not all comparisons are made. 
The submission presents an approach to speed up network training time by using lower precision representations and computation to begin with and then dynamically increasing the precision from 8 to 32 bits over the course of training. The results show that the same accuracy can be obtained while achieving a moderate speed up.   The reviewers were agreed that the paper did not offer a signficant advantage or novelty, and that the method was somewhat ad hoc and unclear. Unfortunately, the authors  rebuttal did not clarify all of these points, and the recommendation after discussion is for rejection. 
This paper tackles the problem of autonomous skill discovery by recursively chaining skills backwards from the goal in a deep learning setting, taking the initial conditions of one skill to be the goal of the previous one. The approach is evaluated on several domains and compared against other state of the art algorithms.  This is clearly a novel and interesting paper. Two minor outstanding issues are that the domains are all related to navigation, and it would be interesting to see the approach on other domains, and that the method involves a fair bit of engineering in piecing different methods together. Regardless, this paper should be accepted.
This paper extends recent multi step dynamic programming algorithms to reinforcement learning with function approximation.  In particular, the paper extends h step optimal Bellman operators (and associated k PI and k VI algorithms) to deep reinforcement learning.  The paper describes new extensions to DQN and TRPO algorithms.  This approach is claimed to reduce the instability of model free algorithms, and the approach is tested on Atari and Mujoco domains.   The reviewers noticed several limitations of the work.  The reviewers found little theoretical contribution in this work and they were unsatisfied with the empirical contributions.  The reviewers were unconvinced of the strength and clarity of the empirical results with the Atari and Mujoco domains along with the deep learning network architectures.  The reviewers suggested that simpler domains with a simpler function approximation scheme could enable more through experiments and more conclusive results.  The claim in the abstract of addressing the instabilities was also not adequately studied in the paper.  This paper is not ready for publication.  The primary contribution of this work is the empirical evaluation, and the evaluation is not sufficiently clear for the reviewers.
Unfortunately, the reviewers of the paper are all not certain about their review, none of them being RL experts.  Assessing the paper myself—not being an RL expert but having experience—the authors have addressed all points of the reviewers thoroughly.   
This paper proposes distributionally robust optimization (DRO) to learn robust models that minimize worst case training loss over a set of pre defined groups. They find that increased regularization is necessary for worst group performance in the overparametrized regime (something that is not needed for non robust average performance).  This is an interesting paper and I recommend acceptance. The discussion phase suggested a change in the title which slightly overstated the paper s contributions (a comment which I agree with). The authors agreed to change the title in the final version.  
This paper combines a well known, recently proposed unsupervised representation learning technique technique with a class conditional negative log likelihood and a squared hinge loss on the class wise conditional likelihoods, and proposes to use the resulting conditional density model for generative classification. The empirical work appears to validate the claim that their method leads to good out of distribution detection, and better performance using a rejection option. The adversarial defense results are less clear. Reporting raw logits is a strange choice, and difficult to interpret; the table is also difficult to read, and this method of reporting makes it difficult to compare against existing methods.  The reviewers generally remarked on presentation issues. R1 asked about the contribution of various loss terms, a matter I feel is underexplored in this work, and the authors mainly replied with a qualitative description of loss behaviour in the joint system, which I don t believe was the question. R1 also asked about the choice of thresholds and the issues of fairness of comparison regarding model capacity, neither of which seemed adequately addressed. R3 remarked on the clarity being lacking, and also that "Generative modeling of representations is novel, afaik." (It is not; see, for example, the VQ VAE line of work where PixelCNN priors are fit on top of representations, and layer wise pre training works of the mid 2000s, where generative models were frequently fit on greedily trained feature representations, sometimes in conjunction with a joint generative model of class labels).  R2 s review was very brief, and with a self reported low confidence, but their concerns were addressed in a subsequent update.  There are three weaknesses which are my grounds for recommending rejection. First, this paper does a poor job of situating itself in the wider body of literature on classification with rejection, which dates to at least the 1970s (see Bartlett & Wengkamp, 2006 and the references therein). Second, the empirical work makes little comparison to other methods in the literature; baselines on clean data are self generated, and the paper compares to no other adversarial defense proposals. In a minor drawback, ImageNet results are also missing; given that one of the purported advantages of the method is scalability, a large scale benchmark would have strengthened this claim. Third, no ablation study is undertaken that might give us insight into the role of each term of the loss. Given that this is a straightforward combination of well understood techniques, a fully empirical paper ought to deliver more insight into the combination than this manuscript has.
The authors provide an analysis of a cross lingual data augmentation technique which they call XLDA. This consists of replacing a segment of an input text with its translation in another language. They show that when fine tuning, it is more beneficial to train on the cross lingual hypotheses than on the in language pairs, especially for low resource languages such as Greek, Turkish and Urdu. The paper explores an interesting idea however they lack comparison with other techniques such as backtranslation and XLM models, and would benefit from a wider range of tasks. I feel like this paper is more suitable for an NLP focussed venue. 
The paper is a contribution to the recently emerging literature on learning                                                         based approaches to combinatorial optimization.                                                                                     The authors propose to pre train a policy network to imitate SOTA solvers for                                                       TSPs.                                                                                                                               At test time, this policy is then improved, in an alpha go like manner, with                                                        MCTS, using beam search rollouts to estimate bootstrap values.                                                                                                                                                                                                          The main concerns raised by the reviewers is lack of novelty (the proposed                                                          algorithm is a straight forward application of graph NNs to MCTS) as well a the                                                     experimental results.                                                                                                               Although comparing well to other learning based methods, the algorithm is far                                                       away from the performance of SOTA solvers.                                                                                                                                                                                                                              Although well written, the paper is below acceptance threshold.                                                                     The methodological novelty is low.                                                                                                  The reported results are an order of magnitude away from SOTA solvers, while previous work                                          has already reported the general feasibility of learned solvers to TPSs.                                                            Furthermore, the overall contribution is somewhat unclear as the policy relies                                                      on pre training with solutions form existing solvers. 
This submission proposes an image generation technique for composing concepts by combining their associated distributions.   Strengths:  The approach is interesting and novel.  Weaknesses:  Several reviewers were not convinced about the correctness of the formulations for negation and disjunction.  The experimental validation of the disjunction and negation approaches is insufficient.  The paper clarity and exposition could be improved. The authors addressed this in the discussion but concerns remain.  Given the weaknesses, AC shares R3’s recommendation to reject.
This paper proposes CEB, Conditional Entropy Bottleneck, as a way to improves the robustness of a model against adversarial attacks and noisy data. The model is tested empirically using several experiments and various datasets.  We appreciate the authors for submitting the paper to ICLR and providing detailed responses to the reviewers  comments and concerns. After the initial reviews and rebuttal, we had extensive discussions to judge whether the contributions are clear and sufficient for publication. In particular, we discussed the overlap with a previous (arXiv) paper and decided that the overlap should not be considered because it is not published at a conference or journal. Plus the paper makes additional contributions.  However, reviewers in the end did not think the paper showed sufficient explanation and proof of why and how this model works, and whether this approach improves upon other state of the art adversarial defense approaches.  Again, thank you for submitting to ICLR, and I hope to see an improved version in a future publication.
This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work (see in particular the updates from AnonReviewer1), and I urge the authors to continue to develop refinements and extensions.
 This paper presents an empirical analysis of the reasons behind BatchNorm vulnerability to adversarial inputs, based on the hypothesis that such vulnerability may be caused by using different statistics during the inference stage as compared to the training stage. While the paper is interesting and clearly written, reviewers point out insufficient empirical evaluation in order to make the claim more convincing.
This paper proposes a communication efficient data parallel SGD with quantization. The method bridges the gap between theory and practice. The QSGD method has theoretical guarantees while QSGDinf doesn t, but the latter gives better result. This paper proves stronger results for QSGD using a different quantization scheme which matches the performance of QSGDinf.  The reviewers find issues with the approach and have pointed some of them out. During the discussion period, we did discuss if reviewers would like to raise their scores. Unfortunately, they still have unresolved issues (see R1 s comment).  R1 made another comment recently that they were unable to add to their review: "The proposed algorithm and the theoretical analysis does not include momentum. However, in the experiments, it is clearly stated that momentum (with a factor of 0.9) is used. Thus, it is unclear whether the experiments really validate the theoretical guarantees. And, it is also unclear how momentum is added for both NUQSGD and EF SGD, since momentum is not mentioned in Algorithm 1 in this paper, or the paper of QSGD, or the paper of EF SignSGD. (There is a version of SignSGD with momentum *without* error feedback, called SIGNUM)."  With the current score, the paper does not make the cut for ICLR, but I encourage the authors to revise the paper based on reviewers  feedback. For now, I recommend to reject this paper.
This paper proposes a novel architecture for learning Hamiltonian dynamics from data. The model outperforms the existing state of the art Hamiltonian Neural Networks on challenging physical datasets. It also goes further by proposing a way to deal with observation noise and a way to model stiff dynamical systems, like bouncing balls. The paper is well written, the model works well and the experimental evaluation is solid. All reviewers agree that this is an excellent contribution to the field, hence I am happy to recommend acceptance as an oral.
This paper describes a new language model that captures both the position of words, and their order relationships.  This redefines word embeddings (previously thought of as fixed and independent vectors) to be functions of position.  This idea is implemented in several models (CNN, RNN and Transformer NNs) to show improvements on multiple tasks and datasets.  One reviewer asked for additional experiments, which the authors provided, and which still supported their methodology.   In the end, the reviewers agreed this paper should be accepted.
The reviewers reached a unanimous consensus that the paper could not be accepted for publication in its current form. There were a number of concerns raised regarding (1) the clarity of the writing; (2) the comparisons, especially to prior work; (3) the details of the experimental setup.
The authors present the task lift the flap where an agent (artificial or human) is presented with a blurred image and a hidden item. The agent can de blur the parts of the image by clicking on it. The authors introduce a model for this task (ClickNet) and they compare this against others. As reviewers point, this paper presents an interesting set of experiments and analyses. Overall, this type of work can be quite influential as it gives an alternative way to improve our models by unveiling human strategies and using those as inductive biases for our models. That being said, I find the conclusions of this paper quite narrow for the general audience of ICLR (as R2 and R3 also point), as authors look into an artificial task and show ClickNet performs well. But what have we learned beyond that? How do we use these results to improve either our models or our understanding of these models? I believe these are the type of questions that  are missing from the current version of the paper and that if answered would greatly increase its impact and relevance to the ICLR community. At the moment though, I cannot recommend this paper for acceptance. 
This paper is very different from most ICLR submissions, and appears to be addressing interesting themes.  However the paper seems poorly written, and generally unclear.  The motivation, task, method and evaluation are all unclear.  I recommend that the authors add explicit definitions, equations, algorithm boxes, and more examples to make their paper clearer.
The authors propose an alternative to batch norm, which they call POP norm, and provide theoretical justification for POP norm in nonconvex optimization on the basis of variance reduction. They then present empirical arguments.  One of the most cogent reviewers believed the theoretical results were known and the empirical arguments unconvincing because the method is similar to batch norm up to a change in learning rate and some minor differences.  Unfortunately, the reviewers did not engage with the author rebuttals at all. The authors seem to have addressed most points. However, if the reviewers are unwilling to engage, despite multiple emails, there s not much I can do, short of redoing the whole process from scratch. And I ll take the lack of engagement as lack of interest by the reviewers. Not being an expert in optimization myself, I m not going to override the scores. I do know enough to know that there are standard bounds for both convex and nonconvex optimization that improve with decreased variance. 
This paper presents a meta learning algorithm that represents uncertainty both at the meta level and at the task level. The approach contains an interesting combination of techniques. The reviewers raised concerns about the thoroughness of the experiments, which were resolved in a convincing way in the rebuttal. Concerns about clarity remain, and the authors are *strongly encouraged* to revise the paper throughout to make the presentation more clear and understandable, including to readers who do not have a meta learning background. See the reviewer s comments for further details on how the organization of the paper and the presentation of the ideas can be improved.
The paper examines the idea that real world data is highly structured / lies on a low dimensional manifold. The authors show differences in neural network dynamics when trained on structured (MNIST) vs. unstructured datasets (random), and show that "structure" can be captured by their new "hidden manifold" generative model that explicitly considers some low dimensional manifold.  The reviewers perceived a lack of actionable insights following the paper, since in general these ideas are known, and for MNIST to be a limited dataset, despite finding the paper generally clear and correct.  Following the discussion, I must recommend rejection at this time, but highly encourage the authors to take the insights developed in the paper a bit further and submit to another venue. E.g. trying to improve our algorithms by considering the inductive bias of structure of the hidden manifold, or developing a systematic and quantifiable notion of structure for many different datasets that correlate with difficulty of training would both be great contributions.
The reviewers generally reached a consensus that the work is not quite ready for acceptance in its current form. The central concerns were about the potentially limited novelty of the method, and the fact that it was not quite clear how good the annotations needed to be (or how robust the method would be to imperfect annotations). This, combined with an evaluation scenario that is non standard and requires some guesswork to understand its difficulty, leaves one with the impression that it is not quite clear from the experiments whether the method really works well. I would recommend for the authors to improve the evaluation in the next submission.
The paper proposed new version of LSTM which is claimed to abandon the redundancies in LSTM.  It is weak both in theory and experiments.  All reviewers gave clear rejects and the AC agree.
This paper proposes to use a generative adversarial network to train a substitute that replicates (imitates) a learned model under attack. It then shows that the adversarial examples for the substitute can be effectively used to attack the learned model. The proposed approach leads to better success rates of attacking than other substitute training approaches that require more training examples. The condition to get a well trained imitation model is that a sufficient number of queries are obtained from the target model. This paper has valuable contributions by developing an imitation attacker. However, some key issues remain. In particular, I agree with R1 that the average number of queries per image is relatively high, even during training. In the rebuttal, the authors made the assumption that “suppose their method could make an infinite number of queries for target models”, which is unfortunately not realistic. Another point that I found confusing: at testing, I don’t see how you can use the imitation model D to generate adversarial samples (D is a discriminative model, not a generator); it should be G, right? 
This paper presents a method that hybridizes the strategies of linear programming and interval bound propagation to improve adversarial robustness.  While some reviewers have concerns about the novelty of the underlying ideas presented, the method is an improvement to the SOTA in certifiable robustness, and has become a benchmark method within this class of defenses.
This paper proposes two contributions to improve uncertainty in deep learning.  The first is a Mahalanobis distance based statistical test and the second a model architecture.  Unfortunately, the reviewers found the message of the paper somewhat confusing and particularly didn t understand the connection between these two contributions.   A major question from the reviewers is why the proposed statistical test is better than using a proper scoring rule such as negative log likelihood.  Some empirical justification of this should be presented.
This paper proposes a modification to the Transformer architecture in which the self attention and feed forward layer are merged into a self attention layer with "persistent" memory vectors. This involves concatenating the contextual representations with global, learned memory vectors, which are attended over. Experiments show slight gains in character and word level language modeling benchmarks.   While the proposed architectural changes are interesting, they are also rather minor and had a small impact in performance and in number of model parameters. The motivation of the persistent memory vector as replacing the FF layer is a bit tenuous since Eqs 5 and 9 are substantially different. Overall the contribution seems a bit thin for a ICLR paper. I suggest more analysis and possibly experimentation in other tasks in a future iteration of this paper.
This paper proposes a new formulation of the non local block and interpret it from the graph view. The idea is interesting and the experimental results seems to be promising.  Reviewer has two major concerns. The first is the presentation, which is not clear enough. The second is the experimental design and analysis. The authors add more video dataset in the revision, but still lack comprehensive experimental analysis for video based applications.   Overall, the idea of non local block from graph view is interesting. However, the presentation of the paper needs further polish and thus does not meet the standard of ICLR 
The paper does not provide theory or experiment to justify the various proposed relaxations. In its current form, it has very limited scope.
This submission analyses the numerical invertibility of analytically invertible neural networks and shows that analytical invertibility does not guarantee numerical invertibility of some invertible networks under certain conditions (e.g. adversarial perturbation).  Strengths:  The work is interesting and the theoretical analysis is insightful.  Weaknesses:  The main concern shared by all reviewers was the weakness of the experimental section including (i) insufficient motivation of the decorrelation task; (ii) missing comparisons and experimental settings.  The paper clarity could be improved.  Both weaknesses were not sufficiently addressed in the rebuttal. All reviewer recommendations were borderline to reject. 
This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.
This paper presents the neural stored program memory, which is a key value memory that is used to store weights for another neural network, analogous to having programs in computers. They provide an extensive set of experiments in various domains to show the benefit of the proposed method, including synthetic tasks and few shot learning experiments.  This is an interesting paper proposing a new idea. We discuss this submission extensively and based on our discussion I recommend accepting this submission.   A few final comments from reviewers for the authors:   Please try to make the paper a bit more self contained so that it is more useful to a general audience. This can be done by either making more space in the main text (e.g., reducing the size of Figure 1, reducing space between sections, table captions and text, etc.) or adding more details in the Appendix. Importantly, your formatting is a bit off. Please use the correct style file, it will give you more space. All reviewers agree that the paper are missing some important details that would improve the paper.   Please cite the original fast weight paper by Malsburg (1981).   Regarding fast weights using outer products, this was actually first done in the 1993 paper instead of the 2016 and 2017 papers.
While the reviewers generally appreciated the ideas presented in the paper and found the overall aims and motivation of the paper to be compelling, there were too many questions raised about the experiments and the soundness of the technical formulation to accept the paper at this time, and the reviewers did not feel that the authors had adequately addressed these issues in their responses. The main concerns were (1) with the correctness and rigor of the technical derivation, which the reviewers generally found to be somewhat questionable   while the main idea seems reasonable, the details have a few too many question marks; (2) the experimental results have a number of shortcomings that make it difficult to fully understand whether the method really works, and how well.
Reviewer worries include: whether the approach scales to distant language pairs, overselling of the paper as a "framework", a few citations and comparisons missing. I agree and encourage the authors not to use the word "framework" here. I would also encourage the authors to evaluate on more interesting language pairs, and analyze what vocabularies are relocated, as well as what their method is better at compared to previous work. 
The authors propose a novel model based reinforcement learning algorithm. The key difference with previous approaches is that the authors use gradients through the learned model. They present theoretical results on error bounds for their approach and a monotonic improvement theorem. In the small sample regime, they show improved performance over previous approaches.  After the revisions, reviewers raised a few concerns: The results are only for 100,000 steps, which does not support the claim that the models achieves the same asymptotic performance as model – free algorithms would. The results would be stronger as the experiments were run with more than 3 random seats. In the revised version of the text, it s unclear if the authors are using target networks.  Overall, I think the paper introduces some interesting ideas and shows improved performance over existing approaches. I recommend acceptance on the condition that the authors tone down their claims or back them up with empirical evidence. Currently, I don t see evidence for the claim that the method achieves similar asymptotic performance to model free algorithms or the claim that their approach allows for longer horizons than previous approaches.
The authors present a new benchmark for evaluating a plethora of models on a variety of tasks. In terms of scores, the paper received a borderline rating, with two reviews being rather superficial unfortunately. The last reviewer was positive. The reviewers generally agreed that the benchmark is interesting and carries value, and the AC agrees. Authors certainly invested a significant effort in designing the benchmark and performing a detailed analysis over several tasks and methods. However, the effort seems more engineering in nature and insights are somewhat lacking. For an experimental paper, presenting the results is interesting yet not sufficient. A much more in depth analysis and discuss would warrant a deeper understanding of the results and open directions for future work. This part is currently underwhelming. 
The paper proposes a new way of estimating treatment effects from observational data. The text is clear and experiments support the proposed model.
This paper presents an approach for the long tailed image classification, where the class frequencies during (supervised) training of an image classifier are heavily skewed, so that the classifier underfits on under represented classes. The authors  responses to the reviews clarified most of their  concerns, although some reviewers pointed out that some of the details regarding experiments such as the construction of the validation set and the selection of balanced/imbalanced sets remain unclear. Overall, we believe this paper contains interesting observations to be shared.
The authors propose Group Connected Multilayer Perceptron Networks which allow expressive feature combinations to learn meaningful deep representations. They experiment with different datasets and show that the proposed method gives improved performance.   The authors have done a commendable job of replying to the queries of the reviewers and addresses many of their concerns. However, the main concern still remains: The improvements are not very significant on most datasets except the MNIST dataset. I understand the author s argument that other papers have also reported small improvements on these datasets and hence it is ok to report small improvements. However, the reviewers and the AC did not find this argument very convincing. Given that this is not a theoretical paper and that the novelty is not very high (as pointed out by R1) strong empirical results are accepted.  Hence, at this point, I recommend that the paper cannot be accepted.  
The paper proposes an intuitive causal explanation for the generalization properties of GD methods. The reviewers appreciated the insights, with one reviewer claiming that there was significant overlap with existing work.  I ultimately decided to accept this paper as I believe intuitive explanations are critical to the propagation of ideas. That being said, there is a tendency in this community to erase past, especially theoretical, work, for that very reason that theoretical work is less popular.  Hence, I want to make it clear that the acceptance of this paper is based on the premise that the authors will incorporate all of reviewer 3 s comments and give enough credit to all relevant work (namely, all the papers cited by the reviewer) with a proper discussion on the link between these.
This paper presents an analysis on different methods of noise injection in adversarial examples, using gaussian noise for example. There are important issues raised by reviewers 1 & 2 about some conclusions not being well supported by the experiments and the utility/importance of some conclusions. After a discussion among reviewers, as of now all 3 reviewers stand by the decision that substantial improvements, and analysis can be made in the paper. Thus, Im recommending a Rejection.
The paper proposes ATR CSPD, which learns a low dimensional representation of seasonal pattern, for detecting changes with clustering based approaches.   While ATR CSPD is simple and intuitive, it lacks novel contribution in methodology. It is unclear how it is different from existing approaches. The evaluation and the writing could be improved significantly.   In short, the paper is not ready for publication. We hope the reviews can help improve the paper for a strong submission in the future. 
The paper studies the amount of over parameterization needed for a quadratic 2 /3 layer neural network to memorize a separable training data set with arbitrary labels. While the reviewers agree that this paper contains interesting results, the review process uncovered highly related prior work, which requires a major revision to put the current paper into perspective and generally various clarifications. The paper will benefit from a revision and resubmission to another venue, and is in its current form not ready for acceptance at ICLR 2020.
This manuscript proposed biologically inspired modifications to convolutional neural networks including differences of Gaussians convolutional filter, a truncated ReLU, and a modified projected normalization layer. The authors  results indicate that the modifications improve performance as well as improved robustness to adversarial attacks.  The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work on robust model architectures. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and importance of the results. In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the approach and results. In the opinion of the AC,  the manuscript in its current state is borderline and could be improved with more convincing empirical justification.
Authors proposed a multi modal unsupervised algorithm to uncover the electricity usage of different appliances in a home. The detection of appliance was done by using both combined electricity consumption data and user location data from sensors. The unit of detection was set to be a 25 second window centered around any electricity usage spike. Authors used a encoder/decode set up to model two different factors of usage: type of appliance and variety within the same appliance. This part of the model was trained by predicting actual consumption. Then only the type of appliance was used to predict the location of people in the house, which was also factored into appliance related and unrelated factors. Locations are represented as images to avoid complicated modeling of multiple people.  The reviewers were satisfied with the discussion after the authors, and therefore believe this work is of general interest to the ICLR community.
This paper extends work on NALUs, providing a pair of units which, in tandem, outperform NALUs. The reviewers were broadly in favour of the paper given the presentation and results. The one dissenting reviewer appears to not have had time to reconsider their score despite the main points of clarification being addressed in the revision. I am happy to err on the side of optimism here and assume they would be satisfied with the changes that came as an outcome of the discussion, and recommend acceptance.
This paper investigates variational models of speech for synthesis, and in particular ways of making them more controllable for a variety of synthesis tasks (e.g. prosody transfer, style transfer).  They propose to do this via a modified VAE objective that imposes a learnable weight on the KL term, as well as using a hierarchical decomposition of latent variables.  The paper shows promising results and includes a good amount of analysis, and should be very interesting for speech synthesis researchers.  However, there is not much novelty from a machine learning perspective.  Therefore, I think the paper is not a great fit for ICLR and is better suited for a speech conference/journal.
This paper investigates the problem of using zero imputation when input features are missing. The authors study this problem, propose a solution, and evaluate on several benchmark datasets. The reviewers were generally positive about the paper, but had some questions and concerns about the experimental results. The authors addressed these concerns in the rebuttal. The reviewers are generally satisfied and believe that the paper should be accepted.
Reviewers uniformly suggest acceptance. Please take their comments into account in the camera ready. Congratulations!
Quoting from R3: "This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices."  With two weak acceptance recommendations and a recommendation for rejection, this paper is borderline in terms of its scores.  The approach and idea are interesting.  The main shortcoming of the paper, as highlighted by the reviewers, is that the approach and theoretical analysis are not properly motivated to solve an actual problem faced in real world data.  The approach does not provide a better algorithm for recovering the eigenvectors of the data, nor is it proposed as part of a learning framework to solve a real world problem.  Experiments are shown on synthetic data and MNIST.  As a stand alone theoretical result, it leaves open questions as to the proposed utility.
Thanks to the reviewers and the authors for an interesting discussion. The reviewers are mixed, learning toward positive, but a few shortcomings were left unaddressed: (i) Turning the task into a mention pair classification problem ignores the mention detection step, and synergies from joint modeling are lost. (ii) Lee et al. (2018) has been surpassed by some margin by BERT and spanBERT, models ignored in this paper. (iii) Several approaches to aggregating structured annotations have already been introduced, e.g., for sequence labelling tasks. [0] Overall, the limited novelty, the missing baselines, and the missing related work lead me to not favor acceptance at this point.   [0] https://www.aclweb.org/anthology/P17 1028/
The paper addresses the setting of continual learning. Instead of focusing on catastrophic forgetting measured in terms of the output performance of the previous tasks, the authors tackle forgetting that happens at the level of the feature representation via a meta learning approach. As rightly acknowledged by R2, from a meta learning perspective the work is quite interesting and demonstrates a number of promising results.  However the reviewers have raised several important concerns that placed this work below the acceptance bar:  (1) the current manuscript lacks convincing empirical evaluations that clearly show the benefits of the proposed approach over SOTA continual learning methods; specifically the generalization of the proposed strategy to more than two sequential tasks is essential; also see R1’s detailed suggestions that would strengthen the contributions of this approach in light of continual learning; (2) training a meta learner to predict the weight updates with supervision from a multi task teacher network as an oracle, albeit nicely motivated, is unrealistic in the continual learning setting   see R1’s detailed comments on this issue.  (3) R2 and R3 expressed concerns regarding i) stronger baselines that are tuned to take advantage of the meta learning data and ii) transferability to the different new tasks, i.e. dissimilarity of the meta train and meta test settings. Pleased to report that the authors showed and discussed in their response some initial qualitative results regarding these issues. An analysis on the performance of the proposed method when the meta training and testing datasets are made progressively dissimilar would strengthen the evaluation the proposed meta learning approach.  There is a reviewer disagreement on this paper. AC can confirm that all three reviewers have read the rebuttal and have contributed to a long discussion. Among the aforementioned concerns, (3) did not have a decisive impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC suggests, that in its current state the manuscript is not ready for a publication and needs a major revision before submitting for another round of reviews. We hope the reviews are useful for improving and revising the paper. 
This work examines a problem that is of considerable interest to the community and does a good job of presenting the work. The AC recommends acceptance.
The paper considers an interesting algorithm on zeorth order optimization and contains strong theory. All the reviewers agree to accept.
This paper presents an analysis of the languages that can be accepted by a counter machine, motivated by recent work that suggests that counter machines might be a good formal model from which to approach the analysis of LSTM representations.  This is one of the trickiest papers in my batch. Reviewers agree that it represents an interesting and provocative direction, and I suspect that it could yield valuable discussion at the conference. However, reviewers were not convinced that the claims made (or implied) _about LSTMs_ are motivated, given the imperfect analogy between them and counter machines. The authors promise some empirical evidence that might mitigate these concerns to some extent, but the paper has not yet been updated, so I cannot take that into account.   As a very secondary point, which is only relevant because this paper is borderline, LSTMs are no longer widely used for language tasks, so discussion about the capacity of LSTMs _for language_ seems like an imperfect fit for an machine learning conference with a fairly applied bent.
The paper extends Gauge invariant CNNs to Gauge invariant spherical CNNs.  The authors significantly improved both theory and experiments during the rebuttal and the paper is well presented. However, the topic is somewhat niche, and the bar for ICLR this year was very high, so unfortunately this paper did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal period.
The  paper proposes a new approach to multi actor RL, which ensure diversity and performance of the population of actors, by distilling the policy of the best performing agent in a soft way and maintaining some distance between the agents. The authors show improved performance over several state of the art mono actor algorithms and over several other multi actor RL algorithms.  Initially, reviewers were concerned with magnitude of the contribution/novelty, as well as some technical issues (e.g. the beta update), and relative lack of baseline comparisons.  However, after discussion the reviewers largely agree that their main concerns have been addressed.  Therefore, I recommend this paper for acceptance.
Paper received mixed reviews: WR (R1), A (R2 and R3). AC has read reviews/rebuttal and examined paper. AC agrees that R1 s concerns are misplaced and feels the paper should be accepted.  
This paper analyzes self training for sequence to sequence models and proposes a noisy version of self training. An empirical study shows the proposed noisy version improves results for machine translation and summarization tasks.  All reviewers appreciate the interesting contributions of the research, as well as clear writing. They also offer several comments for the revision of the paper.   We look forward to seeing this paper presented at the conference!
Unfortunately, this was a borderline paper that generated disagreement among the reviewers.  After high level round of additional deliberation it was decided that this paper does not yet meet the standard for acceptance.  The paper proposes a potentially interesting approach to learning surrogates for non differentiable and non decomposable loss functions.  However, the work is a bit shallow technically, as any supporting theoretical justification is supplied by pointing to other work.  The paper would be stronger with a more serious and comprehensive analysis.  The reviewers criticized the lack of clarity in the technical exposition, which the authors attempted to mitigate in the rebuttal/revision process.  The paper would benefit from additional clarity and systematic presentation of complete details to allow reproduction.
The reviewers all agree that this is an interesting paper with good results. The authors  rebuttal response was very helpful. However, given the competitiveness of the submissions this year, the submission did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal.
This work proposes a new adaptive method for solving certain min max problems.  The reviewers all appreciated the work and most of their concerns were addressed in the rebuttal. Given the current interest in both adaptive methods and min max problems, this work is suited for publication at ICLR.
The authors propose a sample reweighting scheme that helps to learn a simple model with similar performance as a more complex one. The authors contained critical errors in their original submission and the paper seems to lack in terms of originality and novelty of the proposed method.
The authors analyze knowledge graph embedding models for multi relational link predictions. Three reviewers like the work and recommend acceptance. The paper further received several positive comments from the public. This is solid work and should be accepted.
This paper presents an unsupervised method for completing point clouds obtained from real 3D scans based on GAN. Generally, the paper is well organized, and its contributions and experimental supports are clearly presented, from which all reviewers got positive impressions. Although the technical contribution of the method seems marginal as it is essentially a combination of established methods, it well fits in a novel and practical application scenario, and its useful is convincingly demonstrated in intensive experiments. We conclude that the paper provides favorable insights covering the weakness in technical novelty, so I’d like to recommend acceptance.  
The paper received mixed scores: Weak Reject (R1 and R2) and Accept (R3). AC has closely read the reviews/comments/rebuttal and examined the paper. After the rebuttal, R2 s concerns still remain. AC sides with R2 and feels that the generated interpretations are not convincing, and that the conclusions drawn are not fully supported. Thus the paper just falls below the acceptance threshold, unfortunately. The work has merits however and the authors should revise their paper to incorporate the constructive feedback.
The authors consider improvements to model based reinforcement learning to improve sample efficiency and computational speed. They propose a method which they claim is simple and elegant and embeds the model in the policy learning step, this allows them to compute analytic gradients through the model which can have lower variance than likelihood ratio gradients. They evaluate their method on Mujoco with limited data.  All of the reviewers found the presentation confusing and below the bar for an acceptable submission. Although the authors tried to explain the algorithm better to the reviewers, they did not find the presentation sufficiently improved. I agree that the paper has substantial room for improvement around clarity. Reviewers also asked that experiments be run for more time steps. I agree that this would be an important addition as many model based reinforcement learning approaches perform worse asymptotically model free approaches and it would be interesting to see how the proposed approach does. A reviewer pointed out that equation 2 is missing a term, and indeed I believe that is true. The authors response is not correct, they likely refer to an equation in SVG where the state is integrated out. Finally, the method does not compare to state of the art model based approaches, claiming that they use ensembles or uncertainty to improve performance. The authors would need to show that adding either of these to their approach attains similar performance to state of the art approaches.  At this time, this paper is below the bar for acceptance.
This paper tries to bridge early stopping and distillation.  1) In Section 2, the authors empirically show more distillation effect when early stopping. 2) In Section 3, the authors propose a new provable algorithm for training noisy labels.  In the discussion phase, all reviewers discussed a lot. In particular, a reviewer highlights the importance of Section 3. On the other hand, other reviewers pointed out "what is the role of Section 2", as the abstract/intro tends to emphasize the content of Section 2.  I mostly agree all pros and cons pointed out by reviewers. I agree that the paper proposed an interesting idea for refining noisy labels with theoretical guarantees. However, the major reason for my reject decision is that the current write up is a bit below the borderline to be accepted considering the high standard of ICLR, e.g., many typos (what is the172norm in page 4?) and misleading intro/abstract/organization. In overall, it was also hard for me to read the paper. I do believe that the paper should be much improved if the authors make more significant editorial efforts considering a more broad range of readers.   I have additional suggestions for improving the paper, which I hope are useful.  * Put Section 3 earlier (i.e., put Section 2 later) and revise intro/abstract so that the reader can clearly understand what is the main contribution.  * Section 2.1 is weak to claim more distillation effect when early stopping. More experimental or theoretical study are necessary, e.g., you can control temperature parameter T of knowledge distillation to provide the "early stopping" effect without actual "early stopping" (the choice of T is not mentioned in the draft as it is the important hyper parameter). * More experimental supports for your algorithm should be desirable, e.g., consider more datasets, state of the art baselines, noisy types, and neural architectures (e.g., NLP models). * Softening some sentences for avoiding some potential over claims to some readers. 
All three reviewers appreciate the new method (FactorGAN) for training generative networks from incomplete observations. At the same time, the quality of the experimental results can still be improved. On balance, the paper will make a good poster.
This paper proposes an improvement to the popular DARTS approach, speeding it up by performing the search in a subset of channels. The improvements are robust, and code is available for reproducibility.  The rebuttal cleared up initial concerns, and after the (private) discussion among reviewers now all reviewers give accepting scores. Because the improvements seem somewhat incremental and only applied to DARTS, R3 argued against an oral, and even the most positive reviewer agreed that a poster format would be best for presentation.   I therefore strongly recommend recommendation, as a poster.
The author response and revisions to the manuscript motivated two reviewers to increase their scores to weak accept. While these revisions increased the quality of the work, the overall assessment is just shy of the threshold for inclusion.
While the reviewers generally appreciated the idea behind the method in the paper, there was considerable concern about the experimental evaluation, which did not provide a convincing demonstration that the method works in interesting and relevant problem settings, and did not compare adequately to alternative approach. As such, I believe this paper is not quite ready for publication in its current form.
This paper presents a new phenomenon referred to as the "local elasticity of neural networks". The main argument is that the SGD update for nonlinear network at a local input x does not change the predictions at a different input x  (see Fig. 2). This is then connected to similarity using nearest neighbor and kernel methods. An algorithm is also presented.  The reviewers find the paper intriguing and believe that this could be interesting for the community. After the rebuttal period, one of the reviewers increased their score.  I do agree with the view of the reviewers, although I found that the paper s presentation can be improved. For example, Fig. 1 is not clear at all, and the related work section basically talks about many existing works but does not discuss why they are related to this work and how this work add value to this existing works. I found Fig. 2 very clear and informative. I hope that the authors could further improve the presentation. This should help in improving the impact of the paper.  With the reviewers score, I recommend to accept this paper, and encourage the authors to improve the presentation of the paper.
This paper studies the impact of embedding complexity on domain invariant representations by incorporating embedding complexity into the previous upper bound explicitly.  The idea of embedding complexity is interesting, the exploration has some useful insight, and the paper is well written. However, Reviewers and AC generally agree that the current version can be significantly improved in several ways:   The proposed upper bound has several limitations such as looser than existing ones.   The embedding complexity is only addressed implicitly, which shares similar idea with previous works.   The claim of implicit regularization has not been explored in depth.   The proposed MDM method seems to be incremental and related closely with the embedding complexity.   There is no analysis about the generalization when estimating this upper bound from finite samples.  There are important details requiring further elaboration. So I recommend rejection.
This paper focuses on finding universal adversarial perturbations, that is, a single noise pattern that can be applied to any input to fool the network in many cases. Further more, it focuses on the data free setting, where such a perturbation is found without having access to data (images) from the distribution that train  and test data comes from.   The reviewers were very conflicted about this paper. Among others, the strong experimental results and the clarity of writing and analysis were praised. However, there was also criticism of the amount of novelty compared to GDUAP, on the strong assumptions needed (potentially limiting the applicability), and on some weakness in the theoretical analysis.   In the end, the paper seems in current form not convincing enough for me to recommend acceptance for ICLR.  
This paper proves that fully connected wide ReLU NNs trained with squared loss can be decomposed into two parts: (1) the minimum complexity solution of an interpolating kernel method, and (2) a term depends heavily on the initialization. The main concerns of the reviewers include (1) the contribution are not significant at all given prior work; (2) flawed proof,  and (3) lack the comparison with prior work. Even the authors addressed some of the concerns in the revision, it still does not gather sufficient support from the reviewers after author response. Thus I recommend reject.
This paper presents a new link prediction framework in the case of small amount labels using meta learning methods. The reviewers think the problem is important, and the proposed approach is a modification of meta learning to this case. However, the method is not compared to other knowledge graph completion methods such as TransE, RotaE, Neural Tensor Factorization in benchmark dataset such as Fb15k and freebase.  Adding these comparisons can make the paper more convincing. 
A new method for derivative free optimization including momentum and importance sampling is proposed.  All reviewers agreed that the paper deserves acceptance.  Acceptance is recommended.
This paper investigates improving robustness to adversarial examples by using mode connectivity in the loss function. The paper received three reviews by experts working in related areas. In a strongly positive review, R1 recommends Accept, but gives some specific technical questions. The authors submitted a response to these questions; in post review comments, R1 was satisfied and maintained the highly positive review. R2 recommended Weak Reject and also asked specific technical questions, including some additional details on experiments, statistical significance, etc. The author response also convincingly responded to these concerns. R3 recommended Weak Accept but suggested improving the writing, which authors have done in their revision. Given that R1 and R3 are highly positive and R2 s concerns were addressed in the response and revision, we now recommend (weak) Accept.
While prior work has shown the potential of using uncertainty to tackle catastrophic forgetting (e.g. by appropriate updates to the posterior), this paper goes further and proposes a strategy to adapt the learning rate based on the uncertainty. This is a very reasonable idea since, in practice, learning rate control is one of the simplest and most understood techniques to fight catastrophic forgetting.  The overall approach ends up being a well motivated strategy for controlling the learning rate of the parameters according to a notion of their "importance". Of course now the question is if this work uses a good proxy for "importance" so further ablation studies would help, but the current results already show a clear benefit.  
The paper proposed U net for segmentation of stagnant zones in computed tomography. Technical contribution of the paper is severely limited, and is not of the quality expected of publications in this venue. The paper is not anonymized and violates the double blind review rule. I m thus recommending rejection.
The paper has major presentation issues. The rebuttal clarified some technical ones, but it is clear that the authors need to improve the reading substantially, ,so the paper is not acceptable in its current form.
The paper proposes a domain adaptive filter decomposition method via separating domain specific and cross domain features, towards learning invariant representations for unsupervised domain adaptation.  Overall, this well written paper is well motivated with a better technique for learning invariant representations using convolutional filters. Nonetheless, reviewers still have major concerns: 1) the novelty of the paper may be marginal given the significant line of recent work on learning domain invariant representations; 2) when the label distributions differ, learning invariant representations can only lead to worse target generalizations; 3) the provided theory has an unclear connection to the presented filter decomposition method. The paper can be strengthened by further discussions on how to mitigate the aforementioned negative results.   Hence I recommend rejection.
The paper proposes a method to make the filters of the last conv layer more class specific. The motivation for this is to improve upon the interpretability of the CNN, which is empirically shown by comparing the class activation maps (CAMs) of regular CNN and the proposed LSG CNN. While the idea is interesting, one of the concerns from reviewers is about limited applicability of the method, at least the way it is shown in experiments   a concern that I tend to agree with. As primary goal of the work is improving interpretability of CNNs, authors should test LSG CNN with some more recent methods for producing the saliency maps other than CAM to convincingly establish the value of the method. Authors also mention lack of hyperparameter tuning and the use of SGD with limited training epochs as a reason for the drop in accuracy. It will be worth spending some effort so the accuracy matches the standard benchmarks   this will help in arguing more convincingly about practical benefit of the method. 
All reviewers rated this submission as a weak reject and there was no author response. The AC recommends rejection.
This paper studies the effect of clipping on mitigating label noise. The authors demonstrate that standard gradient clipping does not suffice for achieving robustness to label noise. The authors suggest a noise robust alternative. In the discussion the reviewers raised some interesting questions and technical detailed but mostly agreed that the paper is well written with nice contributions. I concur with the reviewers that this is a nicely written paper with good contributions. I recommend acceptance but recommend the authors continue to improve their paper based on the reviewers  suggestions.
This paper studies the problem of devising optimal attacks in deep RL to minimize the main agent average reward. In the white box attack setting, optimal attacks amounts to solving a Markov Decision Process, while in black box attacks, optimal attacks can be trained using RL techniques. Empirical efficiency of the attacks was demonstrated. It has valuable contributions on studying the adversarial robustness on deep RL. However, the current motivation and setup needs to be made clearer, and so is not being accepted at this time. We hope for these comments to help improve a future version.
The submission presents a differentiable take on classic active contour methods, which used to be popular in computer vision. The method is sensible and the results are strong. After the revision, all reviewers recommend accepting the paper.
The paper investigates the trainability and generalization of deep networks as a function of hyperparameters/architecture, while focusing on wide nets of large depth; it aims to characterize regions of hyperparameter space where networks generalize well vs where they do not; empirical observations are demonstrated to support theoretical results. However, all reviewers agree that, while the topic of the paper is important and interesting, more work is required to improve the readability and clarify the exposition to support the proposed theoretical results.  
This paper presents a detailed comparison of different bonus based exploration methods on a common evaluation framework (Rainbow) when used with the ATARI game suite. They find that while these bonuses help on Montezuma s Revenge (MR), they underperform relative to epsilon greedy on other games. This suggests that architectural changes may be a more important factor than bonus based exploration in recent advances on MR.  The reviewers commented that this paper makes no effort to present new techniques, and the insights discovered could be expanded on. Despite this, it is an interesting paper that is generally well argued and would be a useful contribution to the field. I recommend acceptance.
This paper explores the role of excitatory and inhibitory neurons, and how their properties might differ based on simulations.  A few issues were raised during the review period, and I commend the authors for stepping up to address these comments and run additional experiments.  It seems, though, that the reviewer s worries were born out in the results of the additional experiments: "1. The object classification task is not really relevant to elicit the observed behavior and 2. Inhibitory neurons are not essential (at least when training with batch norm)."  I hope the authors can make improvements in light of these observations, and discuss their implications in a future version of this paper. 
The study of the impact of the noise on the Hessian is interesting and I commend the authors for attacking this difficult problem. After the rebuttal and discussion, the reviewers had two concerns:   The strength of the assumptions of the theorem   Assuming the assumptions are reasonable, the conclusions to draw given the current weak link between Hessian and generalization.  I m confident the authors will be able to address these issues for a later submission.
This paper presents a simple method for improving molecular optimization with a learned model. The method operates by repeatedly feeding generated molecules back through an encoder decoder pair trained to maximize a desired property. Reviewers liked the simplicity of the method, and found it interesting but ultimately there were concerns about the metrics used to evaluate the method. Reviewers 3 and 4 both noted issues with the log P (and penalized log P) metric, noting that it is possible to artificially increase both metrics in a way that isn t useful in practice. During the discussion phase, Reviewer 4 constructed a specific example where simply adding long carbon chains to a molecule would yield a linear increase the penalized log P metric, and noted that the "best molecules" found by the method in Figure 3 also have extremely long carbon chains (long carbon chains are not generally desirable for drug discovery).  I recommend the authors resubmit after finding a better way to evaluate that their method generates molecules with more useful properties for drug discovery.
This paper has been assessed by three reviewers who scored it as 3/3/3, and they did not increase their scores after the rebuttal. The main criticism lies in novelty of the paper, lack of justification for MM^T formulation, speed compared to gradient descent (i.e. theoretical analysis plus timing). Other concerns point to overlaps with Baydin et al. 2015 and the question about the validity of Theorem 1. On balance, this paper requires further work and it cannot be accepted to ICLR2020.
The authors offer theoretical guarantees for a simplified version of the deep Q learning algorithm. However, the majority of the reviewers agree that the simplifying assumptions are so many that the results do not capture major important aspects of deep Q Learning (e.g. understanding good exploration strategies, understanding why deep nets are better approximators and not using neural net classes that are so large that can capture all non parametric functions). For justifying the paper to be called a theoretical analysis of deep Q Learning some of these aspects need to be addressed, or the motivation/title of the paper needs to be re defined. 
Although some criticism remains for experiments, I suggest to accept this paper.
This article studies gradient optimization for classification problems with shallow networks with smooth activations, obtaining convergence and generalisation results under a separability assumption on the data. The results are obtained under much less stringent requirements on the width of the network than other related recent works. However, with results on convergence and generalisation having been established in other previous works, the reviewers found the contribution incremental. The responses clarified some of the distinctive challenges with the logistic loss compared with the squared loss that has been considered in other works, and provided examples for the separability assumption. Overall, the article makes important contributions in the case of classification problems. However, with many recent works addressing challenging problems in a similar direction, the bar has been set quite high. As pointed out by some of the reviewers, the contribution could gain substantially in relevance and make a more convincing case by addressing extensions to non smooth activations and deep models. 
This paper proposes a novel differentiable digital signal processing in audio synthesis. The application is novel and interesting. All the reivewers agree to accept it. The authors are encouraged to consider the reviewer s suggestions to revise the paper.
The paper proposes a  deep learning architecture for forecasting Origin Destination (OD) flow. The model integrates several existing modules including spatiotemporal graph convolution and periodically shifted attention mechanism.   The reviewers agree that the paper is not written well, and the experiments are also not executed well. Overall, we recommend rejection.
This paper presents a software library for dealing with neural networks either in the (usual) finite limit or in the infinite limit. The latter is obtained by using the Neural Tangent Kernel theory.   There is variance in the reviewers  scores, however there has also been quite a lot of discussion, which has been facilitated by the authors  elaborate rebuttal. The main points in favor and against are clear: on the positive side, the library is demonstrated well (especially after rebuttal) and is equipped with desirable properties such as usage of GPU/TPU, scalability etc. On the other hand, a lot of the key insights build heavily on prior work of Lee et al, 2019. However, judging novelty when it comes to a software paper is more tricky to do, especially given that not many such papers appear in ICLR and therefore calibration is difficult. This has been discussed among reviewers.   It would help if some further theoretical insights were included in this paper; these insights could come by working backwards from the implementation (i.e. what more can we learn about infinite width networks now that we can experiment easily with them?).  Overall, this paper should still be of interest to the ICLR community. 
This paper proposes an ensemble method to identify noisy labels in the training data of supervised learning.  The underlying hypothesis is that examples with label noise require memorization.  The paper proposes methods to identify and remove bad training examples by retaining only the training data that maintains low losses after perturbations to the model parameters.  This idea is developed in several candidate ensemble algorithms.  One of the proposed ensemble methods exceeds the performance of state of the art methods on MNIST, CIFAR 10 and CIFAR 100.  The reviewers found several strengths and a few weaknesses in the paper.  The paper was well motivated and clear.  The proposed solution was novel and plausible.  The experiments were comprehensive.  The reviewers identified several parts of the paper that could be more clear or where more detail could be provided, including a complexity analysis and  extended experiments.  The author response addressed the reviewer questions directly and also in a revised document.  In the discussion phase, the reviewers were largely satisfied that their concerns were addressed.  This paper should be accepted for publication as the paper presents a clear problem and solution method along with convincing evidence of method s merits. 
The paper proposes a new method for extreme multi label classification. However, this paper only combine some  well known tricks, the technical contributions are too limited. And there are many problems in the experiments, such as the reproducibility, the scal of data set and the results on well known extreme data sets and so on. The authors are encouraged to consider the reviewer s comments to revise the paper.
The paper suggests a new way to defend against adversarial attacks on neural networks. Two of the reviewers were negative, one of them (the most experienced in the subarea) strongly negative. One reviewer is weakly positive. The main two concerns of the reviewers are insufficient comparisons with SOTA and lack of clarity. The authors  response, though detailed, has not convinced the reviewers and has not alleviated their concerns.  
This paper proposed a mixup inference (MI) method, for  mixup trained models, to better defend adversarial attacks.  The idea is novel and is proved to be effective on CIFAR 10 and CIFAR 100.  All reviewers and the AC agree to accept the paper.
This paper proposed graph neural networks based approach for subgraph detection. The reviewers find that the overall the paper is interesting, however further improvements are needed to meet ICLR standard:  1. Experiments on larger graph. Slight speedup in small graphs are less exciting.   2. It seems there s a mismatch between training and inference.  3. The stopping criterion is quite heuristic.  
This paper proposed a method to estimate the instance wise saliency map for image classification, for the purpose of improving the faithfulness of the explainer. Based on the U net, two modifications are proposed in this work. While reviewer #3 is overall positive about this work, both Reviewer #1 and #2 rated weak reject and raised a number of concerns. The major concerns include the modifications either already exist or suffer potential issue. Reviewer #2 considered that the contributions are not enough for ICLR, and the performance improvement is marginal. The authors provided detailed responses to the reviewers’ concerns, which help to make the paper stronger, but did not change the rating. Given the concerns raised by the reviewers, the ACs agree that this paper can not be accepted at its current state.
This paper suggests that datasets have a strong influence on the effects of attention in graph neural networks and explores the possibility of transferring attention for graph sparsification, suggesting that attention based sparsification retains enough information to obtain good performance while reducing computational and storage costs.   Unfortunately I cannot recommend acceptance for this paper in its present form. Some concerns raised by the reviewers are: the analysis lacks theoretical insights and does not seem to be very useful in practice; the proposed method for graph sparsification lacks novelty; the experiments are not thorough to validate its usefulness. I encourage the authors to address these concerns in an eventual resubmission. 
This paper tackles the problem of confidence on neural network predictions for out of distribution (OOD) samples. The authors propose an approach for training neural networks such that the OOD prediction is uniform across classes. The approach requires samples from in  and out of distribution and relies on a mixture of Gaussians for modelling the distributions, allowing to obtain theoretical guarantees on detecting OOD samples (unlike existing techniques).  The main concerns of the reviewers have been addressed during the rebuttal. If this approach does not outperform state of the art in practice, providing such theoretical guarantees is an important contribution.  All reviewers agree that this paper should be accepted. I therefore recommend acceptance.
Two reviewers are negative on this paper while the other reviewer is positive. Overall, the paper does not make the bar of ICLR. A reject is recommended.
All three reviewers agree that the paper provide an interesting study on the ability of generative adversarial networks to model geometric transformations and a simple practical approach to how such ability can be improved. Acceptance as a poster is recommended.
The paper presents a novel graph convolutional network by integrating the curvature information (based on the concept of Ricci curvature). The key idea is well motivated and the paper is clearly written. Experimental results show that the proposed curvature graph network methods outperform existing graph convolution algorithms. One potential limitation is the computational cost of computing the Ricci curvature, which is discussed in the appendix. Overall, the concept of using curvature in graph convolutional networks seems like a novel and promising idea, and I also recommend acceptance.
This paper studies the setting in reinforcement learning where the next action must be sampled while the current action is still executing. This refers to continuous time problems that are discretised to make them delay aware in terms of the time taken for action execution. The paper presents adaptions of the Bellman operator and Q learning to deal with this scenario.  This is a problem that is of theoretical interest and also has practical value in many real world problems. The reviewers found both the problem setting and the proposed solution to be valuable, particularly after the greatly improved technical clarity in the rebuttals. As a result, this paper should be accepted.
Main content:  Physical driven architecture of DeepSFM to infer the structures from motion Discussion: reviewer 1: well motivated model with good solid experimental results. not clear about the LM optimization in BA Net is memory inefficient  reviewer 2: main issue is the experiments could be improved. reviewer 3: well written but again experimental section is lacking Recommendation: Good paper and results, but all 3 reviewers agree experiments could be improved. Rejection is recommended.
This paper proposes a GAN based approach to producing poisons for neural networks.  While the approach is interesting and appreciated by the reviewers, it is a legitimate and recurring criticism that the method is only demonstrated on very toy problems (MNIST and Fashion MNIST).  During the rebuttal stage, the authors added results on CIFAR, although the results on CIFAR were not convincing enough to change the reviewer scores; the SOTA in GANs is sufficient to generate realistic images of cars and trucks (even at the ImageNet scale), while the demonstrated images are sufficiently far from the natural image distribution on CIFAR 10 that it is not clear whether the method benefits from using a GAN.   It should be noted that a range of poisoning methods exist that can effectively target CIFAR, and SOTA methods (e.g., poison polytope attacks and backdoor attacks) can even target datasets like ImageNet and CelebA.
This paper proposes to discover causal mechanisms through meta learning, and suggests an approach for doing so. The reviewers raised concerns about the key hypothesis (that the right causal model implies higher expected online likelihood) not being sufficiently backed up through theory or through experiments on real data. The authors pointed to a recent paper that builds upon this work and tests on a more realistic problem setting. However, the newer paper measures not the online likelihood of adaptation, but just the training error during adaptation, suggesting that the approach in this paper may be worse. Despite the concerns, the reviewers generally agreed that the paper included novel and interesting ideas, and addressed a number of the reviewers  other concerns about the clarity, references, and experiments. Hence, it makes a worthwhile contribution to ICLR.
This paper proposes using Flush+Reload to infer the deep network architecture of another program, when the two programs are running on the same machine (as in cloud computing or similar).   There is some disagreement about this paper; the approach is thoughtful and well executed, but one reviewer had concerns about its applicability and realism. Upon reading the author s rebuttal I believe these to be largely addressed, or at least as realistically as one can in a single paper. Therefore I recommend acceptance.
This paper proposes discriminability distillation learning (DDL) for learning group representations. The core idea is to learn a discriminability weight for each instance which are a member of a group, set or sequence. The discriminability score is learned by first training a standard supervised base model and using the features from this model, computing class centroids on a proxy set, and computing the iter and intra class distances. A function of these distance computations are then used as supervision for a distillation style small network (DDNet) which may predict the discriminability score (DDR score). A group representation is then created through a combination of known instances, weighted using their DDR score. The method is validated on face recognition and action recognition.  This work initially received mixed scores, with two reviewers recommending acceptance and two recommending rejection. After reading all the reviews, rebuttals, and discussions, it seems that a key point of concern is low clarity of presentation. During the rebuttal period, the authors have revised their manuscript and interacted with reviewers. One reviewer has chosen to update their recommendation to weak acceptance in response. The main unresolved issues are related to novelty and experimental evaluation. Namely, for novelty comparison and discussion against attention based approaches and other metric learning based approaches would benefit the work, though the proposed solution does present some novelty. For the experiments there was a suggestion to evaluate the model on more complex datasets where performance is not already maxed out. The authors have provided such experiments during the rebuttal period.  Despite the slight positive leanings post rebuttal, the ACs have discussed this case and determine the paper is not ready for publication.
This paper addresses the problem of performing unsupervised domain adaptation when some target domain data is missing is a potentially non stochastic way. The proposed solution consists of applying a version of domain adversarial learning for adaptation together with an MSE based imputation loss learned using complete source data. The method is evaluated on both the standard digit recognition datasets and a real world advertising dataset.   The reviewers had mixed recommendations for this work, with two recommending weak reject and one recommending acceptance. The key positive point from R3 who recommended acceptance was that this work addresses a new problem statement which may be of practical importance. The other two reviewers expressed concerns over the contribution of the work and the validity of the problem setting. Namely, both R2 and R4 had significant confusion over the problem specification and/or under what conditions the proposed setting is valid.   It is a difficult decision for this paper as there is a core disagreement between the reviewers. All reviewers seem to agree that the proposed solution is a combination of prior methods in a new way to address the specific problem setting of this work.  However, the reviewers differ in precisely whether they determine the proposed problem setting to be valid and justified. Due to this discrepancy, the AC does not recommend acceptance at this time. If the core contribution is to be an application of existing techniques to a new problem statement than that should be clarified and motivated further.  
The paper provides a generalization error bound, which extends the results from PU learning, for the problem of knowledge graph completion. The authors assume a missing at random setting, and provide bounds on the triples (two nodes and an edge) that could be mistakes. Then the paper provides a maximum likelihood interpretation, as well as relations to existing knowledge graph completion methods. The problem setting is interesting, and the writing clear.  This discussion was extensive, with reviewers and authors following the spirit of ICLR and having a constructive discussion which resulted in improvements to the paper. However, there seems to be still some remaining improvements to be made in terms of clarity of presentation, as well as precision of the theoretical arguments.  Unfortunately, there are many strong submissions, and the paper as it currently stands does not satisfy the quality threshold of ICLR.
This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.
This paper considers black box adversarial attacks based on perturbations of the intermediate layers of a neural network classifier, obtained by training a binary classifier for each target class.  Reviewers were happy with the novelty of the approach as well as the presentation, described the presentation as rigorous and were pleased with the situation of this method relative to the literature. R3 had concerns about evaluation, success rate, and that the procedure was "cumbersome".  Some of their concerns were addressed in rebuttal, but remained steadfast that the method was too cumbersome to be practical.  I agree with R1 & R2 that this approach is novel and interesting and disagree with R3 that it is too impractical. The paper could be stronger with the addition of adversarial training experiments (and I disagree with the authors that "there are currently no whitebox attacks that do well at attacking AT models", this is very much not the case), but I concur with R1 & R2 that this is interesting work that may stimulate further exploration, enough so to warrant acceptance.
The article is concerned with depth width tradeoffs in the representation of functions with neural networks. The article presents connections between expressivity of neural networks and dynamical systems, and obtains lower bounds on the width to represent periodic functions as a function of the depth. These are relevant advances and new perspectives for the theoretical study of neural networks. The reviewers were very positive about this article. The authors  responses also addressed comments from the initial reviews. 
I agree with the reviewers that this paper has serious limitations in the experimental evaluation.
This paper proposes a relation based model that extends VAE to explicitly alleviate the domain bias problem between seen and unseen classes in the setting of generalized zero shot learning.  Reviewers and AC think that the studied problem is interesting, the reported experimental results are strong, and the writing is clear, but the proposed model and its scientific reasoning for convincing why the proposed method is valuable is somewhat limited. Thus the authors are encouraged to further improve in these directions. In particular:    The idea of using a variant of the widely used domain discriminator to make seen and unseen classes distinguishable is somewhat contradicted to the basic principle of zero shot learning. How to trade off the balance between seen and unseen classes has been an important problem in generalized ZSL. These problems need further elaboration.    The proposed model itself is not a real "VAE", making the value of an extensive derivation based on variational inference less prominent.     There is also the need to compare with the baselines mentioned by the reviewers.   Overall, this is a borderline paper. Since the above concerns were not addressed convincingly in the rebuttal, I am leaning towards rejection.
This submission proposes a VAE based method for jointly inferring latent variables and data generation. The method learns from partially observed multimodal data.  Strengths:  Learning to generate from partially observed data is an important and challenging problem.  The proposed idea is novel and promising.  Weaknesses:  Some experimental protocols are not fully explained.  The experiments are not sufficiently comprehensive (comparisons to key baselines are missing).  More analysis of some surprising results is needed.  The presentation has much to improve.  The method is promising but the mentioned weaknesses were not sufficiently addressed during discussion. AC agrees with the majority recommendation to reject. 
This paper proposes a novel method for learning Hamiltonian dynamics from data. The data is obtained from systems subjected to an external control signal. The authors show the utility of their method for subsequent improved control in a reinforcement learning setting. The paper is well written, the method is derived from first principles, and the experimental validation is solid. The authors were also able to take into account the reviewers’ feedback and further improve their paper during the discussion period. Overall all of the reviewers agree that this is a great contribution to the field and hence I am happy to recommend acceptance.
This paper proposes a new method for lifelong learning of language using language modeling. Their training scheme is designed so as to prevent catastrophic forgetting. The reviewers found the motivation clear and that the proposed method outperforms prior related work. Reviewers raised concerns about the title and the lack of some baselines which the authors have addressed in the rebuttal and their revision.
The authors study generalization in distributed representation learning by describing limits in accuracy and complexity which stem from information theory.   The paper has been controversial, but ultimately the reviewers who provided higher scores presented weaker and fewer arguments. By recruiting an additional reviewer it became clearer that, overall the paper needs a little more work to reach ICLR standards. The main suggestions for improvements have to do with improving clarity in a way that makes the motivation convincing and the practicality more obvious. Boosting the experimental results is a complemental way of increasing convincingness, as argued by reviewers. 
This paper describes a new generative model based on the information theoretic principles for better representation learning. The approach is theoretically related to the InfoVAE and beta VAE work, and is contrasted to vanilla VAEs. The reviewers have expressed strong concerns about the novelty of this work. Some of the very closely related baselines (e.g. Zhao et al., Chen et al., Alemi et a) are not compared against, and the contributions of this work over the baselines are not clearly discussed. Furthermore, the experimental section could be made stronger with more quantitative metrics. For these reasons I recommend rejection.
This work proposes context aware representation of graph nodes leveraging attention over neighbors (as already done in previous work). Reviewers concerns about lack of novelty, lack of clarity of paper and lack of comparison to state of the art methods have not been addressed at all. We recommend rejection.
This paper studies the optimal value function for the gambler s problem, and presents some interesting characterizations thereof. The paper is well written and should be accepted.
This paper present a learning method for speeding up of LP, and apply it to the TSP problem.  Reviewers and AC agree that the idea is quite interesting and promising. However, I think the paper is far from being ready to publish in various aspects:  (a) much more editorial efforts are necessary (b) the TPS application of small scale is not super appealing   Hence, I recommend rejection.
This paper proposes a scalable approach for graph learning from data. The reviewers think the approach appears heuristic and it is not clear the algorithm is optimizing the proposed sparse graph recovery objective. 
This paper studies the robust reinforcement learning problem in which the constraint on model uncertainty is captured by the Wasserstein distance. The reviewers expressed concerns regarding novelty with respect to prior work, the presentation or the results, and unconvincing experiments. In its current form the paper is not ready for acceptance to ICLR 2020.
Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal with one reviewer hesitating about the appropriateness of this submission to ML venues. The reviewers have raised a number of criticisms such as an incremental nature of the paper (HHL and LMR algorithms) and the main contributions lying more within the field of quantum computing than ML. The paper was discussed with reviewers, buddy AC and chairs. On balance, it was concluded that this paper is minimally below the acceptance threshold. We encourage authors to consider all criticism, improve the paper and resubmit to another venue as there is some merit to the proposed idea. 
This paper proposes to speed up Bayesian deep learning at test time by training a student network to approximate the BNN s output distribution. The idea is certainly a reasonable thing to try, and the writing is mostly good (though as some reviewers point out, certain sections might not be necessary). The idea is fairly obvious, though, so the question is whether the experimental results are impressive enough by themselves to justify acceptance. The method is able to get close to the performance achieved by Monte Carlo estimators with much lower cost, although there is a nontrivial drop in accuracy. This is probably worth paying if it achieves 500x computation reduction as claimed in the paper, though the practical gains are probably much smaller since Monte Carlo methods are rarely used with 500 samples. Overall, this seems a bit below the bar for ICLR. 
This work introduces a simple and effective method for ensemble distillation. The method is a simple extension of earlier “prior networks”: it differs in which, instead of fitting a single network to mimic a distribution produced by the ensemble, this work suggests to use multi head (one head per individual ensemble member) in order to better capture the ensemble diversity. This paper experimentally shows that multi head architecture performs well on MNIST and CIFAR 10 (they added CIFAR 100 in the revised version) in terms of accuracy and uncertainty.  While the method is effective and the experiments on CIFAR 100 (a harder task) improved the paper, the reviewers (myself included) pointed out in the discussion phase that the limited novelty remains a major weakness. The proposed method seems like a trivial extension of the prior work, and does not provide much additional insight. To remedy this shortcoming, I suggest the authors provide extensive experimental supports including various datasets and ablation studies.    Another concern mentioned in the discussion is the fact that these small improvements are in spite of the fact that the proposed method ends up using many more parameters than the baselines. Including and comparing different model sizes in a full fledged experimental evaluation would better convey the trade offs of the proposed approach. 
This paper proposes a novel architecture for question answering, which is trained in an end to end fashion.  The reviewers were unanimous in their vote to accept. Authors are encouraged to revise addressing reviewer comments.
There is insufficient support to recommend accepting this paper.  The authors provided detailed responses, but the reviewers unanimously kept their recommendation as reject.  The novelty and significance of the main contribution was not made sufficiently clear, given the context of related work.  Critically, the experimental evaluation was not considered to be convincing, lacking detailed explanation and justification, and a sufficiently thorough comparison to strong baselines, The submitted reviews should help the authors improve their paper.
This paper was assessed by three reviewers who scored it as 6/3/6.  The reviewers liked some aspects of this paper e.g., a good performance, but they also criticized some aspects of work such as inventing new names for existing pooling operators, observation that large parts of improvements come from the pre processing step rather than the proposed method, suspected overfitting.  Taking into account all positives and negatives, AC feels that while the proposed idea has some positives, it also falls short of the quality required by ICLR2020, thus it cannot be accepted at this time. AC strongly encourages authors to go through all comments (especially these negative ones), address them and resubmit an improved version to another venue.  
Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera ready submission. Especially, the authors should take care to make this paper accessible (understandable) to the ML community as ICLR is a ML venue (rather than quantum physics one). Failure to do so will likely discourage the generosity of reviewers toward this type of submissions in the future.
This paper proposes a method that uses subgoals for planning when using video prediction. The reviewers thought that the paper was clearly written and interesting. The reviewer questions and concerns were mostly addressed during the discussion phase, and the reviewers are in agreement that the paper should be accepted.
The paper provides a theoretical analysis of the recent and popular Generative Adversarial Imitation Learning (GAIL) approach. Valuable new insights on generalization and convergence are developed, and put GAIL on a stronger theoretical foundation. Reviewer questions and suggestions were largely addressed during the rebuttal.
This paper proposes a novel pruning method for use with transformer text encoding models like BERT, and show that it can dramatically reduce the number of non zero weights in a trained model while only slightly harming performance.  This is one of the hardest cases in my pile. The topic is obviously timely and worthwhile. None of the reviewers was able to give a high confidence assessment, but the reviews were all ultimately leaning positive. However, the reviewers didn t reach a clear consensus on the main strengths of the paper, even after some private discussion, and they raised many concerns. These concerns, taken together, make me doubt that the current paper represents a substantial, sound contribution to the model compression literature in NLP.  I m voting to reject, on the basis of:    Recurring concerns about missing strong baselines, which make it less clear that the new method is an ideal choice.   Relatively weak motivations for the proposed method (pruning a pre trained model before fine tuning) in the proposed application domain (mobile devices).   Recurring concerns about thin analysis.
The paper deal with a mutual information based dependency test.   The reviewers have provided extensive and constructive feedback on the paper. The authors have in turn given detailed response withsome new experiments and plans for improvement.   Overall the reviewers are not convinced the paper is ready for publication.  
The paper introduces a novel way of jointly modeling annotator competencies and learning from imperfect annotations. Reviewers were moderately positive. One reviewer mentioned Carpenter (2002) and subsequent work. One prominent example of this line of work, which the authors do not cite, is: https://www.isi.edu/publications/licensed sw/mace/   from 2013. I encourage the authors to cite this paper. In the discussion, the authors point out this type of work is not *end to end* in their sense. However, there s, to the best of my knowledge, a relatively big body of literature on end to end approaches that the authors completely ignore, e.g., [0 3]. In the absence of a discussion of this work, it is hard to accept the paper.   [0] https://link.springer.com/article/10.1007/s10994 013 5411 2 [1] https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber 7405343 [2] http://www.cs.utexas.edu/~atn/nguyen acl17.pdf [3] https://arxiv.org/pdf/1803.04223.pdf
This paper studies when hidden units provide local codes by analyzing the hidden units of trained fully connected classification networks under various architectures and regularizers. The reviewers and the AC believe that the paper in its current form is not ready for acceptance to ICLR 2020. Further work and experiments are needed in order to identify an explanation for the emergence of local codes. This would significantly strengthen the paper.
This paper focuses on hate speech detection and compares several classification methods including Naive Bayes, SVM, KNN, CNN, and many others. The most valuable contribution of this work is a dataset of ~400,000 tweets from 2017 Kenyan general election, although it is unclear whether the authors plan to release the dataset in the future.  The paper is difficult to follow, uses an incorrect ICLR format, and is full of typos.  All three reviewers agree that while this paper deals with an important topic in social media analysis, it is not ready for publication in its current state. The authors did not provide a rebuttal to reviewers  concerns.  I recommend rejecting this paper for ICLR.
The article studies the stability of ResNets in relation to initialisation and depth. The reviewers found that this is an interesting article with important theoretical and experimental results. However, they also pointed out that the results, while good, are based on adaptations of previous work and hence might not be particularly impactful. The reviewers found that the revision made important improvements, but not quite meeting the bar for acceptance, pointing out that the presentation and details in the proofs could still be improved. 
This paper proposes to analyze the space of known sentence to vector functions by comparing the ways in which they induce nearest neighbor lists in a text corpus.  The primary results of the study are somewhat unclear, and the reviewers do not find the method to be novel enough—or sufficiently well motivated a priori—to warrant publication in spite of these results.
This paper studies self supervised video representations with a multi modal learning process that the authors then use for performance on a variety of tasks. The main contribution of the paper is a successful effort to incorporate BERT like models into vision tasks.  Reviewers acknowledged the extensive empirical evaluation and the good performance of the approach. However, they raised some concerns about the lack of clarity and the absence of analysis and interpretation of the results. The AC shares this view, and recommends rejection at this time, encouraging the authors to revise their work addressing these analysis and clarity questions.  
All three reviewers are consistently positive on this paper. Thus an accept is recommended.
The paper addresses an important problem of finding a good trade off between generalization and convergence speed of stochastic gradient methods for training deep nets. However, there is a consensus among the reviewers, even after rebuttals provided by the authors, that  the contribution is somewhat limited and the paper may require additional work before it is ready to be published.
This paper proposes a deep RL framework that incorporates motivation as input features, and is tested on 3 simplified domains, including one which is presented to rodents.   While R2 found the paper well written and interesting to read, a common theme among reviewer comments is that it’s not clear what the main contribution is, as it seems to simultaneously be claiming a ML contribution (motivation as a feature input helps with certain tasks) as well as a neuroscientific contribution (their agent exhibited representations that clustered similarly to those in animals). In trying to do both, it’s perhaps doing both a disservice.   I think it’s commendable to try to bridge the fields of deep RL and neuroscience, and this is indeed an intriguing paper. However any such paper still needs to have a clear contribution. It seems that the ML contributions are too slight to be of general practical use, while the neuroscientific contributions are muddled somewhat. The authors several times mentioned the space constraints limiting their explanations. Perhaps this is an indication that they are trying to cover too much within one paper. I urge the authors to consider splitting it up into two separate works in order to give both the needed focus.   I also have some concerns about the results themselves. R1 and R3 both mentioned that the comparison between the non motivated agent and the motivated agent wasn’t quite fair, since one is essentially only given partial information. It’s therefore not clear how we should be interpreting the performance difference. Second, why was the non motivated agent not analyzed in the same way as the motivated agent for the Pavlovian task? Isn’t this a crucial comparison to make, if one wanted to argue that the motivational salience is key to reproducing the representational similarities of the animals?  (The new experiment with the random fixed weights is interesting, I would have liked to see those results.) For these reasons and the ones laid out in the extensive comments of the reviewers, I’m afraid I have to recommend reject. 
The paper proposes an interesting idea of identifying repeated action sequences, or behavioral motifs, in the context of hierarchical reinforcement learning, using sparsity/compression.  While this is a fresh and useful idea, it appears that the paper requires more work, both in terms of presentation/clarity and in terms of stronger empirical results. 
The paper provides a nice approach to optimizing marginals to improve exploration for RL agents.  The reviewers agree that its improvements w.r.t. the state of the art do not merit a publication at ICLR.  Furthermore, additional experimentation is needed for the paper to be complete.
The authors propose a scale invariant sparsity measure for deep networks. The experiments are extensive and convincing, according to reviewers. I recommend acceptance.
This work proposes new initialization and layer topologies for training a priori sparse networks. Reviewers agreed that the direction is interesting and that the paper is well written. Additionally the theory presented on the toy matrix reconstruction task helped motivate the proposed approach. However, it is also necessary to validate the new approach by comparing with existing sparsity literature on standard benchmarks. I recommend resubmitting with the additional experiments suggested by the reviewers.
All the reviewers recommend acceptance. The reviews found the paper to be interesting with substantial insights. 
This article investigates the optimization landscape of shallow ReLU networks, showing that for sufficiently narrow networks there are data sets for which there is no descent paths to the global minimiser. The topic and the nature of the results is very interesting. The reviewers found that this article makes important contributions in a relevant line of investigation and had generally positive ratings. The authors  responses addressed questions from the initial reviews, and the discussion helped identifying questions for future study departing from the present contribution.  
All reviewers recommend rejection, and the authors have not provided a response. 
All the reviewers recommend rejecting the submission. There is no basis for acceptance.
This paper tackles the problem of exploration in deep reinforcement learning in procedurally generated environments, where the same state is rarely encountered twice. The authors show that existing methods do not perform well in these settings and propose an approach based on intrinsic reward bonus to address this problem. More specifically, they combine two existing ideas for training RL policies: 1) using implicit reward based on latent state representations (Pathak et al. 2017) and 2) using implicit rewards based on difference between subsequent states (Marino et al. 2019).  Most concerns of the reviewers have been addressed in the rebuttals. Given that it builds so closely on existing ideas, the main weakness of this work seems to be the novelty. The strength of this paper resides in the extensive experiments and analysis that highlight the shortcomings of current techniques and provide insight into the behaviour of trained agents, in addition to proposing a strategy which improves upon existing methods.  The reviewers all agree that the paper should be accepted. I therefore recommend acceptance.
While the reviewers found the paper interesting, all the reviewers raised concerns about the fairly simple experimental settings, which makes it hard to appreciate the strengths of the proposed method. During rebuttal phase, the reviewers still felt this weakness was not sufficiently addressed.
This paper proposes a meta learning approach for few shot text classification. The main idea is to use an attention mechanism over the distributional signatures of the inputs to weight word importance. Experiments on text classification datasets show that the proposed method improves over baselines in 1 shot and 5 shot settings.  The paper addresses an important problem of learning from a few labeled examples. The proposed approach makes sense and the results clearly show the strength of the proposed approach.  R1 had some questions regarding the proposed method and experimental details. I believe this have been addressed by the authors in their rebuttal.  R2 suggested that the authors clarified their experimental setup with respect to prior work and improved the clarity of their paper. The authors have made some adjustments based on this feedback, including adding new sections in the appendix.  R3 had concerns regarding the contribution of the approach and whether it trades variance for bias. The authors have addressed most of these concerns and R3 has updated their review accordingly.  I think all the reviewers gave valuable feedbacks that have been incorporated by the authors to improve their paper. While the overall scores remain low, I believe that they would have been increased had R1 and R2 reassessed the revised submission. I recommend to accept this paper. 
This paper aims at making a deep RL policy interpretable and verifiable by distilling the policy represented by a deep neural network into an ensemble of decision trees. This should be done without hurting the performance of the policy. The authors achieve this by extending the existing Viper algorithm. The resulting approach can imitate the deep policy better compared with Viper while preserving verifiability. Experiments show that the proposed method improves in terms of cumulative reward and error rate over Viper in four benchmark tasks.  The amount of improvement over the original Viper is not convincing given the presented results. Moreover, reviewers uniformly agree that the contribution of this work is incremental. I therefore recommend to reject this paper.
A nice idea: the latent prior is replaced by a GAN.  A general agreement between all four reviewers to reject the submission, based on a not thorough enough description of the approach, and possibly not being novel.
The paper proposed an end to end network architecture for graph matching problems, where first a GNN is applied to compute the initial soft correspondence, and then a message passing network is applied to attempt to resolve structural mismatch. The reviewers agree that the second component (message passing) is novel, and after the rebuttal period, additional experiments were provided by the authors to demonstrate the effectiveness of this. Overall this is an interesting network solution for graph matching, and would be a worthwhile addition to the literature.
As the reviewers have pointed out and the authors have confirmed, the original version of this paper was not a significant leap beyond combining recent understanding of Neural Tangent Kernels and previous techniques for kernelized bandits. In a revision, the authors updated their draft to allow the point at which gradients are centered around, theta_0, to now equal theta_t. This seems like a more reasonable algorithm and it is satisfying that the authors were able to maintain their regret bound for this dynamic setting. However, the revision is substantial and it seems unreasonable to expect reviewers to read the revised results in detail the reviewers also felt it may be unfair to other ICLR submissions. All reviewers believe the paper has introduced valuable contributions to the area but should go under a full review process at a future venue. A reviewer would also like to see a comparison to Kernel UCB run on the true NTK (or a good approximation thereof). 
The paper shows how meta learning contains hidden incentives for distributional shift and how a technique called context swapping can help deal with this. Overall, distributional shift is an important problem, but the contributions made by this paper to deal with this, such as the introduction of unit tests and context swapping, is not sufficiently clear. Therefore, my recommendation is a reject.
The authors develop stochastic variational approaches to learn Bayesian "structure distributions" for neural networks. While the reviewers appreciated the updates to the paper made by the authors, there will still a number of remaining concerns. There were particularly concerns about the clarity of the paper (remarking on informality of language and lack of changes in the revision with respect to comments in the original review), and the fairness of comparisons. Regarding comparisons, one reviewer comments: "I do not agree that the comparison with DARTS is fair because the authors remove the options for retraining in both DARTS and DBSN. The reason DARTS trains using one half of the data and validate on the other is that it includes a retraining phase where all data is used. Therefore fair comparison should use the same procedure as DARTS (including a retraining phrase). At the very least, to compare methods without retraining, results of DARTS with more data (e.g., 80%) for training should be reported." The authors are encouraged to continue with this work, carefully accounting for reviewer comments in future revisions.
The paper is proposed a rejection based on majority reviews.
This paper considers options discovery in hierarchical reinforcement learning. It extends the idea of covering options, using the Laplacian of the state space discover a set of options that reduce the upper bound of the environment s cover time, to continuous and large state spaces. An online method is also included, and evaluated on several domains.  The reviewers had major questions on a number of aspects of the paper, including around the novelty of the work which seemed limited, the quantitative results in the ATARI environments, and problems with comparisons to other exploration methods. These were all appropriately dealt with in the rebuttals, leaving this paper worthy of acceptance.
The paper is rejected based on unanimous reviews.
This paper introduces a pruning criterion which is similar to magnitude based pruning, but which accounts for the interactions between layers. The reviewers have gone through the paper carefully, and after back and forth with the authors, they are all satisfied with the paper and support acceptance.
The submission studies the problem of geolocalizing a city based on geometric information encoded in so called "lean" images.  The reviewers were unanimous in their opinion that the submission does not meet the threshold for publication at ICLR.  Concerns included quality of writing, novelty with respect to existing literature (in particular see Review #2), and limited validation on one geographic area.  No rebuttal was provided.
This paper presents an approach to model based reinforcement learning in high dimensional tasks. The approach involves learning a latent dynamics model, and performing rollouts thereof with an actor critic model to learn behaviours. This is extensively evaluated on 20 visual control tasks.  This paper was favourably received, but there were concerns around it being incremental (relative to PlaNet and SVG). The authors highlighted the differences in the rebuttal, clarifying the novelty of this work.   Given the interesting ideas presented, and the convincing results, this paper should be accepted.
The authors presents a method for adapting models to new tasks in a zero shot manner using learned meta mappings.  The reviewers largely agreed that this is an interesting and creative research direction.  However, there was also agreement that the writing was unclear in many sections, that the appropriate metalearning baselines were not compared to, and that the power of the method was unclear due to overly simplistic domains.  While the baseline issue was mostly cleared up in rebuttal and discussion, the other issues remain.  Thus, I recommend rejection at this time.
This paper proposes a model that can learn predicates (symbolic relations) from pixels and can be trained end to end.  They show that the relations learned generate a representation that generalizes well, and provide some interpretation of the model.  Though it is reasonable to develop a model with synthetic data, the reviewers did wonder if the findings would generalize to new data from real situations.  The authors argue that a new model should be understood (using synthetic data) before it can reasonably be applied to natural data.  I hope the reviews have shown the authors which areas of the paper need further explanation, and that the use of a synthetic dataset needs to strong justification, or perhaps show some evidence that the method will probably work on real data (e.g. how it could be extended to natural images).
The paper attacks the important problem of learning time series models with missing data and proposes two learning frameworks, RISE and DISE, for this problem. The reviewers had several concerns about the paper and experimental setup and agree that this paper is not yet ready for publication. Please pay careful attention to the reviewer comments and particularly address the comments related to experimental design, clarity, and references to prior work while editing the paper.
This paper aims to disentangle semantics and syntax inside of popular contextualized word embedding models. They use the model to generate sentences which are structurally similar but semantically different.   This paper generated a lot of discussion. The reviewers do like the method for generating structurally similar sentences, and the triplet loss.  They felt the evaluation methods were clever.  However, one reviewer raised several issues.  First, they thought the idea of syntax had not been well defined. They also thought the evaluation did not support the claims.  The reviewer also argued very hard for the need to compare performance to SOTA models.  The authors argued that beating SOTA is not the goal of their work, rather it is to understand what SOTA models are doing.  The reviewers also argue that nearest neighbors is not a good method for evaluating the syntactic information in the representations.    I hope all of the comments of the reviewers will help improve the paper as it is revised for a future submission.
This paper investigates tradeoffs between preserving accuracy on clean samples and increasing robustness on adversarial samples by using transformations and majority votes. Observations on the distribution of the induced softmax show that existing methods could be improved by leveraging information from that distribution to correct predictions, as confirmed by experiments. The problem space is important and reviewers find the approach interesting. Authors have provided some necessary clarifications during rebuttal and additional experiments. While some reservations remain, this paper s premise and its experimental results appear sufficiently interesting to justify an acceptance recommendation.
This paper describes a new approach to meta learning with generating new useful examples.  The reviewers liked the paper but overall felt that the paper is not ready for publication as it stands.  Rejection is recommended. 
 The paper proposes to train LSTMs to encode car crashes (a temporal sequence of 3D mesh representations).  Decoder LSTMs can then be used to 1) reconstruct the input or 2) predict the future sequence of structural geometry.  The authors propose to use a spectral feature representation based on prior work as input into the encoding LSTM.  The main contribution of the paper (based on the author response) is the introduction of this spectral feature representation to the ML community.  The authors used single 3D truck model to generate 205 simulations, of which 105 was used for training, and 100 for testing.  The authors presented reconstruction errors and TSNE visualization of the LSTM s reconstruction weights.  Discussion Summary: The paper got three weak rejects.  The response provided by the authors failed to convince any of the reviewers to adjust their scores.  The authors did not provide a revision based on the reviewer comments.  Overall, the reviewers found the problem statement to be interesting.  However, they had concerns about the following: 1. It s unclear what is the main technical contribution of the work.  Several of the reviewers pointed out the lack of technical novelty.  From the writing, it s unclear if the proposed spectral feature representation is taken directly from prior work or there was some additional innovation in this submission.  Based on the author response, it seems the proposed feature representation is taken directly from prior work as the authors themselves acknowledge that the submission is taking two known ideas and combining them.  This can be made more explicit in the paper itself.    2. Lack of comparison with existing work and experimental analysis There is no comparison against existing work on predicting 3D structure deformation over time. While the proposed representation is interesting, the is no comparison with other methods or other alternative representations.  Without any comparisons it is difficult to judge how the reconstruction error corresponding to actual reconstruction quality.  How much error is acceptable?  The submissions also fails to elucidate when the proposed representation should be used.  Is it better than alternative representations (use 3D mesh directly? use point clouds? use alternate basis functions?)   3. What is being learned by the model?   R3 pointed out that the authors mention that the model is trained in just half an hour and questioned whether the dynamics function is trivial to learn and that the only two parts of the 3D structure is analyzed.  The authors responded that the "coarse" dynamic is easier to learn than the "fine" scale dynamics.  Is what is learned by the model sufficient?  How well would a model that just modeled the car as a rigid object and predicted the position do?  The lack of comparison against baselines and alternative methods/representations makes it difficult to judge usefulness of the representation/approach that is presented.  4. The paper also has minor typos.  Page 5: "treat the for beams"  > "treat the four beams" Page 7: "marrked"  > "marked"  Overall the paper addresses a interesting problem domain, and introduces a interesting representation to the ML community, but fails to do a proper experimental analysis showing how the representation compares to alternatives.  Since the paper does not claim the novelty of the representation as its contribution, it is essential that it performs a thorough investigation of the task and perform empirical studies comparing the proposed representation/method against baselines and alternatives.
After reading the author s response, all the reviwers still think that this paper is a simple extension of gradient masking, and can not provide the robustness in neural networks.
This paper presents a large scale automatically extracted knowledge base in Chinese which contains information about entities and their relations present in academic papers. The authors have collected several papers that come from around 38 different domains. As such this is a dataset creation paper where the authors have used existing methodologies to perform relation extraction in Chinese.  After having read the reviews and followup replies by authors, the main criticisms of the paper still hold. In addition to the lack of technical contribution, I feel that the writing of the paper can be improved a lot, for example, I would like to see a table with some example entities and relations extracted. That said, with further improvements this paper could potentially be a good contribution to LREC which is focused on dataset creation.  In its current form, I recommend the paper to be rejected.
The paper proposes metrics for comparing explainability metrics.  Both reviewers and authors have engaged in a thorough discussion of the paper and feedback. The reviewers, although appreciating aspects of the paper, all see major issues with the paper.   All reviewers recommend reject.  
This paper proposes a new training technique to produce a learned model robust against adversarial attacks   without explicitly training on example attacked images. The core idea being that such a training scheme has the potential to reduce the cost in terms of training time for obtaining robustness, while also potentially increasing the clean performance. The method does so by proposing a version of label smoothing and doing two forms of data augmentations (gaussian noise and mixup).   The reviewers were mixed on this work. Two recommended weak reject while one recommended weak accept. All agreed that this work addressed an important problem and that the proposed solution was interesting. The authors and reviewers actively engaged in a discussion, in some cases with multiple back and forths. The main concern of the reviewers is the inconclusive experimental evidence. Though the authors did demonstrate strong performance on PGD attacks, the reviewers had concerns about some attack settings like epsilon and how that may unfairly disadvantage the baselines. In addition, the results on CW presented a different story than the results with PGD.   Therefore, we do not recommend this work for acceptance in its current form. The work offers strong preliminary evidence of a potential solution to provide robustness without direct adversarial training, but more analysis and explanation of when each component of their proposed solution should increase robustness is needed.  
All reviewers suggest rejection. Beyond that, the more knowledgable two have consistent questions about the motivation for using the CCKL objective. As such, the exposition of this paper, and justification of the work could use improvement, so that experienced reviewers understand the contributions of the paper.
This paper investigates the properties of deep neural networks as they learn, and how they may relate to human visual learning (e.g. how learning develops across regions of the infant brain). The paper received three reviews, all of which recommended Weak Reject. The reviewers generally felt the topic of the paper was very interesting, but overall felt that the insights that the paper revealed were relatively modest, and had concerns about the connections between DNN and human learning (e.g., the extent to which DNNs are biologically plausible   including back propagation, batch normalization, random initialization, etc.   and whether this matters for the conclusions of the present study). In response to comments, the authors undertook a significant revision to try to address these points of confusion. However, the reviewers were still skeptical and chose to keep their Weak Reject scores.  The AC agrees with reviewers that investigations of the similarity   or not!   between infant and deep neural networks is extremely interesting and, as the authors acknowledge, is a high risk but potentially very high reward research direction. However, in light of the reviews with unanimous Weak Reject decisions, the AC is not able to recommend acceptance at this time. I strongly encourage authors to continue this work and submit to another venue; this would seem to be a perfect match for CogSci conference, for example. We hope the reviews below help authors to improve their manuscript for this next submission.