The paper proposes a variant of the hierarchical VAE architectures. All reviewers felt that the paper s clarity was lacking. While the authors made very significant improvements during the feedback phase, which were recognized by reviewers, the paper could use a revision that takes clarity into account from the ground up. I also think that the ablation studies should be expanded (if SOTA is not the goal, then science should be), e.g., compare to the setting in which q does not share the bijective layers with p.
This paper proposes a new method to perform uncertainty estimation based on ensembles with diverse network architecture.   The reviewers raised a few concerns:   Although it is ok not to compare with (Tao, 2019), an active analytical comparison with baselines for ensemble diversification should not be overlooked e.g. (Yao et al, 2008), (Olson et al, 2019), (Khurana et al, 2018), etc.   The approach presented in this paper is not novel in the general idea of searching for or diversifying ensembles    The reviewers agree that diversity methods can be implemented on top of NES, but it is unclear whether NES+diversity methods would give more over just diversity methods; so either measuring NES+diversity methods or a direct comparison of NES and diversity methods is important.  We encourage the authors address these issues in the next revision. 
This paper looks at chaos in learning in games, extending a line of work in two players zero sum games (that I found quite restrictive in the past). It somehow reduces the class of more general games to zero sum and cooperative games (this decomposition is already known) so that the techniques can be transposed here.  The paper is interesting, yet sometimes difficult to follow, and I am not certain that it gives many new insights.   Nonetheless, we believe its quality justify acceptance.
This paper presents novel results on linear identifiability in discriminative models, with three of the four reviewers arguing for acceptance. The paper went through an extensive round of edits, which incorporated detailed responses to issues raised by the reviewers.   While this paper would be a nice contribution to the conference, some reviewer concerns remain unresolved, so we encourage the authors to revise and resubmit to a future venue.
This paper investigates the one class classification problem, proposing to learn a self supervised representation and a distribution augmented contrastive learning method; thorough results and analysis show that the method is effective and backs up their claims in terms of the underlying mechanism for why it works. In general, reviewers thought the paper was well written, well motivated/argued, and presents a thorough related work comparison and experimentation, though the novelty was found to be somewhat low. Several reviewers brought up some possible weaknesses in terms of demonstrating uniformity of the representations as well as suggesting additional datasets. Through an interesting discussion, the authors provided additional visualizations and results on the Mvtec dataset. This further bolstered the arguments in the paper.   Overall, this is a strong paper with a clear argument and contribution, and so I recommend acceptance. 
Navigation is learned in a two stage process, where the (recurrent) network is first pre trained in a task agnostic stage and then fine tuned using Q learning. The analysis of the learned network confirms that what has been learned in the task agnostic pre training stage takes the form of attractors.  The reviewers generally liked this work, but complained about lack of comparison studies / baselines. The authors then carried out such studies and did a major update of the paper.  Given that the extensive update of the paper seems to have addressed the reviewers  complaints, I think this paper can be accepted.
While the authors appreciated the proposed contrastive training scheme and the strong related work summary, all authors agreed that the approach was severeley limited by being a pure selection based method. Without the help of another model that proposes molecules, the approach can only select reactants from an existing set. As target molecules become more complicated, the modeller must make a choice: (a) use a much larger initial candidate set which hopefully encompases all molecules necessary to make the target molecule, or (b) use another model to propose new intermediate molecules. The authors went with (b) which harmed their novelty claim: a big reason why retrosynthesis is hard is because of the need to generate unseen molecules, and if this is left to an already proposed model, the current approach is not adding much methodological novelty. While their approach does improve upon existing work in the multi step setting, there s even more recent work that has not been compared against (e.g., https://arxiv.org/pdf/2006.07038.pdf) so the improved performance may be outperformed.  The fix is straightforward: modify the methodology to also propose intermediate molecules. This will fix the novelty complaint and strengthen the practicality argument: practitioners could directly use this approach to discover synthesis routes. The authors could slightly update the related work, add comparisons against recent methods, and take into account the other feedback given by the authors. The paper is very nicely written, the proposed changes are purely methodological, and not insurmountable in my opinion. I would urge the authors to make these changes which I believe will result in a very nice paper.
The paper proposes to use pre trained 2D (i.e., image) GANs as a mechanism for recovering 3D shape from a single 2D image. The work demonstrates impressive results on not only human and cat faces, but also cars and buildings. The method is demonstrated with qualitative results and quantitative results on multiple datasets and tasks.  The reviewers were persuaded by the novelty and "neatness" of the idea (and the AC is in agreement) as well as the results. At submission time, there were some concerns with experimental details. For instance, there was a question of how carefully the settings have to be tuned (always a concern with unsupervised methods) as well as an overarching concern about the initialization and whether the method will work on less clean data. The reviewers (and the AC) seem to think that these have been sorted out in discussion.   All three reviewers were in favor of acceptance and the area chair is inclined to agree with the reviewers. In particular, the AC finds the work interesting and compelling. While there is an updated version already uploaded during the discussion, the AC encourages the reviewers to double check all the questions from the reviewers and include the answers from the discussion into the camera ready (even these results are in the appendix).
The paper proposes a novel way to ensemble multi class or multi label models based on a Wasserstein barycenter approach. The approach is theoretically justified and obtains good results. Reviewers were concerned with time complexity, and authors provided a clear breakdown of the complexity. Overall, all reviewers were positives in their scores, and I recommend accepting the paper.
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
This paper presents an unsupervised method for completing point clouds obtained from real 3D scans based on GAN. Generally, the paper is well organized, and its contributions and experimental supports are clearly presented, from which all reviewers got positive impressions. Although the technical contribution of the method seems marginal as it is essentially a combination of established methods, it well fits in a novel and practical application scenario, and its useful is convincingly demonstrated in intensive experiments. We conclude that the paper provides favorable insights covering the weakness in technical novelty, so I’d like to recommend acceptance.  
Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
This paper proposes a spatio temporal embedding loss for video instance segmentation. The proposed model (1) learns a per pixel embedding such that the embeddings of pixels from the same instance are closer than embeddings of pixels from other instances, and (2) learns depth in a self supervised way using a photometric reconstruction loss which operates under the assumption of a moving camera and a static scene. The resulting loss is a weighted sum of these attraction, repulsion, regularisation and geometric view synthesis losses. The reviewers agree that the paper is well written and that the problem is well motivated. In particular, there is consensus that the 3D geometry and 2D instance representation should be considered jointly. However, due to the lack of technical novelty, the complexity of the final model, and the issues with the empirical validation of the proposed approach, we feel that the work is slightly below the acceptance bar.
The paper shows that under a very restrictive assumption on the data, ReLU networks with one hidden layer and zero bias trained by gradient flow converge two a meaningful predictor provided that the network weights are randomly initialized with sufficiently small variances. While there is some overlap with a paper by Lyu & Li (2020), the paper under review establishes its results for networks with arbitrary widths whereas using the results of Lyu & Li (2020) works, at least so far, only for sufficiently wide networks. The assumption on the data is anything than realistic and actually any "simple, conventional" learning algorithm can easily learn in this regime. Nonetheless, getting meaningful results for neural networks is still a notoriously difficult task and for this reason, the paper deserves publication.   
In this paper, the authors proposed an offline policy optimization algorithm, motivated by an analysis of the upper bound error of importance sampling policy value estimator. Specifically, by the decomposition of the error in a particularly way, the authors identified some error which does not converge. Then, the authors introduce the contraints over feasible actions to avoid the overfitting induced by such errors. Finally, the authors tested the proposed algorithm empirically.  The paper is well motivated and the authors addressed some of the questions in their rebuttals. However, there are still several issues need to be addressed,     The alternative practical estimator with plug in behavior distribution would perfectly avoid the over fitting, which is, however, ignored. This is an important and easy to implemented competitor.    The pessimistic principle in the face of uncertainty (PFU) has been exploited extensively in offline policy optimization problem. How the proposed algorithm is connected to the PFU has not been discussed carefully, especially in terms of non asymptotic sample complexity, which makes the paper is not well positioned.     While the motivation is derived from the unbiased importance sampling estimator, the counterfactual risk minimization in Equation 7 is introduced suddently, without clear justification.     In my opinion, for a better clarification of the paper, the expressiveness of the policy family should not be discussed in this way. I understand the authors would like to avoid any possible degeneration, and explain the asymptotic lossless in terms of policy flexibility. However, the whole point of the paper is trying to introduce some mechanism to avoid the possible overfitting by regularizing the policy family. In other words, the restriction is on purpose and beneficial. I think the argument of policy family expressiveness should be re considered and re discussed.    Minor:     Markovian vs. non Markovian baseline comparison is not fair, and more comparison on well known benchmarks, e.g., OpenAI gym, should be conducted.    The \sigma upper bound should be explicitly provided and verified in practice.  In sum, the paper is well motivated, however, need further improvement to be pulished.
This paper studies the problem of modeling logical structure in a neural model.  It introduces a data set for probing various existing models and proposes a new model that addresses shortcomings in existing ones.  The reviewers point out that there is a bit of a tautology in introducing a new task and a new model that solves it.  The revised version addresses some of those concerns.  Overall, it is a thought provoking and well written study that will be interesting to discuss at ICLR.
This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and "pair" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self attentive architectures.   Unfortunately, the paper needs work in presentation (in particular, in Section 3) before it is ready to be published.
This paper studies optimization of over parametrized neural networks in the mean field scaling. Specifically, when the input dimension in larger than the number of training samples, the paper shows that the training loss converges to 0 at a linear rate under gradient flow. It s possible to extend the result by random feature layers to handle the case when input dimension is low. Empirically the dynamics in this paper seems to achieve better generalization performance than the NTK counterpart, but no theoretical result is known. Overall this is a solid contribution to the hard problem of analyzing the training dynamics of mean field regime. There was some debate between reviewers on what is the definition of "feature learning" and I recommend the authors to give an explicit definition of what they mean (and potentially use a different term).
All reviewers generally admit that the motivation of realizing search free autoaugment is reasonable and important. However, they also raised many concerns regarding the experimental evaluation to validate the practical effectiveness of the method. In particular, unclear discussion with respect to ablation studies, and the lack of the baselines implemented by the authors were the central issues that obscure the essential effect of the core contribution of the work.  The authors made great efforts to conduct additional experiments and did address some of those issues, however some experiments are not yet ready such as the baseline implementation on ImageNet and testing on large models. After the discussion phase, all reviewers decided to keep their initial scores toward rejection, and the AC agreed with their opinions.   In summary, the paper focuses on an important problem and the proposed method is potentially very useful, but the paper in its current form should be further polished and completed before publication, thus I recommend rejection this time.   
This is a nice paper on generating adversarial programs. The approach is to carefully use program obfuscators. After discussion and improvements, reviewers were generally satisfied with the approach and evaluation. The problem domain was also found to be of interest.
This paper received 3 quality reviews, with 2 rated 5 and 1 rated 6. While the reviewers recognize the various contributions and insights made by this work, it was also pointed out that this work lacks technical novelty. The authors agreed with this concerns and argued that this work provides a service to the community, citing imageNet and COCO papers. The AC agrees with the contribution and major concerns. Furthermore, the AC would like to point out that in term of the level of efforts, this work might not be on par with the imageNet and COCO. All things considered, the AC believes that this work is not ready for publication at its current form, and hence recommend rejection.
The reviewers all appreciated the results. They expressed doubts regarding the discrepancy between the assumptions made and the reality of the loss of deep networks.  I share these concerns with the reviewers but also believe that, due to the popularity of Adam, a careful analysis of a variant is worthy of publication.
Three reviewers have scored this paper  as 1/1/3 and they have not increased their rating after the rebuttal and the paper revision. The main criticism revolves around the choice of datasets, missing comparisons with the existing methods, complexity and practical demonstration of speed. Other concerns touch upon a loose bound and a weak motivation regarding the low rank mechanism in connection to DA. On balance, the authors resolved some issues in the revised manuscripts but reviewers remain unconvinced about plenty other aspects, thus this paper cannot be accepted to ICLR2020.
The paper provides a procedure for certifying L2 robustness in image classification. The paper shows that the technique indeed works in practice by demonstrating it s accuracy on CIFAR 10 and CIFAR 100 datasets.   The reviewers are positive about the paper. Please do incorporate feedback, especially around experimental setup to ensure that the work compares various methods fairly and provides a clear picture to the reader.
The paper needs work to improve clarity and strengthen the technical message. Also, the authors broke the policy of anonymous submission which disqualifies the paper.
The submission is motivated by an empirical observation of a phase transition when a sufficiently high L1 or L2 penalty on the weights is applied.  The proposed solution is to optimize for several epochs without the penalty followed by introduction of the penalty.  Although empirical results seem to moderately support this approach, there does not seem to be sufficient theoretical justification, and comparisons are missing.  Furthermore, the author response to reviewer concerns contain unclear statements e.g. "The reason is that, to reach the level of L1 norm that is low enough, the model needs to go through the strong regularization for the first few epochs, and the neurons already lose its learning ability during this period like the baseline method." It is not at all clear what "neurons already lose its learning ability" is supposed to mean.
The paper nicely unifies previous results and develops the property of local openness. While interesting, I find the application to multi layer linear networks extremely limiting. There appears to be a sub field in theory now focusing on solely multi layer linear networks which is meaningless in practice. I can appreciate that this could give rise to useful proof techniques and hence, I am recommending it to the workshop track with the hope that it can foster more discussions and help researchers move away from studying multi layer linear networks.
This paper received 3 rejections and 1 marginal accept. Reviewers were unanimous in that empirical evaluation is lacking. No rebuttal was submitted and I have no reason to overturn the reviewers  decisions. I recommendation this paper be rejected.
This paper presents a novel and interesting sketch based approach to conditional program generation. I will say upfront that it is worth of acceptance, based on its contribution and the positivity of the reviews. I am annoyed to see that the review process has not called out the authors  lack of references to the decently body of existing work on generating structure on neural sketch programming and on generating under grammatical constraint. The authors  will need look no further than the proceedings of the *ACL conferences of the last few years to find papers such as: * Dyer, Chris, et al. "Recurrent Neural Network Grammars." Proceedings of NAACL HLT (2016). * Kuncoro, Adhiguna, et al. "What Do Recurrent Neural Network Grammars Learn About Syntax?." Proceedings of EACL (2016). * Yin, Pengcheng, and Graham Neubig. "A Syntactic Neural Model for General Purpose Code Generation." Proceedings of ACL (2017). * Rabinovich, Maxim, Mitchell Stern, and Dan Klein. "Abstract Syntax Networks for Code Generation and Semantic Parsing." Proceedings of ACL (2017).  Or other work on neural program synthesis, with sketch based methods: * Gaunt, Alexander L., et al. "Terpret: A probabilistic programming language for program induction." arXiv preprint arXiv:1608.04428 (2016). * Riedel, Sebastian, Matko Bosnjak, and Tim Rocktäschel. "Programming with a differentiable forth interpreter." CoRR, abs/1605.06640 (2016).  Likewise the references to the non neural program synthesis and induction literature are thin, and the work is poorly situated as a result.  It is a disappointing but mild failure of the scientific process underlying peer review for this conference that such comments were not made. The authors are encouraged to take heed of these comments in preparing their final revision, but I will not object to the acceptance of the paper on these grounds, as the methods proposed therein are truly interesting and exciting.
The contributions of this paper lie in two areas: a new benchmarking dataset and a new way to generate benchmarking datasets. Overall, the reviewers are split in their assessment based on which particular area they are focusing on. Reviewers who focus more on evaluating this work as a new benchmarking dataset, correctly point out that the variation within the search space has been shown to be limited and that the evaluation focuses on an overly studied and toy (by today’s standards) dataset such a CIFAR 10. In terms of choice of dataset, this work is indeed a step backwards from nasbench201, which includes more datasets, although it is a step forward in terms of size of the search space. As one reviewer correctly points out “This paper doesn’t present a benchmark. It provides a model that represents computationally efficient means of getting network accuracies from the DARTS search space”. The authors argue that the combination of the DARTS search space and its evaluation on CIFAR 10 is the de facto evaluation standard in the NAS community, which is also true. Ultimately, benchmark datasets somewhat direct the attention of the community and this attention would be better directed elsewhere, not on DARTS+CIFAR 10, as pointed out by some of the reviewers.  On the other hand, this work is as much about a new benchmark as it is about a *protocol* to generate new benchmarks. Specifically, a big part of the appeal and novelty here lie in the idea of training a predictive model from a small subset of architecture evaluations. From this perspective, the authors showed evidence that their approach is sound, economical (in terms of computational cost) and robust to sources of bias in the selection of the architectures to evaluate. The limitation here is that this was only shown in search spaces that where either small or lacking diversity and thus it’s unclear how general its findings are.  Overall, this is very much a paper that could have gone either way in terms of acceptance. It’s a step in the right direction in terms of methodology that can be used to generate reproducible benchmarks in a computationally efficient way. It’s a step backwards in terms renewing focus on measures of performance that (arguably) we have all overfit to.  
While the reviewers appreciated the problem to learn a multiset representation, two reviewers found the technical contribution to be minor, as well as limited experiments. The rebuttal and revision addressed concerns about the motivation of the approach, but the experimental issues remain. The paper would likely substantially improve with additional experiments.
The paper is about exploration in deep reinforcement learning. The reviewers agree that this is an interesting and important topic, but the authors provide only a slim analysis and theoretical support for the proposed methods. Furthermore, the authors are encouraged to evaluate the proposed method on more than a single benchmark problem.
This paper defines a truly unsupervised image translation scenario. Namely, there are no parallel images or domain labels. To achieve robust performance in this scenario, the authors use 1) clustering and 2) generator discriminator structure to map images from different domains and generate images for target domains.    In all, all the reviewers agree that this definition of unsupervised image translation is interesting. However, there are also several concerns for the real world practical application and empirical results.  Unlike unsupervised text translation whose target language is known, the truly unsupervised image translation is difficult to make sense without identifying what is the target domain. This limits the contribution of this paper to some specific tasks instead of more general tasks. For the empirical results, the selection of data and the hyperparameter K do not convince the reviewers.  
This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. The authors describe this as "maximizing the complement entropy." Rather than adding the cross entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross entropy, then a step is taken to maximize the complement entropy. Extensive experiments on image classification (CIFAR 10, CIFAR 100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English Vietnamese task), and small vocabulary isolated word recognition (Google Commands), show that the proposed two objective approach outperforms training only to minimize cross entropy. Experiments on CIFAR 10 also show that models trained in this framework have somewhat better resistance to single step adversarial attacks. Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem.
This paper applies a metalearning strategy to point cloud registration, which refines 3D registration networks to improve performance on specific datasets/settings.  Reviews for this paper recognized its potential interest but uniformly highlighted that the work is lacking in polish both from an expository perspective and in terms of experiments.  Questions included whether the experiments truly support the claim of generalization, and whether the work would be better considered as a method for scene flow.  Authors did not rebut these points, so I am recommending rejection.
The reviewers all feel that the paper should be accepted to the conference.  The main strengths that they noted were the quality of writing, the wide applicability of the proposed method and the strength of the empirical evaluation.  It s nice to see experiments across a large number of problems (100), with corresponding code, where baselines were hyperparameter tuned as well.  This helps to give some assurance that the method will generalize to new problems and datasets.    Some weaknesses noted by the reviewers were computational cost (the method is significantly slower than the baselines) and they weren t entirely convinced that having more concise representations would directly lead to the claimed interpretability of the approach.  Nevertheless, they found it would make for a solid contribution to the conference.
This paper proposes a general framework for constructing Trojan/Backdoor attacks on deep neural networks. The authors argue that the proposed method can support dynamic and out of scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting. This paper has been very carefully discussed. While the idea is interesting and could be of interest to the broader community, all reviewers agree that it lacks of experimental comparison with existing methods for backdoor attacks on benchmark problems. The paper needs to be significantly revised before publication. I encourage the authors to improve this paper and resubmit to future conference.
The paper studies safer policy improvement based on non expert demonstrations.  The paper contains some interesting ideas, and is supported by reasonable empirical evidence.  Overall, the work has a good potential.  The author response was also helpful.  That said, after considering the paper and rebuttal, the reviewers were not convinced the paper is ready for publication, as the significance of this work is limited by a rather strong assumption (see reviews for details).  Furthermore, the presentation of the paper also requires some work to improve (see reviews for detailed comments).
The paper proposes a framework at the intersection of programming and machine learning, where some variables in a program are replaced by PVars   variables whose values are learned using machine learning from data. The paper presents an API that is designed to support this scenario, as well as three case studies: binary search, quick sort, and caching   all implemented with PVars.  The reviewers and the AC agree that the paper presents and potentially valuable new idea, and shows concrete applications in the presented case studies. They provide example code in the paper, and present a detailed analysis of the obtained results.  The reviewers and AC also not several potential weaknesses   the AC will focus on a subset for the present discussion. The paper is unusual in that it presents a programming API rather than e.g., a thorough empirical comparison, a novel approach, or new theoretical insights. Paper at the intersection of systems and machine learning can make valuable contributions to the ICLR community, but need to provide a clear contributions which are supported in the paper by empirical or theoretical results. The research contributions of the present paper are vague, even after the revision phase. The main contribution claimed is the introduction of the API, and that such an API / system is feasible. This is an extremely weak claim. A stronger claim would be if e.g., the present approach would advance the state of the art beyond an existing such framework, e.g., probabilistic programming, either conceptually or empirically. I want to particularly highlight probabilistic programming here, as it is mentioned by the authors   this is a well developed research area, with existing approaches and widely used tools. The authors dismiss this approach in their related work section, saying that probabilistic programming is "specialized on working with distributions". Many would see the latter as a benefit, so the authors should clearly motivate how their approach improves over these existing methods, and how it would enable novel uses or otherwise provide benefits. At the current stage, the paper is not ready for publication.
The authors provide a cubic regularization approach to non convex concave minimax problems. The reviewers highlight that the paper in its current form is not ready for publication due to issues such as the gap between the theory and the implementable algorithm.
This paper proposes a new method for pre training of language models in the e commerce domain. It introduces five objectives for pre training by incorporating domain knowledge into the model.  Pros • The paper is generally easy to follow. • Design of the pre training objectives is reasonable. • Experimental results are solid and convincing. • A useful method is proposed, and its effectiveness has been verified in the e commence domain.  Cons • Novelty of the work might not be enough. • Presentations can be improved. • It is not clear whether the proposed approach can be applied to other domains which may not have enough structured data.  The authors have made several things clearer in the rebuttal. They have also added new experimental results. However, the overall quality of the paper does not reach the level of ICLR from the viewpoint of novelty, significance, and clarity.  
The paper studies the use of PixelCNN density models for the detection of adversarial images, which tend to lie in low probability parts of image space. The work is novel, relevant to the ICLR community, and appears to be technically sound.  A downside of the paper is its limited empirical evaluation: there evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defense on a dataset like ImageNet.
AR1 is concerned about the novelty and what are exact novel elements of the proposed approach. AR2 is worried about the novelty (combination of existing blocks) and lack of insights. AR3 is also concerned about the novelty, complexity and poor  evaluations/lack of thorough comparisons with other baselines. After rebuttal, the reviewers remained unconvinced e.g. AR3 still would like to see why the proposed method would be any better than GAN based approaches.  With regret, at this point, the AC cannot accept this paper but AC encourages the authors to take all reviews into consideration and improve their manuscript accordingly. Matters such as complexity (perhaps scattering networks aren t the most friendly here), clear insights and strong comparisons to generative approaches are needed.
The paper studies the problem of being able to control text generated by pre trained language models. The problem is timely and important. The paper   frames the problem as constraint satisfaction over a probability distribution. Both pointwise and distributional constraints can be imposed. The proposed algorithm,  Generation with Distributional Control (GDC), is elegant, and is an interesting new addition to this line of work. Overall, the paper brings forth news ideas, and could have impact. 
The problem setting studied in this paper is an extension of the problem setting of multi domain learning, where domain information is missing in training. This is an interesting and practical problem setting. However, regarding technical novelty, the contributions are relatively limited. Specifically, first, the overall idea is an extension of an existing multi domain learning method [Rebuffi et al. 2017] by replacing the domain indicators with learnable gates. Second, the idea of introducing learnable gates is borrowed from some previous works. Third, the sparse activation is also based on an existing work of sparsemax. Though the authors claim sparsemax or sparse activation was only used in the NLP domain, this does not increase the technical novelty by applying sparsemax to the CV domain.   To be fair, the combination of the aforementioned techniques to solve the so called latent domain learning problem looks reasonable, while the technical contribution is not significant. Overall, I feel slightly positive about this work and recommend a weak acceptance (it could be considered for publication only if there is space).
The authors consider an interesting approach for modeling analogical relations through Abelian group networks. While the conceptual contributions in the work, the explicit introduction of Abelian relations in particular, were generally appreciated, the reviewers found the numerical results provided in the paper lacking. In addition, several issues regarding the scope of the problems to which the proposed approach applies have been raised. Thus, given this, and the exchanges between the reviewers and the authors, in its present form, the paper cannot be recommended for acceptance. The authors are encouraged to incorporate the valuable feedback provided by the knowledgeable reviewers.
The paper introduces an interesting new model for MDPs, where the time is divided into random segments, and at the end of each segment the cumulative reward for the given segment is communicated to the agent. Some theoretical results with a policy improvement algorithm, as well as a more practical algorithm are presented. While the reviewers valued these contributions, they all had issues with the presentation of the paper.  These presentation issues make the paper extremely hard to follow   this was a problem for all reviewers, and I also verified it myself. The reviewers also raised issues regarding the experiments, where the algorithms should be tuned properly to be able to draw valid conclusions.  While unfortunately the above issues prevent me from recommending acceptance of the paper, the authors are strongly encouraged to revise their paper and resubmit to the next venue, with a special emphasis on making the presentation proper. There are several problems/recommendations mentioned in the reviews which will certainly help in this regard (I would also add that special care should be made that everything is defined properly, e.g., the equation for your policy iteration should appear in the main text not in a proof in the appendix, or $\hat{Q}_\phi$ should be defined, etc.).
This paper investigates Bayesian optimization where a prior distribution over the optimal is available. The authors conducted a systematic study on a very intuitive prior augmented acquisition function that multiplication the prior probability with the EI heuristic   including an asymptotic analysis on the regret, comprehensive (controlled) synthetic experiments, and moderate empirical support on several real world case studies.  All reviewers find the paper well written and appreciate the rigor of the empirical evaluation. The theoretical analysis is also helpful to provide additional justification for the proposed approaches. I would like to add that the paper also included a brief but comprehensive survey on prior work related to leveraging prior in BO, which I find useful for the general audience.  Reviewers noted that such a bound could become trivial with a bad prior in practice, and further suggest that one may leverage these theoretical insights as general guidelines to practitioners in designing the prior. I think this is a valuable message to convey and suggest authors take it into account in the revision.  There were initial confusions pertaining to the experimental details, mainly concerning the effect of the quality of the prior on the performance of the proposed algorithm. The authors provided an effective rebuttal with much concrete empirical support, and after a few rounds of interaction during the discussion phase, the reviewers are convinced about the empirical significance of the proposed work. Overall, this makes a solid work.
The paper proposes sample robustness (a data dependent measure) which is essentially a point wise Lipschitz constant of the label map. The measure is used to choose a subset of training data for training and it measures how small of a perturbation is required to cause a label change w.r.t. label map. initially, the paper lacked theoretical motivation and backing and the empirical studies were limited to be convincing enough. The authors added additional theoretical explanations. There were some mathematical mistakes that were fixed in the revision.  However, that is not enough to justify the proposal fully. Therefore, I suggest the authors improve the theoretical explanation. The paper would also benefit from more empirical analysis and discussion. As is, the paper has limited significance to the community since the conclusion is not convincing enough.   The paper writing quality although improved from the original version, still has room for improvement.   The proposed measure is simple, which can be a plus. but that means that we are also missing on some relationships and interactions between samples impact on training. Therefore, The paper will benefit from clearly discussing pros and cons of the proposed method. Moreover, discussing how this definition works in choosing the best subset of samples will improve the paper.   i thank the authors for their effort and improving the paper in response to the reviews. However, given that myself and reviewers find the modifications enough for addressing all the concerns, I vote for rejecting the paper. Please improve the paper and resubmit to a future venue.
The paper proposed and analyze a k NN method for identifying corrupted labels for training deep neural networks.  Although a reviewer pointed out that the noisy k NN contribution is interesting, I think the paper can be much improved further due to the followings:  (a) Lack of state of the art baselines to compare. (b) Lack of important recent related work, i.e., "Robust Inference via Generative Classifiers for Handling Noisy Labels" from ICML 2019 (see https://arxiv.org/abs/1901.11300). The paper also runs a clustering like algorithm for handling noisy labels, and the authors should compare and discuss why the proposed method is superior. (c) Poor write up, e.g., address what is missing in existing methods from many different perspectives as this is a quite well studied popular problem.  Hence, I recommend rejection.
All reviewers unanimously accept the paper.
The paper proposes a method of training implicit generative models based on moment matching in the feature spaces of pre trained feature extractors, derived from autoencoders or classifiers. The authors also propose a trick for tracking the moving averages by appealing to the Adam optimizer and deriving updates based on the implied loss function of a moving average update.   It was generally agreed that the paper was well written and easy to follow, that empirical results were good, but that the novelty is relatively low. Generative models have been built out of pre trained classifiers before (e.g. generative plug & play networks), feature matching losses for generator networks have been proposed before (e.g. Salimans et al, 2016).  The contribution here is mainly the  extensive empirical analysis plus the AMA trick.  After receiving exclusively confidence score 3 reviews, I sought the opinion of a 4th reviewer, an expert on GANs and GAN like generative models. Their remaining sticking points, after a rapid rebuttal, are with possible degeneracies in the loss function and class level information leakage from pre trained classifiers, making these results are not properly "unconditional". The authors rebutted this by suggesting that unlike Salimans et al (2016), there is no signal backpropagated from the label layer, but I find this particularly unconvincing: the objective in that work maximizes a "none of the above" class (and thus minimizes *all* classes). The gradient backpropagated to the generator is uninformative about which particular class a sample should imitate, but the features learned by the discriminator needing to discriminate between classes shape those gradients in a particular way all the same, and the result is samples that look like distinct CIFAR classes. In the same way, the gradients used to train GFMN are "shaped" by particular class discriminative features when trained against a classifier feature extractor.  From my own perspective, while there is no theory presented to support why this method is a good idea (why matching arbitrary features unconnected with the generative objective should lead to good results), the idea of optimizing a moment matching objective in classifier feature space is rather obvious, and it is unsurprising that with enough "elbow grease" it can be made to work. The Adam moving average trick is interesting but a deeper analysis and ablation of why this works would have helped convince the reader that it is principled.   This paper was very much on the borderline. Aside from quibbles over the fairness of comparisons above, I was forced to ask myself whether I could imagine that this would be a widely read, influential, and frequently cited piece of work. I believe that the carefully done empirical investigation has its merits, but that the core ideas are rather obvious and the added novelty of a poorly understood stabilized moving average is not enough to warrant acceptance.
This paper makes the observation that, by adjusting the ratio of gradients from skip connections and residual connections in ResNet family networks in a projected gradient descent attack (that is, upweighting the contribution of the skip connection gradient), one can obtain more transferable adversarial examples. This is evaluated empirically in the single model black box transfer setting, against a wide range of models, both with and without countermeasures.  Reviewers praised the novelty and simplicity of the method, the breadth of empirical results, and the review of related work. Concerns were raised regarding a lack of variance reporting, strength of the baselines vs. numbers reported in the literature,  and the lack of consideration paid to the threat model under which an adversary employs an ensemble of source models, as well as the framing given by the original title and abstract. All of these appear to have been satisfactorily addressed, in a fine example of what ICLR s review & revision process can yield. It is therefore my pleasure to recommend acceptance.
The paper introduces a novel way of jointly modeling annotator competencies and learning from imperfect annotations. Reviewers were moderately positive. One reviewer mentioned Carpenter (2002) and subsequent work. One prominent example of this line of work, which the authors do not cite, is: https://www.isi.edu/publications/licensed sw/mace/   from 2013. I encourage the authors to cite this paper. In the discussion, the authors point out this type of work is not *end to end* in their sense. However, there s, to the best of my knowledge, a relatively big body of literature on end to end approaches that the authors completely ignore, e.g., [0 3]. In the absence of a discussion of this work, it is hard to accept the paper.   [0] https://link.springer.com/article/10.1007/s10994 013 5411 2 [1] https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber 7405343 [2] http://www.cs.utexas.edu/~atn/nguyen acl17.pdf [3] https://arxiv.org/pdf/1803.04223.pdf
The paper proposes  a decentralized algorithm with regret for distributed online convex optimization problems. The reviewers worry about the assumptions and the theoretical settings, they also find that the experimental evaluation  is insufficient.
This paper presents a transfer learning framework in neural topic modeling. Authors claim and reviewers agree that this view of transfer learning in the realm of topic modeling is novel.  However, after much deliberation and discussion among the reviewers, we conclude that this paper does not contribute sufficient novelty in terms of the method. Also, reviewers find the experiments and results not sufficiently convincing.  I sincerely thank the authors for submitting to ICLR and hope to see a revised paper in a future venue.
The paper proposes a method to disentangle style from content (two factor disentanglement) using weak labels (information about the common factor for a pair of images). It is similar to an earlier work by Mathieu et al (2016) with main novelty being in the use of the discriminator which operates with pairs of images in the proposed method. Authors also have some theoretical statements about two challenges in disentangling the factors but reviewers have complained about missing connection b/w theory and experiments, and about exposition in general.  The idea has novelty, although somewhat limited in the light of earlier work by Mathieu et al (2016)), and theoretical statements are also of interest but reviewers still feel the paper needs improvement in writing and presentation of results. I would recommend an invitation to the workshop track. 
The main contribution of this paper lies in the novel setting that is being considered: offline data without rewards is combined with meta training tasks to quickly adapt to new long horizon tasks at meta test time. Within this setting, it is shown that the combination of SPiRL and PEARL outperforms the individual algorithms. The technical contribution is limited, as now new methods are introduced. Nevertheless, the setting considered is interesting and the empirical evaluation is solid. For these reasons, I recommend acceptance.
This manuscript presents a reinterpretation of hindsight experience replay which aims to avoid recomputing the reward function, and investigates Floyd Warshall RL in the function approximation setting.  The paper was judged as relatively clear. The authors report a slight improvement in computational cost, which some reviewers called into question. However, all of the reviewers pointed out that the experimental evidence for the method s superiority is weak. Two reviewers additionally raised that this wasn t significantly different than the standard formulation of Hindsight Experience Replay, which doesn t require the computation of rewards for relabeled goals.  Ultimately, reviewers were in agreement that the novelty of the method and quality of the obtained results rendered the work insufficient for publication. The Area Chair concurs, and urges the authors to consider the reviewers  pointers to the existing literature in order to clarify their contribution for subsequent submission.
This paper proposes a novel architecture for question answering, which is trained in an end to end fashion.  The reviewers were unanimous in their vote to accept. Authors are encouraged to revise addressing reviewer comments.
This work deals with the important task of capturing named entities in a goal directed setting. The description of the work and the experiments are not ready for publication; for example, it is unclear whether the proposed method would have an advantage over existing methods such as the match type features that are only mentioned in Table 3 for establishing the baseline on the original bAbI dialogue dataset, but not even discussed in the paper.
The reviews were largely split in the beginning. Some of the concerns are firmly addressed, e.g. new results evaluating the actual latency in real hardware, and one reviewer raised the score from 5 to 6. However, another reviewer is not fully convinced by the response and decided to keep the original score of  "3: Clear rejection".    There are mainly two issues here. One is about the novelty of the method. The reviewer asked about the novelty and difference from CondConv/DynamicConv, however the authors emphasized the techniques to successfully train conditional computation models in general. As the focus of the paper is the proposal of the new (claimed as better) method, I have to say it is missing the points.  (The authors could have organized the storyline of the paper as "best practices for training conditional computation models" or something like that, if that is the true contribution the paper.) Another issue is about the advantage over the CondConv method. In the newly added results, BasisNet does not show clear advantage in terms of accuracy speed trade off without early exiting. Although the authors simply state that it is "infeasible" to do early termination on CondConv, the reason is not clear. Indeed, one can easily try layer level early exiting as done in BranchNet for example, if not the model level early exiting assumed for the BasisNet.   Base on the discussion above, I conclude that the paper has to clarify many issues before publication and thus recommend rejection.
This paper investigates the use of class conditional architectures in GANs. It achieves this by employing neural architecture search (NAS) on top of reinforcement learning. Their main contribution is a “flexible and safe” search space; experiments are carried out on CIFAR 10 and  100. Standard performance results are augmented by diagnostic studies.  This paper received a total of five reviews which, remarkably, yielded the same assessment: the paper had merits but was marginally below the acceptance threshold. In general, the reviewers thought the idea was interesting and straightforward, experiments extensive and the paper was clearly written. The primary concerns brought up were novelty (i.e. just cGAN + NAS?, R1 and R2), minimal performance gain (R4, R5), unclear motivation (R2), lack of comparisons   including to other NAS for GAN methods   (R1,R3), limited to low resolution datasets (R2,R3), no reporting of time or space complexity (R1,R3), unclear where improvement comes from   no control for capacity   (R5).   On the point about comparing to NAS + GAN works, the authors responded, stating that the NAS + GAN methods brought up by the reviewer were unconditional GAN methods and pointed out that they made unconditional GAN comparisons in the Appendix.  The authors also emphasized to multiple reviewers that the point of the paper was not to improve NAS. Interestingly, they also made a comment to R5 that the point of the paper was not to improve performance of cGANs, but to improve understanding of them.	  The reviewers are unanimous in that this paper falls just below the bar for the reasons outlined above. Following the discussion phase, I see no reason to overturn their recommendation. I hope that the authors can use the feedback from these reviews to improve this paper and re submit it.
While the reviewer s noted a number of strengths of your paper, the approach that you took, and agreed that you had tackled an important problem, concerns remained about presentation and clarity. I agree. (Here are just a few miscellaneous comments: the very first paragraph of the Introduction needs to be rewritten for clarity, in my opinion. Later on page 1, you use the term "the dynamics of density" but you should not assume that the reader knows what that means. There are typos as well, e.g. "make all predictions base[d] on Equation (6)" on page 4. It would be helpful to know something about why you chose the experimental setups in Synthetic 1, 2, and 3. )  Regarding the similarities between this paper and a previously published article I believe that the authors have addressed these concerns; I hope they are careful to avoid this situation in the future.
Multi objective learning is an increasingly important topic. This paper presents a method for better finding parts of the Pareto frontier through a new method to estimate the distance to the frontier and use this proxy to refine the state space partition.  The reviewers found this paper interesting and compelling and generally well written. The reviewers also thought the work could be further improved by better clarifying in the text where the proposed approach might fail, and what properties of the domain are needed, and also to better situate this paper within the related work, potentially including additional experimental comparisons. The authors provided detailed responses to the proposed questions and the authors are encouraged to ensure that these suggestions and discussions are well represented in the revised version.
The paper studies out of sample generalisation that require an agent to respond to never seen before instructions by manipulating and positioning objects in a 3D Unity simulated room, and analyzes factors which promote combinatorial generalization in such environment.   The paper is a very thought provoking work, and would make a valuable contribution to the line of works on systematic generalization in embodied agents. The draft has been improved significantly after the rebuttal. After the discussion, we agree that it is worthwhile presenting at ICLR.  
This paper introduces a novel task (i.e., modelling the iterative process of editing sequences) and proposes a Transformer based architecture to address it tractably. The paper also elects a number of metrics that are argued to shed enough light onto the merits of the proposed architecture.   In our view, the current version is not ready for acceptance. Here are some of the reasons I d highlight:   * It is not entirely clear to us that the task in consideration has enough substance to grant acceptance nor that it speaks to a large enough audience. Perhaps the challenges identified here are more general and the developments for this task can be extended to related generation problems? If so, this is something one could consider for a revised version of the paper.  * The motivation does not seem to align well with the datasets used to demonstrate the task. Perhaps the difficulty to find a dataset that matches the motivation is an indication that the task and its challenges are tad too specific.  * It s the impression of more or less everyone involved that the paper lacks comparisons, and that the evaluation is not thorough enough, and the rebuttal did not ease our concerns sufficiently.
Reviewers unanimously vote for rejection for several reasons. First, the draft is incomplete and difficult to read. Second, one of the proposed methods (contextual sentence encoder) appears the same as past work, while the other proposed method (graph encoding) is difficult to interpret from what is written. Third, the draft is missing comparisons with recent work, and some included comparisons may be unfair due to data conditions. No author response was provided. The reviewer consensus is that this draft is underdeveloped, and not yet ready for submission or publication.
The paper studies RL from a rate distortion (RD) theory perspective.  A new actor critic algorithm is developed and evaluated on a series of 2D grid worlds.  The paper has some novel idea, and the connection of RL to RD is quite new.  This seems like an interesting direction that is worth further investigation.  On the other hand, all reviewers agreed there is a severe flaw in this work, casting a doubt where RD can be directly applied to an RL setting because the distribution is not fixed (unlike in standard RD).  This issue could have been addressed empirically, by running controlled experiments, something the the paper might include in a future version.
 The paper offers a more systematic treatment of various symmetry related results in the current literature. Concretely, the invariance properties exhibited by loss functions associated with neural networks give rise to various dynamical invariants of gradient flows. The authors address these dynamical invariants in a unified manner and study them wrt different variants of gradient flows aimed at reflecting different algorithmic aspects of real training processes.   The simplicity and the generality of dynamical invariants are both the strength and the weakness of the approach. On one hand, they provide a simple way of obtaining non trivial generalities for the dynamics of learning processes. On the other hand, they abstracts away the very structure of neural networks from which they derive, and hence only allow relatively generic statements. Perhaps the approach should be positioned more as a conceptual method for studying invariant loss functions.   Overall, although the technical contributions in the paper are rather incremental, the conceptual contribution of using dynamical invariants to unify and somewhat simplify existing analyses in a clear and clean symmetry based approach is appreciated by the reviews and warrant a recommendation for borderline acceptance.  
This paper proposes a neural network model for predicting multi aspect sentiment and generating masks that can justify the predictions. The positive aspects of the paper include improved results over the state of the art.  Reviewers found the technical novelty limited, and the experiments short of being fully convincing. After the author rebuttal, there were discussions between the reviewers and the AC, and the reviewers still thought the paper is not fully convincing given these limitations.  I thank the authors for their submission and detailed responses to the reviewers and hope to see this research in a future venue.
All of the reviewers appreciate the clarity of exposition and the importance of the problem studied. That said, I agree with Reviewer P9Ys that the results are somewhat underwhelming. The baselines appear weak and are likely not well tuned on the Stanford car dataset. Key question that remains unanswered in my opinion is whether this is the most effective approach to using synthetic data to improve classification accuracy (e.g., in contrast to [Ravuri & Vinyals, 2019](https://arxiv.org/abs/1905.10887) and follow up work). Nevertheless, I believe the community will benefit from this paper s contributions and this line of work.
This paper proposes InfoMax Termination Critic (IMTC), a new approach for learning option termination conditions with the aim of discovering more diverse options. IMTC relies on a scalable approximation of the gradient of a mutual information objective with respect to the termination function parameters.  Reviewers liked the motivation and the simplicity of the approach. While there were some initial concerns regarding the similarity of IMTC and VIC, the authors did a good job of clarifying the differences and providing additional results in the rebuttal. While two reviewers raised their scores based on the rebuttal, this left reviewers split on whether to accept or reject the paper.  Given that the paper’s main contributions are evaluated empirically I based my decision on the strength of the evaluation. The main claim in the paper is that IMTC significantly improves the diversity of the learned options when combined with intrinsic control methods like VIC and RVIC. The main supporting evidence of this claim is a visualization of the option policies and termination probabilities reached by VIC and RVIC. There are several issues with this comparison: * This is a poor visualization of the kind of option diversity the paper aims to obtain. Given that mutual information based objectives used by VIC, RVIC and IMTC aim to optimize diversity in the final states reached by the options, visualizing the distribution of final states or the trajectories produced by the options is more meaningful. * The VIC and RVIC baselines are evaluated with a fixed option termination probability of 0.1 which biases the comparison in favor of IMTC because IMTC is able to choose when and where to terminate while VIC and RVIC with random termination get to control neither. Using fixed option duration with MI based option discovery methods like VIC, DIAYN and RVIC is more standard and is known to produce options with very clear terminal state clusters which are well separated for different options. Fixed option duration allows VIC and RVIC precise control of where they will terminate since option duration is fixed, hence it should have been included in the comparison. * As mentioned in point 2 above, it is well established that VIC tends to learn options with well clustered end states, especially in simple gridworld domains like in Figure 3 (see VIC, DIAYN and RVIC papers). The authors seem to obtain different qualitative results raising questions.  Overall, I don’t think the qualitative experiments show that IMTC is able to improve the diversity of options discovered by VIC or RVIC due to issues with how the experiments are done (random option duration for VIC and RVIC) and how the results are presented (visualizing action probabilities instead of final states). Given these concerns and the split among the reviewers I recommend rejecting the paper in its current form.
While the reviewers seem to like the main idea of the work, they had several concerns, particularly regarding the experiments (both their setup and description) and the overall language of the paper that they found it more suitable for the control community than the ML and representation learning community. The authors provided very long response and tried to address the issues raised by the reviewers during the rebuttals. Fortunately, the response addressed some of the issues they raised and now they all see the paper marginally above the line. However, reading the reviews and response shows that the paper can highly benefit from better writing and describing the experiments. So, I would strongly recommend that the authors include all the information they provided for the reviewers during the rebuttal phase in the paper and improve its quality. 
This paper presents a query embedding approach for answering multi hop queries over a hyper relational knowledge graph (KG). The main contributions are a new dataset (WD50K QE) for this task and a simple but sensible extension to an existing model for query embeddings to also handle relation qualifiers. Reviewers wJVm and Bute note that the reification and StarQE models perform similarly. While this is not a negative result, as the authors note, it does raise the question of the relative pros and cons of the two methods. I hope the authors can add a discussion of when one might prefer StarQE over the conceptually simpler reification method in the final version. The authors addressed Reviewer frRt’s concerns about faithfulness and backwards compatibility (though more evidence on purely triple based tasks would be nice here). Reviewer GQAR also raised some concerns about writing, but the other reviewers mostly found the paper to be well written and well motivated and I tend to agree. Overall, while there are some very good suggestions on how the paper can be extended and improved, I find the current contributions to be substantial enough to warrant a publication.
This paper studies over parameterization for unsupervised learning. The paper does a series of empirical studies on this topic. Among other things the authors observe that larger models can increase the number latent variables recovered when fitting larger variational inference models. The reviewers raised some concern about the simplicity of the models studied and also lack of some theoretical justification. One reviewer also suggests that more experiments and ablation studies on more general models will further help clarify the role over parameterized model for latent generative models. I agree with the reviewers that this paper is "compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods". I disagree with the reviewers that theoretical study is required as I think a good empirical paper with clear conjectures is as important. I do agree with the reviewers however that for empirical paper I think the empirical studies would have to be a bit more thorough with more clear conjectures. In summary, I think the paper is nice and raises a lot of interesting questions but can be improved with more through studies and conjectures. I would have liked to have the paper accepted but based on the reviewer scores and other papers in my batch I can not recommend acceptance at this time. I strongly recommend the authors to revise and resubmit. I really think this is a nice paper and has a lot of potential and can have impact with appropriate revision.
The paper proposes a tensor decomposition method that interpolates between Tucker and CP decompositions. The authors also propose an optimization algorithms (AdaImp) and argue that it has superior performance against AdaGrad in this tesnor decomposition task. The approach is evaluated on some NLP tasks. The reviewers raised some concerns related to clarity, novelty, and strength of experiments. As part of addressing reviewers concerns, the authors reported their own results on MurP and Tucker (instead of quoting results from reference papers). While the reviewers greatly appreciated these experiments as well as authors  response to their questions and feedback, the concerns largely remained unresolved. In particular, R2 found the gain achieved by AdaImp not significantly large compared to Adagrad. In addition, R2 found very limited evaluation on how AdaImp outperforms Adagrad (thus little evidence to support that claim). Finally, AdaImp lacks any theoretical analysis (unlike Adagrad).
The reviewers appreciated the clarity of writing, and the importance of the problem being addressed. There was a moderate amount of discussion around the paper, but the two reviewers who responded to the author discussion were split in their opinion, with one slightly increasing their score to a 6, and the other remaining unconvinced. The scores overall are borderline for ICLR acceptance, and given that, no reviewer stepped forward to champion the paper.
This work proves a generalization bound for permutation invariant neural networks (with ReLU activations). While it appears the proof is technically sound and the exact result is novel, reviewers did not feel that the proof significantly improves our understanding of model generalization relative to prior work. Because of this, the work is too incremental in its current form.
This paper combines multiple existing ideas in Bayesian optimization (continuous fidelity, use of gradient information and knowledge gradient) to develop their proposed cfKG method.  While the methodology seems neat and effective, the reviewers (and AC) found that the presented approach was not quite novel enough in light of existing work to justify acceptance to ICLR.  Continuous fidelity Bayesian optimization is well studied and knowledge gradient + derivative information was presented at NIPS.  The combination of these things seems quite sensible but not sufficiently novel (unless the empirical results were *really* compelling).  Pros:   The paper is clear and writing is of high quality   Bayesian optimization is interesting to the community and compelling methods are potentially practically impactful   Outperforms existing methods on the chosen benchmarks  Cons:   Is an incremental combination of existing methods   The paper claims too much
 + Empirically convincing and clearly explained application: a novel deep learning architecture and approach is shown to significantly outperform state of the art in unsupervised anomaly detection.    No clear theoretical foundation and justification is provided for the approach    Connexion and differentiation from prior work on simulataneous learning representation and fitting a Gaussian mixture to it would deserve a much more thorough discussion / treatment. 
The paper proposes an approach to learn policies that can effectively transfer to new environments. The perspective on this problem from the perspective of streaming submodular optimization is nice; the paper introduces new ideas that are likely of interest to the ICLR community. Unfortunately, there are significant concerns about how convincing the results are. Multiple reviewers were concerned about there only being two experiments, and the lack of comparison to ep opt on the half cheetah experiment. Without a more solid empirical validation of the ideas, the paper does not meet the bar for publication at ICLR.
This paper proposes Decoupled Kernel Neural Processes (DKNPs), a new neural stochastic process, which learns a separate mean and kernel function to directly model the covariance between output variables. Numerical experiments on 1 D regression and 2 D image completion are provided.  There  was a concern that the original version of the proposed model was not a valid stochastic process, since the consistency condition of Kolmogorov Extension Theorem might not be satisfied. The authors fixed this by replacing multihead mixed attention (MMA) in the covariance path with multihead cross attention. Overall, reviewers find the work interesting, but there remain concerns that the novelty is limited and that  the current work lacks sufficient experimental evaluation.
The paper proposes an implicit function approach to learning the modes of multimodal regression. The basic idea is interesting, and is clearly related to density estimation, which the paper does not discuss.   Based on the reviews and the fact that the authors did not submit a helpful rebuttal, I recommend rejection.
The paper proposes a strategy for multiple learning agents to explore a large RL problem s state space, via the divide and conqeuer principle. It prescribes a design for each agent s reward function, which when optimized enables the agents to  carve out  and cover different parts of the state space yielding efficient exploratory behavior. The argument for efficacy of the proposed method is purely experimental, with numerical benchmarking on complex simulated environments.  The reviewers have raised several concerns that persist even after receiving detailed responses from the author(s). These include the lack of discussion about comparisons with seemingly closely related and applicable work, the perception that the comparisons of this method with others are not fair ("not apples to apples"), and the assessment that the ablation studies and investigation of the sensitivity to hyperparameters may not be comprehensive to make a compelling argument. Thus, keeping in mind the unanimous impression of the reviewers, I am of the view that while the paper contributes an interesting principle, more work is needed to argue for its acceptance in a clear way.
This paper presents a new mechanism to train spiking neural networks that is more suitable for neuromorphic chips. While the text is well written and the experiments provide an interesting analysis, the relevance of the proposed neuron models to the ICLR/ML community seems small at this point. My recommendation is that this paper should be submitted to a more specialised conference/workshop dedicated to hardware methods.
The paper investigates an active learning strategy for speeding up the convergence for SSL deep learning algorithms. When the SSL objective could learn a good approximation of the optimal model, the proposed method efficiently converges to the result with a few queries. The main idea is that when the eigenvalues of the NTK are large, the convergence rate is faster. The proposed algorithm maximizes the smallest eigenvalue of the NTK. An empirical investigation is also reported. The reviewers appreciated the general idea, but questioned about the actual execution of this paper in terms of both experimental comparison and (lack of) supportive theoretical results. I would like to encourage the authors to consider improving their paper along one of these two lines.  Unfortunately, as it currently stands, this paper is not ready for publication.
The reviewers raised a number of concerns including low readability/ clarity of the presented work and methodology, insufficient and at times unconvincing experimental evaluation of the proposed, and lack of discussion on pros and cons of the presented. The authors’ rebuttal addressed some of the reviewers’ comments but failed to address all concerns and reconfirmed that relatively large changes are still needed for the paper to be useful to the readers. Hence, although I believe this could be a very interesting paper, I cannot suggest it at this stage for presentation at ICLR.
This paper proposes a probabilistic model for data indexed by an observed parameter (such as time in video frames, or camera locations in 3d scenes), which enables a global encoding of all available frames and is able to sample consistently at arbitrary indexes. Experiments are reported on several synthetic datasets.   Reviewers acknowledged the significance of the proposed model, noted that the paper is well written, and the design choices are sounds. However, they also expressed concerns about the experimental setup, which only includes synthetic examples. Although the authors acknowledged during the response phase that this is indeed a current limitation, they argued it is not specific to their particular architecture, but to the task itself. Another concern raised by R1 is the lack of clarity in some experimental setups (for instance where only a subset of the best runs are used to compute error bars, and this subset appears to be of different size depending on the experiment, cf fig 5), and the fact that the datasets used in this paper to compare against GQNs are specifically designed.   Overall, this is a really borderline submission, with several strengths and weaknesses. After taking the reviewer discussion into account and making his/her own assessment, the AC recommends rejection at this time, but strongly encourages the authors to resubmit their work after improving their experimental setup, which will make the paper much stronger.
The paper presents a new online convex optimization algorithm that uses per coordinate learning rates. The learning rates are changed over time using information coming from the gradients. A regret upper bound is proved and the algorithm is empirically validated on deep learning experiments.  While the analysis is in principle correct, it does not seem to provide any advantage over the guarantees of similar algorithm, for example the mirror descent version AdaGrad with diagonal matrices. Also, despite the intuition of the authors, the reviewers have found that the approach used in the analsysis is fundamentally bounded to give a worse guarantee than AdaGrad. Overall, the theoretical contribution appears to be not sufficient.  On the empirical side, the experiments failed to convince the majority of the reviewers that the algorithm has a significative gain over similar algorithms.  More generally, this paper suffers from the same problem of many other similar papers: There is a complete disconnect from the theory proven under restrictive assumptions (convexity, bounded domains, no stochasticity) and the experiments (non convex functions, no projection on bounded domain, stochastic setting). Unfortunately, the deep learning literature is full of such papers, but the community should strive to do better and substantially raise the quality of field. In this view, I strongly suggest to the authors to try to improve the theoretical contribution, for example, trying to prove a convergence guarantee of the gradients to 0, rather than focusing on regret upper bounds. Such analysis would also suggest better ways to design new optimization algorithms better suited to non convex problems.
The paper attempts to extend the recent analysis of random deep networks to alternative activation functions.  Unfortunately, none of the reviewers recommended the paper be accepted.  The current presentation suffers from a lack of clarity and a sufficiently convincing supporting argument/evidence to satisfy the reviewers.  The contribution is perceived as too incremental in light of previous work.
This promising work proves that the proposed contrastive learning approach to representation learning can recover the underlying topic posterior information given standard topic modelling assumptions. The work provides detailed proof and detailed experiments. The analysis is interesting and yields interesting insights. However, the experimental results are somewhat weak by lacking comparison with more recent document representation work.  Pros:   Good detailed proofs and experiments.   Interesting idea of using topic modelling to understand representation learning.  Cons:   The description of DirectNCE is somewhat hidden and could be better introduced in the paper.   Experimental baselines are weak lacking a comparison to recent document representation work such as Arora et al. 2019.   Stronger classification baselines could be incorporated.
Both authors and reviewers agree that the ideas in the paper were not presented clearly enough.
The paper considers the relationship betwee:    perturbations to an input x which change predictions of a model but not the ground truth label   perturbations to an input x which do not change a model s prediction but do chance the ground truth label.   The authors show that achieving robustness to the former need not guarantee robustness to the latter.   While these ideas are interesting, the reviewers would like to see a tighter connection between the two forms of robustness developed. 
Based on the majority of reviewers with reject (ratings: 4,6,3), the current version of paper is proposed as reject. 
Reviewers all agreed that this submission has an interesting new idea for learning object/keypoint representations: parts of a visual scene that are not easily predictable from their neighborhoods are good object candidates. Experimental gains on various Atari games are convincing. The main drawback at this point is that the evaluation is limited to visually rather simple settings, and it is unclear how the approach will scale to more realistic scenes. 
Reviewers agree the paper should be accepted. See reviews below.
This paper introduces a new multilingual parallel Bible dataset for African languages, a new method for determining similarities between languages, and a collection of experiments to evaluate methods for choosing an additional language based on (a) similarity and (b) language history to include in a multilingual MT system. Results show that strategic inclusion of an additional language can substantially improve BLEU. Reviewers universally agree that progress on MT for African languages is a very important goal. However, reviewers pointed to several major concerns with the current draft: (1) lack of sufficient detail for replicating experiments, (2) missing analysis to interpret why experimental gains are so large, and (3) missing discussion and comparison with already existing methods in multilingual MT (e.g. multilingual training for low resource languages). I agree with reviewers that the paper is not ready for acceptance in its current form, but encourage re submission, possibly at an NLP conference.  
The paper is rejected based on unanimous reviews.
This paper addresses automatically learning the neighborhood size (they call adaptive neighbor support) for unsupervised representation learning with a VAE.  The neighborhood size is determined based on z scores from by estimating a normal distribution in the latent space.   The paper is poorly written.  There are several grammatical errors and typos that distracts from understanding the paper.  In addition, the use of terminology is not precise, which adds to the confusion, as pointed out by the reviewers.  AC VAE is better than VAE+KNN in Table 1 but worse in SCAN with KNN in Table 3.  Further analysis to understand why this is so is needed.  Additional measures of cluster quality is recommended.  As pointed out by the reviewers, this paper is below the acceptance threshold for ICLR.  The reviewers provided several constructive suggestions.  Please refer to detailed reviewer comments to help you improve your paper. 
This paper proposes a solution to the decentralized privacy preserving domain adaptation problem. In other words, how to adapt to a target domain without explicit data access to other existing domains. In this scenario the authors propose MDDA which consists of both a collaborator selection algorithm based on minimal Wasserstein distance as well as a technique for adapting through sharing discriminator gradients across domains.   The reviewers has split scores for this work with two recommending weak accept and two recommending weak reject. However, both reviewers who recommended weak accept explicitly mentioned that their recommendation was borderline (an option not available for ICLR 2020). The main issues raised by the reviewers was lack of algorithmic novelty and lack of comparison to prior privacy preserving work. The authors agreed that their goal was not to introduce a new domain adaptation algorithm, but rather to propose a generic solution to extend existing algorithms to the case of privacy preserving and decentralized DA.  The authors also provided extensive revisions in response to the reviewers comments. Though the reviewers were convinced on some points (like privacy preserving arguments), there still remained key outstanding issues that were significant enough to cause the reviewers not to update their recommendations.   Therefore, this paper is not recommended for acceptance in its current form. We encourage the authors to build off the revisions completed during the rebuttal phase and any outstanding comments from the reviewers. 
The paper proposes a gradient based method for OOD detection. While the paper has some interesting contributions, all the reviewers felt that the current version falls below the ICLR acceptance threshold. I encourage the authors to revise and resubmit to a different venue.
The reviewers mostly agree that this paper presents a new deep reinforcement learning based approach to solving a challenging problem in the communications domain   wireless scheduling. However, the main concern, expressed almost unanimously, is about the novelty of the ideas in the paper beyond the assembly of existing deep RL techniques and the translation of the scheduling problem to the language of MDPs in a careful manner that respects modern communication systems standards such as 5G (e.g., URLLC and eMBB traffic demands). A secondary concern, also expressed during the author rebuttal discussion, is about adequate comparison to competing approaches motivated from the literature in wireless scheduling. In view of these issues, I suggest that the author(s) explore more appropriate avenues to submit this piece of valuable translational work, including venues that address the specific topic of wireless communication where a more comprehensive evaluation and comparison could be possible.   (NOTE: The comments and evaluation above disregard the "enhanced" draft submitted by the author(s) during the rebuttal phase. I was informed that the submission was reverted to the original draft due to space constraints being exceeded in the enhanced version.) 
The paper proposes a method to make the filters of the last conv layer more class specific. The motivation for this is to improve upon the interpretability of the CNN, which is empirically shown by comparing the class activation maps (CAMs) of regular CNN and the proposed LSG CNN. While the idea is interesting, one of the concerns from reviewers is about limited applicability of the method, at least the way it is shown in experiments   a concern that I tend to agree with. As primary goal of the work is improving interpretability of CNNs, authors should test LSG CNN with some more recent methods for producing the saliency maps other than CAM to convincingly establish the value of the method. Authors also mention lack of hyperparameter tuning and the use of SGD with limited training epochs as a reason for the drop in accuracy. It will be worth spending some effort so the accuracy matches the standard benchmarks   this will help in arguing more convincingly about practical benefit of the method. 
The authors proposed an algorithm for sampling DAGs that is suited for continuous optimization. The sampling algorithm has two main steps: In the first step, a causal order over the variables is selected. In the second step, edges are sampled based on the selected order. Moreover, based on this algorithm, they proposed a method in order to learn the causal structure from the observational data. The causal structure learning algorithm is guaranteed to output a DAG at any time and it is not required any pre  or post processing unlike previous work.  There were concerns by two reviewers on the slight lack of novelty ("the proposed method of this paper is only a combination of well developed techniques") but I believe the proposed method is still worthwhile. In addition, the paper is overall well written and its experiment evaluation is thorough. It will be a nice addition to the field of differentiable causal discovery.  My recommendation is to accept the paper as a poster.
This paper proposes adaptive neural trees (ANT), a combination of deep networks and decision tress. Reviewers 1 leans toward reject the paper, pointing out several flaws. Reviewer 3 also raises concerns, despite later increasing rating to marginally above threshold.  Of particular note is the weak experimental validation.  The paper reports results only on MNIST and CIFAR 10. MNIST performance is too easily saturated to be meaningful. The CIFAR 10 results show ANT models to have far greater error than the state of the art deep neural network models.  As Reviewer 1 states, "performance of the proposed method is also not the best on either of the tested datasets. Please clearly elaborate on why and how to address this issue. It would be more interesting and meaningful to work with a more recent large datasets, such as ImageNet or MS COCO."  The rebuttal fails to offer the type of additional results that would remedy this situation. Without a convincing experimental story, it is not possible to recommend acceptance of this paper.
The paper suggests a new aggregation rule for federated learning in order to mitigate Byzantine attacks. However, as the reviewers pointed out, the theoreticial results of the paper are weak and incremental and the experiments are not solid.
The paper proposes a decision forest based method for outlier detection.  The reviewers and AC note the improvement over the existing method is incremental.  Although  the problem is of significant practical importance, AC decided that the authors should do more works to attract the attention of a broader range of ICLR audience.
The authors provide an analysis of soft winner take all (WTA) networks with Hebbian local learning as a generative probabilistic mixture model. They then present experiments on comparably simple data sets, MNIST and F MNIST. Results are compared to hard WTA networks and an MLP of the same size (single hidden layer) trained with backprop. Besides accuracy, the learning speed and adversarial robustness of the networks are compared.  This paper is borderline, and I was discussing it quite a bit with the reviewers. The reviewers agree that the manuscript has some merits, but they also point to a number of weak points.  Besides the objective evaluation, I would like to comment on the review dynamics of this paper. The paper had initially rather low ratings. The authors commented extensively on the reviews, in several waves, and with suggestive text such as "All reviewer s points addressed" (as a comment title) or "Based on the Reviewer s earlier comment, the revised paper is now a clear  accept ." to name just a few. I and the reviewers had the impression that the authors strongly urged the reviewers to increase their scores.  Due to the borderline ratings, I decided to read the paper carefully. My impression is in line with the main criticisms of the reviewers, and summarized in the following: On the positive side:   The manuscript tackles an interesting problem. WTA architectures are biologically highly relevant structures and it is relevant to study learning in them.   The authors provide a nice theoretical analysis.   The observation that WTA architectures improve adversarial robustness is very interesting.   Learning is local.   The manuscript is well written.  On the negative side:   Theory: Similar analyses have been performed before. While there are differences, the main ideas are rather similar, in particular with respect to (Nessler et al., 2009). The authors argue that in contrast to their work, Nessler et al. 2009 deals with spiking neurons. But since the authors argue with biological plausibility, I would see that as an advantage of Nessler et al.   Performance: The performance of their model is comparable to the standard hard WTA network, often showing only a very slight advantage. This raises the question why the soft WTA should be preferred over the hard WTA. The performance of the single hidden layer ANN is clearly better. This raises the question of the scalability of the approach.   The analysis of adversarial robustness is interesting, but there is no comparison to other defense methods (e.g. adversarial training). The authors argued in their comments that it is not an adv. defense paper, so this comparison is out of scope. This reasoning is understandable, but since this is maybe the most interesting point of the paper, it would be a nice to have.   Scalability: It is true that the learning is local, but the question is whether it scales to larger problems and deeper networks. After the first reviews, the authors added experiments on CIFAR 10 and a convolutional version of the model. However, the results were clearly below the state of the art and the convolutional model is barely described (5 lines in the appendix).  Conclusion: The manuscript has some interesting points. Given the the strong competition within ICLR however, I cannot propose acceptance.
The AC and reviewers all agree that the paper proposes a very interesting framework to extend Granger Causality to DAG structured dynamical systems with important applications.   The submission was the object of extensive discussion, and the AC and reviewers all agree that the author feedback satisfactorily addresses the vast majority of their concerns. We strongly urge the authors to incorporate all the points and revisions mentioned in their feedback.   We certainly hope that the author will pursue this line of work and consider scaling their approach to tackle larger applications such as those related to social networks.
The paper integrates several dimensionality reduction and sparsity methods for improving the efficiency of large pre trained models. Overall, the paper is interesting and discusses an important topic. However, it seems that it is not ready to be published at the current stage. I would encourage the authors to take reviewers comments into account and further improve the paper   The pros and cons of the papers are summarized in the following:   Pros:  + Improving the efficiency of large pre trained models is an essential research issue.  + The idea is interesting although the technical novelty is a bit limited.   Cons:    The key concern is that the technical and practical benefit of the proposed approach is not clear based on the results demonstrated in the experiments.    The writing of the paper can be further improved in general to make the motivation more clear.
The paper proposes a method called unsupervised temperature scaling (UTS) for improving calibration under domain shift.  The reviewers agree that this is an interesting research question, but raised concerns about clarity of the text, depth of the empirical evaluation, and validity of some of the assumptions. While the author rebuttal addressed some of these concerns, the reviewers felt that the current version of the paper is not ready for publication.  I encourage the authors to revise and resubmit to a different venue.
This paper advances the long running thread of sequence modelling research focussed on differentiable instantiations of stack based models. In particular it builds upon recent work on the Nondeterministic Stack RNN (NS RNN) by introducing three extensions. The first is to relax the need for a normalised distribution over the state and action distribution and allow unnormalised weights, this mostly serves to facilitate gradient flow and thus easier training. The second extension allows the RNN to condition on the top stack state as well as the symbol, improving expressiveness. The third improvement introduces a method for limiting the memory required to run the proposed model on long sequences, thus allowing its application to practical language modelling tasks. Each of these requires substantial algorithmic innovations.  The reviewers all agree that this is a strong paper worthy of publication. The paper includes a useful review of previous differentiable stack models which nicely sets up the rest of the paper where the contributions are well motivated and clearly presented. The reviewers had a number of clarification questions, partly due to the author s use of overly concise citations for key algorithms rather than inline descriptions. This situation has been improved by updates made to the paper. The evaluation includes a series of synthetic experiments which are clear and provide a good elucidation of the various stack models properties. The practical evaluation on language modelling is more limited and serves mostly to demonstrate that the nondeterministic model can be scaled to a basic language modelling task.  Overall this is a strong paper with a well motivated and clear hypothesis. It provides a substantial extension to the prior work on nondeterministic stack models and progresses this line of research toward practical applications.
This paper presents a substantially new way of introducing a syntax oriented inductive bias into sentence level models for NLP without explicitly injecting linguistic knowledge. This is a major topic of research in representation learning for NLP, so to see something genuinely original work well is significant. All three reviewers were impressed by the breadth of the experiments and by the results, and this will clearly be among the more ambitious papers presented at this conference.  In preparing a final version of this paper, though, I d urge the authors to put serious further effort into the writing and presentation. All three reviewers had concerns about confusing or misleading passages, including the title and the discussion of the performance of tree structured models so far.
The authors present an interesting approach but there were multiple significant concerns with the clarity of the presentation, and some concern with the significance of the experimental results.
The reviewers all agree that Monet proposed in the paper which optimizes for both local and global memory saving in Deep learning models is theoretically sound and experimentally convincing. Accept!
This paper proposes to improve VAEs  modeling of out of distribution examples, by pushing the latent representations of negative examples away from the prior.  The general idea seems interesting, at least to some of the reviewers and to me.  However, the paper seems premature, even after revision, as it leaves unclear some of the justification and analysis of the approach, especially in the fully unsupervised case.  I think that with some more work it could be a very compelling contribution to a future conference.
Summary: This paper studies the neural contextual bandit problem, and proposes a neural based bandit approach with a novel exploration strategy, called EE Net. Besides utilizing a neural network (Exploitation network) to learn the reward function, EE Net also uses another neural network (Exploration network) to adaptively learn potential gains compared to currently estimated reward.  Discussions: The reviewers appreciated the novelty and the quality of the ideas and results in this paper. Most questions were about details in algorithm design choices and in the analysis. The authors have addressed these questions and updated their draft. The reviewers have now reached a consensus and recommend accepting this paper.  Recommendation: Accept.
Important problem (explainable AI); sensible approach, one of the first to propose a method for the counter factual question (if this part of the input were different, what would the network have predicted). Initially there were some concerns by the reviewers but after the author response and reviewer discussion, all three recommend acceptance (not all of them updated their final scores in the system). 
The paper proposes a method for learning multi image matching using graph neural networks. The model is learned by making use of cycle consistency constraints and geometric consistency, and it achieves a performance that is comparable to the state of the art. While the reviewers view the proposed method interesting in general, they raised issues regarding the evaluation, which is limited in terms of both the chosen datasets and prior methods. After rounds of discussion, the reviewers reached a consensus that the submission is not mature enough to be accepted for this venue at this time. Therefore, I recommend rejecting this submission.
The paper proposes a method that learns mapping implicitly, by using a generative query network of Eslami et al. with an attention mechanism to learn to predict egomotion. The empirical findings is that training for egomotion estimation alongside the generative task of view prediction helps over a discriminative baseline, that does not consoder view prediction. The model is tested in Minecraft environments.  A comparison to some baseline SLAM like method, e.g., a method based on bundle adjustment, would be important to include despite beliefs of the authors that eventually learning based methods would win over geometric methods.  For example, potentially environments with changes can be considered, which will cause the geometric method to fail, but the proposed learning based method to succeed.  Moreover, there are currently learning based methods for the re localization problem that the paper would be important to compare against (instead of just cite), such as "MapNet: An Allocentric Spatial Memory for Mapping Environments" of Henriques et al.  and "Active Neural Localization" of Chaplot et al. . In particular, Mapnet has a generative interpretation by using cross convolutions as part of its architecture, which generalize very well, and which consider the geometric formation process. The paper makes a big distinction between generative and discriminative, however the architectural details behind the egomotion estimation network are potentially more or equally important to the loss used. This means, different discriminative networks depending on their architecture may perform very differently. Thus, it would be important to present quantitative results against such methods that use cross convolutions for egomotion estimation/re localization. 
The authors have done a good job methodologically addressing reviewer concerns. The empirical results are good, and the application impact is clear. There were some concerns about the technical heft of the approach, but there s overall agreement that the effective application to the domain is interesting and done very well. The AC is a bit concerned about the impact of the regularities of the domain used on the results, especially with regard to semantic regularities (homes have very particular regularities). But even without answering this question (it should be discussed in the camera ready though), this paper makes a solid contribution.
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.  Concerns raised include lack of sufficient motivation for the approach, and problems with clarity of the exposition.
The paper addresses the setting of learning with rejection while incorporating the ideas from learning with adversarial examples to tackle adversarial attacks. While the reviewers acknowledged the importance to study learning with rejection in this setting, they raised several concerns: (1) lack of technical contribution   see R1’s and R2’s related references, see R3’s suggestion on designing c(x); (2) insufficient empirical evidence   see R3’s comment about the sensitivity experiment on the strength of the attack, see R1’s suggestion to compare with a baseline that learns the rejection function such as SelectiveNet;  (3) clarity of presentation   see R2’s suggestions how to improve clarity. Among these, (3) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC can confirm that all three reviewers have read the author responses and have revised the final ratings. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
I ve summarized the pros and cons of the reviews below:  Pros: * The method for time representation in event sequences is novel and well founded * It shows improvements on several but not all datasets that may have real world applications  Cons: * Gains are somewhat small * The task is also not of huge interest to ICLR in particular, and thus the paper might be of limited interest  As a result, because the paper is well done, but drew little excitement from any of the reviewers, I suggest that this not be accepted to the main conference, but encouraged to present at the workshop track.
Dear authors,  While I appreciate the result that a convolutional layer can have full rank output, this allowing a dataset to be classified perfectly under mild conditions, the fact that all reviewers expressed concern about the statement is an indication that the presentation sill needs quite a bit of work.  Thus, I recommend it as an ICLR workshop paper.
The paper presents miniF2F, a dataset of 488 highschool and college level math problems. The problems are fully formalized and include proofs in the Metamath, Lean and Isabelle theorem provers (as the reviewers pointed out, the support for Isabelle is limited, and that should be made clearer in the abstract). This multi platform support is the main selling point of the benchmark, because it will make it possible to make direct comparisons among systems targeting different theorem provers.   The paper also does a good job discussing the benchmark selection and formalization process. This is important since some of the problems were translated from word problems.   As part of the rebuttal, the authors added extra information on the performance of the baselines and some qualitative details on how they fail.   Overall, there is agreement among the reviewers that this is a valuable benchmark that will enable comparisons among systems that today are very hard to compare.
This paper addresses a method that incorporates the task similarity (via task gradients) into the meta learning. The inner loop update is done by kernel regression with the similarity between gradients of tasks considered, and the outer loop is the gradient update with a particular regularization. Without any doubt, it is a timely and important topic to develop a meta learning method in the presence of outlier tasks. All reviewers criticized the experiments were done on only simple datasets without any ablation study. Authors revised their manuscript, to include more experiments, and tried to clarify the relation of their method to MetaSGD. Unfortunately, however, even after the author response, reviewers were not convinced that their concerns were resolved. In particular, it was claimed that the revised version still lacks comparisons to previous relevant work.  
This work presents  a new sample based policy extragradient algorithm for finding an approximate Nash equilibrium in tabular two player zero sum Markov games with improved sample complexity guarantees. While originally the reviewers had concerns regarding the novelty and technical difficulty of the paper, these were successfully resolved during the rebuttal, and now all reviewers agree that this is an interesting contribution. Hence, I recommend acceptance of the paper.  In the final version the authors should make the following changes:   Please mention early on (e.g., in the abstract and the introduction, as well as in the definition of the Markov game) that you consider a tabular problem (finite state and action spaces). Furthermore, it would be important to define informally the quantities in the bound in the abstract and when presenting Table 1.    While not entirely uncommon, Assumption 1 is quite strong, requiring mixing for any policies. It would be great if the authors could also add a comment on this, emphasizing that this is the case, as well as explaining how weakening the assumption would introduce problems (as explained in the response to Reviewer RwGu).   The comparison to the lower bound of Zhang et al. (2020) should also be included, as discussed in the response to Reviewer 5TU3.   Please discuss Assumption 2 in relation to the work of Wei et al. (2021), and rephrase the relation to the latter paper accordingly, as promised in the discussion with Reviewer Hsr5.
This paper addresses some of the well documented instabilities that can arise from fine tuning BERT on a dataset with few samples. Through a thorough investigation, they highlight various bizarre behaviors that have a negative impact on stability: First, that BERT inexplicably uses an unusual variant of Adam that, in fact, harms behavior; and second, that people tend to undertrain BERT on some downstream tasks. Separately, they find that reinitializing some of the final layers in BERT can be helpful. Since fine tuning BERT has become such a common way to attack NLP problems, these practical recommendations will be quite welcome to the community. These findings address issues raised by recent work, so the paper is timely and relevant. The paper has through empirical analysis and is clear to read. There is a concurrent ICLR submission with similar findings, and this paper stands on its own. Reviewers all agreed that this paper should be published.
This paper focuses on  communication efficient Federated Learning (FL) and proposes an approach for  training  large models on heterogeneous edge devices.   The paper is well written and the approach is promising, but all reviewers pointed out that both novelty of the approach and empirical evaluation, including comparison with state of art, are somewhat limited. We hope that suggestions provided by the reviewers will be helpful for extending and improving this work.
This paper proposes the use of gradient of the loss evaluated at the example with respect to the model parameters as the feature representation of that example. The authors performed an empirical analysis on anomaly detection benchmarks to demonstrate the practical benefits of the proposed method. While the reviewers find the idea interesting, the consensus is that the proposed method lacks justification, and that the main claims were not substantiated. While the reviewers proposed several key points of improvement, the raised issues were not addressed in the rebuttal. I will hence recommend rejection of this paper. 
This paper extends the Majorization Minimization principle, particularly the MISO method, to problems where the surrogate of randomly selected batch functions are intractable, such as in formulations rising from Variational Inference. There is a large gap between the reviewers  evaluations even after the author rebuttal and discussions. While the strength of the proposed method looks to be its generality, the main criticism from the reviews are the limited applicability and less convincing arguments and empirical evidences against alternatives such as Monte Carlo versions of popular adaptive stochastic optimization methods. Weighing these considerations and considering strengths of other submissions on similar topics, I have to recommend rejection of the paper at the current form.
Thanks for your submission to ICLR!  This paper presents a novel way to combine domain adaptation with semi supervised learning.  The reviewers were, on the whole, quite happy with the paper.  On the positive side, the results are very extensive and impressive, it s a clever way to combine domain adaptation and semi supervised learning, and it s a fairly general approach in that it works in several settings (e.g., unsupervised vs semi supervised domain adaptation).  On the negative side, the approach itself is somewhat limited technically.  After discussion, the one somewhat negative reviewer agreed that the paper has sufficient merit and should be accepted; thus, everyone was ultimately in agreement.  I also read this paper carefully and personally find it very interesting and promising, so I am happy to recommend acceptance.  It seems to give state of the art performance in several cases, and could possibly lead to more research down the road on methods to combine adaptation techniques with SSL.
Due to the delayed rebuttal made it very hard for reviewers to react.  The paper proposes a new sub type of POMDPs dubbed AFA POMDP. The proposed approach first learns a sequential VAE, then an RL approach learns control and feature acquisition policies jointly. The approach is evaluated on two tasks and shows very promising results compared to baselines. Overall the setting and the approach are very interesting.  The replies and revised paper managed to address some of the concerns of the reviewers. However, there remain a few open questions and doubts (see updated reviews), in particular as some of the arguments of the authors remain in the hypothetical, and the reviewers are still not entirely convinced by the choice of the experimental tasks.
There is insufficient support to recommend accepting this paper.  Although the authors provided detailed responses, none of the reviewers changed their recommendation from reject.  One of the main criticisms, even after revision, concerned the quality of the experimental evaluation.  The reviewers criticized the lack of important baselines, and remained unsure about adequate hyperparameter tuning in the revision.  The technical exposition lacked a sober discussion of limitations.  The paper would be greatly strengthened by the addition of a theoretical justification of the proposed approach.  In the end, the submitted reviews should be able to help the authors strengthen this paper.
Each of the reviewers had a slightly different set of issues with this paper but here is an attempt at a summary:  PROS: 1. Paper is mostly clear and well structured.  CONS: 1. Lack of novelty 2. Unsupported claims 3. Questionable methodology (using dropout confounds the goal of the experiment)  The authors did not submit a rebuttal.
This paper is devoted to "dyadic fairness" in representation learning. All the reviewers agreed that the contribution is novel, original and technically sound. However, all the reviewers agreed that the paper should be improved in terms of presentation   for two reviewers, presentation/clarity issues were at the core of their weak rejects. The most positive reviewers highlighted that the problem is still understudied despite the flurry of work on fair machine learning in the last years and therefore the contribution deserves to be accepted. If there is room, this paper can be accepted as a poster.
Thanks for an interesting discussion. The authors present a supposedly task independent evaluation metric for generation tasks with references that relies on BERT or similar pretrained language models and a BERT internal alignment. Reviewers are moderately positive. I encourage the authors to think about a) whether their approach scales to language pairs where wordpieces are less comparable; b) whether second order similarly, e.g., using RSA, would be better than alignment based similarity; c) whether this metric works in the extremes, e.g., can it distinguish between bad output and super bad output (where in both cases alignment may be impossible), and can it distinguish between good output and super good output (where BERT scores may be too biased by BERT s training objective). 
The problem considered in this paper is of general interest to all reviewers. However, while the reviewers in general appreciate the authors’ effort in providing theoretical analysis for a seemingly effective algorithm, they are unconvinced that the key technical claims are well justified (i.e. separation between theoretical analysis and the algorithm, which ultimately relies on the OOD score), the propositions are clear (e.g., key claims in the quality of kNN density estimator as an OOD detector not well supported by analysis/experiments), or that the experimental results are sufficiently compelling (e.g., lack of controlled experiments/ ablation study) to merit acceptance for the proposed solution.
Dear authors,  All reviewers pointed out that the proximity with Dropout warranted special treatment and that the justification provided in the paper was not enough to understand why exactly the changes were important. In its current state, this work is not suitable for publication to ICLR.  Should you decide to resubmit this work to another venue, please take the reviewers  comments into account.
The paper solves a PDE using an additional penalty function between the derivatives of the function. On toy examples and two PDEs it is shown that these additional terms help.  Pros:   The motivation is to include derivatives in the computationa             Implementation and testing on several examples, including high dimensional ones             Timing is included in the latest version   Cons:  The loss is Sobolev norm of the residuals of the equation.               The usage of the norm of the residual is not 100% consistent with the smoothness properties of the corresponding equation. For example, for the Poisson equation, the problem is selected in such a way the solution is analytic. However, for example, if the zero boundary conditions are enforced, and right hand side is all ones, the solution will have singularities. Thus, the main challenge would be the case when solution does have the singularities (and it will have it in many practical cases). The L2 norm then is not the right functional for the solution to exist, not to say about the higher order derivatives. So, these functionals are not motivated by the theory of the solution of PDEs, but are rather focused on much smoother solution.       Convergence. There are quite a few papers on the convergence of DNN approximations to solution of PDEs. The presented methods might have converged to a local minimum. An important reference is the paper by Yarotsky D. Error bounds for approximations with deep ReLU networks. Neural Networks. 2017 Oct 1;94:103 14.   
The reviewers agree that the paper is addressing an interesting problem, and provides a valuable contribution for the learning of quasimetrics and would be useful for many real world applications.
The reviewers that provided extensive reviews agree that the paper is well written and contains solid technical material. The paper however should be edited to address specific concerns regarding theoretical and empirical aspects of this work. 
Considering reviewers  comments and comparing with similar papers recently published or submitted, this is a good paper but hasn t reached the bar of ICLR.  We believe that the paper is not ready for publication yet, and strongly encourage the authors to use the reviewers  feedback to improve the work and resubmit to one of the upcoming conferences. 
Reviewers unanimous in assessment that manuscript has merits, but does not satisfy criteria for publication.  Pros:   Potentially novel application of neural networks to survival analysis with competing risks, where only one terminal event from one risk category may be observed.  Cons:   Incomplete coverage of other literature.   Architecture novelty may not be significant.   Small performance gains (though statistically significant)
This paper investigates theories related to networks sparsification, related to mode connectivity and the so called lottery ticket hypothesis.  The paper is interesting and has merit, but on balance I find the contributions not sufficiently clear to warrant acceptance.  The authors made substantial changes to the paper which are admirable and which bring it to borderline status.  
The paper proposes a neural network compression technique based on sparse and low rank approximations. The paper received mixed reviews, with one accept, one reject, and two borderline accepts. Most reviewers have appreciated the effort conducted for the evaluation. Three reviewers are nevertheless worried about the limited novelty and two of them found the positioning in the literature unclear with many missing references. In particular, one reviewer makes a strong case against the accceptance of the paper.  The authors have made a significant effort to address the issues raised by the reviewers with a very long rebuttal. The area chair has read in details the responses, the points raised by the reviewers, and the paper itself. He/she tend to agree with the issues raised by the reviewers about the positioning of the paper in the literature and the missing baselines. The rebuttal was very helpful and addresses some of the concerns. There are still some remaining issues    the discussion about related work is relegated to an appendix. Yet, it is critical for positioning the paper and a discussion within the main paper would be more appropriate.    there is no assessment of the statistical significance of the results. Hyper parameters are fixed to some ad hoc values and it is unclear what the effect of different hyper parameter choices is upon the method and other baselines.    for reproductibility purposes, providing code with the submission would be very helpful, especially given the empirical nature of the contribution.  Overall, this is a borderline case, which, unfortunately, would require additional work before being ready for acceptance.
Meta score: 4  This paper presents an approach which uses attention across multiple speech or video channels.  After some synthetic experiments, presents experiments on chime 3, but has a rather weak baseline system  Pros:    addresses an interesting task  Cons:    does not take account of other recent papers in the area    experimental results are weak   very high errors in baseline system    limited novelty
This paper adapts (Nachum et al 2017) to continuous control via TRPO.   The work is incremental (not in the dirty sense of the word popular amongst researchers, but rather in the sense of "building atop a closely related work"), nontrivial,  and shows empirical promise.    The reviewers would like more exploration of the sensitivity of the hyper parameters.
This paper provides an improved feature space adversarial attack.  However, the contribution is unclear in its significance, in part due to an important prior reference was omitted (song et al.)   Unfortunately the paper is borderline, and not above the bar for acceptance in the current pool.
*Overview* This paper applies RL to automated theorem proving to eliminate the need for human written proofs as training data. The method uses TF IDF for premise selections. The experiments compared with supervised baseline demonstrate some good performance.  *Pro* The paper provides a side by side comparison of the effect of the availability of human proofs on the final theorem proving.  The experiments compared with supervised baseline show that the proposed method has good performance even without human knowledge. The prosed TF IDF selection algorithm addresses a challenging issue in exploration of RL.   *Con* The reviewers primarily concern about  the novelty of the methods. It appears the method is not new since there exist a body of work leveraging RL to learn theorem provers. The tasks are also not novel.  After rebuttal, the reviewers are not convinced that the novelty is significant enough for ICLR. The reviewers are also concerned that the proposed method might not be easily generalized to other tasks.   *Recommendation* Although the proposed method and experiment demonstrate some merits, there is a lack of novelty in terms of approaches. Since existing results already consider similar methods and similar tasks, it would make the paper stronger if thorough experimental comparisons are performed.  
Interesting approach aiming to leverage cross domain schemas and generic semantic parsing (based on meaning representation language, MRL) for language understanding. Experiments have been performed on the recently released SNIPS corpus and comparisons have been made with multiple recent multi task learning approaches. Unfortunately, the proposed approach falls short in comparison to the slot gated attention work by Goo et al.  The motivation and description of the cross domain schemas can be improved in the paper, and for replication of experiments it would be useful to include how the annotations are extended for this purpose.  Experimental results could be extended to the other available corpora mentioned in the paper (ATIS and GEO). 
This paper proposes a simple yet powerful generalisation of graph scattering transforms that allows a flexible scale dilation structure, retaining the stability guarantees of dyadic transforms. Experiments with strong empirical performance are reported on a variety of biochemical tasks.  Reviewers acknowledged the soundness of the approach as well as the quality of the empirical evaluation, but also raised some concerns about lack of novelty. Ultimately this AC believes that, although this work solidifies Graph Scattering Transforms as a good alternative to GNNs on certain structured physical domains, it provides little advancements on the theory front. Unfortunately not all good papers can be accepted, and therefore the AC recommends rejection at this time, encouraging a resubmission.
All reviewers agree in their assessment that this paper does not meet the bar for ICLR. The area chair commends the authors for their detailed responses.
The rebuttal period influenced R1 to raise their rating of the paper. The most negative reviewer did not respond to the author response. This work proposes an interesting approach that will be of interest to the community. The AC recommends acceptance.
The paper proposes the use of reinforcement learning to learn heuristics in backtracking search algorithm for quantified boolean formulas, using a neural network to learn a suitable representation of literals and clauses to predict actions. The writing and the description of the method and results are generally clear. The main novelty lies in finding a good architecture/representation of the input, and demonstrating the use of RL in a new domain. While there is no theoretical justification for why this heuristic should work better than existing ones, the experimental results look convincing, although they are somewhat limited and the improvements are dataset dependent. In practice, the overhead of the proposed method could be an issue. There was some disagreement among the reviewers as to whether the improvements and the results are significant enough for publication.
This paper presents several models for recognition aware image enhancement. The authors propose to enhance the image quality in the presence of image degradation (e.g., low resolution, noise, compression artifacts) as well as to improve the recognition accuracy in a joint model. While acknowledging that the paper is addressing an interesting direction, the reviewers and AC note the following potential weaknesses: presentation clarity, limited technical contributions, insufficient empirical evidence. AC can confirm all the reviewers have read the rebuttal and have contributed to the discussion. All the reviewers and AC agree that the rebuttal was informative, and the authors have partially addressed some of the concerns (e.g. additional experiments). R2 has raised the score from reject to weak reject. However, at this stage AC suggest the manuscript is below the acceptance bar and needs a major revision before submitting for another round of reviews. We hope the reviews are useful for improving and revising the paper.
This paper adapts language models (LMs), recurrent models trained on large corpus to produce the next word in English, to two commonsense reasoning tasks: the Winograd schema challenge and commonsense knowledge extraction. For the former, the language model score itself is used to obtain substantial gains over existing approaches for this challenging task, while a slightly more involved training procedure adapts the LMs to commonsense extraction. The reviewers appreciated the simplicity of the changes to existing LMs and the impressive results (especially on the WSC).   The reviewers point out the following potential weaknesses: (1) clarity issues in the writing and the presentation, (2) a lack of novelty in the proposed approach, given a number of recent work has shown the ability of language models to perform commonsense reasoning, and (3) critical methodological issues in the evaluation that raise questions about the significance of the results. A lack of response from the authors meant that there was no further discussion needed, and the reviewers encourage the authors to take the feedback to improve further versions of the paper.
The reviewers liked the clarity of the material and agreed the experimental study is convincing. Accept.
The paper provides a method for constructing PAC confidence scores for pre trained deep learning classifiers. The reviewers were all positive about the paper.  Pros:   Has provable guarantees on the reliability of the prediction. Such guarantees are quite desirable in practice.   The problem of neural network uncertainty is important and timely problem, especially in safety critical applications.   The method is simple and well motivated.   Strong empirical performance.   Interesting applications to fast DNN inference and safe planning.  Cons:   Lack of generalization guarantees  the guarantees in the paper only hold on the training set; but in practice, performance in test is what s important.   Only a handful of baselines tested against, most of which (if not all) were naive.
The area chair agrees with the authors and the reviewers that the topic of this work is relevant and important. The area chair however shares the concerns of the reviewers about the setup and the empirical evaluation:   Having one model that can be pruned to varying sizes at run time is convenient, but in practice it is likely to be OK to do the pruning at training time. In light of this, the empirical results are not so impressive.   Without quantization, distillation and fused ops, the value of the empirical results seems questionable as these are important and well known techniques that are often used in practice. A more thorough evaluation that includes these techniques would make the paper much stronger.
This paper empirically studies when, why, and which pretrained GANs are useful.   All the reviewers are positive about this work, that they all consider very valuable for practitioners and the community.   First building intuition through toy examples, authors conduct a large scale study of transfer learning in GANs (with the stylegan2). They propose a way to understand the relevance of a pre trained generator and discriminator, as well as heuristics to select good initialization.   Overall, this paper makes a solid contribution that should to be accepted.
The submission proposes a method for quantization.  The approach is reasonably straightforward, and is summarized in Algorithm 1.  It is the analysis which is more interesting, showing the relationship between quantization and adding Gaussian noise (Appendix B)   motivating quantization as regularization.  The submission has a reasonable mix of empirical and theoretical results, motivating a simple to implement algorithm.  All three reviewers recommended acceptance.
This paper with the self explanatory title was well received by the reviewers and, additionally, comes with available code. The paper builds on prior work (Sinkhorn operator) but shows additional, significant amount of work to enable its application and inference in neural networks.  There were no major criticisms by the reviewers, other than obvious directions for improvement which should have been already incorporated in the paper, issues with clarity and a little more experimentation. To some extent, the authors addressed the issues in the revised version.   
This paper presents a new technique for constrained offline RL. The proposed method is based on reducing a nested constrained optimization problem to a single unconstrained optimization problem that can be efficiently represented with a neural network. The proposed algorithm is tested against several baselines on both random grid worlds and continuous environments. Results clearly show that the proposed algorithm outperforms baselines while keep the provided constraints satisfied.   The reviewers agree that the paper is well written, the proposed algorithm is novel and technically sound, and the empirical evaluation clearly supports the claims of the paper. There were some concerns regarding the novelty of this idea, but these concerns were properly addressed by the authors in the discussion.
The work proposes a graph convolutional network based approach to multi agent reinforcement learning. This approach is designed to be able to adaptively capture changing interactions between agents. Initial reviews highlighted several limitations but these were largely addressed by the authors. The resulting paper makes a valuable contribution by proposing a well motivated approach, and by conducting extensive empirical validation and analysis that result in novel insights. I encourage the authors to take on board any remaining reviewer suggestions as they prepare the camera ready version of the paper.
This paper studies the robust reinforcement learning problem in which the constraint on model uncertainty is captured by the Wasserstein distance. The reviewers expressed concerns regarding novelty with respect to prior work, the presentation or the results, and unconvincing experiments. In its current form the paper is not ready for acceptance to ICLR 2020.
Thank you for your submission to ICLR.  The reviewers and I are in agreement that the paper presents a substantial contribution to the field at the intersection of differentiable simulation and ML methods.  In particular, the half inverse method is compelling, non obvious, and hints of a nice path forward towards the goal of practical differentiable simulations within models.  Overall I m happy to recommend the paper be accepted.
This paper provides a high level API for working with Neural Tangent Kernels (NTK) and Fisher Information Matrices (FIM). This is an implementation paper, but such concepts are clearly useful in many tasks. However, such methods are available in many in house code (almost every paper on FIM / NTK uses such methods) I would not say it hampers the progress.    Pros:   The proposed methods are already widely used by the communities in the in house codes, but no single library is available             The library implements state of the art approaches  Cons:   This is an implementation library, and no benchmarking is available. More testing is needed to showcase the library.               Matrix by vector products are provided, but typical operations include many more, like Lanczos method for approximating solutions of linear systems and matrix function by vector product, or randomized SVD to compute low rank approximations. I believe that this should be the part of the library in order to make it a serious competitor for existing "local implementations". Also, such kind of methods would be later or sooner part of big libraries such as PyTorch and Tensorflow.   
Most reviewers believe that the paper is not ready for publication. Among their concerns are:   whether the new experiment with 10 runs are conducted correctly,   the significance of the theoretical part,   correctness of Lemma 2,   generalization claims may not follow from the theoretical results,   comparison with Zhang et al. (2020).  Given these and the lack of support from reviewers, unfortunately I cannot recommend acceptance of this paper at this stage. I encourage the authors to improve their paper according to these concerns.  I copy paste some of the comments that came after Nov. 24th. The authors might want to use them to improve their work.   First, given the assumptions, the theorems/lemmas are not sufficient to be a solid contribution. I think what important is, can the Alg 1 lead to the optimal policy? More specifically, I do not doubt lemma 2; I am concerned about if updating the representations in an online manner (it should be highly nonstationary) can result in the optimal policy. Some two time scale analysis may address this question. As an additional note, since f can be a many to one mapping, policy pi_i is a multimodal distribution. I am unsure if the authors consider this during implementation.  Second, without the two time scale analysis, I would give weak acceptance if the authors can persuade that the superior performances are indeed due to the proposed jointly embedding learning method. That s why I ask for a baseline that directly considers environment model learning as an auxiliary task.  In the abstract, the authors state that "In this work, we propose a new approach for jointly learning embeddings for states and actions ...," in fact, this is not new. Almost any deep RL algorithm can be thought of as the process of learning state and/or action representations. In the response, the authors say, "We do not believe ..., as ... are embedded into the same space." How to do embedding is more like an implementation issue; one can encode them into different spaces and learn them by learning an environment model. It is nothing fancy/novel.  I consider this paper s main novelty to learn the optimal policy in the embedded state and action spaces, and the embeddings are learned by environment model learning. Thus, the authors need to have strong evidence to persuade people: 1) using an environment model to learn the embeddings is really useful; 2) a separate process of learning the policy in the embedded spaces is essential. Such evidence is necessary to make this paper a solid contribution.  Learning an environment model has been used as an auxiliary task in deep RL. Using such a baseline is to validate that it is necessary to learn the policy in the embedded space separately. The authors should also actively design other baselines to substantiate their claims.
The paper presents self training scheme for GANs. The proposed idea is simple but reasonable, and the experimental results show promise for MNIST and CIFAR10. However, the novelty of the proposed method seems relatively small and experimental results lack comparison against other stronger baselines (e.g., state of the art semi supervised methods). Presentation needs to be improved. More comprehensive experiments on other datasets would also strengthen the future version of the paper. 
This paper proposes a model to predict the spatiotemporal dynamics of physical simulations on irregular meshes. The observations are modeled as a sequence of graph representations, each graph corresponding to a snapshot of the observation sequence at time t. This model uses two components, a graph encoder decoder to compress the observations and an autoregressive transformer to model the dynamics. The two components are trained sequentially. At inference time, given an initial hidden state infered from the data and some additional conditional information, a sequence of states is predicted in an auto regressive manner in the hidden space, each state of the sequence is then decoded to produce a prediction in the original observation space. The originality of the model lies in the encoder decoder graph and in the use of a transformer for the prediction. Tests are performed on three fluid dynamics simulation data sets.  All reviewers pointed out some original contributions in the proposed method, in particular the use of transformers in the learned hidden space. In the rebuttal, the authors provided substantial additional results and further details and explanations. Their responses led two reviewers to increase their scores. All reviewers ultimately agree that the paper presents interesting results and conclusions.
The main concern raised by the reviewers is that the paper is difficult to read and potentially unclear. Therefore, the area chair read the paper, and also found it fairly dense and challenging to read. While there may be important discoveries in the paper, the paper in its current form makes it too difficult to read. Since four reviewers (including the AC) struggled to understand the paper, we believe the presentation of the paper should be improved. In particular, the claims of the paper should be better put into context.
Paper develops a dataset and model for learning to refer to 3D objects. Reviewers raised concerns about lack of novelty. Fundamentally, it seems unclear what (if any) the take away for an ML audience would be after reading this paper. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future (perhaps a more applied) venue. 
This paper presents an extension to the MAT model by using relative attention that considers graph level distance, geometric distance, and bond type between nodes during the attention computation.  This is shown to lead to improved performance on several benchmark datasets against MAT and GROVER.  The inclusion of additional information into the attention computation is a sensible and natural choice for transformer with the success of relative positional embedding in the NLP and vision domains. But it is somewhat a straightforward extension of the existing ideas from other domains to MAT, hence the novelty is somewhat limited.  It is also worth noting that the proposed method should be considered in the context of the larger body of research on 3D GNNs. The authors drew inspiration from DimeNet s design in the encoding of geometric distances but do not consider it in the empirical comparisons. Instead, it focused exclusively on transformer based models. This limits the scope of conclusions that we can draw from these experiments and makes it difficult to gauge the practical impact of RMAT in comparison to many other GNN methods that uses 3D geometries of the molecule.
The reviewers equivocally reject the paper, which is mostly experimental and the results of which are limited.  The authors do not react to the reviewers  comments.
This paper uses GAN for data augmentation to improve the performance of knowledge distillation.  Reviewers and AC commonly think the paper suffers from limited novelty and insufficient experimental supports/details.  Hence, I recommend rejection.
This paper presents an interesting and novel idea that is likely to be of interest to the community. The most negative reviewer did not acknowledge the author response. The AC recommends acceptance.
A strong paper reporting improved approaches to meta learning.
While all reviewers agree that the topic is interesting and the work has merit, several issues have been pointed out, especially by R1 and R3, that indicate that the work is not  ready for acceptance at this stage. the authors are strongly encouraged to continue to work on this topic, taking into account the feedback received.
The paper introduces a W2GAN method for training GAN by minimizing 2 Wasserstein distance using  by computing an optimal transport (OT) map between distributions. However, the difference of previous works  is not significant or clearly clarified as pointed out some of the reviewers. The advantage of W2GAN over standard WGAN is also superficially explained, and did not supported by strong empirical evidence. 
The submission initially received mixed reviews. The authors presented convincing answers during the author response period, after which all reviewers recommended weak accepts. The AC has carefully read the reviews, responses, and discussions, and agreed with the reviewers  recommendation. Despite the marginal performance gains, the submission has presented a useful and inspiring way of learning shape representations. The AC, therefore, recommends acceptance.  The authors are encouraged to further revise the paper based on the reviews. In addition, the authors should use $\citep$ for all citations that are not used as a pronoun, including all citations in the tables. Please find more information here: https://journals.aas.org/natbib/
The paper proposes a new multimodal neuro symbolic technique for synthesizing programs. The specification is given in natural language (soft constraints) and input output examples (hard constraints). The multimodal program synthesis is formulated as a constrained maximization problem where the goal is to find a program maximizing the conditional probability w.r.t. the natural language specification while satisfying the input output examples. The proposed technique is evaluated on a multimodal synthesis dataset of regular expressions, and significant performance gains are shown w.r.t. the state of the art synthesis methods. Overall this is an important direction of research, and the paper presents significant results in the space of multimodel program synthesis.  I want to thank the authors for actively engaging with the reviewers during the discussion phase. The reviewers generally appreciated the paper s ideas; however, there was quite a bit of spread in the reviewers  assessment of the paper (scores: 4, 5, 6, 7). In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should more clearly describe the paper s primary contributions, compare their technique with related work that combines neural generation approaches with deductive methods, and simplify the presentation of technical sections. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers  feedback when preparing future revisions of the paper.
Reviewers are unanimous in scoring this paper below threshold for acceptance.  The authors did not submit any rebuttals of the reviews.  Pros: Paper is generally clear. Hardware results are valuable.  Cons: Limited simulation results. Proposed method is not really novel. Insufficient empirical validation of the approach.
The paper analyzes the interesting problem of image denoising with neural networks by imposing simplifying assumptions on the Gaussianity and independence of the prior.  A bound is established from the analysis of (Hand & Voroninksi, 2018) that can be algorithmically achieved through a small tweak to gradient descent.    Unfortunately, the contribution of this paper is incremental given the recent works of (Hand & Voroninksi, 2018) and (Bora et al., 2017); an opinion the reviewers unanimously shared.  Reviewer opinion differed on whether they found the overall contribution to be barely acceptable or simply insufficient.  No reviewer detected a major advance, and there seems to be a question of whether the achievement is significant given the strength of the assumptions required to achieve the modest additions.  After scrutiny, the main theoretical contributions of the paper appear to be a bit overstated.  For example, the bound in Theorem 1 is quite weak: it does not establish convergence to a global minimizer (even under the strong assumptions given), but only that Algorithm 1 eventually remains in a neighborhood of the global minimizer.  It is true that this neighborhood can be made arbitrarily small by increasing the strength of the assumptions made on epsilon and omega, but epsilon remains a constant with respect to iteration count.  The subsequent claim that the algorithm achieves a denoising rate of sigma^2 k/n is not an accurate interpretation of Theorem 1, given that this claim would require require (at the very least) that epsilon can be made arbitrarily small, which it cannot be.  More precision is required in stating supportable conclusions from the given results.  The algorithmic motivation itself is rather weak, in the sense that this paper only provides an anecdotal demonstration that there are no spurious critical points beyond the negation of the global minimizer the theoretical support for this claim already resides in (Hand & Voroninski, 2018).  The provenance of such a central observation was not made sufficiently clear in the paper nor in the discussion.  An additional quibble about the experimental evaluation is that it does not compare to plain gradient descent (or other baseline optimization techniques), which the authors observe almost always works in the scenario considered.  It seems that the "negation tweak" embedded in Algorithm 1 has no real impact on the experimental results, raising the question of whether the contributions do indeed have any practical import.  The descriptions offered in the current paper suggest that a serious algorithmic advantage has yet to be demonstrated in any real experiment.  The paper requires a far better evaluation of Algorithm 1 in comparison to standard baseline optimizers, to support the case that the proposed algorithmic tweak has practical significance.  This paper remained in a weak borderline position after the review and discussion period.  In the end, this was a very difficult decision to make, but I think the paper would benefit from further strengthening before it can constitute a solid publication.
This paper received 5 quality reviews, with 3 of them rated 8, 1 rated 6, and 1 rated 5. In general, while there are minor concerns, the reviewers acknowledge the contribution of applying Knowledge distillation to the problem of monocular 3D object detection, and appreciate the SOTA performance on the KITTI validation and test sets. The AC concurs with these important contributions and recommends acceptance.
This paper proposes a certified defense under the more general threat model beyond additive perturbation. The proposed defense method is based on adding noise to the classifier s outputs to limit the attacker s knowledge about the parameters, which is similar to differential privacy mechanism. The authors proved the query complexity for any attacker to generate a successful adversarial attack. The main objection of this work is (1) the assumption of the attacker and the definition of the query complexity (to recover the optimal classifier rather than generating an adversarial example successfully) is uncommon, (2) the claim is misleading, and (3) the experimental evaluation is not sufficient (only two attacks are evaluated). The authors only provided a brief response to address the reviewers’ comments/questions without submitting a revision. Unfortunately none of the reviewer is in support of this paper even after author response. 
This paper makes a connection between one class neural networks and the unsupervised approximation of the binary classifier risk under the hinge loss. An important contribution of the paper is the algorithm to train a binary classifier without supervision by using class prior and the hypothesis that class conditional classifier scores have normal distribution. The technical contribution of the paper is novel and brings an increased understanding into one class neural networks. The equations and the modeling present in the paper are sound and the paper is well written.  However, in its current form, as pointed out by the reviewers, the experimental section is rather weak and can be substantially improved by adding extra experiments as suggested by reviewers #1, #2. Since its submission the paper has not yet been updated to incorporate these comments. Thus, for now, I recommend rejection of this paper, however on improvements I m sure it can be a good contribution in other conferences.
Reviewers raised several concerns about the paper guided by unfounded heuristics as well as the artificiality of the tasks involved.  Rebuttal only answered a few of them and did not convince the reviewers which has been clearly stated in the response. We hope that the authors will improve the paper for future submission based on the reviews. 
The paper received mixed reviews: WR (R1,R3) and WA (R2). AC has carefully read reviews and rebuttal and examined the paper. Unfortunately, the AC sides with R1 & R3, who are more experienced in this field than R2, and feels that paper does not quite meet the acceptance threshold. The authors should incorporate the comments of the reviewers and resubmit to another venue. 
The reviewers recommend rejection due to various concerns about novelty and experimental validation. The authors have not provided a response.
This is an interesting paper, but was quite difficult to follow. As they stand, the empirical results are not altogether convincing nor warrant acceptance.
The paper focuses on attribute object pairs image recognition, leveraging some novel "attractor network".  At this stage, all reviewers agree the paper needs a lot of improvements in the writing. There are also concerns regarding (i) novelty: the proposed approach being two encoder decoder networks; (ii) lack of motivation for such architecture (iii) possible flow in the approach (are the authors using test labels?) and (iv) weak experiments.
It appears that this paper can benefit from additional detail and work before it becomes a stronger publication that is more convincing. The authors have done an impressive job responding to the reviewers and updating their paper, and multiple reviewers raised their score consequently. However, while multiple reviewers now recommend acceptance, there is no agreement on it. Even among the reviewers who recommended acceptance, there is a feeling on being on the fence specifically about the ability of the paper to make a convincing argument without considering a real life scenario and while only using toy settings. Indeed, this is a problematic aspect of the paper because the value of the paper lies in making that argument. Further, the paper would gain further from clarifying the writing further and connecting the paper more directly with the neuroscientific literature it aims to be connected to.
The paper extends posterior sampling to the multi agent RL setting, and develops a novel algorithm with convergence guarantees to a Nash Equilibrium strategy in two player zero sum games. Reviewers raised several questions, many of which were well addressed by the authors and which helped further clarify the approach and contribution of the paper. The paper is timely in that novel connections between Game Theory and RL are being explored in fruitful ways, and the paper provides valuable new insights and directions for future research.
The paper received three good quality reviews which were in agreement that the paper was below the acceptance threshold. The authors are encouraged to follow the suggestions from the reviews to revise the paper and resubmit to another venue.
This paper proposes three modifications of BERT type models two of which is concerned with parameter sharing and one with a new auxiliary loss. New SOTA on downstream tasks are demonstrated.   All reviewers liked the paper and so did a lot of comments.   Acceptance is recommended.
This paper proposes an analysis of regularization for policy optimization. While the multiple effects of regularization are well known in the statistics and optimization community, it is less the case in the RL community. This makes the novelty of the paper difficult to judge as it depends on the familiarity of RL researchers with the two aforementioned communities.  Besides the novelty aspect, which is debatable, reviewers had doubts on the significance of the results, and in particular on the metrics chosen (based on the rank). While defining a "best" algorithm is notoriously difficult, and could be considered outside of the scope of this paper, the fact is that the conclusions reached are still sensitive to that difficulty.  I thus regret to reject this paper as I feel not much more work is necessary to provide a compelling story. I encourage the authors to extend their choice of metrics to be more convincing in their conclusions.
This work introduces a neural architecture and corresponding method for simplifying symbolic equations, which can be trained without requiring human input. This is an area somewhat outside most of our expertise, but the general consensus is that the paper is interesting and is an advance. The reviewer s concerns have been mostly resolved by the rebuttal, so I am recommending an accept. 
This paper proposes to apply a piece wise polynomial filter on the spectral corresponding to the graph convolution to enhance the model expressivity of graph neural networks. The effectiveness of the proposed model is investigated through numerical experiments and it was shown that the method achieves fairly nice performances.    This paper gives a natural extension to the usual adaptive Generalized PageRank approaches to more expressive piece wise polynomial filters. However, the reviewers are not enthusiastic on this paper. This is mainly because of the following concerns: (1) Since it requires diagonalization of the aggregation operator, it requires much more computational burden than the usual polynomial filters, which prevents the method from being applied to data with much more large size. (2) The choice of the filter could be more investigated, in particular, the complexity expressivity trade off (in other words, bias variance trade off) could be discussed more, for example, by theoretical work.    In summary, the paper seems not to be well matured for being published in ICLR conference.
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero shot learning and distillation. The authors provide an extensive theoretical and empirical analysis, showing performance improvements in zero shot learning and finite sample error upper bounds. The reviewers generally agree this is a good paper that should be published.
This paper introduces a planning phase for NMT.  It first generates a discrete set of tags at decoding time, and then the actual words are generated conditioned on those tags.  The idea in the paper is interesting.  However, the paper s experimental settings could improve by comparing on larger datasets and also using stronger baselines.  The writing could also improve   why were only the few coarse POS tags used?  Have the authors tried a larger set?  I think without such controlled comparisons, it would be hard to understand why only those coarse tags are used.  The reviewers express concern about some of the above issues and there is consensus that the paper should be improved for acceptance at a venue like ICLR.
This paper develops a new method, named Augmented Intermediate Level Attack (Aug ILA), to improve the transferability of black box attacks. Specifically, the proposed Aug ILA contains three key modules: image transformations, reverse adversarial update, and attack interpolation.   Overall, the reviewers think it is an interesting paper, but are concerned that the original ablations are not enough to support the effectiveness of the proposed method, including missing strong baseline attacks and defense methods, and only one dataset is considered. During the discussion period, the authors actively provide new results. However, the Reviewer TcRw and the Reviewer 2dLg are not fully convinced by the rebuttal, especially regarding 1) in these additional experiments, no comparison is provided with other SOTA attacks beyond ILA based approaches; 2) Table 11 shows the proposed method even degrades (rather than improves) the performance of VNI CT FGSM on defense models;  3) the attack rate of the proposed method is sensitive to the selection of layers, therefore, need to be carefully tuned in experiments (which could lead to unfair comparisons to other attacks). These concerns are indeed legitimate, and should be addressed carefully before publication.  I encourage the authors to incorporate all the reviewers  comments and make a stronger submission next time.
All reviewers agreed on the major shortcomings of this submission, the most important of which is that the contributions are insufficiently evaluated. There was no author response. 
In my opinion, this is a cool idea, but could use a few more test settings to evaluate the general applicability of their method. It would be interesting to see if the method generalizes to a non reference based task.  Strengths: Novel method that explores the interaction of color masks for learning to prompt about regions in images by identifying the color region they correspond to Paper contains extensive ablation studies & discussions  Weaknesses: Experimental results are run on uncommon benchmarks, making it difficult to compare to SOTA V+L methods Consequently, it’s not clear that this method would generalize beyond visual grounding to tasks such as VQA or captioning
The paper proposes to predict sets using conditional density estimates. The conditional densities of the reponse set given the observed features is modeled through an energy based function. The energy function can be specified using tailored neural nets like deep sets and is trained trough approximate negative log likelihoods using sampling.   The paper was nice to read and was liked by all the reviewers. The one thing that stood out to me was the emphasis on multi modality. (multi appears 51 times).  This could be toned down because little is said about the quality relative to the true p(Y | x) and the focus is mainly on the lack of this in existing work.
This paper proposes a new metric to evaluate the robustness of neural networks to adversarial attacks. This metric comes with theoretical guarantees and can be efficiently computed on large scale neural networks.  Reviewers were generally positive about the strengths of the paper, especially after major revisions during the rebuttal process. The AC believes this paper will contribute to the growing body of literature in robust training of neural networks.  
The paper presents a scalable data poisoning algorithm for targeted attacks, using the idea of designing poisoning patterns which "align" the gradients of the real objective and the adversarial objective. This intuition is supported by theoretical results, and the paper presents convincing experimental results about the effectiveness of the model.  The reviewers overall liked the paper. However, they requested a number of clarifications and some additional work, which should be incorporated in the final version (however, the authors are not required to use the wording as poison integrity/ poison availability). In particular, it would be great to see the experiment the authors suggested in their response to Reviewer 2 about the effectiveness of their method for multiple targets (this is important to better understand the limitations of the proposed approach).
The reviewers and AC all agree that the paper considers an important problem but that several concerns remain which makes the present submission of limited novelty.  We strongly encourage the authors to revise their manuscript to incorporate the reviewers comments as this will significantly strengthen the significance of their work.  In particular it will be important to strengthen the theoretical analysis and expand the empirical evaluation, including incorporating an ablation study and considering settings of various difficulty, noise level, etc.
In this work, the authors explore using genetic programming to search over network architectures. The reviewers noted that the proposed approach is simple and fast. However, the reviewers expressed concerns about the experimental validation (e.g., experiments were conducted on small tasks; issues with comparisons (cf. feedback from Reviewer2)), and the fact that the method were not compared against various baseline methods related to architecture search. 
The paper presents a novel variance reduction algorithm for SGD. The presentation is clear. But the theory is not good enough. The reivewers worry about the converge results and the technical part is not sound.
This paper proposes a categorization of out of distribution examples by texture and semantics, and proposed a model that extracts the texture and semantic information separately before combining them via a normalizing flow based method to obtain good results. While the categorization provides some interesting perspectives, most reviewers found the assumptions too strong, and there are some issues with the derivation. Reviewers have some positive feedbacks on the proposed algorithm for OOD, but also expressed concerns about the fair comparison with more recent baselines. The paper, in its current form, is not ready for the publication, but the authors are encouraged to improve the paper with reviewers  suggestions and resubmit.
The paper proposes to combine three methods of quantization and apply them to neural network compression.  The methods are known in the literature.  There is a lack of theoretical contribution, and experimental results show variable speedups that may not be competitive with the current state of the art in neural network compression.  The majority of reviewers recommend that this paper be rejected.  The authors have not provided a response.
This paper proposes a benchmark suite of offline model based optimization problems. This benchmark includes diverse and realistic tasks derived from real world problems in biology, material science, and robotics contains a wide variety of domains, and it covers both continuous and discrete, low and high dimensional design spaces. The authors provide a comprehensive evaluation of existing methods under identical assumptions and get several interesting takeaways from the results. They found there exists surprising efficacy of simple baselines such as naive gradient ascent, which suggests the need for careful tuning and standardization of methods in this area, and provides a test bed for algorithms that try to solve this challenge. However, most reviewers agreed that a more in depth analysis and insightful explorations for the RL experiment results will help readers understand why their method has superiority even without trajectory data, and  that the paper needs another revision before being accepted. Therefore, I recommend rejection although all reviewers agreed that the tasks is very interesting and a good start.
The reviewers generally feel that the phenomenon discovered in this paper is relevant and could be very important when considering interpretability. However, there are still a number of remaining concerns. The reviewers are not convinced by the human study   they feel there is structure in the SIS’s such that a human trained on these images with an abstract category (i.e., without being told their real world label) could potentially successfully learn to classify them. There is also a concern that SIS is model based, that is, the inductive biases of the model (shape, color, etc.) could be leaking information into the SIS image. Finally, there should be some stronger evidence that this represents a serious practical problem for the community. Are there instances where current interpretable approaches break down because of this phenomenon?  One suggestion to potentially strengthen the human experiment: you could try training a denoising autoencoder on the full images, removing 95% of the pixels at random. Then, given an SIS, use the denoising autoencoder to reconstruct the image and then provide that to a human subject. The question is: how much information about the image as a whole is preserved in the SIS (when combined with an appropriate inductive bias)? 
The paper s initial evaluation was below par, but the author feedback helped clarify several crucial points after which two of the reviewers increased their scores by a point, bringing the current evaluation to borderline.   The paper addresses a relevant and challenging problem in the RL domain. However, in my opinion, from the reviewers  and authors  remarks and from my own reading of the paper, there are concerns that need to be addressed before the paper can be publication worthy. Primary among these is the quantum of novelty   as many reviews point out, the key idea of viewing an episodic trajectory as a multivariate (vector) sample for running hypothesis tests is not novel in itself, as is the claim that new tests have been devised. Another crucial issue is the (parametric) assumption of normality for the episodic reward sequence which is not adequately justified in the paper   even a two time step trajectory with normal rewards per state transition can exhibit a mixture of Gaussians type reward distribution for the second state, breaking the assumption. As it transpired from the reviews of Reviewer4, reducing environment shift/degradation to just a mean change problem, without even considering a change in the variances (2nd order statistics), seems to be too stylized to be effective. There are other, nonparametric approaches in statistics based on testing for changes in the distribution function (kernel density estimation approaches, for instance), which could perhaps be applied without normality assumptions and yield favourable results. The experimental results for detection delay often show significant overlaps of the delay distributions for different procedures (e.g., Hotelling vs. Mean vs. UDT etc.), which does not indicate an advantage of the proposed method.   I would urge the author(s) to assimilate the feedback and delve deeper as to why and how parametric procedures based on normality assumptions may or may not succeed, so as to significantly strengthen the theoretical and practical message of this work.  
This paper received 4 quality reviews, with the final rating of 8 by 2 reviewers, and 6 by the other 2 reviewers. All reviews recognize the contributions of this work, especially its superior performance. The AC concurs with these contributions and recommends acceptance.
meta score: 4  The paper uses a deep autoencoder to rating prediction, with experiments on netflix.  Pros    Proposed dense refeeding approach appears novel    Good experimental results  Cons    limited experimentation    main novelty (dense refeeding) is not well linked to existing data imputation approaches    novel contribution is otherwise quite limited  
After reading the author s response, all the reviewers agree that this paper is an incremental work. The presentation need to be polished before publish.
This paper introduces MELEE, a meta learning procedure for contextual bandits. In particular, MELEE learns how to explore by training on datasets with full information about what every reward each action would obtain (e.g., using classification datasets). The idea is strongly related to imitation learning, and a regret bound is demonstrated for the procedure that comes from that literature. Experiments are performed.   Perhaps due to the generality in which the algorithm was presented, reviewers found some parts of the work unintuitive and difficult to follow. The work may greatly benefit from having an explicit running example for F and pi and how it evolves during training. Some reviewers were not impressed by the experimental results relative to epsilon greedy. Yes, epsilon greedy is a strong baseline, but MELEE introduces significant technical debt and data infrastructure so it seems fair to expect a sizable bump over epsilon greedy or else why is it worth it?  Perhaps with revisions and experiments within a domain that justify its complexity, this paper may be suitable at another venue. But it is not deemed acceptable at this time, Reject.  
This submission received reviews with a very wide range of scores (initially 3,5,5,9; then 5,5,5,9). In the discussion, all reviewers maintained their general position (although a private message by the reviewer giving a score of 9 said he/she would consider going down to an 8).  Because of the high variance, I read the paper in detail myself. I agree with all reviewers that NAS is a very important field of study, that the experiments are interesting, and that purely empirical papers studying what works and what doesn t work (rather than introducing a new method) are definitely needed in the NAS community. But overall, for this particular paper, I agree with the 3 rejecting reviewers. The paper presents a lot of experiments, but I am missing novel deep insights or lasting overarching take aways. The papers reads a bit like a log book of all the experiments the authors did, before having gone through the next iteration in the process to consolidate findings and gain lasting insight.  In a bit more detail, half the results in Section 4 use medium sized super networks, which seem broken to me, yielding much worse performance than small super networks. I did not find any motivation for studying these medium sized networks, no reason given for them to perform poorly, and none stating why the results are still interesting when the networks perform so poorly (apologies if I overlooked these). The poor performance may be due to using a training pipeline that works poorly for these larger networks, but this is hard to know exactly without further experiments. I would either try to fix these networks  performance or drop them from the paper entirely, as I do not see any insights that can be reliable gained from the current results. As is, I believe these results (accounting for half the plots in the paper) only muddy the water and are preventing a crisp presentation of insightful results.  Another factor that I find unfortunate about the paper is that it only uses NAS Bench 201 for its empirical study, and even for that dataset, mostly only the CIFAR 10 part. After getting rid of isomorphic graphs from the original 15625 architectures, NAS Bench 201 only has 6466 unique architectures (see Appendix A of NAS Bench 201), while, e.g., NAS Bench 101 has 423k unique architectures. As the authors indicate themselves in their section "Grains of Salt", it is unclear whether insights gained on the very small NAS Bench 201 space generalize to larger spaces. I therefore believe that there should also be some experiments on another, larger space, to study how well some of the findings generalize. An additional benchmark that the authors could have directly used without performing additional experiments themselves is the NAS benchmark NAS Bench 1shot1 (ICLR 2020: https://openreview.net/forum?id SJx9ngStPH), which studies 3 different subsets of NAS Bench 101, and which was created to allow one shot methods to use the larger space of evaluated architectures in NAS Bench 101.   Minor comments:   It reads as if the authors performed 5 runs, computed averages of the outcomes, and then computed correlation coefficients. That would be a suboptimal experimental setup, though; in practical applications, only one run of the super network would be run, and therefore, in order to assess performance reliably, one should compute correlation coefficients for one run at a time, and then obtain a measurement of reliability of these correlation coefficients across the 5 runs.   The y axis in Figure 2 appears to be broken: for example, in the left column it goes from 99.978 to 99.994, and the caption says these should be accuracy predictions of NAS Bench201. However, even the best architectures in NAS Bench201 only achieve around 95% accuracy.   Overall, I recommend rejection for the current version of the paper. Going forward, I encourage the authors to continue this line of work and recommend that they iterate over their experiments and extract crisp insights from their experiments. I also recommend performing experiments with a much larger search space than that of NAS Bench 201 to assess whether the findings generalize.
The paper proposes Markov coding game (MCG), which generalizes both source coding and a large class of referential games. All the reviews are negative. The reviewers think the work is not ready for publication in its current form.
In this paper, the authors draw upon online convex optimization in order to derive a different interpretation of Adam Type algorithms, allowing them to identify the functionality of each part of Adam. Based on these  observations, the authors derive a new Adam Type algorithm,  AdamAL and test it in 2 computer vision datasets using 3 CNN architectures. The main concern shared by all reviewers is the lack of novelty but also rigor both on the experimental and theoretical justification provided by the authors. After having read carefully the reviews and main points of the paper, I will side with the reviewers, thus not recommending acceptance of this paper. 
Reviews are marginal. I concur with the two less favorable reviews that the metrics  for privacy protection are not sufficiently strong for preserving privacy. 
The paper considers learning settings with distributional change. It makes a lot of assumptions to obtain sample complexities that justify the use of empirical invariant risk minimization, and falls a bit short by not giving a formal converse for the inadequacy of plan empirical risk minimization, despite making the claim. Nevertheless, the contributions are insightful, and the paper may be worth sharing with the community. The grading were overall positive from the reviewers, though particularly critical, and I doubt the whole paper could be fully double checked: one could question the ability of the reviewers to perform a deep analysis on a 48 pages theoretical paper in the time constraints imposed by a conference model... 
The paper studies interpretability in multi instance learning (where model is trained with a label provided for a bag of instances). The author proposes model agnostic weight sampling strategy to improve sampling in prior methods such as (SHAP), and evaluate their performance on three datasets (and authors provided results on more datasets during rebuttal).   All reviewers agree the paper is well written and well motivated. The paper presents a simple but meaningful extensions to existing interpretability study and will be helpful for the community. Reviewers had some concerns with the comprehensiveness of the evaluation, the strength of their proposed results, and the originality/novelty of the paper. The authors have provided further experimental results on new datasets as well as additional baselines. Given the study of MIL setting in interpretability is scarce, I am leaning towards the acceptance.
The paper proposed a new seq2seq method to implement natural language to formal language translation.  Fixed length Tensor Product Representations are used as the intermediate representation between encoder and decoder.  Experiments are conducted on MathQA and AlgoList datasets and show the effectiveness of the methods.  Intensive discussions happened between the authors and reviewers.  Despite of the various concerns raised by the reviewers, a main problem pointed by both reviewer#3 and reviewer#4 is that there is a gap between the  theory and the implementation in this paper.  The other reviewer (#2) likes the paper but is less confident and tend to agree with the other two reviewers.
The paper adds a new level of complexity to neural networks, by modulating activation functions of a layer as a function of the previous layer activations.  The method is evaluated on relatively simple vision and language tasks.  The idea is nice, but seems to be a special case of previously published work; and the results are not convincing.  Four of five reviewers agree that the work would benefit from: improving comparisons with existing approaches, but also improving its theoretical framework, in light of competing approaches.
After the author response multiple reviewers remained concerned over the degree to which the current manuscript makes the case for the proposed hyper network approach to text to image generation. It was felt that this was mainly an empirical paper for which the reviewers remain unconvinced that the proposed hyper network based modulation was better than simple channel wise StyleGAN2 style modulation. While the authors have shown that their approach beats a StyleGAN2 baseline with sentence conditioning on CLIP R the reviewers felt that the comparisons with StyleGAN2 baseline needed fairer word conditioning. Only one reviewer recommended accepting this paper.  The AC recommends rejection.
The paper uses neural networks for system identification.  The novelty of its contributions seems to be marginal, and the demonstration of its usefulness is not experimentally validated well enough.
This paper proposes a novel strategy for deep active learning based on the training dynamics of the underlying deep model, defined as the derivative of the loss of the ultra wide NTK. All reviewers enjoy the clean story and motivation of the proposed acquisition/objective function and appreciate the authors’ effort in providing theoretical justification and analysis.   One note is that   as Reviewer 8dDv highlighted   part of the analysis pertaining to the incompatibility between the generalization bound of NTK and the non iid nature of active learning rely on numerical evidence: the MMD under the covariate shift setting (i.e. assuming that the conditional distributions P(Y |X) remains consistent) is shown empirically to be smaller than the dominant term of the generalization bound. This serves as a reasonable empirical motivation/ justification of the dynamicAL heuristic under the AL setting, but I would suggest the authors be more precise in the abstract / intro (e.g. abstract) that this is an empirical result.   While the theoretical results are interesting, not all reviewers are convinced that the experimental results are sufficiently compelling. In particular, Reviewer YgGb points out that the significant performance boost reported in the main paper was mainly due to the non retraining (i.e. not retraining the model (from scratch)) constraints imposed by the problem setup. Reviewer p3z9 shares the same concern that such a setting would be far from realistic at least for the data sets/labels considered in the experiments. The authors refer to Ostapuk et al, 2018 as a justification of the non retraining setting; yet they assumed a high budget, e.g., up to 50% of all labels of datasets.   In summary, this is a theoretically well motived work, but the empirical components need to be further clarified and supported with more realistic experiments to merit acceptance for the proposed solution.
This work proposes to use policy gradient RL to learn to read and write actions over memory locations using as reward the entropy reduction of memory location distribution. The authors perform experiments on NER in Stanford Dialogue task, that are framed though as few shot learning. The reviewers have pointed out shortcomings of the paper with regards to its novelty, narrow contribution in combination thin experimental setup (the authors only look into one dataset and one task with minimal comparison to previous work and no ablation studies as to understand the behaviour of the model) and clarity (method description seems to be lacking some crucial components of the model). As such, I cannot recommend acceptance but I hope the authors will use the reviewers comments to transform this into a strong submission for a later conference.
The authors obtain nice speed improvements by learning to skip and jump over input words when processing text with an LSTM. At some points the reviewers considered the work incremental since similar ideas have already been explored, but at the end two of the reviewers ended up endorsing the paper with strong support.
This paper proposes an abstractive text summarization model that takes advantage of lead bias for pretraining on unlabeled corpora and a combination of reconstruction and theme modeling loss for finetuning. Experiments on NYT, CNN/DM, and Gigaword datasets demonstrate the benefit of the proposed approach.   I think this is an interesting paper and the results are reasonably convincing. My only concern is regarding a parallel submission that contains a significant overlap in terms contributions, as originally pointed out by R2 (https://openreview.net/forum?id ryxAY34YwB). All of us had an internal discussion regarding this submission and agree that if the lead bias is considered a contribution of another paper this paper is not strong enough.   Due to space constraint and the above concern, along with the issue that the two submissions contain a significant overlap in terms of authors as well, I recommend to reject this paper.
The paper combines PAC Bayes bound with network compression to derive a generalization bound for large scale neural nets such as ImageNet. The approach is novel and interesting and  the paper is well written. The authors provided detailed replies and improvements in response to reviewers questions, and all reviewers agree this is a very nice contribution.
This paper investigates safe reinforcement learning with distinct reward function and safety function. The authors present theoretical analysis and simulation results. The representation of safety is a critical step. The authors define the safety function values based on various events and use linear combination of them to construct safety score. Theoretical guarantees on safety and efficiency are presented. Simulation results also show safety and efficiency of the method.   This was a tricky case as the paper is borderline. Based on reviewers comments, we decided that the paper is not ready for publication in its current form and would benefit from another round revisions.  
Paper is withdrawn by authors.
The focus of the paper is kernel thinning, i.e. the extraction of a core set from a sample with good integration properties meant in MMD (maximum mean discrepancy, hence worst case) sense. Particularly, the authors propose generalizations of the kernel thinning method (Dwivedi and Mackey, 2021) which relax the assumptions imposed on the kernel (k) and the target distribution (P), and possess tighter performance guarantees.  Designing compressed representation of samples for integration is a fundamental problem in machine learning and statistics with a large number of successful applications. As assessed by the reviewers, the authors deliver important new theoretical insights in the area which can be also of clear practical interest. They also pointed out that the self containedness of the paper could be improved and additional intuition would help the dissemination of the results among the members of the ICLR/ML audience.
This work presents some of the first results on unsupervised neural machine translation. The group of reviewers is highly knowledgeable in machine translation, and they were generally very impressed by the results and the think it warrants a whole new area of research noting "the fact that this is possible at all is remarkable.". There were some concerns with the clarity of the details presented and how it might be reproduced, but it seems like much of this was cleared up in the discussion. The reviewers generally praise the thoroughness of the method, the experimental clarity, and use of ablations. One reviewer was less impressed, and felt more comparison should be done.
The authors proposed a simple and effective approach to parallel training based on stochastic weight averaging. Moreover, the authors have carefully addressed the reviewer comments in the discussion period, particularly the relation to local SGD, to the satisfaction of reviewers. Local SGD mimics sequential SGD with noise induced by lack of synchronization, whereas SWAP averages multiple samples from a stationary distribution, and synchronizes at the end. Please clarify these points and carefully account for reviewer comments in the final version. Overall, the proposed approach will make an excellent addition to the program, both elegant and practically useful.
This paper provides theoretical guarantees for adversarial training.  While the reviews raise a variety of criticisms (e.g., the results are under a variety of assumptions), overall the paper constitutes valuable progress on an emerging problem.
The authors address the important issue of exploration in reinforcement learning. In this case, they propose to use reward shaping to encourage joint actions whose outcomes deviate from the sequential counterpart. Although the proposed intrinsic reward is targeted at a particular family of two agent robotic tasks, one can imagine generalizing some of the ideas here to other multi agent learning tasks.  The reviewers agree that the paper is of interest to the ICLR audience.
This paper shows that transformer models can be used to learn certain advanced mathematical concepts such as the local stability of differential equations. Reviewers found this surprising and useful for engineers, and the evaluation was adequate. They also felt that it opens the doors to similar studies on other aspects of mathematics.
main summary:  method for quantizing GAN  discussion: reviewer 1: well written paper, but reviewer questions novelty reviewer 2: well written, but some details are missing in the paper as well as comparisons to related work reviewer 3: well written and interesting topic, related work section and clarity of results could be improved recommendation: all reviewers agree paper could be improved by better comparison to related work and better clarity of presentation. Marking paper as reject.
This paper investigates a non attentive architecture of Tacotron 2 for TTS where the attention mechanism is replaced by a duration predictor.  The authors show that this change can significantly improve the robustness. In addition, the authors propose two evaluation metrics for TTS robustness, namely, unaligned duration ratio (UDR) and word deletion rate (WDR), which appear to be novel to the TTS community. The proposed non attentive architecture yields good MOS scores in the experiments.    Overall, the paper is well written but the reviewers commented on the technical novelty of the work as it is essentially an improvement within the Tacotron 2 framework. There is also a lack of comparative study with other existing frameworks with similar techniques.  Although the authors put together a detailed rebuttal to address the comments, in the end the above two major concerns remain. 
The paper proposes a method for segmentation of thin structures in 2D and 3D, based on persistent homology and using a new filtration. The method performs similarly to state of the art methods on 2D datasets and outperforms some baselines in 3D.  After considering the authors  response and discussing, the reviewers have not arrived at a consensus.   Pros include:   Simple and reasonable approach   Fairly strong experimental results  Some cons are:   Missing theoretical contributions   Experimental results on 2D datasets are not that strong, while on 3D datasets important baselines are missing   At times unclear/unconventional presentation   Overall, at this point I recommend rejection. The paper is promising, but since the main claim is good performance on 3D data, it is important to have a thorough empirical evaluation there, with the relevant baselines (as mentioned by the reviewers). I very much encourage the authors to polish the paper and submit to a different venue.
This paper regularizes deep neural networks via the Hessian trace.  The algorithm is based on Hutchison’s method, further accelerated via dropout.  Connection to the linear stability of dynamical system is discussed.  The proposed regularization shows favorably in the experimental results.  The idea of the method is clear.  The paper’s writing needs a lot of improvement because there are a number of grammatical errors.  The major technical concerns include: a) the experimental results are still not convincing; b) the explanation of favoring instability in the dynamical system that resorts to overfitting prevention (reviewer GDik).  I’ve read the rebuttal, but remain unconvinced.
There is a very nice discussion with one of the reviewers on the experiments, that I think would need to be battened down in an ideal setting. I m also a bit surprised at the lack of discussion or comparison to two seemingly highly related papers:  1. T. G. Dietterich and G. Bakiri (1995) Solving Multiclass via Error Correcting Output Codes. 2. Hsu, Kakade, Langford and Zhang (2009) Multi Label Prediction via Compressed Sensing. 
This paper proposes a data augmentation method based on Generative Adversarial Networks by training several GANs on subsets of the data which are then used to synthesise new training examples in proportion to their estimated quality as measured by the Inception Score. The reviewers have raised several critical issues with the work, including motivation (it can be harder to train a generative model than a discriminative one), novelty, complexity of the proposed method, and lack of comparison to existing methods. Perhaps the most important one is the inadequate empirical evaluation. The authors didn’t address any of the raised concerns in the rebuttal. I will hence recommend the rejection of this paper.
The authors introduce a framework for continual learning in neural networks based on sparse Gaussian process methods. The reviewers had a number of questions and concerns, that were adequately addressed during the discussion phase. This is an interesting addition to the continual learning literature. Please be sure to update the paper based on the discussion.
This paper presents the method of using Fisher information matrix values to identify examples near the decision boundary for a model, and proposes to preferentially use these examples in evaluation.   Pros:   Reviewers found this use of FIM values to be novel and interesting.   The paper presents fairly extensive results demonstrating the properties of examples selected and perturbed under the proposed methods.   Cons:    One of the main aims of this paper is to promote the use of examples selected in this way as evaluation sets. The method relies on values estimated from a specific model to select difficult examples, so this raises a fairly serious objection: If our goal in evaluation is to produce a fair comparison of two models, how do we choose which model we should use to select the test examples?  Reviewers were fairly unanimous in the objection that they raised, and while there was substantial discussion both with the authors and among reviewers, no reviewer was satisfied that the authors took this concern seriously or offered a clear resolution.
This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.
This paper proposes a risk sensitive actor critic reinforcement learning (RL) method that optimizes the policy with respect to a dynamic (iterated) expectile risk measure.  The expectile risk measure has the elicitability property and can be expressed as the minimizer of an expected scoring function, which is exploited in critic update.  The proposed approach is applied to option pricing and hedging.  A main point of discussion was the applicability and effectiveness of the proposed method beyond particular financial problems.  The original submission was indeed specialized to particular financial tasks.  The authors have rewritten the paper in a way that it claims to propose a risk sensitive RL method for the general MDP with finite horizon.  This however leaves the question regarding the advantages of the proposed approach over existing methods of risk sensitive RL, including those that work with non coherent (dynamic) risk measures (since coherence is often not needed in domains outside finance).
This paper studies the problem of heterogeneous domain transfer, for example across different data modalities.  The comments of the reviewers are overlapping to a great extent. On the  one hand, the reviewers and AC agree that the problem considered is very interesting and deserves more attention.  On the other hand, the reviewers have raised concerns about the amount of novelty contained in this manuscript, as well as convincingness of results. The AC understands the authors’ argument that a simple method can be a feature and not a flaw, however this work still does not feel complete. Even within a relatively simple framework, it would be desirable to examine the problem from multiple angles and "disentangle" the effects of the different hypotheses – for example the reviewers have drawn attention to end to end training and comparison with other baselines. The points raised above, together with improving the manuscript (as commented by reviewers) would make this work more complete.
In this paper, the authors present an investigation of the impact of demographics on the peer review outcomes of ICLR. This is an important topic, as the demographics of ICLR and similar conferences are seriously skewed and may cause some people to feel excluded. The authors look into this complex problem with extensive manual annotations and analyses.   The main weakness of this paper is that it is observational, and while the results are interesting, it is difficult to take away a clear and convincing message for the future. Part of the reason is that the whole problem is quite complex, and the hypotheses that are presented and tested in this paper reveal relatively shallow findings. Compared to the NeurIPS experiments which are carefully designed, these are not causal (see one of the reviewers  comments), so it is difficult to draw conclusions beyond correlations.  In summary, the results are interesting, and despite some of the reviewers  concerns, I would not exclude this paper because of the topic being irrelevant to the cfp, but I think the paper needs a more clear and convincing message.
 This paper analyzes several neighbor embedding methods  t SNE, UMAP, and ForceAtlas2  by considering their objectives as consisting of attractive and repulsive terms. The main hypothesis is that stronger repulsive terms contribute towards learning discrete structures, while stronger attractive terms contribute towards learning continuous/manifold structures. The paper empirically explored the space parameterized by the relative weighting of the attractive and repulsive terms for the t SNE and UMAP algorithms, using several data sets, and qualitatively confirmed their conclusions about the impact of the attractive and repulsion terms as the relative weights vary.   The experimental validation of the paper s main hypothesis is thorough and the use of diverse data sets and neighbor embedding methods is appreciated  as the authors point out, several reviewers missed this contribution. However, several reviewers point out that the insight presented in the paper is already largely present in the literature, and that beyond its analysis the paper does not present new algorithms based on this insight. The authors rebut this claim by arguing that the novelty of the paper lies in it: (1) showing the contrary to the established opinion, UMAP works despite, instead of because, it uses cross entropy loss, and (2) the paper offers for the first time a theoretical understanding of why ForceAtlas2 highlights continuous developmental trajectories, and (3) prior work has not made the connection between UMAP, ForceAtlas2, and t SNE or suggested using exaggeration throughout the optimization process for t SNE rather than simply as a warm up. The paper does indeed present intuitions for (1) (3) based on the attraction repulsion ideas, and makes the connection between these neighbor embedding algorithms by viewing them as variations on the theme of attraction repulsion, but these intuitions are not significant steps forward with respect to what is already known about how neighbor embeddings balance attraction and repulsion. The mathematical analyses consist of stating the gradient for the algorithms and explaining how weighing the attraction and repulsion terms differently lead to different qualitative observations. The use of exaggeration throughout the optimization process is straightforward, and no strong mathematical characterization of the properties of the resulting algorithm is given.  It is recommended that this paper be rejected, as it consists of a thorough empirical validation of an understanding of the trade off between attractive and repulsive forces in neighbor embedding methods that was already present in the literature, along with some straightforward arguments connecting several popular neighbor embedding methods, but does not introduce any significantly new actionable insights or novel algorithms. 
The paper is well written and develops a novel and original architecture and technique for RNNs to learn attractors for their hidden states (based on an auxiliary denoising training of an attractor network). All reviewers and AC found the idea very interesting and a promising direction of research for RNNs. However all also agreed that the experimental validation was currently too limited, in type and size of task and data, as in scope. Reviewers demand experimental comparisons with other (simpler) denoising / regularization techniques; more in depth experimental validation and analysis of the state denoising behaviour; as well as experiments on larger datasets and more ambitious tasks. 
Main content:  Blind review #1 summarizes it well:  his paper claims to be the first to tackle unconditional singing voice generation. It is noted that previous singing voice generation approaches leverage explicit pitch information (either of an accompaniment via a score or for the voice itself), and/or specified lyrics the voice should sing. The authors first create their own dataset of singing voice data with accompaniments, then use a GAN to generate singing voice waveforms in three different settings: 1) Free singer   only noise as input, completely unconditional singing sampling 2) Accompanied singer   Providing the accompaniment *waveform* (not symbolic data like a score   the model needs to learn how to transcribe to use this information) as a condition for the singing voice 3) Solo singer   The same setting as 1 but the model first generates an accompaniment then, from that, generates singing voice     Discussion:  The reviews generally point out that while a lot of new work has been done, this paper bites off too much at once: it tackles many different open problems, in a generative art domain where evaluation is subjective.     Recommendation and justification:  This paper is a weak reject, not because it is uninteresting or bad work, but because the ambitious scope is really too large for a single conference paper. In a more specialized conference like ISMIR, it would still have a good chance. The authors should break it down into conference sized chunks, and address more of the reviewer comments in each chunk.
This is a difficult decision, as the reviewers are quite polarized on this paper, and did not come to a consensus through discussion. The positive elements of the paper are that the method itself is a novel and interesting approach, and that the performance is clearly state of the art. While impressive, the fact that a relatively simple task module trained on the features from Zhu et al. can match the performance of GAZSL suggests that it is difficult to compare these methods in an apples to apples way without using consistent features. There are two ways to deal with this: train the baseline methods using the features of Zhu, or train correction networks using less powerful features from other baselines.  Reviewer 3 pointed this out, and asked for such a comparison. The defense given by the authors is that they use the same features as the current SOTA baselines, and therefore their comparison is sound. I agree to an extent, however it should be relatively simple to either elevate other baselines, or compare correction networks with different features. Otherwise, most of the rows in Table 1 should be ignored. Running correction networks in different features in an ablation study would also demonstrate that the gains are consistent.  I think the authors should run these experiments, and if the results hold then there will be no doubt in my mind that this will be a worthy contribution. However, in their absence, I can’t say with certainty how effective the proposed method really is. 
Following the revision and the discussion, all three reviewers agree that the paper provides an interesting contribution to the area of generative image modeling. Accept.
The authors propose a simple but effective method for feature crossing using interpretation inconsistency (as defined by the authors).  I think this is a good work and the authors as well as the reviewers participated well in the discussions. However, there is still disagreement about the positioning of the paper. In particular, all the reviewers  felt that additional baselines should be tried. While the authors have strongly rebutted the necessity of these baselines the reviewers are not convinced about it. Given the strong reservations of the all the 3 reviewers at this point I cannot recommend the acceptance of this paper. I strongly suggest that in subsequent submissions the authors should position their work better and perhaps compare with some of the related works recommended by the reviewers.
This work highlights the problem of biased rewards present in common adversarial imitation learning implementations, and proposes adding absorbing states to to fix the issue. This is combined with an off policy training algorithm, yielding significantly improved sample efficiency, whose benefits are convincingly shown empirically. The paper is well written and clearly presents the contributions. Questions were satisfactorily answered during discussion, and resulted in an improved submission, a paper that all reviewers now agree is worth presenting at ICLR. 
The paper combines graph convolutional networks with noisy label learning. The reviewers feel that novelty in the work is limited and there is a need for further experiments and  extensions. 
Summary: This paper introduces a method to try to learn in environments where a person specifies successful outcomes  but there is no environmental reward signal.  I d personally be interested in knowing where people were able to easily provide such successful outcomes instead of, for instance, providing demonstrations or reward feedback. Similarly, I d be interested in how other methods of providing human prior knowledge compared.  Discussion: Reviewers agreed the paper was interesting, but none of the 4 thought the paper should be accepted.  Recommendation: While I do not think this paper should be accepted in its current form, I hope the authors will find the comments and constructive criticism useful.
This paper extends state of the art semi supervised learning techniques (i.e., MixMatch) to collect new data adaptively and studies the benefit of getting new labels versus adding more unlabeled data. Active learning is incorporated in a natural and simple (albeit, unsurprising) way and the experiments are convincing that this approach has merit.  While the approach works, reviewers were concerned about the novelty of the combination given that its somewhat obvious and straightforward to accomplish. Reviewers were also concerned that the space of both semi supervised learning algorithms and active learning algorithms was not sufficiently exhaustively studied. As one reviewer points out: neither of these ideas are new or particular to deep learning.  Due to lack of novelty, this paper is not suited for a top tier conference. 
The paper proposes "Delaunay Component Analysis", a novel manifold learning technique. Reviewers raised several concerns regarding novelty, computational complexity of the method, and presentation. The authors provided a thorough rebuttal and engaged in discussion with the reviewers that addressed the concerns in a satisfactory manner. After the discussion, all the reviewers and AC recommend acceptance.
Main content:  Blind review #1 summarizes it well:  The paper proposes an algorithmic improvement that significantly simplifies training of energy based models, such as the Restricted Boltzmann Machine. The key issue in training such models is computing the gradient of the log partition function, which can be framed as computing the expected value of f(x)   dE(x; theta) / d theta over the model distribution p(x). The canonical algorithm for this problem is Contrastive Divergence which approximates x ~ p(x) with k steps of Gibbs sampling, resulting in biased gradients. In this paper, the authors apply the recently introduced unbiased MCMC framework of Jacob et al. to completely remove the bias. The key idea is to (1) rewrite the expectation as a limit of a telescopic sum: E f(x_0) + \sum_t E f(x_t)   E f(x_{t 1}); (2) run two coupled MCMC chains, one for the “positive” part of the telescopic sum and one for the “negative” part until they converge. After convergence, all remaining terms of the sum are zero and we can stop iterating. However, the number of time steps until convergence is now random.  Other contributions of the paper are: 1. Proof that Bernoulli RBMs and other models satisfying certain conditions have finite expected number of steps and finite variance of the unbiased gradient estimator. 2. A shared random variables method for the coupled Gibbs chains that should result in faster convergence of the chains. 3. Verification of the proposed method on two synthetic datasets and a subset of MNIST, demonstrating more stable training compared to contrastive divergence and persistent contrastive divergence.     Discussion:  The main objection in reviews was to have meaningful empirical validation of the strong theoretical aspect of the paper, which the authors did during the rebuttal period to the satisfaction of reviewers.     Recommendation and justification:  As review #1 said, "I am very excited about this paper and strongly support its acceptance, since the proposed method should revitalize research in energy based models."
# Quality:  The paper makes a good job of presenting the proposed algorithm, which seems interesting and solid. However, the paper fails to place the proposed approach in the larger context of the existing literature. In addition, only qualitative results are presented, without any comparison.  As such, it is impossible to really understand the goodness of the proposed approach.  # Clarity:  The paper is generally well written and clear.  # Originality: The proposed approach is novel to the best of the reviewers and my knowledge.  # Significance of this work:  The paper deal with a very relevant and timely topic. However, as stated by the authors themself the paper is not concerned with high dimensional systems, which is what would really differentiate this work compared to existing literature. In addition, the paper has no quantitative results nor comparisons against previous literature, and does not evaluate any of the standard benchmarks.  # Overall: There is disagreement from the reviewers regarding the acceptance of this paper, and the overall score is very borderline. After thoroughly reading the paper, I agree with the evaluation of Reviewer 2 and 3 regarding the lack of comparisons and thus lean towards rejection.  
The paper presents modifying latent optimization for representation disentanglement using contrastive learning, resulting in improved performance on disentanglement benchmarks. Despite the empirical success, the proposed algorithm has many moving parts and loss functions. Most reviewers agree that given the incremental and complex nature of the proposed technique, the empirical results are not sufficient for acceptance at ICLR, especially since the results do not present additional insights into the inner workings of the method. I encourage the authors to try to simplify the technique, or provide a convincing evidence that such complexity is necessary.  PS: I didn t find much discussion of how the hyper parameters are chosen (temperature, lambda terms, etc.). A discussion of recent self supervised disentanglement methods (e.g., https://arxiv.org/abs/2102.08850 and https://arxiv.org/abs/2007.00810) can be helpful.
Dear authors,  Improving the theoretical understanding of powerful algorithms is an important contribution to our field. Nevertheless, most of the reviewers are inclined to reject the paper. I somehow have to agree with them as e.g., adding more restrictive assumptions can allow deriving better bounds, but the question then is how useful this result will be to the ICLR community. I would encourage you to chose maybe another venue.  Thanks
Thanks for your submission to ICLR!  This paper considers a novel unsupervised image clustering framework based on a mixture of contrastive experts framework.  Most of the reviewers were overall positive about the paper.  On the positive side, they noted that the paper had an interesting idea, was well motivated, written well, and had solid results.  Also, the authors provided detailed and useful responses to the reviews, which further strengthened the case for accepting the paper.  On the negative side, one reviewer felt that the paper seemed a bit preliminary and its presentation could improve.  Also, there was some concern about missing comparisons / discussion to previous work (including from a public comment) or data sets (e.g. ImageNet 10).  Again, the authors responded well to these concerns.  Given that the overall response was quite positive with the paper, I m happy to recommend accepting it.  
This paper proposes to use implicit neural representations to model how our surroundings affect the sounds reverberating within. Concretely, the proposed approach can produce impulse responses that capture environment reverberations between any two points in a scene.  Reviewers praised the novelty and originality of the idea (and I concur), but raised concerns about the clarity of the writing (especially w.r.t. modelling the phase component), lack of detail, insufficient or inadequate experiments and overclaiming of results. (There were also concerns about overclaiming of contributions, but I am inclined to agree with the authors that this isn t really the case.)  The authors have clearly taken the time to try to address these concerns, and I commend them on their willingness to engage with the reviewers  comments and suggestions. While one of the reviewers raised their score to "accept", I am inclined to agree with the other reviewers and recommend rejection. The required degree of revision is substantial, and therefore difficult to assess within a single review cycle. I believe this work must undergo another thorough assessment in its revised form, before it can be accepted for publication.
the proposed approach of predicting k nearest neighbouring examples as an auxiliary task is an interesting idea. however, the submission should have studied further on how those examples are predicted (e.g., sequence prediction is one, but you could try set prediction, or so on) rather than how sequential prediction of nearest neighbours is done together with different types of classifiers (many of which are arguably not necessarily suitable for classification,) which was a sentiment shared by all the reviewers.   more careful investigation of different ways in which nearest neighbour prediction could be incorporated and more careful/thorough analysis on how the incorporation of this auxiliary task changes the behaviours or properties of the representation would make it a much better paper (also with clearer writing.)
## Description  The paper applies ideas from contrastive representation learning to train binary neural networks. Namely, the algorithm promotes binary representations to be similar to the full precision representations while at the same time it promotes binary representations to be dissimilar from full precision representations corresponding to other input images. This is enforced for activations in all layers by the added contrastive loss (9).  ## Decision  The main weakness of the paper pointed by reviewers were 1) overlap of the large part of derivation with the prior work [25] Tian et al. "Contrastive representation distillation", ICLR 2020; and 2) the meaning of the derivation when applied in the setting of the paper to binary and full precision weights and its soundedness. The authors proposed their arguments for 1). The reviewers board considered these arguments and did not agree (see below). Point 2) was not addressed by authors (no paper revision, justifications, proofs corresponding to the missing supplementary). It was discussed further and was found critical (see below), such that it is a clear reason for rejection regardless of 1). Overall, the idea is interesting and the method appears to be helpful experimentally, however the paper needs a major revision that would address the two points.  ## Details  ### Overlap with CRD  Reviewers were in a consensus on this issue, disagreeing with authors. Since the whole derivation chain of the contrastive loss already exists in the CRD work [25], it is redundant to repeat this derivation if not raising ethical concerns. Instead an original work should review or just refer to the existing derivation and only discuss the new context and e.g. change the critic function $\hat h$.   ### Meaning of the derivation   The reviewers have questioned the soundness of the initial criterion of MI between binary and full precision activations, as it reduces to just the entropy of binary activations. In particular, it seems very different in meaning to the contrastive loss the paper optimizes in the end. Here is additional feedback from the discussion.  1. Maximizing the entropy of binary activations with respect to the data distribution makes some sense. If a single binary activation was considered, its entropy is maximized when it is in the state 1 exactly for 50% of the data. Which makes it discriminative of the input. A similar centering can be achieved by Batch Normalization put in front of the activation   if the preactivation distribution was symmetric, then BN would achieve the max entropy for the sign of preactivation. Such network design is not uncommon. Maximizing the entropy of the full vector of binary activations appears more difficult. However we can also understand it as the mutual information between the input image and the layer of binary activations. Thus the criterion is to retain as much information about the input as possible. This makes sense as a regularization (often neural networks are regularized by adding data reconstruction capabilities / loss), and is aligned well with goals such as re using the features for other tasks (as in Sec .3.5) but contradicts to some other principles proposed in the literature, e.g. the information bottleneck (that the maximum information about the target rather than the input should be preserved). Amongst methods that study the direction of maximizing the entropy in binary networks, reviewers mention IR Net and Regularizing Activation Distribution for Training Binarized Deep Networks. The architecture with BN before activation is used in the latter work and some more recent works, e.g. BoolNet.  2. It is not clear whether optimizing the contrastive loss retains the same meaning as maximizing MI. The derivation from CRD paper used here applies several lower bounding steps. Maybe the strongest one is that the critic is chosen to be of a specific function rather than a universal approximator. However there is no obvious gap. In fact knowing that binary activations are just a sign mapping of full precision ones, should allow one to estimate $p(i j| a^i_B, a^j_F)$ in a simple way.  3. In the estimator $h$ in (8) the authors make a mistake (applying their and CRD theory incorrectly): $h$ should be the probability of a conditional Bernoulli variable estimating $p(i j| a^i_B, a^j_F)$. It should not depend on $a^j_F$ for other values of $j$ than the given one. However in the denominator in (8) it does. Therefore this estimator, and as a result the specific NCE loss proposed, appear unjustified. If the critic from CRD eq. (19) is adopted, it is not clear whether it makes sense for a pair of binary and full precision descriptors (note that for $i j$ the scalar product between the two is just $\|a_F\|_1$).  It seems that the design of a meaningful critic is a serious gap the authors should address. Observing that the initial objective, the MI criterion, was in fact independent of full precision states (as it is the entropy of binary states), one can propose that an appropriate critique should use binary states only, such as $$ h(a_B^i,a_B^j)   \sigma(\left<a_B^i, a_B^j\right>  + c ). $$ When fixing $\hat h$ the result in (10) that the maximum likelihood estimator for $p(i j | a_B^i, a_F^j)$ with a generic neural network can approximate this distribution arbitrary well becomes irrelevant.  When the paper speaks of randomness, e.g. "binary and full precision activations as random variables, considering "i j" as a random variable, it is needed to specify the source of randomness or the distribution, i.e. to add "for a network input drawn from the data distribution" in the first case and "under i and j picked at random uniformly in the batch" in the second.  Theoretically, the paper would become more convincing, if the the entropy of binary activations was measured by independent tools from the literature after training with and without NCE loss and it was shown that indeed the method achieves an improvement in this objective, reconfirming that the principle and the derivation were sound. An ablation study on other modifications such as weight decay may be helpful to convince researchers that the main source of improvements in experiments is the new contrastive loss. Note that not all reviewers were convinced by current experimental results due to lack of descriptions / code to fully reproduce and or lack of such ablation studies.
The paper received diverging review feedback. While reviewers found merits in the work, they also raise serious concerns over experimental validation, comparison with the existing methods, and practicality of the proposed method. It appears that the paper can benefit from better writing and more experimental validations clarifying all these points. 
This paper proposes that the superior performance of modern convolutional networks is partly due to a phase collapse mechanism that eliminates spatial variability while ensuring linear class separation. To support their hypothesis, authors introduce a complex valued convolutional network (called  Learned Scattering network) which includes a phase collapse on the output of its wavelet filters and show that such network has comparable performance to ResNets but its performance degrades if the phase collapse is replaced by a threshold operator.  Reviewers are all in agreement about the novelty and significance of the work. They also find the empirical results compelling. The main weakness of this work which was highlighted by all reviewers is clarity. The paper can be significantly improved in terms of the writing. While I am recommending acceptance, I strongly recommend authors to take reviewers  feedback into account and improve the writing significantly for the final version so that more people would benefit from this paper and build on it in the future.
This paper presents a transformer model for learning representations of assembly code blocks, trained using a variant of the masked language modeling objective that encodes the full code block token sequence into a single bottleneck vector and then uses that vector to decode all the masked out tokens.  Overall reviewer assessment for this paper is on the rejection side, mostly due to the not so novel model architecture and training objective.  Experiments show that this variant of the MLM performs significantly better than the standard MLM objective without the bottleneck, which surprisingly is even worse than the simple TF IDF in many tasks.  This raises questions and it is unclear from the paper why the variant with a sequence level bottleneck should perform better than the standard MLM.  Binary code similarity detection has many implications in security, so this is a good domain to explore more in, and I encourage the authors to continue to improve this work and send it to the next venue.  One related work also published in the ML community comes to mind that the authors might not be aware of: Graph matching networks for learning the similarity of graph structured objects by Li et al., ICML 2019, which also looked at binary code similarity detection, but works at the function level.  It would be good to also take a look for other potentially missing related work.
The paper introduces a simple technique to improve non autoregressive generation by training the model to reconstruct model perturbed inputs in addition to inputs perturbed by a fixed noise source.   Despite interest in the paper, we were worried about a number of aspects missing from section 3. During the rebuttal phase, however, the authors addressed most if not all comments and the section is now rather complete.  For its clarity, and the interesting results, we are recommending this version for acceptance.   A comment on presentation:  The paper attempts to establish a connection with variational diffusion models, but the connection does not seem strong enough at this point. In a variational diffusion approach, the forward view would not involve $f_\theta$, for example. Also, given that the distribution of $\mathbf x_t$ depends on $\theta$, the gradient estimator used in the paper is a heuristic, and I d like to ask that the authors emphasise this clearly and early on in the draft.
This paper experimentally observes the negative transfer in Multi task Graph Representation Learning and proposes to solve the negative transfer with a novel Meta Learning based training procedure. However, the proposed methods seems not technically sound. There are some concerns about this paper：1. The technique contribution of this paper is limited. The method proposed in this paper is just an application of MAML in Graph Representation Learning with a little variation. 2. This paper only compares SAME with the vanilla MTL method, which adopts the uniform weights. However, the vanilla MTL method commonly performs poorly. The state of the art MTL methods should be taken into comparison, for example MGDA [1]. 3. The traditional Meta Learning framework introduced in Algorithm 4 is misleading.  4. The experimental analysis of this paper is not sufficient. For example, the paper has not analyzed whether the improvement comes from the meta updating or comes from the singularly training strategy.  [1]. Sener, Ozan, and Vladlen Koltun. "Multi task learning as multi objective optimization." NIPS 2018.  
This paper aims to study the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The purpose is to understand the regime where the width and depth go to infinity together with a fixed ratio. The paper does not have a lot of numerical experiments to test the mathematical conclusions. In the discussion the reviewers concurred that the paper is interesting and has nice results but raised important points regarding the fact that only the diagonal elements are studied. This I think is the major limitation of this paper. Another issue raised was lack of experimental work validating the theory. Despite the limitations discussed above, overall I think this is an interesting and important area as it sheds light on how to move beyond the NTK regime. I also think studying this limit is very important to better understanding of neural network training. I recommend acceptance to ICLR.
This paper addresses a promising and challenging idea in Bayesian deep learning, namely thinking about distributions over functions rather than distributions over parameters.  This is formulated by doing MCMC in a functional space rather than directly in the parameter space.  The reviewers were unfortunately not convinced by the approach citing a variety of technical flaws, a lack of clarity of exposition and critical experiments.  In general, it seems that the motivation of the paper is compelling and the idea promising, but perhaps the paper was hastily written before the ideas were fully developed and comprehensive experiments could be run.  Hopefully the reviewer feedback will be helpful to further develop the work and lead to a future submission.  Note: Unfortunately one review was too short to be informative.  However, fortunately the other two reviews were sufficiently thorough to provide enough signal.  
The paper proposes inner ensemble method where output of inner layers are replaced by an ensemble average of them during inference time to reduce inference time and reduce variance. The authors include experiment results showing performance improvement of their method and use the theoretical analysis of the dropout and maxout to justify the goodness of the proposed method.   pros.  The authors consider a useful and interesting problem.   The proposed method is simple and easy to plugin.    The results show performance improvements in a number of cases.   cons.   However there are various concerns raised by the reviewers that I find are not well justified.   The use of inner ensembles are not well justified.   The baseline in the experiments miss the usual ensemble methods instead the authors use single model which does not have the same number of parameters.   novelty and significance  The authors do not clearly distinguish their method from earlier results and how it differs and improve over existing work. Given the current state of the paper, the novelty is not significant.   clarity of theoretical analysis is lacking.  Overall I suggest the authors improve the manuscript considering concerns and suggestions listed above and through reviewers and submit to an upcoming venue.    
The AIRL is presented as a scalable inverse reinforcement learning algorithm. A key idea is to produce "disentangled rewards", which are invariant to changing dynamics; this is done by having the rewards depend only on the current state. There are some similarities with GAIL and the authors argue that this is effectively a concrete implementation of GAN GCL that actually works.  The results look promising to me and the portability aspect is neat and useful!  In general, the reviewers found this paper and its results interesting and I think the rebuttal addressed many of the concerns. I am happy that the reproducibility report is positive which helped me put this otherwise potentially borderline paper into the  accept  bucket.
This paper points out connections between the self attention module in transformers and some prior art, including kernel regression, the non local mean algorithm, locally linear embeddings, and the self expression algorithm for subspace clustering. Based on these observations, the authors argue that the innovation of self attention is not modeling the long range relation, which is also proposed in prior work, but the learnable parameters and the multi head design. The authors also suggest several directions for future work, such as using self attention for manifold clustering.  Reviewers pointed out several weaknesses with this paper: that some connections (e.g. connection to kernel regression) had been pointed out before, that the relation between self attention and locally linear embedding and self expression in subspace clustering is a bit nuanced, as pointed out by one of the reviewers, and that while some speculative future directions might be interesting, the paper falls short in actually trying some of them out empirically, or building a proof of concept.   In the discussion period, the authors pointed out that this is a position paper (which unfortunately was not expressed so assertively in their submission), which according to their view liberates them from digging deeper and test empirically some of these connections and speculative directions. According to the authors, a core contribution of their position paper is that "it expresses the opinion that the original attention paper failed to cite and acknowledge that attention mechanisms build upon a series of prior works in sparse coding, subspace clustering, and locally linear embedding."   There are no specific guidelines to review position papers at ICLR that I know of, but I will base my assessment on the assumption that a good position paper should:   provide a good historical perspective of a subject   connect previously unrelated lines of work in non obvious ways   inspire the research community to look at new directions.  While a good position paper can be extremely valuable and enlightening, I am not convinced that this particular paper achieves either of the goals above, and therefore it is my opinion that it does not deserve publication at ICLR.   As pointed out both by the authors and the reviewers, the connection between self attention and kernel regression and non local mean denoising is not new, and so it is not an original contribution of this paper. The relation between self attention and locally linear embedding and self expression in subspace clustering appears to be new, but this relation is a bit nuanced, as pointed out by one of the reviewers.   The tone of this position paper is that some of these connections were missing in the original attention paper   the authors say "attention did not properly acknowledge prior art" in one of their responses (it is not clear if they are referring to Bahdanau et al. s attention paper or to Vaswani et al. s transformer paper). However, the historical perspective of how attention mechanisms came to be seems to be missing from this position paper   attention has been proposed by Bahdanau et al. for machine translation, inspired by the idea of word alignment that has been prevalent in machine translation for decades. Later, in the transformer paper, self attention was suggested as an alternative to recurrent and convolutional models for machine translation (note that self attention has been used before the transformer paper, see e.g. [1]). While a theoretical connection with kernel regression etc. exists, this was not related to the original motivation of these works. There are many ways of arriving at the same construction! And given the simplicity of attention mechanisms it doesn t surprise me that connections with other lines of research exist. Had they been noticed, they would probably be a parenthesis in the original papers, because attention is derived there in a much more direct way (this doesn t mean that the connections aren t interesting, but that they are not _essencial_ to the construction).   In their response, the authors dismissed a constructive suggestion from one of the reviewers which in my opinion would have strengthen this paper   the connection with graph neural networks. If the point of the paper is to point out past research that connects fundamentally to the idea of attention mechanisms, why leaving this out?  In sum, in my view this paper lacks the rigor, the insight, and the historical perspective that should characterize a strong position paper, and as such I cannot recommend acceptance. I strongly suggest that the authors take into account some of the insightful suggestions given by the reviewers in future iterations of their work.   [1] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.
This paper represents the PettingZoo library of multi agent environments, providing a common API and benchmark for multi agent learning. The library has high potential for impact and is likely of interest to a wide range of people in the ICLR community. However, in its current form the paper could be significantly improved by actioning the many pieces of constructive feedback provided by all reviewers.  We have also been made aware of two highly related papers "Multiplayer Support for the Arcade Learning Environment" and "SuperSuit: Simple Microwrappers for Reinforcement Learning Environments." Together all three papers could be one comprehensive manuscript, but appear to have been unnecessarily split into three separate short papers.
Not quite enough for an oral but a very solid poster.
The manuscript describes a method for identifying and correcting classifier performance when labels are assigned incorrectly. The identification is based on clustering classification failure regions in a VAE latent space and the correction phase is based on fine tuning the classifier with additional synthetic samples from the VAE.  Reviewers agreed that the manuscript is not ready for publication. The main issue is that the suggested training method is similar to adversarial training methods used to gain adversarial robustness. The method does not help in debugging and fixing failures in general. 
The paper proposes two regularizers for encouraging "clustered feature embeddings" (use of "disentangled" in the title is misleading). Reviewers have raised points about the lack of proper motivation and justification of the regularizers. There are also concerns on the experiments conducted to evaluate the method, including for hierarchical classification setting. Missing comparison with relevant baselines has also been pointed out as a weakness. I feel the work is not yet mature. 
The paper presents a well conducted empirical study of the Reweighted Wake Sleep (RWS) algorithm (Bornschein and Bengio, 2015). It shows that it performs consistently better than alternatives such as Importance Weighted Autoencoder (IWAE) for the hard problem of learning deep generative models with discrete latent variables acting as a stochastic control flow.  The work is well written and extracts valuable insights supported by empirical observations: in particular the fact that increasing the number of particles improves learning in RWS but hurts in IWAE, and the fact that RWS can also be successfully applied to continuous variables. The reviewers and AC note the following weaknesses of the work as it currently stands:  a) it is almost exclusively empirical and while reasonable explanations are argued, it does not provide a formal theoretical analysis justifying the observed behaviour b) experiments are limited to MNIST and synthetic data, confirmation of the findings on larger scale real world data and model would provide a more complete and convincing evidence.  The paper should be made stronger on at least one (and ideally both) of these accounts.  
This paper addresses the classic medial image segmentation by combining Neural Ordinary Differential Equations (NODEs) and the level set method. The proposed method is evaluated on kidney segmentation and salient object detection problems. Reviewer #1 provided a brief review concerning ICLR is not the appropriate venue for this work. Reviewer #2 praises the underlying concept being interesting, while pointing out that the presentation and experiments of this work is not ready for publication yet. Reviewer #3 raises concerns on whether the methods are presented properly. The authors did not provide responses to any concerns. Given these concerns and overall negative rating (two weak reject and one reject), the AC recommends reject.
The paper studies the problem of leveraging Positive Unlabeled~(PU) classification and conditional generation with extra unlabeled data simultaneously in one learning framework. Some major review concerns on the weaknesses include limited novel technical contributions, poor presentation and weak experimental results (e.g., experiments were mostly conducted on small toy datasets). Overall, the paper has some interesting idea, but the work is clearly below the ICLR acceptance bar. 
This paper provides and analyzes an interesting approach to "de biasing" a predictor from its training set.  The work is valuable, however unfortunately just below the borderline for this year.  I urge the authors to continue their investigations, for instance further addressing the reviewer comments below (some of which are marked as coming after the end of the feedback period).
The paper focuses on the update step in Message Passing Neural Networks, specifically for GNN. A series of sparse variants of the update step, say complete removal and expander graphs with varying density, are compared in empirical studies. The findings are quite useful for practice, and the paper is organized and written well.  As observed by the reviewers, there are several concerns regarding the novelty and contribution of the work. Besides, theoretical analysis of the sparsification approach is lacking. The authors provided a good rebuttal and addressed some concerns, but not to the degree that reviewers think it passes the bar of ICLR. We encourage the authors to further improve the work to address the key concerns. 
This work tackles to address the sparse reward problem in RL. They augment actor critic algorithms by adding an adversarial policy. The adversary tries to mimic the actor while the actor itself tries to differentiate itself from the adversary in addition to learning to solve the task. This in a way provides diversity in exploration behavior. Reviewers liked the paper in general but had several clarification questions. The authors provided the rebuttal and addressed some of the concerns. Considering the reviews and rebuttal, AC and reviewers believe that the paper provides insights that are useful to share with the community. That being said, the paper will still immensely benefit with more extensive experimentation on standard benchmark environments like Atari, etc. Please refer to the reviews for other feedback and suggestions.
The first reviewer summarizes the contribution well: This paper combines [a CNN that computes both a multi scale feature pyramid and a depth prediction, which is expressed as a linear combination of "depth bases"]. This is used to [define a dense re projection error over the images, akin to that of dense or semi dense methods]. [Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg Marquardt (LM). By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back propagation and thus learning of the networks  parameters.]   Strengths: While combining deep learning methods with bundle adjustment is not new, reviewers generally agree that the particular way in which that is achieved in this paper is novel and interesting. The authors accounted for reviewer feedback during the review cycle and improved the manuscript leading to an increased rating.   Weaknesses: Weaknesses were addressed during the rebuttal including better evaluation of their predicted lambda and comparison with CodeSLAM.  Contention: This paper was not particularly contentious, there was a score upgrade due to the efforts of the authors during the rebuttal period.  Consensus: This paper addresses an interesting  area of research at the intersection of geometric computer vision and deep learning and should be of considerable interest to many within the ICLR community. The discussion of the paper highlighted some important nuances of terminology regarding the characterization of different methods. This paper was also rated the highest in my batch. As such, I recommend this paper for an oral presentation. 
The paper presents a lot of empirical evidence that fine tuning pruned networks is inferior to training them from scratch. These results seem unsurprising in retrospect, but hindsight is 20 20.  The reviewers raised a wide range of issues, some of which were addressed and some which were not. I recommend to the authors that they make sure that any claims they draw from their experiments are sufficiently prescribed. E.g., the lottery ticket experiments done by Anonymous in response to this paper show that the random initialization does poorer than restarting with the initial weights (other than in resnet, though this seems possibly due to the learning rate). There is something different in their setting, and so your claims should be properly circumscribed. I don t think the "standard" versus "nonstandard" terminology is appropriate until the actual boundary between these two behaviors is identified. I would recommend the authors make guarded claims here.
The paper proposes a deep learning pipeline to extract lemmas from proofs and how to (re)use them to compress a proof library. The The topic is highly relevant, the motivation behind the work is clear but the current state of the manuscript is yet not ready for publication.  In fact, while all reviewers somehow recognized the potential impact of this work, they also highlighted some critical aspects that should be addressed before full acceptance. Namely:      the role of GNNs in achieving the achieved performance shall be discussed more in detail (what if they are replaced in the pipeline? an ablation study would help)     the broader literature of automated theorem proving should be addressed more precisely in the related works     comparisons against other ML methods for theorem proving should be extended (the comparison with MetaGen added in the discussion is a good starting point that can turn into a full experiment)
This paper proposes to use more varied geometric structures of latent spaces to capture the manifold structure of the data, and provide experiments with synthetic and real data that show some promise in terms of approximating manifolds. While reviewers appreciate the motivation behind the paper and see that angle as potentially resulting in a strong paper in the future, they have concerns that the method is too complicated and that the experimental results are not fully convincing that the proposed method is useful, with also not enough ablation studies. Authors provided some additional results and clarified explanations in their revisions, but reviewers still believe there is more work required to deliver a submission warranting acceptance in terms of justifying the complicated architecture experimentally. Therefore, we do not recommend acceptance.
This paper presents an interesting connection between learning theory and local explainability. The reviewers have reacted to each others  thoughts, as well as the authors  comments; they are largely in favor of acceptance. I think the ICLR community will enjoy discussing this paper at the conference.
The paper describes a new testbed to evaluate Bayesian techniques in the context of joint predictive distribution.  Since this is not the first paper that considers marginal vs joint distribution evaluation, the paper should include a thorough discussion of the differences with prior work.  The paper simply states that it refutes Wang et al. s previous observation that joint distributions do not distinguish techniques much more than marginals.  However, the paper does not really explain why their observation is correct and Wang s observation should be discarded.  Since this is the core contribution of the paper and it is doubtful, this is problematic.  The discussion of epistemic/aleatoric uncertainty also seems superfluous and therefore distract the reader.
Summary: This paper studies an inverse (linear) contextual bandits (ICB) problem, where, given a $T$ round realization of a bandit policy’s actions and observed rewards, the goal is to design an algorithm to estimate the underlying environment parameter, along with the “belief trajectory” of the bandit policy. A particular emphasis is placed on the belief trajectory being “interpretable” and capturing changes in the policy’s “knowledge of the world” over time.  The paper’s main contributions are (i) formalizing the inverse contextual bandits problem, (ii) designing two algorithms for this problem based on two different ways of modelling beliefs of the bandit policy, and (iii) providing empirical illustrations of how their algorithm can be used to investigate and explain changes in medical decision making over time  Discussion: This paper has received high quality, long and detailed reviews that highlighted some flaws, in particular in the well posedness of the problem and the clarity of the writing. The authors  response was long and detailed as well, and its quality was recognized by the committee.  However, the consensus is that this work would require a full pass allowing to include most of the feedback received in the main text rather than in appendices, to discuss related problems in the literature in more depth and perhaps to refocus the exposition on the problem considered.  Recommendation: Reject.
Authors study robustness properties of arbitrary smoothing measures with bounded support using Wasserstein distance and total variation distance. Reviewers pointed out several weaknesses about this work. In particular, they mentioned the paper is not well organized, comparison with prior work is lacking, the conclusion of the theoretical analysis is not novel and the experiments are not comprehensive. I suggest authors to take these comments into account in improving their work.
This paper introduces a new method for jointly training a dense bi encoder retriever with a cross encoder ranker. More precisely, the proposed method is iteratively training the retriever and the ranker, using an objective function inspired by adversarial training. In addition, the authors propose to use a distillation loss from the ranker to the retriever as a regularization term. The proposed method, called AR2, is evaluated on three retrieval benchmarks from question answering: NaturalQuestions, TriviaQA and MS MARCO. The method obtains state of the art retrieval performance on these three datasets.  Overall, the reviewers agree that the strong performance obtained by the proposed method is a strength of the paper. Regarding novelty, some reviewers argue that the method is a combination of existing techniques, hence lacking novelty, while the others believe that combining these different techniques is novel enough. Regarding the experimental section, some concerns were raised about comparisons with previous work (eg, BERT vs ERNIE) or the fact that it was a bit hard to determine where the improvements come from. I believe that these concerns were well addressed by the authors, and I tend to believe that combining existing techniques to obtain a strong system is novel enough. I thus lean towards accepting this paper to the ICLR conference.
The paper presents  a new variant of the Stochastic Heavy Ball method with coordinate wise stepsizes. They prove a regret upper bound in the online convex optimization setting and validate the algorithm on few deep learning tasks.  The reviewers found the paper severely lacking on many aspects. In particular, the formulation appears not motivated at all, the regret upper bounds relies on an unverified assumption of boundedness of the iterates, the momentum parameter must decrease exponentially over time. Note that it is known how to analyze the momentum algorithm under much more general conditions. The empirical evaluation was also judged not sufficient, with only 2 datasets (one of them being MNIST).  Overall, the paper was judged not suited for publication at ICLR.
The submitted paper considers a form of second order extension of successor features building on a second order representation of the reward function in terms of state features. The authors demonstrate that this approach can be useful for transfer learning and also show an application to exploration. All reviewers gave borderline recommendations (2x weak accept, 2x weak reject). While most reviewers agree that the proposed approach can be sensible and that the paper is well written, there are concerns that experimental results do not fully support all claims and additional experiments are required to clearly demonstrate advantages over existing baselines. Also the proposed approach for exploration is rather incomplete and not well studied. The raised concerns were not fully refuted by the authors during the discussion period but rather made some reviewers more concerned about full validty of all claims. Thus, while I think the paper has potential and can be turned into a good paper, I am recommending rejection of the paper in its current form. I would like to encourage to authors to carefully address the reviewers  concerns in future versions of the paper.
The paper sparked a very substantial discussion, not just about its scientific content, but also, as the authors will have seen, from the narrative standpoint. I would like to thank the reviewers who devoted time and efforts to discuss the paper’s content. I encourage the authors to polish further their paper prior to camera ready, using the numerous scientific comments made (e.g. R6VS, KWuM, the refs of Cn5V).   AC.
Four expert reviewers (after much discussion, in which the authors seemed to do a pretty good job addressing a lot of the initial complaints) unanimously voted to accept this paper.   Everyone seemed to agree that the idea was interesting, and it is indeed interesting. There were generally complaints about benchmarking; there always are for papers about program synthesis.   One complaint I have, but that I didn t really see mentioned, is that the system as described is pretty baroque.  I have a hard time imaging how you d scale something like this up to more complicated contexts,  and honestly I m not sure even in some of the contexts where it was tested if it would really outperform a well engineered top down synthesizer. Maybe this is just an aesthetic preference that only I have, and maybe ideas need to start out overly complicated before the most useful bits can be extracted from them and refined.   At any rate, I do think that this paper gives a cool new research contribution and that people will want to read it, so I am recommending acceptance. 
Although scores are somewhat mixed, even ignoring the most negative review the overall score would still be somewhat below the acceptance threshold.  The authors and reviewers had a robust discussion, mostly about the novelty, experimental setting, and the significance of the results. Although the discussion ultimately did not reach a consensus, I think there are valid points on both sides. E.g. I somewhat disagree with the reviewer that the paper is too application focused for ICLR, though several other points remain valid. The overall message that the experiments seem not totally convincing was highlighted by multiple reviewers.
This paper proposes two extensions of the TRPO algorithm in which the trust region is defined using the Wasserstein distance and the Sinkhorn divergence. The proposed methods do not restrict the policy to belong to a parametric distribution class and the authors provide  closed form policy updates and a performance improvement bound for the Wasserstein policy optimization. The authors provide an empirical evaluation of their approaches on tabular domains and some discrete locomotion tasks, comparing the performance with some state of the art policy optimization approaches.  After reading the authors  feedback and interacting with the authors, the reviewers did not reach a consensus: one of the reviewers votes for rejection, while the other three reviewers are slightly positive. In particular, the reviewer that voted for rejection raised a number of concerns that have been discussed at length with the authors, who were able to clarify some of the issues, but some of the answers did not satisfy the reviewer. I went through the paper and I found the paper solid from a technical point of view, but I share some of the reviewers  concerns and I think that the authors should better position their contribution with respect to the state of the art.  Overall, this paper is borderline and I feel it needs still some work to deserve clear acceptance (which I think will be soon).
The paper proposed an autoregressive model with a multiscale generative representation of the spectrograms to better modeling the long term dependencies in audio signals. The techniques developed in the paper are novel and interesting. The main concern is the validation of the method. The paper presented some human listening studies to compare long term structure on unconditional samples, which as also mentioned by reviewers are not particularly useful. Including justifications on the usefulness of the learned representation for any downstream task would make the work much more solid. 
This paper provides well written and thorough analysis demonstrating that closed set recognition performance correlates with open set recognition performance, and that simply making the close set model strong via augmentation, label smoothing, etc. along with small scoring changes (using logits rather than softmax probabilities) can get close to (or better than in some cases) performance than much more complicated methods. The authors also propose a large scale benchmark that varies the semantic similarity across classes, allowing for a more fine grained analysis of this problem.   Overall, all of the reviewers thoughts that the paper provides very thorough validation of an insight that would be very interesting to the community. Reviewer HAFU had some concerns about novelty, since a number of papers have shown closed set classifier improvements (and therefore better embeddings) benefit related problems such as few shot learning and generalization to novel domains, as well as proposed large scale experiments. The rebuttal convinced this reviewer, however, that some of the contributions and findings are unique and provide additional evidence to the community, and the new setting provides more fine grained analysis. Reviewer dw7J had a number of suggestions in terms of additional evaluations, and the rebuttal either clarified why it is not possible or added them. As a result, after the discussion the reviewers all supported acceptance of this paper.   Given the above discussion, and rebuttal/changes to the paper, I recommend acceptance. It is a very well done empirical paper, provides interesting findings, stronger baselines, and thorough experimentation. Further, some of the smaller findings (ViT correlation experiment) as well as larger relationship between open set recognition and out of distribution detection are valuable contributions to the community. Finally, I would recommend this paper as oral, given that it may garner a good discussion of these contributions.
the problem is interesting, and the approach is also interesting. however, the reviewers have found that this manuscript would benefit from more experiments, potentially involving some real data (even at least for evaluation) in addition to largely synthetic data sets used in the submission. i also agree with them and encourage authors to consider this option.
Dear authors,  I have carefully read the reviews, rebuttals and the subsequent discussion. The review scores are mixed (5, 5, 6, 6). Let me comment on some of the key issues raised by the reviewers. I will elaborate on some of them with my own insights.  1) You insist that (P4) cannot be regarded as a particular case of (P3). But this is trivially incorrect. The hard constraint in the reformulation (P6) you mention in the discussion can be written as a regularizer: *indicator function* of the constraint set. Indeed, let  $\cal C$ be the set of points $W (w_1,\dots,w_K)$ for which there exists $w$ such that $w_k   m_k^* \circ w$ for all $k$. Then the regularizer defined by ${\cal R}(W)   0$ if $W\in {\cal C}$ and ${\cal R}(W)   +\infty$ if otherwise does the job. This is a well defined regularizer. Such regularizers are routinely used in optimization to model hard constraints. So, the formulation you consider is a special case of (P3).  Moreover, as pointed out by Reviewer Zg2F, and acknowledged by the authors, "The idea of using sparse masks to model personalization for federated learning is not novel in this work. Prior works utilize this idea with other techniques (Li et al., 2020) (Vahidian et al., 2021). Moreover, several side benefits such as low communication cost, cheaper computation, and fewer memory requirements should also be attributed to those original works where sparse masks are used, and the same side benefits of sparsity were mentioned." The claim that one of the novel contributions of FedSpa is "we formulate a clear optimization problem for FedSpa" is weak, especially in the light of the above comment, and the "moral" existence of the formulation in prior work, albeit not expressed in a mathematical notation. The fact that previous works did not formulate this properly is a major issue with those works, and not a major contribution of this work. A clear mathematical formulation of what one wants to achieve should be a standard requirement. In any case, I appreciate the clarity nevertheless.   2) The same reviewer states that the key idea of the paper that differs from the above two mentioned papers is how the sparse masks are handled. One of the two ideas proposed is trivial and is equivalent to standard non personalized FL (if all masks are the same, the submodes they defined can be considered a global model). The second idea does not seem to have any interesting/distinctive theoretical support.   3) Sparse to sparse training in FedSpa may be novel, but the claim that "the masks continue to evolve (towards the optimal masks) in the training process" is not supported by theory nor experiments. If indeed you can show that the local masks evolve to some meaningful notion of an optimal mask, this would be interesting.   4) I also agree with the other points raised by this reviewer. I have read the author response to these comments. (BTW: Language such as "you bet" is inappropriate). While some of them make sense, they do not reduce the severity of the concerns by a large enough margin.   5) The comment about the weakness of the main theorem is particularly concerning. Indeed, the main theorem may be vacuous, and the authors need to do a thorough explanation of the result and its importance (on its own and in comparison with existing literature and rates). I do not believe such a comparison could be advantageous to the proposed method though. The expressions are complicated. It seems that for any meaningful mask size, the non vanishing term will be too large. The theorem is not a valid convergence result as the authors do not show that the right hand side can indeed be provably made arbitrarily small by some choice of the parameters of the method. For instance, it is not guaranteed that $dist(m_{k,t}, m_k^*)$ will converge to zero. In this sense, calling this theorem "Convergence of personalized models" is incorrect and misleading. This is a fatal issue, unfortunately. The authors should make it absolutely clear that the result does not prove convergence.   6) Assumptions 1, 2, and 4 are very strong. For example, Assumption 2 is not provably satisfied for lower bounded nonconvex smooth functions when subsampling ( minibatching) is used to produce the stochastic gradient. Assumption 3 is also quite strong: it is not satisfied by convex quadratics. Assumption 1 is also strong   most recent works on FL do not require any similarity assumptions.   In summary, while this direction of research is interesting, the level of contributions in this work is marginal at best. The key theoretical result is misleading in that it does not imply convergence while it is marketed as such. Moreover, strong assumptions (relative to what is achieved in the latest papers) are used to obtain it. Because of these concerns, and other concerns raised by the reviewers, I do not have any other choice but to reject the paper.  Area Chair
This paper aims to estimate the 3D location and orientation of vehicle from a 2D image. Instead of using a CNN based 3D detection pipeline, the authors propose to detect the vehicle’s wheel grounding points and then using the ground plane constraint for the estimation. All three reviewers provided unanimous rating of rejection. Many concerns are raised by the reviewers, including poor generalization to new situations, small improvement over prior work, low presentation quality, the lack of detailed description of the experiments, etc. The authors did not respond to the reviewers’ comments. The AC agrees with the reviewers’ comments, and recommend rejection.
The reviewers all  appreciated the insights drawn from this study as well at its thoroughness. I want to commend both authors for running additional experiments to strengthen the paper and reviewers for updating the scores accordingly.  Congratulations.
The paper proposes Fourier temporal state embedding, a new technique to embed dynamic graphs.  However, the paper needs to be improved in writing, computational complexity analysis, and more thorough baseline comparisons.
The paper suggests a new measurement of layer wise margin distributions for generalization ability. Extensive experiments are conducted. Though there lacks a solid theory to explain the phenomenon. The majority of reviewers suggest acceptance (9,6,5). Therefore, it is proposed as probable accept.
This paper proposes a query efficient black box attack that uses Bayesian optimization in combination with Bayesian model selection to optimize over the adversarial perturbation and the optimal degree of search space dimension reduction. The method can achieve comparable success rates with 2 5 times fewer queries compared to previous state of the art black box attacks. The paper should be further improved in the final version (e.g., including more results on ImageNet data).
This paper presents work on multi task learning.  The reviewers appreciated the method based on SVD of loss gradients.  However, concerns were raised regarding empirical effectiveness and overall impact.  The reviewers considered the authors  response in their subsequent discussions.  While the methods are interesting, the concerns over their effectiveness would need to be more thoroughly addressed in order to improve the impact of the paper.  As such, it is encouraged that the authors take these suggestions into account in preparing a new version of the paper for a future submission.
The paper proposed local prior matching that utilizes a language model to rescore the hypotheses generate by a teacher model on unlabeled data, which are then used to training the student model for improvement. The experimental results on Librispeech is thorough. But two concerns on this paper are: 1) limited novelty: LM trained on large tex data is already used in weak distillation and the only difference is the use of multiply hypotheses. As pointed out by the reviewers, the method is better understood through distillation even though the authors try to derive it from Bayesian perspective. 2) Librispeech is a medium sized dataset, justifications on much larger dataset for ASR would make it more convincing. 
The reviewers acknowledge that the paper has some promising experiments. However, they think that the theoretical contributions are not rigorous, specifically the assumption in Theorem 2. It is true that the main part of the proof relies on this assumption. The main question is whether this assumption holds or not. The way that the authors provide some numerical verification is not convincing and does not necessarily help in this situation:   1) MNIST data set with LeNet is definitely not enough to represent all situations.  2) This assumption depends on the algorithm so the parameter choices are also very important.   Some reviewers and I agree that this assumption may hold in some scenarios, but assuming it (without proving it) would significantly reduce the contribution of the paper.   The following is the suggestion to improve the paper. Since this assumption is not standard and hard to verify, the authors should verify more experiments with various data sets and network architectures with difference choices of the algorithm parameters to have some sense whether this assumption may be true or not. Next, please try to show that this assumption holds or come up with different analysis with more reasonable assumptions.   The authors should consider to improve the theory to strengthen the paper and resubmit this paper in the future venues.  
The work proposed to learn causal structure of the environment and use the average causal effect of different categories of the environment, between the current and next state after performing an action as intrinsic reward to assist policy learning. While the reviewers find the ideas presented in the paper interesting and of potential, there are some concerns regarding proper introduction and comparison to related works, and clarity of the algorithm itself. While the two experimental results presented in the paper do show the potential of the work, it is missing an important baseline to disentangle the effectiveness of introducing the causal structure alone vs intrinsic reward. For example, how would A2C with curiosity or surprised based intrinsic reward, which also introduce the surprisingness of the next state as a result of performing an action as additional reward perform on these tasks?  
The paper proposed the use of a lossy transform coding approach to to reduce the memory bandwidth brought by the storage of intermediate activations. It has shown the proposed method can bring good memory usage while maintaining the the accuracy. The main concern on this paper is the limited novelty. The lossy transform coding is borrowed from other domains and only the use of it on CNN intermediate activation is new, which seems insufficient. 
This paper proposes to represent the distribution w.r.t. which neural architecture search (NAS) samples architectures through a variational autoencoder, rather than through a fully factorized distribution (as previous work did).   In the discussion, a few things improved (causing one reviewer to increase his/her score from 1 to 3), but it became clear that the empirical evaluation has issues, with a different search space being used for the method than for the baselines. There was unanimous agreement for rejection. I agree with this judgement and thus recommend rejection.
The paper proposed a method to evaluate latent variable based generative models by estimating the compression in the latents (rate) and the distortion in the resulting reconstructions. While reviewers have clearly appreciated the theoretical novelty in using AIS to get an upper bound on the rate, there are concerns on missing empirical comparison with other related metrics (precision recall) and limited practical applicability of the method due to large computational cost. Authors should consider comparing with PR metric and discuss some directions that can make the method practically as relevant as other related metrics. 
The paper proposes a novel method for representing persistence diagrams by embedding them to a Poincare ball.  The representation is learnable, and unifies essential and non essential features.   The experimental comparisons with existing representation methods show significant improvements in performance.    The flexible data driven embedding to a suitable geometric space is a novel idea, which will certainly advance the usefulness of TDA.  The  experimental resutls demonstrate well the advantage of the proposed repesentation.  The authors have also addressed the review comments appropriately, with some extra experiments.  This is also a good addition.   
This paper proposes a tree based method for interpretable policy learning, for fully offline and partially observable clinical decision environments. The models are trained incrementally, as patient information becomes available.   The method was overall deemed novel by the reviewers, and the interpretability of the model well validated by clinicians.  Numerous points of clarification were brought up by reviewers, related to the notation, learning process and result reporting. All of the concerns were responded to by the authors in great detail and the manuscript was appropriately revised. All the reviewers have raised their scores as a result of the updates.  Thus, the paper is ready for acceptance.
The paper proposed a new pipelined training approach to better utilize the memory and computation power to speed up deep convolutional neural network training. The authors experimentally justified that the proposed pipeline training, using stale weights without weights stacking or micro batching, is simpler and does converge on a few networks.   The main concern for this paper is the missing of convergence analysis of the proposed method as requested by the reviewers. The authors brought up the concern of the limited space in the paper, which can be addressed by putting convergence analysis into appendix. From a reader perspective, knowing the convergence property of the methods is much more important than knowing it works for a few networks on a particular dataset.    
Irrespective of their taste for comparisons of neural networks to biological organisms, all reviewers agree that the empirical observations in this paper are quite interesting and well presented. While some reviewers note that the paper is not making theoretical contributions, the empirical results in themselves are intriguing enough to be of interest to ICLR audiences.
This paper proposes Self Ensemble Adversarial Training (SEAT) for yielding a robust classifier by averaging weights of history models. The solution is different from an ensemble of predictions of different adversarially trained models. The authors also provided theoretical and empirical evidence that the proposed self ensemble method yields a smoother loss landscape and better robustness than both individual models and an ensemble of predictions from different classifiers.  The paper receives a mixed rating of 8 6 6 5 (after private discussion; initially it was 8 6 5 3), and all reviewers actively engaged in discussion. From the three positive reviewers, it is in general consensus that this paper has a clear motivation, is easy to follow, and owns reasonable (not exceptional) novelty. The negative reviewer poses a number of concerns, citing the absence of adaptive attack evaluation, the unclear difference between vanilla EMA and SEAT, and the proof of Proposition 1. The authors provided detailed responses and the negative reviewer was partially convinced (not fully) after viewing other comments.   AC carefully reads all discussions and feels this fall into a borderline case. The authors did solid work and there is no fatal concern as AC can see. The majority sentiment is that this is a good paper, just not an exciting one. Hence, the current recommendation is a borderline acceptance.
Reviewer rRp9 expressed concerns regarding the theoretical results included in Appendix A. In the discussion (not visible to the authors), the AC and Reviewer zn4a agree that the exposition in the original manuscript was confusing and could lead readers to assume these results were valid for the proposed algorithm. Also, in the original manuscript the presentation of the theoretical results in the appendix was quite poor (e.g. Proposition A.1). Having said that, the contributions and main points of the work are not affected by these observations as it is mainly an empirical study.  Following from the previous point, Reviewers rRp9 and zn4a pointed out that the overall presentation of the method, particularly the mathematical presentation could be improved.   Reviewer zn4a points out that the method is not particularly novel, this was also indicated as a weakness by Reviewer iyVU. The main contributions of the work are to simultaneously solve the tensor factorization and vector quantization problems usinga form of projected gradient descent (with hard thresholding). While the empirical results seem promising, are somewhat limited. The authors could make them stronger by studying other applications on top of image classification (e.g. semi supervised setting, object detection or segmentation).  In the discussion (not visible to the authors), Reviewer iyVU stated in light of the other reviews, he/she does not oppose rejecting the work.  Overall, the method is technically sound and produces promising results. In its current form, however, the paper is not yet ready for publication. The AC encourages the authors to incorporate the feedback and resubmit the work to a different venue.
This work performs a frequency domain analysis on gradient based adversarial perturbations. The authors argue that the perturbation deltas are largely concentrated in the high frequency domain and suggest a low pass filtering technique to improve the robustness of image classifiers. Reviewers raised several concerns as to how broadly applicable the given claims will hold, noting that these findings will be largely dependent on how the model was trained, what data the model is trained on, and how the adversarial perturbation will be constructed. Additionally, the proposal to apply a low pass filter to improve robustness is not new it has been attempted in several works in the past and current consensus is this only provides improved robustness to high frequency perturbations/corruptions. As a recent example, Yin et. al. studied the robustness properties of models trained with a low pass filter and found that while robustness to high frequency noise and perturbations is improved, this procedure also degrades robustness to low frequency corruptions and perturbations. By focusing the analysis quite narrowly to restricted l2 perturbations the work is missing critical nuances that are well known in the literature studying distribution shift. This AC recommends the authors connect their analysis to the broader issue of distribution shift, for example can the theory provide understanding into how to improve robustness to both high and low frequency corruptions?
This article studies the length of one dimensional trajectories as they are mapped through the layers of a ReLU network, simplifying proof methods and generalising previous results on networks with random weights to cover different classes of weight distributions including sparse ones. It is observed that the behaviour is similar for different distributions, suggesting a type of universality. The reviewers found that the paper is well written and appreciated the clear description of the places where the proofs deviate from previous works. However, they found that the results, although adding interesting observations in the sparse setting, are qualitatively very close to previous works and possibly not substantial enough for publication in ICLR. The revision includes some experiments with trained networks and updates the title to better reflect the contribution. However, the reviewers did not find this convincing enough. The article would benefit from a deeper theory clarifying the observations that have been made so far, and more extensive experiments connecting to practice. 
Thanks to the authors for the submission and the active discussion. The paper applies deep learning to the duration of stay estimation problem in the warehouse storage application. The authors provide problem formulation and describe the pipeline of their solutions, including datasets preparation and loss functions design. The reviewers agree that this is a good application paper that showcases how deep learning can be useful for a real world problem. The release of the dataset can also be a nice contribution. A major debate during the discussion is whether this paper is in scope of ICLR given that it is mostly a straightforward application existing techniques. After several rounds of discussion, reviewers think that this should fit under the category "applications in vision, ... , computational biology, and others." Overall, this paper can be a good example of applying deep learning to real world problems. 
This paper has been reviewed by three reviewers and received scores: 6/3/8. While two reviewers were reasonably positive, they also did not provide a very compelling reviews (e.g. one rev. just reiterated the rationale behind tensor model compression and the other admitted the paper is of limited novelty). Perhaps the shortest review (and perhaps the most telling) prompts authors to the fact that the model compression with tensor decompositions is quite common in the literature these days. One example could be T Net: Parametrizing Fully Convolutional Nets with a Single High Order Tensor by Kossaifi et al. Very likely the authors will find many more recent developments on model compression with/without tensor decomp. For a good paper in this topic, authors should carefully consider various tensor factorizations (Tucker, TT, tensor rings, t product and many more) and consider theoretical contributions and guarantees. Taking into account all pros and cons, this submissions falls marginally short of the ICLR 2020 threshold but the authors are encouraged to work on further developments.
This paper presents the use of Simulated Annealing (SA) for pruning and optimizing the architecture of a neural network. After reviewing the paper and taking into consideration of the reviewing process, here are my comments:   The contribution of the paper and the novelty is limited and not well presented   The related work is very sparse. It requires a major improvement.   The main concern is about the simplistic experiments and the lack of comparison between the results of the proposal and the SOTA methods.   Conclusions are not well supported by the results. From the above, the paper does not fulfill the standards of the ICLR.
The paper considers the problem of solving time constrained multi robot task allocation (MRTA) problems. Formulating the problem as a Markov decision process (MDP), the paper proposes Covariant Attention based Mechanism (CAM), a graph neural network based policy that can be trained to solve MRTA problems via standard RL methods. The encoder adapts the covariant compositional network to improve generalizability, while the decoder extends a recent combinatorial optimization architecture to the multi agent optimization domain. Experimental results demonstrate that CAM outperforms an encoder decoder baseline in terms of task completion, generalizability, and scalability, while also providing greater computational efficiency than non learning baselines.  The paper considers an important topic multi agent task allocation is an interesting and challenging combinatorial optimization problem. The proposed CAM architecture adapts existing components in an interesting way and seems sensible for the MRTA domain. The reviewers initially raised concerns regarding the conclusions that can be drawn from the experimental evaluation, the significance of the algorithmic contributions, as well as the motivation for the proposed approach. The authors made a concerted effort to address these concerns through the addition of new experimental evaluations (e.g., comparisons to a myopic baseline and ablation studies), updates to the text, and detailed responses to each reviewer. Unfortunately, only one reviewer responded and updated their review (increasing their score). In light of this, the AC also reviewed the paper. The AC agrees with the strengths identified by the reviewers (including those noted above) and with the contributions provided by the additional evaluations. However, the paper remains unnecessarily dense, while at the same time not being self contained (e.g., the new experimental results are relegated to the appendix rather than appearing in the main text). The paper would also benefit from a more concise motivation for learning based solutions to MRTA and a clearer discussion of the paper s contributions.
This paper proposed a variant of the existing training method "progressive stacking", and showed good empirical results comparing with the normal training procedure for BERT models. It contains some interesting points on the technical side, e.g, freezing the bottom layers when training newly added layers, but there are several concerns:  (1) This proposed method is called progressive stacking 2.0 but there is no comparison against the original progressive stacking in experiments. We had to check the empirical results in the original paper of progressive stacking, and did not notice any performance improvement of this new method over the original one; (2) The proposed method even introduced one more hyperparameter to tune: the number of top layers to copy.  This hyperparameter seems hard to choose. Different choices of this hyperparameter may dramatically impact the performance of this new method.  An ablation study on this should be conducted, e.g.,  what will the results look like if we only copy the last layer?  (3) The original progressive stacking method does not provide any practical guidance on how to determine the time to stack when the training goes. This severely limits the practical value of progressive stacking. If one stacks layers too early or too late, the stacking method may have no advantage at all. Unfortunately, this method called 2.0 still leaves this most critical issue away. 
The paper proposes a learning framework that allows for planning in continuous action spaces using tree search. Key to the approach is performing tree search over a discrete set of learned affordances that provide a compact abstraction of the action space that facilitates planning. The affordances are learned by passing gradients through a model based planner that uses learned models of the dynamics, reward, and state value functions. Experimental evaluations demonstrate the ability to perform tree search based planning using the learned affordances in a variety of domains for which tree search would otherwise be difficult.  The paper is topical, both with regards to its consideration of affordances as temporal abstractions that facilitate planning as well as the broader notion of integrating planning and learning. Several reviewers agree that the means by which affordances are learned by passing gradients through the planner is both interesting and novel. The reviewers also emphasize that the paper is well written and easy to follow, and that the approach is reproducible as a result. The reviewers raised a few concerns with the initial submission, notably the need for experimental comparisons to other recent baselines, which are important to clarifying the significance of the contributions, and the susceptibility to collapse in the affordance distribution. The authors clarified some of these questions and proposed adding comparisons to other baselines (e.g., DREAMER, for which there is already a comparison in the appendix), however it is not clear whether the submission was updated accordingly. The authors are encouraged to take this feedback into account and to include a more thorough experimental evaluation in any future version of the paper.
The authors propose a scheme to compress models using student teacher distillation, where training data are augmented using examples generated from a conditional GAN. The reviewers were generally in agreement that 1) that the experimental results generally support the claims made by the authors, and 2) that the paper is clearly written and easy to follow. However, the reviewers also raised a number of concerns: 1) that the experiments were conducted on small scale tasks, 2) the use of the compression score might be impractical since it would require retraining a compressed model, and is affected by the effectiveness of the compression algorithm which is an additional confounding factor. The authors in their rebuttal address 2) by noting that the student training was not too expensive, but I believe that this cost is task specific. Overall, I think 1) is a significant concern, and the AC agrees with the reviewers that an evaluation of the techniques on large scale datasets would strengthen the paper. 
The paper describes a clipping method to improve the performance of quantization. The reviewers have a consensus on rejection due to the contribution is not significant. 
The reviewers all appreciated the area explored by this work but there was a consensus that it lacked a thorough presentation of existing works, as well as relevant baselines.  I encourage the authors to better position their work with respect to the existing literature for what should be a stronger submission for a future conference.
The reviewers present strong concerns about the lack of novelty in the paper. Further there are strong concerns about how the experiments are conducted. I recommend the authors to carefully go through the reviews.
The paper presents a new Bayesian optimization method based on the Gaussian process bandits framework for black box adversarial attacks. The method achieves good performance in the experiments, which was appreciated by all the reviewers.  At the same time, the presentation of the method is quite confusing, which currently precludes acceptance of the paper. In particular, during the discussion phase the reviewers were not able to decipher the algorithm based on the description presented in the paper. It is not clear how the problem is modeled as a bandit problem, what the loss function $\ell$ is minimized and why minimizing it makes sense (assuming, e.g., that $\ell$ it the hinge loss as suggested and the initial prediction is good with a large margin, that is, the loss is zero, equation 6 never changes $x_t$ when the procedure is started from $x$). This connection, since it is the fundamental contribution of the paper, should be much better explained. Once the problem is set up to estimate (maximize?) the reward, it is changed to calculating the difference in the minimization (cf. equation 11), which is again unmotivated. (Other standard aspects of the algorithm should also be explained properly, e.g., the stopping condition of Algorithm 1)  Unfortunately, the paper is written in a mathematically very imprecise manner. As an example, consider equation (6), where $B_p$ and the projection operator are not defined, and while these can be guessed, a projection of the argmin seems to be missing as well in the end (otherwise nothing guarantees that $x_T$, which is the final outcome of the algorithm, remains in the $L_p$ ball). Another example is the $Discrete\ Approximate\ CorrAttack_{Flip}$ paragraph which requires that every coordinate of $x$ should be changed by  $\pm\epsilon$. It is also not clear what "dividing the image into several blocks" means in Section 4.1 (e.g., are these overlapping, do they cover the whole image, etc., not to mention that previously $x$ was a general input, not necessarily an image). It is also unlikely that the stopping condition in Algorithm 1 would use the exact same $\epsilon$ for the acquisition function as the perturbation radius for adversarial examples, etc. While some of these inaccuracies and unclear definitions are also mentioned in the reviews, unfortunately there are more in the paper.  The authors are encouraged to resubmit the paper to the next venue after significantly improving and cleaning up the presentation.  
The paper studies why existing deep GCNs suffer from poor performance and propose DGMLP to improve over existing GCNs. However, the reviewers think there are still many unjustified claims and the paper. Further, several reviewers question about the novelty of the proposed method, which seems to be a combination of existing approaches.   I suggest the authors to revise the paper by defining terms like model degradation and smoothness mathematically and try to justify each claim (e.g., the effect of disentangling) with solid experiments. These will significantly improve the analysis part and make the conclusions stronger.
This paper proposes an alternative explanation of the emergence of oriented bandpass filters in convolutional networks: rather than reflecting observed structure in images, these filters would be a consequence of the convolutional architecture itself and its eigenfunctions.  Reviewers agree that the mathematical angle taken by the paper is interesting, however they also point out that crucial prior work making the same points exists, and that more thorough insights and analyses would be needed to make a more solid paper. Given the closeness to prior work, we cannot recommend acceptance in this form.
The paper points out how set equivariant functions limit the types of functions that can be represented on multisets. They develop an new notion of multiset equivariance to address this limitation. The paper improves an existing multi set equivariant Deep Set Prediction Network through implicit differentiation, which is an area of rising interest. The reviewers and I note that the paper is well written.
This manuscript introduces a theoretical framework to analyze the sim2real transfer gap of policies learned via domain randomization algorithms. This work focusses on understanding the success of existing domain randomization algorithms through providing a theoretical analysis. The theoretical sim2real gap analysis requires two critical components: *uniform sampling* and *use of memory*  **Strengths** All reviewers agree that this manuscript provides a strong theoretical analysis for an important problem (understanding sim2real gap) well written manuscript, and well motivated Intuitive understanding for theoretical analysis is provided   **Weaknesses** analysis is limited to sim2real transfer without fine tuning in the real world the manuscript doesn t provide a novel experimental evaluation lack of take aways  **Rebuttal** The authors acknowledge the limitation of not addressing fine tuning, but also point out that several papers have performed sim2real transfer without fine tuning.  The authors address the lack of novel experimental evaluation by arguing that the theoretical analysis can be directly linked to existing algorithms for which empirical evaluations have already been performed. I agree with the authors that in that context it seems of little value to redo those experiments. However, I also believe that those links could be made even clearer in the manuscript and I would encourage the authors to do so. Furthermore, while the authors do provide intuitive take aways for domain randomization algorithms, it would be helpful if those take aways were more clearly linked to existing algorithms as well (given that there is no experimental evaluation of this).   **Summary** This manuscript provides a theoretical framework for analyzing the sim2real gap and using that framework provides bounds on the sim2real gap. All reviewers agree this is a strong theoretical analysis. Some take aways on what makes domain randomization algorithms successful are provided by the provided sim2real gap analysis (memory use, uniform sampling). Thus I recommend accept.
The paper recieved three consistently positive reviews. While I agree with most of them, I have two major concerns regarding the novelty of the paper, which the authors are strongly recommended to address in the final version.  1. Taking derivative with respect to the parameters of transformation isn t novel. The standard tangent prop algorithm has been around for over a decade:  P. Simard, Y. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern recognition tangent distance and tangent propagation. In Neural Networks: Tricks of the Trade. 1996. (see Eq 26 in https://halshs.archives ouvertes.fr/halshs 00009505/document)  Salah Rifai, Yann N. Dauphin, Pascal Vincent, Yoshua Bengio, Xavier Muller. The Manifold Tangent Classifier. NIPS 2011. (see Eq 6 therein)  I understand there is some normalization in Eq 5, sampling of \alpha, \alpha , and the direction, and using the expectation to approximate the norm of the gradient. But such novelty is really incremental, or at least some empirical comparison will be necessary. It will also be necessary to cite the tangent distance/prop literature.  2. The new gradient based regularizer in Eq 11 and 12 appears completely decoupled with contrastive learning. It can be applied to any representation learning where f_\theta is an encoder. It does not use any substantial element from contrastive learning, although it might be "inspired" by contrastive learning. One may argue that such generality is an advantage, but 1) there is really no need to take such a big detour into contrastive learning just in order to derive the invariance regularizer in Eq 12, and 2) writing in this way can be quite confusing and/or misleading.
The paper shows that using graph neural networks to address multi task control problems with incompatible environments does not provide benefits to the learning process. The authors instead propose to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems.  The paper is well written and the analysis of the literature has been appreciated.  The contribution is original and relevant to the community.  All the reviewers agree that this paper deserves acceptance. We invite the authors to modify the paper by following the suggestions provided by the reviewers. In particular:   improve the analysis of the empirical results   update the plots   add the suggested references
The paper proposes and approach for model based reinforcement learning that adds a constraint to encourage the predictions from the model to be consistent with the observations from the environment. The reviewers had substantial concerns about the clarify of the initial submission, which has been significantly improved in revisions of the paper. The experiments have also been improved. Strengths: The method is simple, the performance is competitive with state of the art approaches, and the experiments are thorough including comparisons on seven different environments. Weaknesses: The main concern of the reviewers is the lack of concrete discussion about how the method compares to prior work. While the paper cites many different prior methods, the paper would be significantly improved by explicitly comparing and contrasting the ideas presented in this paper and those presented in prior work. A secondary weakness is that, while the results appear to be statistically significant, the improvement over prior methods is still relatively small. I do not think that this paper meets the bar for publication without an improved discussion of how this work is placed among the existing literature and without more convincing results.  As a side note, the authors should consider comparing to the below NeurIPS  18 paper, which significantly exceeds the performance of Nagabandi et al  17: https://arxiv.org/abs/1805.12114
This paper investigates neural networks for group comparison   i.e., deciding if one group of objects would be preferred over another. The paper received 4 reviews (we requested an emergency review because of a late review that eventually did arrive). R1 recommends Weak Reject, based primarily on unclear presentation, missing details, and concerns about experiments. R2 recommends Reject, also based on concerns about writing, unclear notation, weak baselines, and unclear technical details. In a short review, R3 recommends Weak Accept and suggests some additional experiments, but also indicates that their familiarity with this area is not strong. R4 also recommends Weak Accept and suggests some clarifications in the writing (e.g. additional motivation future work). The authors submitted a response and revision that addresses many of these concerns. Given the split decision, the AC also read the paper; while we see that it has significant merit, we agree with R1 and R2 s concerns, and feel the paper needs another round of peer review to address the remaining concerns.
This paper proposes a deep clustering method based on normalized cuts.  As the general idea of deep clustering has been investigated a fair bit, the reviewers suggest a more thorough empirical validation.  Myself, I would also like further justification of many of the choices within the algorithm, the effect of changing the architecture.
The paper presents a framework for modeling dynamical systems by combining prior knowledge available as ODE and implemented via a differentiable solver, with statistical modules. This is a key problem consisting in complementing available partial knowledge on a physical system with information extracted from available data with agnostic statistical methods. In their framework, both the ODE parameters and the residual model parameters are learned. Experiments are performed on synthetic and on a simplified but realistic problem.   All the reviewers do agree that the topic is important and that the paper has merits and brings an interesting contribution. They highlight some weaknesses in the presentation and more importantly in the experimental assessment. Overall, this is a good paper that should still be somewhat improved for publication. The authors are encouraged to investigate further the analysis of their framework in different settings and to bring more experimental evidence.
This paper studies discontinuities (i.e., holes) in the latent space of text VAE. Analysis of previous hole detection methods are conducted, and a new efficient hole detection algorithm is proposed. It is an interesting work, but the paper in its current form has a few weaknesses/flaws regarding the proposed algorithm, experiment designs and the resulting conclusions. Reviewers have made various constructive suggestions, which the authors acknowledged.
The authors make a case for a phenomenon of deep network training that they call the "silent alignment effect": that, while the training error is still large, the NTK associated with the network aligns its eigenvectors with key directions in "feature space".  They support this with non rigorous theoretical analysis of linear networks, and extensive experiments with real networks on real data. The consensus view was that this paper provides novel and useful insight into training dynamics, in particular regarding feature learning.
This paper proposes a novel and powerful data augmentation strategy for few shot learning, producing convincing improvements over current approaches. The request by the reviewers to include additional ablations, more backbones, and an additional dataset have been satisfactorily  resolved, with the results remaining strong. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. 
The reviewers agree that addressing long horizon tasks with off line learning and fine tuning afterwards from demonstrations is an interesting and relevant topic. The technical ideas about learning a relevance metric to select relevant off line data, and to learn an inverse skill dynamics models. The experimental results are convincing, even if success rates are sometimes lower than expected. All reviewers recommend acceptance of the paper.
Issues raised by the reviewers have been addressed by the authors, and thus I suggest the acceptance of this paper.
All reviewers recommend reject, and there is no rebuttal.
Straight Through is a popular, yet not theoretically well understood, biased gradient estimator for Bernoulli random variables. The low variance of this estimator makes it a highly useful tool for training large scale models with binary latents. However, the bias of this estimator may cause divergence in training, which is a significant practical issue. The paper develops a Fourier analysis of the Straight Through estimator and provides an expression for the bias of the estimator in terms of the Fourier coefficients of the considered function.   The paper in its current form is not good enough for publication, and the reviewers believe that the paper contains significant mistakes when deriving the estimator. Furthermore, the Fourier analysis seems unnecessary. 
The paper presents some exciting results on the convergence of averaged SGD for overparameterized two layer neural networks. The AC and reviewers all agree that the contributions are significant and well presented, and appreciate the author feedback to the reviews. The corresponding revisions on assumptions and references, and the added simplified proposition in the introduction have nicely improved the manuscript. 
This paper proposes a Conditional Masked Language Modeling (CMLM) method to enhance the MLM by conditioning on the contextual information.   All of the reviewers think the results are good. However, the reviewers also think the intuition and experiments are not so convincing. The responses and revisions still not satisfy all the reviewers  major concern.
The paper shows that standard transformers can be trained to generate satisfying traces for Linear Temporal Logic (LTL) formulas. To establish this, the authors train a transformer on a set of formulas, each paired with a single satisfying trace generated using a classical automata theoretic solver. It is shown that the resulting model can generate satisfying traces on held out formulas and, in some cases, scale to formulas on which the classical solver fails.  The reviewers generally liked the paper. While the transformer model is standard, the use of deep learning to solve LTL satisfiability is novel. Given the centrality of LTL in Formal Methods, the paper is likely to inspire many follow up efforts. There were a few concerns about the evaluation; however, I believe that the authors  comments address the most important of them. Given this, I am recommending acceptance. Please add the new experimental results (about out of distribution generalization) to the final version of the paper. 
The submitted paper is well written and easy to follow and also the idea of using VAEs for making inferences about the opponents on which a policy can be conditioned on is sensible. Also the reported performance in comparison to two baselines is good (although I have concerns about the selection of the baselines—see below). Acceptance of the paper was suggested by 3 of the reviewers and rejection by one of the reviewers. While I don’t share all concerns of the negative reviewer, I also suggest to reject the paper.  My suggestion to reject the paper is mainly based on seeing concerns of the positive reviewers more critical as these reviewers themselves and some concerns I have on my own. In particular, I don’t think that all MARL approaches can simply be discarded for comparison—no matter whether the opponents are learning or not. Regarding the evaluation, I think that an environment with real opponents must be considered and that robustness is a key property that should be studied (otherwise an approach with a fixed set of best response policies and inference about the opponent might perform as well). In that regard I also find the selection of baselines insufficient—the minimum I would expect is to consider a NOM baseline using an RNN (which as far as I can tell is not the case) such that it could make inferences about the opponent.  I want to acknowledge that the paper improved quite a lot during the rebuttal period in which the authors extended their discussion of related work on opponent modeling.  In summary, the paper could be improved substantially by an extended empirically study (more environments + baselines + "mismatch" settings). If the currently observed performance gains also hold in these settings, this can become a good paper but in its current form I think the paper is not demonstrating that the proposed approach performs favorably over natural baselines and works well against real opponents. 
This paper investigates methods for producing and evaluating interval forecasts rather than point forecasts. The authors focus on spatio temporal forecasts whose interval accuracy is measured with the Mean Interval Score.  Pros:  Uncertainty quantification is an important topic that is often ignored in the ML literature where the focus remains on point predictions. The COVID 19 example the authors use is a clear example of the need for such methods: the CDC sponsored COVID forecast hub mandates interval forecasts.  Cons:  In the words of one reviewer "This paper is almost a review paper", a statement with which I fully concur. Neither the evaluation metric (MIS) nor the methods for generating intervals are novel. There is no particular effort to argue "why MIS and not weighted interval score" or "why these methods". If it were a review paper with more comprehensive coverage of the relevant state of the art (a deficiency mentioned by multiple reviewers), it may make a more positive impression. As is, the story is mainly incomplete. To resubmit at another venue, I would suggest the authors either (a) state clearly what is new here in this paper or (b) make it a review paper that more comprehensively evaluates and discusses current state of the art. The statements in the last paragraph of page 1 read more like the authors are shooting for (b) than (a): "we conduct a systematic study", "we investigate". Finally, the evaluations undertaken here do not really make use of (or correct for) the spatio temporal task. Presumably some locations/time periods are more difficult than others. So simply averaging over everything (as in Table 1) doesn t take the structure into account. It is likely better to examine relative accuracy to strawman forecaster or perhaps use a random effects model.
This work presents a method to model embeddings as distributions, instead of points, to better quantify uncertainty. Evaluations are carried out on a new dataset created from mixtures of MNIST digits, including noise (certain probability of occlusions), that introduce ambiguity, using a small "toy" neural network that is incapable of perfectly fitting the data, because authors mention that performance difference lessens when the network is complex enough to almost perfectly fit the data.   Reviewer assessment is unanimously accept, with the following points:  Pros: + "The topic of injecting uncertainty in neural networks should be of broad interest to the ICLR community." + "The paper is generally clear." + "The qualitative evaluation provides intuitive results."  Cons:   Requirement of drawing samples may add complexity. Authors reply that alternatives should be studied in future work.   No comparison to other uncertainty methods, such as dropout. Authors reply that dropout represents model uncertainty and not data uncertainty, but do not carry out an experiment to compare (i.e. sample from model leaving dropout activated during evaluation).   No evaluation in larger scale/dimensionality datasets. Authors mention method scales linearly, but how practical or effective this method is to use on, say, face recognition datasets, is unclear.    As the general reviewer consensus is accept, Area Chair is recommending Accept; However, Area Chair has strong reservations because the method is evaluated on a very limited dataset, with a toy model designed to exaggerate differences between techniques. Essentially, the toy evaluation was designed to get the results the authors were looking for. A more thorough investigation would use more realistic sized network models on true datasets.  
Since this seems interesting, I suggest to accept this paper at the conference. However, there are still some serious issues with the paper, including missing references. 
The paper  builds  fast and high quality SMILES based molecular embeddings  by distilling  state of the art graph based models teachers. This has the advantage of speeding inference time w.rt to graph based methods.   The reviews were split regarding the motivation of the work, in the sense of why not train directly on SMILES instead of distilling graph based methods that are in some tasks behind SMILES transformer. Authors provided clarifications in the rebuttal showing that on Knowledge distillation of graph models  surpasses  SMILES only model training.   I think given the experimental nature of the paper the main motivation of the paper should be better clarified and supported with more experimentation and downstream tasks.
This paper introduced a new ODE integration scheme that allows constant memory gradient computation.  I was concerned that the low order of convergence of this method would make it impractical, but the authors performed extensive experiments and got impressive results.  Overall the paper addresses one of the main practical difficulties with large neural ODE models.  The authors satisfactorily addressed the reviewers  concerns in the discussion.
This paper investigated online safe reinforcement learning problem in the constraint MDP setting. By introducing Safety Agent and Task Agent, the authors translate the RL problem into a Markov game. The AC agrees with all reviewers that there is a lack of theoretical analysis and experimental comparisons with existing benchmarks. It has not reached the bar of ICLR papers.
This paper presents a recurrent tree structured linear dynamical system to model the dynamics of a complex nonlinear dynamical system. All reviewers agree that the paper is interesting and useful, and is likely to have an impact in the community. Some of the doubts that reviewers had were resolved after the rebuttal period.   Overall, this is a good paper, and I recommend an acceptance.
The reviewers mostly raised two concerns regarding the paper: a) why this algorithm is more interpretability than BP (which is just gradient descent); b) the exposition of the paper is somewhat confusing at various places; c) the lack of large scale experiment results to show this is practically relevant. In the AC s opinion, a principled kernel based approach can be counted as interpretable, and there the AC would support the paper if a) is the only concern. However, c) seems to be a serious concern since the paper doesn t seem to have experiments beyond fashion MNIST (e.g., CIFAR is pretty easy to train these days) and doesn t have experiments with convolutional models. Based on c), the AC decided that the paper is not quite ready for acceptance. 
This paper proposes a method to improve word embedding by incorporating sentiment probabilities. Reviewer appreciate the interesting and simple approach and acknowledges improved results in low frequency words.  However, reviewers find that the paper is lacking in two major aspects: 1) Writing is unclear, and thus it is difficult to understand and judge the contributions of this research. 2) Perhaps because of 1, it is not convincing that the improvements are significant and directly resulting from the modeling contributions.  I thank the authors for submitting this work to ICLR, and I hope that the reviewers  comments are helpful in improving this research for future submission.
The paper presents a new take on exploration in multi agent reinforcement learning settings, and presents two approaches, one motivated by information theoretic, the other by decision theoretic influence on other agents. Reviewers consider the proposed approach "pretty elegant, and in a sense seem fundamental", the experimental section "thorough", and expect the work to "encourage future work to explore more problems in this area". Several questions were raised, especially regarding related work, comparison to single agent exploration approaches, and several clarifying questions. These were largely addressed by the authors, resulting in a strong submission with valuable contributions.
The premise of this paper is that the development of time series forecasting methods has traditionally focused on accuracy rather than other criteria such as training time or latency. This work presents a new benchmark, evaluating classical and deep learning based methods on a number of public datasets. They also propose a technique, ParetoSelect that is able to select models from the Pareto front that can efficiently select models in a multi objective setting.   Reviewer XSBL liked the observation that classical methods do not always beat deep learning methods even for very small datasets. They thought that the empirical contribution was valuable and myth breaking. They also commented that the evaluation was robust. Their main concerns were: inadequate description of hyperparameters, lack of evaluations on *really* small datasets, missing confidence measures for latency results. They also made some suggestions for improving clarity. The reviewers responded, pointing to a description of the hyperparameters in Table 2 of the appendix. They also responded to the reviewer’s comment about very small datasets and explained the advantage of the ranking loss. They made some small adjustments to the paper based on the clarity comments.   Reviewer PZ2f also thought that the large scale comparison was valuable for the community. Overall they thought it was well written though could be improved w.r.t. Notation and writing style. They even inspected the code. Their primary concern was that the paper lacked focus and “tries to do too much and too little”. Is this a benchmarking effort of previous methods, or is the main contribution the ParetoSelect algorithm? This reviewer thought that due to its superficial coverage of too many things, and it wasn’t ready yet for publication at ICLR. The authors provided quite a comprehensive response to reviewer PZ2f and pointed to some minor improvements in the manuscript.  Reviewer rQb3, like the others, viewed the benchmark analysis as valuable. They thought that the ParetoSelect approach was “natural” and that it was shown to be effective over baselines. Like PZ2f they had some structural criticisms and pressed for more insights.   Reviewers XSBL and rQb3 continued to engage in discussion through the AC reviewer discussion phase. XSBL said that the authors’ response addressed some concerns yet raised others w.r.t. hyper parameter selection. rQb3 updated their review after considering the author s response, feeling that minor concerns were addressed but the paper could still use further development. Overall, after considering the discussion I think that it’s been difficult for the authors to provide any patterns regarding which model performs best for which datasets. To me, a benchmark paper should provide some deeper insight and the paper appears to be struggling to do that. On the other hand, the study is comprehensive. The authors have argued in their response to all reviews, that their evaluation is at quite a different scale compared to other published time series model evaluations. I think that this benchmark paper can provide value to the community yet it could use further work: specifically the authors need to focus the paper and communicate clearer insights from the study.
This paper investigates the role of BPE and vocabulary sizes in memorization in transformer models. Through a series of experiments on random label prediction, training data recovery and membership inference attacks, the paper shows that larger vocabulary sizes lead to improved memorization. The Reviewers all agree that the paper investigates an important question and does so thoroughly. The main concerns were about: (1) the validity of the conclusion that it is sequence length indeed which affects memorization; and (2) the lack of more tasks to validate the findings. For (1) the authors added another set of experiments which further rule out frequency effects as a factor, but I agree with Reviewer KAZC that more evidence is needed which directly shows that sequence length is responsible (e.g. are shorter PAQ questions memorized better?). For (2) the authors shared a google drive link with additional results on NMT after the deadline, which the reviewers appreciated. Overall, however, the paper needs more work in order to unify all these results in a single draft.
Overall, this paper receives negative reviews due to limited technical novelty and contributions. The reviewers discuss extensively on the merits of this work after the rebuttal phase. However, the authors  rebuttal does not address all the raised concerns. As such, the area chair agrees with the reviewers and does not recommend it be accepted at this conference
Pros:   The paper is well written and clear and presented with helpful illustrations and videos.   The  training methodology seems sound (multiple random seeds etc.)   The results are encouraging.  Cons:   There was some concern generally about how this work is positioned relative to related work and the completeness of the related work.  However, the authors have made this clearer in their rebuttal.  There was a considerable amount of discussion between the authors and all reviewers to pin down some unclear aspects of the paper. I believe in the end there was good convergence and I thank both the authors and reviewers for their persistence and dilligence in working through this.  The final paper is much better I think and I recommend acceptance.
This paper proposes an algorithm for training sequence to sequence models from scratch to optimize edit distance. The algorithm, called optimal completion distillation (OCD), avoids the exposure bias problem inherent in maximum likelihood estimation training, is efficient and easily implemented, and does not have any tunable hyperparameters. Experiments on Librispeech and Wall Street Journal show that OCD improves test performance over both maximum likelihood and scheduled sampling, yielding state of the art results. The primary concerns expressed by the reviewers pertained to the relationship of OCD to methods such as SEARN, DAgger, AggreVaTe, LOLS, and several other papers. The revision addresses the problem with a substantially larger number of references and discussion relating OCD to the previous work. Some issues of clarity were also well addressed by the revision.
Thank you for your submission to ICLR.  This paper presents a straightforward but reasonable approach to (slightly) improving the performance of large batch training via adversarial training.  The basic approach is to apply (small epsilon) adversarial training, shown to help performance in small batch settings, but accelerate the method using stale parameters to allow for parallel computation of the perturbations.  This speeds up adversarial training while improving performance, enough to enable it to be more effective than existing techniques for this large batch setting.  The reviewers are not entirely in agreement about this paper, but I personally found the objections of the reviewer to be fairly generic, and not really addressing the core contributions of the paper.  However, I also felt that the overall contribution of this work seems somewhat incremental, using a not particularly unexpected result (that we can use stale gradients for this form of adversarial training) to achieve moderate speedup in what ultimately seems like one point in hyperparameter space.  That all being said, though, clearly the authors are working within standard benchmark frameworks, and "simple" algorithms here are indeed a positive rather than a negative.  So I am inclined to slightly recommend the paper for acceptance.
The paper uses adversarial data to improve generalization in Programming By Example (PBE). The reviews were somewhat mixed with some people finding this useful and interesting while others finding it straightforward and unsurprsing. The reviewers were not convinced of the ultimate usefulness of the approach since it is evaluated on toy or synthetic datasets. The clarity of the presentation could also be improved.
This paper combines recently emerging NTK theory and kernels with DEQ models. In particular the authors use the root finding capability of DEQ models to compute the corresponding NTK of DEQ models for fully connected and convolutional variants. The reviewers raised various concerns including lack of experimental details, incremental theoretical results(which the authors agree with but postulate that this is a practical paper), lack of proper literature review, explaining how it applies in practical scenarios and grammatical mistakes. Some of these concerns were addressed during the response period but none of the reviewers were fully satisfied with the author s response. While I think there are interesting ideas in this paper I agree with the reviewers that a substantial revision is required and therefore recommend rejection.
While the reviewers appreciated the aim of the work, they found the technical contribution to be too incremental to be of sufficient interest and the exploration of the problem and its significance to be incomplete in the paper s current state.
This paper proposes a learning to optimize approach that is "flatness aware", i.e. it tries to find flatter minima in the loss landscape. The idea is accompanied by both theoretical and empirical verification. However, in the current state, the paper did not convince the reviewers about its potential impact. In particular, reviewer ZY6B points out that comparison with "sharpness aware" minimization (SAM), which is a non learned optimizer for seeking flat minima, is an important comparison that is currently missing. Another issue mentioned by the reviewers  ZY6B and wp2U relates to the lower performance of the learned optimizer compared to standard SGD in large models (lower than 80% test accuracy). These reviewers are interested in the question of whether this method can still produce good results in the competitive setting (e.g. > 90% accuracy on CIFAR). Considering the performance/baseline issue even on small datasets such as MNIST and CIFAR, the impact of the proposed method is unclear. I encourage authors to adopt more conventional baselines to better indicate the potential impact of their method, and do consider comparison with optimizers that directly aim to improve flatness, such as SAM.
This paper proposes an analysis of signSGD in some special cases. SignGD has been shown to be of interest, whether because of its similarity to Adam or in quasi convex settings.  The complaint shared by reviewers was the strength of the conditions. SGC is really strong, I have yet to see increasing mini batch sizes to be used in practice (although there are quite a  few papers mentioning this technique to get a convergence rate) and the strength of the other two are harder to assess. With that said, the improvement compared to existing work such as Karimireddy et. al. 2019 is unclear.  I encourage the authors to address the comment of the reviewers and to submit an improved version to a later, or perhaps to a more theoretical, convergence.
This paper examines the time dependent generalization behavior of high dimensional student teacher linear regression models. It introduces a simple two scale covariance model and examines the exact solutions for the dynamics, finding a tradeoff between the fast  and slow learning features, leading to epoch wise double descent. Qualitative comparisons are made with the SGD dynamics of ResNe18 on CIFAR 10.  The reviewers offer split opinions on this work, with most reviewers finding strength in exhibiting the complex behavior of epoch wise double descent in a simple and analytically tractable setting. Weaknesses highlighted in the discussion include clarity, discussion of prior work, and rigor of the analyses.  I believe a clear demonstration and analytical explanation for epoch wise double descent would certainly be of interest to the ICLR community, and I concur with the reviewers who emphasize these strengths of the paper. However, as one reviewer mentioned, this paper is primarily a theoretical work, and as such, the main theoretical advancements over prior work should be clear, and the novel results should be sufficiently rigorous. In this regard, the paper is lacking, as detailed below.  First of all, the discussion of SGD is imprecise, with no explicit definition of the optimization method that is actually being performed. What is the batch size? How is the sampling performed? What is the learning rate/schedule? The formulas in Secs. 2.1 2.2 suggest that full batch gradient descent is being performed. In Sec. 2.3, stochasticity from SGD is induced via a Gibbs distribution. However, contrary to the discussion, I don t think that this is a "well known" **result** (though of course it is a well known **model**), and in high dimensions I am not sure it is even correct (see e.g. [1]).  Second of all, even assuming the Gibbs distribution, the substitution on line (23) is only justified in words, whereas the cited results from Ali et al., 2020, only provide a bound. What is meant by "$\approx$"? Some discussion is given about this step of the derivation, but more precise statements would really help make the argument convincing.  Finally, the derivations seem to rely on the replica method from statistical physics, which is not rigorous. While I am generally supportive of such methods for technically challenging problems that do not readily admit alternative analyses, given the simplicity of the linear model setup here, I believe a more rigorous approach would not be prohibitively difficult. At the very least, some acknowledgement should be given about the lack of rigor in the derivation.  Overall, this paper presents a simple and analytically tractable model that sheds light on the importance phenomenon of epoch wise double descent. Unfortunately, the presentation is not sufficiently clear and the derivations not sufficiently rigorous to merit publication at this time.  [1] Paquette, Courtney et al. “SGD in the Large: Average case Analysis, Asymptotics, and Stepsize Criticality.” COLT (2021).
This paper applies an existing tool (copulas) to MARL to represent dependencies between variables.  The reviewers appreciate the use of the copulas for this problem. The experimental section shows promising results on several problems. I appreciate that the authors have answered and addressed many points of concerns of the reviewers. The paper is well written  The reviewers seem to see this paper as a first step only, showing promising results but of moderate significance in itself. In particular, reviewer 3 would like to see more justifications for the use of the copulas, and a more experimental settings would make a stronger paper.  
The paper proposes a method for hybrid model based/ML learning, where a model is decomposed into an interpretable parametric prior and a neural net residual.  In this case, the prediction error minimization does no identify the parametric component, and an alternating optimization method is proposed to augments prediction error loss with component specific losses. Empirical and theoretical results are obtained.  Initial questions of several reviewers were addressed.
To solve imbalance classification problem, this paper proposes a method to learn example weights together with the parameters of a neural network. The authors proposed a novel mechanism of learning with a constraint, which allows accurate training of the weights and model at the same time. Then they combined this new learning mechanism and the method by Hu et al. (2019), and demonstrated its usefulness in extensive experiments.  I would like to thank the authors for their detailed feedback to the initial reviews, which clarified most of the unclear points in the manuscript. Overall the paper is well written and the effectiveness was demonstrated in experiments. Since the contribution is valuable to ICLR2022, I suggest its acceptance.
The paper received reviews from experts in representation of invariant functions. They all have expressed concerns regarding the novelty of the technical contributions, and the lack of appropriate comparisons to existing results. This applies in particular to representation of symmetric functions using neural networks which was largely covered by previous works, as acknowledged by the authors. The authors are encouraged to consider the valuable inputs by the reviewers and revise accordingly. 
Three experts in the field recommend accepting the paper (ratings 7,7,6) after the author response, appreciating the improvements the authors made. [Note: The AC is mainly disregarding R3 s rating, as R3 did not respond to the early request of the AC to clarify their review, did not respond to the authors request for clarification, and did not participate in any discussion past their initial short review.]  The solid experimental evaluation and an original methodology for zero shot learning speak for accepting the paper.  [The area chair is certain about accepting the paper, but not fully confident if it should be Poster or Spotlight.]     
This paper tackles the problem of exploration in Deep RL in settings with a large action space. To this end, the authors introduce an intrinsic reward inspired by the exploration bonus of LinUCB. This novel exploration method called anti concentrated confidence bounds (ACB) provably approximates the elliptical exploration bonus of LinUCB by using an ensemble of least squares regressors. This allows ACB to bypass costly covariance matrix inversion, which can be problematic for high dimensional problems (hence allowing it to be used in large state spaces). Empirical experiments show that ACB enjoys near optimal performance in linear stochastic bandits. However, experiments on Atari benchmark fail to show any practical advantage of ACB over current methods, neither computation nor performance wise. That being said, the proposed ACB approach is theoretically transparent, which contributes to advancing our theoretical understanding of usable intrinsic rewards in deep RL and can inform theoretically motivated directions for improvement and further research, while being on par with SOTA. I believe that this makes the contribution of this work strong enough for acceptance.
The general consensus is that this method provides a practical and interesting approach to unsupervised domain adaptation. One reviewer had concerns with comparing to state of the art baselines, but those have been addressed in the revision.  There were also issues concerning correctness due to a typo. Based on the responses, and on the pseudocode, it seems like there wasn t an issue with the results, just in the way the entropy objective was reported.  You may want to consider reporting the example given by reviewer 2 as a negative example where you expect the method to fail. This will be helpful for researchers using and building on your paper.
The paper is well written, it is clear and concise. The idea of learning to generate text from off policy demonstrations is interesting. The results experimental results are good. The authors seem to address the concerns raised by the authors during the rebuttal.
* presents a novel way analyzing GANs using the birthday paradox and provides a theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse * significant contribution to the discussion of whether GANs learn the target disctibution * thorough justifications
This paper proposed a method to reduce the memory of training neural nets, in exchange for additional training time. The paper is simple and looks reasonable. It s a natural followup with previous work.  The improvement over previous work is not significant, with some overhead incurred in training time. This is a borderline paper but given the <30% acceptance rate, I need to downgrade the paper to reject. 
The paper proposes a novel method for greed layer wise training by considering the learning signal from either backprop or from the additional auxiliary losses. SEarching for DecOupled Neural Architecture learns to identify the decoupled blocks by learning the gating parameters similar to gradient based architecture search algorithms, such as DARTs.  The empirical experiments demonstrated the effectiveness of SEDONA on CIFAR and TinyImageNet using various ResNet architectures. Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in the way that satisfied the reviewers. The ideas in this paper are interesting and are broadly applicable. Additional experiments / discussions on the tradeoff between initial search cost and accuracy should be included in the final version. 
All reviewers gave "accept" ratings. it seems that everyone thinks this is interesting work.  The paper generated a large number of anonymous comments and these were addressed by the authors. 
The paper proposes a "compressive transformer", an extension of the transformer, that keeps a compressed long term memory in addition to the fixed sized memory.  Both memories can be queried using attention weights.  Unlike TransfomerXL that discards the oldest memories, the authors propose to "compress" those memories.  The main contribution of this work is that that it introduces a model that can handle extremely long sequences. The authors also introduces a new language modeling dataset based on text from Project Gutenberg that has much longer sequences of words than existing datasets.  They provide comprehensive experiments comparing against different compression strategies and compares against previous methods, showing that this method is able to result in lower word level perplexity. In addition, the authors also present evaluations on speech, and image sequences for RL.  Initially the paper received weak positive responses from the reviewers. The reviewers pointed out some clarity issues with details of the method and figures and some questions about design decisions. After rebuttal, all of the reviewers expressed that they were very satisfied with the authors responses and increased their scores (for a final of 2 accepts and 1 weak accept).  The authors have provided a thorough and well written paper, with comprehensive and convincing experiments. In addition, the ability to model long range sequences and dependencies is an important problem and the AC agrees that this paper makes a solid contribution in tackling that problem.  Thus, acceptance is recommended.
The authors clarified many of R1 and R4 s concerns, but there were important remaining concerns regarding the presentation.  On the bright side, the approach is novel and the experimental results are solid.   However, the main point raised by R1 is the mismatch between the narrative and the theory and the actual algorithm and results. Some exemples of this mismatch include:   Proposition 1 is proved when lambda goes to 0, which is never mentioned in the main paper. One has to look into the appendices to have a discussion of lambda and of the algorithm, while the authors could clearly explain that when discussing the theoretical result. The added discussion on tuning of lambda based on Appendix E does not help because the optimization problem is written as "under the constraint that [...] is maximized", which is not particularly clear.    More generally, the theoretical result (including e.g., the assumption of ergodicity) could be discussed more precisely in terms of what is actually done in the algorithm and the experiments.   as stated by R3, many important aspects of the methods and the experimental setup are only available in appendices, which makes it difficult to understand the similarities and differences in experimental protocol between the curent paper and RL^2, PEARL and IMPORT.   it is unclear what part of DREAM is critical for performance. There is no thorough ablation study nor discussion of the importance of the information bottleneck term, and the only signal given in Figure 5 is that it is critical. The authors could clarify the two aspects (decoupling and information bottleneck).    There was some discussion about this paper, but even under the assumption that the authors answered most R3 s concerns (R3 didn t engage in discussions), the paper is still borderline. In the end there was little support for acceptance because of the presentation issues above.
This work proposes a method to discover neighboring local optima around an existing one. Reviewers all found the idea interesting but argued that the paper needed more work. In particular, some of the claims are too informal or not sufficiently supported and the reviewers found the key section were difficult to follow. The paper should be resubmitted after improving the presentation of the results.
The paper swaps characteristics of an object in one image onto those of another object in another image for example, adding fur to a car.  The authors give some examples where the task could be useful.  Further, they successfully argue  that this task is an illustration that the disentanglement task has been done well.  Two reviewers argued for acceptance, two for just below the bar rejection.  The 2nd of those in favor of rejection engaged thoughtfully with the authors and raised the score by 1 after that engagement.  We have decided to accept the submission as a poster.
This paper is close to the borderline, but I think it is good enough that I recommend its acceptance. Although there were some problems raised by the reviewers, the authors managed to successfully address a majority of them. Having said that, I still recommend that the authors carefully analyze the reviews again and make sure that they incorporated reviewers  comments in the final version of the paper. A lot of them were constructive and might improve the quality of the paper.
Ultimately somewhat below the threshold based on the scores. The reviewers raise issues of the overall contribution, as well as issues with the design/structure of the model/paper and issues with the experiments. While there are some positive aspects, collectively the issues put the paper below the bar for acceptance.
This paper presents an interesting idea for task free incremental learning on the data stream. The reviewers have extensive discussions after reading all the reviews and the author s rebuttal. There are concerns raised about the presentation of the method and the justification for some parts of the model design choices. The reviewers believe that after addressing these weaknesses the work can be made stronger and may be accepted in a competitive venue.  
The paper studies a adversarial attacks and defenses against convolutional networks based on a minimax formulation of the problem. Whilst this is an interesting direction of research, the present paper seems preliminary. In particular, compared to several other independent ICLR submissions, the empirical evaluation is quite weak: it does not consider the strongest known gradient based attack (Carlini Wagner) as baseline and does not report results on ImageNet. The reviewers identify several issues related to Lemma 1 and to the clarity of presentation.
This paper studies the impact of embedding complexity on domain invariant representations by incorporating embedding complexity into the previous upper bound explicitly.  The idea of embedding complexity is interesting, the exploration has some useful insight, and the paper is well written. However, Reviewers and AC generally agree that the current version can be significantly improved in several ways:   The proposed upper bound has several limitations such as looser than existing ones.   The embedding complexity is only addressed implicitly, which shares similar idea with previous works.   The claim of implicit regularization has not been explored in depth.   The proposed MDM method seems to be incremental and related closely with the embedding complexity.   There is no analysis about the generalization when estimating this upper bound from finite samples.  There are important details requiring further elaboration. So I recommend rejection.
This paper provides an interesting method to address the CTDE problem in MARL. While the experiments are promising, the theory is either insufficient or not rigorous. One of the reviewers believe that there is a flaw in the paper. There was an extensive discussion among the authors and the reviewer. The authors could not convince the reviewer for the apparent flaw.
I thank the authors for their constructive engagement in the review process   this paper clearly benefitted from your prompt attention to initial weaknesses   and I thank the reviewers for their excellent, detailed, careful reviews.  This paper presents methods that allow a multi task learning (MTL) system to performs competitively against single task (ST) fine tuning despite using fewer parameters and less data per task. This is a great goal, which disrupts the currently most pursued approach of ST fine tuning and beats aiming for fully single model like BAM!  Pros  * The paper presents a number of useful methods, some novel like conditional attention mechanisms, for improving MTL. There is a lot in this paper. * Clean, motivated, intuitive model modifications * Comprehensive experiments * Generally well written paper, fairly comprehensive discussion of related work  Cons  The initial paper had a number of weaknesses, the most consistently mentioned being that the results presented tend to overclaim and to confuse (flipping between BERT and RoBERTa, etc.). The authors have done a good job revising the paper to address these concerns but should certainly remember these points in producing the final version.  The paper is a bit of a grab bag of small contributions that together help for MTL, but which isn t as strong a story as one big novel idea carefully presented and evaluated. While this paper has a ton of work behind it, and a lot of good stuff in it, I think this does somewhat weigh against the paper being selected for spotlight presentation.  Requests for the final version:  Further small clean up: e.g., still one "STITLS" to become "STILTS", some cleaning up could be done in the references where "stilts" is lowercase, there isn t capitalization after punctuation in the title of either Clark et al. paper, and the de Marneffe et al. paper, Fisch et al. paper and others lack information on where the work appeared).  Despite the improvements, I think more can and should be done to make the paper clearer and better laid out. Here are a few suggestions:  * More consistency in labeling the contributions might be possible. I m imagining that they might be able to be consistently labeled 1 to 5, rather than being 1 to 5 in subsections of section 2, but 1 to 3 with bulleted sub items in the introduction, and several of them (a) through (c) in Fig 1. * Although the reviewers generally said the paper was clear and well written, I still feel that section 2.1 is harder to get than it should be – and indeed you resorted to pasting PyTorch code in this discussion to help us! While people often say that a figure should be standalone, maybe that doesn t really make sense when it s an inline figure like this, and you d be better off explaining things well once (mainly in the text) rather than badly twice?!? I.e., just make the Fig 2 caption: "Figure 2: Conditional attention". I still think it s harder to get what the equations are doing than it should be, because there is inadequate text explaining the equations and the ideas motivating them. Although you now sort of sneak in to the text that ⨁ means diag, you still don t explicitly say so. I think having even 2 more lines of text at your disposal could really help if well deployed. * As R5 observes, the compactification in pages 7 9 just gets kind of ridiculous. I get that you re trying to deal with page breaks and to fit a lot in etc., but it just makes no sense when the tables embedded into paragraphs on p9 don t even belong with the corresponding paragraph! How might this be fixed? It s difficult, since you do just have a lot of material. The best idea I can come up with would be to shorten the main paper related work section to only 1 paragraph that discusses things at a very high level, and to put your detailed related work (even adding a few more of the things reviewers suggest like MT, etc.) to the Appendix. This might give you another half page in the main paper to make things more sensible.  
This paper was particularly discussed between the reviewers, the AC and SAC. A last minute reviewer was also called to clarify some issues raised, as one of the reviews never got into the system.  The paper was overall perceived as well written and well presented, and that the software contribution of implicit differentiation techniques is a nice asset for the community, especially its modularity. The stability guaranty constitutes a nice (though straightforward) result providing a theoretical ground for the proposed approach. Yet, the paper is often loose on mathematical justifications, in particular on minimal validity assumptions. Details on when the proposed framework could fail would be of interest, both on theoretical and practical parts. A discussion on the minimal assumptions required for validity of the approach should be highlighted more in the text.  Furthermore, the paper lacks discussions and comparisons with concurrent works, for instance how would the framework compare with existing estimates for implicit differentiation or for unrolling. This could be improved along with providing more analysis on the implementation efficiency. On the practical part, a high level description the software details would also be much beneficial. A core discussion focused around what should be expected of this type of paper (i.e., "implementation issues, parallelization, software platforms, hardware" papers as suggested by Q3Lr)  A point of concern was the novelty aspects in the discussion phase was the novelty of the proposed framework: even if the contribution is the framework introduced, this is not new per se (the literature on implicit differentiation now contains a considerable amount of results and implementation examples). The relevance of the work, both on theoretical and computational aspects, beyond the development of a computational library was found difficult to assess by several reviewers. Overall, the reviewers judged the novelty and the paper s contribution more on the software side. Hence, a core discussion could focus on aspects expected for code oriented papers (i.e., implementation issues, parallelization, hardware, etc.).  Following the long discussion phase (more than 30 posts on OpenReview) and the aforementioned comments, the paper was rejected.  We encourage the authors to submit a revised version in a future conference or possibly to a software oriented journal, such as JMLR MLOSS or JOSS for instance.
This paper proposes an approximate inference approach for decoding in autoregressive models, based on the method of auxiliary coordinates,  which uses iterative factor graph approximations of the model.  The approach leads to nice improvements in performance on a text infilling task.  The reviewers were generally positive about this paper, though there was a concern that more baselines are needed and discussion was very limited following the author responses.  I tend to agree with the authors that their results are convincing on the infilling task.  The impact of the paper is a bit limited by the lack of experiments on more standard decoding tasks, which, as the authors point out, would be challenging as their approach is computationally demanding.  Overall I believe this would be an interesting contribution to the ICLR community.
This paper presents a method to turn a pretrained unconditional VAE into a conditional VAE by training an encoder to predict the unconditional VAE latents given conditional input. On a variety of image tasks, the method is shown to perform competitively with GANs, yielding good sample quality and diversity, and resulting in training time that improves on direct conditional generation approaches. While the technical novelty is limited, the strong empirical results and relevance given the growing availability of pretrained unconditional models lead me to recommend accepting this paper.  Ethics concerns have been raised for this paper. In particular, there were concerns with respect to the application of generative models, which inherit biases from the dataset, to guide medical imaging. It would be good to discuss this issue in more depth. A second point that was raised by the ethics committee is the fact that chest X rays are usually not taken in a sequential manner. We ask the authors to either provide evidence that X rays can be taken sequentially (one can think of situations where that s the case, e.g., X rays of teeth in the mouth), preferably in the context of chest X rays; if that s not possible, please highlight that the application, as described in the paper, is unrealistic (at the moment), and that it only serves as an illustration.  The key point we therefore ask the authors to address is to ensure that the paper clearly states how realistic the application is and what potential problems may arise when using generative models in this particular domain.
The paper considers representational aspects of neural tangent kernels (NTKs). More precisely, recent literature on overparametrized neural networks has identified NTKs as a way to characterize the behavior of gradient descent on wide neural networks as fitting these types of kernels. This paper focuses on the representational aspect: namely that functions of appropriate "complexity" can be written as an NTK with parameters close to initialization (comparably close to what results on gradient descent get).    The reviewers agree this content is of general interest to the community and with the proposed revisions there is general agreement that the paper has merits to recommend acceptance.
This paper proposes a sparse binary compression method for distributed training of neural networks with minimal communication cost. Unfortunately,  the proposed approach is not novel, nor supported by strong experiments. The authors did not provide a rebuttal for reviewers  concerns.  
 This paper has been reviewed by four knowledgeable referees. Two of them slightly leaned towards acceptance, whereas the other two suggested rejection. The main issues raised by the reviewers were (1) limited novelty [R1,R2], (2) missing baselines and ablations [R1,R3], (3) limited insights on the spectral analysis [R2], and (4) missing motivation behind modeling choices [R1,R3]. The rebuttal included a number of experiments requested by the reviewers (e.g. ablation with diffusion only [R1,R3], extended Diffusion GCN [R1], APPNP baseline [R3]), and adequately motivated some of the modeling choices.   The central question of the reviewers  discussion was whether the contribution of this paper was significant enough or too incremental. The discussion emphasized relevant literature which already considers multi hop attention (e.g. https://openreview.net/forum?id rkKvBAiiz [Cucurull et al.], https://ieeexplore.ieee.org/document/8683050 [Feng et al.], https://arxiv.org/abs/2001.07620 [Isufi et al.]), and which should have served as baseline. In particular, the experiment suggested by R3 was in line with some of these previous works, which consider "a multi hop adjacency matrix " as a way to increase the GAT s receptive field. This was as opposed to preserving the 1 hop adjacency matrix used in the original GAT and stacking multiple layers to enlarge the receptive field, which as noted by the authors, may result in over smoothed node features. The reviewers acknowledged that there is indeed as slight difference between the formulation proposed in the paper and the one in e.g. [Cucurull et al.]. The difference consists in calculating attention and then computing the powers with a decay factor vs. increasing the receptive field first by using powers of the adjacency matrix and then computing attention. Still, the multi hop GAT baseline of [Cucurull et al.] could be extended to use a multi hop adjacency matrix computed with the diffusion process from [Klicpera 2019], as suggested by R3. In light of these works and the above mentioned missing baselines, the reviewers agreed that the contribution may be viewed as rather incremental (combining multi hop graph attention with graph diffusion). The discussion also highlighted the potential of the presented spectral analysis, which could be strengthened by developing new insights in order to become a stronger contribution (see R2 s suggestions).   To sum up, this was a very discussed paper, where the reviewers ultimately reached a consensus to reject, with no strong opposition. I agree with the reviewers  assessment and therefore must reject. I encourage the authors to follow the reviewers  suggestions and consider the multi hop baselines as well as the hints provided by the reviewers about the spectral analysis to strengthen their work. 
This paper presents a method to compress DNNs by quantization. The core idea is to use NAS techniques to adaptively set quantization bits at each layer. The proposed method is shown to achieved good results on the standard benchmarks.  Through our final discussion, one reviewer agreed to raise the score from ‘Reject’ to ‘Weak Reject’,  but still on negative side. Another reviewer was not satisfied with the author’s rebuttal, particularly regarding the appropriateness of training strategy and evaluation. Moreover, as reviewers pointed out, there were so many unclear writings and explanations in the original manuscript. Although we admit that authors made great effort to address the comments, the revision seems too major and need to go through another complete peer reviewing. As there was no strong opinion to push this paper, I’d like to recommend rejection. 
The authors appreciated this submission because (a) the aspect of explainability is novel, (b) its strong performance, (c) the clarity of the paper. I urge the authors to double check all of the reviewer comments to make sure they are all addressed in the updated version. I vote to accept.
The authors build an encoding model of whole brain brain activity by integrating incomplete functional data with anatomical/connectomics data. This work is significant from a computational neuroscience perspective because it constitutes a proof of concept regarding how  whole brain calcium imaging data can be used to constrain the missing parameters of a connectome constrained, biophysically detailed model of the C. elegans nervous system. There were issues related to clarity in the initial submission which all appeared to have been addressed in the final revision. This paper received 3 accepts (including one marginal accept) and 1 reject. The paper was discussed and the reviewers (including the negative reviewer) were unanimous that the current submission should be accepted.
This paper introduces a way to augment memory in recurrent neural networks with order independent aggregators. In noisy environments this results in an increase in training speed and stability. The reviewers considered this to be a strong paper with potential for impact, and were satisfied with the author response to their questions and concerns.
 + interesting approach for a detailed analysis of the limitations of autoencoders in solving a simple toy problem    resulting insights somewhat trivial, not really novel, nor practically useful  > lacks demonstration of a gain on non toy task    regularization study too limited in scope: lacking theoretical grounding, and more exhaustive comparison of regularization schemes.
I do not recommend accepting this paper, although I make this decision with reservations. The review quality for this paper was not particularly strong, and I wish to emphasize to the authors that I read the paper myself in detail in the process of writing this metareview.  This paper proposes a new structured pruning technique called LEAN. It involves computing an operator norm of the convolutions in a convolutional neural network, multiplying these norms over paths through the network, and keeping the paths with the highest such values (and pruning everything else). This paper makes the argument that this metric is robust to scaling and prevents network discontinuities. (In this way, the technique is very reminiscent of SynFlow (Tanaka et al., Pruning Neural Networks without any data by iteratively conserving synaptic flow) in terms of motivation and resulting technique, although SynFlow is unstructured. I do not mean this as a criticism   just a suggestion for the authors of a connection they might be able to make in the future.)  One big concern I have about this paper based on the methodology alone is as follows: the paper states a number of hypotheses about why this is a sensible way to prune (e.g., in the beginning of Section 4). I see no reason why any of these hypotheses are wrong, but the paper never makes an effort to evaluate any of them. I don t mean a theoretical justification here   that s difficult and unlikely to yield useful information about what happens in practice. I mean experiments to ablate the salient properties of the heuristic mentioned in the paragraph at the beginning of Section 4 (Does scaling invariance actually matter in practice? Is network disconnectivity actually a risk in practice?).  My biggest concerns about the paper, however, are in the evaluation. I share two major reviewer concerns that were mentioned: (1) The paper compares to a very limited set of baseline pruning methods, and relatively older ones at that (2019 is indeed old in the world of pruning). (2) The paper does not look at standard, "large scale" benchmarks for computer vision   namely, ResNet 50 on ImageNet.  Neither of these concerns is necessarily decisive in my view. For example, with respect to Concern 1, the reviewers unhelpfully do not suggest very many additional structured pruning benchmarks to consider, and I think the additional baselines added during the revision process have softened this concern. I would also recommend taking a look at "Growing Efficient Deep Networks by Structured Sparsification" (Yuan et al) for a useful method and a good set of baselines. There are an arbitrary number of baselines one could add and the structured pruning space is a confusing mess, but I think the claims in this paper merit more than are currently present.  With respect to Concern 2, I m even more conflicted. On the one hand, I have rarely seen any pruning techniques proposed for or evaluated on vision tasks beyond image classification, despite the fact that   in the real world   segmentation is much more popular than it would seem by reading the ICLR proceedings. To that end, I applaud the authors for focusing on those settings and I see substantial value in a paper that does so. On the other hand, ResNet 50 on ImageNet (among other standard classification benchmarks) is the de facto measuring stick for evaluating pruning methods in computer vision, and the exclusive focus on segmentation here means it is very difficult to compare the proposed technique to other benchmarks. If the paper is to focus on segmentation alone, this places a higher burden on adding many additional comparisons to other methods (i.e., Concern 1). Finally, I don t see any reason why the paper *couldn t* also include ResNet 50 on ImageNet or the like in addition to segmentation; if it is a limitation on the compute available to the authors (something I empathize with), they did not say so in any of the author responses. Upon reading the author responses, I was left asking, "Why not both?"  For those reasons, I do not recommend accepting the paper, although I think there are some good reasons to value the paper s contributions. At the end of the day, there are some relatively simple things that could be changed to make the paper much easier to contextualize within the pruning literature. As of right now, it would be very difficult for me to say whether or in what contexts this method should actually be used in practice.  (P.S. I agree with the reviewers that Figure 3 is exceptionally hard to parse.)
The authors propose a new technique for training networks to be robust to adversarial perturbations. They do this by computing bounds on the impact of the worst case adversarial attack, but that only hold under strong assumptions on the distribution of the network weights. While these bounds are not rigorous, the authors show that they can produce networks that improve the robustness accuracy tradeoff on image classification tasks.  While the idea proposed by the authors is interesting, the reviewers had several concerns about this paper: 1) The assumptions required for the bounds to hold are unrealistic and unlikely to hold in practice, especially for convolutional neural networks. 2) The comparisons are not presented in a fair manner that allow the reader to interpret the difference between the nature of certificates computed by the authors and those computed in prior work. 3) The empirical gains are not substantial if one normalizes for the non rigorous nature of the certificates computed (given that they only hold under hard to justify assumptions).  The rebuttal phase clarified some issues in the paper, but the fundamental flaws with the approach remain unaddressed. Thus, I recommend rejection and suggest that the authors revisit the assumptions and develop more convincing arguments and/or experiments justifying them for practical deep learning scenarios.
The paper proposes augmenting Neural Statistician with a meta context variable that specifies the partitioning of the latent context into the per dataset and per datapoint dimensions. This idea makes a lot of sense but the reviewers found the experimental section clearly insufficient to demonstrate its effectiveness convincingly. Also introducing only the unsupervised version of the model, which looks challenging to train, but performing all the experiments with the less interesting semi supervised version makes the paper both less compelling and harder to follow.
The paper proposes a new measure of difference between two distributions using conditional transport. The paper considers an important problem.  However, some major concerns remain after the discussion among the reviewers. In particular, the paper focuses on the evaluation on a toy dataset. It is unclear whether the claim carries over to large real datasets. The presentation of the paper also needs substantial improvement. 
The paper introduces a new idea for multi view classification: using a Dirichlet distribution over the views to model uncertainty.  The paper appears to be clear, well written and sound. Also, the experimental comparison is thorough.  The authors have given pertinent responses to the reviewers  questions, including w.r.t comparing against Bayesian/deep CCA in terms of accuracy.  Overall, this is a good paper. 
This work investigates the recently proposed hypothesis that enhanced shape bias improves neural network robustness to common corruptions. Several interesting experiments are performed to better understand the contributing factors that lead to improved robustness of models trained with texture randomization. Of particular note, the authors design a data augmentation strategy that verifiably increases the shape bias of model, but for which corruption robustness is not improved. Reviewers agreed that this is an interesting counter example to the shape bias hypothesis and improves our understanding of why stylization improves robustness. Given the carefully designed experiments investigating an important topic I recommend accept.
The experimental work in this paper leaves it just short of being suitable for acceptance. The work needs more comparisons with prior work and other approaches. The numerical ratings of the work by reviewers are just too low. 
This paper proves a global convergence rate of a newly proposed algorithm finite sum problem under some assumptions. While the proposed algorithm provides some interesting ideas to solving the finite sum problem using intermediate proxy solver, the current assumptions are too strong and I m afraid that this can make the result essentially trivial:  For example, assumption 3 assumes that  $ H_i * v \approx \nabla_z\phi_i(h(w, x_i))$ for every i. This simply implies that  $ \|\nabla_w\phi_i(h(w, x_i)) \|_2 $ is as large as $\|\nabla_z\phi_i(h(w, x_i))\|_2^2 $ as long as the norm of v is small, since  $\nabla_w\phi_i(h(w, x_i))    \nabla_z\phi_i(h(w, x_i)) H_i $.    Hence the assumption simply assumes that "If the loss is not small, then the gradient of the objective is not small (using the convexity of $\phi$, so $|\nabla_z\phi_i(h(w, x_i))$ has to be large)"   This would imply that gradient descent can also work (and arguably having the same convergence rate) under this assumption.  Note that "the smallest movement that can decrease the objective the most" is indeed following gradient descent direction   So gradient descent would not move the weights more than this algorithm as well.   Therefore, I am not sure that there is a clear benefit to using this algorithm compared to the standard (stochastic) gradient descent. In particular, I would suggest the authors at least show one example where under the current set of assumptions, gradient descent does not work as efficiently compared to the proposed algorithm   This will make the proposed algorithm much more justified.
A robustness verification method for transformers is presented. While robustness verification has previously been attempted for other types of neural networks, this is the first method for transformers.   Reviewers are generally happy with the work done, but there were complaints about not comparing with and citing previous work, and only analyzing a simple one layer version of transformers. The authors convincingly respond to these complaints.  I think that the paper can be accepted, given that the reviewers  complaints have been addressed and the paper seems to be sufficiently novel and have practical importance for understanding transformers.
In this paper the authors consider a contextual batched bandit setting where they rely on  imputationin order to estimated the non executed actions in each batch. Even though the idea is quite ineteretsing, and can lead to new methods, there is still a lof of issues raised by the reviwers. In particular, part of the proof was incorrect (and the authors tried to fix it) but given the short time, the reviwers felt that this part should be rewritten and scrutanized further. Also, there are many suggestions by reviewers that the authors need to apply in order to make this work publishable.
This paper studies the problem of federated learning for non i.i.d. data, and looks at the hyperparameter optimization in this setting. As the reviewers have noted, this is a purely empirical paper. There are certain aspects of the experiments that need further discussion, especially the learning rate selection for different architectures. That said, the submission may not be ready for publication at its current stage.
Reviewers have expressed concerns about clarity/writing of the paper and technical novelty, which the authors haven t responded to. The paper is not suitable for publication at ICLR.
This paper proposes a real time method for synthesizing human motion of highly complex styles. The key concern raised by R2 was that the method did not depart greatly from a standard LSTM: parts of the generated sequences are conditioned on generated data as opposed to ground truth data. However, the reviewer thought the idea was sensible and the results were very good in practice. R1 also agreed that the results were very good and asked for a more detailed analysis of conditioning length and some clarification. R3 brought up similarities to Professor Forcing (Goyal et al. 2016)   also noted by R2   and Learning Human Motion Models for Long term Predictions (Ghosh et al. 2017)   noting not peer reviewed. R3 also raised the open issue of how to best evaluate sequence prediction models like these. They brought up an interesting point, which was that the synthesized motions were low quality compared to recent works by Holden et al., however, they acknowledged that by rendering the characters this exposed the motion flaws. The authors responded to all of the reviews, committing to a comparison to Scheduled Sampling, though a comparison to Professor Forcing was proving difficult in the review timeline. While this paper may not receive the highest novelty score, I agree with the reviewers that it has merit. It is well written, has clear and reasonably thorough experiments, and the results are indeed good.
The paper proposes a new problem setup as "online continual compression". The proposed idea is a combination of existing techniques and very simple, though interesting. Parts of the algorithm are not clear, and the hierarchy is not well motivated. Experimental results seem promising but not convincing enough, since it is on a very special setting, the LiDAR experiment is missing quantitative evaluation, and different tasks might introduce different difficulties in this online learning setting. The ablation study is well designed but not discussed enough.
This paper addresses the performance of normalizing flows in the tail of the distribution. It does this by controlling tail properties in the marginals of the high dimensional distribution. The paper is well motivated, and the key theoretical insight has merit. However, the general perspective and methodology appears to be incremental relative to past results. Furthermore, some concerns over correctness remain after discussion with authors. Also, clear baselines and more realistic settings are lacking in the experimental results. Thus, while the paper generally has promising ideas on a pertinent topic, it appears to be not developed enough to merit dissemination.
I agree with the reviewers that said that this paper has valuable insights. However, all reviewers ultimately recommended rejection. I think the main reason was that the reviewers did not feel these insights don t accumulate together to a message that would justify a paper. I hope the authors can address these concerns and resubmit. There were additional concerns, like the fact a very simplistic toy model is being used, but I agree with the authors that it makes sense to first explore such phenomena in the simplest model that produces them.
This paper studies the problem of generative modeling by convolving an unknown complex density with a factorial kernel called multi measurement noise model (MNM) to obtain a smoother density that is easier to sample from. Poisson and Gaussian MNMs are proposed for the convolution. Experiment regarding image synthesis are conducted to demonstrate the effectiveness of the proposed framework. The paper studies a problem that is of great interest to the machine learning community, and the results are impressive and promising. However, the paper in the current form lack a comparative discussion and a quantitative comparison with some related works. After the rebuttal, all three reviewers tend to accept the paper. After several rounds of internal discussion among AC, reviewers and authors, the AC agrees with the reviewers, and recommends accepting the paper, given the changes the authors promised to make.    In summary, the AC recommends an acceptance and urges the authors to further revise their paper by adding a comparative discussion with those closely related works regarding generative models using MCMC sampling.
This method proposes a criterion (SNIP) to prune neural networks before training.  The pro is that SNIP can find the architecturally important parameters in the network without full training. The con is that SNIP only evaluated on small datasets (mnist, cifar, tiny imagenet) and it s uncertain if the same heuristic works on large scale dataset. Small datasets can always achieve high pruning ratio, so evaluation on ImageNet is quite important for pruning work. The reviewers have consensus on accept. The authors are recommended to compare with previous work [1][2] to make the paper more convincing.   [1] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. NIPS, 2015.  [2] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. NIPS, 2016.
This paper is a scholarly examination for how to conduct continual learning evaluations, proposing six rules that in large part synthesize work from other papers. While there is certainly scholarly benefit to such an exploration, all reviewers believe that the contribution is not substantial enough in its current form to warrant acceptance. It is certainly true that not all continual learning papers follow all of the guidelines/rules for evaluation, and consequently, papers such as this are useful to improve the scientific process.  However, the contribution needs to be substantially deepened, including more extensive and in depth experiments with novel insights as described in the reviews, before the paper is ready for publication.
The paper analyzes the learning behavior of deep networks inside RL algorithms, and proposes an interesting hypothesis: that many of the observed difficulties in deep RL methods stem from _capacity loss_ of the trained network (that is, the network loses the ability to adapt quickly to fit new functions). As the paper points out, some of these difficulties have popularly been attributed to other causes (such as difficulties in exploration) or to less specific causes (such as reward sparsity: the paper proposes that capacity loss mediates observed problems due to sparsity).   The paper investigates its hypothesis two ways: first by attempting to measure how capacity varies over time during training of existing deep RL methods, and second by proposing a new regularizer to attempt to preserve capacity. These experiments are set up well, and their results are convincing &mdash; while there is likely no perfect way to measure or preserve capacity, the methods chosen here make sense.   This is a strong paper: it proposes a creative, appealing, and interesting hypothesis about an important problem (difficulties in training deep RL methods), and conducts a well designed evaluation. We expect and hope that it will inspire interesting follow on work.  We thank the authors for their thorough and helpful participation in the discussion period, including updates to improve the clarity of the paper.
meta score: 4  This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization.  Pros    good set of experiments using CIFAR, with good results    attempt to explain the approach using expectations Cons    theoretical explanations are not so convincing    limited novelty    CIFAR is relatively limited set of experiments    does not compare with using bn after relu, which is now well studied and seems to address the motivation of this paper (and thus questions the conclusions)
The paper proposes a filtering technique to use less training examples in order to train faster; the filtering step is done with an autoencoder. Experiments are done on CIFAR 10. Reviewers point to a lack of convincing experiments, weak evidence, lack of experimental details. Overall, all reviewers converge to reject this paper, and I agree with them.
This paper studies the problem of inverse reinforcement learning by relying on only demonstrations and no interaction (like imitation learning). The reviewers liked the premise but had major concerns with evaluation and baselines. The paper initially received reviews tending to reject. One of the questions was about missing behavior cloning baseline which the authors added in rebuttal. But the BC baseline seems to be really competitive (in fact, better in 3 out of 4 envs) as compared to the proposed approach. In conclusion, all reviewers still believed that their concerns regarding insufficient evidence for justifying approach and missing comparisons to other prior work still stand. AC agrees with the reviewers  consensus that the paper is not yet ready for acceptance.
This work formulates and tackles a few shot RL problem called subtask graph inference, where hierarchical tasks are characterized by a graph describing all subtasks and their dependencies. In other words, each task consists of multiple subtasks and completing a subtask provides a reward. The authors propose a meta RL approach to meta train a policy that infers the subtask graph from any new task data in a few shots. Empirical experiments are performed on different domains, including Startcraft II, highlighting the efficiency and scalability of the proposed approach.  Most concerns of reviewers were addressed in the rebuttal. The main remaining concerns about this work are that it is mainly an extension of Sohn et al. (2018), making the contribution somewhat incremental, and that its applicability is limited to problems where subtasks are provided. However, all reviewers being positive about this paper, I would still recommend acceptance. 
Based on the contrastive learning loss wildly used in the NLP and computer vision domains, this paper presents Self GenomeNet, a contrastive learning method for representation learning of genomic sequences. As shown in the experiment section, the improvement compared to baselines CPC, Language model, and even supervised learning method is considerable, on three benchmark datasets in both self supervised and semi supervised evaluation.  Even after the discussion phase, there exists disagreement among the reviewers. AC considered all reviews, author responses, and the discussions, as well as read the paper. While the paper has some merit such as an effective Self GenomeNet model for the particular problem setup, reviewers still have several reservations to directly accepting it: + Questionable impact. The proposed framework is overall a simple combination of existing methods and beyond genome datasets, the impact of this proposed method is questionable. + Limited inspiration. The proposed method is mainly constructed on the previously proposed contrastive learning loss wildly used in the NLP and computer vision domains, the benefits of the proposed method may be limited on the genome data (especially the domain specific data augmentation e.g., reverse complement). How can the insights foster future research? + Lack of justification. The innovations introduced by the paper seem ad hoc, and the reasons for the large observed improvement are not entirely intuitive. Meanwhile, even with the provided response from the authors, the connection between motivation and the proposed method is still not crystal clear.  Given the above reservations, AC could not accept the paper for now but encourage the authors to fully revise the paper and strengthen their work.
The authors introduce new evaluation criteria and methods for identifying salient features: rather than earlier approaches which attempt to  remove  or marginalize out features in various ways, here they consider robustness analysis with small adversarial perturbations in an Lp ball.  For text classification, a user study is included which is appreciated.  In discussion, the authors addressed many points and all reviewers converged to recommend acceptance.  A couple of points could be discussed further if space permits: the impact of type of perturbation employed; and the connection between optimizing for adversarial robustness and optimizing for insertion/deletion criteria.
This paper proposes meta learning auxiliary rewards as specified by a DSL. The approach was considered innovative and the results interesting by all reviewers. The paper is clearly of an acceptable standard, with the main concerns raised by reviewers having been addressed (admittedly at the 11th hour) by the authors during the discussion period. Accept.
This paper has been reviewed by four experts. Their independent evaluations were all below the acceptance threshold citing various issues ranging from disconnection between stated goals of the presented work and the means in which the approach was evaluated, to doubts about the scalability of the proposed approach, to the lack of clarity regarding the actual novelty of the approach given some key missed references, to name a few items of criticism. Most reviewers were impressed with the empirical performance achieved in the conducted experiments, and one of the reviewers raised their mark in response to the author s rebuttal. Yet, the overall evaluation places this work as it stands now below the threshold for ICLR acceptance. I would like to encourage the authors to continue pushing their promising endeavor and systematically incorporating the feedback received here to improve the overall quality of this work.
A new method for derivative free optimization including momentum and importance sampling is proposed.  All reviewers agreed that the paper deserves acceptance.  Acceptance is recommended.
The presented idea is aligned with past work using multiple experts or multiple sources for transfer. However, it is positioned uniquely and cleverly in that the approach is developed with scalability in mind. Within this setting, the paper is convincing. Although the approach does not come with strong backing theory, it is intuitive and seems to work well. During the discussions phase, the authors have clarified some questions that made the paper convincing, even if it is a relatively heuristic approach. The results are strong if one is concerned with both quantitative performance and efficiency, a combination of objectives very often encountered in practice. Overall, it is expected that this idea can stimulate further research along those lines, especially since this paper is very nice and easy to read.
 This paper proposed a new method to prune neural networks using a continuous penalty function. All reviewers suggest acceptance (some are on borderline though) as the authors did a good job in the rebuttal phase. AC also could not find any particular reason to reject the paper (in particular, the overall writing is clear) and thinks that this paper is a meaningful addition to ICLR 2021. 
This paper presents a method for preventing exploding and vanishing gradients in LSTMs by stochastically blocking some paths of the information flow (but not others). Experiments show improved training speed and robustness to hyperparameter settings.  I m concerned about the quality of R2, since (as the authors point out) some of the text is copied verbatim from the paper. The other two reviewers are generally positive about the paper, with scores of 6 and 7, and R1 in particular points out that this work has already had noticeable impact in the field. While the reviewers pointed out some minor concerns with the experiments, there don t seem to be any major flaws. I think the paper is above the bar for acceptance. 
This paper is right at the borderline: the reviewers agree it is well written, proposing a simple but interesting idea. However, there was a feeling among the reviewers (especially reviewer 1) that the paper could be strengthened considerably with a better discussion/some theory on the sufficiency of the calibration vectors, as well as experiments on larger datasets. Doing one of these would have substantially strengthened the paper. Due to the remaining shortcomings, the recommendation is not to accept the paper in its present state.
This paper proposes a new scoring function for link prediction model that is based on a generative model for the knowledge graph, based on a random walk model previously used for word embeddings. The new scoring function, as it is accompanied by the generative model, provides interesting theoretical results that the reviewers also appreciate. Finally, the results are quite strong, as they obtain state of art on the primary benchmarks for the task.  Based on the submitted version, the reviewers and AC note the following potential weaknesses: (1) the reviewers felt that the proposed work is a direct application of the random walk model from Arora et al. and thus limited in novelty, (2) given the generative model, the reviewers felt that the paper would benefit from an analysis of the learned embeddings, and their difference from ones from existing approaches, (3) The reviewers noted that the authors were using an incorrect version of FB15k and WN18, (4) the authors were not providing results for all the metrics, (5) the coverage of related work is quite limited.  The authors addressed many of the concerns raised by the reviewers in their comments and revision, in particular, they obtained state of art results for the corrected versions of the benchmarks. Further, they clarified the assumptions made in their modeling and revised the related work to include the papers that the reviewers mentioned. However, the concerns regarding the lack of novelty of the proposed approach, w.r.t Arora et al 2016 and the need for further analysis of the learned embeddings, still remain.  This paper comes really close to getting accepted, but ultimately the reviewers agree that the remaining concerns need to be addressed.
Dear authors,  All reviewers commented that the paper had issues with the presentations and the results, making it unsuitable for publication to ICLR. Please address these comments should you decide to resubmit this work.
This paper received high variance in the reviews.  I personally agree with AnonReviewer4 that the theoretical results presented in this paper are well known results on the sensitivity analysis of linear programs. See for instance "Introduction to linear optimization" by Bertsimas and Tsitsiklis, Chapter 5.  More generally, these results are a special case of Danskin s theorem and the envelope theorem: https://en.wikipedia.org/wiki/Danskin%27s_theorem https://en.wikipedia.org/wiki/Envelope_theorem  Clarke s generalized gradients are just subgradients in the case of convex functions, which is the case here.  My recommentation to the authors if they want to publish their work is to focus on the applications and to stop claiming novelty on the theoretical side.
This paper proposed a method to estimate the instance wise saliency map for image classification, for the purpose of improving the faithfulness of the explainer. Based on the U net, two modifications are proposed in this work. While reviewer #3 is overall positive about this work, both Reviewer #1 and #2 rated weak reject and raised a number of concerns. The major concerns include the modifications either already exist or suffer potential issue. Reviewer #2 considered that the contributions are not enough for ICLR, and the performance improvement is marginal. The authors provided detailed responses to the reviewers’ concerns, which help to make the paper stronger, but did not change the rating. Given the concerns raised by the reviewers, the ACs agree that this paper can not be accepted at its current state.
This paper investigates using sound to improve classification, motion prediction, and representation learning all from data generated by a real robot.  All the reviewers were intrigued by the work. The paper provides experiments on real robots (never a small task), and a data set for the community, and a sequence of illustrative experiments. Because the paper combines existing techniques, its main contribution is the empirical demonstrations of the utility of using sound. Overall, it was not quite enough for the reviewers. The main issues were: (1) motion prediction is perhaps expected given the physical setup, (2) lack of comparison with other approaches, (3) lack of diversity in the demonstrations (10 objects, one domain).  The authors added two new experiments with a different setup, further demonstrating their claims. In addition the authors highlighted that the novelty of this task means there are no clear baselines (to which r3 agreed). The new experiments are briefly described in the response (and visuals on a website), but the authors did not update the paper. The new experiments could potentially significantly strength the paper. However, the terse description in the response and the supplied visuals made it difficult for the reviewers to judge their contribution.  Overall, this is certainly a very interesting direction. The results on real world data demonstrate promise, even if they are not the benchmarking style the community is used too.   
The work AdaFocal proposes an approach to tune Focal Loss  $\gamma$ hyperparameter to improve the model s calibration, particularly to avoid the occasional underconfidence when using focal loss. This tuning is done not as a learned constant hyperparameter across training but as one that evolves over training.  The work is both well motivated and well written. However, multiple reviewers share the concern (which I agree with) that the method fails on ImageNet experiments, and the method often fails to beat even temperature scaling. The experimental comparison arguing that the approach improves upon many methods pre temperature scaling is unfair as no other method leverages the validation set. This makes for a fairly deceiving slight of hand if not read carefully. When compared to temperature scaling, which does use the validation set, the performance improvement gap is diminished altogether.  I recommend the authors use the reviewers  feedback to enhance their preprint should they aim to submit to a later venue. In particular, improve the clarity around the experimental validation.
The paper introduces the notion of interventional consistency of a representation learned using autoencoders, which is claimed to be a desirable property for disentanglement. The reviewers agree that the contributions are novel and relevant, but they also found the paper hard to follow due to a lack of clarity and motivation. Further, they considered the underlying assumptions very strong and possibly hard to find practical instances where they may hold (e.g., the assumption that statistical dependencies in the prior are preserved by the response map). The reviewers also noted that some real world examples showing the interventional consistency would be helpful.   After all, the paper contains interesting ideas and we would like to encourage the authors to pursue this line of work. Still, the paper in its current form is not ready for publication. We encourage the authors to address the reviewers  comments explicitly in a future version of the manuscript.
The reviewers think that the theoretical contribution is not significant on its own. The reviewers find the empirical aspect of the paper interesting, but more analysis of the empirical behavior is required, especially for large datasets. Even for small datasets with input augmentation (e.g. random crops in CIFAR 10) the pre processing can become prohibitive. I recommend improving the manuscript for a re submission to another venue and an ICLR workshop presentation.
The paper focuses proposes a new framework for low resource video domain adaptation leveraging synthetic data with supervised disentangled learning for tackling keystroke inference attacks.  The paper received contrasting reviews, 2 positive and 2 negative, and the overall confidence of the reviewers is not so high. Overall, it is recognized that the work has some merit, but also some problems, which the rebuttal has not fully fixed, and I mainly refer to R2, R3 and R4 remarks.   The first issue is the level of novelty, which is not much high as compared to the former work of (Moiitan et al. 2017). Besides, the questions raised by some reviewers, also discussed in the rebuttal, also denote a certain lack of clarity, despite the paper is considered well organised in general.   The other main issue regards the experimental evaluation. To start with, the application addressed is very specific and it is not clear how this approach can be extended to other problems too, since no evidence is provided in this sense. The reported comparative analysis wrt baselines are in fact quite "simple" (e.g., ADDA is a work dated back to 2017, so as CycleGAN). Moreover, although the considered dataset is the only one in this scenario, its significance is a bit limited since only 3 subjects were considered, and this likely raised the comment of one reviewer questioning if this paper was not better suited to an application oriented, security conference. A discussion for the setting of the lambda parameters is also missing.  Overall, given the above issues, I consider the paper not yet ready for publication in ICLR 2021. 
see my comment to the authors below
The present work proposes to improve backdoor poisoning attacks by only using "clean label" images (images whose label would be judged correct by a human), with the motivation that this would make them harder to detect. It considers two approaches to this, one based on GANs and one based on adversarial examples, and shows that the latter works better (and is in general quite effective). It also identifies an interesting phenomenon that simply using existing back door attacks with clean labels is substantially less effective than with incorrect labels, because the network does not need to modify itself to accommodate these additional correctly labeled examples.  The strengths of this paper are that it has a detailed empirical evaluation with multiple interesting insights (described above). It also considers efficacy against some basic defense measures based on random pre processing.  A weakness of the paper is that the justification for clean label attacks is somewhat heuristic, based on the claim that dirty label attacks can be recognized by hand. There is additional justification that dirty labels tend to be correlated with low confidence, but this correlation (as shown in Figure 2) is actually quite weak. On the other hand, natural defense strategies against the adversarial examples based attack (such as detecting and removing points with large loss at intermediate stages of training) are not considered. This might be fine, as we often assume that the attacker can react to the defender, but it is unclear why we should reject dirty label attacks on the basis that they can be recognized by one detection mechanism but not give the defender the benefit of other simple detection mechanisms for clean label attacks.  A separate concern was brought up that the attack is too similar to that of Guo et al., and that the method was not run on large scale datasets. The Guo et al. paper does somewhat diminish the novelty of the present work, but not in a way that I consider problematic; there are definitely new results in this paper, especially the interesting empirical finding that the Guo et al. attack crucially relies on dirty labels. I do not agree with the criticism about large scale datasets; in general, not all authors have the resources to test on ImageNet, and it is not clear why this should be required unless there is a specific hypothesis that running on ImageNet would test. It is true that the GAN based method might work more poorly on ImageNet than on CIFAR, but the adversarial attack method (which is in any case the stronger method) seems unlikely to run into scaling issues.  Overall, this paper is right on the borderline of acceptance. There are interesting results, and none of the weaknesses are critical. It was unfortunately the case that there wasn t room in the program this year, so the paper was ultimately rejected. However, I think this could be a strong piece of work (and a clear accept) with some additional development. Here are some ideas that might help:  (1) Further investigate the phenomenon that adding data points that are too easy to fit do not succeed in data poisoning. This is a fairly interesting point but is not emphasized in the paper. (2) Investigate natural defense mechanisms in the clean label setting (such as filtering by loss or other such strategies). I do not think it is crucial that the clean label attack bypasses every simple defense, but considering such defenses can provide more insight into how the attack works e.g., does it in fact lead to substantially higher loss during training? And if so, at what stage does this occur? If not, how does it succeed in altering the model without inducing high loss?
The paper proposes a new neural network, the aestheticNet, for a bias free facial beauty prediction. All the reviewers agree that the work is not suitable for publication as it raised some serious ethic concerns: * Prediction of beauty (aesthetic scores) is a potential harmful application. Well intended as it may be, a research along these lines might be harmful. * non anonoymity issue: writing reveals/implies authors identity with reference to previous work * Research integrity issues (e.g., plagiarism, dual submission), a figure is copied from previous work.  There is also a concern that the work is not novel and not interesting as such.  The authors did not respond to the concerns.  I suggest rejection.
This paper addresses the very important problem of ensuring that sensitive training data remain private. It proposes an attack whereby the attacker can reconstruct information about the training data given only the trained classifier and an auxiliary dataset. If done well, such an attack would be a useful contribution that helps make discussion of differential privacy more complete. But as the reviewers pointed out, it s not clear from the paper whether the attack has succeeded. It works only when the auxiliary data is very similar to the training data, and it s not clear if it leaks information about the training set itself, or is just summarizing the auxiliary data. This work doesn t seem quite ready for publication, but could be a strong paper if it s convincingly demonstrated that information about the training set has been leaked.  
The paper proposes a novel approach for estimating the high dimensional intensity function of a Poisson process. The proposed approach builds on generalized additive models, using lower dimensional projections.   The reviewers noted that, although the paper is well written, the position of this paper compared to earlier related work is unclear, and the empirical evaluation of the method should be strenghtened. The authors clarified some points in their response, but the paper would still require some more modifications to be ready for publication. I therefore recommend this paper to be rejected.
The reviewers liked this paper quite a bit. The novelty seems modest and the results are limited to a fairly simple NER task, but there is nothing wrong with the paper, hence recommending acceptance.
This paper proposes a new method for label free text style transfer. The method employs the pre trained language model T5 and makes an assumption that two adjacent sentences in a document have the same style. Experimental results show satisfying results compared with supervised methods.  Pros. • The paper is generally clearly written. • The proposed method appears to be new. • Experiments have been conducted.  Cons • The fundamental assumption of the method is not convincing enough. (Issue 1 of R3, Issue 4 of R4, Issue 1 of R2) • The proposed model is also not convincing enough. (Issues 2 and 3 of R3, Issue 3 of R2) • There are problems with the experiments. For example, it would be better to use more datasets in the experiments. (Issue 4 of R3, Issue 2 of R4)  Discussions have been made among the reviewers. The reviewers appreciate the efforts made by the authors in the rebuttal, including the additional experiments. However, they are not fully convinced and still feel that the submission is not strong enough as an ICLR paper.  
Strengths:  One shot physics based imitation at a scale and with efficiency not seen before. Clear video, paper, and related work.  Weaknesses described include:  the description of a secondary contribution (LFPC)  takes up too much space (R1,4); results are not compelling (R1,4); prior art in graphics and robotics (R2,6); concerns about the potential limitations of the linearization used by LFPC.  The original reviews are negative overall (6,3,4). The authors have posted detailed replies. R1 has posted a followup, standing by their score. We have not heard more from R2 and R3.  The AC has read the paper, watched the video, and read all the reviews. Based on expertise in this area, the AC endorses the author s responses to R1 and R2.  Being able to compare LFPC to more standard behavior cloning is a valuable data point for the community;  there is value in testing simple and efficient models first. The AC identifies the following recent (Nov 2018) paper as being the closest work, which is not identified by the authors or the reviewers. The approach being proposed in the submitted paper demonstrates equal or better scalability, learning efficiency, and motion quality, and includes examples of learned high level behaviors. An elaboration on HL/LL control: the DeepLoco work also learns mocap based LL control with learned HL behaviors.        although with a more dedicated structure.        Physics based motion capture imitation with deep reinforcement learning        https://dl.acm.org/citation.cfm?id 3274506  Overall, the AC recommends this paper to be accepted as a paper of interest to ICLR.  This does partially discount R3 and R1, who may not have worked as directly on these specific problems before.  The AC requests is rating the confidence as "not sure" to flag this for the program committee chairs, in light of the fact that this discounts the R1 and R3 reviews. The AC is quite certain in terms of the technical contributions of the paper. 
There was extension discussion of the paper between the reviewers. It s clear that the reviewers appreciated the main idea in the paper, and the notion of an "online" meta critic that accelerates the RL process is definitely very appealing. However, there were unanswered questions about what the method is actually doing that make me reticent to recommend acceptance at this point. I would refer the authors to R3 and R1 for an in depth discussion of the issues, but the short summary is that it s not clear whether, if, and how the meta loss in this case actually converges, and what the meta critic is actually doing. In the absence of a theoretical understanding for what the modification does to accelerate RL, we are left with the empirical experiments, and there it is necessary to consider alternative hypotheses and perform detailed ablation analyses to understand that the method really works for the reasons stated by the authors (and not some of the alternative explanations, see e.g. R3). While there is nothing wrong with a result that is primarily empirical, it is important to analyze that the empirical gains really are happening for the reasons claimed, and to carefully study convergence and asymptotic properties of the algorithm. The comparatively diminished gains with the stronger algorithms (TD3 and especially SAC) make me more skeptical. Therefore, I would recommend that the paper not be accepted at this time, though I encourage the authors to resubmit with a more in depth experimental evaluation.
Summary of the paper and the reviews: The authors propose a method to explain the GNN predictions in a task agnostic setting, meaning that the method can be applied to a new downstream task without fine tuning. The task is formulated as to predict the important subgraph given the input graph and the ground truth label. The learning algorithm optimizes the mutual information between the embedding of the subgraph and the original graph. The experiment shows the quantitative improvement measured by the fidelity score, the qualitative visualization of highlighted subgraphs and comparison of the cost w.r.t the baseline GNN explainer models.  Strength:  1) The task agnostic setting is novel. 2) The proposed method shows improvement in the fidelity score with a reduced training cost over the baseline models Weakness:  1) The proposed objective requires additional justification. To optimize the intractable mutual information objective, the authors propose JSE and InfoNCE as upper bound estimations of mutual information, but the negative sampling technique in the proposed method is not fully justified. 2) During training, the authors simulate the task specific importance vector by sampling masks from a Laplace distribution. During testing, the importance vector is obtained by gradient based approach. Further analysis is needed to quantify the effect of this training/testing discrepancy.   3) In the empirical experiment, the proposed task agnostic model outperforms the task specific baselines. Why should such an outcome happen? The reason needs additional analysis but is not provided in the current paper. Moreover, the qualitative results of a few examples are not sufficiently convincing for the reported empirical success.  Summary of the discussions and the decision by reviewers: One reviewer asked for a justification of the negative sampling approaches used to approximate the mutual information objective. While the authors described their implementation design in their rebuttal, the theoretical justification of the method was not enough. Two reviewers raised the question about how randomly sampling importance masks during training could affect the downstream tasks performance, which was not fully addressed in the rebuttal. Other than that, the experimental concerns about new baselines and datasets were well addressed by the authors.  Recommendation: The paper has received borderline review scores (5, 5, 5, 6). Although the authors addressed some of the concerns in their rebuttal on the experimental design and added important baselines, more convincing justifications/analysis for the proposed method are still missing. Therefore, the reviewers didn’t raise their scores. Based on the above concerns, the recommendation is to reject.
This paper suggests extending pre trained contextual language models to use both fine grained and coarse grained tokenizations of the sentence. A sentence is tokenized twice and then each is passed through a transformer block, with shared parameters except the embeddings. Having 2 granularities shows gains.  Pros    Easy to read paper, straightforward method   Gets experimental gains from using word/phrase combo   Evaluates on a range of tasks  Cons    Novelty is limited, since other models like SpanBERT and ZEN already explore different tokenization granularities   Improvements may come as much from the ensemble of two models as the two tokenization granularities   Number of parameters or amount of computation are increased by method, though authors do significantly address this in their revised paper.   Some over claiming of results when there are modest incremental gains on small models (the abstract sentence "outperforms the existing best performing models in almost all cases" suggests that we are going to get results of a new model outperforming the state of the art models on tasks, but really we get improvements over baseline models at the BERT base size. I believe this is fine for experiments to show the scientific value of ideas but it should not be described as it presently is in this abstract.   Gains are more for Chinese than English  On the better results for Chinese: Isn t the reason that the results are more impressive for Chinese because in Chinese the fine grained version is just single characters, but this is more fine grained than standard BERT word pieces in English, where the word pieces are already commonly words, most of which would be two or more character sequence in Chinse (whether for common words like, say "fishing" or "vault" or place names like "Mississippi"), so the fine grained Chinese here is more fine grained than the standard English wordpieces, and so not too surprisingly there are bigger gains from using the Chinese word segmenter granularity. But really this is sort of equivalent to how the original BERT authors showed that you could get gains by masking whole words not individual word pieces. And at any rate, the value of word segmentation for Chinese was already shown by Yiming Cui et al. s paper on Chinese BERT, no?  Overall the strong majority of reviewers were unconvinced that this paper was suitable for ICLR 2021. They mainly emphasized concerns of novelty, missed or unfair comparisons, concerns of extra parameters or computation, and the fact the paper is somewhat incremental. I would add to that that to the extent that this paper is primarily an examination of the value of different granularities, that feels much more like a linguistic question for an NLP conference than an ML question well suited in particular to a conference on learning representations like ICLR. That is, the choice of granularities is hand specified, and/or the grouping is done by simple n gram statistics, not by learning representations. As such, I do not think the paper should be accepted to ICLR at this time, and in general think that an NLP venue may be more appropriate for it. 
This paper studies the method to achieve the batch size invariant for policy gradient algorithms (PPO, PPG). The paper achieves this by decoupling the proximal policy from the behavior policy. Empirical results show that the methods are somewhat effective at providing batch size invariance.  After reading the authors  feedback, the reviewer discussed the paper and they did not reach a consensus. On the one hand, the rebuttal made some reviewers change their minds who appreciated the explanations provided by the authors and the new Figure that better highlights the batch size invariance property. On the other hand, some reviewers think that there is still significant work to be done to get this paper ready for publication. In particular, it is necessary to improve the theoretical analysis and the evaluation of the empirical results.  I encourage the authors to follow the reviewers  suggestions while they will update their paper for a new submission.
The presented approach demonstrates an invertible architecture for auto encoding, which demonstrates improvements in performance relative to VAE and WAE s on MNIST.   Pros: + R3: The idea of pseudo inversion is interesting. + R3: Manuscript is clear.   Cons:   R1,2,3: Additional experiments needed on CIFAR, ImageNet, others.   R1: Presentation unclear. Authors have not made any apparent attempt to improve the clarity of the manuscript, though they make their point that the method allows dimensionality reduction in their response.   R1, R2: Main advantages not clear.     R3: Text could be compressed further to allow room for additional experiments.   Reviewers lean reject, and authors have not updated experiments. Authors are encouraged to continue to improve the work.
The paper presents an extension of flow based invertible generative models to a conditional setting. The key idea is fairly simple modification of the original architecture, but authors also propose techniques for down sampling with Haar wavelets. The experimental results on class conditional MNIST generation and colorization are promising. However, in terms of weakness, the technical novelty seems somewhat limited although it s a reasonable extension. In addition, the experimental results lack evaluation on general conditional image generation tasks with more widely used benchmarks (e.g., class conditional generation setting for real images, such as CIFAR and ImageNet; attribute conditional or image to image translation settings; etc.). In other words, colorization seems like a niche task. The baselines compared are not the strongest models. For example, the diversity of  cGANs can be significantly improved by simple plug in modifications (e.g., DSGAN) to any existing GAN architectures, and those methods were demonstrated on broader benchmarks. So I view the experimental validation somewhat limited in scope and significance. While this work presents a reasonable extension of conditional invertible generative models with promising results, I believe that more work needs to be done to be publishable at a top tier conference.  Diversity Sensitive Conditional Generative Adversarial Networks https://arxiv.org/abs/1901.09024  Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis https://arxiv.org/abs/1903.05628 * exactly the same idea as DSGAN above. 
This is another paper, similar in spirit to the Wasserstein GAN and Cramer GAN, which uses ideas from optimal transport theory to define a more stable GAN architecture. It combines both a primal representation (with Sinkhorn loss) with a minibatch based energy distance between distributions. The experiments show that the OT GAN produces sharper samples than a regular GAN on various datasets. While more could probably be done to distinguish the model from WGANs and Cramer GANs, this paper seems like a worthwhile contribution to the GAN literature and merits publication. 
This paper describes a method to generate provably  optimal  adversarial examples, leveraging the so called  Reluplex  technique, which can evaluate properties of piece wise linear representations. Reviewers agreed that incorporating optimality certificates into adversarial examples is a promising direction to follow, but were also concerned about the lack of empirical justification the current paper provides and missed discussion about the relevance of choosing Lp distances. They all recommended pushing experiments to more challenging datasets before the paper can be accepted, and the AC shares the same advice. 
The paper proposes a deep learning approach to blind image denoising based on deep unrolling. In particular, the proposed network is derived from convolutional sparse coding algorithms, which are unrolled, untied across layers and learned from data. The paper proposes a frequency domain regularization scheme in which the filters consist of a single analytically defined low pass filter and a large collection of filters which are constrained to reside in the mid to high frequency ranges. It also proposes to tie the thresholds in the soft thresholding stages of the learned network to estimates of the noise variance, making the proposed scheme more robust to variations in the noise level.   Pros and Cons:  [+] Having a single low pass dictionary atom reduces redundancy (and potentially coherence) in the learned dictionary. This type of regularization may also reduce the time/data required to learn.   [+/ ] Using noise estimators and a noise adaptive threshold renders the model more robust to variations in the noise level. This is important, since in most denoising applications the noise level is not known a priori. As the reviewers note, the idea of tuning thresholds in an unrolled sparse coding method based on the noise level is not a novelty of the paper; the novelty here is coupling this with a wavelet based estimate of the noise level.   [ ] All three reviewers raise concerns regarding the novelty of the work compared to existing convolutional sparse coding based neural networks. The structure of the network is similar; the main difference is the frequency restriction for learned atoms, which is enforced by prefiltering the learned atoms with a high pass filter.   [ ] The paper is not entirely clear in its motivation and argumentation. Reducing the coherence of the learned dictionary makes sense from the perspective of certain worst case results from sparse approximation. However, the coherence is a worst case quantity; moreover, certain approaches to coherence control (e.g., using large stride) control coherence at the expense of the expressiveness of the dictionary, and hence may not actually improve its ability to provide sparse reconstructions of natural signals. The proposed frequency domain regularization is a sensible approach to controlling coherence, since low frequency atoms will tend to be highly coherent, but would benefit from a crisper analytical motivation.   [ ] Reviewers found the experiments lacking in some regards. In particular, the paper only evaluates its proposals on synthetic experiments with Gaussian noise. While this is in line with some previous work on deep learning based denoising, more extensive and realistic experiments would have bolstered the paper s argument.   Overall, the paper makes a sensible proposal regarding the adaptivity to unknown noise levels, and introduces a potentially useful frequency domain restriction on the learned filters in a CSC network. However, the reviewers did not find that the paper made a clear argument for the significance of these proposals, and raised other concerns regarding the clarity and experiments. The consensus of the reviewers is to recommend rejection. 
This paper develops a methodology to perform global derivative free optimization of high dimensional functions through random search on a lower dimensional manifold that is carefully learned with a neural network.  In thorough experiments on reinforcement learning tasks and a real world airfoil optimization task, the authors demonstrate the effectiveness of their method compared to strong baselines.  The reviewers unanimously agreed that the paper was above the bar for acceptance and thus the recommendation is to accept.  An interesting direction for future work might be to combine this methodology with REMBO.  REMBO seems competitive in the experiments (but maybe doesn t work as well early on since the model needs to learn the manifold).  Learning both the low dimensional manifold to do the optimization over and then performing a guided search through Bayesian optimization instead of a random strategy might get the best of both worlds?  
This work combines words and images from Tumblr to provide more fine grained sentiment analysis than just positive negative. The contribution is too slight, as a straightforward combination of existing architectures applied on an emotion classification task with conclusions that aren t well motivated and are not providing any comparison to existing related work on finer emotion classification.
The authors introduce vPERL, a model that generates an intrinsic reward for imitation learning. vPERL is trained on demonstrations to minimise a variational objective that matches a posterior formed by "action backtracking" and a forward model, with the intrinsic reward coming from the reward map. The authors might be interested in related work on few shot imitation learning: e.g., "One shot imitation learning", Duan et al, 2017, "Watch, try learn: meta learning from demonstrations and rewards", Zhou et al 2019. As all reviewers pointed out, and I can confirm, the paper is quite tricky to understand in its present form, and would very much benefit the writing being re visited to more clearly express the ideas within (in particular, section 3, which is the core of the contributions).  
This paper studies an RL problem with vector rewards, where the goal is to maximize the expected minimum total reward (ex post max min fairness). This is different from prior works on a similar topic, where the goal is to maximize the minimum expected total reward (ex ante max min fairness). The authors propose an algorithm for solving the problem with $O(T^{2 / 3})$ regret and evaluate it.  This paper received two borderline reject and two reject reviews. The reviewers recognize the novelty of the objective. However, they are also concerned with its motivation and that the proposed algorithm relies on strong assumptions, such as that the used oracle knows the underlying reward and transition models, or at least has some estimate of them. At the end, the scores of this paper are not good enough for acceptance. Therefore, it is rejected.
This paper received 4 reviews with mixed initial ratings: 5, 6, 4, 4. The main concerns of R1, R4 and R2, who gave unfavorable scores, included: insufficient evaluation (lack of experiments on public datasets, small sample size), an ad hoc nature and overall limited novelty of the method, a number of issues with the presentation. In response to that, the authors submitted a new revision and provided detailed answers to each of the reviews separately. After having read the rebuttals, the reviewers (including R3, who initially gave a positive rating) felt that this work overall lacks methodological novelty and does not meet the bar for ICLR. As a result, the final recommendation is to reject.
The reviewers raised a number of major concerns including the incremental novelty of the proposed (if any), insufficient explanation, and, most importantly, insufficient and inadequate experimental evaluation presented. The authors did not provide any rebuttal. Hence, I cannot suggest this paper for presentation at ICLR.
This paper proposes to pre train contextual semantic parsing models on synthesized data (using a small amount of additional supervised training data and grammar based generalizations therefrom) with two new training objectives: Column Contextual Semantics (CCS), mapping text to database columns, and Turn Contextual Switch (TCS), to deal with the update semantics between turns.   I thank the reviewers for their detailed engagement with this paper, and thanks the authors for their responsiveness in doing extra experiments and rewriting that made this paper better and the decision clearer.  Pros  The authors did such a great job of summarizing the pros, that I think I can just copy their summary: "We are glad that the reviewers appreciate the novelty and the effectiveness of our proposed approach (R5), find our experiments to be comprehensive and convincing by achieving SOTA on 3 out of 4 different tasks (R1, R2, R3, R4), ablation studies and analysis to be informative and well done (R2, R4), and think our paper is clearly written and easy to follow (R1, R2, R3, R4)."  Cons    A somewhat specific and ad hoc data synthesis solution   Stronger pre trained contextual language models might beat assumed baselines or methods shown here (R4, R5)   The story is weak and should be better motivated through discussion of contextualization of interpretation  In general the reviewers recommend accepting the paper, and I agree. However, it is perhaps not of the novelty, clarity, or impact size to qualify for more than a Poster. R5 has a good point about how strong pre trained LMs are a general tool and should be preferred to the extent they work in 2020, but I think they are too opinionated to suggest this is a reason for rejection. Along with the other reviewers and the authors, I think it is most reasonable to accept work showing good progress using "medium sized" pre trained LMs   really we thought BERT was big a couple of years ago!   and this work has comprehensive experiments with good results. I would encourage the authors:    To say more about the alternative strategy of instead using a bigger pre trained LM, as has come out in the discussion on OpenReview, and the pros and cons of this approach (though maybe the results with BART are the only fairly comparable data point)   To strengthen the presentation by orienting the paper more around the importance of contextualization in interpreting dialog turns in conversational semantic parsing (as opposed to the "one turn" nature of the original famous semantic parsing datasets).  p.s. One typo I noticed in the revised paper while reading: fours  > four 
The paper proposes a multi scale spatial temporal joint graph convolution for spatiotemporal forecastings. Many reviewers have concerns regarding novelty, baseline comparisons, and writing clarity of the draft.
The paper explores the setting of *just* using data augmentation without an additional regularization term included.  The submission claims that comparatively good performance can be achieved with data augmentation alone.  The reviewers unanimously felt that the submission was not suitable for publication at ICLR.  The reasons included skepticism that augmentation without regularization is a useful setting to explore, as well as concerns about the experiments used to support the conclusions in the paper.  In particular, there were concerns that the experiments do not match best practice and that the error rates were too high.  Finally, there were concerns about the clarity of definitions of "implicit" and "explicit" regularization.
The paper presents a variational inequality perspective on the optimization problem arising in GANs. Convergence of stochastic gradient descent methods (averaging and extragradient variants) is given under monotonicity (or convex) assumptions. In particular, binlinear saddle point problem is carefully studied with batch and stochastic algorithms. Experiments on CIFAR10 with WGAN etc. show that the proposed averaging and extrapolation techniques improve the GAN training in such a nonconvex optimization practices.  General convergence results in the context of general non monotone VIPs is still an open problem for future exploration. The questions raised by the reviewers are well answered. The reviewers unanimously accept the paper for ICLR publication.
This paper proposes a model which learns simultaneously the dynamics of sequential data, together with a static latent representation. The idea and motivation is interesting and the results are promising.  However, all reviewers agree that the presentation needs much more work to convey the messages correctly and convincingly. Moreover, the reviewers question some design choices and lack of discussion of the results. No rebuttal has been provided.  
This paper proposes a method to improve the robust accuracy of classifiers using test time training.  The reviewers all agree that the method is interesting, and many reviewers had a positive view of the method.  However, two main criticisms remain: (i) the method increases the runtime of inference, and (ii) comparisons to other related methods were lacking.  The authors responded to (i) by reporting runtimes for their method in the rebuttal.  Some reviewers were concerned that the runtime increase of the method is not acceptable, however I am not very concerned with this issue since I think the paper contains an interesting methodology even if it’s not ready for deployment at the industrial scale.  However, issue (ii) does not seem to have been adequately addressed.  The comparison to SOAP is a welcome addition the reviewers acknowledge, but a number of other methods, for example masking and cleansing, are closely related (but different) and so comparisons should be provided.
This work proposes a self supervised segmentation method: building upon Crawford and Pineau 2019, this work adds a Monte Carlo based training strategy to explore object proposals. Reviewers found the method interesting and clever, but shared concerns about the lack of a better comparison to Crawford and Pineau, as well as generally a lack of care in comparisons to others, which were not satisfactorily addressed by authors response. For these reasons, we recommend rejection.
The paper proposes evaluation metrics for quantifying the quality of disentangled representations. There is consensus among reviewers that the paper makes a useful contribution towards this end. Authors have addressed most of reviewers  concerns in their response.
After careful consideration, I think that this paper in its current form is just under the threshold for acceptance. Please note that I did take into account the comments, including the reviews and rebuttals, noting where arguments may be inconsistent or misleading.  The paper is a promising extension of RCC, albeit too incremental. Some suggestions that may help for the future:  1) Address the sensitivity remark of reviewer 2. If the hyperparameters were tuned on RCV1 instead of MNIST, would the results across the other datasets remain consistent?  2) Train RCC or RCC DR in an end to end way to gauge the improvement of joint optimization over alternating, as this is one of the novel contributions.  3) Discuss how to automatically tune \lambda and \delta_1 and \delta_2. These may appear in the RCC paper, but it s unclear if the same derivations hold when going to the non linear case (they may in fact transfer gracefully, it s just not obvious). It would also be helpful for researchers building on DCC.
The paper presents AnoDM (Anomaly detection based on unsupervised Disentangled representation learning and Manifold learning) that combine beta VAE and t SNE for anomaly detection. Experiment results on both image and time series data are shown to demonstrate the effectiveness of the proposed solution.   The paper aims to attack a challenging problem. The proposed solution is reasonable. The authors did a job at addressing some of the concerns raised in the reviews. However, two major concerns remain: (1) the novelty in the proposed model (a combination of two existing models) is not clear; (2) the experiment results are not fully convincing. While theoretical analysis is not a must for all models, it would be useful to conduct thorough experiments to fully understand how the model works, which is missing in the current version.   Given the two reasons above, the paper did not attract enough enthusiasm from the reviewers during the discussion. We hope the reviews can help improve the paper for a better publication in the future.     
This paper provides a series of empirical evaluations on a small neural architecture search space with 64 architectures. The experiments are interesting, but limited in scope and limited to 64 architectures trained on CIFAR 10. It is unclear whether lessons learned on this search space would transfer to large search spaces. One upside is that code is available, making the work reproducible.  All reviewers read the rebuttal and participated in the private discussion of reviewers and AC, but none of them changed their mind. All gave a weak rejection score.  I agree with this assessment and therefore recommend rejection.
This paper receives positive reviews. The authors provide additional results and justifications during the rebuttal phase. All reviewers find this paper interesting and the contributions are sufficient for this conference. The area chair agrees with the reviewers and recommends it be accepted for presentation.
The paper presents an empirical analysis of Vision Transformers   and in particular multi headed self attention   and ConvNets, with a focus on optimization related properties (loss landscape, Hessian eigenvalues). The paper shows that both classes of models have their strengths and weaknesses and proposes a hybrid model that takes the best of both worlds and demonstrates good empirical performance.  Reviewers are mostly very positive about the paper. Main pro is that analysis is important and this paper does a thorough job at it and draws some useful insights. There are several smaller issues with the presentation and the details of the content, but the authors did a good job addressing these in their responses.  Overall, it s a good paper on an important topic and I recommend acceptance.
This paper studies an interesting information theoretic trade off between accuracy and invariance by posing it as a minimax problem. The results are of theoretical nature. However, the implications of the results are not clear. Also, the model/assumptions authors consider are not completely justified. Therefore, the paper at this stage is not recommended for acceptance. However, I highly encourage the authors to improve upon their existing work and resubmit to the next ML conference. 
This paper presents a differentially private mechanism, called Noisy ArgMax, for privately aggregating predictions from several teacher models. There is a consensus in the discussion that the technique of adding a large constant to the largest vote breaks differential privacy. Given this technical flaw, the paper cannot be accepted.
This paper combines two ideas: MAML, and the hierarchical Bayesian inference approach of Amit and Meir (2018). The idea is fairly straightforward but well motivated, and it seems to work well in practice.  The paper is well written and includes good discussion of the relevant literature. The experiments show improvements on various tests of Bayesian inference, and include some good analysis beyond simply reporting better numbers.  On the whole, the reviewers are fairly positive about the paper. (While the numerical scores are slightly below the cutoff, the reviewers are more positive in the discussion.) The reviewers  main complaint is the lack of comparisons against recently published methods, especially Gordon et al. (2018). The lack of comparison to this paper doesn t strike me as a big problem; the preprint was released only a few months before the deadline, their approach was very different from the proposed one, and the proposed approach has some plausible advantages (simplicity, computational efficiency), so I don t think a direct comparison is required for acceptance.  Overall, I recommend acceptance. 
This paper tackles the problem of long term time series forecasting. One challenge in long term forecasting is that often no sufficient date may be available. This paper proposes to use GANs to generate data that can be used to improve long range forecasts.  While reviewers agree that this paper presents an interesting idea towards tackling the problem of long term time series forecasting and appreciate the effort authors put forward in addressing their concerns and comments during the response period, the reviewers believe that in the current form paper is not ready for the publication. In particular:  1. Reviewers find that overall technical contribution of this work is limited compared to other submissions. 2. Comparison of LSTM and cWGAN GEP in the response is only on synthetic data, which does not address reviewers concerns.    
The work proposed an interesting source free adaptation setting, where one is asked to adapt a pre trained source model to a target domain without accessing data from the source domain. While reviewers find the setup interesting and the initial results encouraging, they expressed concerns on the limited novelty of the work as well as incomplete evaluation. Multiple reviewers (reviewer Lx65  and pNRq) raised concerns on the fairness of the evaluation, which was not fully addressed by the authors during rebuttal. Please consider addressing these comments in your draft.
This paper proposes a new method for verifying whether a given point of a two layer ReLU network is a local minima or a second order stationary point and checks for descent directions. All reviewers agree that the algorithm is based on number of new techniques involving both convex and non convex QPs, and is novel. The method proposed in the paper has significant limitations as the method is not robust to handle approximate stationary points. Given these limitations, there is a disagreement between reviewers about the significance of the result . While I share the same concerns as R4, I agree with R3 and believe that the new ideas in the paper will inspire future work to extend the proposed method towards addressing these limitations. Hence I suggest acceptance. 
This paper proposes a new gradient based stochastic optimization algorithm by adapting theory for proximal algorithms to the non convex setting.   The majority of reviewers voted for accept. The authors are encouraged to revise with respect to reviewer comments.
The paper develops a diffusion process based generative model that perturbs the data using a critically damped Langevin diffusion. The diffusion is set up through an auxiliary velocity term like in Hamiltonian dynamics. The idea is that picking a process that diffuses faster will lead to better results.The paper then constructs a new score matching objective adapted to this diffusion, along with a sampling scheme for critically damped Langevin score based generative models. The idea of a faster diffusion to make generative models is a good one. The paper is a solid accept.    Reviewer tK3A was lukewarm as evidenced by their original 2 for empirical novelty that moved to a 3. From my look, it felt like a straightforward application of ideas in one domain, sampling, to another, generative modeling. It s a good paper, but it does not stand out relative to other accepts.
This paper proposes a new kernel learning framework for change point detection by using a generative model. The reviewers agree that the paper is interesting and useful for the community. One of the reviewer had some issues with the paper but those were resolved after the rebuttal. The other two reviewers have short reviews and somewhat low confidence, so it is difficult to tell how this paper stands among other that exist in the literature. Overall, given the consistent ratings from all the reviewers, I believe this paper can be accepted. 
The present paper establishes uniform approximation theorems (UATs) for PointNet and DeepSets that do not fix the cardinality of the input set.   Two nonexperts read the paper and came away not understanding what this exercise has taught us and why the weakening of the hypotheses was important. The authors made no attempt to argue these points in their rebuttals and so I went looking at the paper to find the answer in their revisions, but did not find it after scanning through the paper. I think a paper like this needs to explain what is gained and what obstructions earlier approaches met, and why the current techniques side step those. One of the reviewers felt that the fixed cardinality assumption was mild. I m really not sure why the authors didn t attack this idea. Maybe it is mild in some technical sense?  What I read of the paper seemed excellent in term of style and clarity. I think the paper simply needs to make a better case that it is not merely an exercise in topology. I think the result here is publishable on its own grounds, but for the paper to effectively communicate those findings, the authors should have revised it to address these issues. They chose not to and so I recommend ICLR take a pass. Once the reviewers revised the framing and scope/impact, provided it doesn t sound trivial, I think it ll be ready for publication.  
This paper provides a simple and intuitive method for learning multilingual word embeddings that makes it possible to softly encourage the model to align the spaces of non English language pairs. The results are better than learning just pairwise embeddings with English.  The main remaining concern (in my mind) after the author response is that the method is less accurate empirically than Chen and Cardie (2018). I think however that given that these two works are largely contemporaneous, the methods are appreciably different, and the proposed method also has advantages with respect to speed, that the paper here is still a reasonably candidate for acceptance at ICLR.  However, I would like to request that in the final version the authors feature Chen and Cardie (2018) more prominently in the introduction and discuss the theoretical and empirical differences between the two methods. This will make sure that readers get the full picture of the two works and understand their relative differences and advantages/disadvantages.
The paper proposed a new in processing approach to train fair predictors under several notions of statistical fairness. Tho this end, the author rely  on  the Exponential Renyi Mutual Information (ERMI) between sensitive attributes and the target variable as notion f fairness, and show that it is a strong notion of fairness that provides guarantees on several previously discussed fairness metrics.   The paper is overall well written and interesting, but as with many other papers on this area, I wonder even after rebuttal whether the paper indeed constitute a step forward in the field. I find the concern raised by the reviewers about the tightness of the bound important and, while the authors properly addressed this point in the rebuttal period, I still believe this is an open question which probably does not have a better answer. On the positive side, the experimental evaluation support the theoretical results. However, comparisons to previous methods are only performed on the Adult and the German dataset, which makes me wonder if the advantages of the proposed approach generalize beyond these two well studied datasets. As a consequence, the paper remains borderline, as it is an interesting paper but its impact and significance remain limited.    Moreover, I believe that there are some missing recent related works, that I believe the authors should also compare to. For example, see the recent Neurips 2020 paper, "A Fair Classifier Using Kernel Density Estimation" by Cho et al.  Also, as a side note, previous approached have already considered non binary (although most of the times categorical) sensitive features, see e.g., [42]. Finally, the author may want to consider complementing their italic comment on the second paragraph of the Intro with existing works that already discussed biased in the labels, due to e.g., the selective labeling problem (see [1 3] below).     [1] Lakkaraju, Himabindu, et al. "The selective labels problem: Evaluating algorithmic predictions in the presence of unobservables." Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017. [2] Kilbertus, Niki, et al. "Fair decisions despite imperfect predictions." International Conference on Artificial Intelligence and Statistics. PMLR, 2020. [3] Bechavod, Yahav, et al. "Equal opportunity in online classification with partial feedback." Advances in Neural Information Processing Systems. 2019.     
I agree with the reviewers that this is a strong contribution and provides new insights, even if it doesn t quite close the problem.   p.s.: It seems that centering the weight matrices at initialization is a key idea. The authors note that Dziugaite and Roy used  bounds that were based on the distance to initialization, but that their reported numerical generalization bounds also increase with the increasing network size. Looking back at that work, they look at networks where the size increases by a very large factor (going from e.g. 400,000 parameters roughly to over 1.2 million, so a factor of 2.5), at the same time the bound increases by a much smaller factor. The type of increase also seems much less severe than those pictured in Figures 3/5. Since Dzugate and Roy s bounds involved optimization, perhaps the increase there is merely apparent.
The paper focuses on the task of weakly supervised activity detection (WSAD). The proposed method combines various ideas together: **(i)** a cross attention module for audio visual information fusion and better representation, **(ii)** an open max classifier to treat the background as an open set, and **(iii)** loss terms to encourage temporal continuity of action predictions. The experimental results on well known benchmark datasets are promising as they beat out many other methods in the literature.  Based on the reviewers  comments, it is clear that the reviewers unanimously see value in the proposed methodology and the competitive results. To strengthen the paper, the authors are encouraged to provide a stronger validation of the contributions being claimed for the specific task being addressed. This would more concretely position the claimed contributions in the WSAD literature. 
The authors present a GAN for learning a continuous representation of disease related image patterns from regional volume information generated from structural MRI images. The reviewers find the problem relevant and appreciate the proposed solution. They find the paper well written and find the empirical results on Alzheimer brain MRIs relevant for the neuroscience community.   The overall objective function includes several hyper parameters. As pointed out as the main weak point by multiple reviewers this may hint at overengineering/overfitting to a data set. However, the reviewers also mention that the regularizers are all sufficiently well motivated in the paper and the author response.  Reviewers highlight comparisons on the real data as a strong result demonstrating that Surreal GAN was able to isolate two major sources/locations of atrophy in Alzheimer’s disease. Overall, the reviews are positive in majority.
This paper presents an unsupervised GAN based model for disentagling the multiple views of the data and their content.  Overall it seems that this paper was well received by the reviewers, who find it novel and significant . The consensus is that the results are promising.  There are some concerns, but the major ones listed below have been addressed in the rebuttal. Specifically:  	R3 had a concern about the experimental evaluation, which has been addressed in the rebuttal.  	R2 had a concern about a problem inherent in this setting (what is treated as “content”), and the authors have clarified in the discussion the assumptions under which such methods operate.  	R1 had concerns related to how the proposed model fits in the literature. Again, the authors have addressed this concern adequately. 
The reviewers have reached a consensus that this paper is very interesting and add insights into interpolation in autoencoders.
The paper presents a novel regularization scheme for showing provable robustness guarantees for the class of ReLU networks. The reviews of this paper were mixed but leaning more to reject. Even though geometric approaches to adversarial robustness are appealing there are too many issues left open (see below for detailed comments) so that the paper is not above the bar for ICLR.  Detailed comments:    incremental resp. small theoretical contribution: the bounds in Theorem 1 are worse than what has been derived in Croce et al (2019), which is admitted only in the Experiments section but this should directly be discussed after Theorem 1, or are based on other s work Moosavi Dezfooli et al(2019)    the proofs are still not very clear even after the update e.g. it is not clear to me what is meant with the distance to the decision boundary d_D(x) as this quantity cannot be computed (this is the whole point here)    the experimental results are in most of the cases worse than prior work (KW and MMR). The authors argue that their results hold for both threat models but then the l_infty/l_2 robustness of the other models should have been provided to make the point. Moreover, if multiple norm ball threat models is the point, then the authors should compare to the follow up work of Croce et al at ICLR 2020 which explicitly optimizes all l_p balls.    the point with gradient obfuscation of MMR does not make sense to me as the gap between upper and lower bounds on the robust error is quite small for MMR in most cases. Gradient obfuscation would mean that the PGD attack fails and thus the lower bound would have to be close to zero.    Figure 1c) looks like the Figure from the book of Boyd and Vandenberghe   if this is the case then it has to be correctly referenced in the caption 
In this paper, the authors introduce an exploration method for RL according to experimental design perspective via designing an acquisition function, which quantifies how much information a state action pair would provide about the optimal solution to a Markov decision process, and the state action that maximizes such acquisition function will be used for sampling for policy update. The empirical evidences show the proposed method is promising.   Since most of the reviewers support the paper, I recommend acceptance of this submission.   However, besides the questions raised by the reviewers, e.g., computation cost and planning quality from CEM, there is a major issue need to be clarified in the paper:  >The algorithm designed for RL with generative model, which makes the state action reset can be conducted (this is sometimes impossible in practice where the agent must start from initial state). This is different from the common RL setting, and thus reduce the complexity of RL. This should be emphasized in the paper. Meanwhile, for a fair comparison, this should be explicitly specified in experiment setting.
In the paper, the authors propose a new method for estimating the mutual information based on a neural network classification that is fairly straight forward. The proposed method compares relatively well with known methods for estimating mutual information with a very large number of samples. The main issue of this classifier (a neural network) is that it requires that a classifier that discriminates between x, y pairs coming from p(x,y) and x, y pair coming from p(x)p(y) (this is done via reshuffling). The reviewers point out that the procedure is interesting, but it does not perform significantly better than the other proposed methods.  Also, I want to add that the proposed method is trained by using a given NN trained with 20 epochs and a mini batch of 64. This is a significant issue because if we train the NN to reduce the validation error the posterior probability estimates are typically overconfident a significant work is being done to calibrate them. Why 20? How do we select this number if we cannot use a validation set? With less training example does 20 also work? This is very relevant because in the areas in which p(x,y)/p(x)p(y) is low for very high MI values getting these estimates correctly is critical. The classifier does not need to perform accurately in classification, but an estimation of the posterior probability and NNs will tend to be overconfident here and provide a biased estimate for these values. It will also provide an overestimate probability in the area that both p(x,y) and p(x)p(y) are high.   Finally, the authors reference the paper by Nguyen, Wainwright, and Jordan, but they do not acknowledge how that paper actually estimates log(p(x,y)/p(x)p(y)) similarly. That paper is very general and theoretical, and this paper can only be understood as a particular implementation of their solution. I think the authors missed that point in their paper. Also, I think the authors should acknowledge the papers that have come before using nearest neighbor or histograms for entropy estimation. 
This is an excellent paper that provides analytical and empirical insights on the sample selection bias of pool based active learning. It provides two very practical methods of removing the bias.  Also, it shows that in over parameterized networks (like modern neural networks), the active learning bias could actually be useful.   I enjoyed reading the paper.  Reviewers are mostly very positive about the paper.  Experiments in the initial version were limited, and the authors have since added more experiments.   With the increasing interest on learning with limited data, this paper is very timely and useful.  I expect the paper to be of interest to many in the community.
The authors present a multiple instance learning based approach that uses weak supervison (of which skills appear in any given trajectory)  to automatically segment a set of skills from demonstrations.  The reviewers had significant concerns about the significance and performance of the method, as well as the metrics used for analysis.  Most notably, neither the original paper nor the rebuttal provided a sufficient justification or fix for the lack of analysis beyond accuracy scores (as opposed to confusion matrices, precision/recall, etc), which leaves the contribution and claims of the paper unclear.  Thus, I recommend rejection at this time.
Dear Authors,  The paper was received nicely and discussed during the rebuttal period. However, the current consensus suggests the paper requires another round of revisions before it gets accepted.  In particular:    There were still some gray areas regarding comparison to simple techniques. E.g., one reviewer raised the question how it compares to simply stopping based on validation accuracy for example. The reviewer was missing the justification why stopping at the loss of Ramanujan graph property is preferable in comparison to other criteria.    Several reviewers found the general idea interesting, but all felt that more reasonings about the impact/insights/relationship of Ramanujan graph property with pruning need to be found to get accepted.   Reviewers appreciate that the authors corrected many parts of the submission (see increased scores). However, reviewers felt that the paper requires more data/evidence to get accepted at this level, based on the discussions made during the rebuttal period.   Best AC
The paper proposes how weight encoded neural implicit can be strong 3D shape representations. A neural network is trained such that it overfits over a single shape, and the weights of such network is a great representation for the 3D shape. Results are shown on signed distance field (SDF) generation from meshes.  Strengths:   an interesting idea for generating compact representations of 3D shapes   Will further foster several conversations within the deep learning community  Weaknesses:   Very limited evaluation to support the authors  claims, particularly against other traditional learnable 3D representations
The pros and cons of the paper under consideration can be summarized below:  Pros: * Reviewers thought the underlying model is interesting and intuitive * Main contributions are clear  Cons: * There is confusion between keywords and topics, which is leading to a somewhat confused explanation and lack of clear comparison with previous work. Because of this, it is hard to tell whether the proposed approach is clearly better than the state of the art. * Typos and grammatical errors are numerous  As the authors noted, the concerns about the small dataset are not necessarily warranted, but I would encourage the authors to measure the statistical significance of differences in results, which would help alleviate these concerns.  An additional comment: it might be worth noting the connections to query based or aspect based summarization, which also have a similar goal of performing generation based on specific aspects of the content.  Overall, the quality of the paper as is seems to be somewhat below the standards of ICLR (although perhaps on the borderline), but the idea itself is novel and results are good. I am not recommending it for acceptance to the main conference, but it may be an appropriate contribution for the workshop track.
The authors have conducted a thorough empirical study on the hyperparameters of representative adversarial training methods. The technical novelty of this paper might be insufficient.  But the empirical findings in this paper explain the strange and inconsistent reported algorithm results in the literature to some extent and remind the necessity and importance of a careful study on hyperparameters. The authors have actively interacted with the reviewers and through the discussions, many unclear issues have been fixed.  
This paper adds a new model to the literature on representation learning from correlated variables with some common and some "private" dimensions, and takes a variational approach based on Wyner s common information.  The literature in this area includes models where both of the correlated variables are assumed to be available as input at all times, as well as models where only one of the two may be available; the proposed approach falls into the first category.  Pros:  The reviewers generally agree, as do I, that the motivation is very interesting and the resulting model is reasonable and produces solid results.  Cons:  The model is somewhat complex and the paper is lacking a careful ablation study on the components.  In addition, the results are not a clear "win" for the proposed model.  The authors have started to do an ablation study, and I think eventually an interesting story is likely to come out of that.  But at the moment the paper feels a bit too preliminary/inconclusive for publication.
This paper presents an approach to utilize conventional frequency domain basis such as DWHT and DCT to replace the standard point wise convolution, which can significantly reduce the computational complexity. The paper is generally well written and easy to follow. However, the technical novelty seems limited as it is basically a simple combination of CNNs and traditional filters. Moreover, as reviewers suggested, it is our history and current consensus in the community that learned representations have significantly outperformed traditional pre defined features or filters as the training data expands. I do understand the scientific value of revisiting and challenging that belief as commented by R1, but in order to provoke meaningful discussion, experiments on large scale dataset like ImageNet are definitely necessary. For these reasons, I think the paper is not ready for publication at ICLR and would like to recommend rejection.
This paper presents an extension of MPNN which leverages the random color augmentation to improve the representation power of MPNN. The experimental results shows the effectiveness of colorization. A majority of the reviewers were particularly concerned about lacking permutation invariance in the approach as well as the large variance issue in practice, and their opinion stays the same after the rebuttal. The reviewers unanimously expressed their concerns on the large variance issue during the discussion period. Overall, the reviewers believe that the authors has not addressed their concerns sufficiently.
The paper aims to extract the set of features explaining a class, from a trained DNN classifier.  The proposed approach relies on LIME (Ribeiro et al. 2016), modified as follows: i) around a point x, a linearized sparse approximation of the classifier is found (as in LIME); ii) for a given class, the importance of a feature aggregates the relative absolute weight of this feature in the linearized sparse approximations above; iii) the explanation is made of the top features in terms of importance.  This simple modification yields visual explanations that significantly better match the human perception than the SOTA competitors.   The experimental setting based on the human evaluation via a Mechanical Turk setting is the second contribution of the approach. The feature importance measure is also assessed along a Keep and Retrain mechanism, showing that the approach selects actually relevant features in terms of prediction.  Incidentally, it would be good to see the sensitivity of the method to parameter $k$ (in Eq. 1).  As noted by Rev#1, NormLIME is simple (and simplicity is a strength) and it demonstrates its effectiveness on the MNIST data. However, as noted by Rev#4, it is hard to assess the significance of the approach from this only dataset.   It is understood that the Mechanical Turk based assessment can only be used with a sufficiently simple problem.  However, complementary experiments on ImageNet for instance, e.g., showing which pixels are retained to classify an image as a husky dog, would be much appreciated to confirm the merits and investigate the limitations of the approach. 
 This is an interesting topic but the reviewers had substantial concerns on the clarity and significance of the contribution. 
All the reviewers recommend rejecting the paper. There is no basis for acceptance.
This paper analyzes the weights associated with filters in CNNs and finds that they encode positional information (i.e. near the edges of the image).  A detailed discussion and analysis is performed, which shows where this positional information comes from.    The reviewers were happy with your paper and found it to be quite interesting.  The reviewers felt your paper addressed an important (and surprising!) issue not previously recognized in CNNs.
The paper proposes a hyper net method for multi objective optimization, which trains a neural network that maps preference vector to the corresponding Pareto solution. The proposed idea is interesting and useful, although the evaluation of the work is not overwhelming convincing. The writing of the work can be further improved.   Also, the basic idea of the work is the almost the same as a concurrent work "Lin et al 2020. controllable pareto multi task learning" which is also submitted to this conference. The paper cited that paper briefly, "... The proposed method is conceptually similar to our approach...",  which is too vague and brief. We urge the author to provide a through discussion on the detailed difference and similarity of the works, including empirical comparisons when necessary. 
This paper introduces a few variants of neural ODE architectures to improve their expressivity.  The motivation and method make sense, but are fairly incremental.  The tasks are also fairly low dimensional and as one reviewer pointed out, reconstruction isn t a good benchmark task.  However, the paper seems well executed, and the rebuttals answered the expert rewiewers  concerns.
The paper proposes a novel sample based evaluation metric which extends the idea of FID by replacing the latent features of the inception network by those of a data set specific (V)AE and the FID by the mean FID of the class conditional distributions. Furthermore, the paper presents  interesting examples for which FID fails to match the human judgment while the new metric does not. All reviewers agree, that while these ideas are interesting, they are not convinced about the originality and significance of the contribution and believe that the work could be improved by a deeper analysis and experimental investigation.  
This paper proposes a new mechanism to visualize the latent space of a neural network. The idea is simple and the paper includes several experiments to test the effectiveness of the method. However, the method bears similarity to previous work and the evaluation does not sufficiently show quantitative improvements over other introspection techniques. The reviewers found this was a substantial problem and for this reason the paper is not ready for publication. The paper should improve its discussion of prior work and better establish its place in this regard.
ICLR is selective and reviewers are not sufficiently enthusiastic about this paper. In particular, they point out closely related methods that should be cited and compared to as baselines. The reviews are of good quality, and the authors did not respond.
This paper presented an online continual learning method where there may be a shift in data distribution at test time. The paper proposes a Conditional Invariant Experience Replay (CIER) approach to correct the short which matches the distribution of inputs conditioned on the outputs. This is based on an adversarial training scheme.  The reviewers found the problem setting interesting but found the approach to be lacking in novelty and problem formulation somewhat restrictive (e.g.,  requiring domain id during training). The author feedback was taken into account but the reviewers stayed with their original assessment and, even after the rebuttal phase, none of the reviewers is in favor of accepting the paper.  The authors are advised to consider the feedback from the reviewers which will hopefully help to improve the paper for a future submission to another venue.
This submission proposes an explainability method for deep visual representation models that have been trained to compute image similarity.   Strengths:  The paper tackles an important and overlooked problem.  The proposed approach is novel and interesting.  Weaknesses:  The evaluation is not convincing. In particular (i) the evaluation is performed only on ground truth pairs, rather than on ground truth pairs and predicted pairs; (ii) the user study doesn’t disambiguate whether users find the SANE explanations better than the saliency map explanations or whether users tend to find text more understandable in general than heat maps. The user study should have compared their predicted attributes to the attribute prediction baseline; (iii) the explanation of Figure 4 is not convincing: the attribute is not only being removed. A new attribute is also being inserted (i.e. a new color). Therefore it’s not clear whether the similarity score should have increased or decreased; (iv) the proposed metric in section 4.2 is flawed: It matters whether similarity increases or decreases with insertion or deletion. The proposed metric doesn’t reflect that.  Some key details, such as how the attribute insertion process was performed, haven’t been explained.   The reviewer ratings were borderline after discussion, with some important concerns still not having been addressed after the author feedback period. Given the remaining shortcomings, AC recommends rejection.
This paper proposes a new method to mine sentence from Wikipedia and use them to train an MT system, and also a topic based loss function. In particular, the first contribution, which is the main aspect of the proposal is effective, outperforming methods for fully unsupervised learning.  The main concern with the proposed method, or at least it s description in the paper, is that it isn t framed appropriately with respect to previous work on mining parallel sentences from comparable corpora such as Wikipedia. Based on interaction in the reviews, I feel that things are now framed a bit better, and there are additional baselines, but still the explanation in the paper isn t framed with respect to this previous work, and also the baselines are not competitive, despite previous work reporting very nice results for these previous methods.  I feel like this could be a very nice paper at some point if it s re written with the appropriate references to previous work, and experimental results where the baselines are done appropriately. Thus at this time I m not recommending that the paper be accepted, but encourage the authors to re submit a revised version in the future.
The authors propose a particle based entropy estimate for intrinsic motivation for pre training an RL agent to then perform in an environment with rewards. As the reviewers discussed, and also mentioned in their reviews, this paper bears stark similarity to work of 5 months ago, presented at the ICML 2020 Lifelong ML workshop, namely, "A Policy Gradient Method for Task Agnostic Exploration", Mutti et al, 2020 MEPOL. What is novel here is the adaptation of this entropy estimate to form an intrinsic reward via a contrastive representation and the subsequent demonstration on standardized RL environments.  The authors have added a comparison to MEPOL, and in these experiments, APT outperforms this method, sometimes by some margin. Unfortunately this work does not meet the bar for acceptance relative to other submissions. 
This paper studies the optimal value function for the gambler s problem, and presents some interesting characterizations thereof. The paper is well written and should be accepted.
In this paper, the authors proposed a new approach by the name of LoCal + SGD (Localized Updates) to replace the traditional Backpropagation method. The key idea is to selectively update some layers’ weights using localized learning rules, so as to reduce the computational complexity of training these layers so as to achieve a better tradeoff between overall speed and accuracy. The paper received quite mixed reviewers. Some reviewers criticized the incremental nature of the proposed technology, while some other reviewers thought that this is one of the very early papers that demonstrates the practical effectiveness of localized learning.   The reviewers have made several rounds of discussions, and as a result of that, we think while this direction (localized learning) is very important and promising, this particular paper might not have provided a sufficiently novel and good solution to it.  Specifically, in terms of localized learning, this paper has not proposed brand new concepts or methodologies, instead it adopts existing methods in selective layers. In this sense, it does not really resolve the accuracy issue of localized learning, rather, it achieves the tradeoff by only applying localized learning in some layers. In other words, the current results still heavily rely on BP and has not brought a real breakthrough to localized learning. 
The proposed method has very weak novelty.
The reviewers were unanimous that this submission is not ready for publication at ICLR in its current form.   Concerns raised include a significant lack of clarity, and the paper not being self contained.
The paper proposes a new pipeline parallel training method called WPipe. WPipe works (on a very high level) by replacing the two buffer structure of PipeDream 2BW with a two partition group structure, allowing resources to be shared in a similar way to PipeDream 2BW but with less memory use and less delays in weight update propagation across stages. The 1.4x speedup it achieves over PipeDream 2BW is impressive.  In discussion, the reviewers agreed that the problem WPipe tries to tackle is important and that the approach is novel and interesting. But there was significant disagreement among the reviewers as to score. A reviewer expressed concern about the work being incremental and difficult to follow. And while these were valid concerns, and the authors should take note of them when revising their paper, I do not think they should present a bar to publication, both based on my own read of the work and also in light of the fact that other reviewers with higher confidence scores did not find novelty to be a disqualifying concern. As a result, I plan to follow the majority reviewer opinion and recommend acceptance here.
  This paper derives estimators and minimax guarantees for regression under additive Gaussian noise and distributional shift  Despite, some merits raised, limitations on too strong assumptions (knowing the entire marginal distributions, sample complexity bound that could become meaningless if what is assumed known itself is too sample intensive to approximate) were considered important drawbacks by the reviewers. Last but not least, the importance of domain adaptation for fixed design regression was questioned and the answer to that point was not convincing enough. 
This paper proposes an algorithm to produce well calibrated uncertainty estimates. The work accomplishes this by introducing two loss terms: entropy encouraging loss and an adversarial calibration loss to encourage predictive smoothness in response to adversarial input perturbations.   All reviewers recommended weak reject for this work with a major issue being the presentation of the work. Each reviewer provided specific examples of areas in which the paper text, figures, equations etc were unclear or missing details. Though the authors have put significant effort into responding to the specific reviewer mentions, the reviewers have determined that the manuscript would benefit from further revision for clarity.   Therefore, we do not recommend acceptance of this work at this time and instead encourage the authors to further iterate on the manuscript and consider resubmission to a future venue.  
Although this paper is on an interesting topic, there is a consensus that this paper is below the bar for acceptance. My advice is to take take criticisms of the reviewers seriously, add the extra experiments, rewrite the paper and then submit it to a different conference. If the authors feel that the reviewers misunderstood their paper, please remember that the level to which they were able to understand it is also a function of how the paper is written.
The paper conveys interesting idea but need more work in terms of fair empirical study and also improvement of the writing. The AC based her summary only on the technical argumentation presented by reviewers and authors. 
This paper presents quasi hyperbolic momentum, a generalization of Nesterov Accelerated Gradient. The method can be seen as adding an additional hyperparameter to NAG corresponding to the weighting of the direct gradient term in the update. The contribution is pretty simple, but the paper has good discussion of the relationships with other momentum methods, careful theoretical analysis, and fairly strong experimental results. All the reviewers believe this is a strong paper and should be accepted, and I concur. 
This is a borderline paper which elicited much discussion. The paper proposes to extract features from pre trained networks through Kernel functions. It develops the idea of Fisher kernels for Neural networks calling it NFK. The methodology applies to both supervised and un supervised setting.  The paper shows that proposed kernel has low rank structure and serves as the basis for developing an algorithm for computing the kernel on large datasets. The idea of extending Fisher kernels, their efficient computation, and investigating their usage in both Supervised and Unsupervised are some of the key strengths of the paper. The reviewers though appreciative suggested (1) several new experiments, (2) inclusion of more background work related to Power method  and (3)  have more technical discussion clarifying the contributions related to background.  The author(s) during rebuttal tried to incorporate most of the suggestions in the revised draft.   Since there was consensus on the novelty, the detailed discussions, and the results of the additional experiments, one could potentially accept this paper if there is space. The results will be interesting will be those who investigate the interplay of kernel methods and Deep Networks.
The paper is about a reinforcement learning algorithm that operates in a Constrained MDP and is provided with a baseline policy. Although the reviewers acknowledge that the paper has some merits (well written, clearly organized, significant empirical evaluation, reproducible experimental results), some concerns have been raised about the novelty of the proposed solution and of its theoretical analysis. The reviewers feel that the authors  responses have not properly addressed all their doubts. The paper is borderline and I think that it is not ready for publication in the current form. I encourage the authors to update their paper following the reviewers  suggestions and try to submit it in one of the forthcoming machine learning conferences. 
The authors discuss the disconnect between log likelihood and sample quality of VAEs and relate it to an undesirable focus of the model on high frequency signals. They propose to alleviate it through a two stage training scheme for VAEs. As it is, the paper does not explain well its contributions, especially compared to the rate distortion balance discussion in "Fixing a Broken ELBo" by Alemi et al. (2018) (see [reviews sh3z](https://openreview.net/forum?id  0LuSWi6j4&noteId D52ninjThn1), [7Pio](https://openreview.net/forum?id  0LuSWi6j4&noteId 9qMQNUGk6bx), and [LBJj](https://openreview.net/forum?id  0LuSWi6j4&noteId gyG86hghxsU)), and lacks the experiments to back up its claim (see [LBJj](https://openreview.net/forum?id  0LuSWi6j4&noteId gyG86hghxsU), and [KKon](https://openreview.net/forum?id  0LuSWi6j4&noteId zeFApaHliSv)). While the authors have made a more precise statement about their contributions in their rebuttal, the writing remains unclear. I recommend this submission for rejection.
The paper shows that it is possible to reconstruct private images from CPU cache line and OS page table accesses side channels, using a generative model on top of side channel traces. The reviewers agree that the problem is interesting and the experimental evaluation makes a convincing case that such an attack is possible. The author rebuttal was useful in clarifying some aspects of the paper, and the discussion on possible mitigation strategies is a nice addition to the paper.
Main content: Authors developed graph scattering transforms (GST) with a pruning algorithm, with the aim to reduce the running time and space cost, improve robustness to perturbations on input graph signal, and encourage flexibility for domain adaption.  Discussion: reviewer 1: likes the idea, considers it to be elegant and work well. some questions regarding the proofs in the paper but it sounds like authors have addressed concerns. reviewer 2: solid paper and results, has questons on stability results, like reviewer 2. reviewer 3: likes the idea, including good sufficient theoretical analysis and algorthmic stability. concern is around complexity analysis but sounds like the authors have addressed the concerns. Recommendation: Well written solid paper with good proofs. Authors addressed any reviewer concerns and all 3 reviewres vote weak accept. This is good for poster.
The paper studies three aspects of the representational capabilities of normalizing flows, with a particular focus on affine coupling layers. Normalizing flows are valuable generative modelling tools, so advancing our understanding of their theoretical properties is an important research direction.  Reviewers #2 and #4 found the contribution of the paper significant without expressing major concerns, and so recommended acceptance.  Reviewer #3 reviewed the paper very thoroughly, and expressed some concerns mainly about the experimental evaluation. Most of their concerns were addressed in the rebuttal, so they recommended weak acceptance, recognizing the merits of the paper but also pointing out the potential for improvement.  Reviewer #1 was the most critical: they expressed major concerns regarding the significance of the contributions and the overall clarity of the exposition. Despite a long exchange between the reviewer and the authors, a consensus was not reached, so the concerns remain.  The discussion so far has led me to believe that there are potentially valuable theoretical contributions in the paper, however it s clear that there is significant room for improvement in getting the contributions across. Given the strong concerns expressed, the lack of consensus, and the clear potential for improvement, I m unable to recommend acceptance of the paper in its current form. However, I do believe that the work has potential, and I hope that the discussion here will help improve the paper for a future submission.
The paper presents an approach to semantic segmentation based on text embedding of class labels. This enables zero shot semantic segmentation with class labels that were not seen during training. I appreciate the new ablation against a ResNet 101 backbone. I don t find the similarity with CLIP substantial, and I recommend that the paper is accepted.
The authors re state Mackay s definition of effective dimensionality and describe its connections to posterior contraction in Bayesian neural networks, model selection, width depth tradeoffs, double descent, and functional diversity in loss surfaces. The authors claim the effective dimensionality leads to a richer understanding of the interplay between parameters and functions in deep neural networks models. In their experiments the authors show that effective dimensionality compares favourably to alternative norm  and flatness  based generalization measures.  Strengths:  1   The authors include a description of how to compute a scalable approximation to the effective dimensionality using the Lanczos algorithm and Hessian vector products.  2   The authors include some novel experimental results showing the effective dimensionality with respect to changes in width and depth. These results are informative in how changes in depth and width affect this metric in a different way. The same for the experiments with the double descent curve.  Weaknesses:  1   For some reason the authors seem to have taken the concept of effective dimensionality from David Mackay s approximation to the model evidence in neural networks and ignored all the extra terms in such approximation. It is currently unclear why there is a need to do this and focus only on the effective dimensionality. Almost all the experiments that the authors describe could have been done using a similar approximation to Mackay s model evidence. It is unclear why is there a need to focus just on a part of Mackay s approximation. The fact that the authors state that the effective dimensionality is only meaningful for models with low train loss seems indicative that David Mackay s approximation to the model evidence would be a better metric.  2   With the exception of the experiments for changes in the effective dimensionality as a function of the depth and width and the double descent curve, all the other experiments and results are expected and not new to anyone familiar with David Mackay s work.  3   The experiments on depth and width are for only one dataset and may not be representative in general. The authors should consider other additional datasets.   The authors should improve the paper, including a justification for using only the effective dimensionality and not David Mackay s approximation to the model evidence. They should also strengthen the experiments by comparing with David Mackay s approximation to the model evidence and should consider additional datasets as mentioned above.
The authors propose a new algorithm for clustering direct networks. The key idea behind the paper is to introduce a new flow imbalance measures and a new self supervised GNN model to solve the task.  Overall, the paper is interesting and it introduces some new ideas although it needs additional work before being published.   In particular,   the experiments could be improved by emphasizing more the evaluation on vol_sum/vol_max/etc metrics and by adding additional results on them   the clarity of the experimental results should also be improved(for example, metrics / claims around Figure 4 still a bit hard to parse)   finally, the paper would benefit by some theoretical results on the guarantees of the algorithm(most previous work in the area present interesting theoretical guarantees)
The paper compared between different CNNs for UAV trail guidance. The reviewers arrived at a consensus on rejection due to lack of new ideas, and the paper is not well polished. 
The paper shows that many of the current state of the art interpretability methods are inaccurate even for linear models. Then based on their analysis of linear models they propose a technique that is thus accurate for them and also empirically provides good performance for non linear models such as DNNs.
Unfortunately, the reviewers have unanimously voted to reject this paper.  There was some discussion of whether the paper was out of scope for ICLR;  I don t think that it is, necessarily, but I think that we can kind of screen off that topic because the reviewers had plenty of non scope related concerns that seem disqualifying to me, including both issues of novelty and issues related to the experimental validation. Therefore, I am also recommending rejection in this case.
This paper is proposed to improve base CNN models by dual multi scale attention module. To achieve a better feature representational ability, authors consider the multi scale mechanism from both channel dimension and spatial dimension. The proposed method has been verified on several benchmarks, including ImageNet and MS COCO. However, all reviewers consider rejecting this paper because this work lacks novelty, the results are suspicious, and the writing is poor. No responses are submitted by authors to address the reviewers  concerns.
This paper proposes an alternative for constructing convolution kernels: instead of uniform spatial resolution, it proposes a spatially varying resolution with higher precision at the center of the kernel. The resolution decreases logarithmically as a function of the distance to the center. All reviewers agree that the idea is interesting, but in its current form, the submission is not mature enough to be published.  In particular, reviewers raised some concerns about computational efficiency of the method. The authors explain that their method runs slower than conventional convolution because the implementation uses of the shell conventional convolution modules, and they speculate that the speed can be accelerated if the method is directly implemented with CUDA or by directly adapting the underlying code of convolutions in the integrated framework. While this is a reasonable argument, it is not actually verified. This it is not clear if there would be other road blockers to achieve the promised performance. It would be great if authors could present actual performance of the method using either of their suggested solutions (CUDA or modifying code of convolutions).  In addition, reviewers raised concerns about some aspects of the evaluation setup, where test data is used to report the best performance. Authors respond that baselines are trained in the same fashion, hence the comparison is still fair. However, the reviewers were not convinced by this response. In concordance, I also think the use of test data during training is misleading, even if all methods use the same strategy, because this may tell us more about which approach can better (over)fit to the data as opposed to how well the methods are able to generalize to unseen samples.  Another concern relates to the diminishing return in the performance as networks get larger. The authors respond that this might be because only the first layer uses the proposed log polar convolution, speculating the problem will go away if the proposed approach is used in all layers. However, this is not empirically verified again and remains unclear if this is indeed the reason.  I suggest authors resubmit after accommodating the provided feedback.
This paper tackles the problem of exploration in RL. In order to maximize coverage of the state space, the authors introduce an approach where the agent attempts to reach some self set goals. The empirically show that agents using this method uniformly visit all valid states under certain conditions. They also show that these agents are able to learn behaviours without providing a manually defined reward function.  The drawback of this work is the combined lack of theoretical justification and limited (marginal) algorithmic novelty given other existing goal directed techniques. Although they highlight the performance of the proposed approach, the current experiments do not convey a good enough understanding of why this approach works where other existing goal directed techniques do not, which would be expected from a purely empirical paper. This dampers the contribution, hence I recommend to reject this paper.
This work receives mostly positive rates. Most reviewers agree that the use of Bayesian attention to neural processes is novel, and its interpretation is interesting. Since the reviewer TBTA requests a substantial revision of the submission and fortunately authors’ feedback is thoroughly satisfactory, we highly recommend the authors to prepare for a significantly improved camera ready version that clarifies most of reviewers’ concerns.
This paper investigates several fine tuning methods for adapting the pretrained vison language model CLIP to different downstream tasks.  They found that LayerNorm (Ba et al., 2016) is a rather effective and competitive approach and investigated different ways of combining LayerNorm tuning with other adaptation methods. Reviewers generally agree that the technical novelty (combination of existing techniques) is limited and contribution is marginally significant. While some empirical results look interesting, the overall contribution is incremental. Overall, the paper has done a good job of thorough empirical evaluations, but the overall technical novelty and new empirical findings are not significant enough for publication at this conference.
The paper studies the problem of certified adversarial robustness when the classifier has a reject option (realized here as an additional class) where the certification is done by adapting IBP techniques to this particular problem setting.  Pro: As previous approaches to adversarial robustness with reject option have often be shown to be non robust as one could circumvent both detector and classifier simultaneously with an adaptive attack, the idea to do this instead with a certified approach is interesting and could potentially initiate more research in this direction.  Con:    The approach seems to some extent trade off robust error with normal error which seems in particular true for MNIST where a stronger loss in normal error is less acceptable.   Comments:   One reviewer mentioned that such a certified approach could be interesting for OOD detection. This has been done recently in: Julian Bitterwolf, Alexander Meinke, Matthias Hein Certifiably Adversarially Robust Detection of Out of Distribution Data, NeurIPS 2020 which should be cited in the present paper.   While the empirical PGD attacks done in this paper are strong, sometimes PGD fails due to gradient obfuscation, I would thus recommend to use additionally a black box attack or the recent AutoAttack. 
The paper empirically benchmarks multiple sample selection strategies for offline RL based on the prioritized experience replay framework, including TD errors, N step return, Generalized SIL, Pseudo count, Uncertainty, and Likelihood. These are all benchmarked for the base algorithm TD3BC. The experiments study the performance and bootstrapping errors. Among other things, it is shown that non uniform sampling strategies are also interesting in a batch RL setting. The authors show that non uniform sampling can be helpful in offline RL compared to uniform sampling but they fail to avoid bootstrap error. They also found that there is no one outperforming metric for prioritized sampling in offline RL settings.  The reviewers are in agreement that the question studied is a sensible and interesting one   Are PER strategies which are effective in online RL also useful for batch RL? The overall study conducted by the paper is clear and well presented.   While the study/benchmark and the results presented is clear, the reviewers point out the following shortcomings 1. The study is not comprehensive for this work to become a definitive exploration of this space of ideas. Only algorithm has been tested with these ideas.  2. The results of the study are unfortunately inconclusive   while there are benefits these are achieved via different strategies and as mentioned by the paper no clear conclusions can be drawn.   Since the paper is targeted purely as a benchmark, the originality aspect of the paper is naturally low. For benchmark papers in that case the impact factor squarely falls on comprehensiveness of the study and the emergence of some clear conclusions to further research in that area. The reviewers unanimously believe the paper falls short in both respects and therefore the decision.   Hopefully the authors can consider the feedback provided and incorporate it to improve the paper.
All reviewers agreed that the paper contains interesting experiments. However, as this paper is a systems paper without much algorithmic contributions, all reviewers felt that the paper felt short in terms of describing the results, has too many unsupported claims and it is unclear how the presented results transfer to slightly different domains. I therefore agree with the reviewers and recommend rejection of the paper.
The authors question the assumption that the epistemic uncertainty provided by Bayesian neural networks should be useful for out of distribution detection. They start their analysis in the infinite width limit so as to be able to understand how the induced kernels in a Gaussian process behave. The paper also discusses the potential tradeoffs between generalization and detection. Overall, the paper presents some facts that, while not surprising, (Reviewer fGuy), are helpful in questioning the default assumption. Overall, though, the combination of the lack of surprise with the multi part, somewhat loosely connected message reduces the quality of the submission.
The paper presents the Language complete Abstraction and Reasoning Corpus (LARC): a collection of natural language descriptions by a group of human participants who instruct each other on how to solve tasks in the Abstraction and Reasoning Corpus (ARC).  Overall, the reviewers found the LARC benchmarks to be well motivated. However, there were concerns about whether the value of the dataset to downstream tasks. Results from additional program synthesis systems (like Codex and GPT Neo) would also make the paper stronger. I agree with these objections and am recommending rejection this time around. However, I encourage the authors to continue pursuing this line of work and resubmit after incorporating the feedback from this round.
The paper proposes a technique for defending against adversarial examples that relies on averaging pixels that are close to each other both in position and value. This approach seems to be an interesting preprocessing technique in the robust training pipeline. However, the actual claims made are not well supported and, in fact, seem somewhat implausible. 
This paper proposes a variant of GAIL that can learn from both expert and non expert demonstrations. The paper is generally well written, and the general topic is of interest to the ICLR community. Further, the empirical comparisons provide some interesting insights. However, the reviewers are concerned that the conceptual contribution is quite small, and that the relatively small conceptual contribution also does not lead to large empirical gains. As such, the paper does not meet the bar for publication at ICLR.
This paper proposes a new algorithm for private ERM, when given access to public data, with a dimension independent risk guarantee if  (A) the public and private datasets are of the same distribution, (B) public dataset size exceeds the dimensionality (or, rather, the squared Gaussian width of an appropriate set), and (C) the public and private loss functions share a minimizer (and the gradients at the shared minimizer must satisfy some variance bounds). The algorithm uses the public data as the Bregman mirror map within private mirror descent (where Gaussian noise is added to the gradients), thus implicitly affecting the geometry, as opposed to explicitly learning the geometry as done in earlier works.  One reviewer was very positive, but two hovered around the borderline and expressed some reservations about the theory and experiments. Regarding the experiments, they did not compare to the ICML 21 paper by Asi et al   however the authors of that paper have (surprisingly) still not released their code, so I think this is forgivable. Since the paper was on the borderline, I read it myself, with a focus on the theoretical aspects. I find myself agreeing with the second reviewer that the assumptions are strong, and their justification is weak and unrealistic.   Regardless of whether the paper, is accepted or not, I strongly recommend the authors to add condition (C) to their abstract (just the part about the shared minimizer)   currently the abstract mentions two of the above but not the critical third one. I think (A) is already a strong assumption   their justification that some users opt in to reveal their data does not justify this, because the opt in will not be random (if the opt in depends on covariates like gender/age/..., the datasets will not be identically distributed). On top of that, (C) is also a strong assumption   indeed usually the loss functions would be different (for eg, the private one would be clipped, and clipping will rarely preserve the population minimizer, as well as regularized)   their justification that for a linear model with symmetric noise, clipping does not change the minimizer may be true (though not proved), but we would never expect the linear model to be true in practice even if we employ it as a working model. Last, assumption (B) restricts its use in many common high dimensional data problems. Overall, I am pressed into a corner to find situations in which all three assumptions would be true.  Nevertheless, supposing that these assumptions hold, the algorithm is indeed clean, and the empirics appear reasonable. Overall, the paper remains on the borderline. Whether accepted or rejected, I expect the authors to do a much better job of carefully justifying their assumptions, with realistic and not far fetched examples (as suggested by the second reviewer).
this submission introduces soft local reordering to the recently proposed SWAN layer [Wang et al., 2017] to make it suitable for machine translation. although only in small scale experiments, the results are convincing.
This paper proposes an approach to training language instruction following agents that aims to improve their compositional generalization., by means of an entropy regularization method to reduce redundant dependency on input.  All four expert reviewers agreed that the paper is not ready for publication in its current form. Of biggest concern is the fact that the reviewers could not interpret the exposition of the method, so were unable to be sure exactly how the method worked. This can be addressed in a future submission by clearer presentation.   Another concern was that the authors only consider a single benchmark, and fail to situate the work relative to other grounded language learning tasks and datasets. Thus, reviewers were concerned about the generality of the method, and suspected it may be too specific to the gSCAN setup.   That said, the reviewers were all impressed by the strong results on the gSCAN benchmark. It strikes me that there is some interesting insight here that can be derived from this impressive performance, that may also be applicable to other grounded language learning settings. However, to make the paper acceptable for publication the authors must do a much better job of communicating how their method works, what that specific insight is and how it is relevant beyond the gSCAN dataset (ideally via direct experimentation in other settings).
This work proves that the weights of feed forward ReLU networks are determined, up to a specified set of symmetries, by the functions they define. Reviewers found the paper easy to read and the proof technically sound. There was some debate over the motivation for the paper, Reviewer 1 argues that there is no practical significance for the result, a point that the authors do not deny. I appreciate the concerns raised by Reviewer 1, theorists in machine learning should think carefully about the motivation for their work. However, while there is no clear practical significance of this work, I believe there is value to accepting it. Because the considered question concerns a sufficiently fundamental property of neural networks, and the proof is both easy to read and provides insights into a well studied class of models, I believe many researchers will find value in reading this paper.
in this submission, the authors propose a sophisticated pretraining strategy for neural machine translation based on the paradigm of self supervised learning. despite some interesting and potentially significant improvement in various machine translation settings, the reviewers as well as i myself could not determine where specifically those improvements come from. is it their particular strategy of pretraining or is it just self supervised learning in general? in order for this question, which i believe is a key question to be answered, more thorough ablative experiments and/or comparison to other self supervised learning based pretraining algorithms, such as MASS & BART which were discussed as similar and motivational in the submission, must be done. when these are done, the submission will be much stronger and attract much more interest.
The paper proposes a data augmentation technique to ensemble classifiers. Reviewers pointed to a few concerns, including a lack of novelty, a lack of proper comparison with state of the art models or other data augmentation approaches. Overall, all reviewers recommended to reject the paper, and I concur with them.
The paper is well written overall. However, the algorithmic framework has limited novelty and the reviewers unanimously are unconvinced by experimental results showing marginal improvements on smallish UCI datasets.
This is an interesting work, and I urge the authors to keep pushing this direction of research. Unfortunately, I feel like the manuscript, in its current format is not ready for acceptance.  The research direction is definitely under explored, which makes the evaluation of the work a bit tricky. Still I think that some of the points raised by the reviewers hold, for e.g. the need of additional baselines (to provide a bit of context for what is going on)I understand that the authors view their work as an improvement of the previously proposed DT network, however that is a recent architecture, not sufficiently established not to require additional baseline for comparisons. This combined with the novely of the dataset makes it really hard to judge the work.   The write up might also require a bit of attention. In particular it seems a lot of important details of the work (or clarifications regarding the method) ended up in the appendix. A lot of the smaller things reviewer pointed out the authors rightfully so acknowledged in the rebuttal and propose to fix, however I feel this might end up requiring a bit of re organization of the manuscript rather that adding things at the end of the appendix. I also highlight (and agree) with the word "thinking" being overloaded in this scenario.  Ablation studies (some done as part of the rebuttal) might be also a key component to get this work over the finish line. E.g. the discussion around the progressive loss. I acknowledge that the authors did run some of those experiments, though I feel a more in depth look at the results and interpretation of them (e.g. not looking just at final performance, but at the behaviour of the system), and integrating them in the main manuscript could also provide considerable additional insight in the proposed architecture.   My main worry is that in its current format, the paper might not end up having the impact it deserves and any of the changes above will greatly improve the quality and the attention the work will get in the community.
The authors consider the problem of program induction from input output pairs.                                                      They propose an approach based on a combination of imitation learning from                                                          an auto curriculum for policy and value functions and alpha go style tree search.                                                    It is a applied to inducing assembly programs and compared to ablation                                                              baselines.                                                                                                                                                                                                                                                              This paper is below acceptance threshold, based on the reviews and my own                                                           reading.                                                                                                                            The main points of concern are a lack of novelty (the proposed approach is                                                          similar to previously published approaches in program synthesis), missing                                                           references to prior work and a lack of baselines for the experiments.
This paper proposes and investigates an approach for audiovisual synthesis based on the so called exemplar autoencoders.  The proposed approach is shown to be able to convert an audio input to audiovisual outputs using only very small amount of training data.  All reviewers consider the paper interesting with a lot of potentials in a variety of applications and appreciate the novelty of the work in this domain.  But there are also concerns on the technical presentation and the quality of the samples in the demo.  The authors addressed most of the concerns in the rebuttal but agreed that the quality of the results still had room for further improvements.  Overall, the work presented is interesting. The paper can be accepted. 
This paper provides a natural combination of conditional neural processes with LieConv models. It is a good step forward for stochastic processes with equivariances. While there is still room to improve the experiments, the authors provided a good response to reviewers, and the paper is a nice contribution.
We thank the authors for their response. The reviewers agree that this paper provides contributions in automating privacy analyses under the Gaussian differential privacy (GDP) framework. The reviewers also pointed out several drawbacks of the paper. Most importantly, the reviewers do not find the presented applications to be convincing. In particular, the presented result can be much strengthened if the proposed method can lead to improved privacy analysis for more sophisticated algorithms such as DP SGD across a wide regime of epsilon and delta. (In general, the privacy guarantee is very weak with delta bigger than 1/n.) Overall, the paper does not seem to provide enough evidence to showcase the usefulness of their proposed method.
This paper proposes a more generalized form of certified robustness and attempts to provide new results on applying randomized smoothing to semantic transformations such as different types of blurs or distortions. The main idea is to use an image to image neural network to approximate semantic transformations, and then certify robustness based on bounds on that neural network. The authors provide empirical results on standard benchmark datasets like MNIST and CIFAR showing that their method can achieve improved results on some transformations compared to prior work.  The review committee appreciates the authors taking the time to attempt to respond to the concerns of all reviewers, and for updating and improving their work during the rebuttal process. The committee is glad to see that they do provide empirical evidence of improvement to common corruption robustness, compared to AugMix (one of the state of the art approaches for standard common corruption robustness) and TSS.   However, the reviewers still have concerns about the novelty of the paper. The main novelty is not improvement for resolvable transformations (prior works that the authors cite perform about the same or better), but rather, is the ability to handle non resolvable transformations. The reviewers agree that robustness to non resolvable transformations is important; however, the reviewers think certified robustness to non resolvable transformations is not meaningful, because they are only being certified with respect to a neural network that is trained to approximate those non resolvable transformations. Without MTurk studies to confirm how good the neural network s non resolvable transforms are, the reviewers do not find certified robustness here meaningful.
A new regularized graph CNN approach is proposed for semi supervised learning on graphs.  The conventional Graph CNN is concatenated with a Transposed Network, which is used to supplement the supervised loss w.r.t. the labeled part of the graph with an unsupervised loss that serves as a regularizer measuring reconstruction errors of features. While this extension performs well and was found to be interesting in general by the reviewers,  the novelty of the approach (adding a reconstruction loss),  the completeness of the experimental evaluation, and the presentation quality have also been questioned consistently. The paper has improved during the course of the review, but overall the AC evaluates that paper is not upto ICLR 2019 standards in its current form. 
pros:   the paper is well written and precise   the proposed method is novel   valuable for real world problems  cons:   Reviewer 2 expresses some concern about the organization of the paper and over generality in the exposition   There could be more discussion of scalability
The authors explore different ways to generate questions about the current state of a “Battleship” game. Overall the reviewers feel that the problem setting is interesting, and the program generation part is also interesting. However, the proposed approach is evaluated in tangential tasks rather than learning to generate question to achieve the goal. Improving this part is essential to improve the quality of the work.   
The authors present a way for generating adversarial examples using discrete perturbations, i.e., perturbations that, unlike pixel ones, carry some semantics. Thus, in order to do so, they assume the existence of an inverse graphics framework. Results are conducted in the VKITTI dataset. Overall, the main serious concern expressed by the reviewers has to do with the general applicability of this method, since it requires an inverse graphics framework, which all in all is not a trivial task, so it is not clear how such a method would scale to more “real” datasets. A secondary concern has to do with the fact that the proposed method seems to be mostly a way to perform semantic data augmentation rather than a way to avoid malicious attacks. In the latter case, we would want to know something about the generality of this method (e.g., what happens a model is trained for this attacks but then a more pixel based attack is applied). As such, I do not believe that this submission is ready for publication at ICLR. However, the technique is an interesting idea it would be interesting if a later submission would provide empirical evidence about/investigate the generality of this idea. 
This paper proposed a cluster based task aware meta learning (CTML) approach with task representation learned from its own learning path. Based on the prior work of feature based task characterization, it integrates the rehearsed task gradient descent trajectory into task representation, and further improves computational efficiency by learning a different network to estimate the rehearsed task trajectory characterization from the feature representation. Experiments were conducted on few shot image classification (meta dataset and miniimagenet) and cold start recommendation tasks.   Reviewers had raised various concerns about the work including technical novelty, the shortcut tunnel assumption, empirical comparison, scalability issue, more ablations for in depth analysis, etc. The reviewers and AC appreciate authors for putting good efforts in the rebuttal by replying the review questions carefully and making changes to improve their experiments and paper.   Overall, this paper is a borderline case, where reviewers agree some clear merits (well written, easy to follow, good execution of an interesting idea with code provided, etc). Despite the improvements during rebuttal, some major concerns on the weaknesses still remain (e.g., technical novelty, more convincing justification on the assumption, significance of empirical gains). Therefore, I cannot recommend it for acceptance at its current form, but I hope to see it accepted in the near future after these issues are fully addressed.
This paper provides a simple approach to incorporate temporal information in RL algorithms. AC agrees with authors that simplicity is a virtue. As reviewers point out that experimentally the approach is not conclusively better (given that environments might be hand chosen). Even R3 believes some reported improvements is within variance. Given the discussions, AC agrees that results do not seem convincing enough.
This paper analyzes the gradient behavior of RNNs in terms of the Lyaponuv exponents of its trajectory/orbit, showing that RNNs with cyclic or stable equilibrium dynamics have bounded gradients, but if the dynamics are chaotic the gradients will explode. From these insights, the authors propose an algorithmic remedy for this pathology, which is essentially a teacher forcing method that periodically projects the observation onto the hidden state during training. A thorough empirical investigation is performed showing the utility of the proposed approach for modeling chaotic data.  The reviewers had split opinions on this paper. Some reviewers found value in the theoretical contributions and the connection between Lyapunov exponents and behaviors of the dynamics of recurrent neural networks, while others thought the theoretical framework may have limited practical utility. Several reviewers found the initial experiments to be lacking, though many of their concerns were alleviated after the substantial additions the authors provided during the discussion phase.  I believe the observation that exploding gradients are unavoidable when modeling chaotic data is important and would be of significant interest to the broader ICLR community. However, the practical implications of this observation have not been thoroughly described or investigated, and without this perspective, the theoretical results by themselves are much less impactful. In practice, it is usually the case that the ground truth function is not learned exactly, the time horizon is finite, the gradients are noisy, the data generating process is opaque, etc. Do these caveats have any bearing on the conclusions? The experiments address some of these questions, but only indirectly, and a more explicit discussion of the practical implications would broaden the impact of the paper.  Along the same lines, the practical utility of the theoretical framework could be further supported if there were some analysis of more varied or additional RNN use cases. As one reviewer mentioned, I think the ICLR community in particular would appreciate any theoretical or algorithmic insights that might yield improvements on a standard baseline task like seqMNIST, which has served as a point of comparison for many alternative methods and which would facilitate comparisons to prior work.  Overall, this paper does make some nice and potentially important theoretical insights about training RNNs on chaotic data, and it does include an extensive battery of empirical evaluations, however the practical implications remain largely unconvincing, and I believe the paper falls just short of the bar for acceptance.
RDA improves on RWA, but even so, the model is inferior to the other standard RNN models. As a result R1 and R3 question the motivation for the use of this model   something the authors should motivate.
The paper studies the role of depth on incremental learning, defined as a favorable learning regime in which one searches through the hypothesis space in increasing order of complexity. Specifically, it establishes a dynamical depth separation result, whereby shallow models require exponetially smaller initializations than deep ones in order to operate in the incremental learning regime.   Despite some concerns shared amongst reviewers about the significance of these results to explain realistic deep models (that exhibit nonlinear behavior as well as interactions between neurons) and some remarks about the precision of some claims, the overall consensus   also shared by the AC   is that this paper puts forward an interesting phenomenon that will likely spark future research in this important direction. The AC thus recommends acceptance. 
The paper provides an interesting analysis of aligned GAN models. The paper shows that when a model is obtained (fine tuned) from another, then the corresponding hidden semantic spaces are aligned. The paper uses this property to show that without any additional architecture or training, the models can perform diverse tasks such as image translation and morphing. The paper also demonstrates that zero shot tasks can be performed by learning in the parent domain and transferring to the child domain.  All reviewers agree that the paper presents an interesting analysis and findings and will make a valuable contribution to the field. The reviewers raised some particular concerns, which were addressed by the authors in their response.
One way of avoiding catastrophic forgetting in continual learning is through keeping a memory buffer for experience replay.  This paper addresses the problem of online selection of representative samples for populating such memory buffer for experience replay.  The paper proposes novel information theoretic criteria that selects samples that captures surprise (samples that are most informative) and learnability (to avoid outliers).  They utilize a Bayesian formulation to quantify informativeness. They provide two algorithms: a greedy approach, and an approach that takes timing (when to) update memory into account based on reservoir sampling to mitigate possible issues with class imbalance.  Pros:  The paper is well written and organized.  It was easy to follow. The formulation is novel and technically sound. The idea of taking learnability into account is novel and interesting.  It provides a nice way of avoiding outliers and balancing surprising information. The authors presented the motivation for each part of the framework well.    Cons: To understand the contribution of each component of the formulation and competing criteria, an ablation study is needed. Reviewers had several detailed suggestions and questions, including sensitivity to hyperparameters, additional citations, additional data sets beyond MNIST and CIFAR10, etc.    In the rebuttal, the authors have addressed several of these concerns.  Please make sure to include and incorporate reviewer suggestions in the final revised version.
There seems to be some disagreement between Reviewers, with some borderline scores and some very good scores. After careful consideration of both reviews and answers, and after reading the updated version of the paper with some detail, I believe the approach is valuable. The use of scores for detecting out of distribution data is very novel and presents a number of opportunities for further research, both theoretically and empirically. Overall, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Straightforward method. "Trivial application".   Novel application to medical images.   Robustness of default hyper parameters.   Future open sourcing of the code and model checkpoints.   Topic highly relevant to the ICLR community.   Well written paper + relatively good visualizations.  Cons:   Lack of comparison with other existing approaches.   Intuition/explanation/motivation on why the method works could be improved.   Effect of hyper parameters could be further discussed/analyzed.   Concerns about applicability of the approach. 
All three reviewers recommend acceptance. Good work, accept
The paper addresses the problem semantic segmentation using a sequential patch based model. I agree with the reviewers that the contributions of the paper are not enough for a machine learning venue: (1) there has been prior work on using sequence models for segmentation and (2) the complexity of the proposed approach is not fully justified. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account and improve the paper.
I thank the authors and reviewers for their discussions about this paper. The proposed AT GAN is a GAN based method to generate adversarial examples. Similar methods (e.g. Song et al) have been proposed to use GANs to generate adv. examples more efficiently. Authors show their method has some numerical benefits. However, more experiments are needed to further justify it. Also, creating "unrestrictive" adv. examples can cause a risk of generating samples where the true label is flipped. Authors need to clarify it. Given all, I think the paper needs a bit of more work to be accepted. I recommend authors to address the aforementioned concerns in the updated draft.      AC
The paper investigates what we can learn from _suboptimal_ demonstrations for imitation learning. It suggests that we can learn about the structure of the environment by finding a factored dynamics model including a latent action space. It demonstrates both theoretically and empirically that this information can reduce sample requirements for downstream IL.  The reviewers praised the simplicity of the method (including its minimal assumptions), the theoretical analysis, and the breadth of the experimental validation. The authors were helpful during the discussion period, and addressed any questions or concerns the reviewers raised.  Overall, this is an interesting idea and a well executed paper.
This paper presents a knowledge distillation method for face recognition, by inheriting the teacher’s classifier as the student’s classifier and optimizing the student model with advanced loss functions. It received comments from three reviewers: 1 rated “Ok but not good enough   rejection”, 1 rated “Marginally below” and 1 rated “Marginally above”. The reviewers appreciate the simple yet clear methodology illustration and the well written paper. However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillation methods and their special case in face recognition. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the rating were not changed. The ACs concur these major concerns and more comprehensive comparisons with the state of the art KD methods are necessary to better illustrate the contribution of this work. Therefore, this paper can not be accepted at its current state. 
It can be prohibitively expensive to train a reinforcement learner from scratch &mdash; particularly in cases where experience is expensive to obtain, such as with a physical robot. So, we might hope to speed up RL in a couple of ways: first, by pre training a representation that makes subsequent RL need less data; and second by running our RL on a cheaper proxy environment such as a simulator. For pre training, we hope to be able to take advantage of available pre collected data, and we hope to be able to use supervised learning or reconstruction tasks since they can be cheaper than RL. For either pre training or a proxy environment, we have to deal with distribution shifts: the properties of the environment may change between pre training and RL, and between RL and testing the learned policy.  The paper presents an empirical study of how different pre trained representations and different distribution shifts affect RL performance. It evaluates a number of representations trained by different VAEs (differing in aspects such as loss and hyperparameter settings) under various scenarios of distribution shift. It also asks whether we can predict the performance of the learned policies from properties of the representations, before going to the expense of training and evaluating our reinforcement learner.  The paper concludes that it is possible to significantly reduce RL data requirements using pre trained representations, even in the presence of significant distribution shifts &mdash; including demonstrating zero shot sim2real transfer. And, the paper concludes that inexpensive measurements of OOD performance on supervised tasks can at least partially predict success in generalization.  The reviewers praised the extensive experimental evaluation, including a large number of experiments on a physical robot, as well as the investigation of less expensive ways to predict generalization.  Some reviewers were concerned that the choice of environments was limiting &mdash; e.g., that the distributional distance between in distribution and out of distribution tests was limited, or that the results might not generalize to other related robotic environments. However, in the end there was support for the conclusion that the experiments cover a sufficiently general and interesting question.
This paper proposes a new method of learning ensembles of neural networks based on the Information Bottleneck theory, which increases the diversity in an ensemble by minimizing the mutual information between latent features of the different ensemble models. It shows promising results on classification, calibration and uncertainty estimation. The paper is well written and the comments were properly addressed.
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciate the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
This paper investigates the linear mode connectivity of the loss landscape of neural networks, i.e. whether a convex combination of two parameters of local optima on the SGD paths has low loss values (i.e. low barrier) up to some permutations. To probe this question, this paper empirically studies the loss gap, named as “barrier”, between two local minima and their convex combinations or linear interpolation. Before permutations, such barriers are typically non zero; yet, after taking into account of permutation invariance of models, such barriers could be reduced along to zero with the width increasing, a main conjecture formulated in the paper. To support this conjecture, the authors proposed a simulated annealing algorithm to search for such permutations, demonstrating that the barrier reduces after such permutations.  The reviewers unanimously accept the paper, if the authors make the proposed improvement in the final version. In particular, a reader points out a paper by Singh and Jaggi,  Model Fusion via Optimal Transport , NeurIPS 2020, that supports the same conjecture with a constructive algorithm to find optimal permutations or matching using optimal transport. This should be included in the final version as the authors replied.
The paper considers an interesting algorithm on zeorth order optimization and contains strong theory. All the reviewers agree to accept.
The paper proposed an optimal margin distribution loss and applied PAC Bayesian bounds that are from Sanov large deviation inequalities to give generalization error bounds for such a loss. Some interesting empirical results are shown to support the proposed method.   The majority of reviewers think the paper’s empirical results are encouraging, although still in premature stage. The theoretical analysis is a kind of being standard. After reading the authors’ response and revision, the reviewers do not change much of their opinions and think the paper better undergoes systematic further study on their proposal for big improvement.    Based on current ratings, the paper is therefore proposed to borderline lean rejection.  
The paper addresses the complexity issue of Determinantal Point Processes via generative deep models.  The reviewers and AC note the critical limitation of applicability of this paper to variable ground set sizes, whether authors  rebuttal is not convincing enough.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
The authors propose generative latent flow which uses autoencoder to learn latent representations and normalizing flows to map that distribution. The reviewers feel that there is limited novelty since it is a straightforward combination of existing ideas. 
The paper examines whether it is possible to train agents to follow synthetic instructions that perceives and modifies a 3D scene based on a first person viewpoint, and have the trained agents follow natural language instructions provided by humans.  The paper received two weak rejects and one weak accept.  The main concerns voiced by the reviewers are: 1. Lack of variety in natural language One of the key claims of the paper is that previous work on instruction following can only handle instructions generated from templates and cannot handle ambiguous expressions used by real people, and that the contribution of this work is that it can handle such expresssions.  However, as pointed out by R1, the language considered in this work is very simplistic in form (close to being template based) with the main variation coming from synonyms.  Even the free form natural instructions that are collected, are done so with very specific instructions that restrict diversity of language (e.g don t use colors or other properties of the object). R1 also point out that there are prior work that handles much more diverse language.  2. Limited technical novelty and questions about how much the proposed CMSA method actually contribute  3. Overclaims and lack of precision when using terminology There is concern that the task that is addressed is not actually that complex.  The environments are simple (with just 2 objects) and not that realistic.  Tackling 2 tasks is barely "multi task", and commonly, "manipulation" refers to low level grasping/picking up of objects which is not how it is used here.  While the paper has many strong elements and is mostly well written, considerable improvements still need to be made for the paper to have claims it can support.  It is currently below the bar for acceptance. The authors are encouraged to improve their paper and resubmit to an appropriate venue. 
This paper presents an augmentation based training of autoencoders with stochastic latent space. The proposed method is examined on the representation learning task on several image datasets. While the reviewers found the submission interesting, simple, and easy to implement, they also raised serious concerns around the novelty of the proposed method and the impact of removing the KL term (which removes the generative interpretability of the model). Unfortunately, the experiments do not provide a convincing utility of the model compared to more popular representation learning methods (i.e., contrastive and non contrastive methods). Given these concerns, the paper is not ready for presentation at ICLR.
This paper shows convergence of stochastic gradient descent  for the problem of learning weight matrices for a linear dynamical system  with non linear activation.  Reviewers agree that the problem considered is both interesting and challenging. However the paper makes many simplifying assumptions   1) both input and hidden state are observed, a very non standard assumption, 2) analysis requires increasing activation functions, cannot handle ReLU functions. I agree with R2 and think these assumptions make the results significantly weaker. R1 and R3 are more optimistic, but authors response does not give an insight into how one might extend this analysis to the setting where hidden state is not observed. Relaxing these assumptions will make the paper more interesting. 
This paper proposes several innovations for machine translation. The reviewers had several questions about the claims that were made and the authors addressed these and also acknowledged that some of their formulations (e.g.  better ) would need to be qualified. Overall, there are several interesting ideas that have been put together in a sensible way, but the story is not super consistent.  The detailed exchanges between the reviewers are authors are commendable!
This is an ambitious paper tackling the important and timely problem of controlling non annotated attributes in generated speech.   The reviewers had mixed opinions about the results. R1 asks for more convincing exposition of results but, nevertheless, acknowledging that it is difficult to evaluate TTS systems systematically. Besides, R2 and R3 find the results good.   Judging from the reviews and previous work, this paper does not seem to be very novel, although it certainly has intriguing new elements. Furthermore, it constitutes a mature piece of work.   
This work performs a mean field analysis of a certain class of fully connected networks with and without layer normalization. Theory is provided which successfully predicts when some networks will exhibit either exploding gradients, or "representation shrinkage" which is similar to the extreme ordered phase discussed in prior works on signal propagation. The primary concerns raised by reviewers included, large overlap with prior works on signal propagation, a bug in the proof of the main theorem, lack of clarity, and many assumptions made in the theory which significantly limit the space of architectures for which the theory can be applied. Some of these concerns were addressed in the rebuttal period, notably major flaw in the main theorem was resolved and some concerns on clarity were addressed. However, with the remaining issues (notably overlap with prior work, and overly restrictive assumptions made) a majority of reviewers did not recommend acceptance in the end. The AC agrees with this final decision and recommends the authors look to further expand upon the contributions relative to prior work.
Learning identity preserving transformations from unlabeled data is definitely an important and useful direction. However the paper does not have convincing experiments to establish the effectiveness of the proposed method on real datasets which is a crucial limitation in my view, given that the paper is largely based on an earlier published work by Culpepper and Olshausen (2009). 
Confidence boosting via aggregating multiple run of algorithms has been used before. The main result of the paper relies on a generic confidence boosting trick. The authors for instance cite Shalev Schwartz et al 2010 theorem 26 in remark 4 of their paper and correctly point out that for deterministic algorithms like ERM one can use that for confidence boosting. While that theorem there is proved for excess risk and for deterministic algorithms, the main idea there to me seems like what is used in the authors paper as well.   The basic idea:  Property A holds in expectation, Hence use Markov inequality to get a low grade probability version of it in each of the K pieces Now probability that at least one of the pieces is good is high since each piece is independent of the other Finally aggregate with simple concentration with union bound.  In Shalev Schwartz et al 2010 this is done with property being excess risk, here it is done with generalization error.  (Oh and I should add, the fact that the algorithm is randomized does not affect this line of reasoning as long as we use fresh randomness for each of the K blocks).  Now the missing piece covered is that on average stability implies generalization in expectation. But isn’t this already known to be true in the stability literature?   To me it seems like the main technical contribution of the paper is not as novel. Further, as one of the reviewers points out, the main goal should be to prove high probability guarantee for the algorithm popularly used like SGD not the confidence boosted version of it.  None the less, it seems like the application of the result to SGD seems interesting and somewhat new.   I am reluctant to propose an accept here.
This paper takes results related to the convergence and implicit regularization of stochastic mirror descent, as previously applied within overparameterized linear models, and extends them to the nonlinear case.  Among other things, conditions are derived for guaranteeing convergence to a global minimizer that is (nearly) closest to the initialization with respect to a divergence that depends upon the mirror potential.  Overall the paper is well written and likely at least somewhat accessible even for non experts in this field.  That being said, two reviewers voted to reject while one chose accept; however, during the rebuttal period the accept reviewer expressed a somewhat borderline sentiment.  As for the reviewers that voted to reject, a common criticism was the perceived similarity with reference (Azizan and Hassibi, 2019), as well as unsettled concerns about the reasonableness of the assumptions involved (e.g., Assumption 1).  With respect to the former, among other similarities the proof technique from both papers relies heavily on Lemma 6.  It was then felt that this undercut the novelty somewhat.     Beyond this though, even the accept reviewer raised an unsettled issue regarding the ease of finding an initialization point close to the manifold that nonetheless satisfies the conditions of Assumption 1.  In other words, as networks become more complex such that points are closer to the manifold of optimal solutions, further non convexity could be introduced such that the non negativity of the stated divergence becomes more difficult to achieve.  While the author response to this point is reasonable, it feels a bit like thoughtful speculation forged in the crunch time of a short rebuttal period, and possibly subject to change upon further reflection.  In this regard a less time constrained revision could be beneficial (including updates to address the other points mentioned above), and I am confident that this work can be positively received at another venue in the near future.
The paper tackles the interesting area of cooperative multi agent learning and presents a promising method to make MAL robust to mistakes of teammates, while learning correlated equilibria. Reviewers find the presented setting and theoretical contributions limited and the experiments not extensive enough; also some technical details about the architecture are lacking, and the notation and writing can be substantially improved upon. As such the paper does not seem ready for publication at this stage.
All four knowledgeable referees have indicated reject mainly because the novelty is limited   they thought (and I also agreed) that  it would be difficult to argue the novelty of the proposed framework simply by considering the more recent compressed network training technique, as the reviewer mentioned through rebuttal. In addition, there were concerns about various terms and basics specialized for hardware that are not kindly explained for more diverse audiences in the machine learning field. It improved a little through revision, but I think it needs a more kind explanation. It seems that more thorough experimental verifications  are needed.
The paper presents a significant body of seemingly solid work, but its contribution nevertheless feels limited: It evaluates a single MLM on a single dataset, and results are largely unsurprising. Note: The authors added experiments on other LMs in the rebuttal. The idea of using perturbations is related in spirit to many interpretability methods and adversarial techniques, and using higher order correlations for interpreting neural networks is, for example, at the heart of relational similarity analysis. A few suggestions to make the work more relevant to a wider audience: Compare with several probing techniques   e.g., in a tree decoding set up   or contrast results across domains (using OntoNotes), or across languages (using OntoNotes and other PTB style treebanks). Also: While results were added for multiple LMs, differences were not analysed in detail. 
All three reviewers recommend acceptance. The authors did a good job at the rebuttal which swayed the first reviewer to increase the final rating. This is a clear accept.
This paper has been assessed by three reviewers who scored it as 3/3/3, and they did not increase their scores after the rebuttal. The main criticism lies in novelty of the paper, lack of justification for MM^T formulation, speed compared to gradient descent (i.e. theoretical analysis plus timing). Other concerns point to overlaps with Baydin et al. 2015 and the question about the validity of Theorem 1. On balance, this paper requires further work and it cannot be accepted to ICLR2020.
Note: This meta review is written by the SAC, but it s synced with the AC.  Summary (adopted from Reviewer wCmR): This paper presents a modification of monotone deep equilibrium layers that allows to compute the bounds on the output via the IBP algorithm. This also allows to train a certifiably robust DEQ model with a competitive performance.  Initial reviews were mixed, but post rebuttal the opinions generally improved. Reviewer wCmR intend to increase their score slightly (6 to 7) and Reviewer KViU also mentioned that their opinion improved. Reviewer 7ZJs maintained their score and, during discussion phase, made many arguments against acceptance. One of those was about Tarski s theorem which was deemed not so important by the AC and also KViU. Another concern was about experimental results to which KViU agreed, and this remains the main concern for now.  Most reviewers agree that the work is interesting and is a good step, but then utility of the new modification and significance of the results remains a question. It is likely that the work may be useful in the future, and as there is an overall increase in the opinion, I believe that it is okay to accept the paper.  I encourage the authors to take the comments of the reviewers into account, and clearly mention the issues raised in the paper.  SAC
This paper introduces a soft gradient based subword tokenization module (GBST) that learns latent subword representations from characters. GBST enumerates candidate subword blocks and learns to score them in a position wise fashion using a block scoring network. The resulting model was tested on GLUE, and several cross lingual tasks. The performance is competitive with ByteT5 and often similar to subword models while being more efficient in FLOPs and RAM.  Reviewers are mixed on this. The negative reviewer points to how this not being a real tokenizer and does not produce a tokenization, that experiments that use the base model do not address bigger scales, and that there is a lack of code which is important for this kind of work, and the resulting accuracy gains are not significant and the method being not interpretable. The positive reviewers like the extensive experiments, the efficiency improvements and flexibility / simplicity of the GBST module. The authors seemed to have addressed most of the reviewer issues by providing larger scale experiments and code. I believe the results are fairly strong, since one would not expect a big performance difference in a learned tokenization method, but rather efficiency or flexibility gains. The paper is generally well written though details about the convolution should be included in the text (and not just the code).  Recommending accept.
The paper proposes an RL technique for dealing with the problem of network (graph) rewiring for robustness against attacks. Graph rewiring has been studied in a variety of fields, including graph theory (graph abstraction), graph ML (adversarial robustness, performance of GNNs), and combinatorial optimization. Reviewers had concerns with novelty, the correctness of some of the statements, and empirical evalution (in particular, baselines and scalability). While the rebuttal addressed some of the concerns, the overall feel about the paper is lukewarm and the AC believes the paper is below the bar.
This paper proposes a novel method for improving domain generalization based on the idea of learning different subspaces for each domain. Authors provide theoretical analysis related to their proposal and further evaluate their proposed method on a subset of DomainBed benchmark.     **Strong Points:**     The paper is well written.     The proposed method is novel.     Authors provide theoretical analysis in support of their proposal.     The theoretical results seem to be correct.     Empirical evaluation shows that the proposed method improves over baselines on a subset of datasets included in the DomainBed benchmark.   **Weak Points:**     The complexity of the theoretical results makes it very difficult for the reader to get any intuition about the underlying mechanisms at play.     The theoretical analysis is disconnected from the proposed algorithm. It is hard to see how one could end up proposing such an algorithm following the theoretical results. I suggest that authors would consider reorganizing the paper with less emphasis on the theoretical part, perhaps simplifying the theoretical results and pushing the rest to appendix.     The empirical evaluation can be improved significantly. Domain generalization is a very well established area at this point. WILDS is a carefully designed and well known benchmark and showing improvement in that benchmark would be very convincing but unfortunately authors do not discuss or even refer to it. They instead report their results on a subset of datasets used in DomainBed benchmark. The DomainBed benchmark is less challenging than WILDS but even following DomainBed closely and reporting the 3 evaluation metrics on all 7 datasets would have been satisfying. However, authors only report the results on 3 datasets. Reporting the results on a diverse group of datasets is particularly important in the case of Domain Generalization because we know that many methods are able to show improvements on a few datasets but it is challenging to beat the baselines on a significant majority of datasets.   **Final Decision Rationale**:    This is a borderline paper. On one hand, the proposed method is interesting and novel. On the other hand, the theoretical contributions are very limited and the empirical evaluation is not strong enough for acceptance. Given that all weak points mentioned above can be addressed, I recommend rejection and I sincerely hope that authors would strengthen their paper by addressing them before resubmitting their work.
This paper proposes better methods to handle numerals within word embeddings.  Overall, my impression is that this paper is solid, but not super exciting. The scope is a little bit limited (to only numbers), and it is not by any means the first paper to handle understanding numbers within word embeddings. A more thorough theoretical and empirical comparison to other methods, e.g. Spithourakis & Riedel (2018) and Chen et al. (2019), could bring the paper a long way.  I think this paper is somewhat borderline, but am recommending not to accept because I feel that the paper could be greatly improved by making the above mentioned comparisons more complete, and thus this could find a better place as a better paper in a new venue.
The paper proposes a novel meta algorithm, called Self Imitation Policy Learning through Iterative Distillation (SPLID) , which relies on the concept of  distilled policy to iteratively level up the quality of the target data and agent mimics from the relabeled target data. Several aspects of the paper can be improved. The reviewers are concerned in particular about the experimental section which might not exhaust the core set of tasks, where the method should be compared with baselines. Furthermore the presentation can be significantly improved (lots of grammatical errors). Another major point is the novelty of the presented algorithm.  In the rebuttal the authors tried to address some of the remarks, in particular by adding additional experiments to the empirical section of the paper. Those experiments still do not convince some of the reviewers. Furthermore, one of the biggest concerns is still a limited novelty of the approach. The presentation of the paper still needs to be substantially improved. Thus the paper still requires nontrivial work.
The authors present a method for training a policy for a self driving car. The inputs to the policy are map based perceptual features and the outputs are waypoints on a trajectory, and the method is an augmented imitation learning framework that uses perturbations and additional losses to make the policy more robust and effective in rare events. The paper is clear and well written and the authors do demonstrate that it can be used to control a real vehicle. However, the reviewers all had concerns about the oracle feature representation which is the input and also concerns about the lack of baselines such as optimization based methods. They also felt that the approach was limited to self driving cars and thus would have limited interest for the community.
This paper proposes a method for parameterizing orthogonal convolutional layers that derives from paraunitary systems in the spectral domain and performs a comparison with other state of the art orthogonalization methods. The paper argues that the approach is more computationally efficient than most previous methods and that the exact orthogonality is important to ensure robustness in some applications.  The reviewers had diverging opinions about the paper, with most reviewers appreciating the theoretical grounding and empirical analysis, but with some reviewers finding weakness in the clarity, reproducibility, and discussion of prior work. The revisions addressed many, but not all, of the reviewers  criticisms.  One point that was highlighted in the discussion is that the method is restricted to separable convolutions. The authors acknowledged this limitation, justifying the expressivity of the method with a comparison to CayleyConv (Trockman & Kolter) and a suggestion that more expressive parameterizations are not necessarily available in 2D. I am not sure this is entirely accurate. In the discussion of related work, the paper briefly mentions dynamical isometry and the prior work of Xiao et al. 2018, who develop a method for initializing orthogonal convolutional layers. What the current paper fails to recognize is that Algorithm 1 of Xiao et al. 2018 actually provides a method for parameterizing non separable 2D convolutions: simply represent every orthogonal matrix in that algorithm in a standard way, e.g. via the exponential map. While I think there is certainly value in the connection to paraunitarity systems, it seems to me that the above approach would yield a simpler and more expressive representation, and is at minimum worth discussing.  Overall, between the mixed reviewer opinions and their lingering concerns and the existence of relevant prior art that was not discussed in sufficient depth, I believe this paper is not quite suitable for publication at this time.
*Summary:* Study expressive power of narrow networks.   *Strengths:*   Study the narrow setting, which is not as well studied as the wide setting.    Some reviewers found the paper well written.   *Weaknesses:*   Restricted class of targets and activations.    Similar results have appeared in previous works.   *Discussion:*   99iL asked about the possibility to remove certain assumptions and the extension to other activations. Authors answer negatively to both. 99iL acknowledges the response and concludes the so called maximum principle is the most interesting result, but also points out that similar results appear in previous work and that it would have been good to see some extensions. qHTG indicates that the paper is well written and has interesting contributions but that some of the theoretical results only apply in settings that are more restrictive than in other recent related works. Authors agree that generalizations deserve to be investigated in the context of the presented results, but point out that their principle does not apply in that case, and hence that such generalizations are out of scope. Although qHTG identifies several good aspects in this work, they maintain the overall assessment of just marginally above the threshold. PCRn finds the work very interesting but is concerned about the novelty and points out that although the work is technical, the main message is not very strong and that the extraction of insights to solve tasks is not as clear. PCRn concludes that the paper presents various relatively weak results but not a sufficiently significant message. Authors remark that some of their results constitute a mathematical tool for future works.   *Conclusion:*   One reviewer rated this work marginally below the acceptance threshold and three other marginally above. Considering the reviews and the discussion, I conclude that this paper obtains a few interesting results but leaves much for future work. Further development of the current results would make the article significantly stronger. In view of the very high quality of other submissions to the conference, I find that this article tightly misses the bar for acceptance. Therefore I recommend to reject this article. I encourage the authors to revise and resubmit.
This work investigates the use of graph NNs for solving 2QBF . The authors provide empirical evidence that for this type of satisfiability decision problem, GNNs are not able to provide solutions and claim this is due to the message passing mechanism that cannot afford for complex reasoning. Finally, the authors propose a number of heuristics that extend GNNs and show that these improve their performance.  2 QBF problem is used as a playground since, as the authors also point, their complexity is in between  that of predicate and propositional logic. This on its own is not bad,  as it can be used as a minimal environment for the type of investigation the authors are interested. That being said, I find a number a number of flaws in the current form of the paper (some of them pointed by R3 as well), with the main issue being that of lack experimental rigor. Given the restricted set of problems the authors consider, I think the experiments on identifying pathologies of GNNs on this setup could have gone more in depth. Let me be specific.   1) The bad performance is attributed to message passing. However, this feels anecdotal at the moment and authors do not provide firm conclusions about that. The only evidence they provide is that performance becomes better with more message passing iterations they allow. This is a hint though to dive deeper rather than a firm conclusion. For example do we know if the finding about sensitivity to  message passing  is due to the small size of the network or the training procedure?  2) To add on that, there is virtually no information on the paper about the specifics of the experimental setup, so the reader cannot be convinced that the negative results do not arise from a bad experimental configuration (e.g., small size of network). 3) Moreover, the negative results here, as the authors point, seem to contradict previous work, providing negative results against GNNs.  Again, this is a valuable contribution if that is indeed the case, but again the paper does not provide enough evidence. In lieu of a convincing set of experiments, the paper could provide a proof (as also asked by R3). However with no proof and not strong empirical evidence that this result does not feel ready to get published at ICLR.  Overall, I think this paper with a bit more rigor could be a very good submission for a later conference. However, as it stands I cannot recommend acceptance.  
All reviewers gave, though not very strong,  positive scores for this work.  Although the technical contribution of the paper is somewhat incremental, the reviewers agree that it solidly addresses the known important issues in BERT, and the experiments are extensive enough to demonstrate the empirical effectiveness of the method.  The main concerns raised by the reviewers are regarding the novelty and the discussion with respect to related work as well as some unclear writings in the detail,  but I think the pros outweigh the cons and thus would like to recommend acceptance of the paper.  We do encourage authors to properly take in the reviewers  comments to further polish the paper in the final version.  
The authors present a simple modification of existing byzantine resistant techniques for training in the presence of worst case failures/attacks. The paper studies two of the strongest attacks to date, that no other method, till now, has been able to address. The novelty is significant for the related byzantine ML literature. The authors further do a fantastic job in their experiments and sharing reproducible code. Some weak aspects of theory are in fact attributed to what the metrics and guarantees that the related literature studies. The novelty of this paper does not lie so much in the theory contribution, but more so on their experiments and presented intuition. I believe this will be a paper that people will build up on and the ideas presented here are of solid value and importane.
This paper presents a novel regularization technique for CNNs based on swapping feature vectors in the final layer. It is demonstrated that this simple technique helps with generalization in supervised learning and RL with image inputs. Following the author rebuttal, all reviewers agreed that the simplicity of this method and the nice empirical performance it obtains is important to report to the community. In this respect, I agree with the reviewers, and recommend acceptance.  One important issue that came up during the discussion is how much this work is related to RL, and the authors SL experiments helped to put the contribution in a broader context. Indeed, one way to see the results of this work is that if such performance improvement is obtained in the Procgen benchmark with just image based regularization, perhaps this benchmark is not very suitable for studying generalization in RL (where we expect that more sophisticated techniques would be required). In addition, I can think of RL domains (e.g., Tetris, which was mentioned in the discussion) where I would not expect the proposed method to help. It would be good if the authors discuss these issues in some capacity in their final version.  Please take all reviewer comments into account when preparing the final version.
The paper proposes a progressive pruning technique that achieves high pruning ratio. Reviewers have a consensus on rejection. Reviewer 1 pointed out that the experimental results are weak. Reviewer 2 is also concerned about the proposed method and experiments. Reviewer 3 is is concerned that this paper is incremental work. Overall, this paper does not meet the standard of ICLR. Recommend for rejection. 
The paper proposes a trick for stabilizing GAN training and reports experiment results on spectrogram synthesis. All the reviewers rate the paper below the bar, citing various concerns, including a lack of clarity and unconvincing results. Several reviewers suggest conducting evaluations in the image domain as most of the GAN training techniques are proposed in the image domain. After consolidating the reviews and rebuttal, the area chair finds the reviewer s argument convincing and would not recommend acceptance of the paper.
The reviewers generally agree that the DDRprog method is both novel and interesting, while also seeing merit in outperformance of related methods in the empirical results. However, There were a lot of complaints about the writing quality, the clarity of the exposition, and unclear motivation of some of the work.  The reviewers also noted insufficient comparisons and discussions regarding relevant prior art, including recursive NNs, Tree RNNs, IEP, etc.  While the authors have made substantial revisions to the manuscript, with several additional pages of exposition, reviewers have not raised their scores or confidence in response.
This paper proposes that we can understand the evolution of representations in deep neural networks during training using the concept of "usable information". This is effectively an indirect measure of how much information the network maintains about a given categorical variable, Y, and the authors show that it is in fact a variational lower bound on the amount of mutual information that the network s representations have with Y. The authors show that in deep neural networks the usable information that is maintained for different variables during training depends on the task, such that task irrelevant variables (but not task relevant variables) eventually have their usable information reduced, leading to "minimal sufficient representations".   The initial reviews were mixed. A common theme in the critiques was the lack of evidence of the generalization and scalability of these results. The authors addressed these concerns by including new experiments on different architectures and the CIFAR datasets, leading one reviewer to increase their score. The final scores stood at 3, 7 ,7, 7. Given the overall positive reviews, interesting subject matter, and relevance to understanding learned representations in deep networks, this paper seems appropriate for acceptance in the AC s opinion.
The reviewers agree that the paper presents nice results on model based RL with an ensemble of models. The limited novelty of the methods is questioned by one reviewer and briefly by the others, but they all agree that this paper s results justify its acceptance.
The paper proposes to create models that address tail classes by computing a linear combination over models (concatenated weight vectors). Reviewers had grave concerns about the technical contribution, including justification of linear averaging of non linear models, and about the experimental results, which improve on tail classes but hurt overall performance. As a result, the paper cannot be accepted to ICLR. 
The reviewers appreciate the importance of enforcing safety in RL, and the technical directions considered in the paper related to incorporating cost in advantage estimation.  However, they express several concerns about the formulation of the problem considered and the consistency of the approach, as well as the somewhat incremental contribution w.r.t. CPO.  Three reviewers recommend rejection.
Differentiable neural networks used as a measure of design optimality in order to improve efficiency of automated design.   Pros:   Genetic algorithms, which are the dominant optimization routine for automated design systems, can be computationally expensive. This approach alleviates this bottleneck under certain circumstances and applications.  Cons:   Primarily application paper, machine learning advancement is marginal.   Multiple reviewers: Generalization capability not clear. For example, some utility systems may be stochastic (i.e. turbulence) and require multiple trials to measure fitness, which this method would not be able to model. Overall, the committee feels this paper is interesting enough to appears as a workshop paper.
This paper proposes to synthetize virtual outliers by sampling from low likelihood regions of the feature space of a class conditional distribution, in order to make more robust predictions via a regularization loss term.  In the reviewing phase certain criticisms were raised by reviewers: namely that i) the paper was not clear w.r.t. its goal, motivation and position in the literature of OOD detection for bounding boxes; ii) details about the energy based formulation and covariance definitions and iii) experimental setting and metric used were missing. During rebuttal the authors answered to all the above criticisms up to a satisfying extent and were able to increase two reviewers  scores.  The paper is accepted conditioned on the fact that the camera ready includes the additional details and discussions that arose in the comments with a specific emphasis on properly framing (and limiting) the motivation of OOD detection for open set object detection and it is expected to properly cite the literature of the more general OOD detection task as discussed in the comments.
The main merit of the paper is to try to address some important issues about GNN, e.g. expressivity power and data augmentation, from a novel perspective and using well grounded mathematical tools. Unfortunately, however, this novel perspective is also introducing some confusion about its meaning in the context of graphs. In fact, it is not clear how, in the general case, direction as introduced in the paper makes sense, especially when considering the data augmentation approach. Moreover, although well grounded mathematical tools are used, proofs of theorems, as well as justification of related corollaries, are not sufficiently clear to guarantee their correctness.  In summary, a potentially interesting contribution that needs more work to better clarify motivations, grounding to common graph concepts, better presentation of the theoretical results.
This paper proposes a metalearning objective to infer causal graphs from data based on masked neural networks to capture arbitrary conditional relationships. While the authors agree that the paper contains various interesting ideas, the theoretical and conceptual underpinnings of the proposed methodology are still lacking and the experiments cannot sufficiently make up for this. The method is definitely worth exploring more and a revision is likely to be accepted at another venue.
All reviewers agree that the presented approach to fair calibration of face verification models is interesting and needed in the field. The method does not require access to sensitive attributes for calibrating, which makes it sustainable. The reviewers are satisfied with the presented experimental studies in most cases. The rebuttal addressed a large majority of additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a poster.
The paper proposes a method for OOD detection which leverages the uncertainties associated with the features at the intermediate layers (and not just the output layer).  All the reviewers agreed that while this is an interesting direction, the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about other relevant baselines, some of the reported empirical results, and clarity of the explanation.  I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue. 
Pros   Competitive results on LibriSpeech. Cons   Limited novelty, and lacks enough comparisons.   Comparison with other end to end approaches, and on other commonly used datasets, like WSJ, missing.   Gated convnets have already been proposed.   Letter based systems have been shown to be competitive to phone based systems.   Optimization criterion is quite similar to lattice free MMI proposed by Povey et al., but with a letter based LM and a slightly different HMM topology.  Given the cons pointed out by reviews, the AC is recommending that the paper be rejected.  
The paper studies mixture of expert policies for reinforcement learning agents, focusing on the problem of policy gradient estimation. The paper proposes a new way to compute the gradient, apply it to two reinforcement learning algorithms, PPO and SAC, and demonstrate it in continuous MuJoCo environments, showing results that are comparable to or slightly exceeds unimodal policies. The main issue raised by multiple reviewers is novelty. Mixture of expert models have been widely studied in the context of reinforcement learning, and while the paper proposes a new method for the gradient computation, a more suitable format, as pointed out by Reviewer 2, could be to ground the paper around the proposed gradient estimator, and compare, both analytically and empirically, it to existing alternatives. Therefore, I recommend rejecting this submission.
This paper proposes search guided training for structured prediction energy networks (SPENs).  The reviewers found some interest in this approach, though were somewhat underwhelmed by the experimental comparison and the details provided about the method.  R1 was positive and recommends acceptance; R2 and R3 thought the paper was on the incremental side and recommend rejection. Given the space restriction to this year s conference, we have to reject some borderline papers. The AC thus recommends the authors to take the reviewers comments in consideration for a "revise and resubmit".
The reviewers all generally appreciated the idea in the paper. However, the nature of this contribution necessitates an empirical evaluation, and the reviewers generally found this to not be sufficient convincing. My assessment is that this idea can likely result in a successful publication, but will require additional empirical evaluation and analysis as suggested by reviewers. While the authors did add some additional results during the response period, they do not seem to be sufficient to fully address reviewer concerns.
The paper proposes a new, stable metric, called Area Under Loss curve (AUL) to recognize mislabeled samples in a dataset due to the different behavior of their loss function over time. The paper build on earlier observations (e.g. by Shen & Sanghavi) to propose this new metric as a concrete solution to the mislabeling problem.   Although the reviewers remarked that this is an interesting approach for a relevant problems, they expressed several concerns regarding this paper. Two of them are whether the hardness of a sample would also result in high AUL scores, and another whether the results hold up under realistic mislabelings, rather than artificial label swapping / replacing. The authors did anecdotally suggest that neither of these effects has a major impact on the results. Still, I think a precise analysis of these effects would be critically important to have in the paper. Especially since there might be a complex interaction between the  hardness  of samples and mislabelings (an MNIST 1 that looks like a 7 might be sooner mislabeled than a 1 that doesn t look like a 7). The authors show some examples of  real  mislabeled sentences recognized by the model but it is still unclear whether downweighting these helped final test set performance in this case.   Because of these issues, I cannot recommend acceptance of the paper in its current state. However, based on the identified relevance of the problem tackled and the identified potential for significant impact I do think this could be a great paper in a next iteration. 
This paper presents a reinforcement learning approach for online cost aware feature acquisition. The utility of each feature is measured in terms of expected variations of the model uncertainty (using MC dropout sampling as an estimate of certainty) which is subsequently used as a reward function in the reinforcement learning formulation. The empirical evaluations show improvements over prior approaches in terms of accuracy cost trade off on three datasets. AC can confirm that all three reviewers have read the author responses and have significantly contributed to the revision of the manuscript.   Initially, R1 and R2 raised important concerns regarding low technical novelty. R1 requested an ablation study to understand which of the following components gives the most improvement: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Pleased to report that the authors addressed the ablation study in their rebuttal and confirmed that MC dropout certainty plays a crucial rule in the performance of the proposed method. R1 subsequently increased the assigned score to 6. R2 raised concerns about related prior work Contardo et al 2016, which similarly evaluates the most informative features given budget constraints with a recurrent neural network approach. After a long discussion and a detailed rebuttal, R2 upgraded the rating from below the threshold to 7, albeit acknowledging an incremental technical contribution. R3 raised important concerns regarding presentation clarity that were subsequently addressed by the authors. In conclusion, all three reviewers were convinced by the authors rebuttal and have upgraded their initial rating, and AC recommends acceptance of this paper – congratulations to the authors! 
After the rebuttal, all reviewers rated this paper as a weak accept.  The reviewer leaning towards rejection was satisfied with the author response and ended up raising their rating to a weak accept.  The AC recommends acceptance.
This paper introduces the recall loss for dealing with imbalance training contexts. The authors propose to perform a class wise weighting of examples based on the instantaneous recall performance during training.   The reviewers like the clarity of the presentation, but raise several concerns regarding novelty of the approach, comparison to more baseline loss functions and state of the art methods.  The AC carefully reads the paper and discussions. The AC appreciates the discussion with respect to competitive loss functions, e.g. focal loss or segmentation loss approximations (SoftDice or Lovasz), which clearly highlights some limitations in existing approaches. \ However, the AC  considers that the approach is essentially a new way to setup the compromise between precision and recall. In that respect, the claims in the paper and in discussion regarding the performances of the proposed method are often exaggerated. For example in segmentation, the method obtaining the best accuracy is CB CE in 3 out of 4 experiments (on Syntia the mIOU improvement of the recall loss compared to CB CE corresponds to about the same drop in mACC). This is also verified on Fig 2 where CB CE outperforms the proposed method by a large margin on mean accuracy for small classes. For classification, the performance gains  compared to SDN (CE) are small. \ The AC thus considers that the paper in the current form falls short of the ICLR acceptance threshold. 
The major concern with this paper is the unfair comparison between global and local clipping (at least from the theoretical point of view). The assumption that the norms of the gradients are bounded in Theorem 2 is too strict for the following reasons. Clipping has been introduced exactly because we cannot assume the norm of the gradient to be bounded by a fixed constant  in the first place. Accordingly, comparing two clipping methods under the bounded gradient assumption does not seem relevant. Further, the two methods are not studied under the same set of assumptions (In Theorem 1, the norm of the gradient is not assumed to be bounded, but in Theorem 2 it is).  A fair comparison needs to be presented to make the case for the proposed method.
This paper tackles a small batch online unsupervised learning problem, specifically proposing an online unsupervised prototypical network architecture that leverages an online mixture based clustering algorithm and corresponding EM algorithm. Special features are added to deal specifically with the non stationary distributions that are induced. Results are shown on more realistic streams of data, namely from the RoamingRooms dataset, and compared to existing self supervised learning algorithms including ones based on clustering principles e.g. SWaV.   Overall, the reviewers were positive about the problem setting and method, but had some concerns about hyper parameters (hYzM, cvrN, LjvY) and motivation for the specific setting where the method excels compared to other methods not designed for such a setting (hYzM, cvrN), i.e. small batch setting, where it is not clear where the line should be drawn in terms of batch size and memory requirements with respect to performance differences between the proposed approach and existing self supervised methods. Importantly, all reviewers had significant confusions about all aspects of the work ranging from low level details of the proposed method to the empirical setting and evaluation (including for competing methods). After a long discussion, the authors provided a large amount of details about their work, which the reviewers and AC highly appreciate. However, in the end incorporating all of the feedback requires a major revision of the entire paper. Even the reviewers that were more on the positive side (cvrN and LjvY) mentioned it would be extremely beneficial for this paper to be significantly revised and go through another review. Since so many aspects were confusing, it is not clear to the AC that the underlying method, technical contributions, and other aspects of the works had a sufficient chance to be evaluated fairly, given that much of the review period was spent on clearing up such confusion.   In summary, while the paper is definitely promising and tackles an important area for the community, it requires a major revision and should go through the review process when it is more clearly presented. As a result, I recommend rejection at this point, since it is not ready for publication in its current form.
The paper studies the problem of multi class calibration, proposing new notion of "top label calibration", and presenting and comparing new algorithms for multi class calibration. Reviewers generally found the paper to be well written, and tackling a foundational problem. There were some questions regarding the experiments:  (1) _Lack of explanation of why unnormalised beats normalised._ One of the paper s main empirical findings is that using an unnormalised predictor with histogram binning (CW HB) can significantly outperform a normalised one (N HB). There is however limited discussion prior to this of why such behaviour is expected.  (2) _Lack of comparison to isotonic regression._ One can apply isotonic regression in conjunction with one of the M2B algorithms in Sec 3. It is of interest whether isotonic regression + a suitable M2B wrapper compares to HB + an M2B wrapper.  (3) _Comparison to OVA calibration_. One reviewer raised the concern that the new algorithms proposed in this work are not too surprising; e.g., using a binary calibrator of one label vs everyone else appears natural.  (4) _Overloaded appendices_. One reviewer pointed out that some material in the Appendix is not referenced in the body, thus making the work not self contained.  For point (1), the response indicates that the unnormalised model can obtain distribution free guarantees. The lack of such a guarantee for the normalised model does not suggest that such a guarantee is impossible, however. Certainly the present work need not solve this issue in entirety, but if this is intended to be a main takeaway of the experiments, a little more discussion in the body seems advisable.   On this note, from my reading, the experiments seek to demonstrate that suitable M2B reductions can dramatically improve the performance of HB. However, with the use of multiple evaluation metrics (Top ECE, Top MCE, Classwise ECE) it is not clear if the authors intend to promote one specific M2B reduction as generally favourable; further, the text in Sec 3.2 suggests that N HB is the method considered in prior works, which then seems to do better on top label ECE than prior works. Which suggests that for this particular metric, one does not gain much from other M2B reductions?  For point (2), the response argued that "the main message of the paper is not about HB vs other binary calibrators, but proposing a single agnostic framework for achieving multiple notions of multiclass calibration using any binary calibrator". From my reading of the paper, I think this claim is accurate, and agree that the conceptual advance is the generic M2B framework itself  For point (3), the authors responded to suggest that while Algorithm 2 performs a natural one versus all calibration, this is different from Algorithm 1. Further, the latter is shown to be useful in Table 2 (bottom panel).  For point (4), the revision involved referencing relevant material in the Appendix. These seem better, though I would prefer if theorem statements (e.g., Theorem 1) appear in full or as sketches in the body.  Further to the above, I have a couple of minor suggestions:   consider removing most hline s from Tables 1   3    keep the ordering of Algorithms 1   4 the same as that in which methods are presented in Table 1.  **Summary**. The paper considers a foundational problem. It makes one simple yet interesting contribution in its definition of top label calibration. Detailing the various multi class calibrators in Section 3 is another contribution: albeit simple, it does illuminate the subtle issue of the role of normalisation in post hoc calibration, which empirically is shown to have non trivial impact. There are certainly avenues where the paper could be further strengthened, but overall I do see it as potentially being of broad interest to the community, and inspiring future work. My recommendation is thus for the authors to further incorporate the reviewers  detailed suggestions and the comments above, which can broaden the clarity, scope and impact of the work.
This paper proposes a cognitive science inspired interaction setting between two agents, an "architect" and "builder", in which the architect must produce messages to guide the builder to achieve a task. Unlike other related settings (such as typical approaches in MARL, HRL, or HRI), the builder does not have access to the architect s reward function, and must learn to interpret the architect s messages by assuming the architect is telling it to do something sensible. At the same time, the architect determines what is "sensible" by building a model of the builder s behavior and planning over it. This setting is common particularly in human agent interactions, where humans may not be able to either (1) accurately communicate a scalar reward or (2) provide demonstrations, but can still provide information that the agent ought to be able to learn from. The paper demonstrates that the learned communication protocol generalizes well to new settings.  While this paper generated a lot of discussion, the reviewers did not come to a consensus on whether the paper should be accepted or rejected, with those in favor of the paper maintaining it should be accepted and those not in favor maintaining that it needs work. I have therefore done a particularly close read of both the paper and the discussion in order to weigh the pros and cons brought up by the reviewers.  The positive reviews clearly indicate that this work is insightful and of interest to researchers in the ICLR community (in fact, all reviewers mentioned they found the work interesting and well written). In particular, Reviewer hMeT wrote: "I am positive about this framework as it presents a better model for multi agent communication, especially enriching the communication among agents over the fixed, restricted reward based communication protocol in traditional RL." I am inclined to agree with this assessment and find the communication setting studied in this paper to be much more ecologically valid for human agent interaction settings than having humans communicate scalar rewards or provide demonstrations: humans are typically poor at the former and may not have the same embodiment to achieve the latter.  The negative reviews focused on a few cons: (1) the assumption that the architect has access to a ground truth environment model, (2) confusion about differences from other related fields (e.g. feudal RL, MARL), and (3) lack of analysis of the communication protocol. I have considered these points, but do not feel any of them are fatal flaws: (1) From the perspective of human agent interaction, I think it is very reasonable to assume that a human architect would have a good model of the world and would be generally proficient at solving tasks in the world. Making this approach work in the setting where the architect is *also* learning how the world works seems squarely in the domain of future work. (2) The authors have done an extensive job of clarifying the differences between these related areas, and as discussed above, other reviewers found the way in which AGP is different to be insightful and ecologically valid. (3) This is potentially the most serious con: as the discussion with Reviewer BHGy brought up, the learned communication protocol may just be a simple mapping between messages and environment interactions. After further discussion in which the authors argued that learning a simple mapping is not a problem the main question is how to even induce such a mapping in the first place the reviewer acknowledged that this is not a fatal flaw but that makes the results somewhat less interesting.  In summary, the positive reviews highlighted the interestingness and insightful nature of the questions studied in this paper and have convinced me that this paper will be of interest to the ICLR community as it has provides a new perspective on the problem of agent agent interaction (particularly for the special case of human agent interaction). The negative reviews did highlight a few limitations of the paper, but I expect these can be addressed by future work and do not feel they outweigh the interestingness of the problem. In light of this, I recommend acceptance as a poster.  Suggestion for the authors: I found the discussion with Reviewer BHGy to be particularly insightful and helpful in understanding the aims of the paper. I would encourage you to incorporate some of this into the camera ready version of the paper, and perhaps to lean more heavily on the special case of human agent interaction as motivation of this work (as also hinted at by Reviewer hMeT).
This paper analyzes dropout and shows it selectively regularizes against learning higher order interactions. The paper received mixed reviews, with two in favor of rejection and one in favor of acceptance. Specifically, while all reviewers find the intuitions and ideas in the paper adequate/plausible, two reviewers didn t find sufficient evident that supports the conclusions. The reviewers provided very detail feedback, which the authors responded to, but it is apparent that some of the analysis needs to be reviewed again before the paper can be published.
Addressed semi supervised learning with the MNAR setting.  Well written paper. Several additional experiments were reported in response to the reviewer questions.   General agreement amongst reviewers.
The reviewers raised a number of major concerns including the incremental novelty of the proposed (if any), a poor readability of the presented materials, and, most importantly, insufficient and unconvincing experimental evaluation presented. The authors did not provide any rebuttal. Hence, I cannot suggest this paper for presentation at ICLR.
This paper presents an approach for learning disentangled static and dynamic latent variables for sequence data. In terms of learning objective, the paper extends Wasserstein autoencoder to sequential data, and this approach is novel and well motivated; the aggregated posterior for static variables comes out naturally and plays an important role for regularization (this appears to be new for sequence data). The authors also studies how to model additional categorical variables for weakly supervised learning in real scenarios. The main steps (generation and inference) were illustrated by graphical models with clarity, and rigorous statements are provided to back them up. Experimental results demonstrate the advantages of proposed method, in terms of disentanglement performance and generation quality.  The reviewers think this paper makes nice contributions to the sequential generative model community.
The paper proves that the Donsker Varadhan lower bound on KL divergence cannot be used to estimate KL divergences of more than tens of bits, and that more generally any distribution free high confidence lower bound on mutual information cannot be larger than O(ln N) where N is the size of the data sample. As an alternative for applications such as maximum mutual information predictive coding, a form of representation learning, the paper proposes using the cross entropy upper bound on entropy and estimating mutual information as a difference of two cross entropies. These cross entropy bounds converge to the true entropy as 1/\sqrt(N), but at the cost of providing neither an upper nor a lower bound on mutual information. There was a divergence of opinion between the reviewers on this paper. The most negative reviewer (R3) thought there should be experiments confirming that the DV bound fails when mutual information is high, was concerned that the theory applied only in the case of discrete distributions, and was concerned that the proposed optimization problem in Section 6 would be challenging due to its adversarial (max inf) structure. The authors responded that they felt the theory could stand on its own without empirical tests (a point with which R1 agreed); that although their exposition was for discrete variables, the analysis applies to the continuous case as well; and that they agreed with the point about the difficulty of the optimization, but that GANs face similar difficulties. Because R3 did not participate in the discussion and the AC believes that the authors adequately addressed most of R3 s issues in their response and revision, this review has been discounted. The next most negative reviewer (R2) wanted a discussion relating the ideas in this paper to kNN and kernel based estimators of mutual information, wanted an empirical evaluation (like R3), and was concerned about whether the difference of cross entropies provides an upper or lower bound on mutual information. In their response and revision the authors added some discussion of kNN methods (but not enough to make R2 happy) and clarified that the difference of cross entropies provides neither an upper nor a lower bound. The most positive reviewer (R1) thinks the theoretical contribution of the paper is significant enough to justify publication in ICLR. The AC likes the theoretical work and feels that it raises important concerns about MINE, but concurs with R2 and R3 that some empirical validation of the theory is needed for the paper to appear in ICLR. The authors are strongly encouraged to perform an empirical validation of the theory and to submit this work to another machine learning venue.
The reviewers all seemed to agree that the investigation of other losses is an interesting direction of study, and acknowledged there was some empirical performance improvement for standard computer vision tasks. However, they felt the justification of the specific form of loss was a bit shaky and heuristic, and were furthermore unconvinced by results exclusively for image classification (one reviewer was unmoved by the magnitude of improvement). This was a borderline decision, but we hope the authors refine and resubmit their work as this is an interesting but underexplored direction within DPML.   As one recent related work which investigates the effect of other architecture differences in the DP setting, the authors may be interested in https://arxiv.org/abs/2110.08557.
The proposed notion of star convexity is interesting and the empirical work done to provide evidence that it is indeed present in real world neural network training is appreciated.  The reviewers raise a number of concerns. The authors were able to convince some of the reviewers with new experiments under MSE loss and experiments showing how robust the method was to the reference point. The most serious concerns relate to novelty and the assumptions that individual functions share a global minima with respect to which the path of iterates generated by SGD satisfies the star convexity property. I m inclined to accept the authors rebuttal, although it would have been nicer had the reviewer re engaged. Overall, the paper is on the borderline.
All reviewers recommend acceptance, with two reviewers in agreement that the results represent a significant advance for autoregressive generative models. The AC concurs. 
The paper proposes and uses a fairly involved attention based architecture to perform time series forecasting. The idea of transformers is raised, but, given how sequence embedding is often convolutional, and position encoding input is provided to the model (albeit implictly in the form of features having to do with qualitative things such as promotions, etc among other things), I m  of the opinion it is closer to the paper "Convolutional Sequence to Sequence Learning" https://arxiv.org/abs/1705.03122 than it is to transformers per se... Also the connection to sequence to sequence is not clear, since the chain rule of probability isn t really stressed on much.   The paper proposes some interesting ideas, but I feel that it failed to convince the reviewers of the utility and the novelty. Part of it has to do with the clarity of presentation, and part of it, I think has to do with the fact that paper jam packs a bunch of different ideas together, without carefully ablating the influence of their various ideas. For example, transfomers have been applied for time sequences (https://arxiv.org/pdf/2001.08317.pdf), and its not clear in what ways this paper improves on them   is it the complicated attention model scheme ? or is it the multi horizon context schemes ?    That being said, the results shows are relatively decent and the reviewers liked that aspect. Had the paper been easier to follow and the ideas presented with a little more insight it would be been a better fit for ICLR. As it stands, I have to give it a weak reject.
The authors propose an efficient scheme for encoding sparse matrices which allow weights to be compressed efficiently. At the same time, the proposed scheme allows for fast parallelizable decompression into a dense matrix using Viterbi based pruning.  The reviewers noted that the techniques address an important problem relevant to deploying neural networks on resource constrained platforms, and although the work builds on previous work, it is important from a practical standpoint.  The reviewers noted a number of concerns on the initial draft of this work related to the experimental methodology and the absence of runtime comparison against the baseline, which the reviewers have since fixed in the revised draft. The reviewers were unanimous in recommending that the revision be accepted, and the authors are requested to incorporate the final changes that they said they would make in the camera ready version. 
This paper is truly borderline. On one hand, the theoretical contribution seems novel and interesting, however, there appears to be somewhat of a gap between theory and practice.   There is unfortunately another problem. According to the authors, the main contribution of this publication is arguably the introduction of the nearest neighbor as the positive example in the triplet loss. However, the authors seem to be unaware of the history of the triplet loss. It was originally introduced by Schultz & Joachims 2004 as a loss over all triplets.  Weinberger et al. 2005 changed it and use the nearest neighbor as "target neighbor", which is called "easy positives" here, as the objective of LMNN. In 2009 Chechik et al. subsequently relaxed this positive neighbor formulation to any similarly labeled sample (going back to the Schultz & Joachims formulation) but sampling triplets. The re introduction of the nearest neighbor as "easy positive" was then covered by Xuan et al. 2020.    Unfortunately all of this diminishes the novelty significantly and it is clear that the paper in its current form does not have a strong enough contribution. I do encourage the authors to take a close look at the original LMNN publication and Xuan et al and write an improved re submission for the next conference that maybe focuses more on the theoretical contribution.  Good luck,  AC
 The paper considers exploiting low rank structure in Q function and the Hamiltonian Monte Carlo (HMC) to approximate the expectation in Q learning to reduce the stochastic approxiamtion error, and thus, achieves "efficient RL". The authors tested the algorithm empirically within some simple environments.   As reviewers (R1, R3, R4) mentioned, the major bottleneck of this algorithm is the assumption that the dynamics is known up to a constant, which is extremly strong, and thus, limits the application of the algorithm. I suggest the authors to consider the common RL setting, without any knowledge about the transition models, and make fair empirical comparison with baselines in the same setting.  
Thanks for your submission to ICLR.  This paper presents an extension to Deep Hashing Networks that utilizes angular similarity, and show improved results using the proposed method.  The reviewers were somewhat mixed on this paper, with two of three reviewers on the negative side.  Some reviewers appreciated that the paper was easy to follow and well written, though one reviewer felt that the paper s writing and presentation could improve.  A big concern about the paper expressed by multiple reviewers was that the paper was incremental, in that the main architectural difference seemed to be a change in loss function over existing work.  Unfortunately, the reviewers were fairly unresponsive to attempts to get them to respond to the rebuttals offered by the authors.    Ultimately, I took a look at the paper and found it to be borderline.  I do think the contribution is a bit limited, particularly as it is in an area which has seen many papers over the years (and thus has a high bar for new work).  However, with some additional work this paper could definitely be acceptable.  I think it could use an additional round of editing and review, and I d encourage the authors to submit this paper to another venue.
The authors propose an approach for the task of categorizing competencies in terms of worker skillsets. This is a potentially useful (if somewhat niche) task, and one strength here is a resource to support further research on the topic. However, the contribution here is limited: The methods considered are not new, and while the problem has some practical importance it does not seem likely to be of particular interest to the broader ICLR community. 
This paper presents a language model for Amharic using HMMs and incorporating POS tags. The paper is very short and lacks essential parts such as describing the exact model and the experimental design and results. The reviewers all rejected this paper, and there was no author rebuttal. This paper is clearly not appropriate for publication at ICLR. 
The authors propose a novel method for conditioning deep neural network. They replace the activation function with a linear combination of activation functions (e.g., ReLu). The weights for the activation functions are dynamically computed from the input during inference and training. The approach is evaluated on standard public tasks and shows improvement over well established alternatives.  Pros + A simple novel method for condition that is widely applicable + Adequate empirical evaluations to demonstrate it s effectiveness  Cons   No major weakness  The reviewers provided several feedback. The authors incorporated the suggestions and clarified residual concerns. The revised version of the paper has improved the readability and utility substantially.
Three out of the four reviews rated this paper well below the acceptance threshold. Although the review scores show a relatively large spread, I think that the review contents are more or less coherent across the four reviewers. The equivalence of the state equations (SEs; a set of equations that macroscopically characterizes optimal solutions of certain high dimensional regression problems) derived from three different approaches (AMP, CGMT, and LOO) is well expected to hold, as the optimal solutions should be independent of how their macroscopic characterization in the form of an SE is derived, and this paper concretely showed such equivalence to hold for three problems. More concretely, Theorem 1 states the equivalence of the SEs for M estimator derived from the three approaches, Theorem 2 states the equivalence of the SEs for LASSO derived from AMP and CGMT, and Theorem 4 states the equivalence of the SEs for logistic regression derived from LOO and CGMT. The main concern raised by all the reviewers is that this paper does not provide novel and significant insights as to why and how the equivalence arises. Some reviewers also pointed out that this paper lacks citation to the relevant statistical mechanics literature, as well as that this paper contains so many typos, grammatical errors, and inappropriate typesetting styles. The authors responses were not instrumental in persuading the reviewers with negative evaluation. On the basis of these I would not be able to recommend acceptance of this paper for presentation at ICLR 2021.
The paper gives an extension of the transformer model that is suited to computing representations of source code. The main difference from transformers is that the model takes in a program s abstract syntax tree (AST) in addition to its sequence representation, and utilizes several pairwise distance measures between AST nodes in the self attention operation. The model is evaluated on the task of code summarization for 5 different languages and shown to beat two state of the art models. One interesting observation is that a model trained on data from all languages outperforms the monolingual version of the model.  The reviewers generally liked the paper. The technical idea is simple, but the evaluation is substantial and makes a convincing case about setting a new state of the art. The observation about multilingual models is also interesting. While there were a few concerns, many of these were addressed in the authors  responses, and the ones that remain seem minor. Given this, I am recommending acceptance as a poster. Please incorporate the reviewers  comments in the final version. 
This paper proposed two variants of the SELU activation function, termed the leaky SELU (lSELU) and scaled SELU (sSELU), respectively, in order to yield a stronger self normalization property. The review process and the discussion find the following issues:    The hyperparameter tuning for the baselines is insufficient for the baselines so that the comparison may be unfair.    The experiment results (Table 2) do not show superiority of the proposed activation functions. In addition, the results appear to be unrelated to each other. (see Reviewer 3 s detailed update)   Reviewer 2 pointed out that the architecture that the authors used was far from the SOTA. I read the authors  response. This paper may benefit from adding some even naive workaround and making fair comparisons under the SOTA architecture.   I do not think (6) is a good way to present this equation. The authors may want to perform change of variable and replace $\sqrt{q}z$ by $z$ in the integral and add this form to the right hand side of (6). In addition, $\epsilon$ appears in Definition 2. However, when the authors mention the self normalization property in Definition 2, they omit $\epsilon$. It might be better to call it $\epsilon$ self normalization property to stress that this definition depends on $\epsilon$.    Other minor issues:   The line right below (15) on page 11, the authors did not need to capitalize "orthogonal".   In eq (16), $W_l^{,T}$, the comma is unnecessary. 
This paper use Group convolutional neural networks in both generators and discriminator of GANs, and demonstrates advantages of this approach when training with a relatively small sample size. While the novelty is limited in the work  as it simply applies G CNN for GANs , I believe this application is interesting and  the authors have applied it to many GAN image synthesis applications (conditional generation , pix2pix) on various benchmarks, which gives  evidence of  the potential of GCNNs in generative modeling. Accept
This paper showcases how saliency methods are brittle and cannot be trusted to obtain robust explanations. They define a property called input invariance that they claim all reliable explanation methods must possess. The reviewers have concerns regarding the motivation of this property in terms of why is it needed. This is not clear from the exposition. Moreover, even after having the opportunity to update the manuscript they seem to have not touched upon this issue other than providing a generic response.
The paper proposes a method that combines imitation learning and meta learning, which aims to be able to explore beyond the provided demonstrations.   While the paper addresses an important topic, and the authors are commended on a productive conversations, there is a consensus among the reviews that the work is not yet ready for publication. The future manuscript should address: reexamine the assumptions and improve presentation.
This paper proposes a bottom up multi person pose estimation method using a Transformer model. There is consensus among the reviewers that this paper is not ready for acceptance/publication. Although some reviewers find the proposed idea interesting (some find it lacking novelty though), all the reviewers agree that the quantitative experimental results are not promising. Some reviewers explicitly criticized lacking empirical accuracy compared to state of the arts. The authors provided additional details and results in the rebuttal, but they were not sufficient to change the opinions of the reviewers.  We recommend rejecting the paper.
It seems the reviewers are in an agreement that the work seems interesting, well motivated, and results are meaningful. The main complaints or issues that remain is the amount of rewriting involved, which might be hard for the reviewers to track, and maybe question regarding the results given for e.g. the choice of architectures for CIFAR 10 making numbers harder to interpret.   However, I do see that the quality of the manuscript is quite good (and multiple reviewers commented on this), and the idea seems natural to me. I think the results, especially with the addition of CIFAR 100 and IMDB seem sufficient, and given the overall positive feeling of the reviewers over the work with no major concerns, I am happy to recommend this paper for acceptance.
The reviewers are unanimous in their opinion that the theoretical results in this paper are of limited novelty and significance. Several parts of the paper are not presented clearly enough. As such the paper is not ready for ICLR 2018 acceptance.
This paper shows minimax lower bounds on transfer learning for binary classification, in terms of a notion of transfer distance, defined in this paper. Experimental results try to show the validity of the proved minimax lower bounds.   All reviewers acknowledge that the lower bounds are worthy contributions; however, none of the reviewers felt strong enough to champion this work, due to that:   the theoretical sharpness of the lower bounds are not discussed in detail (Reviewers TfQT, pmYF, and dGvD). Remark 6 only discusses the regime of a small amount of source data and a large transfer distance, which is fairly limited.   it is unclear to what extent the experiments validates the theory (Reviewer WXNa). Note that for a minimax lower bound, for any algorithm, there is some corresponding "worst case" datasets such that the algorithm does not do well; it is unclear if the datasets considered here are worst case at all.    the lower bound techniques are fairly standard.   the comparisons between the lower bounds in this work and prior lower bounds (e.g. those in [1,2]) need to be discussed more thoroughly.  We encourage the authors to take into account the reviewers  feedback and revise the paper.   [1] Hanneke and Kpotufe. On the value of target data in transfer learning. NeurIPS 2019. [2] ​​Mansour, Mohri, Ro, Suresh, and Wu. A theory of multiple source adaptation with limited target labeled data. AISTATS 2021.
The paper proposes speeding up iterative simulations of complex dynamics systems based on connected rods. Traditionally, these systems alternate between forward integration of the dynamics and constraint projections. Instead of replacing this entirely with end to end trained ML, here ML is only added a single point in the method to speed up the iterative solver itself, more precisely by providing initial estimates for the constraint projection step. This is done with graph networks.  At initial evaluation, the paper had two slightly favorable reviews (6) and two unfavorable reviews (4) and was therefore on the fence leaning towards rejection.  Reviewers appreciated a well motivated method and in an interesting problem.  However, on the downside, issues raised where lukewarm performance; novelty (a direct application of graph networks); lack of generality of the approach; similarity to graph networks applied on mesh based physics simulations, and similarity to NN applied for speeding up elasticity simulations; application on the finest level only; memorization/lack of generalization; simplicity of baselines; simplicity of tasks (including the added more complex tree task).  There seemed to be some confusion on whether one of the reviewers had read the initial NeurIPS submission only (which he also had reviewed) or also the ICLR submission; the authors seemed to be upset up this possibility and made it clear in their responses, but the AC can assure them that the proper version has been read, reviewed and discussed; the author s responses in that respect were not helpful.  The authors provided responses to most of the raised issues, and several reviewers acknowledged that the paper had been improved, in particular by adding comparisons (e.g NN search).  However, in spite of these improvements, the discussion among reviewers and AC revealed that the paper still has serious issues, in particular minor novelty, lukewarm improvements, and some issues re: comparisons to baselines. While the reviewers acknowledged merits in the idea, the weaknesses hindered them to champion the paper for acceptance at this point, and the AC concurs, recommending rejection.
 The paper proposes to use a regularization term for stabilizing the perturbation trajectories in generating adversarial examples for medical image tasks. The authors tested the effectiveness of their proposal on different medical image datasets obtained by different modalities, and the experimental results are generally encouraging. All the reviewers see the value of the paper and give positive comments. At the same time, they also point out some aspects for further improvement, including 1)	The datasets used are relatively small 2)	The title is a little misleading since the paper only tackles the image attacks (but the title is stabilized medical attacks). 3)	Case studies and visualization are needed to help people better understand the paper  The authors have done a good job in their rebuttal and paper revision, by adding experiments on larger datasets, changing the title to “stabilized medical image attacks”, and adding some geometric figures for better illustration. These have largely addressed the concerns of the reviewers, and we see no problem with accepting the paper. 
This paper proposes a data imputation method for MCAR and MAR data by combining EM and normalizing flows.  The paper is clearly written.  The idea is interesting and they show better performance compared to MCFlow and competing methods on ten multivariate UCI data, MNIST and CFAR10 image data.  Issues regarding limited novelty compared to MCFlow was raised. Issues regarding the validity of Assumption 2 on the dependencies in the latent space and observation space was also raised.
The reviews are of good quality. The responses by the authors are commendable, but ICLR is selective and reviewers still believe that the research would be better as two separate papers: one about the problem and solution from an ML perspective, and the other about the application to surgery. Papers that provide a new method in the context of a single application domain run the risk of making a contribution to neither, and of being evaluated by reviewers who are not experts in both.
This work explores the distillation of language models using MixUp for data augmentation. Distillation with MixUp seems to be novel in the narrow context of distilling language models, although it has been used before in different contexts as the reviewers point out. The results of the experimental validation are encouraging, and the application is valuable and of wide interest to the ICLR audience. I therefore recommend accepting this paper for a poster presentation.
This paper presents a method for measuring the degree to which some representation for a composed object effectively represents the pieces from which it is composed. All three authors found this to be an important topic for study, and found the paper to be a limited but original and important step toward studying this topic. However, two reviewers expressed serious concerns about clarity, and were not fully satisfied with the revisions made so far. I m recommending acceptance, but I ask the authors to further revise the paper (especially the introduction) to make sure it includes a blunt and straightforward presentation of the problem under study and the way TRE addresses it.  I m also somewhat concerned at R2 s mention of a potential confound in one experiment. The paper has been updated with what appears to be a fix, though, and R2 has not yet responded, so I m presuming that this issue has been resolved.  I also ask the authors to release code shortly upon de anonymization, as promised.
The reviewer acknowledged that the proposed method is simple and seems to work well on the chosen benchmarks. Yet the expressed several concerns that were not fully addressed by the authors in their responses. The major concern is about the experimental setup. The chosen tasks have been judged too simple and quite different from those where the baselines were tested initially (e.g. DQfD was demonstrated on a diverse set of Atari games).   The clarity of the paper should also be improved. For instance, the way the number of trajectories that are added to the replay buffer increases with time is not well explained and it seems to be crucial for the algorithm to outperform the baselines. The authors also seemed to select the experiments so that the "results are more persuasive", discarding experiments where you can be unlucky. This looks very much like cherry picking and didn t convince the reviewers. 
The paper explores the representation power of GNNs, in particular, studying the bottleneck and improving expressiveness with new aggregators, which are analyzed theoretically. This issue was highlighted in previous works, but the merit of this paper is a constructive analysis.   The reviewers were overall not enthusiastic and  raised a few concerns:   Not enough context is provided about related work, in particular, the early work of Corso et. al.    Insufficiently convincing experiments  While the authors provided an elaborate rebuttal and extended the experimental section to address experiment concerns raised by most of the reviewers, the final evaluation was still lukewarm. Given that the conference has a very high bar and there have been many very good submissions on graphs, we find the paper not quite above the bar and hence have no choice but to recommend rejection with a heavy heart. The authors should be commended on their efforts and are encouraged to seek publication elsewhere.   
The paper proposes a new method for extreme multi label classification. However, this paper only combine some  well known tricks, the technical contributions are too limited. And there are many problems in the experiments, such as the reproducibility, the scal of data set and the results on well known extreme data sets and so on. The authors are encouraged to consider the reviewer s comments to revise the paper.
This paper proposes a theory for understanding the context representation in pretrained language models. The strengths of the paper, as identified by reviewers, are in the importance of an attempt to explain contextualization in language models, and in the novelty of using the category theory to model the connection between contexts and their representations. However, all the reviewers identify several major weaknesses, including flawed/incoherent definitions of concepts in the proposed theory and insufficient experimental results. Although the authors  rebuttal put a great deal of effort to address raised concerns, all five reviewers agree (and provide very detailed justifications along with suggestions for improvements) that the work is not yet ready for publication.
This work focuses on how one can design models with robustness of interpretations. While this is an interesting direction, the paper would benefit from a more careful treatment of its technical claims.  
This paper presents nucleus sampling, a sampling method that truncates the tail of a probability distribution and samples from a dynamic nucleus containing the majority of the probability mass. Likelihood and human evaluations show that the proposed method is a better alternative to a standard sampling method and top k sampling.  This is a well written paper and I think the proposed sampling method will be useful in language modeling. All reviewers agree that the paper addresses an important problem.   Two reviewers have concerns regarding the technical contribution of the paper (i.e., nucleus sampling is a straightforward extension of top k sampling), and whether it is enough for publications at a venue such as ICLR. R2 suggests to have a better theoretical framework for nucleus sampling. I think these are valid concerns. However, given the potential widespread application of the proposed method and the strong empirical results, I recommend to accept the paper.  Also, a minor comment, I think there is something wrong with your style file (e.g., the bottom margin appears too large compared to other submissions).
This paper proposes a conditional language specific routing (CLSR)  mechanism for multilingual NMT, which also considers the trade off between language specificity and generality.  All of the reviewers think this paper is interesting for both idea and empirical findings. Therefore, it is a clear acceptance.
The paper received mixed reviews, with one review voting for acceptance, one strongly opposed, and two borderline ones. The discussion essentially involved R1 and R2, who gave the most informative reviews. After discussion, they did not update their score, even though they appreciated the work and effort done by the authors during the rebuttal.   In short, the paper has some merit, but several concerns were raised, which the area chair agrees with, leading to a rejection recommendation. The innovation was found to be limited and the discussion between practice and theory (meaning assumptions made in this work) are not discussed in a convincing manner, and these concerns remained after the rebuttal. The experiments were also subject to improvements.  It is however likely that with a major revision, this work may become publishable to a another venue.
This paper presents a critical appraisal of variational mutual information estimators, and suggests a slight variance reducing improvement based on clipping density ratio estimates, and prove that this reduces variance (at the cost of bias). They also propose a set of criteria they term "self consistency" for evaluation of MI estimators and, and show convincingly that variational MI estimators fall short with respect to these.  Reviewers were generally positive about the contribution, and were happy with improvements made. While somewhat limited in scope, I believe this is nonetheless a valuable contribution to the conversation surrounding mutual information objectives that have become popular recently. I therefore recommend acceptance.
This paper presents a range of methods for over coming the challenges of large batch training with transformer models.  While one reviewer still questions the utility of training with such large numbers of devices, there is certainly a segment of the community that focuses on large batch training, and the ideas in this paper will hopefully find a range of uses. 
This paper shows the possibility to design a relatively shallow architecture, ParNet, based on parallel subnetworks, instead of traditionally deeply stacked blocks. During discussions, the reviewers pointed out two important concerns: (1) the current design heavily hinges on the recently proposed RepVGG block, whose comparison was even missed in the original submission (later added in rebuttal); (2) comparing ParNet with RepVGG, there seems no performance advantage. Although RepVGG is 2.5 times deeper than ParNet, it is still faster due to highly optimized layers.  The authors mainly argued that their contribution is to answer the scientific question “is it possible to build high performing non deep neural networks?” While this is indeed an interesting question, AC feels: (1) it is perhaps unfair for this paper to claim as the first work proving the feasibility. WideResNet provided similar insight much earlier, among others; (2) the presented results, with tools being not novel, are pre mature as they display no real appeal of using ParNet, in any aspect. Probing a new question is of course valuable, but presenting an immature and novelty lacking answer shouldn t automatically grant publication.   In sum, the reviewers were unanimously UN convinced by this paper s value, nor was the AC. The authors are suggested to very seriously take into account reviewers  suggestions to make improvements, before submitting their work to the next venue.
The paper augments pre trained language models by introducing “adapter”, where each adapter is another language model pre trained for a specific knowledge source (e.g., Wikidata) and an objective (e.g., relation classification). The representation from each adapter is concatenated to the representation from the generic LM. Specifically, they introduce two adaptors, “factual” (mostly derived from Wikipedia), and “linguistic” (from dependency parser), and the experiment shows modest improvements over various benchmarks.  This is a borderline paper, as both methods and experiments are reasonable yet not very novel or strong. The clarity of the paper can be improved (as pointed by R1 and R4), without any mathematical notations, model details are to be interpolated from figures. The novelty is limited and experimental rigor can be improved (i.e., for many settings, gains are fairly small and no variance reported). 
This paper shows that various discrete loss functions can be formulated as an LP. It proposes to relax the constraint Ax   b, x >  0 using a soft constraint and following Mangasarian, proposes to solve the relaxed problem using Newton s method. Backpropagation through these iterations is further proposed. The main motivation is that this results in a GPU friendly implementation.  I think the proposed approach is novel. However, as pointed out by reviewers, the current writing lacks clarity and the experiments are quite weak. There is now a wealth of methods for differentiating through an LP using implicit differentiation, smoothing (which the present paper is a form of, see below) and perturbations. It is important to compare to these methods. The paper also ignores a large literature on convex surrogates for ranking metrics.  I recommend the authors to strengthen the writing and experiments, and to resubmit to a top conference.  Additional comments by the AC    As the sentences "Hence, solving such LPs using off the shelf solvers may slow down the training process" or "Often, this would involve running the solver on the CPU, which introduces overhead" indicate, the authors seem to imply that LPs need to be solved in canonical LP form, min_x <c,x> s.t. Ax <  b, x > 0, using an off the shelf LP solver. This is not how many LPs are solved in practice. For every loss, there will always be an ad hoc solver for the corresponding LP. For instance, the Hungarian algorithm for the Birkhoff polytope.  The paper is missing an important reference: SparseMAP (https://arxiv.org/abs/1802.04223). In this paper, the authors add regularization to the primal LP and use Frank Wolfe or active set methods to solve the problem.  Equation (6) corresponds to relaxing the hard constraint Ax b, x> 0 with a soft one. This approach is in a sense opposite to SparseMAP. Indeed, relaxing the constraints in the primal is equivalent to adding regularization in the dual LP (see, e.g., https://papers.nips.cc/paper/2012/hash/bad5f33780c42f2588878a9d07405083 Abstract.html). Speaking of (6), the authors should clarify that it s a convex objective.  In section 2, the authors review a number of losses which can be written as an LP. It would be better to explicitly state what are A, b and c for each loss (or g, h, E, F, p, B, G, q).  The matrix A could potentially be huge, depending on the LP. Do you need to materialize it in memory in practice? This would limit the approach to relatively small LPs.
The paper proposes an interesting setting in which the effect of different optimization parameters on the loss function is analyzed.  The analysis is based on considering cross entropy loss with different softmax parameters, or hinge loss with different margin parameters.  The observations are interesting but ultimately the reviewers felt that the experimental results were not sufficient to warrant publication at ICLR.  The reviews unanimously recommended rejection, and no rebuttal was provided.
This paper received mixed reviews: two positives (6, 6) and two negatives (5, 3). However, the positive reviewers have very low confidence, do not show strong supports for this paper. The reviewers raised various concerns about this paper, and there still exist remaining critical issues although the authors made substantial efforts to answer the questions.  After reading the paper and all the comments by the reviewers, I decided to recommend rejecting this paper mainly due to its weak technical contribution and ignorance of privacy issues. Note that this opinion is shared with two negative reviewers. The proposed model and alternative training scheme are straightforward, and the novelty is not distinct. Also, the authors seem to assume that "the extracted feature vectors and corresponding gradients are not sensitive". This comment is given by R2 but has not been clarified. The proposed method is lacking in this aspect and it is hard to say that it is an FL approach.  
This paper has a mixture of weak reviews, the majority of which lean towards reject. All reviews mention a lack of novelty, and 2 of 3 a lack of support in experiments. While the authors argue, perhaps legitimately, for the novelty of the paper with respect to current literature, this is not convincing in the exposition. I recommend that the authors improve the justification for the novelty of their methodology, and strengthen the experiments to convince reviewers. As it stands, this paper is not quite ready for publication.
This work describes an interesting approach of using a reinforcement learning algorithm for federated learning. The paper is well organized and the use case of performing federated learning while preserving patient privacy is also important. However, the paper has room for improvement. Important baselines used for client selection are missing and so the deep reinforcement learning approach is not well motivated. Many important technical details are missing such as hyperparameters and distributions for MNIST and CIFAR. The approach is also lacking novelty, DRL has been used for neural scheduling before and the authors do not suggest improvements to that. Finally, the experiments showing robustness to backdoor attacks is unconvincing and can benefit from more analysis.
The paper introduces a method for differentially private deep learning, which the authors term Gradient Embedding Perturbation. This is similar to several (roughly) concurrent works, which project gradients to a subspace based on some auxiliary public data. However, a crucial difference involves the use of the residual gradients, which allows the method to achieve the first significant accuracy gains using subspace projection. The reviewers believe this method will be important for the practice of DP deep learning.
The paper proposes BitRand, a bit aware randomized response algorithm, to preserve local differential privacy in federated learning. The main idea is to take into account the bit indices and prioritise higher order bits focussed towards achieving a utility which is higher than other algorithms which are oblivious to the floating point bit representation. Additionally, the analysis allows the bit randomization probabilities not to be affected too much by the dimension of the data.  Overall, the paper lay right at the borderline of acceptance. The paper s core idea, their development and experiments were all liked by the reviewers. The main issue the reviewers brought up was the writing and structuring of the paper and the presentation of the overall results. Most reviewers agreed that the paper presentation was not ready to match the bar for ICLR. There are multiple suggestions that reviewers have made   through their questions and direct comments and addressing those and rewriting the paper to highlight these aspects better will significantly improve the paper.
The paper proposes a modification to the DeepMind Control Suite to measure generalization with respect to visual variation. The authors run baseline experiments against their new benchmark and discover, unsurprisingly, that RL agents learning from visual observations overfit to spurious details of the observations.  Reviewers generally found the work to be clearly written, and the experimental analysis to be thorough and well done, though concerns about the rather simple nature of the visual augmentations persisted even after updates and author rebuttals. There were also concerns that by focusing only on Soft Actor Critic in the experiments.  3 of 4 reviewers felt the work met the acceptance bar, albeit only marginally. The dissenting reviewer s concerns centered on clarity (many specific issues appear to have been remedied), the relatively limited nature of the augmentations, and the fact that reviewers were not given access to the code.   While the submission has potential, improvements needed are not minor, and given the short process, we can only accept papers as is, rather than expecting certain changes. Please take the reviewers  comments into consideration as you revise and resubmit to a future venue.
This paper shows that combining GAN and VAE for video prediction allows to trade off diversity and realism. The paper is well written and the experimentation is careful, as noted by reviewers. However, reviewers agree that this combination is of limited novelty (having been used for images before). Reviewers also note that the empirical performance is not very much stronger than baselines. Overall, the novelty is too slight and the empirical results are not strong enough compared to baselines to justify acceptance based solely on empirical results.
The paper worked on fully unsupervised anomaly detection and proposed to use self supervised representation learning to improve the performance of one class classification. This is a borderline case close to acceptance but cannot make it. Specifically, it is useful, but its novelty is the main issue, since it is not surprising that self supervised representation learning can improve one class classification without representation learning (this part is still much of the taste of ICLR) and an ensemble of multiple models can improve upon a single model (which is just "bootstrap aggregating" or "bagging" used everyday in practice and known to machine learning and statistics societies a very long time ago). After seeing the rebuttal, the concerns were not really addressed well and the issues were only partially solved. Thus, the paper is not enough to guarantee an acceptance to ICLR unfortunately.
The paper studies how adversarial robustness and Bayes optimality relate in a simple gaussian mixture setting. The paper received two recommendations for rejection and one weak accept. One of the central complaints was whether the study had any bearing on "real world" adversarial examples. I think this is a fair concern, given how limited the model appears on the surface, although perhaps the model is a good model of any local "piece" of a decision boundary in a real problem. That said, I do not agree with the strong rejection (1) in most places. The weak reject asked for some experiments. The revision produced these experiments, but I m not sure how convincing these are since only one robust training method was used, and it s not clear that it s the best one could do among SOTA methods. For whatever reason, the reviewers did not update their scores. I am not certain that they reviewed the revision, despite my prodding.
The authors addressed the reviewers concerns but the scores remain somewhat low. The method is not super novel, but it is an incremental improvement over existing approaches.
The authors consider improvements to model based reinforcement learning to improve sample efficiency and computational speed. They propose a method which they claim is simple and elegant and embeds the model in the policy learning step, this allows them to compute analytic gradients through the model which can have lower variance than likelihood ratio gradients. They evaluate their method on Mujoco with limited data.  All of the reviewers found the presentation confusing and below the bar for an acceptable submission. Although the authors tried to explain the algorithm better to the reviewers, they did not find the presentation sufficiently improved. I agree that the paper has substantial room for improvement around clarity. Reviewers also asked that experiments be run for more time steps. I agree that this would be an important addition as many model based reinforcement learning approaches perform worse asymptotically model free approaches and it would be interesting to see how the proposed approach does. A reviewer pointed out that equation 2 is missing a term, and indeed I believe that is true. The authors response is not correct, they likely refer to an equation in SVG where the state is integrated out. Finally, the method does not compare to state of the art model based approaches, claiming that they use ensembles or uncertainty to improve performance. The authors would need to show that adding either of these to their approach attains similar performance to state of the art approaches.  At this time, this paper is below the bar for acceptance.
The paper proposes a new algorithm for adversarial training of language models.  This is an important research area and the paper is well presented, has great empirical results and a novel idea. 
This paper received 4 quality reviews. The reviewers like the problem formulation and various ideas presented to enabling a working pipeline. However, almost all experiments are conducted on synthetic data. Concerns are also raised regarding its usage and application on real world data. In the updated version, the authors add some results on real world data, yet without quantitative evaluation. After the rebuttal and discussion, the final rating is 6 from 3 reviewers, and 5 from 1 reviewer (note that reviewer SKNa stated that the rating will be increased to 6 but did not end up changing it in the "recommendation" entry). The AC sees both pros and cons of this work. Given that a conference paper does not have to be comprehensive in all frontiers and the novel idea presented in this paper, the AC is leaning toward in favoring of this work and thus recommends acceptance.
This paper proposes to use longstanding statistical learning techniques to identify the nationality of the author of a text.  Reviewers agreed that this work is a poor fit for ICLR, as there is nothing here that advances our understanding of representation learning. Reviewers were further concerned about the soundness of the claims, raising issues about data contamination and comparison with prior work.   Finally, reviewers pointed out (correctly in my view) that work that aims to infer protected identity characteristics of non user human subjects should be held to an especially high ethical standard, and needs a highly persuasive cost benefit analysis that defends why the problem is ethical to study at all. The available discussion of ethics is not up to this standard.
In this paper, the authors studied reinforcement learning applications that have access to both online and offline data (with limited online interaction though).  In order to handle the mixture of online and offline data efficiently, the authors proposed a new paradigm called adaptive Q learning, which treats offline and online data differently (as reflected by whether pessimism is implemented or not). The effectiveness of the proposed paradigm has been tested empirically. The reviewers have raised concerns about the sufficiency and significance of the experiments conducted in the paper, and pointed out that the proposed algorithmic idea is a somewhat incremental change over existing ones. The changes the authors promised to make will make the paper stronger.
This paper introduces fairly complex methods for dealing with OOV words in graphs representing source code, and aims to show that these improve over existing methods. The chief and valid concern raised by the reviewers was that the experiments had been changed so as to not allow proper comparison to prior work, or where comparison can be made. It is essential that a new method such as this be properly evaluated against existing benchmarks, under the same experimental conditions as presented in related literature. It seems that while the method is interesting, the empirical section of this paper needs reworking in order to be suitable for publication.
There is growing evidence that optimized deep networks (typically dense in the sense of nonzero parameters) often contain sparse sub networks that can be trained from scratch to achieve similar performance as the full network. Such “skeletonization” is of obvious importance, given the rate at which deep networks in practice are increasing in size. However, many approaches to find such optimal sub networks train the full model (and hence implicitly, the intermediate sub networks as well), which is not a scalable path.   Some recent works show that skeletonization at initialization may provide all the efficiency benefits of sparsity, while minimally impacting accuracy. This work first notes that accuracy in such approaches degrades significantly beyond a certain level of sparsity (around 95%). One of the ideas of this work is to resurrect parameters that were pruned away earlier in this work’s iterative skeletonization, via an approach called foresight connection sensitivity (FORCE) where the “trainability”  of the pruned network is also taken into consideration. An additional idea is “Iterative SNIP”, building on the SNIP approach of Lee et al. (2019). The empirical improvements, and the observations showing the limitations of SNIP and GRASP in the regime of high sparsity, are useful.   The evaluation and overall contribution were generally appreciated by the reviewers.  
The authors agree after reading the rebuttal that attacks on MOT are novel.  While the datasets used are small, and the attacks are generated in digital simulation rather than the physical world, this paper still demonstrates an interesting attack on a realistic system.
This paper presents a new NAS benchmarks for hardware aware NAS. For each of the architectures in the search space of NAS Bench 201, it measures hardware performance (energy cost and latency) for six different hardware devices. This is extremely useful for the NAS research community, since it takes very specialized hardware domain knowledge (including machine learning development frameworks, device compilation, embedded systems, and device measurements) as well as the hardware to make these hardware aware measurements on as many as six (very different) devices.   The code has been made available to the reviewers during the author response window and has been checked by the reviewers in the meantime. All reviewers appreciated the paper and gave (clear) acceptance scores.   Before this work, it was very hard for the average NAS researcher to assess their method properly in a hardware aware setting, and I expect this work to change this, and to open up the very important field of hardware aware NAS to many more researchers. For this reason I recommend to accept this paper as a spotlight.  
This paper was reviewed by 3 expert reviewers. All three recommend rejection citing significant concerns (e.g. missing baselines).
The authors propose a novel way of incorporating a large pretrained language model (BERT) into neural machine translation using an extra attention model for both the NMT encoder and decoder.   The paper presents thorough experimental design, with strong baselines and consistent positive results for supervised, semi supervised and unsupervised experiments. The reviewers all mentioned lack of clarity in the writing and there was significant discussion with the authors. After improvements and clarifications, all reviewers agree that this paper would make a good contribution to ICLR and be of general use to the field. 
Thanks to reviewers and authors for an interesting discussion. It seems the central question is whether learning to identify correct bijections should be part of graph classification problems, or whether this leads to bias and overfitting. Reviews are generally negative, putting this in the lower third of the submissions. The paper, however, inspired an interesting discussion, and I would encourage the authors to continue this line of work, addressing the question of bias and overfitting more directly, possibly going beyond dataset evaluation and, for example, thinking about how to evaluate whether training on non isomorphic graphs leads to better off training set generalization.
The paper proposes  an interesting idea to leave a very simple form for piecewise linear RNN, but separate units in to two types, one of which acts as memory. The "memory" units are penalized towards the linear attractor parameters, i.e. making elements of $A$ close to 1 and off diagonal of $W$ close to $1$.  The benchmarks are presented that confirm the efficiency of the model. The reviewer opinion were mixed; one "1", one "3" and one "6"; the Reviewer1 is far too negative and some of his claims are not very constructive, the "positive" reviewer is very short. Finally, the last reviewer raised a question about the actual quality on the results. This is not addressed. Although there is a motivation for such partial regularization, the main practical question is how many "memory neurons" are needed. I looked through the paper   this addressed only in the supplementary, where the value of $M_{reg}$ is mentioned ( 0.5 M). For $M_{reg}   M$ it is the L2 penalty; what happens if the fraction is 0.1, 0.2, ... and more? A very crucial hyperparameter (and of course, smart selection of it can not be worse than L2RNN). This study is lacking. In my opinion, one can also introduce weights and sparsity constraints on them (in order to detect the number of "memory" neurons more or less automatically). Although I feel this paper has a potential, it is not still ready for publication and could be significantly improved.
This paper introduces a method for classifying corrupted data and quantifying uncertainty by training semi supervised autoencoders only on clean (uncorrupted) data.  Pro:  The approach is novel utilizing metric Gaussian variational inference.  Cons: More thorough experiments are needed:  (1) extensive experiments on more complex data, (2) ablation study, (3) comparison to additional baselines.  Summary:  The paper introduces a novel method, however experiments are limited.
The paper considers the high resolution continuous limit of Nesterov s Accelerated Gradient (NAG) algorithm and its connections to sampling (MCMC methods). The paper develops a Hessian Free High Resolution (HFHR) ODE and injects noise into it to obtain an accelerated sampling algorithm. Further, the paper provides a discrete time variant of the algorithm by appropriately discretizing HFHR using simple discretization schemes. For strongly log concave potential functions (log densities), the paper proves convergence of the order $\tilde{O}(\sqrt{d}/\epsilon)$ in Wasserstein 2 distance. In the asymptotic sense, the result matches the convergence of the underdamped Langevin algorithm; however, the paper argues that the constants in the proposed algorithm are smaller and empirically shows that the proposed algorithm is faster in practice. The main contributions of the paper are theoretical; however, the theoretical results are supplemented by numerical experiments.  Overall, the reviewers found the contributions interesting and the theoretical contributions of the paper technically sound. The main concerns that were not completely addressed were related to the presentation of the results and reproducibility of some of the numerical experiments. While both seem minor and possible to address, ultimately there was not enough support to recommend acceptance. However, the paper is solid and merits acceptance after suitable revisions. Thus, the authors are encouraged to revise the paper and resubmit it to one of the conferences in the equivalence class of ICLR.
Thank you for your submission to ICLR.  While all reviewers felt that there were some interesting aspects to the proposed work, the consensus was also that the work didn t properly situate itself within the existing literature on related methods.  In particular, I agree with Reviewer kLFD that a numerical comparison to Pfaff et al., is notably missing here; while the authors did provide qualitative comparisons in their discussion, it s not clear to me that these differences are ultimately that significant, and the methods need to be compared directly if a case is to be made for the advantages of the proposed approach.
This paper was quite contentious.  While there is clearly promise in the method and the idea, and reviewers appreciate the importance of encoding non trivial prior knowledge into BO, three reviewers express major concerns regarding the presentation (including worries about over claiming contributions), the specification of the probabilistic model as well as to some extent about the experimental evaluation.  Lastly, the title appears to be a kind of pleonasm   arguably, the key point of using a Bayesian prior is to be able to encode prior knowledge.  The authors consider a different form of prior than perhaps usually meant in BO (in a generative rather than discriminative sense), but the terminology is still confusing.  
This paper presents an energy efficient architecture for quantized deep neural networks based on decomposable multiplication using MACs. Although the proposed approach is shown to be somehow effective, two reviewers pointed out that the very similar idea was already proposed in the previous work, BitBlade [1]. As the authors did not submit a rebuttal to defend this critical point, I’d like to recommend rejection. I recommend authors to discuss and clarify the difference from [1] in the future version of the paper.   [1] Sungju Ryu, Hyungjun Kim, Wooseok Yi, Jae Joon Kim. BitBlade: Area and Energy Efficient Precision Scalable Neural Network Accelerator with Bitwise Summation. DAC 2019 
The paper provides a study of the impact of preconditioning/second order methods on generalization by giving a precise analysis in tractable regression settings. It illustrates conditions under which preconditioning might be useful for better generalization.  The readability issues raised by the reviewers have been taken into account, as well as some missing references, except  Wu, D. and Xu, J. "On the Optimal Weighted Regularization in Overparameterized Linear Regression" NeurIPS 2020, raised by reviewer (though it is a really recent reference). Overall the contributions are significant enough to accept the paper for publication.
 The reviewers have  different views on the papers but agreed that the paper can be accepted. However, they suggested some points of improvements including the writing (clarity and style) and experiments showing strong improvements compared to WGAN.
This paper was generally well received by reviewers and was rated as a weak accept by all. The AC recommends acceptance. 
Due to uniformly unfavourable reviews and lack of author engagement in the discussion period, this paper is rejected.
This papers presents a method for solving symbolic mathematic tasks. It first pretrains a transformer model with language translation, and then fine tunes the pretrained model to the downstream mathematic tasks. It contains interesting points but our reviewers have serious concerns which are not fully resolved in the rebuttal. For the integration task, the proposed method achieves good results comparing with Lample & Charleston 2019 (LC) with much less training data. However, as the authors also noted (see the rebuttal), the higher accuracies in LC are achieved with more data. If the authors could also at least show how much data the proposed method needs to achieve the best result in LC, it will be very helpful for understanding the value of this work. In addition, the proposed method did not show similar improvements on the ODE task. So it is hard to see how this proposed method can be generally useful. Our reviewers also have big concerns on writing. Many sentences are really confusing.
This paper proposes to extract a character from a video, manually control the character, and render into the background in real time.  The rendered video can have arbitrary background and capture both the dynamics and appearance of the person. All three reviewers praises the visual quality of the synthesized video and the paper is well written with extensive details. Some concerns are raised. For example, despite an excellent engineering effort, there is few things the reader would scientifically learn from this paper. Additional ablation study on each component would also help the better understanding of the approach. Given the level of efforts, the quality of the results and the reviewers’ comments, the ACs recommend acceptance as a poster.
This paper introduces a simple NAS method based on sampling single paths of the one shot model based on a uniform distribution. Next to the private discussion with reviewers, I read the paper in detail.   During the discussion, first, the reviewer who gave a weak reject upgraded his/her score to a weak accept since all reviewers appreciated the importance of neural architecture search and that the authors  approach is plausible.  Then, however, it surfaced that the main claim of novelty in the paper, namely the uniform sampling of paths with weight sharing, is not novel: Li & Talwalkar already introduced a uniform random sampling of paths with weight sharing in the one shot model in their paper "Random Search and Reproducibility in NAS" (https://arxiv.org/abs/1902.07638), which was on arXiv since February 2019 and has been published at UAI 2019. This was their method "RandomNAS with weight sharing".  The authors actually cite that paper but do not mention RandomNAS with weight sharing. This may be because their paper also has been on arXiv since March 2019 (6 weeks after the one above), and was therefore likely parallel work. Nevertheless, now, 9 months later, the situation has changed, and the authors should at least point out in their paper that they were not the first to introduce RandomNAS with weight sharing during the search, but that they rather study the benefits of that previously introduced method.  The only real novelty in terms of NAS methods that the authors provide is to use a genetic algorithm to select the architecture with the best one shot model performance, rather than random search. This is a relatively minor contribution, discussed literally in a single paragraph in the paper (with missing details about the crossover operator used; please fill these in). Also, this step is very cheap, so one could potentially just run random search longer. Finally, the comparison presented may be unfair: evolution uses a population size of 50, and Figure 2 plots iterations. It is unclear whether each iteration for random search also evaluated 50 samples; if not, then evolution got 50x more samples than random search. The authors should fix this in a new version of the paper.  The paper also appears to make some wrong claims in Section 2. For example, the authors write that gradient based NAS methods like DARTS inherit the one shot weights and fine tune the discretized architectures, but all methods I know of actually retrain from scratch rather than fine tuning. Also, equation (3) is not what DARTS does; that does a bi level optimization. In Section 3, the authors say that their single path strategy corresponds to a dropout rate of 1. I do not think that this is correct, since a dropout rate of 1 drops every connection (and does not leave one remaining). All of these issues should be rectified.  The paper reports good results on ImageNet. Unfortunately, these may well be due to using a better training pipeline than other works, rather than due to a better NAS method (no code is available, so there is no way to verify this). On the other hand, the application to mixed precision quantization is novel and interesting.  AnonReviewer2 asked about the correlation of the one shot performance and the final evaluation performance, and this question was not answered properly by the authors. This question is relevant, because this correlation has been shown to be very low in several works (e.g., Sciuto et al: "Evaluating the search phase of Neural Architecture Search" (https://arxiv.org/abs/1902.08142), on arXiv since February 2019 and a parallel ICLR submission). In those cases, the proposed approach would definitely not work.  The high scores the reviewers gave were based on the understanding that uniform sampling in the one shot model was a novel contribution of this paper. Adjusting for that, the real score is much lower and right at the acceptance threshold. After a discussion with the PCs, due to limited capacity, the recommendation is to reject the current version. I encourage the authors to address the issues identified by the reviewers and in this meta review and to submit to a future venue. 
The reviewers agree the proposed idea is relatively incremental, and the paper itself does not do an exemplary job in other areas to make up for this.
This paper presents a VAE approach where the model learns representation while disentangling the location and appearance information. The reviewers found issues with the experimental evaluation of the paper, and have given many useful feedback. None of the reviewers were willing to change their score during the discussion period. with the current score, the paper does not make the cut for ICLR, and I recommend to reject this paper. 
The paper proposed a new pretrained language model which can take visual information into the embeddings. Experiments showed state of the art results on three downstream tasks. The paper is well written and detailed comparisons with related work are given. There are some concerns about the clarity and novelty raised by the reviewers which is answered in details and I think the paper is acceptable.
The authors propose a communication efficient distributed LAMB optimizer using a 1 bit compression. This work is similar in spirit to other prior work, eg 1 bit Adam. Although the algorithm works reasonably well it is a bit unclear how much compression is achieved. Overall, the algorithmic novelty is limited, given the prior work, and the benefits of the algorithm don t shine through as the experiments are quite limited in their data sets and models. The theoretical results are also of unclear usefulness due to the assumptions made.
State of the art results on Squad (at least at time of submission) with a nice model. Authors have since applied the model to additional tasks (SNLI). Good discussion with reviewers, well written submission and all reviewers suggest acceptance. 
The paper develops a methodology for using graph neural networks for mesh based physics simulation. This extends recent work that focused on grids or particles to mesh based domains, which are challenging due to irregular (and possibly changing) connectivity. The reviewers had some concerns but recognized that this is an important work that will be of broad interest and may have significant impact.
Pros:   an explicitly multi objective approach to neural architecture search   multiple datasets   ablation experiments  Cons:   lack of baselines like hyperparameter search   ill justified increase in capacity after search   ineffective use of the multiple objectives in assessment   not clearly beating random search baseline  The reviewers adjusted their scores upward after the rebuttal, but serious concerns remain, and the consensus is still to (borderline) reject the paper.
This paper studies the “suspended animation limit” of various graph neural networks (GNNs) and provides some theoretical analysis to explain its cause. To overcome the limitation, the authors propose Graph Residual Network (GRESNET) framework to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. The main concern of the reviewers is: the assumption made for theoretical analysis that the fully connected layer is identical mapping is too stringent. The paper does not gather sufficient support from the reviewers to merit acceptance, even after author response and reviewer discussion.  I thus recommend reject.
The paper proposes a novelty detection method when training data is itself noisy. A VAE based approach is developed that promotes robustness of the VAE. The paper assumes that the encoder a two component Gaussian mixture distribution, individual components denoting inliers and outliers.  The paper hopes that the posterior of the inliers (normal data points) can be represented by a low rank covariance matrix, while the outliers need a full covariance. Another notable modification is that the Wasserstein 1 regularization is used to replace the KL regularization in the ELBO, which is claimed to be more suitable to the low rank modeling.   While this is a relevant problem, and the idea is perhaps interesting, some concerns have been raised. * The details how to fit the model with the desired mixture posterior in practice is unclear. * The arguments of section 3 to illustrate the superiority of Wasserstein were found unconvincing, with limiting/unclear assumptions * The ultra low latent space dimension (2) is not sufficiently justified  * The experimental section and the selected datasets are small scale, it would be good to include a free larger scale datasets (at least cifar10). * Comparisons to the open set recognition, or out of distribution (OOD) detection would have been a plus.  Overall, this is an OK paper but not yet of sufficient quality.
This paper proposes a very interesting approach using Wasserstein distance between graph embedded by GNN to perform prediction. The paper is well written and the experiments suggest that the method works  well in practice.  Several reviewers had some concerns about  computational complexity and model parameters that were very well answered during the discussion and with the new version of the paper but it was not enough to convince them to change their overall opinion on the paper.   Note that a lengthy discussion with the reviewers stemming from an unclear Figure 3  was done about the need for a  contrastive regularization that raised important questions that should be addressed. In short, despite the claim by the authors that the contrastive regularization is important the experiments show very little difference in performances with or without the regularization which asks the question of its usefulness. As a matter of fact looking at Figure 3 the regularization will spread the samples in the distributions, making the prototype more similar . The argument about the sample collapse is not good enough because if the sample collapse completely during the optimization the method converges toward prototype L2 which is not the case since the proposed approach is much better than L2 even without regularization. So if there is some collapse it actually serves the discrimination and leads to a better solution. Also the Wasserstsein can reverse the collapsing (the samples  are never exactly at the same position and the gradient can be very different) if it helps for the optimization problem as is well known from the Wasserstein GAN literature.   Due to the remaining concerns of the majority of reviewers, the AC recommends a reject but encourages the authors to resubmit the paper after taking into account the reviewers comments and the questions about the regularization .
Three reviewers have reviewed this paper and they maintain their findings after the rebuttal. The reviewers are mainly concerned about the novelty (several highly related papers exist) and well as the technical contribution (more theoretical developments are needed). Therefore, this paper in its current form cannot be accepted.
This paper attempts to jointly search for the sensor and the neural network architecture. More specifically, the proposed approach jointly optimizes the parameters governing the PhlatCam sensor and the backend CNN model. In terms of the approach, the paper follows a well known DARTS formulation for the differentiable architecture search. A very straightforward solution was proposed for the problem.  Although all the reviewers place that the paper is marginally above the acceptance threshold, none of them strongly support the paper and the reviewers point out that the paper is limited in terms of the setting and data. The problem formulation of the paper itself is interesting, but the AC agrees with the reviewers that the paper is limited and lacks enough technical contributions to warrant the acceptance to ICLR. 
While the reviewers place this manuscript right at the threshold of acceptance, I find the revisions that they have made to address the majority of the reviewers concerns. That, combined with some of the reviewers  scores being slightly miscalibrated with their (largely positive) reaction to the author feedback, I am advocating for this paper to be accepted.
This work proposes a novel reparameterization of batch normalization that is hypothesized to give a better inductive bias for learning several tasks, including neural architecture search, conditional image generation, adversarial robustness and neural style transfer. The reviewers indicate that this is useful and is of interest to the ICLR audience, but they are not satisfied with the analysis offered in the paper. Specifically, the reviewers request that the authors provide a more detailed analysis of why the proposed reparameterization improves results, given that it does not change the expressive power of the model class. Additionally, the reviewers have some concerns about the structure of the paper. I therefore recommend rejecting the paper at this time.
Although the technical novelty is not very high, the finding that long run Langevin dynamics with convergently learned model provides comparable defense performance to adversarial training will give some impact to the community.  
What is investigated is what kind of representations are formed by embodied agents; it is argued that these are different than from non embodied arguments. This is an interesting question related to foundational AI and Alife questions, such as the symbol grounding problem. Unfortunately, the empirical investigations are insufficient. In particular, there is no comparison with a non embodied control condition. The reviewers point this out, and the authors propose a different control condition, which unfortunately is not sufficient to test the hypothesis.  This paper should be rejected in its current form, but the question is interesting and hopefully the authors will do the missing experiments and submit a new version of the paper.
The authors develop a strategy to learn branching strategies for branch and bound based neural network verification algorithms, based on GNNs that imitate strong branching. This allows the authors to obtain significant speedups in branch and bound based neural network verification algorithms relative to strong baselines considered in prior work.  The reviewers were in consensus and the quality of the paper and minor concerns raised in the initial reviews were adequately addressed in the rebuttal phase.   Therefore, I strongly recommend acceptance.
The paper according to Reviewers needs more work for publication and significantly more clarifications. The Reviewers are not convinced on publishing even after intensive discussion that the AC read in full. The AC recommends further improvements on the paper to address better Reviewer s concerns.
The authors propose a well presented approach to likelihood free inference. The reviewers are all in alignment in recommending this paper for acceptance. There was a healthy discussion between authors and reviewers, where the authors have already incorporated many of their recommendations. The potential for this methodology to be applied to situations with expensive simulators should be intriguing to a broad audience. As a result, I recommend for this paper to be accepted as a spotlight.
The reviewers generally like the paper, in particular the scalability of the proposed approach. The author response and revised version clarified some questions of the reviewers, however, it didn t fully mitigate their concerns.
A method is introduced to estimate the hidden state in imperfect information in multiplayer games, in particular Bridge. This is interesting, but the paper falls short in various ways. Several reviewers complained about the readability of the paper, and also about the quality and presentation of the interesting results.   It seems that this paper represents an interesting idea, but is not yet ready for publication.
This paper investigates a technique for projecting contextual embeddings into static embeddings. Neither the technique is ver novel, nor are the empirical results very strong. While the reviewers did not engage in a discussion, the area chair does not see this paper reaching the quality bar of the conference.
Since the reviewers unanimously recommended rejecting this paper, I am also recommending against publication. The paper considers an interesting problem and expresses some interesting modeling ideas. However, I concur with the reviewers that a more extensive and convincing set of experiments would be important to add. Especially important would be more experiments with simple extensions of previous approaches and much simpler models designed to solve one of the tasks directly, even if it is in an ad hoc way. If we assume that we only care about results, we should first make sure these particular benchmarks are difficult (this should not be too hard to establish more convincingly if it is true) and that obvious things to try do not work well.
One of the reviewers pointed out similarity to existing very recent work which would require significant reframing of the current paper. Hence, this work is below the bar at the moment.
This work shows that there exist neural networks that can be certified by interval bound propagation. It provides interesting and surprising theoretical insights, although analysis requires the networks to be impractically large and hence does not directly yield practical advances. 
The paper analyses theoretically the  Matthew effect  (disparate impact) in the setting  of the semi supervised learning and its effect on fairness and performance.   All reviewers agree that the paper deals with a very interesting topic and important problem.  The paper discusses and presents a thorough and convincing analysis of the effect. There were multiple concerns raised mainly around the lack of clarity at parts of the paper. The authors did a very good job at resolving those and bringing their submission to a good standard.  In the rebuttal I was glad to see a great dialog evolving among the authors and reviewers.  I congraultate both sides.   Happy to recommend acceptance.
The reviewers initially assessed this paper as slightly below the acceptance threshold. The reviewers seem to agree on the novelty and potential impact of this project, but they also highlighted the lack of clarity of the manuscript including lack of clarity in the method used to encode the graph data.   As the authors noted, graph related questions were the focus of most of the comments and questions from the reviewers. This is not because the reviewers did not understand and assess the method from the continual learning side (I am also meta reviewing several continual learning papers and I believe that I can assess the novelty of this work). As I wrote above, reviewers were convinced of the paper s motivation.   The authors provided good responses and discussed with at least one reviewer thoroughly. These interactions seem to have clarified important aspects of your proposed methodology and notably the properties of your graph construction method. I found that your new results on larger datasets also provide an improvement. However, to be properly assessed, this number of clarifications regarding the core method requires a new round of reviews. The discussions have also highlighted some of the limits of your approach which do not seem to be acknowledged in your paper. This includes the discussion with reviewer2 regarding constraints on L & K, node classification (also I find that one to less important), and comparison to GraphSage on the non lifelong learning scenario.  Overall, and while I agree that continual learning from graph data is an important and unexplored problem, I also find that the current manuscript lacks clarity and, even though the ICLR discussion allowed reviewers to discuss these with the authors, there are still significant ways to improve the clarity of the current manuscript. As a result, I do not recommend acceptance of the current manuscript.   I strongly suggest the authors keep on working on their manuscript as their idea seems to have potential and I would imagine that it may become one of the first works in a new interesting line of research.
The paper provides a simple approach to explaining GNN predictions for each node by greedily selecting nodes or features in each computation graph so as to increase the fidelity score. The fidelity score is based on comparing the original GNN output to what is obtained with noisy versions of the masked nodes/features. While simple, the approach seems somewhat inefficient (efficiency should be assessed/characterized). Also, several improvements to the evaluation expressed in the reviews/discussion (e.g., human evaluation, practical utility, comparison to gradient based methods) would make the submission somewhat stronger.  
The paper investigates the use of flow models for out of distribution detection. The paper proposes to use a combination of random projections in the latent space of flow models and one sample / two sample statistical tests for detecting OOD inputs. The authors present results on image benchmarks as well as non image benchmarks.   The reviewers found the approach well motivated and appreciated the ablations. The authors did a good job of addressing reviewer concerns during the rebuttal. During the discussion phase, the consensus decision leaned towards acceptance. I recommend accept and encourage the reviewers to address any remaining concerns in the final version.  It might be worth discussing this paper in the related work: Density of States Estimation for Out of Distribution Detection https://arxiv.org/abs/2006.09273
This is a well executed paper that makes clear contributions to the understanding of unrolled iterative optimization and soft thresholding for sparse signal recovery with neural networks.
I thank the authors for their submission and participation in the author response period. The reviewers unanimously agree that the papers proposes an interesting and original approach to using a costly model on a learner node, while distilling to a cheaper model run on actor nodes to gather experiences in a distributed RL framework. During discussion, R1 and myself emphasized the concern that the experiments in this paper leave open the question whether the approach will work beyond toy environments. However, I side with R2 and R3 in that the paper presents a valuable contribution to the community as it stands, and that the experiments proof the concept to the point that the paper should be accepted. I therefore recommend acceptance.
This paper provides an interesting analysis on the research on Domain Generalization with main principles and limitations. The authors provide a strong rebuttal to address some comments pointed by reviewers. All the reviews are very positive. Hence, I recommend acceptance.
This paper provides an investigation into the quality of generations made by multimodal VAEs. All reviewers were in favor of accepting the paper, and there was quite a bit of detailed discussion and clarifications in the revised version of the paper which led two reviewers to raise their ratings. Overall this is an interesting contribution to the area and is an excellent fit for ICLR.
The paper investigates an incremental form of Sliced Inverse Regression (SIR) for supervised dimensionality reduction.  Unfortunately, the experimental evaluation is insufficient as a serious evaluation of the proposed techniques.  More importantly, the paper does not appear to contribute a significant advance over the extensive literature on fast generalized eigenvalue decompositions in machine learning.  No responses were offered to counter such an opinion.
While the reviewers appreciated the ideas presented in the paper and their novelty, there were major concerns raised about the experimental evaluation. Due to the serious doubts that the reviewers raised about the effectiveness of the proposed approach, I do not think that the paper is quite ready for publication at this time, though I would encourage the authors to revise and resubmit the work at the next opportunity.
This paper studies a timely problem and consider an interesting approach, but overall there were many concerns about technical details and the validity of the framework. The positive reviewer also mentioned concerns about the experiments, which others also found to be an insular comparison with weak baselines. Following the response period, in discussion there are additional concerns arising related to the lack of details, for instance related to possible unidentifiability of the model. As one reviewer discusses,  the authors are attempting to use RNNs to impute missing infection status labels when the missingness mechanism is assumed to be (i) not at random, (ii) playing out over time (as it is unclear whether Y^t is assumed (conditionally) independent of Y^t  with t  << t), and (iii) subject to interference (whether someone is tested is the  treatment  here since it s a missingess problem and one person s propensity to be tested could causally affect another person s downstream infection status since apparently no Markov independence is assumed. There is also consensus that the writing quality can be greatly improved. Overall this work contains some ideas with potential in a thorough revision 
The reviewers acknowledge that the paper is well written and contains interesting ideas to combine adaptive control and learning. However, they identified issues regarding the claims about transient tracking and STL formula. Moreover, the significance of the presented learning rule was unclear regarding one reviewer. While the authors could respond well to the identified transient tracking issue, they also needed to weaken their claims, limiting the contribution of the paper. The reviewers therefore stayed with a a reject rating.
In this paper a method for refining the variational approximation is proposed.  The reviewers liked the contribution but a number reservations such as missing reference made the paper drop below the acceptance threshold. The authors are encouraged to modify paper and send to next conference.  Reject. 
This paper studies the problem of optimization for neural networks, by comparing the optimization problem in parameter space with the corresponding problem in function space. It argues that overparametrised models leads to a convex problem formulation leading to global optimality.   All reviewers agreed that this paper lacks mathematical rigor and novelty relative to the current works on overparametrised neural networks. Its arguments need to be substantially reworked before it can be considered for publication, and as a consequence the AC recommends rejection. 
This paper proposes a novel unsupervised task of colour conversion. In this respect, the task becomes more like a regression problem   rather than autoencoding the decoder needs to reconstruct the pixels in a different color system.  While the idea is potentially interesting, there are fundamental problems with the paper:  * The motivations of the paper are obscure, (understanding colour representation in complex visual systems? Learning better representations? Disentangling color related information from the rest) * No analysis is provided to highlight what the novel objective is achieving  The answers of the authors to AnonReviewer1 are not very convincing. As AnonReviewer1 has pointed out, the mapping between the color spaces is typically a simple invertible map so any conclusion that the authors arrive about ‘substantial impact’ could be simply the artefact of the particular architecture choice. The other claim, that ‘the proposed framework is able to encompass additional constraints relevant in understanding why the considered representations could have emerged in the brain’ quite far fetched and speculative at best.  The authors have a point in their reply ii) to AnonReviewer1 but if the claim is about the particular color coding schemata, it would be natural to include simple experiments where some arbitrary 3x3 invertible mapping (e.g. rgb in spherical or cylindrical coordinates) next to other color schemata to make a stronger point.  In point iii) the authors refer to tasks without being very explicit about what the tasks are. Colorization is a known proxy pretext task for learning representations when the downstream classification task is not known a priori. The paper would have been much more easy to motivate if the authors could demonstrate the merit of the proposed objective using a more extensive and careful representation learning evaluation methodology.  In light of the above points, I feel that the paper needs further iterations to be presented at ICLR. 
Theoretical analysis and understanding of DNNs is a crucial area for ML community. This paper studies characteristics of the relu DNNs and makes several important contributions.
 Pros: + Interesting and promising approach to multi domain, multi task learning. + Paper is clearly written.  Cons:   Reads more like a technical report than a research paper: more space should be devoted to explaining the design decisions behind the model and the challenges involved, as this will help others tackle similar problems.  This paper had extensive discussion between the reviewers and authors, and between the reviewers.  In the end, the reviewers want more insight into the architectural choices made, either via ablation studies or via a series of experiments in which tasks or components are added one at a time.  The consensus is that this would give readers a lot more insight into the challenges involved in tackling multiple domains and multiple tasks in a single model and a lot more guidance on how to do it. 
The paper considers the problem of imitating multi modal expert demonstrations using a variational auto encoder to embed demonstrated trajectories into a structured latent space. The problem is important, and the paper is well written. The model is shown to work well on toy examples. However, as pointed out by the reviewers, given that multi modal has been studied before, the approach should have been compared both in theory and in practice to existing methods and baselines (e.g., InfoGAIL). Furthermore, the technical contribution is somewhat limited as it using an existing model on a new application domain.
Two of the reviewers raised their scores during the discussion phase noting that the revised version was clearer and addressed some of their concerns.  As a result, all the reviewers ultimately recommended acceptance.  They particularly enjoyed the insights that the authors shared from their experiments and appreciated that the experiments were quite thorough.  All the reviewers mentioned that the work seemed somewhat incremental, but given the results, insights and empirical evaluation decided that it would still be a valuable contribution to the conference.  One reviewer added feedback about how to improve the writing and clarity of the paper for the camera ready version.
This paper proposes a simple but effective method to obtain ensembles of classifiers (almost) for free.  Essentially you train one network on multiple inputs to predict multiple outputs. The authors show that this leads to surprisingly diverse networks   without a significant increase in parameters   which can be used for ensembling during test time.  Because of its simplicity, I can imagine that this approach could become a standard trick in the "deep learning tool chest".    AC
This submission has been assessed by three reviewers and scored 3/6/1. The reviewers also have not increased their scores after the rebuttal. Two reviewers pointed to poor experimental results that do not fully support what is claimed in contributions and conclusions. Theoretical support for the reconstruction criterion was considered weak. Finally, the paer is pointed to be a special case of (Zhang 2019). While the paper has some merits, all reviewers had a large number of unresolved criticism. Thus, this paper cannot be accepted by ICLR2020. 
Sequence generation models trained via maximum likelihood estimation (or variants of so called  teacher forcing ) condition on *data* samples during training and on *model* samples for predictions. The susceptibility to this potential "mismatch" in input distribution is often referred to as exposure bias (EB).   This paper stresses that most research around EB is focused on addressing it, rather than defining and/or quantifying it. Thus the submission questions the severity of EB and attempts to operationalise a testable definition for it. Myself and all the reviewers strongly support the observations and the agenda, we find the question this paper asks an important one.   Despite our appreciation for this paper s relevance, we have identified a number of problems that prevent me from recommending this paper. I will comment on the two most important points:  1. The  operational definition  of EB in this paper is not sufficiently precise to be testable. It builds on the somewhat commonly accepted view that the effects of EB accumulate as the conditioning context grows longer, and that this causes a model to generate badly distorted sentences. This definition still leaves quite some room for interpretation (without specifying reasonable expectation about how these effects  accumulate  and what/how bad they are, it seems difficult to design tests). We acknowledge that the submission attempts to shed light onto some of these aspects by having some  control groups  using gold data and shuffled strings, but we did not find those sufficient (mostly in light of the next point).  2. MT evaluation metrics (essentially, string similarity metrics), most notably (but not exclusively) BLEU, are used in this work in a setting where we cannot easily grant that they have the discriminating power that the authors expect of them. See this is not a criticism about the imperfections of BLEU (or any other automatic metric), but about the lack of evidence supporting its use against unrelated sentences. We do not find it sufficient that some recent NLG papers have made similar use of it (I, for example, would have criticised those papers on similar grounds).  Overall, we believe this submission asks a relevant question, the insight about dependence on prefix is nice and might lead to a first operational definition of EB (which might be only a few refinements away from the version proposed here). The current evaluation is unconvincing and I believe the authors should be able to find more credible strategies, especially, strategies that have already gone through some scrutiny (for example, in literature around OOD detection and tests for distribution shift).   Though I do not recommend this paper for acceptance, I hope the authors will find valuable feedback in the expert reviews attached.
The paper proposes a mechanism for obtaining diverse policies for solving a task by posing it as a multi agent problem, and incentivizing the agents to be different from each other via maximizing total variation.  The reviewers agreed that this is an interesting idea, but had issues with the placement and exact motivations   precisely what kind of diversity is the work after, why, and what accordingly related approaches does it need to be compared to. Some reviewers also found the technical and exposition clarity to be lacking.  Given the consensus, I recommend rejection at this time, but encourage the authors to take the reviewers  feedback into account and resubmit to another venue.
This paper presents a variant of SARAH, which employs the stochastic recursive gradient and adjustable step size based on local geometry. The main concerns about this paper include (1) the empirical comparison with other algorithms might not be fair (which is arguable); and (2)  the theorem proved in the paper is for a simplified algorithm rather than the algorithm used in the experiments. Even after author response and reviewer discussion, this paper does not gather sufficient support from the reviewers. Thus I recommend rejection.
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
Although all the reviewers find the problem and the approach of using hierarchical models important and interesting, how it has been executed in this submission has not been found favourable by the reviewers.
Authors present an evaluation of end to end training connecting reconstruction network with detection network for lung nodules.  Pros:   Optimizing a mapping jointly with the task may preserve more information that is relevant to the task.  Cons:   Reconstruction network is not "needed" to generate an image   other algorithms exist for reconstructing images from raw data. Therefore, adding the reconstruction network serves to essentially add more parameters to the neural network. As a baseline, authors should compare to a detection only framework with a comparable number of parameters to the end to end system. Since this is not provided, the true benefit of end to end training cannot be assessed.    Performance improvement presented is negligible    Novelty is not clear / significant
This work extends the successor feature framework by focusing on the question of which policies should be learned in order to get the best generalization performance. The reviewers all agree that the question being addressed is interesting and important. One concern raised by two of the reviewers is that the work is rather incremental, providing a relatively small extension from the work of Barreto et al. Nevertheless, the authors have provided a convincing rebuttal, resulting in an increase in score of two of the reviewers. Hence, I recommend acceptance. I do want to ask the authors to carefully read the post rebuttal point mentioned by reviewer 3QcK about clarifying the unsupervised RL setting.
The authors consider the problem of using expert data with unobserved confounders for both imitation and reinforcement learning settings. They showed how latent confounders  negatively affect the learning process and proposed a sampling algorithm that mitigates the  impact and delivers good empirical results.  I agree with the reviewers, this is a borderline paper but with a preference to accept.  The most salient concern was the lack of clear contribution. While the algorithm is interesting  with good experimental results that attract interest, it lacks actual theoretical backbone.   That being said, the authors put in solid effort and addressed concerns sufficiently in the rebuttal stage. Thus I would prefer to see it accepted. The proposed research direction should be explored in the future.
This paper presents a numerical approach to solving the multi body Schrodinger equation.  Three reviews give low confidence scores and the one review with high confidence, and high score, is very brief and the reviewer appears to have a weak background in this area.  My feeling is that the ICLR reviewer pool does not contain reviewers who are really competent to review this paper.  There is a large literature in the physics community on this problem and the paper should be reviewed in an appropriate venue.  This is especially true for evaluating the empirical results. If the mathematical techniques are relevant to general machine learning, and the authors want to have an impact on machine learning community, then it should be possible to give empirical results on a problem commonly used to evaluate machine learning methods at machine learning venues. Whether or not this is important for physics should be judged by physicists.  In any case, the reviews are for the most part not enthusiastic.
This paper is not ready for a publication at ICLR, as agreed unanimously by the reviewers.   There are three main reasons for that: 1. Novelty: it is mentioned in the paper that “To the best of our knowledge, a multi span QA architecture has not been proposed", which is certainly incorrect. See the multiple references provided by the reviewers. 2. Evaluation: there is no evaluation in multi span setting on a public dataset. SQuAD being single span, As stated by R1, "Experiment on Amazon internal data is included, however, as the detailed description or the data statistic is missing, it cannot be considered as academic empirical evaluation." Several public datasets could be used like DROP, Quoref, or Natural Questions. 3. Motivation: the reviewers also note that the clarity and motivation behind the work could be improved. Some choices of the architecture or the model should be more clearly justified.  We encourage the authors to look into the multiple comments in the reviews in order to improve the paper and the research project overall.    
While the authors provided extensive responses to the reviewers and most of the reviewers did a good job of accounting for the author responses the final ratings for this paper was unanimously 5s   all marginally below acceptance. The paper s positioning, writing were identified as key issues that remained to be addressed. The AC recommends rejection.
The authors propose a graph inference learning framework to address the issues of sparse labeled data in graphs. The authors use structural information and node attributes to define a structure relation which is then use to infer unknown labels from known labels. The authors demonstrate the effectiveness of their approach on four benchmark datasets.  The approach presented in the paper is sound and the empirical results are convincing. All reviewers have given a positive rating for this paper. Two reviewers had some initial concerns about the paper but after the rebuttal they have acknowledged the answers given by the authors and adjusted their scores. R1 still has concerns about the motivation of the paper and I request the authors to adequately address this in their final version.
This paper was reviewed by three experts (I assure the authors R3 is indeed familiar with RL and this area). Initially, the reviews were mixed with several concerns raised. After the author response, R2 and R3 recommend rejecting the paper, and R1 is unwilling to defend/champion/support it (not visible to the authors). The AC agrees with the concerns raised (in particular by R2) and finds no basis for overruling this recommendation. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue. 
An interesting analysis of the issue of short horizon bias in meta optimization that highlights a real problem in a number of existing setups. I concur with Reviewer 3 that it would be nice to provide a constructive solution to this issue: if something like K FAC does indeed work well, it would be a great addition to a final version of this paper. Nonetheless, I think the paper would be a interesting addition to ICLR and recommend acceptance.
This paper provides a good finding that maximizing a lower bound of the M H acceptance rate is equivalent to minimizing the symmetric KL divergence between target the proposal. This lower bound is then used to learn sampler for both density and sample based settings. It also nicely connects GAN with MCMC by providing a novel loss function to train the discriminator. Experiment on MNIST dataset in Sec 4.2 shows training the proposal with the symmetric KL is better than variational inference that optimizes KL(q||p).  However, there are a few concerns raised in both the reviews and other comments that should be further clarified. 1. Training an independent proposal may reduce the rate of convergence.  2. In the density based setting experiments, the learnt independent proposal is only used to provide an initial point and a random walk kernel is actually used for sampling. This is different from what is proposed algorithm in Section 3. 3. The proposed algorithm is only compared with VI in density based setting, and there are no comparison with other baselines in the sample based setting, despite the close connections of the proposed method with other models. Stochastic gradient MCMC methods, A NICE MC, GAN will be good baselines for empirical comparisons. Also, the dataset in Sec 4.2 is a subset of the standard MNIST, which makes comparison with other literatures difficult.  For the first concern, the authors provided new experiments for low dimensional synthetic distributions. It is very helpful to show the comparable performance with A NICE MC in this case, but the real challenge in high dimensional distributions remains unexamined. For the second concern, the authors consider the use of random walk as a heuristic that allows to obtain better samples from the posterior, but that significantly changes the proposed transition kernel in Alg. 1.  This paper would be significantly stronger and make a very good contribution to this area by addressing the problems above.
Motivated by the fact that the benefit of overparameterization in unsupervised learning is not well understood than supervised learning, this paper analyzes normalized flow (NF) when the underlying neural network is one hidden layer overparameterized network and proves that for a certain class of NFs, one can efficiently learn any reasonable data distribution under minimal assumptions. The paper is very well motivated. However, the main concerns from the reviewers include (1) the writing quality and presentation are poor, even after revision during the author’s response; and (2) the analysis is limited in the neural tangent kernel (NTK) regime, which makes the results less significant. I agree with the reviewers’ evaluation and I think the first concern can be addressed by a careful revision, while the second concern needs additional nontrivial effort. Thus, I recommend rejection.
In this paper, authors study adversarial robustness against the union of Lp threat models. Reviewers had some concerns about this work. They mentioned the paper is not well organized and the explanations of the novel components should be clearer.  In particular, they suggested authors to study the effects of different components of E AT and motivate their combinations with fine tuning. The lack of novelty was another concern. I suggest authors to focus on the fine tuning part in their revised draft which has more novelty. Given all, I think the paper needs a bit more work before being accepted.
The paper describes a framework for multi agent reinforcement learning that uses Markov Random Fields. Unfortunately, the paper is not clearly written and would benefit from significant revisions that improve its structure and make the model and approximations more explicit.  In particular, the paper says a graph says which agents $i,j$ communicate. This is typically called the "coordination graph" in this setting, see "Collaborative Multiagent Reinforcement Learning by Payoff Propagation", Kok and Vlassis, 2006. Note that within that paper they provide Q function decomposition, which can only serve to approximate the optimal policy.  The authors of this submission claim that an MRF is sufficient for optimal policies. I fail to see how this is true. In particular, Proposition 1 has to be checked more carefully. I tried to go through it, but it did not seem to make sense to me. Why is there an exp() term in the definitoin of the optimal trajectory probability? Why would minimising the KL divergence be enough to obtain an optimal policy? Perhaps it gives an optimal policy within the class of MRF policies, but that s not the same thing as the globally optimal policy.  Overall, I find the lack of clarity and in depth discussion of early related work disturbing, particularly with respect to the theoretical claims in the paper. 
This paper does not seem completely appropriate for ICLR.
The reviews are concerned about the novelty/incremental nature of the paper and partially also about the  conclusions drawn from the experiments. The authors did not take the chance to write a response.
This paper presents a way to use a translation memory (TM) to improve neural machine translation.  Basically the proposed model uses a n gram retrieve matching sentence （or pieces） and takes advantage of the useful parts using gated attention and copying mechanism.  Although the idea of leveraging TM in the context of NMT is not new,  this work seems to be a fair contribution. My major concerns are the following 1. The retrieval part  is not clearly presented, raising questions about  complexities and the noise brought by the common words. The authors should give a better exposition on the ranking mechanism.  2. The experiments are not convincing enough since the proposed model is not compared to the SOTA and the competitive models described in the prior work.  In conclusion I would suggest to reject this paper.
The paper introduces a new method to probe contextualized word embeddings for syntax and sentiment properties using hyperbolic geometry. The paper is written well and relevant to the ICLR community. Reviewers highlight that the proposed Poincaré probe offers solid results, extensive experiments that support the benefits of the approach, and proposes a new approach to analyze the geometry of BERT models. The revised version clarified various concerns of the initial reviews and improved the manuscript (comparison to Euclidean probes, low dimensional examples, new results on edge length distributions etc.). Overall, the paper makes valuable contributions to probing contextualized word embeddings and the majority of reviewers and the AC support acceptance for its contributions. Please revise your paper to take feedback from reviewers after rebuttal into account (especially to further improve clarity and discussion of the method).
All five reviewers unanimously agree that the paper needs to be rejected. One of the main concerns is the lack of technical novelty/originality. The reviewers also point out lacking citation and comparison to prior work, and missing experiments. The authors have not provided any rebuttal.This paper describes an approach for zero shot detection of seen and unseen objects in scenarios. All five reviewers unanimously agree that the paper needs to be rejected. One of the main concerns is the lack of technical novelty/originality. The reviewers also point out lacking citation and comparison to prior work, and underwhelming experiments. The authors have not provided any rebuttal.  We recommend rejecting the paper.
The authors challenge the idea that good representation in RL lead are sufficient for learning good policies with an interesting negative result   they show that there exist MDPs which require an exponential number of samples to learn a near optimal policy even if a good but not perfect representation is given to the agent for both value based and policy based learning.  Reviewers had some minor technical questions which were clarified sufficiently by the authors, leading to a consensus of the contribution and quality of this work.  Thus, I recommend this paper for acceptance.
This paper proposes a RNA structure prediction algorithm based on an unrolled inference algorithm. The proposed approach overcomes limitations of previous methods, such as dynamic programming (which does not work for molecular configurations that do not factorize), or energy based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima).  Reviewers agreed that the method presented here is novel on this application domain, has excellent empirical evaluation setup with strong numerical results, and has the potential to be of interest to the wider deep learning community. The AC shares these views and recommends an enthusiastic acceptance. 
This paper presents an ensemble method for reinforcement learning.  The method trains an ensemble of transition and reward models.  Each element of this ensemble has a different view of the data (for example, ablated observation pixels) and a different latent space for its models.  A single (collective) policy is then trained, by learning from trajectories generated from each of the models in the ensemble.  The collective policy makes direct use of the latent spaces and models in the ensemble by means of a translator that maps one latent space into all the other latent spaces, and an aggregator that combines all the model outputs.  The method is evaluated on the CarRacing and VizDoom environments.    The reviewers raised several concerns about the paper. The evaluations were not convincing with artificially weak baselines and only worked well in one of the two tested environments (reviewer 2). The paper does not adequately connect to related work on model based RL (reviewer 1 and 2). The paper does not motivate its artificial setting (reviewer 2 and 1).  The paper s presentation lacks clarity from using non standard terminology and notation without adequate explanation (reviewer 1 and 3).  Technical aspects of the translator component were also unclear to multiple reviewers (reviewers 1, 2 and 3).  The authors found the review comments to be helpful for future work, but provided no additional clarifications.  The paper is not ready for publication.
The paper introduces GANGSTR, an agent that performs goal directed exploration both individually and "socially", with suggestions from a partner. It builds a graph of different configurations of a 5 block manipulation domain, and navigates this graph. The theoretical motivations for this algorithm are solid, and the direction is interesting. However, the results are less than convincing. In particular, as was mentioned in the discussion, it is not clear how this algorithm would generalize beyond the very simple 5 block manipulation domain. While having a simple benchmark has the advantage that you can explore it in depth, it also might obscure problems with the algorithm, unless complemented by a set of other benchmarks. It therefore seems that the paper is not ready for publication yet.
The paper provides a generalization error bound, which extends the results from PU learning, for the problem of knowledge graph completion. The authors assume a missing at random setting, and provide bounds on the triples (two nodes and an edge) that could be mistakes. Then the paper provides a maximum likelihood interpretation, as well as relations to existing knowledge graph completion methods. The problem setting is interesting, and the writing clear.  This discussion was extensive, with reviewers and authors following the spirit of ICLR and having a constructive discussion which resulted in improvements to the paper. However, there seems to be still some remaining improvements to be made in terms of clarity of presentation, as well as precision of the theoretical arguments.  Unfortunately, there are many strong submissions, and the paper as it currently stands does not satisfy the quality threshold of ICLR.
The manuscript presents a training method for Spiking Neural Networks (SNN). The method jointly optimizes input spike encoding parameters, spiking neuron parameters (membrane leak and voltage threshold), and weights in an end to end fashion using gradient descent. SNNs are very interesting for energy efficient implementations of neural networks. Their energy efficiency strongly depends on inference latency (SNNs compute in time, unlike feed forward ANNs) and activation sparsity.   All reviewers acknowledged that the approach directly improves inference latency and activation sparsity on large convolutional models at very good performance levels.  The main concern of all reviewers was the limited conceptual novelty. The paper combines some known techniques (hybrid SNN training, direct input encoding, training of neuron parameters like leak time constant and threshold) and scale the setup up to large networks and datasets (e.g. ImageNet).  In summary, the paper presents impressive results, but the conceptual innovation is missing.    
The authors have done a very thorough job of responding to the comments from reviewers. The paper has a clear contribution, namely that attention maps predict contacts as well as existing unsupervised pipelines. This paper deserves to be published.  In the final version, the authors should discuss briefly "BERTology Meets Biology: Interpreting Attention in Protein Language Models"(https://openreview.net/forum?id YWtLZvLmud7) and "Improving Generalizability of Protein Sequence Models via Data Augmentations" (https://openreview.net/forum?id Kkw3shxszSd). However, the authors should also make sure that the final version respects the ICLR length limits.  I am recommending poster acceptance because the results are anticlimactic given the recent success of Deepmind at CASP 2020.  
This paper attempts to answer its suggestive title by arguing that this generic lack of invariance in large CNN architectures is due to aliasing introduced during the downsampling stages.  This paper received mixed reviews. Positive aspects include the clarity and exhaustive empirical setups, whereas negative aspects focused on the lack of substance behind some of the claims. Ultimately, the AC took these considerations into account and made his/her own assessment, summarized here.  The main claim of this paper implies the following: modern CNNs are unable to build invariance to small shifts, but somehow are able to learn far more complex invariances involving lighting, pose, texture, etc. This must be empirically verified beyond reasonable doubt, and the AC thinks that the current experimental setup does not achieve this threshold. As mentioned by reviewers and by public comments, the preprocessing pipeline is a key factor that may be confounding the analysis, and this should be better analysed. For example, as mentioned in the reviews below, the shift in the image can be either done by inpainting, cropping, or using a fixed background. The authors claim that there are no qualitative differences between those preprocessing choices, but by inspecting Figures 2B and 8C, the AC notices a severe change in  jaggedness ; in other words, the choice of preprocessing *does* affect the quantitative measures of (un)stability, even though the qualitative assessment (unstable in all setups) is the same. In particular, using non centered crops should be the default setup, since it requires no preprocessing. It is confusing that it appears in the appendix instead of the inpainting version of figure 2b. This is important, since it implies that the analysis is mixing two perturbations: the actual action of the translation group and the choice of preprocessing, and that the latter is by no means negligible. I would suggest the authors to perform the following experiment to disentangle the effect of translation by the effect of preprocessing. Since the translation forms a group, for any shift applied to the image, one can  undo  it by applying the inverse shift. Say one applies a shift to image x of d pixels and obtains x  T(x,+d) as a result (by using whatever border handling procedure). If border effects were negligible, then x  T(x , d) should give us back x, so a good measure of how unstable the network is is to measure the difference in prediction between x,x  and x . If predicting x  is as unstable as predicting x , it follows that the network is actually unstable to the border effect introduced by T.   Given this, the AC recommends rejection at this time, and encourages the authors to resubmit their work by addressing the above point. 
This paper is about learning the output noise variance of a VAE and its effect on the generated image quality as measured by FID. The paper argues that the output variance parameter plays an important role and proposes a simple procedure, where a maximum likelihood estimate of the noise variance is estimated. Experiments on some standard datasets are provided. Overall, the paper is well written and has been perceived positively by the reviewers. However, the effect of observation variance has been in detail analysed by earlier work, in particular Dai and Wipf 2019. The novelty of the current paper is somewhat limited in scope. The paper is somewhat borderline in these respect; a much stronger experimental section would have been helpful.  One key contribution of the work is empirical comparison of alternative parametrizations of the output noise. Overall, the paper would be stronger if this aspect is analysed more in detail, possibly with careful comparisons with competing methods. Inclusion of controlled experiments (e.g. by adding extra noise to data) to show how precise the noise variance estimation and how the procedure influences the convergence of other parameters would have made the paper much more impactful. 
This to me looks like quality work not yet adequately developed, and thus is borderline work.  The authors seem to have achieved a good result:  equalling SotA SEAL (although, one reviewer did preliminary experiments and could not match this) with a sophisticated algorithm using a variety of Bayesian tricks, a more scalable algorithm, and one potentially adapted to further tasks.  However, not all of these impressive feats are adequately demonstrated in this paper, though many had parts included in the rewrite.  So I d say the paper needs a rewrite and more focussed experimental work to broaden the presentation of empirial performance, for instance to node classification. I certaintly appreciated the use of IBP and Dirichlet models within the system, so would love to see the work further developed. The reviewers agreed in several aspects:  (1) more experimental work, for instance on better and larger benchmark data, (2) better presentation and discussion of the theory, (3) better discussion of the motivation for the model (as per reviewer D8S8), and oftentimes linked to the ablation study to support this, which you have done some of (4) additional connections to recent related work in graph representation learning on link prediction works The authors have done a good job or addressing many of the reviewers concerns, ultimately lifting the paper from Reject to Borderline Negative, but I think more work is needed.
This paper proposes to incorporate additional prior knowledge into transformer architectures for machine translation tasks. The definition of problem is reasonable,  despite the fact that there is a long thread of work on adding knowledge of different types into neural architectures of NMT. The proposed model, however, needs to be better motivated, as to why the same thing cannot be done in a simpler way in the framework of transformers.  Judging from the exposition and the experiments, the proposed model is neither novel or empirically significant enough. The writing needs to be greatly improved to get rid of the grammatical errors and notational inconsistency.    I’d suggest to reject this paper  
There were concerns that the paper has a fairly limited novelty, being based on the combination of two known ideas: bucketing and 2 party secure median for distributed learning. Also, the scale of experiments is quite limited. Other issues include the lack of comparison to relevant related work, some doubts on correctness, and issues with independence and scalability that weren t fully resolved. Overall the reviewers felt that the paper shoud not be accepted in its current form.
This paper surveys a collection of existing works that the author frames as evidential deep learning.  While the paper has been recognized as a nicely written survey, all reviewers have raised the major concern that the paper does not have a sufficient academic contribution compared to the surveyed papers. In particular, novelty appears to be limited as the paper does not offer novel views into the surveyed subfield.  Given the strong consensus among reviewers, I recommend rejecting this paper.
The reviewers were fairly consistent in agreeing that this is a reasonable paper with an interesting idea.  However, the use case is fairly narrow, as the main benefit is less intermediate storage (and only significant for very rectangular matrices) but compared to alternatives it require many passes over the data (usually 5 or so). So it s a narrow use case and many of the comparisons are not apples to apples since the accuracy, time, space complexity and number of passes differ from algorithm to algorithm.  So while acknowledging the potential benefits of the method, there are downsides too, and thus a clear presentation is very essential. The reviewers mention that presentation (listing the algorithm, clear experiments) could be improved.  On my own reading, I noted that the choice of SketchySVD as the dominant baseline is misleading. SketchySVD is for streaming data (more restrictive than single pass) so this is an unfair comparison. The appendix does a better job of including other baselines (block Lanczos), though it mischaracterizes them (it says "BlockLanczos requires persistence presence of the data matrix X in memory", but this is not true, the method could easily be implemented in a matrix free fashion). Another method to compare with is the single pass algorithm randSVD in Yu et al., who show how to implement one of the Halko et al. 2011 2 pass methods in just one pass.  Other reviewers mention baseline algorithm issues too.  I do acknowledge the improved accuracy of your method over all these baselines for some matrices, in terms of the Frobenius norm (or tail error); however, I m not sure the differences in spectral norm are are great, and see Remark 2.1 in Martinsson and Tropp  20 for arguments about why Frobenius norm guarantees are often not as desirable as spectral norm guarantees.  Another issue is related to the left vs right singular vectors. A reviewer noted: "It is not fair to compare RangeNet with SketchSVD, RangeNet just produces the right singular vectors while SketchSVD produces both left and right singular vectors." The authors respond "Range Net computes both left and right singular vectors but does not consume main memory to store left singular vectors at run time". However, if we allow another pass over the matrix to find the left singular vectors, this post processing can be applied to *any* technique that approximates the singular values and right singular vectors, hence PCA methods are applicable, including deterministic methods like the "Frequent Directions" method (Ghashami et al.  16).    In summary, this method is high accuracy and low memory, yet it also has downsides compared to other methods, and the paper could use some improvement.  I don t think the paper is ready at this time for acceptance, but given the advantages of the method, I encourage the authors to make changes and resubmit an improved version to ICLR next year or other similar venue.   References:  Yu, Gu, Li, Liu, Li, "Single Pass PCA of Large High Dimensional Data". IJCAI  17, https://doi.org/10.24963/ijcai.2017/468  Ghashami, Liberty, Phillips, Woodruff, "Frequent directions: Simple and deterministic matrix sketching". SIAM Journal on Computing. 2016;45(5):1762 92.  Martinsson, Tropp. "Randomized numerical linear algebra: Foundations and algorithms". Acta Numerica. 2020 May;29:403 572.
The paper presents a method for intrinsically motivated exploration using successor features by interleaving the exploration task with intrinsic rewards and extrinsic task original external rewards. In addition, the paper proposes "successor feature control" (distance between consecutive successor features) as an intrinsic reward. The proposed method is interesting and it can potentially address the limitation of existing exploration methods based on intrinsic motivation. In experimental results, the method is evaluated on navigation tasks using Vizdoom and DeepMind Lab, as well as continuous control tasks of Cartpole in the DeepMind control suite, with promising results.   On the negative side, there are some domain specific properties (e.g., moderate map size with relatively simple structures, different rooms having visually distinct patterns, bottleneck states generally leading to better rewards, etc.) that make the proposed method work well. In addition, off policy learning of the successor features could be a potential technical issue. Finally, the proposed method is not evaluated against stronger baselines on harder exploration tasks (such as Atari Montezuma s revenge, etc.), thus the addition of such results would make the paper more convincing. In the current form, the paper seems to need more work to be acceptable for ICLR.
The authors study empirically and theoretically the behavior of neural networks under $l_\infty$ perturbations on the weight matrix. For this purpose they first derive bounds on the logit layer of the neural networks under perturbuations of a single or all layers. Then they propose to merge this bound (which depends on the product of the weight matrices) into a margin based loss function/cross entropy loss and suggest to optimize this bound while simultaneously penalizing the 1,infty norm of the weight matrices (which appears in the bound). Furthermore they derive a generalization bound for the robust error under weight perturbations.  There was a discussion among the reviewers over this paper. While several ones appreciated the setting, there was concern about that the bound is potentially vacuous and that the theoretical results are "messy". One reviewer criticized heavily the bound as not very useful and overly pessimistic.  This paper is in my point of view borderline but I argue for rejection. There are several reasons for this   the motivation for this paper remains unclear. While the authors argue that a network could be attacked by changing logical values, this seems at the moment unrealistic as if the attacker has already hacked into the system much more harm can be caused in a much easier way e.g. by directly changing the output of the network. But even if one considers adversarial bit error attacks to be realistic, then the threat model would be completely different from $l_\infty$ and would rather be like $l_0$ (with potential unbounded changes if the network is not quantized). If the target is to study that more flat minima generalize better, then the target would not be a bound on the robust error but a bound on the normal test error which integrates an upper bound which measures "flatness" of the function. As the derived Theorem 4 contains a term where one has to take the supremum over the product of matrices over all functions in the derived function class, this is not true for the derived bound. Thus I don t see why this bound is related to either of these two motivating topics.    the bound is incomplete in the sense that it contains the sup_f of the product of 1,infty norms of the weight matrices. Why did the authors not try to upper bound this term over the chosen function class? Even better would clearly to derive a bound on the Rademacher complexity in terms of the norms which are actually appearing in the bound. In the current way the terms in the bound and the chosen function class are mis aligned.    From a practical perspective the resulting loss is not useful for training deeper models (in the experiments a four layer network is used) as the product of the norm of the weight matrices grows exponentially in depth. Moreover, as pointed out the IBP bounds of Weng et al (2020) are much tighter than the bounds derived in this paper (and even IBP bounds are loose)     The experiments suggests that one gets a minor improvement in robustness while having to suffer from a significant drop in test accuracy (Figure 1b). Regarding the achieved robustness for epsilon 0.01 (it remains completely unclear how this noise model relates to the normal size of the model weights) one gets a robust error of around 30% on MNIST. This is much worse than what has been achieved for MNIST with much larger input perturbations (epsilon 0.3)    The authors are missing an assumption on the activation function. With just non negativity, monotonicity and 1 Lipschitz the upper bound in A.2.1 (original version) cannot be derived. I guess you are implicitly assuming that rho(0) 0 but this assumption is not stated. There are other typos in the main text. In fact in the original version Theorem 4 did not contain the term depending on b_h and s_j   this term was added in the revised version but not highlighted as all other changes.  In total there are too many open issues here. While I appreciate the hard work the authors put into the author rebuttal, I think that this paper needs a major revision before it can be published.  
The submission proposes a novel conditional GAN formulation where continuous scalars (named regression labels) are fed into the GAN as a conditioning variable. Since cGANs with discrete labels are trained to minimize the empirical loss, they fail for continuous conditions, because there might be few or even zero samples for many labels values and also the label cannot be embedded by one hot encoding like discrete labels. As a solution, the authors propose new methods of encoding the label.   The paper received a clear accept, two weak accepts and a weak reject. As agreed by all the reviewers, the paper proposes an interesting framework to eliminate some weaknesses of GANs. The rebuttal adequately addresses the reviewer comments and hence the meta reviewer recommends acceptance. 
All three reviewers viewed this paper as marginally above the acceptance threshold (6).  Most of the initial concerns of reviewers were around (a) the applicability of the theory to actual practical use cases and networks, and (b) the presentation and framing of the work, and scope of its results. There were fairly detailed responses from the authors: two of the three reviewers increased their scores after the author response. There s still some lingering questions as to how "real world" relevant the theory is, but the consensus at this point is to accept the paper.  My primary concern for acceptance would be that the proofs techniques are based on Boolean circuits, and none of the reviewers (nor the AC) are particularly familiar with this, and thus the proofs in the appendix have been only lightly reviewed. The "impression" of all reviewers is of correctness.
The paper proposes two new generalized additive models (GAM) based on neural networks and referred to as NODE GAM and NODE GA2M. An empirical analysis shows that the proposed and carefully designed architectures perform comparably to several baselines on medium sized datasets while outperforming them on larger datasets. Moreover, it is shown that the differentiability of the proposed models allows them to benefit from self supervised learning.   Reviewers agreed on the technical significance and novelty of the proposed models and valued the clever design of the new architectures. Most concerns and open questions could be answered in the rebuttal and by changes in the revised manuscript. Based one the suggestions of one reviewer new experiments comparing the proposed models to NAM were added, which improved the paper further.  The paper should be accepted.
While there was some support for the ideas presented in this paper, it was on the borderline, and ultimately did not make the cut for publication at ICLR.  Concerns were raised as to the significance of the contribution, beyond that of past work.
The paper presents a generic way to add group sparsity based regularizers to a family of adaptive optimizers leading to generalizations of many popular optimizers ADAM, ADAGRAD etc  to their group versions. Overall the reviewers appreciated the algorithmic contribution and its genericness in terms of application to most known adaptive optimizers. While the paper s revision during the rebuttal phase satisfied some reviewer concerns regarding the experimental baselines and the precise experimental methodology, reviewers continued to have concerns regarding the experiments performed   the potential lack of fine tuning post pruning, the use of s_t tilde as opposed to s_t in the practical algorithms amongst others listed in the review. Overall, the reviewers deemed the theoretical contribution of the paper not significant enough in terms of novelty and the decision hinged on the efficacy of the experimental evaluation   the lingering concerns for which led to the decision.  
This work proposes an efficient method for modelling long range connections in point cloud data. Reviewers found the paper to be generally well written. On the less positive side, reviewers felt that the novelty of the work was marginal, and that the experimentation, limited to synthetic data in one domain, was too limited. These concerns remain after the discussion phase. In addition, the authors stated during the discussion that "Our goal is indeed to develop an efficient strategy to model LRIs in real chemical and materials systems”, which conflicts with the presentation of the work as motivated by more general point cloud modelling problems. Given these weaknesses, the final decision was to reject.
The focus of the submission is blind source separation (BSS). The authors propose a log linear model based formulation to tackle the task and to relax assumption/restrictions (linear mixing, non convex objective, ...) present in previous techniques. They use the maximum likelihood approach [Eq. (3)] with natural gradient descent for optimization, and illustrate the efficiency of the approach in two toy examples (separation of mixed images and that of sin/sign/sawtooth signals).  BSS is an important task in machine learning with various applications. As assessed by the reviewers, however, the submission is in a quite preliminary stage: (i) Section 1 is rather long, still it lacks providing relevant context to the work. (ii) The introduction of the main ideas/motivation, the assumptions imposed, and the explanation of the notations are missing. (iii) The usefulness of the proposed approach is questionable; the demos focus on artificial toy examples. More work and significant revision are needed before publication.
The paper received unanimous accept over reviewers (7,7,6), hence proposed as definite accept. 
The paper addresses the training time of CNNs, in the common setting where a CNN is trained on one domain and then used to extract features for another domain.  The paper proposes to speed up the CNN training step via a particular proposed training schedule with a reduced number of epochs.  Training time of the pre trained CNN is not a huge concern, since this is only done once, but optimizing training schedules is a valid and interesting topic of study.   However, the approach here does not seem novel; it is typical to adjust training schedules according to the desired tradeoff between training time and performance.  The experimental validation is also thin, and the writing needs improvement.
Strengths: The work proposes a novel architecture for graph to sequence learning. The paper shows improved performance on synthetic transduction tasks and for graph to text generation.   Weaknesses: Multiple reviewers felt that the experiments were insufficient to evaluate the novel aspects of the submission relative to prior work.  Newer experiments with the proposed aggregation strategy and a different graph representation were not as promising with respect to simple baselines.   Points of contention: The discussion with the authors and one of the reviewers was particular contentious. The title of the paper & sentences within the paper such as "We propose a new attention based neural networks paradigm to elegantly address graph  to sequence learning problems" caused significant contention, as this was perceived to discount the importance of prior work on graph to sequence problems which led to a perception of the paper "overclaiming" novelty.  Consensus: Consensus was not reached, but both the reviewer with the lowest score and one of the reviewers giving a 6 came to the consensus that the experimental evaluation does not yet evaluate the novel aspects of the submission thoroughly enough.  Due to the aggregate score, factors discussed above (and others) the AC recommends rejection; however, this work shows promise and additional experimental work should allow a new set of reviewers to better understand the behaviour and utility of the proposed method.  
The paper studies Knowledge Distillation (KD) to better understand the reasons behind the performance gap between student and teacher models. The analysis is done by conducting exploratory experiments. The paper establishes that the distillation data used for training a student can play a critical role in the performance gap apart from the model capacity. Building on this idea, the authors propose a new approach to distillation, KD+, utilizing out of distribution data when training a student. Extensive experiments are performed to demonstrate the efficiency of KD+. Overall, the paper studies an interesting problem. The results provide a more in depth explanation of how the distillation data and model capacity play a role in the performance gap between student and teacher models in KD.  I want to thank the authors for providing the rebuttal and sharing their concerns about the quality of one of the reviews.  The reviewers appreciated the paper s ideas; however, all the reviewers were on the fence with borderline scores. In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should incorporate the reviewers  feedback to better position the work w.r.t. the existing literature and provide clear reasoning behind the gains for KD+ in experiments. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers  feedback when preparing future revisions of the paper.
One of the major challenges in model based RL is the learning of accurate world models. This work proposes a method that can learn to decompose the dynamics of the world into several sub dynamics, which is postulated to lead to better prediction accuracy (which in turn should lead to better policies/downstream performance). The proposed method clusters the sub dynamics in latent space, and can be combined with any existing MBRL method. Here it is specifically combined with Dreamer and MBPO and evaluated on the deepmind control suite/mujoco.  **Strengths** This work addresses an important problem, and on a high level the problem/approach is well motivated The proposed algorithm is "simple" (which can be a really good thing) and very general in that it can be combined with seemingly any MBRL method  **Weaknesses** The manuscript was lacking in clarity on a technical level (partially addressed during rebuttal) On average the experimental results are not very convincing (yet)  **Rebuttal** The authors clarified a few misunderstandings the reviewers and also updated the manuscript accordingly  **Summary** I agree with the reviewers that the experimental results are not fully convincing. When looking at the model error plots, the y scale is very small, and it looks like there is no significant improvement. Also in the "downstream tasks" only for the humanoid do we seem to see a significant improvement. Overall this seems promising, but the authors should investigate why there seems to be a clearer benefit for the humanoid, and show more such results. Furthermore, there are some concerns/clarity issues with respect to what your approach learns   I would recommend you take a small ish toy example to introduce the intuition of your approach, and maybe visualize learned sub dynamics. Overall, while promising, in it s current state this manuscript is not quite ready yet for publication
I thank the authors for their submission and very active participation in the author response period. The paper is well motivated, clearly written and demonstrates empirical gains. In discussions, R4 and R5 were championing the paper. R1 stated that the paper improved, but insists that claims of state of the art, given the simplifications induced by a deterministic simulator and hard valid action constraint, are not justified without additional baselines to compare to. The authors have toned down state of the art claims in the revised version of the paper. I agree with R4 that the added requirements in Table 1 sufficiently explain the constraints under which MC LAVE can be applied. Given the strong positive sentiment by R4 and R5, the positive assessment by R3, and the detailed response by the authors, I am recommending acceptance of the paper.
The paper provides new insights about how to identify latent variable distributions, making explicit assumptions about invariances. A lot of this is studied in the literature of non linear ICA, although the emphasis here is on dropping the "I". I think more could be said about how allowing for dependencies among latents truly change the nature of the problem since any distribution can be built out of independent latents, by some more explicit contrast against the recent references given by the reviewers. In any case, the role of allowing for dependencies in the context of the invariances adopted is discussed, and despite no experimentation, the theoretical results are of general interest to the ICLR community and a worthwhile contribution to be discussed among researchers in this field.
The AC summarizes the major strengths and weaknesses of the paper pointed out by the reviewers (with possible omissions, and additions by the AC)  Strengths: 1. The paper makes an important observation that the linear MDP assumptions can be met when the true dynamics has additive noise  2. Inspired by the theory, the paper proposes a new algorithm that empirical outperforms SAC. The success of the algorithm is very interesting (and surprising to some degree.)   Weaknesses 3. Most of the reviewers and the AC thinks the representation learning perspective is questionable. If one strongly believes that the $\phi, \mu$ in the linear MDP assumption should be interpreted as representations, then yes, this paper is about representation learning in RL and the representation learning is a free lunch. However, suppose one ignores the linear MDP perspective for the moment, and only looks at the modeling assumption $s    f^*(s,a) +\epsilon$, then $f^*$ can only be interpreted as a "dynamics model" and has nothing to do with the term "representation" that is commonly used in practice. (representation means the penultimate layer of the neural nets typically in emprical RL.)  Moreover, in the theory part of the paper, the dynamics model is learned via a (standard) model based approach fitting $f(s,a)$ to $s $ which also suggests that $f$ should be interpreted as a dynamics model instead of a representation. How to reconcile these two perspectives? The AC s own opinion is that this suggests we shouldn t blindly call the $\phi$ in the linear MDP formulation a representation in all scenarios. But regardless of AC s own opinion, I suspect that the paper needs to very explicitly discuss and clarify these discrepancies (instead of somewhat sweeping it under the rug and claiming the paper is about representation learning without a stronger justification.)   4. The sample efficiency depends on the Eluder dimension, which is only known to be polynomial for linear models. Recent works have shown that the Eluder dimension for even simple nonlinear models can be exponential. The analysis seems to be also quite related to previous analysis that uses the Eluder dimension. I think this fact limits the theoretical contribution of the paper.   5. There could be a better exposition of the empirical implementation in the paper. It appears that the implemented algorithm still has some major differences from the theoretical algorithms.   6. It s unclear if the paper should only compare with model free algorithms. At least the theoretical algorithm fits $f(s,a)$ to $s $ explicitly (in the definition of confidence region). Therefore, it does not seem to be quite fair to compare with model free algorithms.   Given these considerations, and given that the majority of the reviewers express some concerns about various subsets of these concerns (3 6), the AC would recommend the authors revise the paper and resubmit to another top ML venue. The AC thinks that the paper contains really interesting and novel observations, but the interpretation of the observation might require more thoughts and clarification.
This paper proposes to use an ensemble of VAEs to learn better disentangled representations by aligning their representations through additional losses. This training method is based on recent work by Rolinek et al (2019) and Duan et al (2020), which suggests that VAEs tend to approximate PCA like behaviour when they are trained to disentangle. The method is well justified from the theoretical perspective, and the quantitative results are good. Saying this, the reviewers raised concerns about the qualitative nature of the learnt representations, which do not look as disentangled as the quantitative measures might suggest. There was a large range of scores given to this paper by the reviewers, which has generated a long discussion. I have also personally looked at the paper. Unfortunately I have to agree that the latent traversal plots do not look as disentangled as the metric scores would suggest, and as one might hope to see on such toy datasets as dSprites. The traversals are certainly subpar to even the most basic approaches to disentanglement, like beta VAE. For this reason, and given the reviewer scores, I unfortunately have to recommend to reject the paper this time around, however I hope that the authors are able to address the reviewers  concerns and find the source of disagreement between their qualitative and quantitative results for the future revisions of this work.
This paper proposes to use a single parametric Householder reflection to represent Orthogonal weight matrices. It demonstrates that this is sufficient provided that we make the reflection direction a function of the input vector. It is also demonstrated under which conditions this modified transformation is invertible. The derivations are sound.  This insight allows for cheaper forwarding of the model but it also comes with extra costs: It has an increased computational cost for inversion (e.g. requires optimisation) and, importantly, it does not allow to cache the $O(d)$ matrix so  it is not clear there is an advantage of the method over exp maps when we have parameter sharing (e.g. as in RNNs), since the action of the matrix has to be recomputed every time. The presented experiments are OK, but comparisons to other (potentially more efficient) methods are lacking as pointed out by the reviewers. As it stands it is not clear that this is an idea of broad interest, perhaps more suited to a specialised venue such as a workshop.
The paper proposes a new pseudometric, DARD, for comparing reward functions that avoid policy optimization. DARD builds on a recent work by Gleave et al. 2020 where the pseudometric EPIC was proposed. In contrast to EPIC, DARD operates on an approximate transition model and evaluates reward functions only on transitions close to their training distribution. Empirical experiments in different domains demonstrate the effectiveness of the proposed pseudometric. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers  questions and increased their overall assessment of the paper. At the end of the discussion phase, there was a clear consensus that the paper should be accepted. The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper.
This paper presents a differentiable simulator for protein structure prediction that can be trained end to end. It makes several contributions to this research area. Particularly training a differentiable sampling simulator could be of interest to a wider community.  The main criticism comes from the clarity for the machine learning community and empirical comparison with the state of the art methods. The authors  feedback addressed a few  confusions in the description, and I recommend the authors to further polish the text for better readability. R4 argues that a good comparison with the state of the art method in this field would be difficult and the comparison with an RNN baseline is rigorously carried out.  After discussion, all reviewers agree that this paper deserves a publication at ICLR.
After reading the reviews and discussing this paper with the reviewers, I believe that this paper is not quite ready for publication at this time. While there was some enthusiasm from the reviewers about the paper, there were also major concerns raised about the comparisons and experimental evaluation, as well as some concerns about novelty. The major concerns about experimental evaluation center around the experiments being restricted to continuous action settings where there is a limited set of baselines (see R3). While I see the authors  point that the method is not restricted to this setting, showing more experiments with more baselines would be important: the demonstrated experiments do strike me as somewhat simplistic, and the standardized comparisons are limited.  This might not by itself be that large of an issue, if it wasn t for the other problem: the contribution strikes me as somewhat ad hoc. While I can see the intuition behind why these two auxiliary objectives might work well, since there is only intuition, then the burden in terms of showing that this is a good idea falls entirely on the experiments. And this is where in my opinion the work comes up short: if we are going to judge the efficacy of the method entirely on the experimental evaluation without any theoretical motivation, then the experimental evaluation does not seem to me to be sufficient.  This issue could be addressed either with more extensive and complete experiments and comparisons, or a more convincing conceptual or theoretical argument explaining why we should expect these two particular auxiliary objectives to make a big difference.
perhaps the biggest issue with the proposed approach is that the proposed approach, which supposedly addresses the issue of capturing long term dependency with a faster convergence, was only tested on problems with largely fixed length. with the proposed k_n gate being defined as a gaussian with a single mean (per unit?) and variance, it is important and interesting to know how this network would cope with examples of vastly varying lengths. in addition, r3 made good points about comparison against conventional LSTM and how it should be done with careful hyperparameter tuning and based on conventional known setups.   this submission will be greatly strengthened with more experiments using a better set of benchmarks and by more carefully placing its contribution w.r.t. other recent advances.
The paper conducted a thorough experimental analysis of the attention map in the Conformer models for CTC based speech recognition models and connected it with phonetic and linguistic information in the speech. Using these insights, the paper presented some computation improvement and marginal quality gains. The authors actively conducted additional experiments to further justify the claims. The paper is strong in terms of the systematic way of in depth analysis and further development (i.e. sharing the attention map across layers for speedup). But as pointed out by the reviewers, it lacks some comparisons with other alternatives to justify the importance of sharing attention maps in reducing computations.  Also it would be better if there s justifications on how the observations generalize to other types of models (such as LAS, RNN T).   The decision is mainly because of the thorough analysis conducted in the paper which can be a good contribution to the community.
This paper proposes an image tesselation scheme to improve the robustness of image classifiers.  The reviewers agree that the method is simple and intuitive, and view this as a positive attribute.  At the same time, the reviewers want to see if the method works on higher resolution images.  It was also not clear to reviewers how the attacks on the method were constructed, whether they were white box, and whether they were adaptive.  Without a rebuttal, these questions remain unanswered.
This paper presents a new inference mechanism for latent variable models, by taking the derivative of log likelihood with respect to a zero valued vector. Initially, the reviewers raised concerns mostly regarding the limited experimentation and missing baselines. However, in the revised version, the authors addressed most of these concerns.   Given that most reviewers are positive after the revision and since the proposed method is simple and interesting, I recommend accepting this paper.
A heterogeneous federated learning framework is proposed which does not require auiliary public data sets, and does not reveal the private data to the server or answering parties if they operate as honest but curious entities. It builds a new protocol for private inference, which can run on GPUs, and proposes a dataset expansion method to not need an auxiliary data set. The paper presents extensive empirical experiments on the method.  The paper was extensively discussed with the authors. The concerns included both technical issues and more general issues on missing DP guarantees and realisticness of the threat model. Many of the issues were resolved by the clarifications provided by the authors, and as a result two reviewers increased their scores. However, all reviewers still place the paper to the borderline.  While the paper contains solid work, and improves efficiency compared to previous models, this is a borderline paper where the final judgement needs to be based on importance of the presented new contributions in advancing the field. The paper may not yet quite reach the bar, but I believe the reviewer comments have enabled the authors to improve the paper for further work.
The arguments the paper makes require a stronger foundation and justification. The reviewers and AC didn t find the author response sufficient. For example, in response to ZbHJ, the authors argue that their benchmark doesn t use automatically generated trajectories and therefore the language is not synthetic in some sense. It s not clear how it s related to synthetic language, but generated trajectories does create artificial regularities in the task, so an issue, but one that the author must address accurately. This argument also seems to focus on ALFRED and R2R, and ignored many other benchmarks, like the data used in DRIF (mentioned later), RxR, Touchdown, etc. There is also mis used of technical term (e.g., Decision Transformer). Generally, the reviewers consider the work of potential, but it requires significant refinement, which the author response did not provide.
The work proposed a new approach to encode time series that are irregularly sampled and multivariate using time attention module and an encoder decoder framework based on VAE. All the reviewers find the approach novel and the experiments extensive with encouraging results. Please continue to improve the presentation of the paper. I would  suggest to move the diagram showing the overall architecture to the main text to assist the explanation. Reviewers also would like to see more explanation on the experimental results and some ablation studies to show the importance of each component of the proposed architecture. 
The paper has been discussed by the reviewers that have acknowledged the rebuttal and the authors’ responses. However, the reviewers still had the following weaknesses and concerns (not solved post rebuttal):  * Expensive procedure (e.g., exhaustive enumeration before finding Pareto frontier) * The experiments should be more rigorous, with more realistic real world problems. * Missing comparison with baselines (unanimously acknowledged by the reviewers). * No explanations and insights provided as to why the method should work well * Clarity of the presentation  As a result, the paper is recommended for rejection. The detailed comments of the reviewers provide an actionable list of items to improve the paper for a future resubmission. 
This paper proposes a simple, theoretically motivated approach for post training quantization. The authors justify its effectiveness with both a sound theoretical analysis, and strong empirical results across many tasks and models, including a state of the art result for 2 bit quantized weights/activations. All reviewers agreed the paper is worth accepting, with 3/4 rating it as a clear accept following the discussion period, and the fourth reviewer not giving strong reasons not to accept.
The paper considers the convergence of the Monte Carlo Exploring Start (MCES) algorithm, a basic method in RL. Although the method is very simple and known for a long time, the condition for its convergence is not completely understood.  One of the latest results is from almost 20 years ago (Tsitsiklis, "On the Convergence of Optimistic Policy Iteration," 2002). That paper shows the convergence of the method under some restrictive assumptions on how the algorithm operates. In particular, that result requires that only the action value function of the initial state action pair of an episode be updated, as opposed to all visited state action pairs. What is not known is whether we can update the action values of all state action pairs observed in a trajectory.  It is notable that one of Tsitsiklis (2002) result requires a uniform selection of the initial state action. But he also describes a modification of the algorithm that allows convergence with a non uniform initial distribution (see page 66 of that paper, close to the end of Section 3   Optimistic Policy Iteration Using Monte Carlo for Policy Evaluation).  On the other hand, this paper establishes the convergence of MCES with no assumption on how the algorithm works. In particular, the algorithm updates the action value function for all state action pairs observed in an episode. As long as all state action pairs are visited infinitely often (and not necessarily from a uniform distribution), the convergence is established.  There is a catch, however. The paper requires some assumptions on the class of MDPs. In particular, it requires the environment to be either Stochastic Feed Forward (SFF) (more restrictive) or Optimal Policy Feed Forward (OPFF) (less restrictive). The OPFF assumption states that under any optimal policy, a state is never revisited within an episode.  The proof technique of this paper is different from the usual stochastic approximation method, and may be considered simpler.  We have two positive reviewers (with score of 8) and two slightly negative ones (with score of 5). The concern of negative reviewers is that the OPFF assumption is very restrictive. And given such an assumption on the class of MDPs, the proof becomes very simple. Moreover, it is not clear that the proof techniques developed in this work is a step toward analyzing MCES for more general MDPs.  My related concern is whether OPFF vs. Non OPFF is a good way to characterize MDPs for which MCES with every state update is convergent. Figure 3 in the paper shows the current state of knowledge in the analysis of variants of MCES. We know a problem with "No convergence" within the fourth quadrant (which is Non OPFF). Is the class of Non OPFF a tight superset of the non convergent ones?  Or is it a much larger superset? If it is tight, then OPFF is a good characterization of when MCES works or not. If it is not tight, OPFF may not be the right way to characterize the convergence of MCES.  All being said, I believe this paper positively contributes to our knowledge of a basic and fundamental RL algorithm. It does not fully resolve the convergence question, but it is indeed a progress. Whether OPFF characterization is going to be the right one or not remains to be seen in the future. I would act optimistically here and recommend acceptance of the paper.  I encourage the authors to incorporate the comments by the reviewers in updating their papers, including:   fixing all the typos   providing more examples of problems that are OPFF   resolving the claim about alphaZero   providing empirical comparison with Q Learning
The reviewers unanimously agreed that this is an interesting paper that belongs at ICLR. The use of optimal transport in neural topic models is novel and the paper is well written.  A common theme among the reviewers was that they would like to see more intuition and justification. I suggest you bear this in mind while editing the final version of the paper. I also believe that R3 brings up valid points about evaluating perplexity   I don t think the lack of perplexity results are a reason to reject the paper, but I believe they can be calculated here (see eg the reference R3 provided) and they would give a clearer view of the model s performance. 
This paper proposes to learn representations in an unsupervised manner using a generative model in which observations are generated by combining independent causal mechanisms (ICMs), in combination with a global mechanism. The authors introduce an unconventional mixture prior for the shared and independent components of the representation and train an encoder, discriminator and generator using a Wasserstein GAN with additional terms that enforce consistency in the data and latent space. Experiments consider variations of MNIST and Fashion MNIST and perform comparisons against a standard VAE, a β VAE, and the Ada GVAE.   Reviewers are broadly in agreement that this submission is not ready for publication in its current form. R4 in particular has left very detailed comments regarding clarity. The authors were able to in part address these comments, and R4 raised their score in response. That said, from a read of the manuscript in its latest form, the metareviewer (who is very familiar with literature on disentangled representations) is inclined to agree with the reviewers that this is work that has value, but is very difficult to follow in its current form. The metareviewer would like to suggest that the authors regroup, think carefully about how to improve clarity (in addition to addressing concrete points raised in reviews) and resubmit to a different venue. 
This paper is entitled Lipschitz regularized deep networks generalize. In fact, the paper has nothing in particular to do with neural networks. It is really the study of minimizers of a Lipschitz regularized risk functional over certain nonparametric classes. The connection with neural networks is simply that one can usually achieve zero empirical risk for (overparametrized) neural networks and so, in deep learning practice, neural networks behave like a nonparametric class. Given the lack of connection with neural networks, one cannot logically learn anything specific about neural networks from this paper. It should be renamed... perhaps "Lipschitz regularization with an application to deep learning".  One could raise issues of technical novelty, as it seems many of the key results are known.  I also question the insight that the bounds provide: they end up depending exponentially on the dimension of the data manifold. In the noiseless case, this exponential dependence arises from a triangle inequality between an arbitrary data point x and the nearest training data point! In the noisy case, this exponential dependence appears in a nonasymptotic uniform law of large numbers over the class of L Lipschitz functions. There s no insight into deep learning here. It s also hard to judge whether these rates are what is explaining deep learning practice: it s unclear what the manifold  dimensionality is, but it seems unlikely that this bound explains empirical performance (even if it describes the asymptotic rate of convergence).   One of the main results shows that, in the face of corrupted label (corrupted in a particular way), Lipschitz regularization can ```"undo" the corruption. However, convergence is not measured with respect to the true labeling function, but rather to the solution to the population regularized risk functional. How this solution relates to the true labeling function is unclear.  The paper also purports to resolve a mystery of generalization raised by Zhang et al (ICLR 2017). In that paper, the authors point to the diametrically opposed generalization performance on "true" and "random" labels. In fact, this paper does not resolve this problem because Zhang et al. were interested in how SGD solves this problem without explicit regularization. That Lipschitz regularization could solve this problem is borderline obvious.  I wanted to make a few comments.  In the rebuttal with reviewers, the question of parametric rates comes up. I think there s some confusion on both the part of the reviewer and authors. The parametric rates are often apparent but not real. The complexity terms often have an uncharacterized dependence on the number of data (through the learning algorithm) and on the size of the network (which is implicitly chosen based on the data complexity). In practice, these bounds are vacuous.  At some point, the authors argue that "In practice, u_n(x) is rounded to the nearest label, so once |u_n u_0| < 1/2, all classification results will be correct after rounding." I m not entirely sure I understand the logic here. First, convergence to u_0 is not controlled, but rather convergence to u*. u* may spend most of its time near the decision boundary, rendering uniform convergence almost useless. One would need noise conditions (Tysbakov) to make some claim.  Some other issues:  1. in (1), u ranges over X\to Y, but is then applied also to a weight vector.  2.  Is"continuum variational problem" jargon? If so, cite. Otherwise, taking limits of rho_n and J makes sense only if J is suitably continuous, which depends on the loss function. You later address this convergence and so you should foreshadow.  3. Notation L[u;\rho] in (5) should be L[u,\rho], no?  4. (Goodfellow et al., 2016, Section 5.2) is an inappropriate citation for the term "Generalization".  5. in Thm 2.7.,  there is reference to a sequence mu_n and i assume the sequence elements is indexed by n, but then  n appears in the probability with which the bound holds, and so this bound is not about the sequence but about a solution for \rho_n for fixed n.  6. Id should not be italicized in the statement of Lemma 2.10.  Use mathrm not text/textrm.  it should also be defined.  7. "convex convex" typo.
This is a representation learning time series paper.  The reviewers appreciated aspects of the paper, but all agreed that primarily the experiments are lacking and to a lesser degree the presentation is unclear and needs further proofreading.  So definitely this work has merits. It is also much appreciated that the authors throughout the discussion have been engaged in adding results and further clarification. This can be used for an updated version for the next conference.
All reviewers find the idea of self supervised learning on mathematical reasoning with the proposed skip tree training interesting and gave the firmly positive scores.  The paper is clearly written, and the experiments and the analysis are well organized, particularly the ability of free form conjecturing is quite thought provoking.  Also, the reviewers  initial concerns have been properly addressed during the discussion phrase.   I think this is a good paper from which people can learn a lot, and should be broadly presented at the conference either as an oral or a spotlight presentation.
The paper analyses the frequency filtering properties of self attention in vision architectures, shows that it mainly acts as a low pass filter, and proposes fixes that allow to better preserve the higher frequencies. These fixes yield moderate classification accuracy gains (~0.5 1%) for several existing attention based architectures.  The reviewers are quite borderline about the paper, but after considering the authors  responses lean towards acceptance. Pros include interesting and novel analysis and sound model improvements leading to non trivial empirical gains. The main con is that the experimental results are fine, but not outstanding.  Overall, I recommend acceptance. Empirical results are indeed good but not outstanding, but the theoretical analysis is interesting and it is good to see that it leads to actionable insights on the model design side that actually help in practice   even is not by a huge amount. One part that in my opinion is confusing (and might have been confusing to the reviewers too) is that the title seems to suggest the paper will present very deep vision transformers while it does not. Adding deeper models or adjusting the title would help here.
This paper addresses a promising method for unpaired cross domain image to image translation that can accommodate multi instance images. It extends the previously proposed CycleGAN model by taking into account per instance segmentation masks. All three reviewers and AC agree that performing such transformation in general is a hard problem when significant changes in shape or appearance of the object have to be made, and that the proposed approach is sound and shows promising results. As rightly acknowledged by R1 ‘The formulation is intuitive and well done!’  There are several potential weaknesses and suggestions to further strengthen this work:  (1) R1 and R2 raised important concerns about the absence of baselines such as crop & attach simple baseline and CycleGAN+Seg. Pleased to report that the authors showed and discussed in their response some preliminary qualitative results regarding these baselines. In considering the author response and reviewer comments, the AC decided that the paper could be accepted given the comparison in the revised version, but the authors are strongly urged to include more results and evaluations on crop & attach baseline in the final revision if possible. (2) more quantitative results are needed for assessing the benefits of this approach (R3). The authors discussed in their response to R3 that more quantitative results such as the segmentation accuracy of the synthesized images are not possible since no ground truth segmentation labels are available. This is true in general for unpaired image to image translation, however collecting annotations and performing such quantitative evaluation could have a substantial impact for assessing the significance of this work and can be seen as a recommendation for further improvement.  (3) the proposed model performs translation for a pair of domains; extending the work to multi domain translation like StarGAN by Choi et al 2018 or GANimation by Pumarola 2018 would strengthen the significance of the work. The authors discussed in their response to R3 that this is indeed possible.  
The paper introduces a new image compression approach that preserves the patterns indicating image manipulation. The reviewers appreciate the idea and the method. Please take into account the suggestions of Reviewer1, when preparing the final version.
Main summary: Paper is about generating feature representations for set elements using weighted multiset automata  Discussion: reviewer 1: paper is well written but experimental results are not convincing reviewer 2: well written but weak motivation reviewer 3: well written but reviewer has some questions around the motivation of weighted automata machinery.  Recommendation: all the reviewers agree its well written but the paper could be stronger with motivation and experiments, all reviewers agree. I vote Reject.
All reviewers agree in their assessment that this paper is not ready for acceptance into ICLR.
Reviewers raised various concerns about the motivation, unclear justification of the idea and claim, insufficient comparison with related work, and weak experimental results. While authors had made efforts to improve some of these issues in the rebuttal, the revision was not satisfied for publication quality. Overall, the paper has some interesting idea, but is not ready for publication.  
This work studies statistics of ensemble models that capture the prediction diversity between ensemble members.  The goal of the work is to identify or construct a metric which is predictive of the holdout accuracy achieved by the ensemble prediction.  Pros: * Studies empirically how measures of ensemble diversity relate to ensemble prediction accuracy. * Proposes improvements to diversity metrics that correlate better with accuracy.  Cons: * Unclear/confusing presentation. * Limited empirical validation that relies mostly on CIFAR 10 results to justify claims * Some claims made (trend between ensemble diversity and accuracy, Q diversity capturing not capturing negative correlations) are not substantiated.   All reviewers recommend this paper to be rejected and the authors did not reply to any reviews.
I recommend this paper for acceptance but I do so with significant reservations. Since this metareview will be public for all time, I direct this metareview to future readers of this paper so that they can weigh its merits and drawbacks in a clear minded way.  This paper proposes a "dual lottery ticket hypothesis." For those unfamiliar, the original lottery ticket hypothesis (Frankle & Carbin, ICLR 2019) states approximately that any randomly initialized neural network contains a subnetwork that can be trained in isolation to full accuracy in the same number of steps as the original network. That is, $\forall$ neural networks, $\exists$ a subnetwork such that $Accuracy(Train($subnetwork$)) \geq Accuracy(Train($network$))$ for a standard, fixed training procedure $Train$. (For the sake of posterity, note that this claim was supported on small scale neural networks but there is not evidence that it holds in general; only that it holds on the state of networks *early* in training. See *Linear Mode Connectivity and the Lottery Ticket Hypothesis* by Frankle et al. 2020.) To support this claim, Frankle & Carbin develop a procedure that finds such subnetworks, demonstrating that they exist in certain settings.  As far as I understand, the dual lottery ticket hypothesis states that, $\forall$ subnetworks of a neural network, $\exists$ a setting of the weights such that $Accuracy(Train($subnetwork$)) \geq Accuracy(Train($network$))$. Like the original lottery ticket paper, this paper shows that such subnetworks exist: it trains the subnetwork with an L2 penalty on all of the weights except those of the subnetwork, allowing them to gradually fade away and leaving a new setting of the weights for the subnetwork that then allows it to train in isolation to full accuracy (like those subnetworks found in the original lottery ticket hypothesis paper).  The reason that I have reservations about this approach is that the subnetwork found by the dual lottery ticket hypothesis procedure contains fully trained weights. This is novel but   to me   much less surprising and interesting: a randomly sparse subnetwork can be set with trained weights such that, after all of the other weights are fully pruned away, it can recover full accuracy. On the one hand, this is almost reminiscent of a standard pruning procedure where the network is both trained and pruned until a sparse subnetwork reaches full accuracy, with the dense network needed for much or all of training. On the other hand, the impressive part is that this can be done with a *randomly selected* sparse network rather than one chosen by a pruning heuristic. To me, that is the most interesting part of the paper. (And, for those readers wondering why specifically this paper is distinct from standard pruning, this is it.)  I wonder about the significance of this finding given that the subnetwork is set by training (not by random initialization or a tiny amount of training as in work on the lottery ticket hypothesis), but it s a novel idea and I think future scholars and future research should be the judge of that significance, not me or the reviewers. The novelty alone merits publication, and we will have to wait and see about the significance. Thus, I weigh in favor of acceptance, although with reservations.
This paper combines a well known, recently proposed unsupervised representation learning technique technique with a class conditional negative log likelihood and a squared hinge loss on the class wise conditional likelihoods, and proposes to use the resulting conditional density model for generative classification. The empirical work appears to validate the claim that their method leads to good out of distribution detection, and better performance using a rejection option. The adversarial defense results are less clear. Reporting raw logits is a strange choice, and difficult to interpret; the table is also difficult to read, and this method of reporting makes it difficult to compare against existing methods.  The reviewers generally remarked on presentation issues. R1 asked about the contribution of various loss terms, a matter I feel is underexplored in this work, and the authors mainly replied with a qualitative description of loss behaviour in the joint system, which I don t believe was the question. R1 also asked about the choice of thresholds and the issues of fairness of comparison regarding model capacity, neither of which seemed adequately addressed. R3 remarked on the clarity being lacking, and also that "Generative modeling of representations is novel, afaik." (It is not; see, for example, the VQ VAE line of work where PixelCNN priors are fit on top of representations, and layer wise pre training works of the mid 2000s, where generative models were frequently fit on greedily trained feature representations, sometimes in conjunction with a joint generative model of class labels).  R2 s review was very brief, and with a self reported low confidence, but their concerns were addressed in a subsequent update.  There are three weaknesses which are my grounds for recommending rejection. First, this paper does a poor job of situating itself in the wider body of literature on classification with rejection, which dates to at least the 1970s (see Bartlett & Wengkamp, 2006 and the references therein). Second, the empirical work makes little comparison to other methods in the literature; baselines on clean data are self generated, and the paper compares to no other adversarial defense proposals. In a minor drawback, ImageNet results are also missing; given that one of the purported advantages of the method is scalability, a large scale benchmark would have strengthened this claim. Third, no ablation study is undertaken that might give us insight into the role of each term of the loss. Given that this is a straightforward combination of well understood techniques, a fully empirical paper ought to deliver more insight into the combination than this manuscript has.
The reviewers unanimously agreed that the paper was a significant advance in the field of machine learning on graph structured inputs. They commented particularly on the quality of the research idea, and its depth of development. The results shared by the researchers are compelling, and they also report optimal hyperparameters, a welcome practice when describing experiments and results.  A small drawback the reviewers highlighted is the breadth of the content in the paper, which gave the impression of a slight lack of focus. Overall, the paper is a clear advance, and I recommend it for acceptance. 
The paper proposes a BNP topic model that uses a stick breaking prior over document topics and performs VAE style inference over them. Unfortunately, the novelty of this work is limited, as VAE like inference for LDA like models, inference with stick breaking priors for VAEs, and placing a prior on the concentration parameter in a non parametric topic model have all been done before (see e.g. Srivastava & Sutton (2017), Nalisnick & Smyth (2017), and Teh, Kurihara & Welling (2007) respectively). There are also concerns about the correctness of treating topics as parameters (as opposed to random variables) in the proposed model. The authors  clarification regarding this point was helpful but not sufficient to show the validity of the approach.
This paper suggests a problem with the standard ELBO for the multi modal case, and proposes a new objective to address this problem.  However, I (and some of the reviewers) disagree with the motivation.  First of all, there s no reason one can t train a separate encoder for every combination of modalities available, at least when there are only 2 or 3.  Failing that, one could simple optimize per example approximate posteriors without using an encoder.  Second, once you stop optimizing the ELBO, you ve lost the motivating principle for training VAEs, and must justify your new objective empirically.  Almost all of the results are (in my opinion) ambiguous plots of latent encodings.  Finally, a point made throughout the paper and discussions was that different modalities should give the same encodings, which is plainly false.  One of the reviewers made this point: "The fact that z_a !  z_b !  z_{a,b} should be expected if a and b provide different information. I don t see the problem with this.", which you dismiss.  Additionally, the encoder s job is to approximate the true posterior.  The true posteriors will in general be different for different modalities.  I would recommend focusing on ways to train the original ELBO in the presence of different modalities, instead of modifying it based on these intuitions. 
The manuscript presents a deep network approach for heteroscedastic regression problem. It assumes the variance of heteroscedastic noise is known as privileged information and suggests to reweight the samples by their noise variance in the loss.  Three reviewers agreed that the manuscript is not ready for publication. The major issue is the lack of novelty. Heteroscedastic regression is a classic problem in statistics. And reweighting using the inverse variance is a textbook method.   R2 and R4 confirmed that they have read author response. The rebuttals are useful to clarify some points, especially related to experimental settings and results. However, they are not convinced by the authors  argument on novelty and whether the assumption is realistic.  
This paper addresses the problem of recommendations within user sessions from a reinforcement learning perspective. The problem is naturally modeled as an RL problem, given its sequential nature and inherent uncertainty of any model over user preferences. The problem suffers from delayed and sparse rewards, which the authors propose to address using self supervised prediction. The approach is empirically validated in a simulated setting, using data from the 2015 ACM RecSys Challenge.  The reviewers and AC note that the problem studied is an important application area where RL has high potential to improve over current research results and industry practice. The proposed idea is interesting, and the strong empirical evaluation on a publicly available data set is highlighted. R1 also commends the authors  decision to address the challenging cold start problem.  The reviewers and AC also note several potential weaknesses. The choice of addressing the problem from a reinforcement learning perspective is not clearly motivated. This is needed, as many supervised learning (and other types) approaches to the problem exist. A performance comparison to current state of the art RL baselines is missing. The proposed approach is related to both imagination augmented (I2A, Racaniere et al. 2017) and agents with auxiliary rewards (UNREAL, Jaderberg et al. 2016), but does not compare to either method. Neither does the related work section sufficiently clarify why the proposed approach is expected to improve over these prior approaches. A thorough comparison to these baselines in a real world application like session based recommendation would be a strong contribution in itself, but without the contributions of the paper are hard to assess. Reviewers also noted lack of clarity. Some concerns are addressed by the authors, but the consensus is that the paper would benefit from a major revision to clearly work out the method, as well as it s conceptual and empirical differences from existing reinforcement learning approaches. R3 mentions missing related work, some of which the authors include in the revision. The AC recommends also following up on references in cited papers to ensure a future revision of the paper is well placed in the context of prior work on recommender systems, especially when modeled as a reinforcement learning problem.  Overall, the paper was assessed as borderline by the reviewers. The ACs view is that there are too many concerns for acceptance at ICLR in the present form, and that the paper will benefit from a thorough revision.
The idea presented in the paper is interesting and has caught the attention of the reviewers. However there seem to be only a tepid support for acceptance with a reviewer championing rejection.  There is little novelty in the approach but empirical validation shows results that consistently improve over selected baselines. I am afraid that more evaluations would be needed at this stage to consider this work for acceptance.
This paper studies the setting in reinforcement learning where the next action must be sampled while the current action is still executing. This refers to continuous time problems that are discretised to make them delay aware in terms of the time taken for action execution. The paper presents adaptions of the Bellman operator and Q learning to deal with this scenario.  This is a problem that is of theoretical interest and also has practical value in many real world problems. The reviewers found both the problem setting and the proposed solution to be valuable, particularly after the greatly improved technical clarity in the rebuttals. As a result, this paper should be accepted.
This work analyzes the ability of pre trained language models to maintain entity coherence and consistency in long narrative generation. Along with new automatic metrics for analyzing narrative generation, it proposes a memory augmented model that allows tracking entities to improve narrative generation.  Although all the reviewers appreciated the importance of the problem, the novelty of the proposed approach, as well as empirical improvements in a subset of experiments, they also acknowledge several major weaknesses including the lack of rigor in defining the method, the lack of clarity in writing (especially in the experiments section), insufficiently strong baselines, and an issue of reproducibility since the code cannot be released. These concerns were in part addressed during rebuttal, but not enough to accept the paper.
Pros:   simple, sensible subgoal discovery method   strong inuitions, visualizations   detailed rebuttal, 15 appendix sections  Cons:   moderate novelty   lack of ablations   assessments don t back up all claims   ill justified/mismatching design decisions   inefficiency due to relying on a random policy in the first phase  There is consensus among the reviewers that the paper is not quite good enough, and should be (borderline) rejected.
This paper presents a method which creates a representative subset of testing examples so that the model can be tested quickly during the training. The procedure makes use of the famous HGS selection algorithm which identifies and then eliminates the redundant and obsolete test cases based on two criteria: (1) structural coverage as measured by the number of neurons activated beyond a certain threshold, and (2) distribution mismatch (as measured by KL divergence) of the last layer activations. The algorithm has two phases: (1) a greedy subset selection based on the coverage, and (2) an iterative phase were additional test examples are added until the KL divergence (as defined above) falls below some threshold.  This approach is incremental in nature   the resulting multi objective optimisation problem is not a significant improvement over BOT. After the discussion phase, we believe that the advantages over BOT were not clearly demonstrated and that the main drawback of BOT (requiring the number of samples) is not hindering practical applications. Finally, the empirical evaluation is performed on very small data sets and I do not see an efficient way to apply it to larger data sets where this reduction could be significant. Hence, I will recommend the rejection of this paper. To merit acceptance to ICLR the authors need to provide a cleaner presentation (especially of the algorithms), with a focus on the incremental improvements over BOT, an empirical analysis on larger datasets, and a detailed look into the computational aspects of the proposed approach. 
This paper presents an auxiliary module to boost the representation power of GNNs. The new module consists of virtual supernode, attention unit, and warp gate unit. The usefulness of each component is shown in well organized experiments. This is the very borderline paper with split scores. While all reviewers basically agree that the empirical findings in the paper are interesting and could be valuable to the community, one reviewer raised concern regarding the incremental novelty of the method, which is also understood by other reviewers. The impression was not changed through authors’ response and reviewer discussion, and there is no strong opinion to champion the paper. Therefore, I’d like to recommend rejection this time.  
This paper makes an innovative change to the adjacency matrix definition in graph convolutional neural networks (GCNs) (Kipf & Welling, 2017).  The change results in computationally efficient isometric transformation invariance.   There were a number of concerns raised by reviewers, and the author responses and revisions, and the subsequent discussion, resulted in most of these concerns being satisfactorily addressed.  On reviewer continued to feel the paper was entirely theoretical and therefore not appropriate to ICLR, but that opinion was not shared more broadly and is not held by the area chair.
The paper proposes an end to end architecture, Net DNF,  for handling tabular data. This is a novel approach in the relatively under explored domain of application of neural networks; the paper also presents justification of the design choices via ablation studies. The paper is clearly written, and empirical results are convincing. 
The paper builds upon hypergraph convolutional networks (HCN), extending them to time varying hypergraphs in dynamical settings.  However, as some of the reviewers pointed out, it would be useful to explore other system variations to better justify the choices in this particular approach; perhaps an evaluation on a wider set of datasets would also strengthen the contribution of the paper,  as well as adding evaluation metrics that can be more appropriate for the application considered (stock market prediction). Also, concerns were raised by several reviewers regarding the somewhat incremental  improvement over the state of art, and the degree of novelty in the proposed approach. Overall, while the problem considered is important and the approach is promising, the paper in its current shape  is somewhat borderline and may require a bit of additional work to be ready for publication. 
This paper introduces a new RNN architecture which uses a small network to decide which cells get updated at each time step, with the goal of reducing computational cost.  The idea makes sense, although it requires the use of a heuristic gradient estimator because of the non differentiability of the update gate.  The main problem with this paper in my view is that the reduction in FLOPS was not demonstrated to correspond to a reduction in wallclock time, and I don t expect it would, since the sparse updates are different for each example in each batch, and only affect one hidden unit at a time.  The only discussion of this problem is "we compute the FLOPs for each method as a surrogate for wall clock time, which is hardware dependent and often fluctuates dramatically in practice."  Because this method reduces predictive accuracy, the reduction in FLOPS should be worth it!  Minor criticism: 1) Figure 1 is confusing, showing not the proposed architecture in general but instead the connections remaining after computing the sparse updates. 
This paper proposed a new variant of knowledge distillation. The basic idea is interesting although similar ideas have more or less appeared in the literature as pointed out by the reviewers. Our main concern on this work is that the real empirical improvements are too limited such that it is hard to conclude that the proposed method can really perform better than the baseline. In the meantime, the proposed method is much more computationally expensive. 
This submission received 4 diverging ratings: 6, 5, 5, 3. On the positive side, reviewers appreciated the central idea and a quality manuscript. At the same time, they have raised important concerns around unfair comparisons with baselines, experiments not fully supporting the claims and lack of comparisons with some prior methods. After discussions with the authors most reviewers stayed with their original ratings.  The AC agrees that the weaknesses in this case outweigh the strengths. The final recommendation is to reject.
This paper addresses the problem of performing unsupervised domain adaptation when some target domain data is missing is a potentially non stochastic way. The proposed solution consists of applying a version of domain adversarial learning for adaptation together with an MSE based imputation loss learned using complete source data. The method is evaluated on both the standard digit recognition datasets and a real world advertising dataset.   The reviewers had mixed recommendations for this work, with two recommending weak reject and one recommending acceptance. The key positive point from R3 who recommended acceptance was that this work addresses a new problem statement which may be of practical importance. The other two reviewers expressed concerns over the contribution of the work and the validity of the problem setting. Namely, both R2 and R4 had significant confusion over the problem specification and/or under what conditions the proposed setting is valid.   It is a difficult decision for this paper as there is a core disagreement between the reviewers. All reviewers seem to agree that the proposed solution is a combination of prior methods in a new way to address the specific problem setting of this work.  However, the reviewers differ in precisely whether they determine the proposed problem setting to be valid and justified. Due to this discrepancy, the AC does not recommend acceptance at this time. If the core contribution is to be an application of existing techniques to a new problem statement than that should be clarified and motivated further.  
This paper introduce a protein pretraining framework that enhances representations learnt from protein language modeling with knowledge graph embeddings. The new framework, OntoProtein, optimizes jointly a masked Protein objective and a Knowledge Graph Embedding objective producing knowledge aware protein embeddings. These embeddings are evaluated on downstream tasks including protein protein interactions and protein GO association prediction. The paper also introduces a new large scale KG dataset, ProteinKG25.  The reviewers were in agreement that the paper presents an important research direction and that the work is well framed and motivated. The dataset contribution was also considered important by the reviewers and they are in agreement that the paper is clear and generally easy to understand. Reviewers were concerned that the novelty of the work is in the application of existing techniques to a new domain rather than introducing new domains, but generally the reviewers considered the novelty to be sufficient for publication. There were some other concerns about missing references and some of the presentation, but the authors addressed these concerns in their response and in the updated version that they produced.
* Strengths  The paper addresses an important topic: how to bound the probability that a given “bad” event occurs for a neural network under some distribution of inputs. This could be relevant, for instance, in autonomous robotics settings where there is some environment model and we would like to bound the probability of an adverse outcome (e.g. for an autonomous aircraft, the time to crash under a given turbulence model). The desired failure probabilities are often low enough that direct Monte Carlo simulation is too expensive. The present work provides some preliminary but meaningful progress towards better methods of estimating such low probability events, and provides some evidence that the methods can scale up to larger networks. It is well written and of high technical quality.  * Weaknesses  In the initial submission, one reviewer was concerned that the term “verification” was misleading, as the methods had no formal guarantees that the estimated probability was correct. The authors proposed to revise the paper to remove reference to verification in the title and the text, and afterwards all reviewers agreed the work should be accepted. The paper also may slightly overstate the generality of the method. For instance, the claim that this can be used to show that adversarial examples do not exist is probably wrong adversarial examples often occupy a negligibly small portion of the input space. There was also concern that most comparisons were limited to naive Monte Carlo.  * Discussion  While there was initial disagreement among reviewers, after the discussion all reviewers agree the paper should be accepted. However, we remind the authors to implement the changes promised during the discussion period.
This paper proposes a response generation approach that aims to tackle the generic response problem. The approach is learning a latent semantic space by maximizing the correlation between features extracted from prompts and responses. The reviewers were concerned about the lack of comparison with previous papers tackling the same problem, and did not change their decision (i.e., were not convinced) even after the rebuttal. Hence, I suggest a reject for this paper.
This paper propose a method to explain the contextualization of BERT by identifying a set of influence paths from the input to the output.  Although all reviewers give overall score 6, their comments are pointing to the negative direction.  The following excerpts summarize the general sentiment of the reviews: R2: Overall, I incline toward rejecting. This paper provides an instrument to explain BERT, but I have a hard time understanding the result itself (influence paths or patterns). I also have a major concern with the final analysis and its findings.   R4: I think the paper has improved substantially. The direction is exciting, but even with the updates I believe this paper needs quite some work to improve the presentation and clarity, which is why I have not updated my score. R1: Overall, I think there are certainly interesting analyses that could come out of this work, but the current paper does not provide a clear enough contribution to be ready for publication. R3: It looks like the proposed method is largely based on (Lu et al, 2020); the major difference is the introduction of Multi partite patterns, which basically expands the objects of influence analysis from paths to patterns (partial paths). This doesn t look like a significant novelty. 
This paper conducts a comprehensive study on different retrieval algorithms and show that the two tower Transformer models with properly designed pre training tasks can largely improve over the widely used BM 25 algorithm. In fact, the deep learning based two tower retrieval model is already used in the IR field. The main contribution lies in the comprehensive experimental evaluation.  Blind Review #3 has a major misunderstanding of the paper; hence his review will be excluded. The other two reviewers tend to accept the paper with several minor comments.  As the authors promise to release the code as a baseline for further works, I agree to accept the paper. 
Four knowledgeable referees support acceptance for the contributions, and I also recommend acceptance. There is agreement among all reviewers that this paper is about  a highly relevant topic, that the model presented is technically sound and has significantly novel aspects, and that the experimental results are convincing. There were several points of criticism raised by the reviewers, concerning, for instance, further comparison experiments,  the heuristic nature of masking rules, or the treatment of homologous sequences. In my opinion, however, most of these points have been addressed in a rather convincing way during the rebuttal phase.  
This paper focuses on improving the efficiency of sharpness aware minimization method for training neural networks. The proposals are stochastic weight perturbation, namely selecting subset of the parameters at any step, and sharpness sensitive data selection. The philosophy behind sounds quite interesting to me, namely, sharpness aware minimizer can be approximated properly with fewer computations after analyzing the min max procedure. This philosophy leads to a novel algorithm design I have never seen.  The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please include the additional experimental results in the next version.
 The authors present a novel method for tackling exploration and exploitation that yields promising results on some hard navigation like domains. The reviewers were impressed by the contribution and had some suggestions for improvement that should be addressed in the camera ready version. 
The two main concerns raised by reviewers is that whether the results are significant, and a potential issue in the proof. While the rebuttal clarified some steps in the proof, the main concerns about the significance remain. The authors are encouraged to make this significance more clear.  Note that one reviewer argued theoretical papers are not suitable for ICLR. This is false, as a theoretical understanding of neural networks remains a key research area that is of wide interest to the community. Consequently, this review was not considered in the final evaluation.
This paper considers generalization of polynomial networks. It gives a characterization of the Rademacher complexity as well as Lipschitz constants for polynomial nets. Inspired by the theoretical results, the paper also proposed regularization schemes that empirically improves accuracy and robustness. Most reviewers found the theoretical results to be interesting (but there are some concerns about the mismatch between the upperbound in theory and used in practice, which was partially addressed in the response). There are some more concerns about the experiments but many of them are addressed in the new version. Overall although polynomial networks are not popular in practice, this paper provides some interesting theoretical results.
This paper addresses an interesting learning problem of a generative neural network on a simulated ensemble of protein structures obtained using molecular simulation to characterize the distinct structural fluctuations of a protein bound to various drug molecules. The main technical contribution is a geometric autoencoder architecture with separate latent spaces for representing intrinsic and extrinsic geometry. However, the reviewers think the benefit for modeling intrinsic and extrinsic geometry is not clearly explained and the experiments are not convincing at the moment. The paper can be potentially improved by addressing these two main issues. 
Dear Authors,  The paper was received nicely and discussed during the rebuttal period. The current discussions mostly lie on the acceptance side.   Some prons of the paper include:    Timely topic: This paper deals with the problem of distributed training of GNNs.    New algorithm: this method captures the idea of transmitting only local averages but adds a centralized step on the server to account for global structural information lost in the subgraph partition.   Theory: The authors further provide theoretical convergence guarantees.    Clarity: The paper is fairly well written and the proposed result is simple and powerful.   The current consensus is that the paper deserves publication.  Best AC
This work presents a new loss function that combines the usual cross entropy term with a margin maximization term applied to the correctly classified examples. There have been a lot of recent ideas on how to incorporate margin into the training process for deep learning. The paper differs from those in the way that it computes margin. The paper shows that training with the proposed max margin loss results in robustness against some adversarial attacks. There were initially some concerns about baseline comparisons; one of the reviewers requesting comparison against TRADES, and the other making comments on CW L2. In response, authors ran additional experiments and listed those in their rebuttal and in the revised draft. This led some reviewers to raise their initial scores. At the end, majority of reviewers recommended accept. Alongside with them, I find extensions of classic large margin ideas to deep learning settings (when margin is not necessarily defined at the output layer) an important research direction for constructing deep models that are robust and can generalize. 
This manuscript studies the problem of continual learning and introduces a reinforcement learning agent to select hyperparameters for replay/training. Ordinarily, replay based mechanisms for continual learning use settings and hyperparameters that are chosen and fixed through training. If it was possible to adjust replay dynamics online (in this case by looking at performance on a held aside test set), performance might be improved. This is the approach taken by this manuscript.  Reviewers were generally happy with the writing of the paper and presentation of the material. At the same time, more than one reviewer worried about the novelty of the approach. In essence, the proposal amounts to using a black box optimizer (in this case RL) to adjust online the hyperparameters (e.g. the replay ratio) for continual learning (of the shelf ER and SCR). Viewed through this lens, and given that the optimizer in this case was a straightforward application of DQN, this concern is potentially well founded. The primary novelty then is the construction of the reward function to be optimized: in this case defined as the decrease of the CL loss measured on a held aside test set that is constructed online. Nevertheless, novelty is only part of the equation and strong empirical results can easily be a deciding factor in readiness for publication. On this front, reviewer GhFg points out that the empirical results and comparisons with baseline methods are not as clear as they need to be. Several issues are raise in discussion: the primary one is around the question of how the authors have allowed task specific information for the Q functions used by RL, and what the implications of this might be. The baselines compared against do not use any task specific information, which muddies the waters when trying to understand the comparisons. I agree with the reviewer that the manuscript needs to do a better job of making the empirical setting and comparisons as transparent and fair as possible. Given this, and the fact (raised by several reviewers) that some empirical evidence presented in the manuscript actually points to RL selecting near static parameters over time, I recommend that the manuscript be rejected. At the same time, I want to encourage the authors to focus on a streamlined version of the manuscript that addresses the issues raised by GhFg, as I believe that if the concerns can be addressed the work is close to making a compelling contribution for the field.
The submission proposes two new things: a repulsive loss for MMD loss optimization and a bounded RBF kernel that stabilizes training of MMD GAN. The submission has a number of unsupervised image modeling experiments on standard benchmarks and shows reasonable performance. All in all, this is an interesting piece of work that has a number of interesting ideas (e.g. the PICO method, which is useful to know). I agree with R2 that the RBF kernel seems somewhat hacky in its introduction, despite working well in practice.  That being said, the repulsive loss seems like something the research community would benefit from finding out more about, and I think the experiments and discussion are sufficiently extensive to warrant publication.
The paper proposes a VAE variant by embedding spatial information with multiple layers of latent variables. Although the paper reports state of the art results on multiple datasets, some results may be due to a bug. This has been discussed, and the author acknowledges the bug. We hope the problem can be fixed, and the paper reconsidered at another venue. 
This paper proposes a framework for learning disentangled representations of content and style in an unsupervised way, using a permutation invariant network. It adopts VQ network for content encoding, and Cross Attention for Style and Linking Attention at decoder. It is shown to be domain agonistic, working well in image and audio domain. Experiments are conducted on speech and image datasets.  The paper is recommended as an accept (weak) to ICLR. The reviewers have given detailed feedback and suggestions   please address them in the next revision of the paper.
The reviews agree the paper is not ready for publication at ICLR. 
While the author response clarified some concerns, it could not convince the reviewers that the current version of the paper should be accepted for publication at ICLR.  
This paper introduces a "model agnostic" attack to evaluate the robustness of machine learning models, specifically focusing on adversarial training style defenses. The attack uses a RNN optimizer and meta learning to achieve this goal.  The reviews are mixed. The two more positive reviewers appreciated the technical ideas of the attack, found the paper well written, and noted the results were stronger than prior attacks.  However the more negative reviewer raises valid concerns around (a) the magnitude of the contribution compared to prior attacks, and (b) to what extent this attack will be useful more generally.  Starting with the first point (also raised by the other reviewers) the total contribution of this paper is to improve attack success rates by ~0.1% compared to the best prior attacks. This is a fairly limited total gain, especially because this technique requires much more sophisticated attack techniques.  More fundamental is the question if this attack is useful to the community. I tend to agree with reviewer 8AXD here that this contribution is rather limited for two reasons: 1. As the authors acknowledge, the attack is most effective for adversarial   training techniques, or others that don t make the gradient hard to   optimize. This limits the attack to a smaller subset of defenses. 2. Complexity complicates attack evaluations. It s hard enough to get an   attack working in the first place, and this paper has to use some fairly   sophisticated tools to just get the attack marginally better than   prior attacks that are (much) simpler. It s not clear that we would   expect authors of future defenses to be able to get as good results,   when prior methods are much simpler to apply.  And so on the whole, this paper s main contribution is improving attack success rate by a small amount, with a fairly complex method, that only applies to a class of defenses that don t make gradient descent difficult. So while there is nothing outright wrong with this paper, in its current form it seems to be adding unnecessary complexity to achieve something that can already be done with existing techniques.
This paper introduces the "reversible instance normalization" (RevIN), a method for addressing temporal distribution shift in time series forecasting. RevIN consists in normalizing (subtracting the mean and dividing by the standard deviation) each layer of of deep neural network in a given temporal window for a given instance, and de normalizing by introducing learnable shifting and scaling parameters.   The paper initially received one weak accept and two weak reject recommendations. The main limitations pointed out by reviewers relate to the limited novelty of the approach, the positioning with window normalization methods and hybrid methods in times series, and clarifications on experiments. The authors  rebuttal did a good job in answering the main concerns: rV5fo increased its grade from weak reject to clear accept, and RuPmn maintained its weak acceptance recommendation.  The AC carefully read the submission. The AC considers that the idea is simple yet meaningful. The large set of experiments are well conducted and conclusive. The rebuttal successfully answers to relevant issues raised by reviewers, regarding ablation studies (for highlighting the importance of the learnable de normalization), the impact of the temporal window, the comparison to hybrid approaches and the difference with respect to Adaptive normalization. The AC thus acknowledge that this submission draws important take home messages for the community, and therefore recommends acceptance.
This paper proposes speeding up certain optimization problems common in physics by reparameterizing their parameters as the output of a graph neural network. The reviewers appreciate the idea, but are not convinced enough to recommend the paper for acceptance. They point out the following weaknesses: * The method amounts to linear preconditioning, and hence it s reasonable to expect a fairly complete comparison to the many linear preconditioning approaches that have been proposed previousl. The reviewers are not satisfied with the currently provided comparison.  * The main idea is not presented clearly enough. In particular, it s not obvious the proposed method is best described as neural reparameterization, since it seems to amount to linear preconditioning. * The experiments are not persuasive enough: The presented problems may not be relevant to all of the target audience of ICLR, and the experimental evaluation does not seem sufficiently exhaustive.  The suggested areas of improvement provided by the reviewers seem reasonable to me: I therefore recommend not accepting the paper in its current form. To make the paper more accessible and appealing, the authors may consider rewriting the paper to more closely match the perspective taken by the reviewers, and to provide a more thorough comparison to the previous approaches and the existing literature.
This paper proposes a sequential latent variable model for the knowledge selection task for knowledge grounded dialogues. Experimental results demonstrate improvements over the previous SOTA in the WoW, knowledge grounded dialogue dataset, through both automated and human evaluation. All reviewers scored the paper highly, but they also made several suggestions for improving the presentation. Authors responded positively to all these suggestions and provided updated results and other stats. The paper will be a good contribution to ICLR.
This paper presents two new architectures that model latent intermediate utilities and use non additive utility aggregation to estimate the set utility based on the computed latent utilities. These two extensions are easy to understand and seem like a simple extension to the existing RNN model architectures, so that they can be implemented easily. However, the connection to Choquet integral is not clear and no theory has been provided to make that connection. Hence, it is hard for the reader to understand why the integral is useful here.  The reviewers have also raised objection about the evaluation which does not seem to be fair to existing methods. These comments can be incorporated to make the paper more accessible and the results more appreciable. 
The paper looks into theoretical analysis of self training beyond the existing linear case and considers deep networks under additional assumption on data. namely: expansion and minimal overlap in the neighborhood of examples in different classes. The results shed some light on self training algorithms that use input consistency regularizers. Although the assumptions are very hard to check for all input distributions, the authors make an attempt by considering output of BigGAN generator. In summary, the paper is a great first step in understanding self training for deep networks.  The paper is overall clearly written. please add the explanation of  Assumption 4.1 as requested by Reviewer 4.  Pros:   given the extensive use of self training the paper is of great importance to the community  extending the analysis of self training to deep networks  the paper is clearly written and easy to follow  cons:  the assumptions are very hard to validate on all datasets 
In general, the reviewers appreciated the elegant concept behind the paper and the good results. However, they also raised considerable reservations about the significance of a method that decreases the parameter count but not necessarily computational efficiency (FLOPS) or memory. While the additional analysis that the authors provided definitely helps to understand the limitations of the method, the reviewers were in the end quite divided on the significance of the results. In addition, all reviewers agreed that the writing was in somewhat rough shape and needed improvement.  In summary, this is definitely a borderline paper, but given the current reviewer assessment, I would recommend that it is not quite ready for publication.
The paper tackles the problem of mitigating the effect of model discrepancies between the learning and deployment environments. In particular, the author focus on the worst case possible performance. The paper has both an empirical and theoretical flavor. The algorithm they derived is backed by theoretical guarantees. There exists a gap between the theory presented and the final practical algorithm, which generated some elements of concern from the reviewers. Some of these issues (choice and sensitivity of the Lipschitz constant, in what cases can we make that assumption, choice of p_w, discrepancy between the theoretical proposal and the practical algorithm) are well addressed in the rebuttal. However, after careful examination of the reviews, the meta reviewer is still not convinced that the paper meets the minimum requirements for acceptance, as many of the reviewers  initial concerns still remain.
this submission presents a novel way in which a neural machine reader could be improved. that is, by learning to reformulate a question specifically for the downstream machine reader. all the reviewers found it positive, and so do i.
The paper shows hardness results for batch reinforcement learning. Authors show that even if all value functions are linear in a given set of features and the exploration data covers all directions, evaluating any policy might require a sample size that is exponentially large in the problem horizon. This is an interesting and somewhat surprising result, and I believe it would be of interest to the wider RL community. I recommend acceptance of this paper.
The paper suggests a new way to defend against adversarial attacks on neural networks. Two of the reviewers were negative, one of them (the most experienced in the subarea) strongly negative. One reviewer is weakly positive. The main two concerns of the reviewers are insufficient comparisons with SOTA and lack of clarity. The authors  response, though detailed, has not convinced the reviewers and has not alleviated their concerns.  
The paper studies word embeddings using the matrix factorization framework introduced by Levy et al 2015. The authors provide a theoretical explanation for how the hyperparameter alpha controls the distance between words in the embedding and a method to estimate the optimal alpha.  The authors also provide experiments showing the alpha found using their method is close to the alpha that gives the highest performance on the word similarity task on several datasets.   The paper received 2 weak rejects and 1 weak accept.  The reviews were unchanged after the rebuttal, with even the review for weak accept (R2) indicating that they felt the submission to be of low quality.  Initially, reviewers commented that while the work seemed solid and provided insights into the problem of learning word embeddings, the paper needed to improve their positioning with respect to prior work on word embeddings and add missing citations.  In the revision, the authors improved the related work, but removed the conclusion.  The current version of the paper is still low quality and has the following issues 1. The paper exposition still needs improvement and it would benefit from another review pass Following R3 s suggestions, the authors have made various improvements to the paper, including modifying the terminology and contextualizing the work.  However, as R3 suggests, the paper still needs more rewriting to clearly articulate the contribution and how it relates to prior work throughout the paper.  In addition, the conclusion was removed and the paper still needs an editing pass as there are still many language/grammar issues.  Page 5: "inherites"  > "inherits" Page 5: "top knn"  > "top k"  2. More experimental evaluation is needed. For instance, R1 suggested that the authors perform additional experiments on other tasks (e.g. NER, POS Tagging).  The authors indicated that this was not a focus of their work as other works have already looked at the impact of alpha on other task.  While prior works has looked at the correlation of alpha vs performance on the task, they have not looked at whether alpha estimated the method proposed by the author will give good performance on these tasks as well.  Including such analysis will make this a stronger paper.  Overall, there are some promising elements in the paper but the quality of the paper needs to be improved.  The authors are encouraged to improve the paper by adding more experimental evaluation on other tasks, improving the writing, as well as incorporating other reviewer comments and resubmit to an appropriate venue.  
The paper proposes a method for out of distribution (OOD) detection for neural network classifiers.  The reviewers raised several concerns about novelty, choice of baselines and the experimental evaluation. While the author rebuttal addressed some of these concerns, I think the paper is still not ready for acceptance as is.   I encourage the authors to revise the paper and resubmit to a different venue.
The paper is proposing a domain generalization method based on the intuition that an invariant model would work for any split of train/val. Hence, the method uses adversarial train/val splits during training. The paper is reviewed by three expert reviews and none of them championed the paper to be accepted. I carefully checked the reviews and the authors  response and agree with the reviewers. Specifically:    R#1: Argues that the paper is not ready for publication. Also argues the optimization problem is only a motivation as it is not directly solved. This is an important issue and it needs to be addressed in a conclusive manner.   R#2: Argues empirical studies do not show the value of train/val splitting. I partially disagree with this issue but it is clear that more qualitative and quantitative study is needed to properly justify the proposed method.   R#3: Argues the contribution is not enough for publication. The paper is clearly novel but the contribution and novelty is not presented in a clear manner. Moreover, the empirical study does not complement the novelty. Hence, I disagree with the comment.  Overall, I believe the paper proposes an interesting idea. However, the presentation and empirical studies need to be improved significantly. I recommend authors to address these issues and submit to the next conference.  
# Paper Summary  This paper considers calibrating the output of a multiclass classifier in such a way that the output probabilities approximately are approximately "correct". They observe that if such a method is able to re order the logits, then it will change the accuracy of the classifier. Therefore, if they use a calibrator that is constrained to be monotonic in the input logits, then they can train it to optimize any metric they choose, without impacting the accuracy.  They propose minimizing ECE (expected calibration error, which is essentially a binned approximation to the L1 distance between the model s confidence in its top label, and the probability that the top label is correct). Borrowing an idea from local temperature scaling, they also allow their calibrator to see the input features, by also taking the top hidden layer (the layer before the logits) as an input.  Their experiments are, with one glaring exception, comprehensive: they have a good selection of datasets, a reasonable choice of metrics, and they dig pretty deep into what the results mean. Reviewer 2, however, believes that the baselines are far from state of the art, and two of the other reviewers (and I) agree.  # Pros  1. Well organized and well written (not exceptionally so, but above the bar) 1. Good insight overall. In particular, the observation that imposing a monotonicity constraint enables one to optimize any metric, including ECE, was considered both original and significant by the reviewers 1. Well thought out experiments. They were generally praised, aside from the (unfortunately crucial) question of whether the baselines are state of the art  # Cons  1. The paper seems to mainly discuss related work coming from the temperature scaling "tradition". The reviewers would like to see a more comprehensive discussion of other calibration approaches (Reviewer 2 provided a number of references) 1. There are some misstatements (e.g. that LTS is "state of the art"), and incorrect implications (e.g. that temperature scaling like methods are dominant). Reviewer 2 listed several of these, all of which are fairly minor, but more care should be taken 1. The TS and LTS baselines are not state of the art. The reviewers were generally impressed with the experiments, but the lack of a strong baseline is a fatal flaw  # Conclusion  This was a paper that initially received mostly positive reviews, but Reviewer 2 raised several concerns that were not adequately addressed in the author response, causing two other reviewers to lower their scores. Ultimately, three of the four reviewers recommended rejection.  The general consensus is that this is a well written paper, with good insight, well thought out experiments (except for the baselines), and that it overall makes a worthwhile contribution. The main issues, all of which were raised by Reviewer 2, are eminently fixable: (i) adding a more thorough discussion of related work, especially work unrelated to temperature scaling, (ii) being more careful to avoid misstatements, or to seem to imply incorrect statements (e.g. that temperature scaling like methods are dominant), and (iii) adding a couple of new state of the art baselines to the experiments.
This paper tackles an interesting problem (that of federated continual deep learning) and proposes an effective approach for it with good results.  This is a good contribution. However, there are presentation issues in several aspects of the paper that require improvement before publication. The authors  claims of the novelty of the federated continual learning problem is overstated (there was an AAMAS 18 paper they cited which IS applicable to the federated setting, although their method is certainly a substantial improvement, more flexible, and supports deep models), and there are aspects of the analysis and experiments that could be improved (the authors are somewhat dismissive of one reviewer s concerns about the insensitivity to the regularization parameters. While I agree with the authors that this aspect of the review is focusing on this one detail in lieu of the bigger picture, the author s insensitivity study in Figure 6 and subsequent analysis are lacking and could use improvement).  The authors  revisions did help clarify/address a number of issues raised in the initial reviews,  but it was felt that more improvement was needed. In particular, there are still mistakes with characterizing this work with respect to existing work, especially in overstating the novelty of the federated continual learning problem.  I do believe this paper is the first to call it "federated continual learning" (a name I like), but the paper should be less dismissive of existing works on "multi agent lifelong learning" that could also apply to this setting, albeit with shallow models.  Consequently, while this reduces the novelty of the federated continual learning problem, the authors still have a substantive contribution   just one that needs improvement in presentation before publication.
First, I would like to thank all the reviewers for their efforts in reading and understanding this paper. I tried to read the paper as well and I also find it s really difficult (if possible) for me to understand the ideas presented here. The most important task in writing a paper (as Reviewer Svha also suggested in his/her review) in the field of machine learning is to explain to your peers what is the problem you are trying to solve and how you solve (or partially solve) that problem. I think there is a consensus among the reviewers that the paper did not do a great job of that. I am not questioning the quality of the idea or the research here, but I think the paper here will need to do a significantly better job here in explaining the idea before it can be a good ICLR publication.
This paper proposes a method to cope with large vocabulary sizes. The idea is to find a small number of anchor words and to express every other word as a sparse nonnegative linear combination of them. They give an end to end method for training, and give a statistical interpretation of their algorithm as a Bayesian nonparametric prior (in particular an Indian restaurant process). They give extensions that allow them to deduce the optimal number of anchors which allows them to avoid needing to tune this hyperparameter. Finally they give a variety of experiments, particularly in language and recommendation tasks. The results on language are particularly impressive, and in the author response period, at the behest of a reviewer, they were able to extend the experiments to the Amazon Review dataset which contains 233M reviews on 43.5 M items by 15.2 M users.   This paper is a nice combination of a simple but powerful idea, and a range of experiments demonstrating its utility. Other papers have proposed related ideas, but here the main novelty is in (1) using a small number of anchors that can incorporate domain knowledge and (2) using a sparse linear transformation to express other words in this basis. One reviewer did not find the Bayesian nonparametric interpretation to be fruitful, since it does not lead to techniques for handling growing datasets (e.g. if the ideal number of anchors changes over time). 
The paper proposes a constituent based transformer for aspect based sentiment analysis. The approach allows conducting aspect based sentiment analysis to leverage the syntactic information without pre specified dependency parse trees.   Overall, the idea is interesting. However, all the reviewers shared the following concerns:     Paper descriptions of methodology and experiments are not clear and require significant rewriting and reorganization.    The proposed approach is not well justified by the empirical study presented in the paper. Especially, a more detailed ablation study is required to justify the design.   We would suggest the authors addressing the feedback from the reviewers to improve the paper. 
This paper conducts an extensive study of training BERT and shows that its performance can be improved significantly by choosing a better training setup (e.g., hyperparameters, objective functions). I think this paper clearly offers a better understanding of the importance of tuning a language model to get the best performance on downstream tasks. However, most of the findings are obvious (careful tuning helps, more data helps). I think the novelty and technical contributions are rather limited for a conference such as ICLR. These concerns are also shared by all the reviewers. The review scores are borderline, so I recommend to reject the paper.
This paper seeks to adapt behavioural cloning to the case where demonstrator and learner have different dynamics (e.g. human demonstrator), by designing a state based objective. The reviewers agreed the paper makes an important and interesting contribution, but were somewhat divided about whether the experiments were sufficiently impactful. They furthermore had additional concerns regarding the clarity of the paper and presentation of the method. Through discussion, it seems that these were sufficiently addressed that the consensus has moved towards agreeing that the paper sufficiently proves the concept to warrant publication (with one reviewer dissenting).  I recommend acceptance, with the view that the authors should put a substantial amount of work into improving the presentation of the paper based on the feedback that has emerged from the discussion before the camera ready is submitted (if accepted).
This paper adopts ViT in the VQ GAN framework replacing CNN, and achieves SOTA FID and IS scores. The empirical results are pretty impressive. It could benefit some practical applications.   The technical novelty is limited, but the tricks such as l2 normalization of codes are interesting.
This paper that defines a “Residual learning” mechanism as the training regime for variational autoencoder. The method gradually activates individual latent variables to reconstruct residuals.  There are two main concerns from the reviewers. First, residual learning is a common trick now, hence authors should provide insights on why residual learning works for VAE. The other problem is computational complexity. Currently, reviews argue that it seems not really fair to compare to a bruteforce parameter search. The authors’ rebuttal partially addresses these problems but meet the standard of the reviewers.  Based on the reviewers’ comments, I choose to reject the paper. 
This paper proposes a new generative model that has the stability of variational autoencoders (VAE) while producing better samples. The authors clearly compare their work to previous efforts that combine VAEs and Generative Adversarial Networks with similar goals.  Authors show that the proposed algorithm is a generalization of Adversarial Autoencoder (AAE) and minimizes Wasserstein distance between model and target distribution. The paper is well written with convincing results. Reviewers agree that the algorithm is novel and practical; and close connections of the algorithm to related approaches are clearly discussed with useful insights.  Overall, the paper is strong and I recommend acceptance.
This paper is proposed to address a novel but practical setting that the test set consists of both seen and unseen classes of the training set. To tackle the crucial challenge of distribution mismatch between the inlier and outlier features, the authors proposed a new method named ORCA by grouping similar instances to enlarge the class wise margin for de biasing. The experimental results on ImageNet have shown the proposed ORCA has significantly outperformed baselines in both inlier classification and outlier detection. The whole paper is written with clear logic and is easy to follow. Moreover, such a new setting may bring more inspiration to the community.
A line of work since 2016 has investigated learning NN based optimisers, which produce optimisation updates by processing loss/gradient info with neural networks. This paper tries to understand the learned dynamics of these NN based optimisers by linear approximation to the learned non linear dynamics. Visualisation of these approximations are shown on 3 optimisation problems: linear regression, Rosenbrock function, and a toy neural network classification problem, with the hope of covering different types of objective landscapes.   Reviewers agreed that the paper studies an important research question, which would interest researchers working on meta learning learning algorithms. However there are several major concerns raised by the reviewers: (1) the example optimisation problems are toyish, and (2) the paper does not explain very well the link between the visualised behaviour and the better optimisation results, i.e. it is unclear to the reviewers why the learned dynamics lead to better optimisation results.   While I am not too concerned about issue (1), I think issue (2) is a significant one, flagging that the clarity of the paper needs to be improved. Ultimately, the paper is motivated by the question "How is a learned optimizer able to outperform a well tuned baseline?", so a reader would expect some clear explanation towards answering this question. Also some reviewers are concerned about the fact that only the RNN based optimiser in Andrychowicz et al. (2016) is analysed; since there exists other forms of learned optimisers, focusing on studying only one type of them might lead to early conclusions that are not so accurate.
The paper proposes a using pixel adaptive convolutions to leverage semantic labels in self supervised monocular depth estimation. Although there were initial concerns of the reviewers regarding the technical details and limited experiments, the authors responded reasonably to the issues raised by the reviewers. Reviewer2, who gave a weak reject rating, did not provide any answer to the authors comments. We do not see any major flaws to reject this paper.
The paper proposes an architecture for semantic instance segmentation learnable from coarse annotations and evaluates it on two microscopy image datasets, demonstrating its advantage over baseline. While the reviewers appreciate the details of the architecture, they note the lack of evaluation on any of popular datasets and the lack of comparisons with baselines that would be more close to state of the art. The authors do not address this criticism convincingly. It is not clear, why e.g. the Cityscapes or VOC Pascal datasets, which both have reasonably accurate annotations, cannot be used for the validation of the idea. If the focus is on the precision of the result near the boundaries, then one can always report the error near boundaries (this is a standard thing to do). Note that the performance of the baseline models is far from saturated near boundaries (i.e. the errors are larger than mistakes of annotation).  At this stage, the paper lacks convincing evaluation and comparison with prior art. Given that this is first and foremost application paper, lacking some very novel ideas (as pointed out by e.g. Rev1), better evaluation is needed for acceptance.
The paper proposes a novel way to have weight decay like update rule. Empirically, the authors claim that it improves generalization when applied to momentum based optimizers and optimizers with coordinate wise learning rates.  This paper has been thoroughly discussed, both in public and private mode. The strength of this paper lies in the possible gain in generalization performance due the proposed change. The weaknesses are:   the very confusing and not scientific motivation of the proposed change   the experiments are not fully convincing  More in details, we all found the discussion on "stable" and "unstable" weight decay extremely confusing. The claim of the paper is that "stable" weight decay should be preferred over "unstable" one. However, to validate a scientific claim it is necessary to carry out an empirical or theoretical evaluation. The theoretical one is simply missing: a number of proposition and corollaries are stated with some simple mathematical facts completely disconnected from the optimization or generalization issues. As it is, removing these arguments would actually make the paper better. On the empirical side, there is no experiment that supports the simple claim that "the unstable weight decay problem may undesirably damage performance". Instead, what we see are experiments in which the modified update rule seem to perform better, but they don t actually show that "stability" or "instability" are the specific issues at play here. Indeed, any other explanation is equally valid and the experiments do not support any specific one, but rather they can only support the claim that the proposed algorithm might be better than some other optimization algorithms. The *specific reason* why this is happening is not clear.  Turning to the empirical evaluation, the discussion elicited the fact that, a part for CIFAR10, the experiments are carried out without tuning of the learning rates. Hence, it is difficult for us to even validate the claim of superiority of the method. I don t subscribe to the idea that a deep learning paper requires experiments on ImageNet to be valid. Yet, given that there is no supporting theory in this paper, the empirical evaluation should be solid and thorough.  For the above reasons, the paper cannot be published at ICLR.
The paper is on the borderline. A rejection is proposed due to the percentage limitation of ICLR.  
PROS: 1. Overall, the paper is well written, clear in its exposition and technically sound. 2. With some caveats, an independent team concluded that the results were "largely reproducible" 3. The key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models. 4. The implementation seems technically sound.  CONS: 1. The results were a bit over stated (the authors promise to correct) 2. Could benefit from more comparison with other approaches (e.g. RL)
 While reviewers find the ideas in the paper interesting, they also raise several major concerns. In particular, R1 and R4 find the claims of "invertible" and "lossless" to be potentially misleading. The bijective property is achieve on the first stage (L 1 layers) due to a sequence of one to one mappings, as is done in previous work (e.g. i RevNet)  so the novelty is limited.  As stated by R3,  since the paper is a combination of previous methods, the writing should be substantially improved to clarify what the real, new contributions are. The interpretation of the results (e.g. Figure 4) should also be better explained.
The paper proposes an approach to meta learning symmetries. While several approaches have recently emerged with similar goals, and sometimes greater convenience and empirical performance, the proposed approach has some interesting characteristics, such as changing properties of the architecture to extrapolate these symmetries. There was a quite a spread of opinions about the paper, the empirical results were not strong, and updates to the paper focused on helpful text additions, but did not substantively improve the evaluation or experiments. Notwithstanding, the paper is conceptually interesting, there are no major flaws, and there is sufficient support for it.
The authors introducing programming puzzles as a way to help AI systems learn about reasoning. The authors then propose a GAN like generation algorithm to generate diverse and difficult puzzles.  This is a very novel problem and the authors have made an interesting submission. However, at least 2 reviewers have raised severe concerns about the work. In particular, the relation to existing work as pointed by R2 was not very clear. Further, the paper was also lacking a strong empirical evaluation of the proposed ideas.  The authors did agree with most of the comments of the reviewers and made changes wherever possible. However, some changes have been pushed to future work or are not feasible right now.   Based on the above observations, I recommended that the paper cannot be accepted now. The paper has a lot of potential and I would strongly encourage a revised submission addressing the questions/suggestions made by the reviewers.
The paper proposes  to effectively learn representation of 3D data (point clouds/meshes) using a spherical GNN architecture over concentric spherical maps. A method for converting point clouds to concentric spherical images is also proposed. Evaluation is done via 3D classification tasks on rotated data.  Strengths:   Interesting novel method for learning 3D representations   Technically sound   Performs similarly to spherical CNNs and other STOA on the Modelnet40 dataset  Weaknesses:   Presentation of the work needs to be further improved such that it is easier for others to reproduce   More in depth experiments are needed to justify how much Spherical GNN improves over other STOA, particular given how classification accuracy is very similar to STOA.
All reviewers agree that the paper is to be rejected, provided strong claims that were not answered. In this form (especially with such a title) it could not be published (it is more of a technical/engineering interest).
This paper introduces a variant of DQN optimized for desktop environments to make large scale experiments more feasible for anyone.  This paper was close. The reviewers appreciated the effort and motivation, but in the end the reviewers all seemed to think that the paper was not ready. The main contribution is framed as making DQN training more feasible, but the reviewers expected the paper to show examples of what the workflow for another architecture would look like and ideally present results for domains beyond Atari. In addition, several reviewers thought the paper could be more precise about (1) ruling out speed differences due to hardware and low level software, and (2) contextualizing the speedups reported does 3x matter, what should we expect, etc.  This is certainly an interesting direction. The AC personally thinks that if the authors take some steps to address the points above this will be a great and potentially high impact paper.
This paper presents a differentiable neural architecture search method for GNNs using Gumbel softmax based gating for fast search. It also introduces a transfer technique to search architectures on smaller graphs with similar properties as the target graph dataset. The paper further introduces a search space based on GNNs message aggregators, skip connections, and layer aggregators. Results are presented on several undirected graph datasets without edge features on both node and graph classification.  The reviewers mention that the results are promising, but they unanimously agree that the paper does not meet the bar for acceptance in its current form. I tend to agree with the reviewers in that the effect of the individual contributions (search space vs. method vs. transfer) needs to be better disentangled and studied independently, and that it is unclear why selecting a single aggregation function out of many is important vs. choosing multiple ones at the same time such as in PNA [1] as pointed out by R1. This should be carefully studied going forward. Lastly, all reviewers agreed that the proposed transfer method requires more detailed experimental validation and motivation.  [1] Corso et al.: Principal Neighbourhood Aggregation for Graph Nets (NeurIPS 2020)
The topic of universal adversarial perturbation is quite intriguing and fairly poorly studied and the paper provides a mix of new insights, both theoretical and empirical in nature. However, the significant presentation issues make it hard to properly understand and evaluate them. In particular, the theoretical part feels rushed and not sufficiently rigorous, and it is unclear why focusing on the case of equivariant network is crucial. Also, it would be useful if the authors put more effort in explaining how their contributions fit into the context of prior work in the area.  Overall, this paper has a potential of becoming a solid contribution, once the above shortcomings are addressed.
The paper discusses smooth market games and demonstrate the merit of the approach.    The reviewers agree on the quality of the paper, and the comments have been addressed well by the authors. 
The paper proposes a novel approach to interfacing robots with humans, or rather vv: by mapping instructions to goals, and goals to robot actions.   A possibly nice idea, and possibly good for more efficient learning.  But the technical realisation is less strong than the initial idea.  The original idea merits a good evaluation, and the authors are strongly encouraged to follow up on this idea and realise it, towards a stronger publication.  It be noted that the authors refrained from using the rebuttal phase.
This paper presents a meta learning approach which relies on a learned prior over neural networks for different tasks.   The reviewers found this work to be well motivated and timely. While there are some concerns regarding experiments, the results in the miniImageNet one seem to have impressed some reviewers.   However, all reviewers found the presentation to be inaccurate in more than one points. R1 points out to "issues with presentation" for the hierarchical Bayes motivation, R2 mentions that the motivation and derivation in Section 2 is "misleading" and R3 talks about "short presentation shortcomings".  R3 also raises important concerns about correctness of the derivation. The authors have replied to the correctness critique by explaining that the paper has been proofread by strong mathematicians, however they do not specifically rebut R3 s points. The authors requested R3 to more specifically point to the location of the error, however it seems that R3 had already explained in a very detailed manner the source of the concern, including detailed equations.   There have been other raised issues, such as concerns about experimental evaluation. However, the reviewers  almost complete agreement in the presentation issue is a clear signal that this paper needs to be substantially re worked. 
The paper formally studies the problem of partial identifiability when inferring a reward function from a given data source (e.g., expert demonstrations or trajectory preferences). To formally characterize this ambiguity in a data source, the paper proposes considering the infinite limit data regime, which bounds the reward information recoverable from a source. Furthermore, this ambiguity is then studied in the context of different downstream tasks, as recovering an exact reward function may not be necessary for a given task. The paper is primarily theoretical, and the results provide a unified view of the problem of partial identifiability in reward learning for different sources and downstream tasks.  Overall, the reviewers acknowledged the importance of the problem setting and found the results promising. There is quite a bit of spread in the reviewers  final assessment of the paper with ratings 8, 8, 3, 3 (note: one of the reviewers with rating 3 has a low confidence). The authors  responses did help in discussions; however, a few of the concerns, as raised by reviewers, still remained. The key issues are related to the general accessibility of the paper and the lack of concrete examples to highlight the proposed theoretical framework. At the end of the discussions,  several reviewers (including those with an overall positive rating) shared concerns about the paper s accessibility.  With this, unfortunately, the paper stands as borderline. Nevertheless, this is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers  feedback when preparing a future revision of the paper.
The paper proposes to bring together a GAN, a differentiable renderer, and an inverse graphics model. This combined model learns 3D aware image analysis and synthesis with very limited annotation effort (order of minutes). The results look impressive, even compared to training on a labeled dataset annotation of which took several orders of magnitude more time.  The reviewers point out the novelty of the proposed system and the very high quality of the results. On the downside, R2 mentions that the model appears over engineered and some important experimental results are missing. The authors’ response addresses these concerns quite well.  Overall, this is a really strong work with compelling results, taking an important step towards employing generative models and neural renderers “in the wild”. I think it can make for a good oral.  
This paper empirically studies various design choices in offline model based RL algorithms, with a focus on MOPO (Model based Offline Policy Optimization). Among the key design choices is the uncertainty measure used in MOPO that provides an (approximate) lower bound on the performance, the horizon rollout length, and the number of model used in ensemble.  The reviewers are positive about the paper, found the experiments thorough, and the results filling a gap in the current literature. They have raised several issues in their reviews, many of which are addressed in the rebuttal and the revised paper. I would like to recommend acceptance of the paper. Also since the results of this work might be of interest to many researchers working on model based RL, I also recommend a spotlight presentation for this work.  I have some additional comments:  (1) The paper studies the correlation of uncertainty measures with the next state MSE, with the aim of showing which one has a higher correlation. The underlying assumption is that the next state MSE is the gold standard that we should aim for.  If we go back to the MOPO paper, we see that to define an uncertainty penalized reward, we need an upper bound on the absolute value of G(s, a), which is the difference between the expected value of the value function at the next state according to the true model and the estimated model.  If we assume that the value function belongs to the Lipschitz function class w.r.t. a metric d, this upper bound is proportional to the 1 Wasserstein distance between the true next state distributions and the model s distribution. If the dynamics is deterministic, 1 Wasserstein distance becomes the $d( T(s, a), \hat{T}(s,a) )$. If the distance d is the Euclidean distance, this becomes the squared error.  Therefore, the squared error makes sense for deterministic dynamics, and it only provides an upper bound of $|G(s, a)|$. If the environment is not deterministic, the squared error may not be a reasonable gold standard anymore to compare the correlation of various uncertainty measures with.  The paper introduces a generic MDP framework, but does not mention anything about its focus on MBRL for deterministic environments until the last sentence of its conclusion. Please clarify this in your camera ready paper.  (2) The experiments are conducted using 3 or 4 seeds. Although this is the common practice in the deep RL community, it is too small. Standard deviations in Tables 1, 2, ... are computed with 3 seeds, which would be cringeworthy to statisticians and empirical scientists. I encourage the authors to increase the number of independent random experiments to make their results more powerful.
All of the reviewers agree that this paper is well written, and provides sound theoretical analyses and comprehensive empirical evaluations. Overall, this paper makes a useful contribution in the direction of individual fairness. The authors have also addressed the concerns raised by the reviewers in their response.
This paper presents a benchmarking suite, primarily targeting the domain of evolutionary style optimization algorithms, and an effective heuristic algorithm selection procedure ABBO.  The reviewers seemed quite split in their reviews with significant variance, particularly with one outlier review (9) lifting up the average.  They all felt that there was significant value in the work presented and that the benchmark could be useful for designing and evaluating new methods.  However, there were concerns regarding details about the contributions (e.g. a detailed description of ABBO and which contributions to the suite were novel vs obtained from other benchmarks), the relevance of this work to the ICLR community, and choice of algorithms presented (i.e. not SOTA).    In general, this seems like a useful contribution for the evolutionary algorithm community but this paper seems off topic from the conference.  Certainly optimization is important and of interest to the community.  However, there is no machine learning component to the technical contribution of this paper, and it ignores many of the contributions to black box optimization within this community (see e.g. the citations from AnonReviewer1, and the literature on surrogate based black box optimization   i.e. Bayesian optimization).  The RL optimization problems are somewhat relevant, but AnonReviewer1 raises concerns about the reporting of those results and the representation of the current literature.  There is an algorithm proposed in this work, but it s largely heuristic and no comparison is given to state of the art portfolio optimization algorithms from the machine learning community (e.g. P3BO from Angermueller et al., ICML 2019).  A venue such as GECCO seems much more well suited to this work.
The paper proposes an original idea for training a generative model based on an objective inspired by a VAE like evidence lower bound (ELBO), reformulated as two KL terms, which are then approximately optimized by two GANs. They thus use implicit distributions for both the posterior and the conditional likelihood. The idea is original and intriguing. But reviewers and AC found that the paper currently suffered from the following weaknesses: a) The presentation of the approach is unclear, due primarily to the fact that it doesn t throughout unambiguously enough separate the VAE like ELBO *inspiration*, from what happens when replacing the two KL terms by GANs, i.e. the actual algorithm used. This is a big conceptual jump that would deserve being discussed and analyzed more carefully and thoroughly. b) Reviewers agreed that the paper does not sufficiently evaluate the approach in comparative experiments with alternatives, in particular its generative capabilities, in addition to the provided evaluations of the learned representation on downstream tasks. Reviewers did not reach a clear consensus on this paper, although discussion led two of them to revise their assessment score slightly towards each other s. One reviewer judged the paper currently too confusing (point a) putting more weight on this aspect than the other reviewers.  Based on the paper and the review discussion thread, the AC judges that while it is an original, interesting and potentially promising approach, its presentation can and should be much clarified and improved. 
Pros: + Clearly written paper.  Cons:   Limited empirical evaluation: paper should compare to first order methods with well tuned hyperparameters, since the block Hessian free hyperparameters likely were well tuned, and plots of convergence as a function of time need to be included.   Somewhat limited novelty in that block diagonal curvature approximations have been used before, though the application to Hessian free optimization is new.  The reviewers liked the clear description of the proposed algorithm and well structured paper, but after discussion were not prepared to accept it primarily because (1) they wanted to see algorithmic comparisons in terms of convergence vs. time in addition to the convergence vs. updates that were provided; (2) they wanted more assurance that the baseline first order optimizers had been carefully tuned; and (3) they wanted results on larger scale tasks. 
The paper addresses the problem of batch normalization (BN) in federated learning, which is of great interest to the community including practitioners.  The proposed method here simply excludes the BN parameters from the aggregation, and evolves them locally.  As a main contribution, reviewers particularly liked the solid justification of the proposed scheme, both with substantial theory and extensive experiments. Presentation style can be slightly improved, the usage at test time can be clarified more, and some references mentioned by R3 should be added, but this overall does not affect the strong level of contributions present in the work, and the discussion phase with the authors was already constructive.
The paper tackles the problem of missing data in centralized training multi agent RL approaches. The authors propose 1) using generative adversarial imputation networks for imputing missing data and 2) discarding training data where data from multiple consecutive timesteps is missing.  Reviewers agreed that the problem of missing data in multi agent RL is interesting. At the same time, several reviewers shared two main concerns about the experimental evaluation: * The lack of comparisons to baselines other than MADDPG, especially decentralized critic approaches. * The lack of experiments on non toy domains such as SMAC.  The author response did not sufficiently address these concerns leaving the reviewers in agreement that the paper should not be accepted without these additional experiments.
The paper presents a promising approach for continual learning with no access to data from the previous tasks. For learning the current task, the authors propose to find an optimal structure of the neural network model first (select either to reuse, adapt previously learned layers or to train new layers) and then to learn its parameters.   While acknowledging the originality of the method and the importance of the problem that it tries to address, all reviewers and AC agreed that they would like to see more intensive empirical evaluations and comparisons to state of the art models for continual learning using more datasets and in depth analysis of the results – see details comments of all reviewers before and after rebuttal.  The authors have tried to address some of these concerns during rebuttal, but an in depth analysis of the results (evaluation in terms on accuracy, efficiency, memory demand) using different datasets still remains a critical issue.  Two other requests to further strengthen the manuscript: 1) an ablation study on the three choices for structural learning (R3), and especially the importance of ‘adaptation’ (R3 and R1) The authors have tried to address this verbally in their responses but a proper ablation study would be desirable to strengthen the evaluation. 2) Readability and proofreading of the manuscript is still unsatisfying after revision. 
This paper takes inspiration from the brain to add a behavioral module to a deep reinforcement learning architecture. Unfortunately, the paper s structure and execution lacks clarity and requires a lot more work: as noted by reviewers, the link link between motivation and experiments is too fuzzy and their execution is not convincing.
This paper studies the problem of unsupervised inpainting occluded areas in spatiotemporal sequences and propose a GAN based framework which is able to complete the occluded areas given the stochastic model of the occlusion process. The reviewers agree that the problem is interesting, the paper is well written, and that the proposed approach is reasonable. However, after the discussion phase the critical point raised by AnonReviewer1 remains: in principle, when applying different corruptions in each step, the model is able to see the entire video over the duration of the training. This coupled with the strong assumptions on the mask distribution makes it questionable whether the approach should be considered unsupervised. Given that the results of the supervised methods significantly outperform the unsupervised ones, this issue needs to be carefully addressed to provide a clear and convincing selling point. Hence, I will recommend rejection and encourage the authors to address the remaining issues (the answers in the rebuttal are a good starting point).
The paper studies efficient strategies for selection of pre trained models for a downstream task. The main concerns consistently raised by the reviewers were limited methodological novelty, insufficient experimental analysis, unclear findings, and positioning of the paper with respect to related work that was ignored in the initial version. After the author response, R4 raised the score to borderline accept (still indicating the paper is weak without proper comparisons with other methods), whereas all other reviewers remained negative. The paper does have merits, as the methods are simple, and the problem is very practical (and somewhat understudied). However, the AC agrees with the majority that the paper is not ready for ICLR. The novelty is limited and the paper would benefit from more experiments, such as comparisons with simple baselines like early stopping as indicated by R1 and R3, and other methods such as Task2vec which address the same problem. The authors are encouraged to revise the paper according to the reviewers comments and submit it to another top conference.
The focus of the submission is kernel ridge regression in the distributed setting. Particularly, the authors present optimal learning rates under this assumption both in expectation and in probability, while they relax previous restrictions on the number of partitions taken. The effectiveness of the approach is demonstrated in synthetic and real world settings.  As summarized by the reviewers, the submission is well organized and clearly written, the authors focus on an important problem, they present a fundamental theoretical contribution which also has clear practical impact. As such the submission could be of interest to the ICLR and ML community.
This paper considers the problem of sequential decision making through the lens of submodular maximization. I read the paper myself and found the idea quite appealing and interesting. The authors also make a very effective rebuttal and brought a borderline paper into a clear accept. 
This paper addresses the problem of visual object navigation by defining a novel visual transformer architecture, where an encoder consisting of a pretrained object detector extracts objects (i.e. their visual features, position, semantic label, confidence) that will serve as keys in an attention based retrieval mechanism, and a decoder computes global visual features and positional descriptors as a coarse feature map. The visual transformer is first pretrained (using imitation learning) on simple tasks consisting in moving the state less agent / camera towards the target object. Then an RL agent is defined by adding an LSTM to the VTNet and training it end to end on the single room subset of the AI2 Thor environment where it achieves state of the art performance.  After rebuttal, all four reviewers converged on a score of 6. The reviewers praised the novelty of the method, extensive evaluation with ablation studies, and the SOTA results. Main points of criticism were about clarity of writing and some explanations (which the authors improved), using DETR vs. Faster R CNN, and the relative simplicity of the task (single room and discrete action space). There were also minor questions, a request for more recent transformer based VLN bibliography, and a request for a new evaluation on RoboThor. One area of discussion   where I empathise with the authors   was regarding the difficulty of pure RL training of transformer based agents and the necessity to pre train the representations.  Taking all this into account, I suggest this paper gets accepted. 
All the reviewers rate the paper above the bar. They like the experiment results and think the proposed latent space editing approach makes intuitive sense. While several weakness points were raised, including a lack of continuous editing comparison and sometimes vague descriptions, they were not considered major to reject the paper. After consolidating the reviews and rebuttal, the AC agrees with the reviewer assessment and recommends accepting the paper.
This paper studies the implicit regularization of the gradient descent in homogeneous and shows that when the training loss falls below a threshold, then the smoothed. This study generalizes some of the earlier related works by relying on weaker assumptions. Experiments on MNIST and CIFAR 10 are provided to backup the theoretical findings of the paper.  R2 had some concern about one of the assumptions in this work (A4). While authors admitted that (A4) may not hold for all neural networks and all datasets, they stressed that this assumptions is reasonable when the network is overparameterized and can perfectly fit the training data. Overall, all reviewers are very positive about this submission and find a valuable step toward understanding implicit regularization. 
The authors in the paper perform empirical studies to investigate the trade off between accuracy and privacy (measured by membership inference attacks) in deep ensembles. They find out that the level of correct agreement among models is the most dominant factor that improves the performance of MI attacks in deep ensembles. They support their claim by visualizing the distribution shifts of correct agreement in train/test examples. They further implement a variety of existing defenses, such as differential privacy and regularizations, etc., to investigate the ​​effects of existing defense mechanisms. Overall, the paper is well written and the experiments are well conducted. While these findings are interesting, they do not reveal something useful or surprising about deep ensemble learning. It is not clear what the contribution is to the membership inference attack literature and private machine learning literature. they do not propose anything new to make the attacks stronger or defenses stronger.
This paper introduces two regularizers that are meant to improve out of domain robustness when used in the fine tuning of pretrained transformers like BERT. Results with ANLI and Adversarial SQuAD are encouraging.  Pros:   New method with concrete improvements in several difficult task settings.   New framing of adversarial generalization.  Cons:   The ablations that are highlighted in the main paper body don t do a good job of isolating the specific new contributions. (Though the appendix provides enough detail that I m satisfied that the main empirical contribution is sound.)   Reviewers found the theoretical motivation very difficult to follow in places.
The paper provides a theoretical analysis of graph neural networks, as the number of layers goes to infinity. For the graph convolutional network, they relate the expressive power of the network with the graph spectra. In particular for Erdos Renyi graphs, they show that very deep graphs lose information, and propose a new weight normalization scheme based on this insight.  The authors responded well to reviewer comments. It is nice to see that the open review nature has also resulted in a new connection. Unfortunately one of the reviewers did not engage further in the discussion with respect to the author rebuttals.  Overall, the paper provides a nice theoretical analysis of a widely used graph neural network architecture, and characterises its behaviour on a popular class of graphs. The fact that the theory provides a new approach for weight normalization is a bonus.
The authors study the problem of video classification and propose a new module which promises to increase accuracy while keeping the computational overhead low. The main idea is not to share the spatial convolution weights over different time steps, but allow some modulation based on pooled local and global frame descriptors. The resulting module can be used as a drop in replacement for spatial convolutions in existing models and yields competitive performance on multiple video action recognition and localisation benchmarks.  The reviewers appreciated this challenging setting and the simplicity of the main idea. They found that the paper was clearly written, well organised, and easy to follow. The reviewers raised some issues in connections with related work and the empirical evaluation which were successfully resolved during the discussion phase.  Given that computational efficiency remains as one of the most challenging topics in video understanding, I believe that this technique will be relevant for the larger video understanding community. I strongly suggest that the authors incorporate the feedback received during the discussion, especially the GFLOPS vs accuracy plots, and further clarify the relationship to existing work.
This submission investigates the properties of the Jacobian matrix in deep learning setup. Specifically, it splits the spectrum of the matrix into information (large singulars) and ``nuisance (small singulars) spaces. The paper shows that over the information space learning is fast and achieves zero loss. It also shows that generalization relates to how well labels are aligned with the information space.  While the submission certainly has encouraging analysis/results, reviewers find these contributions limited and it is not clear how some of the claims in the paper can be extended to more general settings. For example, while the authors claim that low rank structure is suggested by theory, the support of this claim is limited to a case study on mixture of Gaussians. In addition, the provided analysis only studies two layer networks. As elaborated by R4, extending these arguments to more than two layers does not seem straighforward using the tools used in the submission. While all reviewers appreciated author s response, they were not convinced and maintained their original ratings. 
The paper combines the ideas of VAT and Bad GAN, replacing the fake samples in Bad GAN objective with VAT generated samples. The motivation behind using the K+1 SSL framework with VAT examples remains unclear, particularly in the light of Prop. 2 which shows smoothness of classifier around the unlabeled examples is enough (which VAT already encourages). R2 and R3 have raised the point of limited insight and lack of motivation behind combining VAT and Bad GAN objectives in this way. R2 and R3 are also concerned about the empirical results which show only marginal improvements over VAT/BadGAN in most settings.   AC feels that the idea of the paper is interesting but agrees with R2/R3 that the proposed objective is not motivated well enough (what is the precise advantage of using K+1 SSL formulation with VAT examples?). The paper really falls on the borderline and could be improved if this point is addressed convincingly. 
Thanks for your detailed replies to the reviewers, which helped us a lot to clarify several issues. Although the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited. Given the high competition of ICLR2020, this paper is still below the bar unfortunately.
Inspired by dendritic nonlinearity, this paper extends previous work on PLRNN/PWL dynamical system modeling by Durstewitz s group. The extension replaces the ReLU nonlinearity with a linear combination of ReLUs. This preserves the theoretical properties of PLRNN, however, the dimensionality of the latent dynamics remains the same, increasing the expressive power of prior PLRNNs. I (area chair) actually read this paper since not all reviewers provided high quality reviews and one key reviewer is having a personal emergency. Though I appreciate the premise, detailed numerical evaluations, and the inference approach, the novelty is marginal and I do not buy the theoretical advantage of this class of models as presented (see below). Therefore I cannot recommend this paper to appear at ICLR at this time.  Some additional weaknesses that reviewers did not point out: 1. Dendritic nonlinearity is summarized as a point nonlinearity; It lacks the interesting phenomena of dendrites such as nonlinear summation and calcium spikes with its own internal dynamics. 2. The many analytical properties of PLRNN may sound nice on paper, but very impractical. To search for the fixed points and cycles, the amount of required computation exponentially increases as the number of neurons and cycle length increases. In addition the boundary effects cannot always be ignored. In general detailed analysis can become quite non trivial quickly, e.g., https://arxiv.org/abs/2109.03198 3. High dimensional PLRNN that approximates a low dimensional dynamical system due to model mismatch won t have the same topological stability structures. Theoretical analysis of higher dimensional DS may be very misleading.
The reviewers were unanimous in their assessment that the paper was not ready for publication in ICLR.  Their concerns included:    lack of novelty over Niepert, Ahmed, Kutzkov, ICML 2016    The approach learns combinations of graph kernels and its expressive capacity is thus limited    The results are close to the state of the art and it is not clear whether any improvement is statistically significant.  The authors have not provided a response to these concerns.
This paper develops a technique to provide both privacy and robustness at the same time using differential privacy.  Unfortunately the paper in its current form does not have meaningfully interpretable security or privacy claims. The reviewers point at a number of these flaws that the authors do not address to the satisfaction of the reviewers, but there are a few others as well.   What is actually private, at the end of this whole procedure? If the   actual "pretrained classifier" is not made private, then what s the   purpose of the entire privacy setup in this paper? Why does the denoiser   need to be private if the classifier isn t?   The proof of Lemma 1 appears incorrect. The proof in Appendix E says that   Equation 10 is true, but this sweeps all of the remaining Taylor series   terms under the rug and doesn t deal with them. How are they handled?   In Figure 4(a), what does it even mean to have a "FGSM privacy budget   epsilon"? Or a "MIM privacy budget epsilon"? A privacy budget is almost   always something defined with respect to the *training data privacy*,   how does this relate to the attack in this paper?   How does this paper compare prior *canonical* defenses, both on the   robustness and privacy side? In particular, comparisons to adversarial   training on the robustness side, and some recent DPSGD result on the   privacy side?
The paper proposes a new defense against adversarial attacks on graphs using a reweighting scheme based on Ricci flow. Reviewers highlighted that the paper introduces interesting ideas and that the use of Ricci curvature/flow is a novel and promising contribution. Reviewers also recognized that the paper has significantly improved after rebuttal and clarified some aspects of their initial reviews.  However, there exist still concerns around the current version of the manuscript. In particular, important aspects of the method and algorithm, as well as some design choices are currently unclear. This includes evaluating and discussing robustness, training method, and practicality/improvements in real world scenarios. I agree with the majority of the reviewers that the current version requires an additional revision to iron out the aforementioned issues. However, I also agree with the reviewers that the overall idea is promising and I d encourage the authors to revise and resubmit their work with considering the feedback from this round of reviews.
This paper proposes a variant of stochastic gradient descent that parallelizes the algorithm for distributed training via delayed gradient averaging. While the algorithm (DaSGD) proposed is sensible and seems to work, it also seems to miss a lot of related work. As pointed out by one of the reviewers, the class of asynchronous decentralized methods already seem to cover the space of DaSGD, and it s not clear how DaSGD differs from the existing methods in this space. As a result of this lack of comparison to related work, the reviewers recommended that the paper not be accepted at this time, and this evaluation was not challenged by an author response. I agree with this consensus.
This paper presents a new generative modeling approach to transform between data domains via a neuron editing technique. The authors address the scenario of source to target domain translation that can be applied to a new source domain. While the reviewers acknowledged that the idea of neuron editing is interesting, they have raised several concerns that were viewed by AC as critical issues: (1) given the progress that have been made in the field, an empirical comparison with SOTA GANs models is required to assess the benefits/competitiveness of the proposed approach   see R1’s comments, also [StarGAN by Choi et al, CVPR 2018], (2) the literature review is incomplete and requires a major revision   see R1’s and R3’s suggestions, also [CYCADA by Hoffman et al, ICML 2018], (3) presentation clarity   see R1’s and R2’s comments. AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews are useful for improving and revising the paper. 
All reviewers express concerns, such as about the presentation, the situation of the paper w.r.t. prior work, the experimental evaluation etc., and recommend rejection.
This paper tackles a very important problem: evaluating natural language generation. The paper presents an overview of existing unsupervised metrics, and looks at how they correlate with human evaluation scores. This is important work and the empirical conclusions are useful to the community, but the datasets used are too limited and the authors agree it would be better to use newer bigger and more diverse datasets suggested by reviewers for drawing more general conclusions. This work would indeed be much stronger if it relied on better, more recent datasets; therefore publication as is seems premature. 
  The paper is overall difficult to read and would benefit from a revised presentation.   The practical relevance of the recovery conditions and algorithmic consequences of the work is not sufficiently clear or  convincing.
Although originally all reviewers were leaning towards rejection, the authors have done a very good job at addressing their concerns, significantly strenghtening the paper. There is now a consensus towards weak acceptance, with the exception of R3. However, I have decided to ignore R3 s review for the following reasons:   The original review was way too short and uninformative   R3 did not reply to the authors  request for more constructive feedback   R3 did not reply to my own request (private email)  That being said, even if other reviewers decided to increase their score after the rebuttal and discussion period, none of them was particularly enthusiastic about it: this remains a borderline paper combining ideas that, although promising, are not particularly original. At this time it falls slightly short off meeting the bar for an ICLR publication. I do believe that combining ideas from the RL and evolutionary research communities is a promising research direction, and I encourage the authors to take into account the reviewers  remaining comments to polish their paper (in particular, adding even stronger empirical results, and ensuring the key take aways are clearly communicated).
This paper explores several embedding models (Skip gram, BERT, XLNet) and describes a framework for comparing, and in the end, unifying them.  The framework is such that it actually suggests new ways of creating embeddings, and draws connections to methodology from computer vision.  One of the reviewers had several questions about the derivations in your paper and was worried about the paper s clarity.  But all of the reviewers appreciated the contributions of the paper, which joins multiple seemingly disparite models under into one theoretical framework.  The reviewers were positive about the paper, and in particular were happy to see the active response of authors to their questions and willingness to update the paper with their suggested improvements.
The paper describes a new offline meta RL technique that addresses the distributional shift problem with a self supervised online exploration phase where reward labels are not available.  The framework is novel and interesting.  The authors addressed many concerns of the reviewers.  However, the additional experiments raised additional questions.  For instance, why does meta BC perform so well, even better than the proposed method without online data, and other baselines seem not to work at all?  In the discussion, the reviewers expressed concerns about the experimental results in the case of changing dynamics.  Those experiments are questionable since the proposed method only considers the reward information to deal with different dynamics.  Finally, an important question regarding SMAC remains unanswered: how much does the proposed method depend on the quality of the offline dataset and the quality of the reward decoder? Overall, the work is promising and the authors are encouraged to continue their work by addressing the reviewers concerns.
The reviews were a bit mixed: on one hand, by combining and adapting existing techniques the authors obtained some interesting new results that seem to complement existing ones; on the other hand, there is some concern on the novelty and on the interpretation of the obtained results. Upon independent reading, the AC agrees with the reviewers that this paper s presentation can use some polishing. (The revision that the authors prepared has addressed some concerns and improved a lot compared to the original submission.) Overall, the analysis is interesting but the significance and novelty of this work require further elaboration. In the end, the PCs and AC agreed that this work is not ready for publication at ICLR yet. Please do not take this decision as an under appreciation of your work. Rather, please use this opportunity to consider further polishing your draft according to the reviews. It is our belief that with proper revision this work can certainly be a useful addition to the field.   Some of the critical reviews are recalled below to assist the authors  revision:  (a) The result in Theorem 4.1 needs to be contrasted with a single machine setting: do we improve the convergence rate in terms of T here? do we improve the constants in terms of L and M here? What is the advantage one can read off from Theorem 4.1, compared to a single machine implementation? How should we interpret the dependence of (optimal) H on r and lambda_2?   (b) The justification for $T \geq n^4$ is a bit  weak and requires more thoughts: one applies distributed SGD because n is large. What happens if T does not satisfy this condition in practice, as in the experiments?  (c) Extension 1 perhaps should be more detailed as its setting is much more realistic than Theorem 1. One could use Theorem 1 to motivate and explain some high level ideas but the focus should be on Extension 1 3. In extension 2, the final bound seems to be exactly the same as in Theorem 1, except a new condition on T. Any explanations? Why asynchronous updates only require a larger number of interactions but retain the same bound? These explanations would make the obtained theoretical results more accessible and easier to interpret.
This paper proposes to split the GNN operations into two parts and study the effects of each part. While two reviewers are positive about this paper, the other reviewer R1 has raised some concerns. During discussion, R1 responded and indicated that his/her concerns were not addressed in author rebuttal. Overall, I feel the paper is borderline and lean towards reject.
This paper provides a surprising result: that randomization and FGSM can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc. This paper produced a fair bit of controversy among both the community and the reviewers to the point where there were suggestions of bugs, evaluation problems, and other issues leading to the results. In the end, the authors released the code (and made significant updates to the paper based on all the feedback). Multiple reviewers checked the code and were happy. There was an extensive author response, and all the reviewers indicated that their primary concerns were address, save concerns about the sensitivity of step size and the impact of early stopping.  Overall, the paper is well written and clear. The proposed approach is simple and well explained. The result is certainly interesting, and this paper will continue to generate fruitful debate. There are still things to address to improve the paper, listed above. I strongly encourage the authors to continue to improve the work and make a more concerted effort to carefully discuss the impacts of early stopping. 
The paper presents a novel unit making the networks intrinsically more robust to gradient based adversarial attacks. The authors have addressed some concerns of the reviewers (e.g. regarding pseudo gradient attacks) but experimental section could benefit from a larger scale evaluation (e.g. Imagenet).
This paper proposes Perceiver IO, a general neural architecture that handles general purpose inputs and outputs. It operates directly in the raw input domains, and thus does away with modality specific architecture components. The paper contains extensive experiments showing the capabilities of this architecture in different domains. The paper received very positive reviews from all reviewers. Some concerns included a need for additional details such as a missing task from GLUE, FLOPs comparisons to past works, nomenclature for the versions of Perceiver IO, etc. These concerns were well addressed by the authors. Others concerns by reviewers were the lack of experiments in a multi task setting. However, it was acknowledged by the authors and reviewers that this is an open area of research and is a good fit for future work. Given this high quality submission, strong reviews and a very positive discussion amongst authors and reviewers, I recommend accepting this paper.
This paper presents a method for using byte level convolutional networks for building text based autoencoders.  They show that these models do well compared to RNN based methods which model text in a sequence.  Evaluation is solely based on byte level prediction error.   The committee feels that the paper would have been stronger if evaluation was on some actual task (say summarization, Miao and Blunsom, for example) and show that it works as well as RNNs, the paper would have been stronger.
The paper proposes a method to perform self supervised model ensembling by learning representations directly through gradient descent at inference. The effectiveness is evaluated by k nearest neighbors accuracy.  The reviewers agreed that the paper studies an important and interesting problem of leveraging model ensembling for self supervised learning, which could improve both the performance and robustness of the learned representations. However, the reviewers also agreed that there were issues with the soundness of the empirical evaluation, which was a key reason for rejection.
This paper proposes a new formulation of the non local block and interpret it from the graph view. The idea is interesting and the experimental results seems to be promising.  Reviewer has two major concerns. The first is the presentation, which is not clear enough. The second is the experimental design and analysis. The authors add more video dataset in the revision, but still lack comprehensive experimental analysis for video based applications.   Overall, the idea of non local block from graph view is interesting. However, the presentation of the paper needs further polish and thus does not meet the standard of ICLR 
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
This paper presents new analysis for self supervised learning. All reviewers are positive about some new perspectives of the analysis. However, some serious concerns have been raised about the rigorousness and the presentation clarity. The paper would be significantly improved, if the authors could address the concerns.
This paper proposed a new graph matching approach. The main contribution is a Hungarian attention mechanism, which dynamically generates links in computational graph. The resulting matching algorithm is tested on vision tasks.  The main concern of reviews is that the general matching algorithm is only tested on vision tasks. The authors partially addressed this problem by providing new experimental results with only geometric edge features. Other comments of Blind Review #2 are about some minor questions, which have also been answered by the authors.  Overall, this paper proposed a promising graph match approach and I tend to accept it.  
The paper reports experiments where a LSTM language model is pretrained on a large corpus of reviews, and then the produced representation is used within a classifier on a number of sentiment classification datasets.  The relative success of the method is not surprising. The novelty is very questionable, the writing quality is mixed (e.g., typos, the model is not even properly described). There are many gaps in evaluation (e.g., from the intro it seems that the main focus is showing that byte level modeling is preferable to more standard set ups   characters / BPE / words). However, there are (almost) no experiments supporting this claim. The same is true for the  sentiment neuron : its effectiveness is also not properly demonstrated. In general, the results are somewhat mixed.  Pros:   good results on some datasets Cons:   limited novelty   some claims are not tested / issues with evaluation   writing quality is not sufficient / clarity issues   Overall, the reviewers are in agreement that the paper does not meet ICLR standards.  
This work proposed to detect backdoor in a black box manner, where only the model output is accessible.   Most reviewers think it is a valuable task, and this work provides a novel perspective of using adversarial perturbation to diagnosis the backdoor. Some theoretical analysis for linear models and kernel models are provided. There is still huge gap to analyze the DNN model. But on the other side, it provides some insight to understand the proposed method and could inspire further studies.   Besides, since there have been many advanced backdoor attack methods, and many more are coming out, I am not sure that the proposed detection criteria is well generalizable, considering only some typical attack methods are tested. However, I think the studied problem is valuable, and the presented analysis is inspired for future works. Thus, I recommend for accept.
This paper presents an "attack"—TextExplanationFooler (TEF)—that adversarially (minimally) edits inputs such that the resultant attribution assigned by common explanation methods changes substantially, while the prediction does not. This is an extension of methods and results in vision to NLP. The authors find that all methods are vulnerable to this attack.   Reviewers agreed that the demonstration that perturbation attacks used in vision are applicable in NLP is interesting and may lead to follow up work. The paper would benefit from technical clarifications in several places (see R2), which were largely resolved in discussion.
Generally solid engineering work but a bit lacking in terms of novelty and some issues with clarity. At the end of the day the empirical gains are not sufficient for acceptance   the results are state of the art relative to published work, but not in the top 10 based on the official leaderboard (not even at time of submission). Since the technical contributions are small and the engineering contributions have been made obsolete by concurrent work, I suggest rejection.
This work extends Leaky Integrate and Fire (LIF)  by proposing a recurrent version. All reviewers agree that the work as submitted is way too preliminary. Prior art is missing many results, presentation is difficult to follow and incomplete and contains errors. Even if these concerns were addressed, the benefit of the proposed method is unclear. Authors have not responded. We thus recommend rejection.
This manuscript proposes a gradient based learning scheme for non differentiable and non decomposable metrics. The key idea is to optimize a soft predictor directly (instead of aiming for a deterministic predictor), which results in a differentiable loss for many of these metrics. Theoretical results are provided which describe the performance of this approach.  The reviewers and ACs noted weakness in the original submission related to the clarity of the presentation and novelty as related to already published work. There was also a concern about the usefulness the main theoretical results due to asymptotic assumptions. The manuscript would be significantly strengthened if the reliance on infinite sample sizes is resolved, or sufficient empirical evidence is provided which suggests that the asymptotic issues are not practically significant.
As the reviewers said, it is unclear what the main contribution of the paper is.
All three reviewers raised the issues that (a) the problem tackled in the paper was insufficiently motivated, (b) the solution strategy was also not sufficiently motivated and (c) the experiments had serious methodological issues.
This paper focuses on studying neural network based denoising methods. The paper makes the interesting observation that most existing denoising approaches have a tendency to overfit to knowledge of the noise level. The authors claim that simply removing the bias on the network parameters enables a variety of improvements in this regard and provide some theoretical justification for their results. The reviewers were mostly postive but raised some concerns about generalization beyond Gaussian noise and not "being very well theoretically motivated". These concerns seem to have at least partially been alleviated during the discussion period. I agree with the reviewers. I think the paper looks at an important phenomena for denoising (role of variance parameter) and is well suited to ICLR. I recommend acceptance. I suggest that the authors continue to further improve the paper based on the reviewers  comments.
The paper develops an unrolled version of the PALM algorithm for sparse blind (or semi blind) source separation. The unrolled version includes a soft thresholding update, in which the thresholding parameter and one of the weight matrices is learned from data, with a least squares dictionary update, in which the step size is learned from data. The paper provides experimental results showing that this LPALM algorithm is less sensitive to the choice of hyper parameters (since step sizes, etc. are learned from data), and to the choice of the initial dictionary (perhaps since the W matrices are learned from similar examples). It also improves over PALM on experimental data from an astronomy problem.   Reviewers expressed appreciation for the paper’s experimental results, and detailed investigation of the parameterization of unrolled PALM. They also highlighted some issues in the initial submission s exposition   in particular, the setting of the problem (what kind of training data is available, what is the relationship between the mixing matrices A at training and at test time), and a clearer explanation of why it makes sense to learn fixed matrices W^{(k)} which do not depend on A (given that A may change at test time). The revision improved the clarity of the paper, addressing most of these concerns. The submission contributes to the discussion on how to unroll dictionary learning / blind source separation algorithms, how the unrolled algorithm should be parameterized, and demonstrates good results on multispectral data analysis.
This paper provides an alternative way to enable differentiable optimization to the neural architecture search problem.  Different from DARTS, SNAS reformulates the problem and employs Gumbel random variables to directly optimize the NAS objective. In addition, the resource constrained regularization is interesting. The major cons of the paper is that the empirical results are not quite impressive, especially when compared to DARTS, in terms of both accuracy and convergence. I think this is a borderline paper but maybe good enough for acceptance.  
This paper does extensive experiments to understand the lottery ticket hypothesis. The lottery ticket hypothesis is that there exist sparse sub networks inside dense large models that achieve as good accuracy as the original model. The reviewers have issues with the novelty and significance of these experiments. They felt that it didn t shed new scientific light. They felt that epochs needed to do early detection was still expensive. I recommend doing further studies and submitting it to another venue.
This paper explores the role of hyperparameters in the separate phases of a classic pruning pipeline: mask identification and retraining. Key observations include a set of the hyperparameters to search relative to a standard regime as well as the identification that the layerwise pruning rates from mask finding are intertwined with these hyperparameters and are what chiefly affects the eventual performance of the pruned network.  The pros of this paper are that it works against the contemporary wisdom that the default hyperparameters for a model are the best for finding a mask for the model. Instead, there are improvements to be had by identifying a set of hyperparameters that lead to worse overall model accuracy, but better masks. Second, the work shows that the layerwise pruning rates are the key elements of these hyperparameters effect. The rates can in fact be transferred to more poorly performing network configurations and improve performance.  The cons of this paper, as noted by the reviewers, are the somewhat unclear implications of the technique. The added guidance on directions to improve hyperparameters is valuable but does not necessarily provide a cost effect strategy to find these. At its strongest, this guidance offers practitioners a recommendation to also consider hyperparameters for the initial model.   The stronger, forward looking implication is, instead, the connection to layerwise pruning rates. Specifically, while layerwise pruning rates have been demonstrated to be important in the literature (e.g., [1]), there has been a limited study into the exact nature of a good set of pruning rates versus a bad set of pruning rates.  Where this paper stops short of a clear result, is if were to connect excessive pruning of the earlier layers, or simply the layerwise rates themselves to another property of the network (e.g., gradient flow, or capacity) that indicates the improved eventual performance.  My Recommendation is to Reject. The paper s core experiments are well executed. However, this final detail, closing the gap between the portability of these layerwise rates and a conceptual understanding, is a key missing component.  Once done, that will make for a very strong paper.  [1] AMC: AutoML for Model Compression and Acceleration on Mobile Devices. Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li Jia Li, Song Han. EECV, 2018  
This paper proposes a new normalization scheme that attempts to prevent all units in a ReLU layer from being dead. The experimental results show that this normalization can effectively be used to train deep networks, though not as well as batch normalization. A significant issue is that the paper does not sufficiently establish that their explanation for the success of Farkas layer is valid. For example, do networks usually have layers with only inactive units in practice?
This paper proposes a novel and interesting active learning approach, that  trains a classifier to discriminate between the examples in the labeled and unlabeled data at each iteration. The top few samples that are most likely to be from the unlabeled set as per this classifier are selected to be labeled by an oracle, and are moved to the labeled training examples bin in the next iteration. The idea is simple and clear and is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. Experiments performed on CIFAR 10 and MNIST benchmarks demonstrate good results in comparison to baselines.  During the review period, authors considered most of the suggestions by the reviewers and updated the paper. Although the proposed method is similar to density based active learning methods, as also suggested by the reviewers, baselines do not include such approaches in the comparison experiments.
Catastrophic forgetting in neural networks is a real problem, and this paper suggests a mechanism for avoiding this using a k nearest neighbor mechanism in the final layer. The reason is that the layers below the last layer should not change significantly when very different data is introduced.   While the idea is interesting none of the reviewers is entirely convinced about the execution and empirical tests, which had partially inconclusive. The reviewers had a number of questions, which were only partially satisfactorily answered. While some of the reviewers had less familiarity with the specific research topic, the seemingly most knowledgeable reviewer does not think the paper is ready for publication.  On balance, I think the paper cannot be accepted in its current state. The idea is interesting, but needs more work.
This paper presents a variational learning framework for inferring relations between data points. The authors further introduce novel regularizers to avoid unfavorable solutions to their relational learning problem. Qualitative results are provided on rotated versions of MNIST. Additional qualitative results on the Yale face dataset are provided in the appendix.  The reviewers agree that the idea overall is interesting, and the chosen experiments certainly provide some insight into the idea behind the method and demonstrate that the method is learning reasonable representations of relations between data points. I share the sentiment of the reviewers, however, that this paper is not yet ready for publication. The paper in its current form lacks clear positioning against related problems and related research in this community, and the experiments are all qualitative in nature without the attempt to rigorously compare the proposed method against established techniques. The argument brought forward by the authors that the proposed problem is completely novel and therefore no baselines exist is unconvincing as pointed out by the reviewers, as there is related research on e.g. spatial transformer networks [1], neural relational inference [2], and discovery of causal mechanisms [3], which similarly address the problem of discovering relations, interactions, or transformations. Even if these methods don t exactly fit the problem setup presented in this paper, an attempt should be made to design an evaluation that allows one to compare against some of these approaches, especially given that the paper claims to address the general problem of inferring relations between pairs of data points. Overall, I am confident that this would make the paper stronger and more relevant to this community.  [1] Jaderberg et al., Spatial Transformer Networks (NeurIPS 2015) [2] Kipf et al., Neural Relational Inference for Interacting Systems (ICML 2018) [3] Parascandolo et al., Learning Independent Causal Mechanisms (ICML 2018) 
The reviewers generally expressed considerable reservation about the novelty of the proposed method. After reading the reviews in detail and looking at the paper, I m inclined to agree that the contribution is rather incremental. Using normalizing flows for representing policies in RL has been studied in a number of prior works, including with soft actor critic, and I think the novelty in this work is limited in relation to prior work. Therefore, I cannot recommend acceptance at this time.
This paper presents a new idea and approach for self supervised video representation learning.  The reviewers  opinions diverge. R1 suggests that the paper is (marginally) above the threshold. R2 supports the paper, saying that he/she likes the idea behind the paper. R3 explicitly mentioned that he/she would like to provide a borderline rating (but cannot due to the system). R4 is not in favor of the paper, even after the rebuttal.  The AC’s opinion is more aligned with R3 and R4, who are the more senior reviewers among the four. There are two main concerns with the paper: technical contribution and experimental comparison. In terms of the contribution, both the reviewers find that the paper is lacking: "What I m not entirely sure about is how much this method manages to push the boundary of SSL." (by R3). "Overall, the paper presents yet another method to design the pretext task for SSVRL. But my major concern is it lacks enough insights for inspiring future research for this topic." (by R4). The authors argue the difficulties of being in academia in doing this research with limited computation resources and argue that the reviewers should focus more on novelty and contribution, but even after the rebuttal, R4 is not convinced and R3 is still mildly concerned whether the proposed approach really brings something new to the field as the paper fails to show "clear superiority over existing methods".  In addition, as pointed out by the reviewers, there are several state of the art self supervised video representation learning works that the paper misses to cite, or compare against. In addition to Pace and SpeedNet R3 mentioned, below are approaches reporting results on UCF101 and HMDB with the standard self supervised classification task setting (Table 1):  AVTS 89.0, 61.6 CVRL 92.1, 65.4 ELo 93.8, 67.4 XDC 94.2, 67.4 GDT 95.2, 72.8  We note that all these results are much superior to the best results reported by the proposed approach, 79.5 and 50.9 on UCF101 and HMDB. The authors mention in the rebuttal that these superior approaches not included in the paper use stronger backbones (and are thus omitted), but we believe a more academically proper attitude is to include all these numbers and explicitly describe why the proposed approach is not performing better, instead of completely omitting their results.  The AC also questions whether the R2D3D 34 backbone used in this paper really is computationally lighter compared to the backbones used in previous approaches like R(2+1)D 18, which alternates 2D residual modules and 2+1D residual modules (using much fewer parameters and compute than 3D modules) and also has fewer layers. XDC using R(2+1)D 18 backbone reports 86.8/52.6 (UCF/HMDB) accuracies with Kinetics 400 unlabeled data. AVTS also reports 84.1/52.5 (UCF/HMDB) accuracies using MC3 18 backbone. Similarly, GDT uses R(2+1)D 18 backbone and reports 89.3/60.0 (UCF/HMDB) accuracies using unlabeled Kinetics 400.  Even MemDPC reports 86.1/54.5 using R 2D3D backbone when optical flow feature is added. All these are far superior to the results being reported in the paper.  Overall, we view the experimental section of this paper as incomplete, and we cannot convince ourselves that the paper reaches the quality of ICLR.   [AVTS] Korbar, B., Tran, D., Torresani, L.: Cooperative learning of audio and video models from self supervised synchronization. In: NeurIPS (2018)  [ELo] A. Piergiovanni, A. Angelova, and M. S. Ryoo. Evolving losses for unsupervised video representation learning. In Proc. CVPR, 2020  [XDC] H. Alwassel, D. Mahajan, L. Torresani, B. Ghanem, and D. Tran. Self supervised learning by cross modal audio video clustering. arXiv preprint arXiv:1911.12667, 2019  [GDT] M. Patrick, Y. M. Asano, R. Fong, J. F. Henriques, G. Zweig, and A. Vedaldi. Multi modal self supervision from generalized data transformations. arXiv preprint arXiv:2003.04298, 2020  [CVRL] R. Qian, T. Meng, B. Gong, M. H. Yang, H. Wang, S. Belongie, and Y. Cui. Spatiotemporal contrastive video representation learning. arXiv preprint arXiv:2008.03800, 2020
This paper introduces the Stiffness aware neural network (SANN) for improving numerical stability in Hamiltonian neural networks. To this end, the authors introduce the stiffness aware index (SAI) to classify time intervals into stiff and non stiff portions, and propose to adapt the integration scheme accordingly.  The paper initially received three weak accept and one weak reject recommendations. The main limitations pointed out by reviewers relate to missing references from the literature, assumptions behind the proposed approach (e.g. structure of the mass matrix, separable Hamiltonian), and clarifications on experiments including additional baselines and hyper parameter settings.  The rebuttal did a good job in answering reviewers  concerns: RiTTU increased his rating to a clear accept, and RMYXe increased his rating to weak accept. Eventually, there is a consensus among reviewers to accept the paper.   The AC s own readings confirmed the reviewers  recommendation. The method is straightforward yet effective, and the paper is well written. The effectiveness of the proposed approach is shown in different contexts. Since several complex systems exhibit chaotic characteristics, the paper brings a meaningful contribution to the community.
The paper presents a large scale empirical comparison between different prominent losses, regularization and normalization schemes, and neural architectures frequently used in GAN training. Large scale comparisons in this field are rare and important and the outcome of the experimental analysis is clearly of interest for practitioners. However, as two of the reviewers point out, the significance of the new insights is limited, and after rebutal all reviewers agree that the paper would profit from a clearer write up and presentation of the main findings. I see the paper therefore, as lying slightly under the acceptance trashhold.
The paper develops and investigates the use of a spike and slab prior and approximate posterior for a VAE. It uses a continuous relaxation for the discrete binary component in the reconstruction term of the ELBO, and an analytic expression for the KL term between the spike and slab prior and approximate posterior. Experiments on MNIS, Fashion MNIST and CelebA convincingly show that the approach works to learn sparse representations with improved interpretability that also yield more robust classification   All reviewers agreed that this approach to sparsity in VAEs is well motivated and sound, that the paper is well written and clear, and the experiments interesting. One reviewer noted that the accuracy on MNIST remains really poor, so the approach does not cure VAEs yielding subpar representations for classification (although not the goal of this research).  The reviewers and the AC however all judged that it currently constitutes a too limited contribution because a) the approach is a straightforward application of vanilla VAEs with a different prior/posterior, and is thus rather incremental. b) the scope of the paper is rather limited, in particular as it does not sufficiently discuss and does not empirically compare with other (VAE related) approaches from the literature that were developed for sparse latent representations. 
This article studies universal approximation with deep narrow networks, targeting the minimum width. The central contribution is described as providing results for general activation functions. The technique is described as straightforward, but robust enough to handle a variety of activation functions. The reviewers found the method elegant. The most positive position was that the article develops non trivial techniques that extend existing universal approximation results for deep narrow networks to essentially all activation functions. However, the reviewers also expressed reservations mentioning that the results could be on the incremental side, with derivations similar to previous works, and possibly of limited interest. In all, the article makes a reasonable theoretical contribution to the analysis of deep narrow neural networks. Although this is a reasonably good article, it is not good enough, given the very high acceptance bar for this year s ICLR.  
This paper analyzes the expressive power of NTK corresponding to deep neural network. It is shown that the depth hardly affects the behavior of the spectrum of the corresponding integral operator, which indicates that depth separation does not occur as long as NTK is considered. The analysis is novel and gives a significant insight to the NTK research literature. The theoretical framework considered in this paper is considerably broad and potentially can be applied to several types of activation functions (while only ReLU is analyzed as a concrete example in the paper). Moreover, some numerical experiments are conducted that support the validity of the theoretical analysis.  All reviewers are positive on this paper. I agree with their evaluations. For these reasons, I think this paper is worth acceptance.
This paper received a split of scores, from 3 to 10. Among the reviewers, there are both strong advocates and strong rejects. All reviewers agree that finding a policy that is not only improving value but also has lowered variance is an interesting ideas. However, many reviewers point out that are major clarity issues that might hide fundamental problems. The proved guarantees seem to require strong assumptions that are unlikely to hold in practice, and experimental comparisons also have some subtleties. Taking together, although this could be a very interesting work, it will require a major revision and another round of review+discussions before it can be shaped into an acceptable paper.
This paper seeks to integrate tensor based models from physics into machine learning architectures. The two main objections to this paper are first that, despite honest (I assume) efforts from the authors, it remains somewhat hard to understand without substantial background knowledge of physics. Second, that the experiments focus on MNIST and CIFAR image classification tasks, two datasets where linear models perform with high accuracy, and as such are unsuitable for properly evaluating the claims made about the models in this paper. Unfortunately, it does not seem there is sufficient enthusiasm for this paper amongst the reviewers to justify its inclusion in the conference.
All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods.  I believe this paper will be of broad interest to different communities at ICLR.
The paper presents an online algorithm for dynamic tensor rematerialization.  The theoretic analysis on the tensor operation and memory budget bound of the proposed method, as well as on the relationship between the proposed method and optimal static analysis method is novel and interesting.  It covers a pretty comprehensive study across theory, simulation and system implementation. In addition, the paper is well written. 
The paper proposes to augment the original  model to introduce the "luring effect", which can be used for detection and black box defense. Despite being an interesting setup, there are several weaknesses in the threat model (whether it is practical) and the evaluation (lack of adaptive attacks). Those concerns remain after the rebuttal phase.   Threat model: see the concerns raised by Reviewer 1 and the updated comments after the rebuttal phase.   Lack of adaptive attack: the authors assume that the attacker has very limited knowledge to how the system works. This could be viewed as a "black box setting" for adversarial detection evaluation, and actually many other detection works can perform almost perfectly in this setting, so it s not clear how significant are the results. The authors tried to consider some adaptive attacks in their rebuttal but the reviewers are still not fully convinced.  
The paper proposes a new algorithm called Expected Information Maximization (EIM) for learning latent variable models while computing the I projection solely based on samples. The reviewers had several questions, which the authors sufficiently answered. The reviewers agree that the paper should be accepted. The authors should carefully read the reviewer questions and comments and use them to improve their final manuscript. 
There was some support for this paper, but it was on the borderline and significant concerns were raised. It did not compare to the exiting related literature on communications, compression, and coding. There were significant issues with clarity.
This paper expands the spectral bias, which has been studied in a constrained situation such as the fully connected network, to a more practical situation of a multi class classification situation, and proposes a novel technique that can measure the smoothness through linear interpolation of test examples.  Two reviewers highly evaluated the importance of the research question considered in this study and the value of diverse experiments applying the proposed method in various directions, and suggested acceptance. On the other hand, two other reviewers suggested rejection due to the lack of rigor in writing and experiments. I strongly agree with the reviewer s concern that  the method was only verified  on CIFAR10 and the rigor of the experiment was lacking. Unlike the spectral bias paper, which is the basis of this study, this submission is not a theoretical paper, but rather an experimental paper. I admit that it is impossible to verify in various domains as mentioned by the author. However, I believe that verification on more diverse, especially larger scale datasets is essential at least focusing on the image classification task.
This paper extends the idea of hindsight experience replay (HER) to learn Q functions with relative goals by constructing a distribution over relative goals sampled from a replay buffer using a clustering algorithm. This approach is evaluated on three multi goal RL environments and is shown to learn faster than baselines.  ${\bf Pros}$: 1. Faster convergence as compared to baselines 2. Interesting use of clustering in the context of HER but this choice is made without strong justifications or formal arguments  ${\bf Cons}$: 1. Some of the key choices made in this paper are not justified or explained property, e.g.   the goal sampling strategy, choices made in the clustering algorithm and associated heuristics, implicit assumptions (e.g. R1 raised the question of using L2 distance in measuring metrics between two states)  2. There are several choices made without sufficient formal arguments, verification or guarantees.   The paper studies an interesting problem but could be made stronger by incorporating feedback received during the discussion period. 
quality: interesting idea to train an end to end attention together with CNNs and solid experiments to justify the benefits of using such attentions. clarity: the presentation has been updated according to review comments and improved a lot significance: highly relevant topic, good improvements over other methods
This paper presents a simple yet effective method for weight dropping for an LSTM that requires no modification of an RNN cell s formulation.  Experimental results shows good perplexity results on benchmarks compared to many baselines.  All reviewers agree that the paper will bring good contribution to the conference.
The paper presents a method for collaborative task solving via an attention mechanism. The method is evaluated on manipulation task in simulation.   The reviewers agree that the paper is well written and the idea is novel and intuitive. They also share concerns about the limited applicability (too many assumptions and only two robots) of the method and that it contains unjustified claims, and therefore does not meet the ICLR bar.    Constructive feedback for the next version of the manuscript:    The authors should decide if this is a robot learning (a learning based method that advances robotics, specifically robot manipulation) or a machine learning paper (a method that advances cooperative multi agent learning). The decision should drive the publication venue and the baselines. If the target is robot learning, the paper should consider adding on robot experiments. If the target is ML method, that more baselines and benchmarks from the MARL community should be added to the evaluation section.    The authors should be careful not to confuse safety guarantees, which have theoretical and analytical implications, with empirical evaluation without collisions.   Evaluate the learning on more that 3 seeds.
This paper proposes a unified model based framework for high level skill learning and composition through hierarchical RL. The proposed approach combines high level planning in a low dimensional space with low level skill learning, where each low level skill is a policy conditioned on the high level task. The low level policies are learned by using a mutual information objective. The proposed approach is evaluated on locomotion tasks, and is shown to be overall more data efficient than alternative baselines. The reviewers agree that this work is original and sufficiently empirically motivated for acceptance. Two reviewers were concerned by the experimental setup and the transfer setting that are somehow too simple, but the authors fixed these issues in the improved version based on the feedback.
This application oriented paper has been carefully evaluated by three expert reviewers. Their assessments all agreed on a quite marginal methodological novelty of the presented work, yet they recognized nicely engineered pipeline of pre existing modules that appears to satisfy an important remote sensing application. I agree with that assessment and concur with the reviewer s opinion that the work as presented, in spite of being of potential interest to healthcare or biomedical communities, will be of little interest to the ICLR audience. Therefore I recommend rejecting it.  
The paper proposes to apply Neural Architecture Search for pruning DenseNet.   The reviewers and AC note the potential weaknesses of the paper in various aspects, and decided that the authors need more works to publish. 
The paper aims at characterizing conditions for optimal representations required for the domain generalization problem under covariate shift. Under the Idealized Domain Generalization (IDG), the paper provides a variational characterization of the optimal representation and shows a number of intriguing results: (i) optimal representation should remain discriminative  across domains, (ii)  the representation’s marginal support needs to be the same across source and target. (ii) It is also shown that without any target information, no representation can do uniformly well over constant representation, thus supporting the necessity of target knowledge. Finally, the paper provides practical objectives of the proposed variational characterization by self supervised learning using data augmentation with experimental results.   The reviewers had raised a number of concerns, many of which were alleviated through the responses provided by the authors. All in all, in the discussion period, the reviewers indicated the novelty of the results and their importance in learning good representations for the domain generalization problem. The reviewers still had reservations about the following points, and I strongly recommend to the authors to address these points in the revised version (please see the reviews for more details):   (i) For the experiments, it is clear that there is a noticeable gap between the derived theory and the experimental results. It was argued that alt text augmentation of CLIP is one of the practical choices for (approximate) domain agnostic augmentation, but it is difficult to verify this.    (ii) The empirical gain for the proposed method over CLIP is not very significant. As shown in Table 1 (or the complete results in Table 4), the performance of the proposed method is tightly related to the original CLIP method. In the case where the pre trained representations of CLIP fail to cover the target set (see the TerraIncognita column in the table), the proposed method can be much worse than other alternatives. Thus I m worried about the usefulness of the proposed method in practice.  (iii) One of the reviewers had asked about the importance of the necessity condition and its implications (e.g. in practice). Please, make sure to address this in the final version.   Also, there has been several recent works on learning disentangled representations for the domain generalization problem, using e.g. weakly supervised approaches (Matsuura et al,  20), or model based approaches (Robey et al,  21). It would be interesting to see how the results/ideas in this paper would connect to/improve those settings.
The paper studies the problem of identifying what information to forget in attention mechanisms, with the goal of enabling attention mechanisms to deal with longer contexts. This is a simple yet intuitive extension:  self attention is augmented with an expiration value  prediction. Experiments were carried out on NLP and RL tasks. Overall, the paper has novelty in the proposed idea, however, there are concerns about the strength of the experiments; that the experiments fall short.
The proposed routing networks using RL to automatically learn the optimal network architecture is very interesting. Solid experimental justification and comparisons. The authors also addressed reviewers  concerns on presentation clarity in revisions.
The authors propose a novel framework for Distributing Black Box Optimization (DiBB) which can encapsulate any Black Box Optimization (BBO) method. DiBB overcomes some of the limitations of existing methods by leveraging expert knowledge in the problem. The reviewers raised a variety of important technical concerns. The authors seem to agree that they need to substantially rewrite the paper. Therefore I recommend a rejection.
This paper presents empirical evaluation and comparison of different generative models (such as GANs and VAE) in the continual learning setting.  To avoid catastrophic forgetting, the following strategies are considered: rehearsal, regularization, generative replay and fine tuning. The empirical evaluations are carried out using three datasets (MNIST, Fashion MNIST and CIFAR).   While all reviewers and AC acknowledge the importance and potential usefulness of studying and comparing different generative models in continual learning, they raised several important concerns that place this paper bellow the acceptance bar: (1) in an empirical study paper, an in depth analysis and more insightful evaluations are required to better understand the benefits and shortcomings of the available models (R1 and R2), e.g. analyzing why generative replay fails to improve VAE, why is rehearsal better for likelihood models, and in general why certain combinations are more effective than others – see more suggestions in R1’s and R2’s comments. The authors discussed in their response to the reviews some of these questions, but a more detailed analysis is required to fully understand the benefits of this empirical study. (2) The evaluation is geared towards quality metrics for the generative models and lacks evaluation for catastrophic forgetting in continual learning (hence it favours GANs models)   See R3’s suggestion how to improve.   To conclude, the reviewers and AC suggest that in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
The paper proposes an interesting neural architecture for traffic flow forecasting, which is tested on a number of datasets. Unfortunately, the lack of clarity as well as  precision  in writing appears to be a big issue for this paper, which prevents it from being accepted for publication in its current form. However, the reviewers did provide valuable feedback regarding writing, explanation, presentation and structure,  that the paper would benefit from. 
The paper proposes a framework for learning the physical parameters of a physical system’s dynamics from a video. The model combines a differentiable neural ODE solver (NODE) with neural implicit representations through a local coordinate based network which reconstruct the frames based on the ODE solution. Both the static background and the moving objects are modeled via implicit representations. The system being differentiable, it can be trained to recover the physical parameters and the initial conditions of the ODE. Experiments are performed on two toy problems (pendulum and masses that are connected by a spring).  The reviewers agree on the originality of the approach. They however all consider that the paper falls short to demonstrate the potential of the proposed approach because of limited experiments, limited ablation analyses and comparison with baselines. The authors added a new experiment during the rebuttal, but this was not found sufficient to change the reviewers’ opinion.
The reviewers agreed that the paper was somewhat preliminary in terms of the exposition and empirical work.  They all find the underlying problem quite interesting and challenging (i.e. spiking recurrent networks).  However, the manuscript failed to motivate the approach.  In particular, everyone agrees that spiking networks are very interesting, but it s unclear what problem the presented work is solving.  The authors need to be more clear about their motivation and then close the loop with empirical validation that their approach is solving the motivating problem (i.e. do we learn something about biological plausibility, are spiking networks better than traditional LSTMs at modeling a particular kind of data, or are they more efficiently implemented on hardware?).  Motivating the work with one of these followed by convincing experiments would make this a much stronger paper.  Pros:   Tackles an interesting and challenging problem at the intersection of neuroscience and ML   A novel method for creating a spiking LSTM  Cons:   The motivation is not entirely clear   The empirical analysis is too simple and does not demonstrate the advantages of this approach   The paper seems unfocused and could use rewriting  
The discussion with the expert reviewers reached the consensus that the paper lacks in novel *technical* contributions, and as such it does not meet the bar for a theory oriented paper at ICLR.
Pros: + The paper introduces a non trivial interpretation of MAML as hierarchical Bayesian learning and uses this perspective to develop a new variation of MAML that accounts for curvature information.  Cons:   Relatively small gains over MAML on mini Imagenet.   No direct comparison against the state of the art on mini Imagenet.  The reviewers agree that the interpretation of MAML as a form of hierarchical Bayesian learning is novel, non trivial, and opens up an interesting direction for future research.  The only concerns are that the empirical results on mini Imagenet do not show a particularly large improvement over MAML, and there is no direct comparison to the state of the art results on the task.  However, the value of the new perspective on meta learning outweighs these concerns. 
Alternating minimization is surprisingly effective for low rank matrix factorization and dictionary learning problems. Better theoretical characterization of these methods is well motivated. This paper fills up a gap by providing simultaneous guarantees for support recovery as well as coefficient estimates for  linearly convergence to the true factors, in the online learning setting. The reviewers are largely in agreement that the paper is well written and makes a valuable contribution.  The authors are advised to address some of the review comments around relationship to prior work highlighting novelties.
The paper addresses the task of context agnostic learning and presents an algorithm to solve the problem while assuming the ability to sample objects and contexts independently. It is reported a theoretical ground proposing to decompose factors contributing to the classification risk in context bias and object error. The method makes use of only one synthetic sample in training, still being able to generalize well.  The paper received contrasting reviews, 2 positive (7 and 6) and 2 below threshold (5 and 5). R2, R3 and R4 raised similar issues, especially regarding the experimental validation, which is the main shortcoming of the work: addressing "simple" datasets only, no comprehensive comparative analysis only in relation to baselines (vanilla SGD) but not in relation to state of the art methods, possibly slightly revised to accomplish the experimental protocol proposed in this work (authors claim that the originality of the work do not allow a proper comparison with SoA method as is). Indeed, I deem R3 s rating (6) a bit overestimated given the provided comments.  AC does not see an issue the use of only one sample and no info about target, rather, it d be interesting to know if the proposed method could use more than one sample in order to make the comparison with SoA methods fair, while assessing performance in comparative terms with SoA, to give value to the method also in relation to performance.   Unfortunately the rebuttal did not lead an increase of the ratings, nor to better comments.  After the rebuttal, R2 and R4 still remained below threshold; R1 was also not changing idea, remaining positive, and R3 did not react after rebuttal.  Overall, the AC deems this paper containing interesting contributions, but it is not sufficiently ready to be accepted at ICLR mainly because the experiental validation is not showing a fully convincing evaluation of the proposed approach (see above).  
This paper describes how to apply a combination of case based reasoning and RL methods to improve the performance of agents in text adventure games.  The reviewers unanimously recommend acceptance.  This work is both insightful and practical.  This is a valuable contribution.  Well done!
The paper proposes to improve VAE by using a prior distribution that has been previously proposed for independent subspace analysis (ISA). The clarity of the paper could be improved by more clearly describing the proposed method and its implementation details. The originality is not that high, as the main change to VAE is replacing the usual isotropic Gaussian prior with an ISA prior. Moreover, the paper does not provide comparison to VAEs with other more sophisticated priors, such as the VampPrior, and it is unclear whether using the ISA prior makes it difficult to scale to high dimensional observations. Therefore, it is difficult to evaluate the significance of ISA VAE. The authors are encouraged to carefully revise their paper to address these concerns. 
The paper got four accepts (after the reviewers changed their scores), all with high confidences. The theories are complete and the experiments are solid. The AC found no reason to overturn reviewers  recommendations. However, the AC deemed that all the pieces are just routine, thus only recommended poster.
There was a consensus among reviewers that this paper should be accepted as the authors addressed reviewers  concerns in the discussion phase. This paper is well written and easy to read. It provides a coherent story and investigation on two important hypotheses: that natural images have a lower intrinsic dimension than the extrinsic dimension (e.g. the number of pixels) and that a lower intrinsic dimension lowers the sample complexity of learning. These results appear to be novel and significant for the ICLR community as it provides justifications for numerous work on understanding and designing convolutional neural networks based on low dimensional assumptions.
The main concern raised by reviewers is the limited experiments, which are on simple tasks and missing some baselines to state of the art methods. While the overall approach is interesting, the reviewers found the empirical evidence to be fairly unconvincing. 
Reviewers have concerns about poor writing of the paper, lack of technical novelty, and the methodology taken by the paper not being very principled. 
The paper offers a new take on generalization, motivated by the empirical success of self supervised learning.  Two reviewers found the contribution novel and interesting, and recommend acceptance (with one reviewer championing for it).  Two reviewers remain skeptical about the value of the paper, and the authors are encouraged to add a discussion about the points made in these reviews.  I agree with the positive reviewers and would like to recommend acceptance.
The paper reports interesting NAS patterns, supported by empirical and theoretical evidence that the pattern arises due to smooth loss landscape. Reviewers generally agree the this paper would be of interest for the NAS researchers. Some questions raised by reviewers are answered by authors with a few more extra experiments. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper before camera ready. 
This paper proposes a semi supervised graph classification technique that unifies feature and label propagation techniques. The resulting algorithm is a simple extension that attains strong performance. Reviewers were divided on this submission. Some reviewers felt the proposed algorithm did not constitute a sufficient technical contribution given that it was a simple combination of existing techniques. I tend to agree with other reviewers that the simplicity is a benefit. However, despite the methods simplicity there was significant confusion about the details of the method and multiple reviewers flagged that the paper was difficult to read and understand. It further could benefit from additional discussion and some clarification/cleanup of the experimental results. Finally, multiple reviewers asked for better situating of the proposed method with respect to prior work. Given these concerns, I do not think the paper is ready for publication. I would recommend the reviewers do a thorough re write of the paper to address these concerns and consider resubmitting.
This paper proposes a confidence calibrated adversarial training (CCAT). The key idea is to enforce that the confidence on adversarial examples decays with their distance to the attacked examples. The authors show that CCAT can achieve better natural accuracy and robustness. After the author response and reviewer discussion, all the reviewers still think more work (e.g., improving the motivation to better position this work, conducting a fair comparison with adversarial training which does not have adversarial example detection component) needs to be done to make it a strong case. Therefore, I recommend reject.
This paper proposes an alternative method to improve UNMT by using only a pre trained generative language model to bootstrap the process. In all, the reviewers think the proposed method is reasonable.   However, the empirical part is not convincing.  Most reviewers think that evaluating the method on one language pair (En Fr) is not enough to show the effect of the proposed method. In addition, some reviewers argue the clarity of this paper.  In all, I think the proposed method is meaningful. However, the current version is not ready to be published in this ICLR. I hope the authors can improve their paper according to the reviews.
The paper introduces a new variant of the Dropout method. The reviewers agree that the procedure is clear. However, motivations behind the method are heuristic, and have to lean much on empirical evidence. A strong motivation behind the procedure is lacking, and the motivation behind the method is unclear. Furthermore, the empirical evidence is lacking in detail and could use better comparisons with existing literature.
The paper considers the use of adversarial self supervised learning to render robust data representations for various tasks, in particular to integrate the Bootstrap Your Own Robust Latents (BYOL) with adversarial training, where a small amount of labeled data is available together with a sizable unlabeled dataset.  Especially the low data regime is of interest.  It extends a previous method with a new adversarial augmentation technique, it is compared against several methods, and the robust representations are shown to be useful more generally.  There were some confusing presentations and questions that were resolved in a detailed discussion with the reviewers.
This paper develops ideas for enabling the data generation with GANs in the presence of structured constraints on the data manifold. This problem is interesting and quite relevant to the ICLR community. The reviewers raised concerns about the similarity to prior work (Xu et al  17), and missing comparisons to previous approaches that study this problem (e.g. Hu et al  18) that make it difficult to judge the significance of the work. Overall, the paper is slightly below the bar for acceptance.
This paper proposes a deep RL framework for the traditional schedule problem. The proposed algorithm is shown to be effective and has zero shot generalization abilities. Reviewers are mostly satisfied with the response and the overall evaluation is slightly positive. However, there are some drawbacks of the current paper preventing it from getting a higher evaluation: (1) The reviewers believe that the contribution might be small   at least for the RL area; the experimental performance for the scheduling problem is also not significantly improved compared to other methods (e.g. the search based ones). Hence the reviewers believe the contribution of the paper is limited. (2) There is a number of typos and language issues in its present version. The paper may need several rounds of polishment before publication. (3) There is a lack of theoretical justification for the proposed method.  In sum, the AC recommends a borderline rejection.
The authors propose a new dataset and compositional task based on the EPIC Kitchens dataset.  The goal is to test novel compositions and to build a transformer based network specifically for this inference (by analogy). Specifically, the analogy here references the use of nearest neighbors in the dataset.  There are a lot of concerns raised by reviewers which require a large number of changes to the presentation of the manuscript and they are not at present convinced by the current setup or experiments. Explicitly motivating which pretraining methods do or do not violate which aspects of composition and what role other factors like synonymy play in generalization is necessary. Several aspects of the claims made in the paper and in the discussion are big claims that require substantial discussion and analysis (e.g. the surprising weakness of pretrained models) which the reviewers do not feel can be so easily explained away (e.g. by domain shift).
This is a borderline paper. While reviewers believe the findings from this paper may be of potential interest, they are fully convinced. For instance, if the authors want to claim the proposed mechanism is general for UDA, then they should demonstrate its effectiveness to other application domain(s), such as the NLP domain, where the pretrain finetuning strategy is widely adopted for transfer learning. However, the authors did not provide correspondingly additional experiments as requested by a reviewer but claimed they only focused on the CV domain. If the focus is on the CV domain, then the authors need to explain in detail why in the CV domain, the proposed mechanism works well (while in other domains, it may not). There are many other concerns about the assumptions, experimental settings, etc.   In summary, this is a borderline paper below the acceptance bar of ICLR.
The main idea of the paper is to transform graph classification into image representation (via adjacency matrices). Two reviewers are positive, while one is negative. The concerns are novelty (as mentioned by R2), while the last reviewer thinks the method is too simple and unprincipled (here the AC agrees with authors that simple is not necessarily bad). Overall, none of the reviewers champions this paper. Due to many excellent submissions, unfortunately this paper cannot be accepted in present form.
Reviewers liked the self supervised learning of compressed videos, noting that it is an "exciting topic" and an "important problem", although they found the proposed methods (PMSP andCTP) less exciting. Reviewers were satisfied with the execution and the extensive experimental studies. AC felt the community may benefit from the paper s intuitive integration of self supervised learning and the compressed video s signals (I and P frames, residuals, motion vectors, etc). 
This paper proposes to use a mixture of Gaussians to variationally encode high dimensional data through a latent space. The latent codes are constrained using the variational information bottleneck machinery.   While the paper is well motivated and relatively well written, it contains minimal novel ideas. The consensus in reviews and lack of rebuttal make it clear that this paper should be significantly augmented with novel material before being published to ICLR. 
All reviewers agreed that this submission is still premature to be accepted to ICLR2020. We hope the review comments are useful for improving your paper for potential future submission.
This paper aims to relate brain activity (of people reading computer code) to properties of the computer code. They relate the found representations to those obtained from ML computational language  models applied to the same programs.  The paper is clearly written and an interesting idea.  There was a lot of discussion and the author(s) updated their paper a lot.  Program length as a potential confound was raised and  successfully rebutted.  The extent of novelty from Ivanova et al 2020 was also discussed and successfully rebutted.  In the end, the main issues the reviewers had were 1) that the paper had been updated substantially since submission (and would therefore benefit from a thorough re review) and 2) whether the results provide enough new   insights about the brain or about ML language models.    To summarize, the authors spent a lot of time addressing issues in the  rebuttal phase and the paper got a lot better with the reviewers  suggestions, but reviewers agreed it would benefit from more work and further review before acceptance.  I agree with this assessment.
The paper proposed Twin L2O (learning to optimize) for extending L2O from minimization to minimax problems. The authors honestly discussed the limitation of Twin L2O and proposed two improvements upon it with better generalization/transferability. While some reviewer had some concerns on the motivation of applying L2O to solve minimax problems and the motivation of the loss function design (why objective based one is chosen but not gradient based one), the authors have done a particularly good job in the rebuttal. Even though this is more a proof of concept paper, it indeed has novel and solid contributions, and should be accept for publication.
This paper proposes a new variant of adaptive stochastic gradient method that has notable differences from Adam, and claims the advantage of adaptive variance reduction. While the algorithm construction looks novel, there are several concerns by the expert reviewers on the theoretical results of the paper, including lack of clarity and its guidance/relevance to the practical performance. There are also questions on the practical merit of the paper pointing to in limitations on the numerical experiments. I recommend rejection of the paper in the current form, and hope the reviews can help the authors to improve it in both theoretical and empirical aspects. 
The paper proposes an iterative learning method that jointly trains both a model and a scorer network that places a non uniform weights on data points, which estimates the importance of each data point for training.  This leads to significant improvement on several benchmarks.  The reviewers mostly agreed that the approach is novel and that the benchmark results were impressive, especially on Imagenet.  There were both clarity issues about methodology and experiments, as well as concerns about several technical issues.  The reviewers felt that the rebuttal resolved the majority of minor technical issues, but did not sufficiently clarify the more significant methodological concerns. Thus, I recommend rejection at this time.
Important problem (navigation in unseen 3D environments, Doom in this case), interesting hybrid approach (mixing neural networks and path planning). Initially, there were concerns about evaluation (proper baselines, ambiguous environments, etc). The authors have responded with updated experiments that are convincing to the reviewers. R1 did not participate in the discussion and their review has been ignored. I am supportive of this paper. 
This paper explores the representations that are learned by the same network, on the same data, but with different objectives/tasks (RL, supervised, unsupervised). Though the reviewers were positive about some aspects of the paper, the reviews were generally low (3,3,3,6) and indicated rejection. The principle recurring theme in the reviews as to why this was a rejection was the lack of clear motivations/implications. The authors decided not to submit an updated version of the paper. As such, this was a reject decision.
This paper examined a pure exploration method for efficient action value estimates in tabular reinforcement learning.  The paper is on the theoretical properties of value estimates in the large sample regime.  The method is shown to outperform baseline algorithms for this task in tabular reinforcement learning.  The reviewers were divided on the merits of this work.  The use of the central limit theorem was viewed as elegant, and the results were thought to be potentially useful.  However, the reviewers several limitations.  They found the assumption of a communicating MDP to be overly restrictive (reviewer 1).  The algorithm may be computationally inefficient (reviewer 2).  The nature of "exploration" in this work is not the conventional meaning in reinforcement learning (reviewer 3).  The paper is not yet ready for publication at ICLR.  The theoretical results do not clearly convey insights for reinforcement learning with function approximation, and the reviewers are also not in agreement that the current results are applicable to a general MDP setting.
The paper proposes a layer wise magnitude based tuning method through the adoption of LAMP score, motivated by minimizing the model output distortion. The new importance score differs from vanilla magnitude based score in that it incorporate more layer wise information. Extensive experiments are conducted on image and language models to show the improved accuracy upon prior arts under same model compression ratio. Ablation study is also provided to further explain the intuition and comparison of LAMP with other pruning methods.   Though the experiments are extensive, some reviewers raised questions that only image datasets are tested. In the rebuttal, the authors addressed more on Appendix D which provides non image results, and also modified the abstract to highlight the efficacy on image data. In all, given the extensive empirical evaluation on various datasets and model architectures, the improvement of LAMB over prior methods seems convincing. Nevertheless, we urge the authors to include more experimental results, for example ResNet 18 on ImageNet as promised to Reviewer 1, to make the results more solid. It is also suggested to include and discuss some relevant papers mentioned by the reviewers in the final version.  
The paper presents a DKL variant with a linear kernel. Representations from several networks is combined through concatenation, making it not quite an ensemble. It s shown that the model is a universal kernel approximator. Experiments are conducted on a large number of UCI datasets.  Following the discussions, the paper still has the following shortcomings:   some lack of clarity in the presentation (for instance, explaining the equivalence between a multi output learner and M different single output learners)   lack of experiments on data where deep learning is typically used (images); the UCI datasets have structured data and other ensembles like XGBoost may outperform the baselines presented in this paper   difference in performance between DKL and DEKL, especially since DKL benefits from a larger model space, theoretically. maybe DEKL has better sample complexity, but does this advantage hold in the case of the large datasets that deep learning is used for?
All three reviewers agreed that the paper was an interesting, giving a demonstration of what quantum computer could achieve. However, they all also felt that the topic was outside the main interests of the conference and better suited to other venues, e.g. a quantum computation workshop. The AC agrees with them. Thus unfortunately, the paper cannot be accepted.
This paper studies the compression aspect of the information bottleneck. It seeks to clarify a debate about the evolution of mutual information between inputs and representations during training in neural networks. The paper discusses numerous ideas and techniques and arrives at valuable conclusions.   A concern is that parts of the paper (theoretical parts) are intended for a separate paper, and are included in the paper only for reference. This means that the actual contribution of the present paper is mostly on the experimental part. Nonetheless, the discussion derived from the theory and experiments seem valuable in the ongoing discussion of this topic. In any case, I encourage the authors to make efforts to obtain a transparent separation of the different pieces of work.   A concern was raised that the current paper mainly addresses a discussion that originated in a paper that has not passed peer review. On the other hand, this discussion does occupy many researchers and justifies the analysis, even if the originating paper has not been published in a peer reviewed format.    All reviewers are confident in their assessment. Two of them regard the paper positively and one of them regards the paper as ok, but not good enough, with main criticism in relation to the points discussed above.   Although the paper is in any case very good, unfortunately it does not reach the very high bar for acceptance at this ICLR. 
This work introduces Moving Average Batch Normalization (MABN) method to address performance issues of batch normalization in small batch cases. The method is theoretically analyzed and empirically verified on ImageNet and COCO. Some issues were raised by the reviewers, such as restrictive nature of some of the assumptions in the analysis as well as performance degradation due lack of centralizing feature maps. Nevertheless, all the reviewers found the contributions of this paper interesting and important, and they all recommended accept. 
In this work, the authors conduct experiments using variants of RNNs and Gated CNNs on a speech recognition task, motivated by the goal of reducing the computational requirements when deploying these models on mobile devices. While this is an important concern for practical deployment of ASR systems, the main concerns expressed by the reviewers is that the work lacks novelty. Further, the authors choice to investigate CTC based systems which predict characters. These models are not state of the art for ASR, and as such it is hard to judge the impact of this work on a state of the art embedded ASR system. Finally, it would be beneficial to replicate results on a much larger corpus such as Librispeech or Switchboard. Based on the unanimous decision from the reviewers, the AC agrees that the work, in the present form, should be rejected. 
This paper establishes the currently sharpest regret bounds for reinforcement learning in episodic factored MDP. The result improve the result by Osband and Van Roy 2014. The proposed FMDP BF is a model based algorithm that construct confidence sets of the transition distributions using Bernstein  and adapt policies by optimistic planning. The regret bounds holds with high probability. They also provide a lower bound for this class of problems. Reviewers all see merit in the theoretical results of the paper and reach a consensus that this is a good paper. We d still like to request that the authors make all corrections and clarifications following the reviewers s suggestions, especially to improve the clarity of the formulation and proof sketches.  A separate suggestion: Model based RL is a long existing approaches. For MDP belongs to a specific family, there exist regret bounds that depend on Eluder dimension of the the MDP family, see eg. https://arxiv.org/abs/1406.1853 and https://arxiv.org/abs/2006.01107. Can these results be applied to the factored MDP family and yield similar regret bounds? It would be necessary to add discussions about these papers, and explain why or why not these general regret bounds can apply to analyze HMDP.
Paper was reviewed by four expert reviewers. Unfortunately all reviewers, uniformly felt that paper fell marginally bellow bar and argue for rejection. A number of concerns have been identified by the reviewers in the review phase.  Those included: (1) lack of novelty [Reviewer3, Reviewer4], (2) lack of various ablations [Reviewer 2], (3)  issues with experimental setup [Reviewer4], and (4)  lack of significant improvements in performance [Reviewer 1, Reviewer 3]. While authors addressed some of the concerns with provided experiments and ablations during the rebuttal, Reviewers remained unconvinced on the main concerns of novelty and significance. As such, the reviewers are unanimous in their assessment and AC does not see a reason to overturn this consensus. 
The AC has carefully looked at the paper/comments/discussion in order to arrive at this meta review.  Looking over the paper, the FGL layer is an interesting idea, but its utility is only evaluated in a limited setting (fMRI data), rather that other types of images/data. Also, the approach seems to work on some of the fMRI datasets, on others the performance is on par with the baselines.   Overall, the paper is borderline but the AC believes the paper would be a good contribution to the conference.
Reviewers agree that the paper is well motivated and the proposed method is somewhat interesting and well experimented. However, reviewers feel that the paper relies on many existing methods and does not appear to be novel enough.
This is a well written paper addressing a challenging problem with an original approach.  While one reviewer claims there is not a strong call for calibration of regression tasks, this may well be because methods don t exist.  Certainly, calibration is a critical tool for classification.  The major failing of the paper, however, is the empirical evaluation.  Given that no prior work exists, it is arguably OK to not do this, but one could easily reject the paper on this issue alone, as AnonReviewer4 was inclined to do.  One reviewer, however, thought highly of the paper, which bumped up its average score, more than I think it should have got (due to the poor experimental work).  The abstract could be improved by mentioning the use of kernels, the nature of this solution is a substantial part of the paper.
This work proposes a modification of gradient based saliency map methods that measure the importance of all nodes at each layer. The reviewers found the novelty is rather marginal and that the evaluation is not up to par (since it s mostly qualitative). The reviewers are in strong agreement that this work does not pass the bar for acceptance.
In principle, the idea behind the submission is sound: use a generative model (GANs in this case) to learn to generate desirable "goals" (subsets of the state space) and use that instead of uniform sampling for goals. Overall I tend to agree with Reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a high dimensional state space, which seems to be be whole raison d etre of GANs in this proposed method. The coverage experiment in Figure 5 seems like a good *illustration* of the method, but for this work to be convincing, I think we would need a more diverse set of experiments  (a la Figure 2) showing how this method performs on complicated tasks.  I encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the Assumptions being made in Section 3.3 are *violated* somehow (to actually test how the method fails in those cases).
This paper develops an approach to learning hierarchical representations from sequential data. The reviewers were very positive about the overall approach, finding it well motivated and interesting with strong potential, and thought that the paper was extremely well written with clear examples throughout. There was a good back and forth between the reviewers and the authors, discussing several aspects of the paper and providing constructive suggestions for improvement. In particular, the reviewers suggested improvements in terms of independence testing, comparison to further baselines, further experiments, and other improvements as detailed in the reviews. The authors were extremely receptive of these suggestions, which is to be commended and is very much appreciated, and in a response state that they are planning to take the time needed to revise this paper before publication.
The reviewers appreciate the simplicity of the approach, but found the exposition lacking. There were also concerns about strong similarities to CascadeRCNN, which were not resolved in the rebuttal. In the end all reviewers recommend rejection. The AC sees no reason to overturn this recommendation.
The paper provides some insight why model based RL might be more efficient than model free methods. It provides an example that even though the dynamics is simple, the value function is quite complicated (it is in a fractal). Even though the particular example might be novel and the construction interesting, this relation between dynamics and value function is not surprising, and perhaps part of the folklore. The paper also suggests a model based RL methods and provides some empirical results.  The reviewers find the paper interesting, but they expressed several concerns about the relevance of the particular example, the relation of the theory to empirical results, etc. The authors provided a rebuttal, but the reviewers were not convinced. Given that we have two Weak Rejects and the reviewer who is Weak Accept is not completely convinced, unfortunately I can only recommend rejection of this paper at this stage.
The paper is interested in Chinese Name Entity Recognition, building on a BERT pre trained model. All reviewers agree that the contribution has limited novelty. Motivation leading to the chosen architecture is also missing. In addition, the writing of the paper should be improved. 
This manuscript presents a method to refine high level task descriptions into mid level executable steps. The idea of using language models to generate steps for a robot to follow is very interesting. Reviewer concerns focused on the general applicability of the approach and the evaluation.  Reviewers pointed out that the method is tied to VirtualHome which has various properties that are in general not true: the action space is small, the action space is very sparse, and objects tend to be unique.  First, the method enumerates a sentence for every possible action and object combination in the environment. The fact that VirtualHome has few verbs and few objects and that neither of these has complex additional structure (adjectives, adverbs, etc.) means that this is practical. But in any other practical setting this will be impossible. The manuscript mentions this limitation and hints at possible ways to resolve it.  Second, the method requires that the action space must be incredibly sparse. Moreover, a set of common sense rules are needed which are environment specific and must be hand curated. VirtualHome disallows microwaving a cup for example. It also disallows opening the TV. Both of these are valid actions that happen all the time.  Third, the method requires that objects be unique. If multiple plates, vacuum cleaners, lotions, etc. existed and had to be manipulated, e.g., there is no mechanism to refer to any one plate consistently. The model could generate something like "the first plate" but how to actually execute such an action is far from clear.  This third issue is related to the problem of grounding. Normally, grounding means connecting an abstract concept to something concrete in the environment. All of the grounding that is performed here is by virtue of VirtualHome having unique objects in its environments and the actions not requiring multiple instances of the same object. This is not addressing the problem of grounding. Reviewers requested that grounding be removed from the manuscript. This would significantly enhance it, as the model is inherently incapable of grounding as the authors say: "Indeed, one limitation of our approach is that we do not condition on environment state"  Reviewers took issue with details of the evaluation, which are largely a consequence of the choice of VirtualHome. Sometimes this manifested as strange results like models outperforming humans in terms of correctness. As reviewers pointed out, this is worrisome.  Reviewers were also concerned about the title. It implies that language models are zero shot planners, but this is not the case. They are instead able to decompose actions into mid level steps. Reviewers suggested that it would be better to focus the title and tone of the manuscript on extracting task/subtask structures from language models.  The idea presented here, that language models can break tasks into subtasks is interesting. But the manuscript goes a step further and discusses embodied agents which to reviewers appeared to be a reach: there is no grounding and in no sense is the output of the language model any different if the agent is embodied. Even the most positive reviewers felt that discussing embodied agents is unhelpful: it would be better to focus on task/subtask structures. And indeed, this would be more general. All of the concerns that reviewers had around the evaluation would be alleviated by focusing on a language task instead. And the effect of a narrow space of actions, constraints on those actions, and multiple objects of the same class, could be evaluated and reported. Even if the authors had to collect such a corpus, given the difficulties they describe in evaluating on VirtualHome, this would be less of a burden. This could be a strong submission in the future.
Two referees indicate reject, one supports (weak) accept. My impression is that major points of criticism raised by the reviewers   mostly about limited novelty and somewhat inconclusive experimental results   could not be addressed in a clearly convincing way during the rebuttal phase. I will therefore recommend rejection. 
The authors provide an alternative method to [1] for placement of ops in blocks. The results are shown to be an improvement over prior RL based placement in [1] and superior to *some* (maybe not the best) earlier methods for operations placements. The paper seems to have benefited strongly from reviewer feedback and seems like a reasonable contribution. We hope that the implementation may be made available to the community.  [1] Mirhoseini A, Pham H, Le Q V, et al. Device Placement Optimization with Reinforcement Learning[J]. arXiv preprint arXiv:1706.04972, 2017. 
This paper proposes a pseudo labeled data selection method for semi supervised pose estimation. The investigated task in this paper is practical and useful. The framework is well designed and reasonable, and extensive ablation studies are conducted to test the efficacy of the method. After discussion, all the reviewers recommend accept of this paper.
This paper focuses on mitigating the effect of label noise. They provide a new class of loss functions along with a new stopping criteria for this problem. The authors claim that these new losses improves the test accuracy in the presence of label corruption and helps avoid memorization. The reviewers raised concerns about (1) lack of proper comparison with many baselines (2) subpar literature review and (3) state that parts of the paper is vague. The authors partially addressed these concerns and have significantly updated the paper including comparison with some of the baselines. However, the reviewers were not fully satisfied with the new updates. I mostly agree with the reviewers. I think the paper has potential but requires a bit more work to be ready for publication and can not recommend acceptance at this time. I have to say that the authors really put a lot of effort in their response and significantly improved their submission during the discussion period. I recommend the authors follow the reviewers  suggestions to further improve the paper (e.g. comparing with other baselines) for future submissions
The paper extends results from the recent work of Steinke and Zakynthinou (SZ) for the test loss of randomized learning algorithms. They provide bounds in the single draw as well as PAC Bayes setting. The main result is about fast rates the proof of which follows with minor modifications from the corresponding result in SZ. It is unclear to me the contribution over existing work is sufficient to merit acceptance.
This paper studies the really hard problem of zero shot learning in acoustic modeling for languages with limited resources, using data from English. Using a novel universal phonetic model, the authors show improvements compared to using an English model for 20 other languages in phone recognition quality.  Strengths   Reviewers agree that the problem is an important one, and the presented ideas are novel.   Universal phonetic model to represent phones in any language is interesting.  Weaknesses   The results are really weak, to the point that it is unclear how effective or general the techniques are. The work is an interesting first step, but is not developed enough to be accepted at this point.   The universal phonetic model being trained only in English might affect generalizability to languages that do not share phonetic characteristics. The authors agree partly, and argue that the method already addresses some issues since the model can already represent unseen phones. But, coupled with the high phone error rates, it is still unclear how appropriate the technique will be in addressing this issue.   Novelty: Although the idea of mapping phones to attributes, and using those for ASR is not novel (e.g., using articulatory features), application for zero shot learning is. The work assumes availability of a small text corpus to learn phone sequence distribution, so is similar to other zero resource approaches that assume some data (audio, as opposed to text) is available in the new language.  This paper presents interesting first steps, but lacks sufficient experimental validation at this point. Therefore, AE recommendation is to reject the paper. I encourage the authors to improve and resubmit in the future.
The authors propose approaches to handle partial observability in reinforcement learning. The reviewers agree that the paper does not sufficiently justify the methods that are proposed and even the experimental performance shows that the proposed method is not always better than baselines.
This paper tackled the reward shaping problem under the framework of Markov games. The authors proposed reward shaping algorithms for RL with mild theoretical guarantees. The AC agrees with the reviewers that the empirical performance is ambiguous. The paper should be substantially improved before being accepted.
The reviewers highly appreciated the replies and the additional experiments. We also had a private discussion on the paper. To summarize: the replies alleviated quite a few concerns, however the consensus was that the paper still does not meet the bar for a highly competitive conference like ICLR.  The idea of combining MPC (on a  wrong  model)  with a learned cost function is very interesting and a promising direction. On the downside the reviewers are still not entirely convinced about the contribution and believe that the paper requires a significant re write to incorporate the discussed points as well as an additional round of reviews.
Strengths: A co evolution of body connectivity and its topology mimicing control policy is presented.  Weaknesses: Reviewers found the paper to be lacking in detail. The importance of message passing in achieving the given results is clear on one example but not some others. Some reviewers had questions regarding the baseline comparisons. The authors provided lengthy details in responses on the discussion board, but reviewers likely had limited time to fully reread the many changes that were listed. AC:  The physics in the motions shown in the video require signficant further explanation. It looks like the ball joints can directly attach themselves to the ground, and make that link stand up. Thus it seems that the robots are not underactuated and can effectively grab arbitrary points in the environment. Also it is strange to see the robot parts dynamically fly together as if attracted by a magnet.  The physics needs significant further explanation.  Points of Contention: The R2 review is positive on the paper (7), with a moderate confidence (3). R1 contributed additional questions during the discussion, but R2 and R3 were silent.  The AC further examined  the submission (paper and video).  The reviewers and the AC are in consensus regarding the many details that are behind the system that are still not understood.  The AC is also skeptical of the non physical nature of the motion, or the unspecified behavior of fully actuated contacts with the ground. 
Four reviewers have evaluated this paper. The reviewers have raised concerns about the specific formulation used for adversarial example generation which requires further clarity in motivation and interpretation. The reviewers have also made the point that the experimental evaluation is against previous work which tried to solve a different problem (black box based attack) and hence the conclusions are unconvincing.
Shapley values are an important approach in extracting meaning from trained deep neural networks, and the paper proposes an innovative approach to address inefficiencies in post processing to compute Shapley values, by instead incorporating their computation into training.  There was a robust discussion of this paper, and the authors  comments and changes substantially strengthened the paper and the reviewers  view of it, to the point that all reviewers now recommend acceptance.  Some lingering concerns remain that the authors should continue to work to address.  Is the method of computing Shapley values used as the baseline in the paper really state of the art, or artificially weak?  The empirical results were methodologically sound but not as strong as one might expect or hope.  These concerns detract somewhat from enthusiasm, but nevertheless the paper yields an innovation to a widely used approach to one of the most pressing current research problems.  The reviewers had a number of smaller suggestions that should also be incorporated including more significance testing and reporting of resulting p values.
This paper demonstrates that current post hoc methods to explain black box models are not robust to spurious signals based on three metrics especially when the spurious signals are implicit or unknown.  Technical novelty is limited because the paper presents primarily empirical results instead of novel machine learning techniques. However, the problem is very important and timely, and significance to the field and potential impact of the presented results to advance the field are high as reviewers emphasized. There are ways to further improve the paper, including the clarity of presentation, although the authors improved in the revised manuscript.  Overall, this paper deserves borderline acceptance.
This paper considers an important problem of aligning two knowledge graphs (the entities and relations therein). The reviewers found the use of adversarial training quite novel and appropriate for the the task, especially as it works in the unsupervised setting as well. The reviewers were also impressed that the proposed work outperforms existing approaches in terms of the accuracy of the alignment.  The following potential weaknesses were raised by the reviewers and the AC: (1) Reviewer 3 brings up the fact that the hyperaparameters were set different from the original publications of the baselines, and thus are not convinced of the soundness of the results, (2) Reviewer 2 notes that the evaluation is limited, and more variations should be considered, such as varying the overlap, taking larger subsets of knowledge graphs, and going beyond TranE as the choice for embedding, and (3) Reviewer 3 notes that a simpler baseline based on alignment discrepancy should be  considered, which would alleviate the need for RL based training.  Although the reviewers raised very different concerns with the paper, none of them were addressed in a response or revision, and thus they agree that the paper should be rejected.
The paper introduces a novel approach to transfer learning in RL based on credit assignment. The reviewers had quite diverse opinions on this paper. The strength of the paper is that it introduces an interesting new direction for transfer learning in RL. However, there are some questions regarding design choices and whether the experiments sufficiently validate the idea (i.e., the sensitivity to hyperparameters is a  question that is not sufficiently addressed). Overall, this research has great potential. However, a more extensive empirical study is necessary before it can be accepted.
The authors propose a Simple Spectral Graph Convolution (S2GC) variant of GCNs, using a modified Markov Diffusion kernel. The approach proposed aims to tackle GCN performance degradation with increased depth (oversmoothing), by combining techniques from earlier works (Simple Graph Convolution and APPNP). This is complemented by a spectral analysis of the properties of the scheme, as well by a comparison to state of the art on several datasets.
This paper was reviewed by 3 expert reviews and received largely negative reviews, with concerns about the toy ish nature of the 2D environments and limited novelty.  Since ICLR18 received multiple papers on similar topics, we took additional measures to ensure that papers were similar papers were judged under the same criteria. Specifically, we asked reviewers of (a) this paper and (b) of a concurrent submission that also studies language grounding in 2D environments to provide opinions on (b) and (a) respectively. Unfortunately, while they may be on similar topic and both working on 2D environments, we received unanimous feedback that (b) was much higher quality ("comparison with multiple baselines, better literature review, no bold claims about visual attention, etc). We realize this may be disappointing but we encourage the authors to incorporate reviewer feedback to make their manuscript stronger. 
This paper studies learning with noisy labels by integrating the idea of curriculum learning.  All reviewers and AC are happy with novelty, clear write up and experimental results.  I recommend acceptance.  
In this paper, the authors studied a robust method for detecting out of distribution (OOD) instances. OOD instance detection is an important practical problem, and multiple reviewers recognized the proposed approach is interesting. However, it was the common opinion of several reviewers that the main theoretical analysis was imported from existing studies, and the novelty is not sufficiently high. It was also observed that the relationship between the proposed method and closely related studies was not properly discussed. Although this point has been improved in the revision, a reviewer and area chair still concern that enough evidence is not provided for some of the points the authors claim as advantages over existing studies. Although the proposed method is interesting and could be an important contribution to the ICLR community, the current paper needs non trivial revision before publication.
The authors provide four rigorous upper bounds on the operator norm of the linear transformation associated with a 2D convolutional layer of a neural network.  One of these is a heuristic proposed in earlier work by Miyato et al, and widely used, so, among other things, their result provides theoretical context for that method which will be of broad interest.  All four of their bounds can be efficiently computed and have easily computed gradients, so they propose using the minimum of the four bounds for various purposes.  Since, for standard architectures, the Lipschitz constant of a network can be bounded above by the product of the operator norms of its layers, there are a variety of applications of differentiable bounds on these operator norms.  They show that their new bound is sometimes much tighter than the bound of Miyato et al, and can be computed much more efficiently than two known methods for exact computation.  The paper is written well, which will facilitate future work building on this work.  The analysis builds on earlier work, but insight was required to obtain the new results;  the fundamental novelty of the mathematical development was confirmed by an expert reviewer.  While they experimentally compared the accuracy of their approximations to those of the method of Miyato, et al, the case for the practical utility of their method would have been stronger if they had shown that their regularizer led to better results for some tasks.  However, I believe that the paper should be accepted purely on the basis of its theoretical contribution, which enhances our understanding of this important topic, and, even if it cannot be directly applied, seems like to inspire practically useful methods in the future. 
This paper presents an approach to reward shaping in RL centred on the question of how to select between different shaping signals. As such this is an interesting research direction that could make important contributions in the area. Generally the reviewers felt that the paper is too preliminary in its current form. There were several questions raised around problems with the technical formulation. It was also felt that the experiments could be more rigourous to fully validate the claims of the paper.
With an average post author response score of 4   two weak rejects and one weak accept, it is just not possible for the AC to recommend acceptance. The author response was not able to shift the scores and general opinions of the reviewers and the reviewers have outlined their reasoning why their final scores remain unchanged during the discussion period.
The paper presents a differentiable upper bound on the performance of classifier on an adversarially perturbed example (with small perturbation in the L infinity sense). The paper presents novel ideas, is well written, and appears technically sound. It will likely be of interest to the ICLR community.  The only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defenses on a dataset like ImageNet.
The authors develop a novel robustness certificate based on randomized smoothing that accounts for second order smoothness of functions smoothed with Gaussian noise. They develop a variant of Gaussian smoothing based on these insights that improves sample efficiency of randomized smoothing using gradient information.  While the ideas presented were interesting, reviewers were concerned about the quality of presentation of the paper (confused positioning of results relative to prior work) as well as the lack of significant improvements upon existing methods in the experimental section. Overall, the paper is borderline based on the reviewers  comments and ratings   however, there is not sufficient evidence to justify acceptance.  I would encourage the authors to consider a significant revision to improve the clarity of contributions made and strengthen experimental results to demonstrate significant improvements, which would validate the power of the theoretical ideas presented.
All reviewers agreed that this paper addresses an important question in deep learning (why doesn t SVRG help for deep learning)? But the paper still has some issues that need to be addressed before publication, thus the AC recommends "revise and resubmit".
The paper presents interesting idea, but the reviewers ask for improving further paper clarity   that includes, but is not limited to, providing in depth explanation of assumptions and also improving the writing that is too heavy and difficult to understand.
This paper proposes methods for replacing parts of neural networks with tensors, the values of which are efficiently estimated through factorisation methods. The paper is well written and clear, but the two main objections from reviewers surround the novelty and evaluation of the method proposed. I am conscious that the authors have responded to reviewers on the topic of novelty, but the case could be made more strongly in the paper, perhaps by showing significant improvements over alternatives. The evaluation was considered weak by reviewers, in particular due to the lack of comparable baselines.  Interesting work, but I m afraid on the basis of the reviews, I must recommend rejection.
The paper focuses on the task of learning audiovisual representations through contrastive learning on unlabelled videos. This work is another addition to the ever growing literature on self supervised learning (SSL) with emphasis on video and multi modal data. The main contribution of this work is the manner in which it tackles a well known drawback of contrastive learning, namely the strategy used to sample negatives in the contrastive pipeline. The authors propose an active sampling strategy that adaptively chooses negative samples that are informative and diverse. This active selection technique is similar in spirit to many selector functions proposed in the active learning literature. It seems to be the first time it is used for contrastive SSL.   Based on all the reviews and the subsequent discussions, it seems that the reviewers  comments were addressed. The authors are commended on integrating the reviewers  suggestions and making the necessary edits to the paper in a timely manner. 
The paper leverages variational auto encoders (VAEs) and disentanglement to generate data representations that hide sensitive attributes. The reviewers have identified several issues with the paper, including its false claims or statements about differential privacy, unclear privacy guarantee, and lack of related work discussion. The authors have not directly addressed these issues.
The paper introduces an adaptive label smoothing technique, where the smoothing factor is computed based on the relative object size within an image, in order to address the problem of overconfident predictions. All reviewers recommend rejection based on limited technical contribution and unclear benefits of the proposed method. During the rebuttal phase, the authors carried out more experiments and clarified several other questions asked by the reviewers. The response was well received, but did not eliminate the main concerns about the paper. While the idea is interesting and has potential, the AC agrees with the reviewers that the paper is not ready for ICLR, and encourages the authors to improve the paper according to the reviews and submit it to another top conference.
This paper examines the interplay between the related ideas of invariance and robustness in deep neural network models. Invariance is the notion that small perturbations to an input image (such as rotations or translations) should not change the classification of that image. Robustness is usually taken to be the idea that small perturbations to input images (e.g. noise, whether white or adversarial) should not significantly affect the model s performance. In the context of this paper, robustness is mostly considered in terms of adversarial perturbations that are imperceptible to humans and created to intentionally disrupt a model s accuracy. The results of this investigation suggests that these ideas are mostly unrelated: equivariant models (with architectures designed to encourage the learning of invariances) that are trained with data augmentation whereby input images are given random rotations do not seem to offer any additional adversarial robustness, and similarly using adversarial training to combat adversarial noise does not seem to confer any additional help for learning rotational invariance. (In some cases, these types of training on the one hand seem to make invariance to the other type of perturbations even worse.)  Unfortunately, the reviewers do not believe the technical results are of sufficient interest to warrant publication at this time. 
This paper explores the use of a texture based foveation stage in scene categorization.  They show that the foveated system shows presevation of high spatial frequency information relative to other matched transformations.  This paper engendered a lot of discussion and had a wide range of ratings.  An extra review was requested that fell intermediate between the high and low scores.  Generally reviewers agree that the question is interesting but that the paper does not clearly elucidate the logic of the paper.  That is, Reviewer 1, and another reviewer in discussion, had issues with the logic of the paper. I also found the motivation behind the paper, difficult to understand at first.  I had to find the Rosenholtz paper to understand why the authors were considering this particular representation. This should be better explained in the paper   the main points of the paper should be understandable by itself.   There is also concern that the claims are not validated by the presented results.  For example, the authors claim that their foveated network were more robust to occlusion but Reviewer 5 points out that this is likely due to the foveation nets having more unoccluded information.   On the positive side, Reviewer 4 points out that the experiments are extensive and several reviewers commented that they trust that the experiments were done correctly.  Reviewer 2, 4 and 5 all mention that there are too many results reported and recommend paring down to the most important results.  In my view the paper is right at the border of acceptance.  Acceptance/rejection will depend on capacity limits and balancing areas.  I recommend that if accepted,  or resubmitted to another conference, that the results be pared down, and  more space be devoted to explaining the question(s) and why they are  interesting and relevant and why the comparison networks allow the questions  to be answered.    Originality   High Quality   High  Clarity   Could be improved Significance   Could be better articulated and is hard to assess as is. Pros:   interesting idea, many well done experiments Cons:   claims not well validated, clarity could be improved to	emphasize significance Other: paper too dense   should be pared down to improve clarity 
This paper proposed a spatial smoothing layer for CNNs which is composed out of a feature range bounding layer (referred to as prob) and  a bluring layer (referred to as blur). An empirical analyses shows that the proposed layer improves the accuracy and uncertainty of both deterministic CNNs and Bayesian NN (BNNs) approximated by MC dropout. The paper further provides theoretical arguments for the hypothesis that bluring corresponds to an ensemble and represents the proposed method as a strategy to reduce the sample amount during inference in BNNs.  Reviewers valued the extensive (theoretical as well as practical) analyses. However, the theoretical analysis should still be improved. First of all, the  the proposed technique is motivated in the context of BNNs, which is not very strongly supported. Second, the argument that „the smoothing layer is an ensemble“ is based on the observation that it has some properties ensembles have as well: (1) they reduce feature map variances, (2) filter out high frequency signals, and (3) flatten the loss landscape. But two things sharing the same properties do not need to be the same thing. Moreover, the proofs of the prepositions stating the properties are difficult to follow and may contain some flaws. Furthermore, the paper is not well self contained and highly depends on the appendix. Given these, the paper can not be accepted in its current state.   A future version could improve over the current manuscript by making the theoretical statements and proofs more clear. Another option would be to analyze the contribution without connecting it to a Bayesian setting and ensembles, and instead focus on showing that the proposed smoothing layer has those good properties, doing detailed empirical studies, and showing that CNN components like global average pooling and ReLU + BN are special cases of the propose method.
This paper does an excellent job at helping to clarify the relationship between various, recently proposed GAN models. The empirical contribution is small, but the KID metric will hopefully be a useful one for researchers. It would be really useful to show that it maintains its advantage when the dimensionality of the images increases (e.g., on Imagenet 128x128).
The authors addressed the issues raised by the reviewers; I suggest to accept this paper.
here, yet another sentence representation method is proposed. i agree with R1 and R3 that this does not contribute significantly to be a full length conference paper.
The paper received a majority voting of rejection, although the author response successfully convinced one reviewer to increase his/her score from 5 to 6. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.  **Presentation**  The presentation of this paper needs huge efforts to further improve. Several reviewers and I suffered from difficulties to understand the motivation and challenges of this paper. It seems that Section 3.5 is the novelty part of this paper, but I failed to catch their points.   **Contribution**  Two contributions points were claimed in this paper. (1) The combination of data augmentation and MCR$^2$. Without knowing the challenges in this paper, it is difficult to evaluate this point. Based on my current understanding (The presentation heavily affects my understanding), this point is very incremental. (2) The proposed method achieved state of the art performance. This point is problematic. I will explain below.  **Related Work**  The authors failed to notice a huge body of manifold learning work and contrastive clustering work. Some state of the art methods are not included for comparisons.  **Experimental Evaluation**  (1) Lack of state of the art methods; (2) No standard deviation; (3) The experimental results are incomplete; and (4) It seems that the proposed method only achieved high performance on CIFAR 10 and CIFAR 20. I am not the person who requests the authors achieve the best performance on all the datasets. Everyone knows no algorithm always wins. But the authors should provide some analyses on the inferior performance for better understanding the model.  No objection from reviewers was raised to again this recommendation.
The reviewers were confused by several elements of the paper, as mentioned in their reviews and, despite the authors  rebuttal, still have several areas of concerns.  I encourage you to read the reviews carefully and address the reviewers  concerns for a future submission.
During discussion, the reviewers acknowledge improvement of the revised version of the paper through author rebuttal and agree with that the paper is overall well written.  However, the novelty of the paper is not strong, and reviewers share the concern that distinction between self supervised learning and deep clustering is not convincing. Also, the concerns (2) raised by the Reviewer #2 is important, which is about the effectiveness of the proposed two step procedure: first apply t SNE to transform into two dimensions, followed by running K means, while the answer is not well  justified. In my opinion, parameter sensitivity should be studied more carefully. The authors mention that $k$ and $\kappa$ have little effect on clustering performance, while this could imply that the associated terms do not have impact on the proposed method. This point should be clarified further.  Overall, the paper is still not ready for publication, I will therefore reject the paper.
This paper proposes a channel pruning method to compress and accelerate pre trained CNNs. The reviewers suggest further analysis of the experimental results to help explain the gains in performance, as well as point out some errors in the formulation. The paper is also found similar to meta pruning method. The authors are encouraged to re submit the paper after adding the analysis and improving the related work section. 
To improve the data augmentation for an existing self supervised learning framework, the paper presents identity disentangled adversarial augmentation (IDAA) that utilizes a pretrained VAE and adversarial perturbation in the VAE latent space to generate identity preserving hard negative/positive samples. The proposed method has been clearly described and is of interest to the community. A wide variety of experiments have been done to illustrate the effectiveness of IDAA. Given the concerns on the validity and generalizability of the experimental results presented in the paper, the AC does not consider the paper of its current form to be ready for publication, as discussed below.  The AC appreciates the amount of effort that the authors have spent responding to the reviewers. However, thees very detailed rebuttals do not appear to be that effective in directly answering several key concerns of the reviewers.  1. While many experiments have been added to improve the paper, most of the settings are still considered by the reviewers to be unconvincing due to the use of a small number of epochs, unusual network backbones, and/or non standard datasets (e.g., downsampled 64*64 ImageNet with about 30% accuracy).  2. The comparison to the literature is considered to be insufficient. While the basic idea is still how to generate effective views or hard negative/positive views, the paper chooses to narrowly focus its comparison with CLAE that relies on adversarial perturbation in the pixel space, ignoring a careful comparison with many existing methods that boost the performance by mining hard negatives/positives and/or using a memory bank to accommodate a large number of negatives.   3. Related to point 2, a main justification of IDAA is that it outperforms CLAE, which, however, is shown by the authors to be a weak baseline that barely improves an existing self supervised learning (SSL) framework in a variety of different settings. For example, the results in Table 1 show CLAE hurts SimCLR s performance. The weak performance of CLAE makes it even more important to include stronger baselines.  4. A reviewer also pointed out that the use of a pretrained VAE introduces additional model parameters and increases the computation cost, and hence a careful discussion on how to ensure a fair comparison with the baselines is desired.   5. The presentation could be somewhat misleading in giving the impression that IDAA is a stand alone domain agnostic data augmentation technique. While it is totally fine that IDAA is a complementary data augmentation technique, the authors need to carefully revise the presentation to minimize the risk of giving the reader a wrong impression.    6. The authors dismissed some of the negative reviews by arguing them to be “mostly incorrect, wrong, makes no sense”. These strong arguments, if not supported with clear evidence asked by the reviewers, may convince neither the reviewers nor the AC and lead to unnecessarily tedious lengthy discussions. From the AC s reading of the paper and reviews, it appears that there are several cases where a reviewer was asking the answer to Question A, but the authors were providing an answer to Question B and pushing the reviewer to accept that as a satisfactory answer to Question A. For example, while many experiments have been done on several different datasets, there is a legitimate concern on why these computing resource has not been spent on getting results on ImageNet with ResNet 50. When the experiments are only being done on ImageNet 100 or the downsampled full ImageNet with a small network, there could be clear concerns on whether the observed performance gains are only applicable to relatively small/low resolution datasets or small networks. For example, the VAE and/or the adversarial attack may not work that well on large scale/high resolution images and hence IDAA may break down when scaling it to a larger dataset/model. That type of concern can only be addressed if the authors have taken the time to follow a standard setting, such as ResNet50 on ImageNet, and make comparisons with a wide variety of baselines whose results on these standard settings are readily available.
This was a borderline paper and a very difficult decision to make.  The paper addresses a potentially interesting problem in approximate POMDP planning, based on simplifying assumptions that perception can be decoupled from action and that a set of sensors exhibits certain conditional independence structure.  As a result, a simple approach can be devised that incorporates a simple greedy perception method within a point based value iteration scheme.  Unfortunately, the assumptions the paper makes are so strong and seemingly artificial to the extent that they appear reverse engineered to the use of a simple perception heuristic.  In principle, such a simplification might not be a problem if the resulting formulation captured practically important scenarios, but that was not convincingly achieved in this paper indeed, another major limitation of the paper is its weak motivation.  In more detail, the proposed approach relies on decoupling of perception and action, which is a restrictive assumption that bypasses the core issue of exploration versus exploitation in POMDPS.  As model of active perception, the proposal is simplistic and somewhat artificial; the motivation for the particular cost model (cardinality of the sensor set) is particularly weak a point that was not convincingly defended in the discussion.  Perhaps the biggest underlying weakness is the experimental evaluation, which is inadequate to support a claim that the proposed methods show meaningful advantages over state of the art approaches in important scenarios.  A reviewer also raised legitimate questions about the strength of the theoretical analysis.  In the end, the reviewers did not disagree on any substantive technical matter, but nevertheless did disagree in their assessments of the significance of the contribution.  This is clearly a borderline paper, which on the positive side, was competently executed, but on the negative side, is pursuing an artificial scenario that enables a particularly simple algorithmic approach.  Despite the lack of consensus, a difficult decision has to be made nonetheless.  In the end, my judgement is that the paper is not yet strong enough for publication.  I would recommend the authors significantly strengthen the experimental evaluation to cover off at least two of the major shortcomings of the current paper: (1) The true utility of the proposed method needs to be better established against stronger baselines in more realistic scenarios.  (2) The relevance of the restrictive assumptions needs to be more convincingly established by providing concrete, realistic and more challenging case studies where the proposed techniques are still applicable.  The paper would also be improved if the theoretical analysis could be strengthened to better address the criticisms of Reviewer 4.
This paper proposes a method to improve the convergence time of PSRO. The paper was well received by all reviewers and is likely to be of interest to a similar sub community within ICLR, but may be of less relevance to the wider community not focused on multi agent learning.   A number of issues were raised by reviewers regarding the clarity of the originally submitted version of the paper. I encourage the authors to consider all constructive feedback given and revise the paper to maximise its impact. This will be of particular help in reaching a wider audience than those with pre existing experience with the methods this work builds on.
This paper extends work on NALUs, providing a pair of units which, in tandem, outperform NALUs. The reviewers were broadly in favour of the paper given the presentation and results. The one dissenting reviewer appears to not have had time to reconsider their score despite the main points of clarification being addressed in the revision. I am happy to err on the side of optimism here and assume they would be satisfied with the changes that came as an outcome of the discussion, and recommend acceptance.
As Reviewer 2 pointed out in his/her response to the authors  rebuttal, this paper (at least in current state) has significant shortcomings that need to be addressed before this paper merits acceptance.
This paper uses prototype memories for learning generative models. Inspired by the finding that there is sparse activity and complex selectivity in the supragranular layers of every cortical region, even primary visual cortex, the authors propose to use prototype memories at each level of the hierarchy, which marks their work as novel. They show superior performance in few shot image generation tasks.  The reviewers  scores were borderline (5,5,8), making this a case that required some AC consideration. The reviewers generally agreed that the paper was relevant and interesting, though the two more negative reviewers had some concerns about (1) the tests used, (2) the interpretation relative to neuroscience data, and (3) the novelty. After reading through the paper, the reviews, and the rebuttal s, the AC felt that the authors had made a decent attempt at addressing items (1) and (2), and item (3) was ultimately a subjective question. The authors were reasonably clear about what marks their work as novel, and it is certainly not *exactly* the same as previous work. Altogether, given these considerations, the AC felt that this paper deserved to be accepted, given the reasonable attempts from the authors to respond to the reviewers  concerns and an average score above acceptance threshold (though the scores did not change post rebuttal, it should be noted).
The paper proposed a new metric to define the quality of optimizers as a weighted average of the scores reached after a certain number of hyperparameters have been tested.  While reviewers (and myself) understood the need to better be able to compare optimizers, they failed to be convinced by the proposed solutions. In particular (setting aside several complaints of the reviewers with which I disagree), by defining a very versatile metric, this paper lacks a strong conclusion as the ranking of optimizers would clearly depend on the instantiation of that metric.  Although that is to be expected, by the very behaviour of these optimizers, it makes it unclear what the added value of the metric is. As one reviewer pointed out,  all the points made could have been similarly made with other, more common plots.  Ultimately, it wasn t clear to me what the paper was trying to achieve beyond defining a mathematical formula encompassing all "standard" evaluation metric, which I unfortunately see of limited value.
All reviewers recommend acceptance. The problem is an interesting one. THe method is interesting. Authors were responsive in the reviewing process.  Good work. I recommend acceptance :)
The paper provides a distributed optimization method, applicable to decentralized computation while retaining provable guarantees.  This was a borderline paper and a difficult decision.  The proposed algorithm is straightforward (a compliment), showing how adaptive optimization algorithms can still be coordinated in a distributed fashion.  The theoretical analysis is interesting, but additional assumptions about the mixing are needed to reach clear conclusions: for example, additional assumptions are required to demonstrate potential advantages over non distributed adaptive optimization algorithms.  The initial version of the paper was unfortunately sloppy, with numerous typographical errors.  More importantly, some key relevant literature was not cited:   Duchi, John C., Alekh Agarwal, and Martin J. Wainwright. "Dual averaging for distributed optimization: Convergence analysis and network scaling." IEEE Transactions on Automatic control 57.3 (2012): 592 606. In addition to citing this work, this and the other related works need to be discussed in relation to the proposed approach earlier in the paper, as suggested by Reviewer 3.  There was disagreement between the reviewers in the assessment of this paper.  Generally the dissenting reviewer produced the highest quality assessment.  This paper is on the borderline, however given the criticisms raised it would benefit from additional theoretical strengthening, improved experimental reporting, and better framing with respect to the existing literature.
This paper proposes a deep reinforcement learning algorithm Supe RL that combines model free RL with genetic updates. The idea is to periodically mutate and evaluate the actor and greedily choose the best performing child, and incorporate it in the main actor via Polyak averaging on a target policy network. The algorithm can be in principle combined with any gradient based deep RL method. Supe RL was demonstrated by combining it with Rainbow and PPO and evaluated in navigation tasks as well as standard MuJoCo benchmarks.  Overall, the reviewers found the idea interesting and to have value to the RL community. The reviewers raised some questions regarding technical rigor, evaluations, and the choice of base DL algorithms. As is, I find this a slightly above borderline submission, and thus recommend acceptance. However, I would encourage the authors to test their method also with a state of the art off policy algorithms, such as TD3 or SAC, in continuous domains, to better calibrate its overall performance.
Most of the existing GNN based methods model the node labels independently and ignore the joint dependency of node labels. The CRF based methods work in this setting, but they are hard to learn. Hence, this paper proposes to ease the learning difficulty by solving the proxy problem and simplifying the max min problem.  The SMN model proposed in this work is much cheaper than the CRF method. For parameters, since the node GNN and edge GNN share parameters in layers, only a few amounts extra parameters are introduced. As for the training time, it just doubles the general GNNs. Compared with CRF methods, the cost saved by SMN is significant.  Empirically, SMN works well in most settings, in terms of both node level accuracy and graph level accuracy, the different backbones, and different datasets. Meanwhile, the authors provide results to show the effect of refinement, the shared GNNs, the different learning methods, convergence, and a tiny case study. The experimental results are significant and well organized.  After the rebuttal and discussion, all reviewers are in a favor of accepting this submission.
This paper proposes a method for self training in an open world setting where a significant portion of unlabeled data might include examples that are not task related. The proposed method (ODST) uses a more accurate OOD detection technique which allows an improved sample selection leading to higher accuracy.  Strong Points:   This paper studies a very important and impactful problem.   The paper is well written.   The empirical results show that the proposed method improves over prior work.   To better understand the iterative scheme, authors provide theoretical analysis using Bayesian decision theory.  Weak Points:   Novelty: Given prior work on different variants of noisy students, this work has limited novelty.   Dataset diversity: The main results are provided for CIFAR 10 and CIFAR 100 datasets which are very similar to each other. During the discussion period, authors added results on SVHN datasets but the accuracy gap between the proposed method and FixMatch is insignificant (FPR gap is higher but since the main goal is improving performance, I think showing accuracy is a more important measure here).   Connecting theoretical results to the rest of the paper: The paper can be improved significantly if the theoretical results are more connected to the rest of the paper and in particular with the proposed algorithm.  While 4 out of 5 reviewers are recommending rejection, I think this was a very close decision. Most reviewers were concerned with novelty which I think is a valid point. Given that and the fact that the theoretical results are very limited, showing strong empirical results are required to accept this paper. Even though the provided results on CIFAR datasets are strong, the result on SVHN does not show a significant improvement. I understand that running experiments on ImageNet might not be budget friendly. However, it is possible to run similar experiments or other datasets to show the robustness of the proposed method to the choice of the dataset. Consequently, I recommend rejecting the paper and propose authors to resubmit after adding more datasets as part of their evaluation.
This paper aims to study the effect of curvature correction techniques on training dynamics. The focus is on understanding how natural gradient based methods affect training dynamics of deep linear networks. The main conclusion of the analysis is that it does not fundamentally affect the path of convergence but rather accelerates convergence. They also show that layer correction techniques alone do not suffice. In the discussion the reviewers raised concerns about extrapolating too much based on linear networks and also lack of a cohesive literature review. One reviewer also mentioned that there is not enough technical detail. These issues were partially addressed in the response. I think the topic of the paper is interesting and timely. However, I concur with Reviewer #2 that there are still lots of missing detail and the connection with the nonlinear case is not clear (however the latter is not strictly necessary in my opinion if the rest of the paper is better written). As a result I think the paper in its current form is not ready for publication. 
This paper considers the valuation problem for a cooperative game, and shows that some classical metrics (e.g. Shapley value), can be considered as approximations to the maximum entropy.  Reviewers were generally very positive. They especially praised the novelty and writing quality, while having some concerns about the quality of the empirical results. The authors did an excellent job responding to the reviewers, and resolved their main concerns. A few quibbles remain, however, and while the manuscript is very good as is, please consider the reviewer criticisms in creating an updated version.
The submission introduces the sparse hierarchical table ensemble (S HTE), based on oblivious decision trees for tabular data. The reviewers acknowledged the clarity of the presentation and the importance of the computational complexity analysis. However, they also raised concerns regarding the novelty of the proposed method and the significance of the results compared to competing methods (e.g., CatBoost). Given the consensus that the submission is not ready for publication at ICLR, I recommend rejection at this point.
 The paper developed a method that estimates treatment effect with longitudinal observational data under temporal confounding. It extends the idea of the synthetic control method and offers flexibility and ease of estimation. However, some major concerns remain after the discussion among the reviewers. In particular, the proposed method lacks a clear use case. Moreover, some arguments around ``trustworthiness`` (detecting unreliable ITE estimates) and ``avoid over matching`` need to be refined. The error bound for ``trustworthiness`` can not detect hidden confounding. For overlap issues, the rejection of units with larger error could be overly conservative because the error bound may often be too loose. Regarding ``avoid over matching``, while SyncTwin uses a low dimensional representation as opposed to the whole x vector for matching, it is unclear whether SyncTwin can avoid over match. It is possible that using low dimensional representation makes it easier to find a match in the data and may still over match. Finally the paper would benefit from proper causal identification results.
The reviewers have pointed out that there is a substantial amount of related work that this paper should be acknowledging and building on.
This paper was on the borderline. While there was some support for the ideas presented, concerns were raised about the experiments. The exposition would also need to better demonstrate the significance of the contribution.
The paper performs a theoretical analysis of the representation power of convolutional networks with inter layer connections. Whilst the results themselves are interesting, the current presentation of the paper stands in the way of the reading grasping and appreciating the main insights from the paper.  The authors acknowledge these issues in their rebuttal, but have not yet revised the paper to resolve them. I encourage the authors to revise the paper to address the reviewer comments, and re submit it to another venue. 
This paper tackles a very valuable problem of learning object detection and object dynamics from video sequences, and builds upon the method of Zhu et al. 2018. The reviewers point out that there is a lot of engineering steps in the object proposal stage, which takes into account background subtraction to propose objects. In its current form, the writing of the paper is not clear enough on the object instantiation part, which is also the novel part over Zhu et al., potentially due to the complexity of using motion to guide object proposals. A limitation of the proposed formulation is that it works for moving cameras but only in 2d environments. Experiments on 3D environments would make this paper a much stronger submission. 
Pros:   All reviewers agreed that the idea was particularly interesting/novel. I personally appreciated the perspective of unlearning invariances that prove inconsistent with the training data, rather than learning invariances that are demonstrated by the training data.   The authors significantly improved clarity during the rebuttal period, and two out of three reviewers raised scores or confidence as a result.  Cons:   There were significant concerns raised by reviewers about clarity of presentation, and some concern around whether the specific instantiation of the high level idea was the most sensible. From a *lightweight* reading of the paper on my part, I also feel that the writing style is unnecessarily dense, though I believe the underlying ideas are solid.   One of the reviewers (AnonReviewer4) continues to have serious concerns. I believe the authors and AnonReviewer4 may have both become more entrenched in their positions during the discussion, in a way that wasn t particularly productive.  This paper is borderline score wise. I believe it is particularly important to reward and encourage unusually novel work. Primarily for this reason I bias my decision upwards, and recommend acceptance.  nit: belive  > believe
The paper received mixed reviews that overall lean negative.   The main concern shared by reviewers is the novelty of the findings. Although the paper presents a systematic study that certainly has value, reviewers do not find sufficient insights from the analysis. The ACs agree with the reviewers that the paper is below the bar for acceptance. 
The paper proposes a framework for training autoregressive flows based on proper scoring rules. The proposed framework is shown to be a computationally appealing alternative to maximum likelihood training, and is empirically validated in a wide variety of applications.  All three reviewers are positive about the paper and recommend acceptance (one weak, two strong). The reviewers describe the paper as well written and well motivated, and recognize the paper s contribution as significant.  Overall, this is a nice and promising methodological exploration of flow model training that is worth communicating to the ICLR community.
The author s revisions addressed clarity issues and some experimental issues (e.g., including MAML results in the comparison). The work takes an original path to an important problem (transfer learning, essentially). There is a question of significance, and this is due to the fact that the empirical comparisons are still very limited. The task is an artificial one derived from MNIST. I would call this "toy" as well. On this toy task, the approach isn t that much different from MAML, which is not in of itself a problem, but it would be interested to have a less superficial discussion of the differences.  The authors mention that they didn t have time for a larger empirical study. I think one is necessary in this case because the work is purposing a new learning algorithm/framework, and the question of its potential impact/significance is an empirical one.
The paper proposes a method to robustify neural networks which is an important problem. They uses ideas from causality and create a model that would only depend on "stable" features ignoring the easy to manipulate ones. The paper has some interesting ideas, however, the main concern is regarding insufficient comparison to existing literature. One of the reviewers also has concerns regarding novelty of the approach.
The paper presents the use of information bottlenecks as a way to identify key "decision states" in exploration, in a goal conditioned model. The concept of "decision states" is actually common in RL, states where exploring can lead to very diverse/new states. The implementation of the "information bottleneck" is done by adding a regularizing term, the conditional mutual information I(A;G|S).  The main weaknesses of the paper were its lack of clarity and the experimental section. It seems to me that the rebuttals, and the additional experiments and details, made the paper worthy of publication. The authors cleared enough of the gray areas and showcased the relative merits of the methods.
This paper extends Randomized least square value iteration (RLSVI), which is a method for exploration exploitation tradeoff that is suitable for linear FA, to the deep RL setting. A key component is using Hypermodels of Dwaracherla et al. (ICLR, 2020) to generate the weights of last layer of the DNN. This generates a learnable randomness required in an RLSVI like procedure. The paper provides some theoretical results regarding Hypermodels, and provides extensive experiments to show that their method is a competitive one in solving exploration problems.  The majority of the reviewers are positive about this work. The concerns include the incremental nature of this work and the empirical results. In my opinion, the algorithmic contribution is reasonable, but somehow incremental. The theoretical results are minor, but acceptable. The empirical results are extensive, though they have some shortcomings. I explain the algorithmic and empirical contributions below:   **Algorithmic Contribution:** Similar formulation has been done by  Dwaracherla et al. But that work does not consider the RL setting, and instead focuses on the bandit setting. This work provides such an extension. A straightforward application of Hypermodels does not work for DRL, but some simple, yet crucial, tricks needs to be applied to make it work. The trick is to use Hypermodel to generate the weights of the last layer, instead of all layers. Although this is simple, the fact that it enables the method to work for DRL paper is significant.   **Empirical Results:** The empirical results are quite extensive. There are two main issues with them though:  a) Many experiments on the Atari Suite are terminated after 20M samples. This is shorter than usual. b) The experiments are only repeated for 3 runs (seeds)   except one, which used 5 runs.  The authors  answer for (a) is that they have a limited compute budget, and running for 200M samples would cost them about $20K. Also they argue that 20M samples is enough to show the benefit in a better exploration method, as shown by some other papers.  After some inquiries, it seems that this $20K value has the correct order to run the experiments on a cloud (maybe within a factor of 2 or 3). Given this prohibitive cost, I am willing to accept that 20M samples might be sufficient for proving the main points of this paper.  I am not giving a large weight to this in my evaluation.  The main concern for me, however, is having only 3 independent runs of the algorithms, especially given that the issue under study is the efficiency of exploration exploitation tradeoff, and that the proposed method has a lot of randomness built in. This is the main weakness of the empirical results in my opinion, and not the 20M samples issue.  For instance, Figure 3 (Human normalized score over 56 environments in Atari 2600 suite) does not have any confidence interval information. And Figure 4 has some shaded areas around the curves, but it is not clear whether it is standard deviation, standard error, or some other quantification of uncertainty.  Even though this is a borderline paper, I recommend acceptance of this work under the expectation that the authors should improve their empirical studies. In particular, I recommend much larger number of independent runs (seeds), maybe around 10 or so, with proper information about the uncertainty of the estimates. If running this for all games is prohibitive, showing the performance with more runs on a subset of the games is sufficient. Also I encourage the authors to consider NeurIPS 2021 paper "Deep Reinforcement Learning at the Edge of the Statistical Precipice".
The paper proposes a multitask navigation model that can be trained on both vision language navigation (VLN) and navigation from dialog history (NDH) tasks. The authors provide experiments that demonstrate that their model can outperformance single task baseline models.  The paper received borderline scores with two weak accept and one weak reject.  Overall, the reviewers found the paper to be well written and easy to understand, with thorough experiments.  The reviewers had minor concerns about the following: 1. The generalizability of the work.  No results are reported on the test set, only on val. 2. The gains for val unseen are pretty small and there are other models (e.g. Ke et al, Tan et al) that have better results.  Would the proposed environment agnostic multitask learning be able to improve those models as well?  Or is the gains limited to having a weak baseline? 3. It s unclear if the gains are due to the multitasking or just having more data available to train on. 4. There are some minor issues with the misspellings/typos.  Some examples are given: Page 1: "Manolis Savva* et al"  > "Savva et al" Page 5: "x_1, x2, ..., x_3"  > Should the x_3 be something like x_k where k is the length of the utterance?  The AC agrees with the reviewers that the paper is interesting and is mostly solid work.  The AC also feels that there are some valid concerns about the generalizability of the work and that the paper would benefit from a more careful consideration of the issues raised by the reviewers.  The authors are encouraged to refine the work and resubmit.
 This paper studies the effect of label smoothing on knowledge distillation. A previous work on this topic (Muller et al.) has claimed that label smoothing can hurt the performance of the student model in knowledge distillation. The rationale behind this argument is that label smoothing erases information encoded in the labels. This work shows that such claimed effect does not necessarily happen. Specifically, by a comprehensive study on image classification, binary neural networks, and neural machine translation, the authors show that label smoothing can be compatible with knowledge distillation. However, they conclude that label smoothing will lose its effectiveness with long tailed distribution and increased number of classes.  Overall ratings of this paper are all on the positive side, and R2 finding this paper an important step toward understanding the interaction between knowledge distillation and label smoothing. I concur with the reviewers about the importance of this research direction and I think this submission provides a reasonable empirical evidence to change our earlier perspectives. I recommend accept.  While the paper specifically studies the effect of label smoothing on knowledge distillation, I think providing a bigger context and reviewing some of the recent demystifying efforts on understanding knowledge distillation could allow paper to communicate with a broader audience. I hope this can be accommodated in the final version. 
This paper tackles the problem of exploration in deep reinforcement learning in procedurally generated environments, where the same state is rarely encountered twice. The authors show that existing methods do not perform well in these settings and propose an approach based on intrinsic reward bonus to address this problem. More specifically, they combine two existing ideas for training RL policies: 1) using implicit reward based on latent state representations (Pathak et al. 2017) and 2) using implicit rewards based on difference between subsequent states (Marino et al. 2019).  Most concerns of the reviewers have been addressed in the rebuttals. Given that it builds so closely on existing ideas, the main weakness of this work seems to be the novelty. The strength of this paper resides in the extensive experiments and analysis that highlight the shortcomings of current techniques and provide insight into the behaviour of trained agents, in addition to proposing a strategy which improves upon existing methods.  The reviewers all agree that the paper should be accepted. I therefore recommend acceptance.
This paper shows how "road rules" (e.g., implicit designation of fast lanes on a highway) naturally emerge in a multi agent MDP. The paper shows that interesting traffic rules do emerge, and it presents a detailed analysis of the factors that lead to this emergence. The paper is complemented by documented source code, with the aim to encourage the community to further work on the topic.  The reviewers agreed that this is original work, and appreciated its simplicity. Two concerns that were recurrently voiced were that 1) there is no algorithmic innovation and 2) there is no comparison to baseline models, or more generally a better placement in the context of existing literature.  The authors provided a detailed and, to my eyes, convincing response. With respect to the two concerns above, I would go as far as saying that 1) (no algorithmic innovation) is a feature, not a bug. The paper is interesting exactly because it studies emergent phenomena after framing multi agent driving as a standard RL problem. Concerning 2) (lack of baselines), it seems to me somewhat besides the point: The paper is not claiming state of the art on some benchmark for a new algorithm, but studying how certain implicit rules emerge in a given setup. In this sense, as the authors point out, rather than looking at alternative baselines, it is informative to look at which aspects of the setup contribute to rule emergence, which is what the paper does.  Although I realize that in proposing this I am going beyond the reviewers  ratings, I found this to be an original and exciting paper, that I would strongly like to see accepted at the conference. 
The submission proposes a robustness certification technique for smoothed classifiers for a given l_2 attack radius.  Strengths:  The majority opinion is that this work is a non trivial extension of prior work to provide radius certification.  The work is more efficient that strong recent baselines and provides better performance.  It successfully achieves this while avoiding adversarial training, which is another novel aspect.  Weaknesses:  There were some initial concerns about missing experiments and unfair comparisons but these were sufficiently addressed in the discussion.  AC shares the majority opinion and recommends acceptance.
This paper proposes to use the GAN (i.e., minimax) framework for adversarial training, where another neural network was introduced to generate the most effective adversarial perturbation by finding the weakness of the classifier. The rebuttal was not fully convincing on why the proposed method should be superior to existing attacks.
The submission proposes a transductive few shot classification method on the basis of the simple Conditional Neural Adaptive Processes (CNAPS) introduced by Bateni et al. The paper received two borderline accept and two borderline reject reviews, indicating that the paper may not be yet ready for a publication. The meta reviewer recommends rejection based on the observations below.  All reviewers indicated that the paper is well motivated, clearly written and neatly organized. However, all four reviewers agree that the novelty of the paper compared to the CNAPS paper is limited. The main novelty of the method being transductive CNAPS extends the task encoder of CNAPS to incorporate both a support set embedding and a query set embedding through Long Short Term memory (LSTM) network. Similarly, the classifier in CNAPS has been modified to operate in the transduction setting, i.e. it is extended to include the unlabeled examples in the query set.  The reviewers indicate that extension of the task encoder via LSTM may not be enough technical novelty for such a competitive venue. Additionally, in terms of experimental evaluation, although R1 found the experimental evaluation adequate, R3 indicated some concerns about the unexpected behaviour of the method and R4&R2 found the benchmark evaluations limited. 
Exciting work at the intersection of continual learning and representation learning. The reviewers have all commented that the proposed work addresses a number of issues related to catastrophic forgetting, which is very encouraging. The work also shows that the representation learning with the proposed method is more general than the one learned with supervised CL. The reviewers have praised the work as being well written and with thorough experiments. There was a robust back and forth between the reviewers and the authors during the rebuttal period, in which the authors appear to have addressed most of the concerns. Given the insights, results and potential impact of this work, I think this work definitely should be published at ICLR.
Unfortunately, this was a borderline paper that generated disagreement among the reviewers.  After high level round of additional deliberation it was decided that this paper does not yet meet the standard for acceptance.  The paper proposes a potentially interesting approach to learning surrogates for non differentiable and non decomposable loss functions.  However, the work is a bit shallow technically, as any supporting theoretical justification is supplied by pointing to other work.  The paper would be stronger with a more serious and comprehensive analysis.  The reviewers criticized the lack of clarity in the technical exposition, which the authors attempted to mitigate in the rebuttal/revision process.  The paper would benefit from additional clarity and systematic presentation of complete details to allow reproduction.
The paper addresses unsupervised conditional text generation extending emb2emb (Mai et al, 2020) with bag of vectors antoencoders.  Reviewers shared several concerns about the clarity of this paper and empirical results.
This paper presents an ODE based latent variable model, argues that extra unobserved dimensions are necessary in general, and that deterministic encodings are also insufficient in general.  Instead, they optimize the latent representation during training.  They include small scale experiments showing that their framework beats alternatives.  In my mind, the argument about fixed mappings being inadequate is a fair one, but it misses the fact that the variational inference framework already has several ways to address this shortcoming: 1) The recognition network outputs a distribution over latent values, which in itself does not address this issue, but provides regularization benefits. 2) The recognition network is just a strategy for speeding up inference.  There s no reason you can t just do variational inference or MCMC for inference instead (which is similar to your approach), or do semi amortized variational inference.  Basically, this paper could have been somewhat convincing as a general exploration of approximate inference strategies in the latent ODE model.  Instead, it provides a lot of philosophical arguments and a small amount of empirical evidence that a particular encoder is insufficient when doing MAP inference.  It also seems like a problem that hyperparameters were copied from Chen et al 2018, but are used in a MAP setting instead of a VAE setting.  Finally, it s not clear how hyperparameters such as the size of the latent dimensions were chosen.
Four knowledgeable referees recommend Accept. I also think the paper provides a unique contribution to the field of deep survival models and I, therefore, recommend Accept
The paper tackles the important problem of spurious feature detection in deep neural networks. Specifically, it proposes a framework to identify core and spurious features by investigating the activation maps with human supervision. Then, it produces an annotated version of the ImageNet dataset with core and spurious features, called Salient ImageNet, which is then used to empirically assess the robustness of the method against spurious training signals in comparison with current SOTA models.  As pointed out by the reviewers, this work is not about causality and the definitions of causal and spurious features were originally vague and inaccurate. During the revision and discussion,  the authors changed the terms "causal" features/accuracy to "core" features/accuracy. They also called the provided dataset "Salient Imagenet", instead of "Causal Imagenet", and changed the title to "Salient ImageNet: How to Discover Spurious Features in Deep Learning?". Following the prior discussion, we strongly recommend the authors discard any discussion about causality in the camera ready version of the paper to avoid confusion. Further, we encourage the authors to consider the reviewers’ thoughts and comments in preparing the camera ready version of their manuscript.
Strengths:   This paper was clearly written, contained novel technical insights, and had SOTA results.  In particular, the explanation of the generalized dequantization trick was enlightening and I expect will be useful in this entire family of methods.  The paper also contained ablation experiments.  Weaknesses:   The paper went for a grab bag approach, when it might have been better to focus on one contribution and explore it in more detail (e.g. show that the learned pdf is smoother when using variational quantization, or showing the different in ELBO when using uniform q as suggested by R2).  Also, the main text contains many references to experiments that hadn t converged at submission time, but the submission wasn t updated during the initial discussion period.  Why not?  Points of contention:   Everyone agrees that the contributions are novel and useful.  The only question is whether the exposition is detailed enough to reproduce the new methods (the authors say they will provide code), and whether the experiments, which meet basic standards, of a high enough standard for publication, because there was little investigation into the causes of the difference in performance between models.  Consensus:   The consensus was that this paper was slightly below the bar.
This paper deals with the task of long text summarization. Inspired by earlier work on top down and bottom up architectures, this work focuses on improving the traditional bottom up converter encoder structure, and the fine resolution representations.   Pros:   Their model can model longer documents in coarse and fine granularity levels.   The performance on benchmark datasets looks pretty good compared to strong baselines   Computationally efficient.   Cons: The reviewers have raised several concerns including:   the experimental verification for calculation efficiency and memory usage of model is not sufficient.   the novelty of this design is somehow limited since the bottom up and top down idea is not new.    several details about the figures and especially the experiments were missing.  The authors have addressed several of the suggestions, added new experiments results addressing the issues raised by the reviewers. During the rebuttal period, the authors further conducted empirical investigations showing that the top down update for token representations, especially with good top level representations, leads to good summarization because of enriched token level representations by the top down. Despite positive results, some reviewers raised concerns that with only using BART as a backbone, it is surprising to achieve this great performance boost with the top down/bottom up models on long document summarization when they compared to the state of the art transformer models (BigBird, Longformer and T5) that have been shown to encode longer sequences and beat several summarization models.
 pros:   well written and clear   good evaluation with convincing ablations   moderately novel  cons:   Reviewers 1 and 3 feel the paper is somewhat incremental over previous work, combining previously proposed ideas.  (Reviewer 2 originally had concerns about the testing methodology but feels that the paper has improved in revision) (Reviewer 3 suggests an additional comparison to related work which was addressed in revision)  I appreciate the authors  revisions and engagement during the discussion period.  Overall the paper is good and I m recommending acceptance.
The paper introduces new tighter non asymptotic confidence intervals for off policy evaluation, and all reviewers generally liked the results. I recommend acceptance of this paper. Some concerns of Reviewer2 and Reviewer3 are not fully addressed in your rebuttal. Please make sure to address all remaining issues.
All reviewers are in agreement to reject this paper. The main objection is that the tasks chosen are small scale and that the mixed results are not strong enough. The authors did not attempt to raise substantial issues to be discussed.
The paper addresses hierarchical kernels and provides an analysis of their RKHS along with generalization bounds and cases where improved generalization can be obtained. The reviewers appreciated the analysis and its implications. There were multiple concerns regarding presentation clarity, which the authors should address in the camera ready version.
 pros:   The paper is well written and includes a lot of interesting connections to cog sci (though see specific clarity concerns)   The tasks considered (visual and symbolic) provide a nice opportunity to study analogy making in different settings.  cons:   There was some concerns about baselines and novelty that I think the authors have largely addressed in revision  This is an intriguing paper and an exciting direction and I think it merits acceptance.
This paper proposes the unknown aware deep neural network (UDN), which can discover out of distribution samples for CNN classifiers. Experiments show that the proposed method has an improved rejection accuracy while maintaining a good classification accuracy on the test set. Three reviewers have split reviews. Reviewer #2 provides positive review for this work, while indicating that he is not an expert in image classification. Reviewer #1 agrees that the topic is interesting, yet the experiment is not so convincing, especially with limited and simple databases. Reviewer #3 shared the similar concern that the experiments are not sufficient. Further, R3 felt that the main idea is not well explained. The ACs concur these major concerns and agree that the paper can not be accepted at its current state.
This paper presents a pre training strategy for learning graph representations using a graph to subgraph contrastive learning objective that also simultaneously discovers motifs. Pre training for graph representation learning is an important research topic and this work presents a unique solution leveraging the fact that graphs sharing a lot of motifs should be similar to one another. The approach is novel and interesting, the ability to simultaneously identify motifs are highly desirable. The results are promising showing that the proposed approach, when pretrained on the ogbn molhiv molecule dataset, worked well for several downstream chemical property prediction tasks.   However, the paper is not without weaknesses and the reviewers noticed several of them. There are many parts of the system, the graph segmenter, which relies on spectral clustering (on the affinity matrix), the EM style clustering component to extract the motifs based on the subgraphs,  the sampling loss based on the subgraph to motif similarity, and the graph to subgraph contrastive learning loss. These parts are tied together through different mechanisms and the training procedure becomes very confusing. It is unclear which parts are updated on the backpropagation path from which loss, and what choices are decided offline (i.e., not integrated into the backpropagation).  This presents great difficulty in understanding and probably using /building on the method. The paper has improved some aspects of its presentation during the review/discussion process, but the training/optimization procedure of the current version still appears quite opaque, and the reviewers heavily relied on the back and forth discussion to understand what is really going on.   Another concern is that the intuition behind some aspects of the approach and the connections between different components of the approach are a bit difficult to get/digest at places.  The intuition behind graph to subgraph contrastive learning appeared weak to the reviewer. It would be desirable to see a directly comparison to the subgraph to subgraph version. The connection between the motif discovery and the representation learning can be somewhat lost as we try to keep the many moving parts straight in the mind.   For these reasons, the paper, in its current form, cannot be accepted.
Rather than using backprop to train RNNs, this paper explores instead using GA s to train them along with an extra Minimal Description Length objective to search in the space of the simplest possible networks that can perform the task at hand. They demonstrate that the method can indeed find minimal RNNs that, when trained even on small corpus dataset of formal languages can generalize beyond the training data.  Most reviewers and myself agree that this work is really interesting, and also refreshing to see a new approach compared to the typical way of doing things. However, as we can see in the reviews, the work is not at the level of an ICLR conference submission at this point. R1, R3, and R4 s reviews breaks down the points of the papers into strengths and weaknesses, and I am inclined to believe that if the authors spend more time to try to address the weakness and improve the work, this can be a great paper in the future. Although R3 gave a score of clear reject (which is too low IMO), and the authors responded to their points, I m inclined to believe that this work warrants another revision.  Specifically, reviewers (and myself) believe that the baseline methods can indeed perform better than reported. And while, for a novel method, we don t expect the approach to scale to SOTA approaches for sequence modeling, it would improve the work vastly if there is evidence to show that it can scale to larger tasks, and give an impression that there can be a roadmap of attacking larger problems that standard methods can currently handle.  Currently, I would say the work is a great workshop paper, but would encourage the authors to continue to consider the feedback given here to work on a future revision.
This paper presents a method to learn representations of programs via code and execution.  The paper presents an interesting method, and results on branch prediction and address pre fetching are conclusive. The only main critiques associated with this paper seemed to be (1) potential lack of interest to the ICLR community, and (2) lack of comparison to other methods that similarly improve performance using other varieties of information. I am satisfied by the authors  responses to these concerns, and believe the paper warrants acceptance.
This paper presents White Box Network (WBN), which allows for composing function blocks from a given set of functions to construct a target function. The main idea is to introduce a selection layer that only selects one element of the previous layer as an input to a function block.  The reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR in its current form.  There were significant concerns about the clarity in writing, and reviewers have provided detailed discussion should the authors wish to improve the paper.
The paper received scores of 8 (R1), 6 (R2), 6 (R3). R1 s review is brief, and also is optimistic that these results demonstrated on ConvACs generalize to real convnets.  R2 and R3 feel this might be a potential problem. R2 advocates weak accept and given that R1 is keen on the paper, the AC feels it can be accepted.  
This paper proposes a “warp operator” based on Taylor expansion that can replace a block of layers in a residual network, allowing for parallelization. Taking advantage of multi GPU parallelization the paper shows increased speedup with similar performance on CIFAR 10 and CIFAR 100. R1 asked for clarification on rotational symmetry. The authors instead removed the discussion that was causing confusion (replacing with additional experimental results that had been requested). R2 had the most detailed review and thought that the idea and analysis were interesting. They also had difficulty following the discussion of symmetry (noted above). They also pointed out several other issues around clarity and had several suggestions for improving the experiments which seem to have been taken to heart by the authors, who detailed their changes in response to this review. There was also an anonymous public comment that pointed out a “fatal mathematical flaw and weak experiments”. There was a lengthy exchange between this reviewer and the authors, and the paper was actually corrected and clarified in the process. This anonymous poster was rather demanding of the authors, asking for latex formatted equations, pseudo code, and giving direction on how to respond to his/her rebuttal. I don t agree with the point that the paper is flawed by "only" presenting a speed up over ResNet, and furthermore the comment of "not everyone has access to parallelization" isn’t a fair criticism of the paper.
The key contribution of the paper is identifying that datasets have the so called DDD property. In short, datasets are predominantly composed of examples that are either consistently trivial or challenging (often misclassified) for neural networks.  Reviewer MrqK pointed out that it is well known (and provided four references) that many examples are consistently very hard or very easy for neural networks. This is true. It is somewhat novel how the authors attribute this phenomenon to datasets. Here, I would like to note that I slightly disagree with the attribution of the phenomenon to dataset alone. While it is a property of datasets, it is not self evident that deep nets trained with SGD have to learn these trivial examples. Attributing it either only to dataset or to model/optimization seems to be oversimplistic.  The second key issue of the paper is that it is somewhat inconclusive. Many datasets have the DDD property, but the Authors provide a somewhat unclear motivation for why it matters. In particular, the fact the two models make correlated errors on a dataset does not mean we cannot distinguish them. In fact, we have been able to distinguish models using IID and OOD datasets. They make correlated errors, but one makes, with a significant margin, less errors than the other. Having said that, I agree that we without the DDD property we would be able to more easily distinguish models. This is a useful perspective.  Reviewers appreciated added experiments that help better characterize what are these trivial and impossible examples.   Despite the issues with novelty and framing, I think it is a useful perspective and hopefully will encourage more research into understanding the interaction between data and training. It is my pleasure to recommend acceptance and thank you for submitting the paper.  In the camera ready, please: (a) describe much more clearly and openly relation to prior work; (b) bring to the main text more data from the psychophysical experiments; and (c) address any other remarks made by reviewers.
This paper has, at its core, a potential for constituting a valuable contribution. However, there was a shared belief among reviewers (that I also share) that the paper still has much room for improvement in terms of presentation and justification of the claims. I hope that the authors will be able to address the feedback they received to make this submission get where it should be. 
This paper empirically studies the impact of different types of negatives used in recent contrastive self supervised learning methods. Results were initially shown on Mocov2, though after rebuttal simCLR was also added, and several interesting findings were found including that only hardest 5% of the negatives are necessary and sufficient. While the reviewers saw the benefit of rigorously studying this aspect of recent advances in self supervised learning, a number of issues were raised including: 1) The limited scope of the conclusions, given that only two (after rebuttal) algorithms were used on one datasets, 2) Limited connections drawn to existing works on hard negative mining (which is very common across machine learning including metric learning and object detection), and 3) Limited discussion of some of the methodological issues such as use of measures that are intrinsically tied to the model s weights (hence being less reliable early in the training) and WordNet as a measure for semantic similarity. Though the authors provided lengthy rebuttals, the reviewers still felt some of these issues were not addressed. As a result, I recommend rejection in this cycle, and that the authors bolster some of these aspects for a submission to future venues.   I would like to emphasize that this type of work, which provides rigorous empirical investigation of various phenomena in machine learning, is indeed important and worth doing. Hence, the lack of a new method (e.g. to address the selection of negatives) was not the basis of the decision. While the paper clearly does a thorough job at investigating these issues for a limited scope (e.g. in terms of datasets), a larger contribution is expected for empirical papers such that 1) we can ensure the generality of the conclusions (across methods and datasets), 2) we have a conceptual framework for understanding the empirical results especially with respect to what is already known in adjacent areas (e.g. metric learning and object detection), and 3) we understand some of the methodological choices that were made and why they are sufficiently justified. 
This paper studies maximum entropy reinforcement learning in more detail. Maximum entropy is a popular strategy in modern RL methods and also seems used in human and animal decision making. However, it does not lead to optimize expected utility. The authors propose a setting in which maximum entropy RL is an optimal solution.   The authors were quite split on the paper, and there has been an animated discussion between the reviewers among each other and with the authors.   The technical quality is good, although one reviewer commented on the restricted setting of the experiments (bandit problems). The authors have addressed this by adding an additional experiment. Futhermore, two reviewers commented that the clarity of the paper could be improved.   A larger part of the discussion (also the private discussion) revolved around relevance and significance, especially of the meta pomdp setting that takes up a large part of the manuscript.    A reviewer mentioned that after reading the paper, it does not become more clear why maximum entropy RL works well in practice. The discussion even turned to why MaxEntropyRL might be *unreasonable* from the point of view of needing a meta POMDP with Markov assumptions, which doesn t help shed light on its empirical success. The meta POMDP setting does not seem to reflect the use cases where maximum entropy RL has done well in emperical studies.    Another reviewer mentioned that earlier papers have investigated maximum entropy RL, and that the paper tries to offer a new perspective with the Meta POMDP setting. The discussion of this discussion was not deemed complete in current state and needs more attention (splitting the paper into two along these lines is a possibility mooted by two of the reviewers). A particular example was the doctor patient example, where in the meta POMDP setting the doctor would repeatedly attempt to cure a fixed sampled illness, rather than e.g. solving for a new illness each time.   Based on the discussion, I would conclude that the topic broached by the paper is very relevant and timely, however, that the paper would benefit from a round of major revision and resubmission rather than being accepted to ICLR in current form. 
This paper proposes a “Mixup” type of data augmentation for graphs that accounts for the difficulty of mixing graphs of different number of nodes. The authors show that the mixed graphs are invertible functions of the original graphs.  Reviewer d3Ri liked the simplicity and effectiveness of the technique. They called it a “healthy and useful contribution for the field”. Reviewer n1Dk thought that the paper explored an important problem and thought the paper was clear, though some of the math could have been simplified. This reviewer was concerned that a central claim of the paper, that the method avoids “Manifold Intrusion” was unsubstantiated. Specifically that it could not be deduced from the fact that edge connectivity could be recovered from the mixed graphs. The reviewer claimed that node features of the individual graphs were unrecoverable. The authors responded in detail to the reviewer’s criticism, adding two new lemmas which purportedly guaranteed node feature vectors could be uniquely recovered. The authors admitted to some conversion between “Manifold intrusion” and invertibility and added a Theorem and its proof that invertibility guarantees no manifold intrusion. The authors also responded to reviewer n1Dk’s concern about the significance of the reported improvements. Reviewer n1Dk responded to the author rebuttal with concerns about the strong and unrealistic assumption of linear independence of the feature matrix. They had further concerns that for the case of weighted edges the “Intrusion free” property could not be enforced. Discussion ensued, with the authors arguing that the independence assumption was not as strong as the reviewer claimed and that the “Intrusion free” property was only every for graphs with binary edge weights.   Reviewer 7hBS and q8bs were both on the fence. 7hBS also raised some concern with the case of non binary weighted edges. They also raised the same issue with respect to the connection b/w invertibility and the “Intrusion free” property, which the authors addressed. Reviewer q8bs also thought the problem was interesting, the paper was clear, yet like n1Dk thought the performance improvement was marginal and had concerns with technical novelty of the work.  This was a tough call, so I engaged the reviewers in further discussion. 7hBS agreed with n1Dk’s opinion that the central claim of the paper (the method being intrusion free) was not presented with strong evidence. They also raised another concern, which was that the paper didn’t evaluate on node classification like most other graph mixup style models. Q8bs agreed with n1Dk’s concerns and felt that post discussion the technical novelty of the work was limited. Without strong support from the reviewers, I think that this paper could use further development, either lightening the “intrusion free” claim or presenting evidence for it in other settings.
The paper proposes a recurrent neural network architecture for abstract rule learning. An LSTM is augmented with a two stream memory structure: one block is populated with visual representations, and the other is populated by hidden state vectors from the RNN controller.  The authors also introduce a set of tasks that require a simple symbolic reasoning on visual inputs and strong extrapolation ability. They show that previous memory augmented neural networks fail on these tasks, whereas their model exhibits excellent generalization with limited training data.  Pro: The work addresses an important and open question in neuroscience and deep learning. The proposed solution is simple and effective. The manuscript is well written. It was also improved in a revised version after the first review round.  Con: The main criticism raised by the reviewers was that the considered tasks may be a too simplified synthetic task.  It would have been good to consider other the more complex tasks involving symbolic reasoning such as CLEVR or bAbI.  While this is a valid criticism, all reviewers agreed that this is an interesting and important work worth publishing. In particular, the considered question is of pivotal importance for the community and the work presents a significant progress.
The paper explores the effect of normalization and initialization in residual networks, motivated by the need to avoid exploding and vanishing activations and gradients. Based on some theoretical analysis of stepsizes in SGD, the authors propose a sensible but effective way of initializing a network that greatly increases training stability. In a nutshell, the method comes down to initializing the residual layers such that a single step of SGD results in a change in activations that is invariant to the depth of the network. The experiments in the paper provide supporting evidence for the benefits; the authors were able to train networks of up to 10,000 layers deep. The experiments have sufficient depth to support the claims. Overall, the method seems to be a simple but effective technique for learning very deep residual networks.   While some aspects of the network have been used in earlier work, such as initializing residual branches to output zeros, these earlier methods lacked the rescaling aspect, which seems crucial to the performance of this network.  The reviewers agree that the papers provides interesting ideas and significant theoretical and empirical contributions. The main concerns by the reviewers were addressed by the author responses. The AC finds that the remaining concerns raised by the reviewers are minor and insufficient for rejection of the paper. 
# Quality: I personally feel that the comment from Reviewer1 regarding "real world" is a minor but valid point. Even after the rebuttal, the abstract seems to suggest that the proposed algorithm is effective to solve real world challenges. Maybe further rephrasing or explicitly stating that the experiments are in simulation might help to clarify this point.  Reviewer3 also raised valid points regarding the experimental evaluation and the use of just 3 seeds. Given the struggle in reproducibility in RL and the shady experimental practices from even leading AI companies (e.g., cherry picking of seeds), it is paramount that experiments follow strong methodologies and good practices. As such, the experimental results presented in this manuscript should be strengthened.  # Clarity: All the reviewers pointed out that the paper writing should be improved. Although the authors significantly improved the manuscript during the rebuttal period. Several reviewers suggested that the manuscript should be further polished before publication.  # Originality: The two proposed approaches are novel to the best of the reviewers and my knowledge. Two reviewers pointed out that the theoretical results should be explained more thoroughly and to clearly differentiate from prior work.  # Significance of this work:  The paper deal with an important and timely topic. Although the work could be very impactful for real world applications, there is no real world application. Hence it is difficult to gauge the significance of the work.   # Overall: The paper does not feel quite ready for publication yet. A clearer presentation and extended experiments would certainly improve the quality of the manuscript.
This paper investigates a notion of recognizing insideness (i.e., whether a pixel is inside a closed curve/shape in the image) with deep networks. It s an interesting problem, and the authors provide analysis on the limitations of existing architectures (e.g., feedforward and recurrent networks) and present a trick to handle the long range relationships. While the topic is interesting, the constructed datasets are quite artificial and it s unclear how this study can lead to practically useful results (e.g., improvement in semantic segmentation, etc.). 
The authors address a very important question pertaining to the relevance of morphological complexity in the ability of transformer based conditional language models. Through extensive (controlled) experiments using 6 languages they answer as well as raise very interesting questions about the role of morphology/segmentation/vocab size which mat spawn more work in this area.  All the reviewers were positive about the paper and agreed that the paper made significant contributions which would be useful to the community. More importantly, the authors and reviewers engaged in meaningful and insightful discussions through the discussion phase. The authors did a thorough job of addressing all reviewer concerns and changing the draft of the paper accordingly.   I have no hesitation in recommending that this paper should be accepted.
The paper presents some efficiency improvements over existing methods to compute matrix square root and its gradient.  Reviewers find that the novelty over existing methods is sufficient, and that the improvements are valuable.  I propose a poster despite the relatively high numerical scores, because the group of practitioners who will use the result is somewhat niche   the reviewers are of course selected from this group and hence value the paper more highly.  In addition the real world speedups are modest, but it is nevertheless important to document this approach.
The reviewers all like the idea, and though the performance is a little better when compared to prototypical networks, the reviewers felt that the contribution over and above prototypical networks was marginal and none of them was willing to champion the paper. There is merit in that there is increased robustness to outliers, and future iterations of the paper should work to strengthen this aspect.  As a quick nitpick: based on my reading, and on Figure 3, it looks like there might be a typo in the definition of X_k (bottom of page 4). Right now it is defined in terms of the original data space x, when I think it should be defined in terms of the embedding space f(x). Overall this paper is a good contribution to the few shot learning area.
The paper presents a Bayesian approach for classification able  to  adapt  to  novel  classes  given  only  a  few  labeled  examples. The models combines a one vs each approximation of the likelihood combined with a Gaussian process. This allows to resort to a data augmentation scheme based on Polya gamma random variables.  The paper is clearly written and combines existing techniques in a convincing manner; the experiments demonstrate better accuracy and uncertainty quantification on benchmark datasets.   I recommend acceptance.
The reviewers were unanimous that this submission is not ready for publication at ICLR in its current form.  Concerns raised included that the method was not sufficiently general, including in choice of experiments reported, and the lack of discussion of some lines of significantly related work.
This paper tackles hard exploration RL problems using learning from demonstrations. The idea is to combine the existing R2D2 algorithms with imitation learning from human demonstrations. Experiments are conducted on a new set of challenging tasks, highlighting limitations of strong current baseline while highlighting the strength of the proposed approach.  The contribution is two folds: the proposed algorithm which clear outperforms previous SOTA agents and the set of benchmarks. All reviewers being positive about this paper, I therefore recommend acceptance.
The paper gives high probability bounds on excess risk for differentially private learning algorithms, in the setting where the loss is assumed to be Lipschitz, smooth, and assumed to satisfy the Polyak Łojasiewicz (PL) condition. The key idea in the paper is to leverage the curvature in the loss (PL condition) and the generalized Bernstein condition.   Authors show that they get sharper bounds of the order \sqrt{p}/(n\epsilon) when the loss is assumed to satisfy the PL condition besides being convex Lipschitz/smooth. Without using some curvature information about the loss function, the best upper bounds we can get are in the order of \sqrt{p}/(n\epsilon) + 1/\sqrt{n} — and this is tight at least in terms of the dependence on n given the nearly matching lower bounds — in fact, the dependence on n is tight as it matches the non private settings.    So, I find it a bit misleading when authors say that they improve over the existing results. That statement is not true in its generality — it is true that we can leverage the PL condition to give faster rates but that is not the setting of prior work. Again, the bounds that authors compare against are for smooth/Lipschitz convex loss functions and without any assumption on the curvature of the loss.   If we do look at the literature for when and/or how can curvature help, we can compare against the existing bounds for strongly convex losses. The best known result in the setting that is most closely related is that of Feldman et al. (STOC 2020): https://dl.acm.org/doi/pdf/10.1145/3357713.3384335. As we can check from Theorem 4.9 in that paper, the bounds we get are in the order of 1/n + d/n^2 which is actually better — not surprising since PL condition is a weaker condition. There is merit to the results in this paper but the current narrative is quite misleading and a more careful comparison with the existing literature is needed. The bounds are hard to parse — for example, what is the dependence on the strong convexity parameter (\mu)? It would also help to instantiate specific loss functions so that we can fix some of the parameters in the bound to have a clear comparison with the existing bounds.
This paper studies generalizations of Variational Autoencoders to Non Euclidean domains, modeled as products of constant curvature Riemannian manifolds. The framework allows to simultaneously learn the latent representations as well as the curvature of the latent domain.   Reviewers were unanimous at highlighting the significance of this work at developing non Euclidean tools for generative modeling. Despite the somewhat preliminary nature of the empirical evaluation, there was consensus that the paper puts forward interesting tools that might spark future research in this direction. Given those positive assessments, the AC recommends acceptance.  
The paper proposes a definition of and an algorithm for computing the importance                                                    of features in time series classification / regression.                                                                             The importance is defined as a finite difference version of standard sensitivity                                                    analysis, where the distribution over finite perturbations is given by a                                                            learned time series model.                                                                                                          The approach is tested on simulated and real world data sets.                                                                                                                                                                                                           The reviewers note a lack of novelty in the paper and deem the contribution                                                         somewhat incremental, although exposition and experiments have improved compared                                                    to previous versions of the manuscript.                                                                                                                                                                                                                                 I recommend to reject this paper in its current form, taking into account on the reviews and my own                                 reading, mostly due t a lack of novelty.                                                                                            Furthermore, the authors call their method a "counterfactual" approach.                                                             I don t agree with this terminology.                                                                                                No attempt is made to justify is by linking it to the relevant causal literature                                                    on counterfactuals.                                                                                                                 The authors do indeed motivate their algorithm by considering how the classifier                                                    output would change "had an observation been different" (a counterfactual), but                                                     mathematical in their model this the same as asking "what changes if the observation is                                             different" (interventional query). 
This paper proposes a novel variational autoencoder to utilize functional connectivity (FC) features from resting state fMRI (rs fMRI) scans in order to uncover latent nosological relationships between diverse yet related neuropsychiatric disorders. The methodology and main technical contributions are clearly articulated and explained, and the experimental results seem convincing. On the other hand, the proposed framework is somewhat limited in scope and clinical applicability, and the writing in the paper needs improvement (as pointed out by two reviewers).
This paper proposes a conditional quantile generative model using optimal transport. Although the problem addressed in this paper is interesting and important, the proposed convex potential quantile (CPQ) approach is highly relevant to a recent work (Carlier et al. 2017). Due to the lack of clear explanations of the contributions compared to the existing work, none of the reviewers suggested acceptance of this paper.
This paper proposes a theoretically motivated method for combining reinforcement learning and imitation learning. There was some disagreement amongst the reviewers, but the AC was satisfied with the authors  rebuttal.
The work considers sparse and short blind deconvolution problem, which is to inverse a convolution of a sparse source (such as spikes at cell locations in microscopy) with a short (of limited spatial size) kernel or point spread function, not known in advance. This is posed as a bilinear lasso optimization problem. The work applies a non linear optimization method with some practical improvements (such as data driven initialization, momentum, homotopy continuation).  The paper extends the work by Kuo et al. (2019) by providing a practical algorithm for solving those inverse problems. A focus of the paper is to solve the bilinear lasso instead of the approximate bilinear lasso, because this approximation is poor for coherent problems. Having read the rebuttal and the paper, I believe the authors addressed the issues raised by Reviewer #2 in a sufficient way.  small things:   it would be good to define $\iota$ (zero padding operator) in (1)   it would be good to define $p, p_0$ just below (3). They seem to be appearing out of the blue without any direct relation to anything mentioned prior in section 2.   it would be good to cite some older/historic references for various optimization methods , e.g. [1] below.    [1] Richter & deCarlo  Continuation methods: Theory and applications IEEE Transactions on Systems, Man, and Cybernetics, 1983 https://ieeexplore.ieee.org/abstract/document/6313131
Most reviewers seems in favour of accepting this paper, with the borderline rejection being satisfied with acceptance if the authors take special heed of their comments to improve the clarity of the paper when preparing the final version. From examination of the reviews, the paper achieves enough to warrant publication. Accept.
This paper presents an algorithm for approximating the hypergradient for bilevel optimization using a trick based on evolution strategies. It seems like an interesting approach, somewhat reminiscent of STNs, so it s interesting to see experiments with it.  I have a big concern about the proposed justification of the method, namely that each iteration is more efficient than methods based on HVPs. The authors claim that because they only require gradient computations and not HVPs, their method is more efficient. However, as various reviewers point out, the proposed method requires numerous inner optimization runs. By comparison, a method based on unrolled backprop simply requires a single inner run, followed by backprop on the trajectory; hence it should be about as expensive as 2 3 inner optimizations (or less if it is truncated BPTT). Similarly, each HVP has a small multiple of the cost of an inner optimization step, so methods based on HVPs ought to be cheaper unless they re doing quite a lot of HVPs.  It s conceivable the proposed method could be more efficient than AID, etc. if each hypergradient estimate is more accurate. However, this isn t shown, and it would seem surprising for an ES based approximation to be more accurate than the exact gradient.  The authors claim in the rebuttal that the efficiency claims aren t based on the theoretical analysis, but rather on the experiments (which use Q 1); however, Section 3.2 still finishes with the conclusion that ESJ is more efficient, which seems problematic.  I encourage the authors to formulate their theoretical claims more carefully and to consider the reviewers  other feedback, and I think this could make an interesting submission for the next cycle.
This paper proposes a decomposition based explanation method for graph neural networks. The motivation of this paper is that existing works based on approximation and perturbation suffer from various drawbacks. To address the challenges of existing works, the authors directly decompose the influence of node groups in the forward pass. The decomposition rules are designed for GCN and GAT. Further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. Experiments on synthetic and real world datasets verify the improvements over existing works. During their initial responses, reviewers suggested that the authors experiment with more baselines and also clarify some of the technical details. The authors revised their manuscript to address several of these comments. So, I am tentatively assigning an accept to this paper.
The paper proposes new regularizations on contrastive disentanglement. After reading the author s response,  all the reviewers still think that the contribution is too limited and all agree to reject.
   The paper proposed the use of a combination of RL based iterative improvement operator to refine the solution progressively for the capacitated vehicle routing problem. It has been shown to outperform both classical non learning based and SOTA learning based methods. The idea is novel and the results are impressive, the presentation is clear. Also the authors addressed the concern of lacking justification on larger tasks by including an appendix of additional experiments. 
This work extends previous work on unsupervised learning of goal conditioned policies: an abstract skill policy, which drives exploration of the state space, is used to propose goals as well as derive rewards for a goal conditioned policy.   Reviewers agreed the approach was novel and interesting. All reviewers raised significant concerns about clarity and/or lack of details, as well as a lack of comparison to DIAYN/DISCERN, though these points were adequately addressed in revisions. One remaining issue raised by two reviewers are that the content related to the information bottleneck/disentangled representation learning seems out of place and ill justified. Detailed discussion of this aspect of the work has been relegated to the appendix.  This is an important problem and a growing area of study, and while the submission has potential, improvements needed are not minor, and given the short process, we can only accept papers as is, rather than expecting certain changes. We urge the authors to further improve the focus of the work and perhaps plan to investigate the role and importance of disentanglement with IB in this setting in follow up work wherein they have the space to properly do justice to the topic in its own right.
This was a very borderline decision. Here are the major factors involved in the decision.   1. The concurrent works by Li et al and Yu et al. It is unclear about the relationship/strength between those results and the ones in the present paper. However, in accordance with the ICLR policy on simultaneous work, we ignore them to the extent possible. 2. The novelty of the approach. All reviewers agreed that the modifications to the PATE approach are fairly minor or incremental, and this appears to be largely an application of this method. That said, methods for the natural language setting are important. 3. The strength of the findings. Reviewers were mixed on the strength of the results: they appeared significant in some cases but rather lackluster in others.  4. The poor quality of the writing. I personally read the original version and found the writing to make it impossible to understand in parts. The writing is still subpar in many places, which I would have expected the authors to fully polish given the substantial time during the response period.   While in isolation, any of 2, 3, and 4 may be acceptable, their combination makes it difficult to recommend this paper for acceptance at this time.
The reviewers uniformly vote to accept this paper. Please take comments into account when revising for the camera ready. I was also very impressed by the authors  responsiveness to reviewer comments, putting in additional work after submission.
This paper provides a method of encoding inputs to a spiking neural network (SNN) using the discrete cosine transform (DCT). The goal is to create a more energy and time efficient means of doing inference with SNNs. The authors provide a description of the method, then show accuracy results on a variety of standard benchmarks. They also compare to a number of other methods for ANN and SNN based inference in the literature. Altogether, they show that their method allows for accurate inference using fewer spikes than other approaches, which can potentially reduce the energy used for inference.  The paper is fairly clearly written, and the results well articulated. The reviewers had a number of concerns, most notably related to questions of (1) clarity about the actual benefits of this approach, and (2) fair comparisons to other models. Altogether, the authors did try to address the reviewers comments, and at least one reviewer increased their score.   However, the actual scores for this paper remained very close to the acceptance threshold, and the first point was difficult to rebut without a lot more added to the paper. Ultimately, this paper is using a classic signal processing strategy to improve SNN run time, and the reviewers asked for some reason as to why that is desirable/novel. The author s answer was effectively that SNNs provide promise for low energy edge computing, and their method could make SNNs for edge computing even more efficient.  This is potentially of interest for edge computing, but the paper could do a lot more to demonstrate that. Specifically, some consideration of how this would actually operate on spiking chips or a more robust estimate of energy efficiency than that given at the end of section 4 would be required to make this paper a clear accept. Notably, the paper does not demonstrate that this technique could be used to significantly reduce the energy requirements for spiking chips, relative to other SNNs, just that this is more energy efficient than ANNs, which is already known for other spiking neural network approaches. Given this, and the scores relative to other papers at ICLR, a "reject" is recommended. However, the AC notes that this was a difficult decision, and this paper was right at the threshold.
The paper considers the problem of learning both the physical design (morphology and parameters) of a robot together with the corresponding control policy to optimize performance at a target task. Unlike several contemporary methods that formulate this as two separate, but coupled, optimization problems, the paper unifies these decisions into a single decision making framework. More specifically, a conditional policy learns to first change an agent s physical design (i.e., the morphology/skeletal structure and its associated parameters), and then to control the design. The policy is formulated as a graph neural network, enabling a single policy to simultaneously control robots with different morphologies (and, in turn, different action spaces). Experimental results demonstrate that the approach outperforms recent baselines on a variety of simulated control tasks.  The paper considers an interesting and challenging problem, that of jointly optimizing an agent s physical design and its control policy, an area of research that has received renewed attention of late. As the reviewers note, the idea of treating design and control in the context of a single decision making process is novel. The approach is principled and the experimental results largely justify the significance of the contributions. The reviewers agree that the approach is described clearly and that the paper is well written. The reviewers initially raised a few concerns regarding the experimental evaluation, including the desire for more in depth evaluations and the need for more random seeds. They also questioned some of the claims made in the initial submission. The authors provided a detailed response to each of these points and made changes to the paper to resolve most of the concerns.  In summary, the paper proposes a novel approach to an interesting problem with convincing results.
This paper addresses the problem of causal inference from incomplete data. The main idea is to use a latent confounders through a VAE. A multiple imputation strategy is then used to account for missing values. Reviewers have mixed responses to this paper. Initially, the scores were 8,6,3. After discussion the reviewer who rated is 8 reduced their score to 6, but at the same time the score of 3 went up to 6. The reviewers agree that the problem tackled in the paper is difficult, and also acknowledge that the rebuttal of the paper was reasonable and honest. The authors added a simulation study which shows good results.  The main argument towards rejection is that the paper does not beat the state of the art. I do think that this is still ok if the paper brings useful insights for the community even though it does not beat the state fo the art. For now, with the current score, the paper does not make the cut. For this reason, I recommend to reject the paper, but I encourage the authors to resubmit this to another venue after improving the paper.
This manuscript proposes and evaluates new metrics for measuring the quality of disentangled representations for both supervised and unsupervised settings. The contributions include conceptual definitions and empirical evaluation.  In reviews and discussion, the reviewers and AC note missing or inadequate empirical evaluation with many available methods for learning disentangled representations. On the writing, reviewers mentioned that the conciseness of the manuscript could be improved. The reviewers also mentioned incomplete references and discussion of prior work, which should be improved.
The paper proposes a novel approach for DNN inversion mainly targeted towards semi supervised learning. However the semi supervised learning results are not competitive enough. Although the authors mention in the author response that semi supervised learning is not the main goal of the paper, the experiments and claims of the paper are mainly targeted towards semi supervised learning. As the approach for inversion is novel, the paper could be motivated from a different angle with appropriate supporting experiments. In its current form it s not suitable for publication. 
The paper proposes a domain adaptation method that is specific to auto encoder and wireless domain. The proposed method shows solid gain over baseline and is simple. There were multiple complaints on reviewer s side regarding clarity of the abstract and application of the work and related work that was responded to by the authors during the rebuttal period. However, the modifications were not enough to address all concerns. Mainly, the assumptions for the method to work such as source and target having the same number of components and diagonal covariance. It would help the paper to discuss the cases where the model fails. In addition, the paper will benefit from a stronger baseline.
This paper studies the memorization power of Relu Neural networks and obtains sharp bounds in terms of parameters. The writing is very clear and the results very interesting.
This paper studies the combination between model uncertainty and data uncertainty based on the spectral normalized Gaussian process. Empirical results show the effectiveness of the proposed method. Overall, the paper is well motivated and well written. However, there are several concerns about the paper. (1) The novelty is marginal. The contribution of combining SNGP and heteroscedastic models into a single model may not be enough. (2) More analyses and insights are needed on why the mentioned two types of uncertainty are complementary. (3) More recent state of the art methods on classification with noisy labels are suggested to be included to interest the readers. There are diverse scores. However, no one wants to champion the paper. We believe that the paper will be a strong one by addressing the concerns.
This paper proposes a method to do zero shot ICD coding, which involves assigning natural language labels (ICD codes) to input text. This is an important practical problem in healthcare, and it is not straightforward to solve, because many ICD codes have none or very few training examples due to the long distribution tail. The authors adapt a GAN based technique previously used in vision to solve this problem. All of the reviewers agree that the paper is well written and well executed, and that the results are good. However, the reviewers have expressed concerns about the novelty of the GAN adaptation step, and left this paper very much borderline based on the scores it received. Due to the capacity restrictions I therefore have to recommend rejection, however I hope that the  authors resubmit elsewhere. 
This paper introduces a clever new problem that may prove useful in the advancement of Automatic Theorem Proving   finding intermediate steps in a proof. A non synthetic benchmark is created based on a large human created dataset of proofs. Neural models were shown to have non trivial performance. Reviewers were convinced that this is ultimately a useful benchmark.
The paper introduces an cross layer attention mechanism for image restoration. To reduce the computational complexity, the framework uses deformable convolutions and an adaptive selection for reducing the number of keys, as well as a neural architecture search. The paper received three borderline reject recommendations and a clear accept. After reading the reviews, responses, and the paper in details, the area chair agrees with Reviewer 6N93 that the paper has some merit. Unfortunately, he/she also agrees with the fact that the proposed framework is quite complicated with many components for a marginal improvement (something that also Reviewer 6N93 has mentioned in the discussion between reviewers). Overall, this points towards rejection, which is the final recommendation of the area chair.  Another point that would be helpful, in case this paper is resubmitted elsewhere, is to release the code for the method, given its complexity.
The paper studies federated learning in what they call ``` single sided trust  scenario, i.e. there is no dedicated server and the trust relationship is asymmetric.  This paper was a trickier case to decide on, and more borderline, in our opinion, than the reviewers  scores suggest, primarily, because the reviewers  recommendations are based on more subjective notions of novelty and importance/appropriateness of the studied setting, rather than identifying specific flaws in theoretical analysis or experiments. Ultimately, it boils down to three reviewers being (rather) negative about the paper, and the only (very) positive reviewer not stepping in to champion it. The negative reviewers believed that the novelty is not substantial enough to meet the high ICLR acceptance bar  (see details in reviews by R1 and R2 re similarity to ) and also have questioned the general motivation  (R1) and/or the online learning setting (R3).  While this assessment may be too harsh (esp. R1)   I think that the paper has merit   I share their feeling that in its current form it does not have a strong enough contribution.    
The authors introduce an approach to learn a random forest model and a representation simultaneously. The basic idea is to modify the representation so that subsequent trees in the random forest are less correlated.  The authors evaluate the technique empirically and show some modest gains. While the reviews were mixed, the approach is quite different from the usual approaches published at ICLR  and so I think it s worth highlighting this work. 
The paper receives mixed ratings. All reviewers agree that the paper is well motivated and the updated draft is clear. However, the experiment results are not fully convincing especially given the added complexity. In addition, it is hard to fully understand where the gain comes from. We hope the reviews can help improve the draft for a strong publication in the future. 
### Summary  The paper investigates the relation between pruning and splines. The results show that the prunable nodes of networks correspond to splines that do not affect the ultimate decision boundary of the network. The results also show that splines stabilize early in training, in correspondence with the claims of early bird tickets. Finally, the results show the connection between similarity based pruning and splines, with an evaluation of the quality of similarity based pruning.  ### Discussion  The paper demonstrates a very interesting connection with splines and promising results with the multiple analyses. However, there could be more precision in the stated connections between splines and pruning. Also, reviewers requested more elaboration of the base technical definitions (e.g., splines).  ### Recommendation  I recommend Reject. While the first part of this paper is quite strong in that the spline view of networks provides an interesting analysis framework that draws out interesting connections to pruning. The latter part that develops the concrete pruning algorithm needs reenvisioning. Specifically, to the best of my reading, using the similarity of filters for pruning is not new [1,2,3]. That means the proposed spline pruning policy approach cannot be claimed as new. Though it is reasonable to claim the connection to splines as new.  Second, while the evaluation is extensive in its comparison to other techniques, it is quite murky. The results claim victory by including both spline and EB spline, neither of which strictly dominates the other. My suggestion is to remove the EB spline and, in the worst case, claim that similarity based pruning is (1) competitive with other approaches and (2) rests on a foundational theory that is a new alternative to typical importance based techniques. That latter observation is an important one for the community that can lead everyone into new productive directions.   Additionally, in line with some requests from the reviews for more analysis, simplifying the evaluation (with fewer and less ambitious claims) and, even perhaps, eliminating the EB analysis altogether, would make room available for additional analysis and elaboration.  [1] BCAP: An Artificial Neural Network Pruning Technique to Reduce Overfitting Kiante Brantley. UMD Masters Thesis, 2016  [2] A dynamic CNN pruning method based on matrix similarity Mingwen Shao, Junhui Dai, Jiandong Kuang, Deyu Meng. Signal, Image and Video Processing volume 15, pages 381–389 (2021). Published August, 2020  [3] Similarity Based Filter Pruning for Efficient Super Resolution Models. Chu Chu; Li Chen; Zhiyong Gao. IEEE International Symposium on Broadband Multimedia Systems and Broadcasting, 2020
meta score: 4 This paper concerns a variant to previous RNN architectures using temporal skip connections, with experimentation on the PTB language modelling task The reviewers all recommend that the paper is not ready for publication and thus should be rejected from ICLR.  The novelty of the paper and its relation to the state of the art is not clear.  The experimental validation is weak. Pros:    possibly interesting idea Cons:    weak experimental validation    weak connection to the state of the art    precise original contribution w.r.t state of the art is not clear 
This paper proposes to pre train a feature embedding, using Siamese networks, for use with few shot learning for SVMs.  The idea is not very novel since there is a fairly large body of work in the general setting of pre trained features + simple predictor.  In addition, the experimental results could be stronger   there are stronger results in the literature (not cited), and better data sets for testing few shot learning.
(Please note that I am basing the meta review on two reviews plus my own thorough read of the paper) This paper proposes an interesting adaptation of the non autoregressive neural encoder decoder models previously proposed for machine translation to dialog state tracking. Experimental results demonstrate state of the art for the MultiWOZ, multi domain dialog corpus. The reviewers suggest that while the NA approach is not novel, author s adaptation of the approach to dialog state tracking and detailed experimental analysis are interesting and convincing. Hence I suggest accepting the paper as a poster presentation.
This paper studies Differentiable Neural Architecture Search, focusing on a problem identified with the approximated gradient with respect to architectural parameters, and proposing an improved gradient estimation procedure. The authors claim that this alleviates the tendency of DARTS to collapse on degenerate architectures consisting of e.g. all skip connections, presently dealt with via early stopping.  Reviewers generally liked the theoretical contribution, but found the evidence insufficient to support the claims. Requests for experiments by R1 with matched hyperparameters were granted (and several reviewers felt this strengthened the submission), though relegated to an appendix, but after a lengthy discussion reviewers still felt the evidence was insufficient.  R1 also contended that the authors were overly dogmatic regarding "AutoML"   that the early stopping heuristic was undesirable because of the additional human knowledge involved. I appreciate the sentiment but find this argument unconvincing   while it is true that a great deal of human knowledge is still necessary to make architecture search work, the aim is certainly to develop fool proof automatic methods.   As reviewers were still unsatisfied with the empirical investigation after revisions and found that the weight of the contribution was insufficient for a 10 page paper, I recommend rejection at this time, while encouraging the authors to take seriously the reviewers  requests for a systematic study of the source of the empirical gains in order to strengthen their paper for future submission.
The paper describes a method to train a convolutional network with large capacity, where channel gating (input conditioned) is implemented   thus, only parts of the network are used at inference time. The paper builds over previous work, with the main contribution being a "batch shaping" technique that regularizes the channel gating to follow a beta distribution, combined with L0 regularization. The paper shows that ResNet trained with this technique can achieve higher accuracy with lower theoretical MACs. Weakness of the paper is that more engineering would be required to convert the theoretical MACs into actual running time   which would further validate the practicality of the approach. 
This work studies small but critical subnetworks, called winning tickets, that have very similar performance to an entire network, even with much less training. They show how to identify these early in the training of the entire network, saving computation and time in identifying them and then overall for the prediction task as a whole.   The reviewers agree this paper is well presented and of general interest to the community. Therefore, we recommend that the paper be accepted.
This paper proposed an improvement on VAE GAN which draws multiple samples from the reparameterized latent distribution for each inferred q(z|x), and only backpropagates reconstruction error for the resulting G(z) which has the lowest reconstruction.  While the idea is interesting, the novelty is not high compared with existing similar works, and the improvement is not significant.
This paper was evaluated by four reviewers. After rebuttal, several concerns remained, e.g. Rev. 1 is interested in more thorough comparisons even if the model is claimed to be backbone agnostic. Rev. 2 is concerned about re print of some theories and authors  response that  contribution is not in theoretical innovation . Rev. 3 is overall not impressed with the clarity of the paper. Finally, Rev. 4 also remains unconvinced after rebuttal due to several somewhat loose explanations provided by authors.  At this point, AC agrees with reviewers that the paper requires more clear cut theoretical contributions, ablations and improvements in writing clarity. While some reviewers might have been more inspired by the aspect of noisy labels, even ignoring this aspect, the overall consensus among all reviewers stands.
The submission proposes a new approach to deriving a policy gradient type algorithm for multi agent RL (MARL) where the agents are interested in a common objective but with potentially different action spaces. It extends the monotone improvement property for single agent trust region based methods like TRPO to a multi agent update setting where the updates are performed in sequence by the agents, and uses this idea to derive new multi agent analogues of TRPO and PPO. These algorithms are shown to be competitive with existing strategies for MARL on a Starcraft environment, and superior in the case of common Mujuco benchmarks.   All reviewers are unanimous in their appreciation for the paper s contributions. The initial concerns about clarity of the technical results, especially the improvement guarantee of the key lemma, that some reviewers had were addressed adequately by the author responses. Hence, I gladly recommend acceptance.
The paper talks about a novel setting in Federated Learning and argues that personalization methods may cause the personalized models to overfit on spurious features, thereby increasing the accuracy disparity compared to the global model. To this end the authors propose a debiasing strategy using a global model and adversarial tranferability.    There were some positive opinion about the problem being interesting .However reviewers had several concerns about the validity of assumption and hand wavy arguments used in the solutions for existence adversarial tranferability. Overall, the settings and the need for removing personalization bias needs to be validated more convincingly and rigorously, with concrete real scenarios and experiments.
This submission received 4 final ratings above the acceptance threshold: 6, 6, 6, 8. The reviewers mentioned limited novelty, but acknowledged practical importance of this work, and particularly appreciated thorough analysis provided by the authors. After a strong rebuttal, most of remaining concerns have been addressed. The final recommendation is therefore to accept this submission as a poster.
Implicit neural representations are a new and promising method to represent images and scenes. Implicit neural representations enable good performance on task like view synthesis. Those networks generate an image of scene pixel by pixel and are therefore computationally expensive. The paper proposes a method to accelerate inference with an MLP by learning each dimension of the input (e.g., x, y, and z coordinates) separately. The paper reports speedups by a factor of up to three.  The four reviewers all agree that this paper should be accepted. The reviewer s highlight that the speedup is a solid technical contribution, and agree that the idea of splitting the coordinates is well motivated and well evaluated, and leads to the advertised speedups.  During the review process, the reviewers also raised some issues, such as a slight performance loss at that cost of a slight increase in efficiency, but were convinced by the response of the reviewers.   I recommend accepting the paper. Like the reviewers, I think that the proposed idea is interesting and technically sound, however, I m a bit concerned about the slight drop in performance: If a slight drop of performance is allowed, then a speedup is possible by simply making the networks smaller, reducing the layers, or through other more immediate means than the proposed one. The contribution would be even more convincing if the paper also compares the speedup in a fair setup where the performance is kept constant.
This paper presents promising and ambitious work in the context of Byzantine tolerant learning in the decentralized setup. The reviews raised several critical points of concern: completeness of technical derivations and details, experimental results and comparisons to other work, excluding several state of the art attacks. The reviewers provided with a generous amount of feedback, that should be incorporated in the paper before publication. Unfortunately the camera ready timeline is quite sort for such an extensive feedback to be integrated in this paper.  The authors are urged to resubmit after taking into account all the suggestions that were made during this review cycle.
This paper arose a number of questions and concerns among Reviewers that made it get below average scores (unfortunately, Reviewers did not provide further feedback on the rebuttal). After discussion between the Program Chairs, calibrating decisions across all submissions and, given the drawbacks mentioned below, it is decided that this paper does not meet the bar for this year s ICLR. Therefore, the final decision is to REJECT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Further developing on a simplification of previous approaches (learning diffusion sigmas).   Proposal of a new noise schedule.   Improving the log likelihood of diffusion based generative models.   Improving generation time.  Cons:   Similar FIDs as non improved approaches (in some cases).   Focus on log likelihood may not be of paramount importance for a generative task.   Dichotomy between better FID and better NLL could be further discussed.   More comparison with other approaches and further data sets could be done.   A bit ad hoc noise schedule. 
This paper investigates the problem of unsupervised domain adaptation and proposes a framework based on a specific type of disentangled representations learning. The paper is well written and the proposed method seems plausible. However, according to Reviewers #3 and #4, the proposed framework does not seems to be sufficiently different from existing ones, and the empirical results do not seem convincing enough.   Please also double check in C3, whether T and S should be marginally independent or conditionally independent conditioning on X.
The paper proposed to learn a disentangled representation of spatiotemporal mobility data using a VAE based architecture, in order to separate spatial and temporal dependencies. This is an interesting and relevant problem, but the reviewers found the paper to be weak in motivation and empirical evaluations.
This paper overall received borderline negative scores. All the reviewers agree that the paper proposed an interesting approach to exploration for RNN based recommender systems. However, there are concerns around the experiments as well as the theoretical contribution. Specifically, a few reviewers pointed out that the experimental results are not convincing. Furthermore, some reviewers mentioned that the theoretical analysis is a relatively straightforward extension of LinUCB (to which the authors disagreed, but there wasn t any followup discussion from the reviewers). The bandit analysis is unfortunately outside my expertise.   I think this paper touches upon two different domains (RNN based recommender systems and bandit). If the authors consider that their contribution mainly lies in the latter, maybe ICLR is not the most suitable venue. 
The paper is addressing an important problem, but misses many related references (see Reviewer 2 s comments for a long list of highly relevant papers).   More importantly, as Reviewer 3 pointed out (which the AC fully agrees):   "The gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re derived through importance sampling."  "The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered."  
The reviewers generally appreciated the problem statement and topic of the paper, but raised concerns across the board about the empirical evaluation. Since the paper is largely experimental in nature, a compelling experimental evaluation is important. Reviewer concerns generally centered around: (1) the relative simplicity of the evaluated tasks; (2) somewhat questionable baselines. While the authors provided responses to some of these concerns, after discussion the reviewers generally still felt that these issues were rather severe, and preclude publication at this time. I would encourage the authors to take this feedback into account in improving the empirical evaluation in the future.
I think this is a very promising paper, but the work is not ready for publication.   The most significant concern shared by several reviewers is the insufficient evaluation. For example, the work is not compared with more traditional approaches to equivalence checking or any other baselines beyond ablations of the proposed method. Given that this is not the first paper to propose the use of deep learning to search for proofs, it seems important to compare to alternative methods. There is also a misalignment between the claims of novelty and the evaluation. For example, section 4 cites the novel approach to generating data as key to this approach, but the evaluation does not really address this claim. On the positive side, I was impressed with the ability to search for proofs of length 10 given the large branching factor, and I thought the results were promising.   The authors should also consider some of the concerns with presentation raised by the reviewers. 
This paper argues several loosely related points about the evaluation of pretrained models on commonsense reasoning datasets in the Winograd style, and presents experiments with existing models on several datasets, including a novel 20 example benchmark. All four reviewers struggled to find a clear contribution or theme in this paper that is novel and thorough enough to meet the bar for publication at a selective general ML venue.  I d urge the authors to focus in on just one of these points and expand, and to consider submitting to a venue that more narrowly focuses on methods for commonsense reasoning in NLP.
 * Strengths  This paper studies adversarial robustness to perturbations that are bounded in the L2 norm. It is motivated by a theoretical sufficient condition (non expansiveness) but rather than trying to formally verify robustness, it uses this condition as inspiration, modifying standard network architectures in several ways to encourage non expansiveness while mostly preserving computational efficiency and accuracy. This “theory inspired practically focused” hybrid is a rare perspective in this area and could fruitfully inspire further improvements. Finally, the paper came under substantial scrutiny during the review period (there are 65 comments on the page) and the authors have convincingly answered a number of technical criticisms.  * Weaknesses  One reviewer and some commenters were concerned that the L2 norm is not a realistic norm to measure adversarial attacks in. There were also concerns that the empirical level of robustness of the network was too weak to be meaningful. In addition, while some parts of the experiments were thorough and some parts of the paper were well presented, the quality was not uniform throughout. Finally, while the proposed changes improve adversarial robustness, they also decrease the accuracy of the network on clean examples (this is to be expected but may be an issue in practice).  * Discussion  There was substantial disagreement on whether to accept the paper. On the one hand, there has been limited progress on robustness to adversarial examples (even under simple norms such as the L2 norm) and most methods that do work are based on formal verification and therefore quite computationally expensive. On the other hand, simple norms such as the L2 norm are somewhat contrived and mainly chosen for convenience (although doing well in the L2 norm is a necessary condition for being robust to more general attacks). Moreover, the empirical results are currently too weak to confer meaningful robustness even under the L2 norm.  * Decision  While I agree with the reviewers and commenters who are skeptical of the L2 norm model (and would very much like to see approaches that consider more realistic threat models), I decided to accept the paper for two reasons: first, doing well in L2 is a necessary condition to doing well in more general models, and the ideas and approach here are simple enough that they might provide inspiration in these more general models as well. Additionally, this was one of the strongest adversarial defense papers at ICLR this year in terms of credibility of the claims (certainly the strongest in my pile) and contains several useful ideas as well as novel empirical findings (such as the increased success of attacks up to 1 million iterations).
The paper presents a new technique that infers the endogenous states of an RL problem, as well as the corresponding model and optimal policy.  A bound is derived that shows that the amount of data needed depends only on the number of endogenous states, while being independent of the number of exogenous states and the complexity of the observation space.  This is remarkable since this is the first technique that is shown to have a complexity that depends only on the number of endogenous states.  Furthermore, the bound derived is not just a theoretical bound.  It is a practical bound in the sense that it is used in the associated algorithm, which is demonstrated effectively on two problems.  Perhaps the main weakness of the paper is that no intuition is provided in the main paper to explain why the sample complexity can be made independent of the number of exogenous states and the complexity of the observation space.  The reader has to look at the proof in the supplementary material.  Nevertheless, this is remarkable work.
The paper proves that the locus of the global minima of an over parameterized neural nets objective forms a low dimensional manifold. The reviewers and AC note the following potential weaknesses:     it s not clear why the proved result is significant: it neither implies the SGD can find a global minimum, nor that the found solution can generalize. (Very likely, most of the global minima on the manifold cannot generalize.)    the results seem very intuitive and are a straightforward application of certain topological theorem. 
The authors introduce a framework for inverse reinforcement learning tasks whose reward functions are dependent on context variables and provide a solution by formulating it as a convex optimization problem.  Overall, the authors agreed that the method appears to be sound.  However, after discussion there were lingering concerns about (1) in what situations this framework is useful or advantageous, (2) how it compares to existing, modern IRL algorithms that take context into account, and (3) if the theoretical and experimental results were truly useful in evaluating the algorithm.  Given that these issues were not able to be fully resolved, I recommend that this paper be rejected at this time.
The reviewers agreed that the paper can be improved in several aspects such a motivation, novelty and framing of the contribution. The reviewers also liked the clarity of the presentation and some of the ideas. The reviewers constructive input may be used to improve this work. 
The paper proposes to use Anderson Mixing to accelerate value iteration and DQN.  The idea is interesting, with some theoretical and empirical support.  However, reviewers feel that the contribution is somewhat limited, and certain parts (e.g., the DP view) can be further developed to strengthen the technical contribution.  Furthermore, one reviewer points out that the empirical results are not very strong, where the improvements on 3 Atari games are not very substantial.  Overall, while the paper is interesting and does have the potential, it seems too preliminary to be published in its current form.  Minor comments: 1. The paper is partially motivated by the claim given at the beginning of section 3: "Based on the observation that full policy evaluation accelerates convergence, ..."  Can a reference be given?  2. Another way to look at Anderson Mixing is the standard linear value function approximation framework, where the previous K value functions serve as basis functions.  See Mahadevan & Maggioni (JMLR 07), Parr et al. (ICML 08) and Konidaris et al. (AAAI 11) for a few examples of constructing basis functions; the approach here seems to provide another way to automatically construct basic functions.  A discussion would be helpful.
The paper provides an astonishingly simple experiment: the parameters in the network are fixed, but only the parameters in the BatchNorm (taking less than 1% of the total number of parameters) are trained and also the last linear layer is trained.   The resulting networks provide better accuracies than training a random subset of the network. Another part of this work is the study of the effect of $\beta$ and $\gamma$ when doing full training.   Pros:   All the reviewers agree this is an interesting and important observation.               Contribution is clear and paper is well written             In future, better understanding of different parameters may   Cons: A concern has been raised by one of the reviewers that it is more like a technical report            Some previous work which studies the effect of $\gamma$ was not mentioned.   I think, the most interesting part is training only $\beta$ and $\gamma$. It will provide a ground for theoretical investigations of the properties of deep neural network models, and maybe lead to more efficient training algorithms.   
This submission presents an interesting contribution on differentiable sorting, providing an analysis of monotonicity for these operations.  The reviewers overall argue for acceptance.
This article studies convergence of WGAN training using SGD and generators of the form $\phi(Ax)$, with results on convergence with polynomial time and sample complexity under the assumption that the target distribution can be expressed by this type of generator. This expands previous work that considered linear generators. An important point of discussion was the choice of the discriminator as a linear or quadratic function. The authors  responses clarified some of the initial criticism, and the scores improved slightly. Following the discussion, the reviewers agreed that the problem being studied is a difficult one and that the paper makes some important contributions. However, they still found that the considered settings are very restrictive, maintaining that quadratic discriminators would work only for the very simple type of generators and targets under consideration. Although the article makes important advances towards understanding convergence of WGAN training with nonlinear models, the relevance of the contribution could be greatly enhanced by addressing / discussing the plausibility or implications of the analysis in a practical setting, in the best case scenario addressing a more practical type of neural networks. 
This paper explores ways in which *emergent communication* (EC) methods from representation learning can be evaluated extrinsically, by hooking them into downstream NLP tasks. Reviewers agree that the paper is thorough, and finds encouraging results.  This paper is borderline, and difficult to evaluate, even after very substantial discussion (some of it private). From my reading of the reviews and pieces of the paper, I m very sympathetic to wvqW s concern that none of the present day applications under study seem likely to benefit from this kind of emergent communication pretraining: *Natural* language pretraining, even transferring across natural languages, is for too strong a baseline, and it s not even conceptually clear how one could substantially outperform that baseline. I m very concerned that the results in this paper will be—misleadingly—cited as proof that EC research is already contributing to downstream progress in NLP.  However, the narrow claims in the paper itself seem to be sound, and two confident reviewers whom I trust argue strongly that the ideas results here are surprising and novel, and that the paper could be the starting point for productive discussion and future work in this area. I m recommending spotlight presentation in the hope that the paper will provoke a nuanced discussion in that setting.
The paper proposes an intuitive causal explanation for the generalization properties of GD methods. The reviewers appreciated the insights, with one reviewer claiming that there was significant overlap with existing work.  I ultimately decided to accept this paper as I believe intuitive explanations are critical to the propagation of ideas. That being said, there is a tendency in this community to erase past, especially theoretical, work, for that very reason that theoretical work is less popular.  Hence, I want to make it clear that the acceptance of this paper is based on the premise that the authors will incorporate all of reviewer 3 s comments and give enough credit to all relevant work (namely, all the papers cited by the reviewer) with a proper discussion on the link between these.
This work studies the problem of building powerful representations of low dimensional point clouds with permutation and rotational equivariance, with the motivation to tackle applications in the physical sciences. Their main technical contribution is the use of the so called geometric algebra, a series of operations between scalar and vector quantities that respect rotational symmetries, which the authors then combine with attention mechanisms to provide permutation symmetry.   Reviewers generally found this work full of interesting ideas, in particular the novel geometric algebra structure to deal with rotational symmetry. However, they also found several issues, such as lack of clarity and somewhat unclear experimental validation. In particular, the authors are encouraged to formalise the rotational equivariance property, and to further address the "small" aspect of the title. Taking all these considerations into account, the AC recommends rejection at this time, but encourages the authors to pursue this exciting line of research.
This paper studies the effect of anomaly detection using supervised learning with non representative abnormal examples on the TPR of the anomaly detection model. Experiments demonstrate that when the abnormal examples presented in the training set are not representative of the abnormal examples in the target distribution, this can lead to bias in estimating the TPR.   Pros:    This paper considers an important issue of tuning anomaly detection models in the absence of representative anomalies.   The paper is well written and easy to read. Cons:   The analysis and experiments provided in the paper are not surprising, and repeat, although perhaps in more detail, known effects of learning with a non representative training sample. 
The paper identifies the limitation of graph neural networks and proposed new variants of graph neural works. However, the reviewers feel that the theory of the paper have some problems:  1. A major concern is that the theoretical analyses in this paper are limited to graphs sampled from the SBM model. It is unclear how these analyses can be generalized to real graphs.  2. The robustness definition is inconsistent.  Furthermore, more extensive experiments on more datasets will also be helpful. 
The paper proposes two methods for link prediction in knowledge hypergraphs. The first method concatenates the embedding of all entities and relations in a hyperedge. The second method combines an entity embedding, a relation embedding, and a weighted convolution of positions. The authors demonstrate on two datasets (derived by the authors from Freebase), that the proposed methods work well compared to baselines. The paper proposes direct generalizations of knowledge graph approaches, and unfortunately does not yet provide a comprehensive coverage of the possible design space of the two proposed extensions.  The authors should be commended for providing the source code for reproducibility. One of the reviewers (who was unfortunately also the most negative), was time pressed. Unfortunately, the discussion period was not used by the reviewers to respond to the authors  rebuttal of their concerns.  Even discounting the most negative review, this paper is on the borderline, and given the large number of submissions to ICLR, it unfortunately falls below the acceptance threshold in its current form.   
The paper proposes an algorithm for training flow models by minimizing the KL divergence in the latent space Z. The paper addresses an important problem in training flow models. However, some major concerns remain after the discussion among the reviewers. The scale of the experiments and the scalability of the approach appear limited in the current version of the paper. Moreover, the applicability of the current theoretical analysis to general distributions is quite limited. 
The paper studies  neural architecture search for hyper relational knowledge graphs (HKGs).  A  search space is put forth, and  it is  searched with a differentiable search algorithm. The paper is technically strong. However,  there are some concerns about the narrow scope of the problem/solution, given that other more general formulations have also been studied.
This manuscript outlines procedures to address fairness as measured by disparity in risk across groups. The manuscript is primarily motivated by methods that can achieve "no harm" fairness, i.e., achieving fairness without increasing the risk in subgroups.  The reviewers and AC agree that the problem studied is timely and interesting. However, in reviews and discussion, the reviewers noted issues with clarity of the presentation, and sufficient justification of the results. The consensus was that the manuscript in its current state is borderline, and would have to be significantly improved in terms of clarity of the discussion, and possibly improved methods that result in more convincing results. 
The reviewers agree that the problem tackled is important but raise several substantial issues that justify not to accept the paper in its current form. I would encourage the authors to clarify further the crypto part of the paper (dHiP, 2., 4.) and work on how to relax or improve the model assumptions (NGrb). Also, the author s reply to chCc, point 2. becomes more disputable as federated learning is further developed. The argument can be refined.  On a personal note, the statement of Theorem 3.3 could be made clearer, in particular in simplifying (while weakening a bit) the probability bound.  AC.
This paper asks when SGD+M can beat adaptive methods such as Adam, and then suggests a variant of SGD+M with an adaptive controller for a single learning rate and momentum parameter.  There is are comparisons with some popular alternatives.  However, the bulk of the paper is concerned with a motivation that didn t convince any of the reviewers.
The pros and cons of the paper cited by the reviewers can be summarized as follows:  Pros:   good problem, NL2SQL is an important task given how dominant SQL is   incorporating a grammar ("sketch") is a sensible improvement.  Cons:   The dataset used makes very strong simplification assumptions (that every token is an SQL keyword or appears in the NL)   The use of a grammar in the context of semantic parsing is not novel, and no empirical comparison is made against other reasonable recent baselines that do so (e.g. Rabinovich et al. 2017).  Overall, the paper seems to do some engineering for the task of generating SQL, but without an empirical comparison to other general purpose architectures that incorporate grammars in a similar way, the results seem incomplete, and thus I cannot recommend that the paper be accepted at this time.
This paper introduces an interesting application of VAE GAN to the problem of Spectral Synthesis across satellite observations with some additional domain specific changes (new loss, ...). The introduction of a new dataset is also very interesting and can open the door for more methodological development in the community.   While the application is original, the methodological contributions have been judged limited and domain specific by most of the reviewers. The responses from the authors was appreciated by the reviewers (especially the comparison to the UNIT baseline) but did not change their opinion about the limited methodological novelty of the approach. The AC recommends a reject but encourages the authors to resubmit it to a more applied remote sensing venue. 
This paper presents an inference softmax cross entropy (I SCE) loss, a modification to the widely adopted "Softmax Cross Entropy" (SCE) loss, to achieve better robustness against adversarial attacks. The original submission had critical issues on motivation, theoretical analysis and experiments. Although the authors provided a revised version, it needs another round of thorough examination before publishing. 
The paper proposes a GAN based method for synthesizing various types of defects as foreground on different product images (background). The method builds upon StarGANv2, and adds the cycle/content consistency loss and classification loss between foreground and background. While the paper considers an important problem/application, the reviewers found it lacking sufficient novelty for publication. The paper will be more suited for publication at an application oriented venue.
This is a clear accept. Solid and timely work extending normalizing flows to implicitly defined mappings. Convincing presentation. Supported by all four reviewers. Best paper in my batch. Has the potential to spark further developments in the field. I recommend to feature this paper as a spotlight.
This paper presents a study of on policy data in the context of model based reinforcement learning and proposes a way to ameliorate the resulting model errors.  This is a timely and interesting contribution, and all reviewers agree on the quality of the manuscript. Please incorporate all the remaining feedback from the reviewers.  Minor comment: There might be interesting points of contact between this work and the concept of objective mismatch (https://arxiv.org/abs/2002.04523)
Thank you for your submission to ICLR.  Overall the reviewers and I think that this paper presents some nice contributions to the adversarial attacks literature, demonstrating a low sample complexity, "physically realizable" attack in a domain of clear importance and interest in machine learning.  The move to considering more "in the loop" adversarial examples is particularly compelling, and the threat model and improvement over BO methods are both compelling here.  The main downside of this paper, of course, is the fact that the "physical adversarial examples" are of course nothing of the sort: they are simulated.  Rather, they are just simulated in a manner that may plausibly be slightly more amenable to real world deployment. The authors claim that they don t carry out an evaluation on a real system because it is "dangerous" is a bit overly dramatic: the tests could easily be carried out in a controlled environment, and demonstration on an actual physical system (even, e.g., and RC car) would vastly improve the impact of this work.  As it is, the paper is borderline, but ultimately slightly below the high bar set by ICLR publications.  I would strongly encourage the authors to reconsider the inclusion of the word "physical" in the title, as it honestly sets expectations high for a promise that the paper cannot deliver on, or (even better) to run real experiments on even a small physical system, demonstrating the transferability there.  The paper ultimately has the potential for a high impact in this field, if these issues are addressed.
The paper diligently setup and conducted multiple experiments to validate their approach   bucketizating attributions of data and analyze them accordingly to discover deeper insights eg biases. However, reviewers pointed out that such bucketing is tailored to tasks where attributions are easily observed, such as the one of the focus in this paper  NER. While manuscript proposes this approach as ‘general’, reviewers failed to seem this point. Another reviewer recommended this manuscript to become a journal item rather than conference, due to the length of the page in appendix (17). There were some confusions around writings as well, pointed out by some reviewers. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission.  
This paper builds on the recent DCFNet (Decomposed Convolutional Filters) architecture to incorporate rotation equivariance while preserving stability. The core idea is to decompose the trainable filters into a steerable representation and learn over a subset of the coefficients of that representation.  Reviewers all agreed that this is a solid contribution that advances research into group equivariant CNNs, bringing efficiency gains and stability guarantees, albeit these appear to be incremental with respect to the techniques developed in the DCFNet work. In summary, the AC believes this to be a valuable contribution and therefore recommends acceptance. 
There is a lot of agreement on this paper, also reflected in the ratings. There were some technical comments initially, on the approach not being IC and interpretable, missing links to other works and technical descriptions of the network and experiments. The authors cleared up many of these issues though with their responses, providing good arguments in favor of their work. In general, reviewers agree the paper would be interesting to be included in ICLR.
meta score: 8  This is a good paper which augments the data by mixing sound classes, and then learns the  mixing ratio.  Experiments performed on a number of sound classification results  Pros    novel approach, clearly explained    very good set of experimentation with excellent results    good approach to mixing using perceptual criteria  Cons    discussion doesn t really generalise beyond sound recognition  
This paper is concerned with fairness in the generative setting, specifically the setting in which various groups have very different sizes, and are therefore treated disproportionately by the model, with the group memberships further being unknown.  The reviewers generally agreed that the setting was interesting and important. However, they were critical of the writing quality, significance, and quality of the theoretical contribution.  The authors made significant improvements in the review period, and while these were not quite enough to satisfy enough reviewers, opinions clearly changed in a positive direction during the discussion period. Future changes motivated by the existing reviewer concerns should significantly improve the paper.
despite not amazing scores, this is a solid paper. it created a lot of discussion and was found to be reproducible. we should accept it to let the iclr community partake in the discussion and learn about this method of n gram embeddings 
The paper proposes a new method that builds on the Bayesian modelling framework for GANs and is supported by a theoretical analysis and an empirical evaluation that shows very promising results. All reviewers agree, that the method is interesting and the results are convincing, but that the model does not really fit in the standard Bayesian setting due to a data dependency of the priors. I would therefore encourage the authors to reflect this by adapting the title and making the differences more clear in the camera ready version.
This paper reviews a number of parameter decomposition methods for BERT style contextual embedding models. The authors argue for the application of Tucker decomposition to the attention and feedforward layers of such models. Evaluation is performed for a range of models on the GLUE benchmark. Further ablation studies indicate that the distillation procedure employed is crucial for obtaining competitive results and the raw decomposition approaches are ineffective at directly approximating the original pre trained model.  Strengths: The reviewers generally agree that the methods explored and results presented in this paper are interesting and could be of use to those deploying large embedding models. The authors review a range of possible decomposition methods and use this to motivate their approach. The resulting levels compression are high while maintaining good performance, while the ablation study clearly shows the contribution of the various steps of the training pipeline.  Weaknesses: The main weakness identified by the reviewers is the incremental nature of this work in comparison to previous works applying various decomposition and compression techniques to neural networks. They also highlight that many of the techniques discussed early in the paper are not compared in the evaluation. The authors have effectively responded to this issue by providing further comparisons and justification for their modelling choices (e.g. not compressing the embedding layers).   Overall, despite the incremental nature of this work, I believe that there are enough though provoking ideas and results presented to warrant publication. Interestingly, as the authors emphasise in their response, the ablation study highlights that this work is not really about approximating the original models weights, as all of the work appears to be being done by the distillation procedure in concert with the choice weight decomposition. In general I wonder whether this paper would be better presented as exploring a structured distillation procedure rather than weight compression.
This paper proposes a method for automatically discovering graph algorithms using GNNs. In general, the reviewers find the paper well written, and the problem and the approach interesting.  However, there is a concern on the practical usefulness of proposed method as shown in the following comments:  “My main concerns are on Q2, i.e., the practical usefulness of the algorithm”[R1]; “It sounds like the proposed model is hard to generalize to different datasets” [R3]; “The proposed explainer does not generate practically useful outputs for discovering new algorithms”[R4]. 
This paper introduces a technique called EquiNorm, which normalizes the weights of convolutional layers in order to control covariate shift. The paper is well written and the reviewers agree that the solution idea is elegant. However, the reviewers also agree that the experiments presented in the work were insufficient to prove the method s superiority. Reviewer 2 also expressed concerns about the poor results on ImageNet, which calls into question the significance of the proposed method. 
This paper proposes an improvement to the popular DARTS approach, speeding it up by performing the search in a subset of channels. The improvements are robust, and code is available for reproducibility.  The rebuttal cleared up initial concerns, and after the (private) discussion among reviewers now all reviewers give accepting scores. Because the improvements seem somewhat incremental and only applied to DARTS, R3 argued against an oral, and even the most positive reviewer agreed that a poster format would be best for presentation.   I therefore strongly recommend recommendation, as a poster.
The paper applies a reinforcement learning (RL) approach to a medical diagnosis dialog task. Motivated by a large action space, the authors utilize a hierarchical model where the higher level model triggers a lower level model comprising of symptom checkers and disease classifiers. They evaluate their approach on real world and synthetic data sets.  Pros + The application (societal relevance) and the hierarchical approach (large action space) are motivated well + The paper is presented relatively clearly (with caveats: see reviewer comments) and improves performance over reasonable baselines (with caveats over one metric: why longer dialog is better?)  Cons   The novelty of the work was not entirely clear, other than the application to a new task   Lack of examples make it difficult to gauge the complexity of the task   Ablation studies would also have provided better insight into task and the proposed model  The reviewers have several concerns about the work described in the paper. But the authors did not provide any response unfortunately.
The paper proposes a framework for object detection on lidar scans, with query of scene feature extracted offline from previous traversals. Overall there is good agreement among reviewers, with three recommending accepting the paper and one marginally accepting it   to me the authors satisfactorily addressed most aspect raised in reviewing.
Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. However, there is still room for improvement; for example, convergence to a good solution needs to be further investigated. Given the  high competition at ICLR2020, this paper is unfortunately below the bar. We hope that the reviewers  comments are useful for improving the paper for potential future publication.
The paper was judged by the reviewers as providing interesting ideas, well written and potentially having impact on future research on NN optimization.  The authors are asked to make sure they addressed reviewers comments clearly in the paper.
Pros: + The paper is very clearly written. + The proposed re embedding approach is easily implemented and can be integrated into fancier architectures.  Cons:   A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (e.g., ESIM with full training data and full word vectors).  This paper is rather close to the decision boundary. The authors had reasonable answers for some of the reviewers  concerns, but in the end the reviewers were not completely convinced. 
This work proposes a system for generating piano music (in the symbolic domain) using a learned reward function. Reviewers raised concerns about the organisation of the paper, clarity of writing, a lack of experimental comparison with previously published approaches (and the quality of the baseline), several unsubstantiated claims, and some missing related work. Unfortunately no attempt was made to address these issues.
This paper proposes using conditional VAEs for multi domain transfer and presents results on CelebA and SCUT. As mentioned by reviewers, the presentation and clarity of the work could be improved. It is quite difficult to determine the new/proposed aspects of the work from a first read through. Though we recognize and appreciate that the authors updated their manuscript to improve its clarity, another edit pass with particular focus on clarifying prior work on conditional VAEs and their proposed new application to domain transfer would be beneficial.   In addition, as DIS is the main metric for comparison to prior work and for evaluation of the final approach, the conclusions about the effectiveness of this method would be easier to see if a more detailed description of the metric and analysis of the results were provided.   Given the limited technical novelty and discussion amongst reviewers of the desire for more experimental evidence, this work is not quite ready for publication.
This paper proposes an improved mean field analysis for multi player residual networks. Compared with prior works, the proposed analysis removes a full support assumption needed in prior works. The authors have addressed some of the reviewers’ concerns by adding comparisons with the existing analysis of ResNet in the NTK regime, and a more detailed comparison with Ding et al. 2021. While this paper gathers some support from a reviewer, there is still concern that the novelty of this paper is not significant, especially given that the analysis is heavily built upon prior works. I think this paper can benefit from providing a proof sketch to highlight the key difference between the new analysis and existing analyses,  or explicitly demonstrating the key proof technique/technical lemmas that enable the removal of the full support assumption. This paper might be a strong work after careful revision.
The paper proposes the unique setting of adapting to multiple target domains. The idea being that their approach may leverage commonality across domains to improve adaptation while maintaining domain specific parameters where needed. This idea and general approach is interesting and worth exploring. The authors  rebuttal and paper edits significantly improved the draft and clarified some details missing from the original presentation.   There is an ablation study showing that each part of the model contributes to the overall performance. However, the approach provides only modest improvements over comparative methods which were not designed to learn from multiple target domains. In addition, comparison against the latest approaches is missing so it is likely that the performance reported here is below state of the art.   Overall, given the modest experimental gains combined with incremental improvement over single source information theoretic methods, this paper is not yet ready for publication.
The paper proposes a data driven approach to learning atomic resolution energy functions. Experiment results show that the proposed energy function is similar to the state of art method (Rosetta) based on physical principles and engineered features.   The paper addresses an interesting and challenging problem. The results are very promising. It is a good showcase of how ML can be applied to solve an important application problem.   For the final version, we suggest that the authors can tune down some claims in the paper to fairly reflect the contribution of the work. 
The paper proposes a modification of the well known FILM model for VQA which targets counting problems in particular, which have been a known weakness of existing models. The improvements have also been tested beyond counting. The experimental results are convincing, in particular a scientific competition has been won. The reviewers also appreciated convincing ablation studies.  The idea bas been perceived as interesting enough for publication, and in combination with the experimental results, this compensated several perceived weaknesses (limited novelty w.r.t. the modified FILM model; justifications of some design choices).  All reviewers agreed that this paper is of interest to the community and proposed acceptance. The AC concurs.
After a healthy discussion between reviewers and authors, the reviewers  consensus is to recommend acceptance to ICLR. The authors thoroughly addressed reviewer concerns, and all reviewers noted the quality of the paper, methodological innovations and SotA results.
This paper propose a novel framework to increase cooperation in second order social dilemmas. This is based on encouraging homophilic incentives. Reviewers agree that the paper does not meet the standards of publication yet. In particular, they worry that the assumptions made are so restrictive as to make model inapplicable to interesting problems. There is also a concern that the work is simply not novel enough.
This paper represents a practical extension of sound theoretical uncertainty propagation ideas for exploration in deepRL. All the reviewers agreed this was a promising direction and the empirical results strong. It was nice to see additional qualitative analysis of the proposed method, beyond the typical "my number is bigger than yours" type of claims. The discussion was extensive; reviewers with specific subject matter expertise provided high quality and detailed reviews.  Several reviewers were in favour of the paper, but none were willing to champion it as a clear accept.   Indeed the discussion highlighted important concerns with the paper. Several reviewers found the paper to overclaim: most importantly the paper suggests strong theoretical underpinnings of the method without clear evidence. Some found the text very imprecise. The reviewers were torn if such changes represented wording changes or major rewrites. The original submission missed two key pieces of work which were added during the rebuttal phase UBE was added by including the scores from the literature, and the other (Bayesian DQN) was only added to the discussion. Good on the authors for doing so, though ideally both would be implemented again.   The AC s own reading of the paper highlighted a few other concerns. The writing needs work. In addition, the majority of improvement in overall performance appears to be due to very large improvements in a handful of games (e.g. Atlantis, Krull) and significant losses in performance in other games. This was not discussed at all. More surprisingly these games with huge performance gains were not used in the qualitative visualizations of the utility of the bonus found in the paper. The game breakout was used for analysis instead. Oddly the proposed method actually does worse or the same as SOTA methods (e.g., Adaptive EBU, Boot DQN, UBE) in breakout.  This is difficult to get to the bottom of because: (a) the per game score tables in the appendix don t include the scores achieved by important baselines (e.g., Boot DQN, EBU, Adapt EBU), (b) different setups are used across the relevant literature (Boot Q & UBE papers use 200m frames, EBU paper uses 10m frames, and this paper uses 20m), (c) the per game analysis in the appendix focuses on comparing methods proposed in the paper under review. That might all make sense, but it is left to the reader to figure out and I never got to the bottom of it all (table 3 of the Lee et al contains some of the relevant comparison data). Such missing details and lack of analysis are particularly important when the paper boldly claims state of the art performance improvement.  All put together a clear picture emerges: the paper needs polishing, is unclear in places, over claiming, and missing important analysis and explanations in regards to both the theory and the experiments. The reviews are extensive and have provided many insights in how to improve the paper. 
The paper evaluates the generalization capabilities of model based agents, in particular, MuZero, compared with model free agents. Reviewers agree that the paper is well written and the topic is interesting. The ablation study is especially interesting, as it disentangles the effect of different algorithmic components. Some concerns are raised about the significance of this work, as the scope is limited to an empirical study and the results are not necessarily very surprising.   Since the paper presents clear results on an important and relevant topic, I recommend acceptance.
The paper proposes to address the out of distribution generalization problem by means of conditional computation in form of a feature modulating module. While the approach is interesting and brings a new take on how to perform feature modulation (although initially felt too similar to Conditional Batch Normalization) some major concerns about the experiments and validation of the approach are raised by all reviewers. Some of the hypothesis made are also challenged due to lack of proper validation. Although the discussion clarified some points I am afraid many open questions are left unanswered and would require a more work to be fully addressed before acceptance.
This paper proposes to use mixture distributions to improve uncertainty estimates in BNNs. Ensemble methods are interpreted as a Bayesian mixture posterior approximation. To reduce the computation, a modification to BBB is provided based on a concrete mixture distribution.  Both R1 and R3 have given useful feedback. It is clear that interpretation of ensemble as a Bayesian posterior is well known, and some of them also have theoretical issues. The experiment to clearly comparing proposed mixture posterior to more commonly used mixture distribution is also necessary.   Due to these reasons, I recommend to reject this paper. I encourage the authors to use reviewers feedback to improve the paper.
This paper has been assessed by three reviewers scoring it as follows: 6, 3, 8. The submission however attracted some criticism post rebuttal from the reviewers e.g., why concatenating teacher to student is better than the use l2 loss or how the choice of transf. layers has been made (ad hoc). Similarly, other major criticism includes lack of proper referencing to parts of work that have been in fact developed earlier in preceding papers. On balance, this paper falls short of the expectations of ICLR 2020, thus it cannot be accepted at this time. The authors are encouraged to work through major comments and resolve them for a future submission.
The paper points out an interesting and, to me unexpected, problem when learning Q functions to do with spectral bias. Figures 1 and 2 are quite striking. The diagnosis and proposed solution elegantly combines ideas from NTKs and NeRFs. The proposed random Fourier actor critic performs well in practice. The main problem reviewers had in the end is that the authors added substantial new empirical results too late to review thoroughly.
This paper proposes a framework to train a discriminative model robust against (i) label noise, (ii) out of distribution input, and (iii) input corruption. To tackle these problems, a complex model is proposed that combines several existing models including InfoNCE style contrastive learning, prototypical contrastive loss, Mixup, and reconstruction loss. Noisy training labels are cleaned using a temporally consistent label smoothing mechanism, combined with a curriculum learning algorithm.   Originally, the reviewers raised concerns regarding the limited ablation experiments and the lack of studies on real world noisy labels. The additional experiments in the revised version addressed some of these concerns. Thus, the reviewers increased their rating slightly.  However, the reviewers in the discussion phase agree that the proposed method has a limited novelty, is complex, and involves many moving parts that require a careful design and hyperparameter tuning, and they do not recommend accepting the submission. I agree with the reviewers and recommend rejection. 
This paper studies the following model: The input to our classifier is the instance X which determines the label Z and we observe a noisy version of this label Y. The key assumption is that the label noise is independent of the instance, and the goal is to learn the channel from Z to Y. The main motivation is that generally algorithms that can handle instance independent noise need to know the noise model. Thus the main contribution of this paper is to decouple the problem of learning the noise channel and the problem of learning a high accuracy classifier. In particular they inject their own label noise and design a discriminator to test if the noise on the labels has maximum entropy. They show that their method is statistically consistent. Finally they complement this with synthetic experiments on CIFAR to show that their algorithm works.   While the reviewers all found the ideas promising, they brought up a few deficiencies in this work which they hope could be improved in later versions. First, the writing is at times unclear and imprecise. For example, there are many places that could benefit from further discussion, particularly in terms of justifying why the assumptions are "mild" or not. Second, the experiments would be more compelling if there were an application where learning the noise model actually led to improved performance on some downstream application. Third, the approach crucially relies on having a separable map, which seems like a rather strong assumption. 
This paper provides a clear and useful empirical study of how the initialization scale and activations function affects the generalization capability of neural networks. Previous works showing the effect of the initialization scale (Chizat and Bach (2018), Geiger et al. (2019), Woodorth et al. (2020)) had a more limited set of experiments. Moreover, here an extreme case is shown, wherewith sin activation function no generalization is possible at a large init scale (there the kernel regime is useless for generalization since the hidden layer output becomes very sensitive to any small perturbation in the input). Lastly, two alignment measures are suggested, which are correlated with the generalization across several architectures and initialization scales.  All the reviewers argued for acceptance, and one strongly so. I agree that the paper is sufficiently interesting and clear to be accepted. However, despite the high scores, I only recommend a poster and not spotlight/oral: I think the novelty of the empirical study is not groundbreaking, given the experiments in previous works, and the usefulness of the suggested measures are not completely clear without a thorough comparison against previously suggested measures.
The authors introduce a new associative inference task from cognitive psychology, show shortcomings of current memory augmented architectures, and introduce a new memory architecture that performs better with respect to the task. The reviewers like the motivation and thought the experimental results were strong, although they also initially had several questions and pointed to areas of the paper which lacked clarity. The authors updated the paper in response to the reviewer s questions and increased the clarity of the paper. The reviewers are satisfied and believe the paper should be accepted.
This paper provides a method for offline RL in settings where the environment may exhibit significant similar structure, such as one part having nearly the same dynamics as other parts. The work is motivated in part by healthcare settings. The reviewers appreciated the potential applications to areas like healthcare but also thought there is a strong body of related work (e.g. transfer learning, meta RL and other related papers) and it was unclear how novel the approach was within that related work, or how it would compare. The authors did not respond to the reviewers’ reviews. We hope their input is useful to the authors’ in revising their work for the future.
The paper proposes a method to detect and correct adversarial examples at the input stage (using a sparse coding based model) and/or at a hidden layer (using a GMM). These detector/corrector models are trained using only the natural examples. While the proposed method is interesting and has some novelty wrt to the specific models used for detection/correction (ie sparse coding and GMMs), there are crucial gaps in the empirical studies:    It does not compare with a highly relevant prior work MagNet (Meng and Chen, 2017) which also detects and corrects adversarial examples by modeling the distribution of the natural examples    The attacks used in the evaluations do not consider the setting where the existence (and architecture) of the defender models is known to the attacker    It does not evaluate the method on a stronger PGD attack (also known as iterative FGSM)
The paper examines the advantage of using models in RL.  The authors  rebuttals convinced us of the value of the paper.
Strengths: Well written paper on a new kind of spherical convolution for use in spherical CNNs. Evaluated on rigid and non rigid 3D shape recognition and retrieval problems. Paper provides solid strategy for efficient GPU implementation.  Weaknesses: There was some misunderstanding about the properties of the alt az convolution detected by one of the reviewers along with some points needing clarifications. However, discussion of these issues appears to have led to a resolution of the issues.  Contention: The weaknesses above were discussed in some detail, but the procedure was not particularly contentious and the discussion unfolded well.  All reviewers rate the paper as accept, the paper clearly provides value to the community and therefore should be accepted. 
This work presents an interesting take on how to combine basic functions to lead to better activation functions. While the experiments in the paper show that the approach works well compared to the baselines that are used as reference, reviewers note that a more adequate assessment of the contribution would require comparing to stronger baselines or switching to tasks where the chosen baselines are indeed performing well. Authors are encouraged to follow the many suggestions of reviewers to strengthen their work.
This paper aims to address the robustness issues by considering natural accuracy, sensitivity based robustness and spatial robustness at the same. However, the reviewers pointed out that many things, like the expriment, the presentation, the algorithm, are not clear. In addition, the technique part is weak and below the bar of ICLR.
This paper proposes a new paradigm   called in sample Q learning   to tackle offline reinforcement learning. Based on the novel idea of using expectile regression, the proposed algorithm enjoys stable performance by focusing on in sample actions and avoiding querying the values of unseen actions. The empirical performance of the proposed algorithm is appealing, outperforming existing baselines on several tasks. The paper is also well written.
The reviewers are satisfied that this paper makes a good contribution to policy gradient methods.
This paper studies the spectrum of the Hessian through training, making connections with the NTK limit. While many of the results are perhaps unsurprising, and more empirically driven, together the paper represents a valuable contribution towards our understanding of generalization in deep learning. Please carefully account for the reviewer comments in the final version.
The paper proposes a subspace regularization technique that encourages the new class weight vector to be in the subspace spanned by those of the base classes for few shot class incremental learning. Even though similar techniques exist in few shot learning literature, reviewers appreciate the simplicity of the method and thorough experiments. The authors have revised the paper to include missing references suggested by reviewers during the rebuttal. They were not able to add experiment comparisons to Tao et al. (2020) and Chen & Lee (2021) as requested by reviewer Vrap due to missing code release.  Please consider adding them in your draft later.
In this paper, authors introduce and study provably robust adversarial examples. Reviewers had mixed thoughts on the work. One reviewer mentioned that the "provable" robustness is somehow overstated in the work: looking at the title and abstract, it sounds like the paper develops a new algorithm that is guaranteed to be robust, but in reality the robustness hinges on the black box verifiers (which is acknowledged by the authors during discussion). I agree with this. This should be more clearly stated in the work. I strongly suggest authors to calibrate exaggerated statements of contributions in the revised draft. Having said this, reviewers liked the the experimental study of the paper and found it to be comprehensive and convincing.
*Summary:* Compare neural networks and tasks using TDA, particularly persistence diagrams.   *Strengths:*    Some reviewers found this a fresh perspective.    Distance calculation using TDA can offer advantages and a theoretical basis.   *Weaknesses:*    Insufficient motivation and experimental evidence for utility of the proposed approach.    Computational cost and hyperparameter choices in PD computation.    Difficulty of interpreting proposed distance matrices.   *Discussion:*   ZGgm found the paper interesting and that it offered a fresh perspective, but that the purpose of the comparison was not sufficiently well motivated. The authors provide some explanations, particularly about the method allowing to compare networks of different sizes, but ZGgm found their comments were not adequately addressed. rtBj found that even though the authors made efforts to address their comments, the paper still requires substantial improvements. HwgX appreciated the authors’ responses but considers that the paper needs to be improved with additional validation. They expressed doubts about the adequacy of the approach and found that although it improves upon certain methods, it is insufficiently verified.    *Conclusion:*   All reviewers agree that this work has some strengths but also significant weaknesses and does not reach the acceptance bar for this conference. Main weaknesses are insufficient motivation and experimental evidence. The reviewers made several suggestions on how the paper could be improved. I agree with the reviewers and hence I must reject this article.
The paper is poorly written and below the bar of ICLR. The paper could be improved with better exposition and stronger experiment results (or clearer exposition of the experimental results.) 
This work studies a variant of a message passing scheme,  aiming to improve the efficiency of GNNs to heterophilic graphs, as well as improving its stability to noise. The authors provide a new architecture, called $p$ Laplacian message passing, as well as some theoretical analysis and empirical evaluation.  Reviewers highlighted several positive aspects on this work, such as the general idea of considering p Laplacians, as well as the extensive empirical evaluation. However, during the review discussions, several important issues arose, namely important concerns regarding the theoretical contributions, as well as concerns in calibrating the baselines in some empirical evaluations. Overall, the AC is of the opinion that this paper requires a further iteration before it can be considered for publication, and encourages the authors to take the time to address the comments raised by the reviewers.
Thanks for your submission to ICLR.  This paper considers binary hashing schemes, and makes two related contributions.  First, it analyzes a simple extension to SQ RFF; second, it introduces and analyzes a novel metric for ranking called ranking efficiency.  Some experiments are also performed on standard data sets.  This is very much a borderline paper, and could go either way.  I took a close look at the paper to offer my opinions in addition to the reviewers.  The paper itself is well written and seems to be correct.  I do like the simplicity of the proposed SignRFF method as well as the ranking efficiency measure.  However, the contributions are somewhat limited, and it s in an area that hasn t seen much work in the last several years (this paper mainly builds off of methods from 10+ years ago).  Further, it doesn t seem that methods such as KLSH and SignRFF are used much in practice, so I don t know if this will have substantial impact.  So while it s a reasonably interesting paper with some nice insights, I think it falls just below the acceptance threshold for me.
I thank the authors for their submission and active participation in the discussions. This papers is borderline with the majority of three reviewers leaning towards rejection and one leaning towards acceptance. All four reviewers unanimously agree on the empirical validation being a major weakness of this paper with reviewer kQnm putting less emphasis on this shortcoming. Overall, I side with reviewers Bne6, 2GeS and HJjY, and believe the paper needs to a more thorough set of experiments. I therefore recommend rejection.
This paper proposes a new loss function for molecular conformation comparison to be used in generation tasks. All reviewers found the research topic is interesting, but the work lacks in multiple aspects. Major concerns include limited contributions and novelty, lack of comparison with prior methods, limited improvements, writing and clarity, etc. The authors did not provide any response during discussion. Given the consistency and extent of concerns, and lack of response, I recommend this paper be rejected at this time.
This paper formulates and solves a capacitated vehicle routing problem (CVRP) in the presence of costs for deploying additional vehicles: a mixture of supervised learning, algorithms, and OR techniques is used. In particular, a mix of greedy decoding, repairing of the solution, and post processing with OR tools is used to extract a feasible solution from the probabilistic prediction.   The paper makes a good case that existing methods do not solve the CVRP with a hard constraint on the fleet size.  On the other hand, there is a strong dependence on heuristic improvements: e.g., a strongly dependence on the post processing, and an additional repair procedure for the decoding process. The authors are encouraged to investigate how such improvements would work with existing approaches: i.e., how novel the new model’s contributions are.
The paper investigates the effect of temperature in the loss function for graph contrastive learning and proposes a novel method to dynamically adjust it during learning (GLATE). The paper is concerned with an interesting problem, is timely, and relevant for the ICLR community.   After author response, reviewers did not come to a full agreement on the paper, with two reviewers indicating (weak) reject and two reviewers indicating (weak) accept. Reviewers highlighted the potential impact of improvements in graph contrastive learning as well as the theoretical analysis as strengths of the paper. However, reviewers raised also concerns regarding scope, novelty of the contributions, and clarity of presentation (method, evaluation, etc.). While the authors  response addressed some concerns regarding aspects of the experimental evaluation, it did not change the overall evaluation of reviewers.   Taking author response and reviewer feedback into account, I narrowly agree that the manuscript is not ready yet for acceptance at ICLR due to the aforementioned concerns. However, I encourage the authors to revise and resubmit their work based on the feedback of this reviewing round.
The authors propose a hierarchical VAE model with a discrete latent variable in the top most layer for unsupervised learning of discriminative representations.  While the reported results on the two flow cytometry datasets are encouraging, they are insufficient to draw strong conclusions about the general effectiveness of the proposed architecture. Also, as two of the reviewers stated the proposed model is very similar to several VAE models in the literature. This paper seems better suited for a more applied venue than ICLR.
The reviewers raise an important issue about the parameters in the proposed gradient in Theorem 1. There could be different parameters for each policy in the gradient (though some parameter sharing could be possible), and computing this gradient would be prohibitive. The solution is to just use the most recent parameters, but then the gradients become off policy again without motivation for why this is acceptable. This approximation needs to be better justified.   As an additional point, there are other off policy policy gradient methods, than just DPG. The authors could consider comparing to these strategies (which can use replay buffers) and explain why the proposed strategy provides benefits beyond these. What is inadequate about these methods? Further motivation is needed for the proposed strategy. This is additionally true because the proposed strategy requires entire sampled trajectories for a fixed policy (to make the policy gradient sound, with weighting dpi_n(s)), whereas DPG and other off policy AC methods do not need that.
The paper seeks to point out the difficulty of size generalization in GNNs for node prediction and analyze why this happens. The analysis is anchored on the construction of so called d patterns. The main argument is presented in Corollary 3.4 which shows that discrepancy in d patters between small and large graphs introduces the possibility of finding a GNN that fits the d patterns well in a small graph while yielding poor answers on a large graph (assuming the task is solvable by a constant depth GNN). While good to show, the existence of bad parameters does not necessarily suggest that this setting is likely to arise after training. As pointed out by AnonReviewer1, size generalization is also not a new issue but has already been introduced / highlighted in previous publications (Barrett et al 2018, Joshi et al 2020, Dai et al 2017). The authors do provide an algorithm and empirical arguments to mitigate the effect of d pattern discrepancy. A revised version of the manuscript can hopefully make a stronger case for d patterns, constant depth computation, and the relevance of these for size generalization. 
*Summary:* Investigate the NTK of PNNs and enhanced bias towards higher frequencies.   *Strengths:*    Spectral bias is a contemporary topic.    Some reviewers found the paper well written.  *Weaknesses:*    Restricted setting (two layers / no bias / infinite width), particularly in view of the objective to provide architecture design guidance. Restricted experiments (Introduction indicates learning spherical harmonics).    Sparse discussion of related works, particularly on spectral bias.   *Discussion:*   During the discussion period authors made efforts to address some of the concerns of the reviewers. A late new experiment prompted KnZp to raise score. TQnp found the paper good but also expected a more profound theorem addressing broader PNN families given the existing work. They found that experiments and discussion of prior work could be improved. The authors added discussion of prior works and provided an explanation for their choices, but left extensions and further analysis for future work. nFMY expressed concerns about applicability of the analysis and evidence in experiments. Author responses addresses this in part. cEcf points out that the main theoretical contributions have straight forward proofs based on previous works and asks about extensions. Authors agree that the paper does not introduce novel techniques and that extending the analysis is an important direction, but leave this for future work. FuRi finds the paper provides an interesting viewpoint and raised score from 3 to 5 following the discussion (improving presentation, rigor, clarity), but considers that the paper has several drawbacks (oversimplification, lack of technical novelty) that need to be addressed.   *Conclusion:*   One reviewer found this work marginally below the acceptance threshold, three marginally above, and one good. I find that the paper considers an interesting problem and makes some interesting observations and some valuable advances. I appreciate the authors’ efforts during the reviewing period. Hence I am recommending accept. At the same time, I find that, clarity, technical and experimental contributions still can be improved and encourage the authors to carefully consider the reviewers comments when preparing the final version of the paper.
The paper introduces a non stationary bandit strategy for adapting the exploration rate in Deep RL algorithms. They consider exploration algorithms with a tunable parameter (e.g. the epsilon probability in epsilon greedy) and attempt to adjust this parameter in an online fashion using a proxy to the learning progress. The proposed approach is empirically compared with using fixed exploration parameters and adjusting the parameter using a bandit strategy that doesn t model the learning process.  Unfortunately, the proposed approach is not theoretically grounded and the experiments lack comparison with good baselines in order to be convincing. A comparison with other, provably efficient, non stationary bandit algorithms such as exponential weight methods (Besbes et al 2014) or Thompson sampling (Raj & Kalyani 2017), which are cited in the paper, is missing. Moreover, given the whole set of results and how they are presented, the improvement due to the proposed method is not clear. In light of these concerns I recommend to reject this paper.
The paper suggests using an ensemble of Q functions for Q learning. This idea is related to bootstrapped DQN and more recent work on distributional RL and quantile regression in RL. Given the similarity, a comparison against these approaches (or a subset of those) is necessary. The experiments are limited to very simple environment (e.g. swing up and cart pole). The paper in its current form does not pass the bar for acceptance at ICLR.
This paper introduces a new architecture based on intrinsic rewards, to deal with partially observable and sparse reward domains.  The reviewers found the novelty of the work not particularly high, and had concerns about the general utility of the method based on the empirical evidence. This paper has numerous issues and could use significant revision in terms of writing, connections to literature, experiment design, and clarity of results.   Much of the discussion focused on the scaling parameter. From an algorithmic point of view, the scaling parameter is very problematic. It is domain specific and when tuned per domain resulted in very different values. The ablation study showed that only two settings in one domain led to good performance, whereas the other resulted in no learning (for some reason the other two values were not plotted).   There are concerns that the baselines were not completely fair. In many cases different domains were used to compare against RND and ICM, and there appears to be no tuning of these baselines for the new domains this a problem due to the inherent bias in favor one s own method. In the solaris domain which was used in the RND paper, the results don t appear to match the RND paper, and in vizdoom the performance numbers are difficult to compare for ICM because a different metric is used even if you don t like their performance numbers at least report them once so we can be confident the baselines are well calibrated. One reviewer pointed out the meta parameters where different for RND than the published previous, but the paper does not describe what approach was used to tune those parameters and this is not acceptable. We cannot have much confidence that these results are reflective of those methods. Finally, there is no comment on how the performance numbers were computed and no description of how the errorbars where computed or what they represent.   The paper focuses on partially observable domains, the evidence that this method is effective in closer to Markov settings is unclear. The Atari experiments do not yield significant results by large (solairs looks as if there is no learning occurring at all a no comment about it in the text to explain). The paper claims evidence the approach can work well in both cases, but it was not even indicated if frame stacking was used in the Atari experiments. In fact, the result was only alluded too in the conclusion there was no reference in the main text to a specific result in the appendix. Text is very challenging to read. The language is informal and imprecise, and the paper frequently uses terms incorrectly or in different ways through (e.g., the use of the term novelty throughout)  This is clearly an interesting direction. The authors should keep working, but this paper is not ready for publication. I urge the authors to dig deeper in the literature to gain a more nuanced understanding of the topic. Barto et al s excellent paper on the topic is a great place to start: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858647/ 
The paper proposes to use the recently introduced "Barlow twins" contrastive learning objective, to the case of graph networks. The main concern raised by reviewers was the limited novelty of this work, which they argued mostly combines existing lines of work, and does not introduce sufficiently new concepts. This was also discussed between the authors and the reviewers. Having read the paper and the reviews, I tend to agree with the reviewers that this paper is more of a combination of existing works, and their relatively straightforward application to the graph network domain. Thus, although the empirical results are encouraging, I agree the paper has limited novelty, and falls below the ICLR acceptance bar.
The paper formulates ResNet like classifiers as the evolution of a base classifier through an operator corresponding to a PDE, up to a given final time. Using a set of assumptions on the desired properties of the flow operator, the authors show that it can be obtained as the solution of a convection diffusion equation. This generalizes ideas developed e.g. for Neural ODEs. The authors provide several examples showing that their formulation encompasses regularization methods proposed for deep NNs.  They further provide robustness guaranties for a classifier defined according to their framework. They introduce an algorithm based on a restricted version of their framework and propose different experiments showing the increased robustness of their model to a family of adversarial attacks compared to baseline ResNets.  The paper introduces an original idea, providing a very interesting connection between ResNets and PDEs. This allows the authors to exploit known properties of PDEs and opens the way to new theoretical insights on DNNs while allowing the development of DNN models with proved properties. As mentioned this generalizes the view of ResNets introduced in Neural ODEs.  Besides, the paper presents weaknesses. First the form will make it accessible only to a very small audience in the ML community. No effort is made in the writing to introduce the required PDE concepts that would help a lot understanding and appreciating the contribution. This is a pity since given the current trend on this topic this could be of interest to a large community. Then the use cases in the experiments focus solely on robustness properties and one type of attacks. This illustrates only one aspect of the potential of the framework, and this does not provide a strong case in support of the ideas introduced before. The global message carried out by the paper then becomes unclear. Overall, the current version could be largely improved and this will certainly lead to a strong contribution.
There was some disagreement between reviewers regarding the quality of the paper. Reading the paper, I had difficulty understanding what you were trying to achieve and, similarly to reviewer VgPP, felt the experimental section to be weak. While I can appreciate that compute is expensive, it would have been relevant to design more controllable continuous environments to get cleaner results in addition to those on MuJoCo. As it is, there is a lot of noise (and Table 1 does not contain confidence intervals) which, added to the general brittleness of RL algorithms, makes the experiments lack convincing power.  I encourage the authors to take all the feedback from the authors into account and resubmit an improved version of their work to another conference.
### Summary  The key idea behind this approach is a new technique to map irregular sparsity to a regular, compressed pattern. The results can, in principle, therefore overcome several standard limitations with irregular data storage formats.  The results improve over existing (though related) techniques.  ### Discussion  #### Strenghts    An interesting and timely topic to study    Results show non compute improvements  #### Weakness  The primary weakness noted among the reviewers was the lack of study on actual decoding performance. As I note below, this is a serious oversight that given the already existing theoretical work in the area warrants study as the community should begin to turn towards mapping that theory to practice.  ### Recommendation  I recommend Accept (poster). This is a strong piece of theoretical work. However,  I would like to note that while I believe this work meets the current evaluation standards set in the area, it is time for follow on work to take the additional step to validate the practicality of the approach through a performance evaluation (either in simulation or FPGA/ASIC work).
This paper develops an approach to modular lifelong learning over hierarchical tasks, proving the learnability of certain task classes under different modular architectures, with empirical evaluations on toy supervised tasks. The authors are to be commended for being one of the few works that develop lifelong learning theory. However, the reviewers found the theoretical contributions to be relatively minimal and that the empirical work needs to provide more substantial insight before it is ready for publication. Moreover, the reviewers had substantial concerns with the paper s overall presentation, in many cases finding the paper s organization confusing with many asides and critical details relegated to the appendices. The confusing presentation especially needs to be remedied, and the authors are advised to take the reviewers  concerns into consideration when preparing future versions of their manuscript.  On a minor point, the reviewers identified several places where the paper didn t cite or develop connections to relevant current literature. The authors might also be interested in connections to some much earlier work by Utgoff and Stracuzzi on many layered learning (Neural Computation 14.10, 2002), which shares some high level similarities to ideas explored in this paper.
This paper proposes an Optimal Binary Functional Search (OBFS) algorithm for searching with general score functions, which generalizes the standard similarity measures based on Euclidean distances. This yields an extension of the classical approximate nearest neighbor search (ANNS). As observed by the reviewers, this work targets an important research direction. Unfortunately, the reviewers raised several concerns regarding the clarity and significance of the work. The authors provided a good rebuttal and addressed some concerns, but not to the degree that reviewers think it passes the bar of ICLR. We encourage the authors to further improve the work to address the key concerns. 
Reviewer worries include: whether the approach scales to distant language pairs, overselling of the paper as a "framework", a few citations and comparisons missing. I agree and encourage the authors not to use the word "framework" here. I would also encourage the authors to evaluate on more interesting language pairs, and analyze what vocabularies are relocated, as well as what their method is better at compared to previous work. 
Main content: Introduces Progressive Compressed Records (PCR), a new storage format for image datasets for machine learning training. Discussion: reviewer 4: Interesting application of progressive compression to reduce the disk I/O overhead. Main concern is paper could be clearer about setting.  reviewer 5: (not knowledgable about area): well written paper. concern is that related work could be better, including state of the art on the topic. reviewer 2: likes the topic but discusses many areas for improvement (stronger exeriments, better metrics reported, etc.). this is probably the most experienced reviewer marking reject. reviewer 3: paper is well written. Main issue is that exeriments are limited to image classification tasks, and it snot clear how the method works on larger scale.   Recommendation: interesting idea but experiments could be stronger. I lean to Reject.
The paper applies tensor analysis techniques to anomaly detection from satellite data. The proposed solution is simple and seems to achieve good results. However, there is limited novelty in methodology and no sufficient experiments have been conducted to explain the performance gain. The paper is not ready for publication in ICLR but could be suitable for an application oriented venue. 
This paper presents a large scale automatically extracted knowledge base in Chinese which contains information about entities and their relations present in academic papers. The authors have collected several papers that come from around 38 different domains. As such this is a dataset creation paper where the authors have used existing methodologies to perform relation extraction in Chinese.  After having read the reviews and followup replies by authors, the main criticisms of the paper still hold. In addition to the lack of technical contribution, I feel that the writing of the paper can be improved a lot, for example, I would like to see a table with some example entities and relations extracted. That said, with further improvements this paper could potentially be a good contribution to LREC which is focused on dataset creation.  In its current form, I recommend the paper to be rejected.
The paper is rejected based on unanimous reviews.
This paper analyzes the behavior of score function as $t \rightarrow$ 0 and proposes simple approaches to mitigate issues around estimating an unbounded data score. The reviewers have acknowledged the importance of the problem and its relevance to current efforts on denoising diffusion models. However, they have also raised serious concerns regarding the clarity of the presentation and missing information across different sections. Additionally, the unbounded data score is only shown on a toy example, and it is not clear if it exists in real world datasets. Finally, e3pu pointed out that in the commonly used score parameterization from Song et al. the score parameterization can in fact grow to infinity when needed. Given these criticisms and without a response from the authors, we don t believe that the paper is ready for publication at ICLR.
This paper studies the decision boundaries of shallow ReLU network using the formalism of tropical geometry. Its main takeaway is to provide a new interpretation of the lottery ticket hypothesis in terms of network pruning strategies that preserve certain geometric structure.   Reviewers were appreciative of the clarity of the exposition, and the novel perspective on interesting and elusive phenomena such as the lottery ticket hypothesis. On the other hand, they also expressed some doubts about the significance of some aspects of the theory (such as proposition 1 and corollary 1), as well as the computational considerations required to elevate the analysis to large scale architectures from applications.   Ultimately, and after taking into consideration all the reviewing discussions, the AC believes that this submission is not yet ready for publications, but it is in a trajectory to become an important piece of work. In particular, the AC encourages delving deeper into the tropical network pruning. Additionally, the authors might want to discuss [Breaking the Curse of Dimensionality with Convex Neural Networks, Bach 17] in the related work, since this is the first instance the AC is aware of where the connection between zonotopes and shallow ReLU networks is established. 
In this paper, the authors propose a method to generate sets, which are order invariant, with a sequence to sequence model. The main idea is to order the elements of the sets, and then treat them as regular sequences. The authors propose to use PMI and conditional probability to obtain a partial order on the elements of sets. Overall, while the reviewers note that the proposed method is simple and intuitive, they also raised concerns about the paper: one of the main concerns is about missing baselines, such as non seq2seq models for set generation, such as binary classification (to predict whether an element should be included or not). For this reason, I recommend to reject the paper.
It seems that the reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
This paper improves upon the PATE GAN framework for differentially private synthetic data generation. They eliminate the need for public data samples for training the GAN, by providing a distribution which can be sampled from instead.  The authors were unanimous in their vote to accept.
The paper presents a new defense against backdoor attacks based on the discovery of homogeneous populations in the training data and subsequent filtering of poisoned data due to its difference from the said populations. The method has a solid theoretical foundation which, however, requires strong assumptions on attacks and benign data. Due to these assumptions the theoretical guarantees alone cannot ensure that the defense is robust against adaptive attacks. The experimental validation of the proposed method is limited to one benchmark datasets (CIFAR), additional results are briefly presented in the response but not elaborated on.
This paper focuses on neural network models for source code edits. Compared to prior literature that focused on generative models of source codes, this paper focuses on the generative models of edit sequences of the source code. The paper explores both explicit and implicit representations of source code edits with experiments on synthetic and real code data.  Pros: The task studied has a potential real world impact. The reviewers found the paper is generally clear to read.  Cons: While the paper doesn t have a major flaw, the overall impact and novelty of the paper are considered to be relatively marginal. Even after the rebuttal, none of the reviewers felt compelled to increase their score. One point that came up multiple times is that the paper treats the source code as flat text and does not model the semantic and syntactic structure of the source code (via e.g., abstract syntax tree). While this alone would have not been a deal breaker, the overall substance presented in the paper does not seem strong. Also, the empirical results are reasonable but not impressive given that the experiments are focused more on the synthetic data, and the experiments on the real source code are weaker and less clear as has been also noted by the fourth reviewer.   Verdict: Possible weak reject. No significant deal breaker per say but the overall substance and novelty are marginal.
This paper tackles a relatively novel problem that is the result of recent work on prefix tuning   specifically the need to be robust to adversarial perturbation in the context of prefix tuning and they show a method for achieving this without requiring more storage and obtain good results.  There were some clarity issues that were addressed by the reviewers during the rebuttal. The main issue that was pointed out was the effect of batch size on the success of the model. The authors gave experiments with batch size 1 where results are less impressive but still outperform the baseline. Also the authors say that for now they are not considering the case where only some of the elements in the batch are adversarial, which I think is ok for a research paper on such a cutting edge topic.   Thus, the result of the discussion is to lean to accept this paper given that it is now more clear, has experiments that make it clear what the benefits are in realistic settings and obtains improvements.
The paper proposes a multitask deep learning method (called Deep AMFTL) for preventing negative transfer. Despite some positive experimental results, the contribution of the paper is not sufficient for publication at ICLR due to several issues: similarity between the proposed method and existing method (e.g., AMTL), unclear rationale/intuition of the proposed model, clarity of presentation, technical formulation, and limited empirical evaluations (see reviewer comments for details). No author rebuttal was submitted. 
As the reviewers point out, this paper has potentially interesting ideas but it is in too preliminary state for publication at ICLR.  
This work proposes to define densities via the pushforward of a base density through the gradient field of a convex potential as studied in OT theory and, in particular, inspired by Brenier s theorem.  More concretely, it proposes to use ICNNs to parametrize the convex potentials and considers two mechanisms to match a target density: 1) with a known (normalized) target approximately solve the Monge Ampere equation via optimization; 2) with only samples available, they propose to use the maximum likelihood approach.  While the paper is overall well written, the idea is very close to existing work that was not mentioned or discussed in the paper. The paper would benefit from a substantial revision to incorporate the missing references and emphasize the relative novelty.
The paper provides empirical evidence that the sampling strategy used in point cloud GANs can drastically impact the generation quality of the network. Specifically, the authors show that discriminators that are not sensitive to sampling have clustering artifact errors, while those that are sensitive to sampling do not produce reasonable looking point clouds. They also provide a simple way (i.e. including AVG feature pooling) to improve generation quality for insensitive discriminator GAN setups. The reviewers agree that this is an interesting insight into the problem and this insight can help the community.  Based on the reviewers  comments and subsequent discussions, it becomes clear that the paper would be stronger and more compelling if the underlying hypothesis (i.e. the idea of sampling spectrum) is more rigorously defined (e.g. ideally with a theoretical grounding) and the claims/analyses are tied in with this definition. Such a grounded and precise setup would help in analyzing future generation discriminators that may not simply fall into the two discrete groups defined in the paper (i.e. sampling over sensitive and sampling insensitive). The results have promise, so the authors are encouraged to take into consideration the reviewer discussions to produce a stronger future submission. 
The paper presents a new algorithm, BOIL, on the importance of representation change vs reuse in MAML. All reviewers found the paper insightful, with some proposing a few changes to make the paper even stronger. Like them, I recommend accepting the paper.
The paper describes principles for endowing a neural architecture with invariance with respect to a Lie group. The contribution is that these principles can accommodate discrete and continuous groups, through approximation via a base family (B splines).   The main criticisms were related to the intelligibility of the paper and the practicality of the approach, implementation wise. Significant improvements have been done and the paper has been partially rewritten during the rebuttal period.  Other criticisms were related to the efficiency of the approach, regarding how the property of invariance holds under the approximations done. These comments were addressed in the rebuttal and the empirical comparison with data augmentation also supports the merits of the approach.  This leads me to recommend acceptance. I urge the authors to extend the description and discussion about the experimental validation.  
Following the unanimous vote of the submitted reviews, this paper is not ready for publication at ICLR. Among other concerns raised, the experiments need significant work, and the exposition needs clarification.
The paper discusses a new threat model for multi exit DNNs: attacks against efficiency of inference. The proposed attack increases the inference time of such networks by the factor of 1.5 5, while at the same reducing the accuracy of attacked networks. Unlike classical adversarial examples, the new type of attack cannot be thwarted by adversarial training.  Overall, the paper exhibits a novel contribution, is well written and methodically sound. Its practical motivation is somewhat weak, as it is currently unclear for which applications such attacks may be feasible. However, the novelty of the threat model addressed by this paper makes it an interesting methodical contribution. 
The paper proposes a method for learning identity preserving transformations through a set of learned Lie group operators. It builds upon previous work ( (Connor & Rozell, 2020; Connor et al., 2021) addressing two points: (i) how to select semantically related pairs of points, (ii) how to identify which operators are appropriate for a given local region of the manifold. Authors use nearest neighbors computed via the penultimate layers of a pretrained network to address (i), and learn a separate network q(c|z) that predicts the coefficients given a latent input z. Reviewers have two main concerns with the paper   limited novelty over earlier work that learns the Lie group operators; and the complicated nature of the method which needs training in three stages and uses a pretrained ResNet for finding nearest neighbors. Lack of comparison with relevant baselines is also pointed out by the reviewers. Given these issues, the paper is unfortunately not suitable for publication in ICLR at this point.
In this paper, the authors showed that for differentially private convex optimization, the utility guarantee of both DP GD and  DP SGD is determined by the expected curvature rather than the worst case minimum curvature. Based on this motivation, the authors justified the advantage of gradient perturbation over other perturbation methods. This is a borderline paper, and has been discussed after author response. The main concerns of this paper include (1) the authors failed to show any loss function that can satisfy the expected curvature inequality; (2) the contribution of this paper is limited, since all the proofs in the paper are just small tweak of existing proofs; (3) this paper does not really improve any existing gradient perturbation based differentially private methods. Due to the above concerns, I have to recommend reject.
The paper analyzes neuron activations for neural networks trained via RL to perform reaching with planar robot arms. This analysis includes an evaluation of the correlation between neurons of different models trained to control arms with different degrees of freedom. In performing these evaluations, the paper proposes a heuristic pruning algorithm that reduces the size of the network and increases information density. Correlation is assessed based on a projection of the source network on the target network.  The paper is well written and considers a challenging problem of interest to the community. The proposed pruning strategy as a means of maximizing information content is reasonable and seems to perform well. However, the significance of the contributions is limited by the experimental evaluation. The experiments consider a large number of models, however the scope of problems on which the method is evaluated is narrow, making it difficult to draw conclusions about the merits and significance of the work. The authors are encouraged to extend the analysis to a more diverse set of problems.
Reviewers largely agree that the proposed method for finetuning the deep neural networks is interesting and empirical results clearly show the benefits over finetuning only the last layer. I recommend acceptance. 
The paper is interesting in video prediction, introducing a hierarchical approach: keyframes are first predicted, then intermediate frames are generated. While it is acknowledge the authors do a step in the right direction, several issues remain: (i) the presentation of the paper could be improved (ii) experiments are not convincing enough (baselines, images not realistic enough, marginal improvements) to validate the viability of the proposed approach over existing ones. 
Reviewers all agree on acceptance for this paper. The initial issues with clarity seem to have been addressed by the authors.  The paper introduces a new transformer based architecture for MARL that enables variable input and output sizes, which is used to train the agent in a more general setting and on more diverse tasks for multi task training. The method also produces more interpretable agents. The paper shows results on the Starcraft multi agent challenge (not the full game of Starcraft, but still a recognised and widely used multi agent benchmark). The method produces solid results both in terms of final training performance and zero shot generalisation.  Although reviewers are generally supportive of this paper, they mention that the Starcraft challenge used is somewhat simple (only few units used), and that the transformer based architecture may not be applied to domain which lack the proper structure.   
The paper shows linear convergence for generalized mirror descent on smooth function under the PL assumption. It extends the result to stochastic generalized mirror descent under an additional assumption on the Jacobian of the mirror map. Reviewers pointed out several technical issues with the submission. While some of the problems have since been resolved in the updated version, the paper still lacks sufficient novelty, and some concerns regarding the correctness/clarity of the claims remain. Unfortunately, I can not recommend acceptance at this time. 
All reviewers agree that the proposed is interesting and innovative. One reviewer argues that  some additional baseline comparisons could be beneficial and the other two suggest inclusion of additional explanations and discussions of the results. The authors’ rebuttal alleviated most of the concerns. All reviewers are very appreciative of the quality of the work overall and recommend probable acceptance. I agree with this score and recommend this work for poster presentation at ICLR.
This paper presents a new dataset for open domain QA where the evidence required for answering a question is gathered from both structured data as well as unstructured data. The authors first show that a standard iterative retriever with a BERT based reader performs poorly on this task. They then propose fused retrieval (grouping relevant tabular and textual elements) followed by a cross block reader which improves performance.    R4 has raised strong objections about the artificiality of the dataset. I agree with that and it is unfortunate that the authors did not adequately address the reviewer s concern but instead digressed a bit. As suggested by R4, the authors should tone down their claims about the nature of the dataset. The authors should also simplify the presentation of the dataset as suggested by R2 and not make it unnecessarily complex for the reader.   However, overall, based on reviewer feedback, the authors have made significant changes to the paper. In particular they have added more baselines, ablation studies and error analysis which makes the paper much more informative.   I am okay with this paper getting accepted with the assumption that the authors will make the changes suggested above.   
This paper theoretically and empirically studies the inner and outer learning rate of the MAML algorithm and their role in convergence. While the paper presents some interesting ideas and add to our theoretical understanding of meta learning algorithms, the reviewers raised concerns about the relevance of the theory. Further the empirical study is somewhat preliminary and doesn t compare to prior works that also try to stabilize the MAML algorithm, further bringing into question its usefulness. As such, the current form of the paper doesn t meet the bar for ICLR.
The paper studies RL based on data with confounders, where the confounders can affect both rewards and actions.  The setting is relevant in many problems and can have much potential.  This work is an interesting and useful attempt.  However, reviewers raised many questions regarding the problem setup and its comparison to related areas like causal inference.  While the author response provided further helpful details, the questions remained among the reviewers.  Therefore, the paper is not recommended for acceptance in its current stage; more work is needed to better motivate the setting and clarify its relation to other areas.  Furthermore, the paper should probably discuss its relation to (1) partially observable MDP; and (2) off policy RL.
This paper explores replacing the Gaussian noise typically used in diffusion based generative models with noise from other distributions, specifically the Gamma distribution. The effect of this change is studied empirically for both image and speech generation.  Reviewers welcomed the exploration of the design space of diffusion models, and several reviewers consider the study of alternative noise distributions in particular an important contribution. They also raised several issues with precision and clarity (several mistakes in the manuscript were pointed out), the quality of the experiments, and, especially, a lack of convincing motivation for this exploration / sufficient demonstration of its impact.  While the authors have made a significant effort to address the reviewers  comments and suggestions, which includes running additional experiments, all reviewers have nevertheless chosen borderline ratings, with half erring on the side of rejection, and the other half tentatively recommending acceptance.  I am inclined to agree that, as it stands, the benefit of the proposed change of noise distribution is not convincingly shown to outweigh the additional complexity this introduces, so I am also recommending rejection.
This submission received 4 ratings, all below the acceptance threshold. The reviewers expressed concerns around overall novelty of contributions and quality of produced results, and also pointed out lack of comparisons with some prior works and gaps in empirical evaluation. The authors responded to most of these comments, but did not convince the reviewers to upgrade their ratings. The final recommendation is therefore to reject.
There is value in analyzing pre training for few shot learning, and the observation that improved disentanglement might lead to better initialization schemes for few shot learners is worth exploring. However, in its current state, the reviewers do not think the paper is ready for publication. Specifically, work needs to be done to improve the clarity, comparison to related work, and experimental analysis. 
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
With ratings of 6, 5 & 3 the numerical scores are just not strong enough to warrant acceptance. The author rebuttal was not able to sway opinions. 
Important problem and all reviewers recommend acceptance. I agree.
The paper proposes a methodology for alternatively growing and pruning a subset of layers within a network in order to eventually produce a trained, sparse model.  After discussion, all reviewers favor accept.  Empirical performance of the sparse models appears strong, but requires significant computational expense during training to achieve.
Three reviewers had a positive impression of this paper, two of them were willing to champion it. The main positive aspects mentioned by these reviewers were clarity, methodological strength, novelty and convincing experimental evaluation. On the other hand, the was one clearly negative vote, raising issues about the proposed concept of  entropy of entanglement  and about the use of tensor products. It seems that after the rebuttal, this reviewer was still not fully convinced. In my opinion, however, the rebuttal addressed most of these points of criticism in a clear and transparent way, so I recommend acceptance.
The reviewers agree that the paper is well written, and they all seem to like the general idea. One of the earlier criticisms was that you did not compare against other robust loss functions, but you have partially rectified that by comparing to L1 in the appendix. As per the request of reviewer 2 I would also compare to the Huber loss.  One remaining concern is the lack of theoretical justification, which could help address the comment of reviewer 3 regarding blurry images from location uncertainty. The other concern is that you should compare your method using FID scores from a standard implementation so that your numbers are comparable to other papers. Some of the reviewers were impressed, but confused by your relatively low scores.
This submission generated a lot of discussion.  The main strengths of the paper: * It is an interesting application of meta learning (to 3D shape completion), and a novel one. * It appears to work well: it is remarkable that the proposed model can reconstruct shapes as well as it does given a point cloud with only 50 input points.  The main concern (raised by two reviewers) is that while the method is described using the language of meta learning, the proposed architecture ends up looking very similar to a variational autoencoder: a point cloud encoder that outputs some distribution in a latent space, which is then sampled from to produce a code which drives an implicit surface decoder. The only difference appears to be that the proposed method uses a factored distribution in latent space, whereas a traditional VAE uses a non factored one (i.e. a single multivariate Gaussian over all dimensions of the latent code).  One reviewer engaged the authors in a discussion about this point, but the resulting conversation was not satisfactory. One interpretation of the authors  response is that they are simply not aware of how a VAE could be trained using variable sized point clouds as input (which is quite possible using many standard point cloud processing networks). However, at other points, they do seem to grasp this, when they write that "This flexible representation of the uncertainty [i.e. the one proposed by the authors] cannot be attained by the mere average or max pooling aggregation [what a PointNet encoder would do]." They even go on to provide an additional ablation study where they replace their factored probabilistic encoder with a deterministic mean/max pool encoder, and show worse results. Unfortunately, they never compare against *probabilistic* variants of such encoders (i.e. where the mean/max pool output is then used to compute a mean and variance).  Without seeing this comparison, the reviewers believe that this paper cannot be accepted, and I am inclined to agree.  On a related note: one reviewer pointed out an issue with unfair comparisons, in that baselines were trained on high density point clouds and evaluated on low density ones. The reviewer noted that these methods could be (and should have been) trained on point clouds of varying density. Perhaps this relates to my hypothesis that the authors initially did not understand that training such encoders on variable sized point clouds was possible. In any event, in their rebuttals, they have reported some preliminary results from experiments which do this type of training, but these results are not conclusive. Complete, conclusive results from these experiments would also need to be presented before this paper could be accepted.
The authors propose an end to end object tracker by exploiting the attention mechanism. Two reviewers recommend rejection, while the last reviewer is more positive. The concerns brought up are novelty (last reviewer), and experiments (second reviewer). Furthermore, the authors seem to overclaim their contribution. There indeed are end to end multi object trackers, see Frossard & Urtasun s work for example. This work needs to be cited, and possibly a comparison is needed. Since the paper did not receive favourable reviews and there are additional citations missing, this paper cannot be accepted in current form. The authors are encouraged to strengthen their work and resubmit to a future venue.
The reviewers overall agree that excitation dropout is a novel idea that seems to produce good empirical performance. However, they remain optimistic, but unconvinced by the experiments in their current form. The authors have done an admiral job of addressing this through more experiments, including providing error bars, however it seems as though the reviewers still require more. I would recommend creating tables of architecture x dropout technique, where dropout technique includes information dropout, adaptive dropout, curriculum dropout, and standard dropout, across several standard datasets. Alternatively, the authors could try to be more ambitious and classify Imagenet. Essentially, it seems as though the current small scale datasets have become somewhat saturated, and therefore the bar for gauging a new method on them is higher in terms of experimental rigor. This means the best strategy is to either try more difficult benchmarks, or be extremely thorough and complete in your experiments.  Regarding the wide resnet result, while I can appreciate that the original version published with higher errors, the later draft should still be taken into account as it has a) been out for a while now and b) can been reproduced in open source implementations (e.g., https://github.com/szagoruyko/wide residual networks).
This paper studies n step returns in off policy RL and introduces a novel algorithm which adapts the return’s horizon n in function of a notion of policy’s age. Overall, the reviewers found that the paper presents interesting observations and promising experimental results. However, they also raised concerns in their initial reviews, regarding the clarity of the paper, its theoretical foundations and its positioning (notably regarding the bias/variance tradeoff of uncorrected n step returns) and parts of the experimental results.  In the absence of rebuttal or revised manuscript from the authors, not much discussion was triggered. Based on the initial reviews, the AC cannot recommend accepting this paper, but the authors are encouraged to pursue this interesting research direction. 
This paper investigates the use non convex optimization for two dictionary learning problems, i.e., over complete dictionary learning and convolutional dictionary learning. The paper provides theoretical results, associated with empirical experiments, about the fact that, that when formulating the problem as an l4 optimization, gives rise to a landscape with strict saddle points and as such, they can be escaped with negative curvature. As a result, descent methods can be used for learning with provable guarantees. All reviews found the work extremely interesting, highlighting the importance of the results that constitute "a solid improvement over the prior understandings on over complete DL" and "extends our understanding of provable methods for dictionary learning". This is an interesting submission on non convex optimization, and as such of interest to the ML community of ICLR . I m recommending this work for acceptance.
The paper addresses the problem of costly human supervision for training supervised learning methods. The authors propose a joint approach for more effectively collecting supervision data from humans, by extracting rules and their exemplars, and a model for training on this data. They demonstrate the effectiveness of their approach on multiple datasets by comparing to a range of baselines.  Based on the reviews and my own reading I recommend to accept this paper. The approach makes intuitively a lot of sense and is well explained. The experimental results are convincing. 
As the reviewers pointed out, the strength of the paper mostly comes from the analysis of the non linear quantization which depends on the double log of the Lipschitz constants and other parameters. The AC and reviewers agree with the dimension independent nature of the bounds, but also note that dimension independent gound may not necessarily be significantly stronger than the dimension dependent bounds as the metric of measuring the difficulty of the problem also matters. Although the paper does seem to lack result that shows the empirical benefit of the non linear quantization. In considering the author response and reviewer comments, the AC decided that this comparison was indeed important for understanding the contribution in this work, and it is difficult to assess the scope of the contribution without such a comparison. 
The paper presents a GAN based generative model, where the generator consists of the base generator followed by several editors, each trained separately with its own discriminator. The reviewers found the idea interesting, but the evaluation insufficient. No rebuttal was provided.
The paper proposes an attention mechanism to focus on robust features in the context of adversarial attacks. Reviewers asked for more intuition, more results, and more experiments with different attack/defense models. Authors have added experimental results and provided some intuition of their proposed approach. Overall, reviewers still think the novelty is too thin and recommend rejection. I concur with them.
This paper studies the effect of data quality on adversarial robustness. It focuses on a single measure of data quality (number of times there is a perturbation that is misclassified across training iterations). The authors study the effect of data quality on robust overfitting, robustness accuracy tradeoffs and "robustness overestimation" (gap between strong and weak attacks). The main conclusions reported are that data quality as measured by their metric plays an important role in all three aspects, and a takeaway is that data of higher quality may improve robustness. While the reviewers appreciated the premise of this work, some concerns remain post rebuttal. For example, few reviewers remain skeptical of the universality of the notion of "data quality" as measured in the paper because different training methods behave differently and the data quality measured in the paper is tailored to a particular training algorithm. Some reviewers also opined that at least one of the practical implications discussed in the rebuttal should be systematically investigated and that it is important to study the effectiveness of different data quality measures, especially for the extra data. Given all this, we are unable to recommend acceptance at this time. We hope the authors find the reviewer feedback helpful.
This paper proposes a computationally efficient method to detect adversarial examples in reinforcement learning models. The detection method is based on the curvature of the loss landscape around the inputs, which is shown to have larger negative value for clean examples compared to adversarial ones. The experiments on Atari environment models show the effectiveness of the method.  The paper is well written and backs up the experimental results with mathematical intuition and analysis.   However, the baseline of Roth et al. and all attack methods used have been designed for image classifiers.   If the authors decide to focus on RL, the attack methods should be tailored to RL. The word  “worst case” in the title is misleading, since the attacks used in the paper are not optimal for RL algorithms. This reduces the credibility of the claimed successful detection.   If the authors decide to frame this work as introducing a new property of adversarial examples which can be applied to other tasks, the authors should test this method on other tasks such as benchmark image classification datasets (for example CIFAR10).   With the current experiment section, it is unclear whether this method works in RL applications since the authors use attack methods designed for image classifiers rather than RL algorithms (Please refer to the following papers for some existing RL attack methods). It is also unclear whether this paper introduces a new property of adversarial examples that is general.     Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies   Ezgi Korkmaz. Nesterov momentum adversarial perturbations in the deep reinforcement learning domain.    Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep reinforcement learning with adversarial attacks.   Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations.   Huan Zhang, Hongge Chen, Duane S Boning, and Cho Jui Hsieh. Robust reinforcement learning on state observations with learned optimal adversary.
This paper proposes a new stochastic optimization scheme similar to Adam. The authors claim that Adam can be improved upon by decorrelating the second moment estimate v_t from gradient estimates g_t. This is done through the temporal decorrelation scheme, as well as block wise sharing of estimates v_t.  The reviewers agree that the paper is sufficiently well written, original and significant to be accepted for ICLR, although some unclarity remains after the reviews. A disadvantage of the method is mainly an increased computational cost (linear in  n , however this might be negligible when sharing v_t across blocks).
The authors present a learnt scheduling mechanism for managing communications in bandwidth constrained, contentious multi agent RL domains. This is well positioned in the rapidly advancing field of MARL and the contribution of the paper is both novel, interesting, and effective. The agents learn how to schedule themselves, how to encode messages, and how to select actions. The approach is evaluated against several other methods and achieves a good performance increase. The reviewers had concerns regarding the difficulty of evaluating the overall performance and also about how it would fare in more real world scenarios, but all agree that this paper should be accepted.
The paper studies three kinds of memory augmented Transformers, focusing on one (the MemTransformer, which adds [MEM] tokens to a document.)  This is a nice clean extension of Transformers and a topic well worth investigating.  Unfortunately, the experimental results were considered unconvincing:     The baselines were relatively weak    The experimental setting was unusual (eg only 10 epochs)    The experiments did not show consistent improvement  Overall the paper was considered below acceptable quality for ICLR. 
This article studies the inductive bias in a simple binary perceptron without bias, showing that if the weight vector has a symmetric distribution, then the cardinality of the support of the represented function is uniform on 0,...,2^n 1. Since the number of possible functions with support of extreme cardinality values is smaller, the result is interpreted as a bias towards such functions. Further results and experiments are presented. The reviewers found this work interesting and mentioned that it contributes to the understanding of neural networks. However, they also expressed concerns about the contribution relying crucially on 0/1 variables, and that for example with  1/1 the effect would disappear, implying that the result might not be capturing a significant aspect of neural networks. Another concern was whether the results could be generalised to other architectures. The authors agreed that this is indeed a crucial part of the analysis, and for the moment pointed at empirical evidence for the appearance of this effect in other cases. The reviewers also mentioned that the motivation was not very clear, that some of the derivations were difficult to follow (with many results presented in the appendix), and that the interpretation and implications were not sufficiently discussed (in particular, in relation to generalization, missing a more detailed discussion of training). This is a good contribution and the revision made important improvements on the points mentioned above, but not quite reaching the bar. 
Meta Review of Federated Learning with Heterogeneous Architectures using Graph HyperNetworks  This work investigates a method for federated learning in a neural architecture agnostic setting. They do this by using a graph hypernetwork to predict the weights of given neural network architectures (which is not exactly known at the onset). The authors conduct federated learning experiments to demonstrate good performance on several real datasets, and also showed that the trained GHN model can generalize (somewhat) to unseen architectures (which are mainly in the ResNet family). Personally, as AC, I find the results very promising, and the experiments show that GHNs are highly applicable to real world applications. But the reviewers outline several weaknesses in the discussion that makes it difficult to recommend acceptance of this paper for ICLR 2022.  The main weaknesses of the work are that application is mainly focused on a narrow family of ResNet architectures (can it be shown to go beyond this? If not, can the writing be improved to show that this is useful enough for many applications?) Reviewer U48w suggested improvements to the generalization experiments, and other details that can be addressed in the writing. Reviewer Tk9o mentioned that this work can be seen as a straightforward application of GHNs (limited novelty), while other reviewers do acknowledge the novelty of the work. I recommend improving the writing to clearly address this and defend why this is not a straightforward application of previous work. With these improvements, I m confident that this work will be accepted at a future ML conference or journal.  Even though I cannot recommend acceptance, both myself and other reviewers are looking forward to seeing improved versions of this work for publication in the future. As jPp2 also noted, “Previous works on federated learning either focus on the mechanism of parameter aggregation or the aspect of privacy. This paper opens a new direction in FL where clients may not be willing to share their unique model designs. From this perspective, I think this paper has promising impact on the research field of FL.” Good luck!
Main description:  paper focuses on training neural networks using 8 bit floating point numbers (FP8). The goal is highly motivated: training neural networks faster, with smaller memory footprint and energy consumption.  Discussions reviewer 3:  gives a very short review and is not knowledagble in this area (rating is weak accept) reviewer 4: well written and convincing paper, some minor technical flaws (not very knowledgable) reviewer 1: interesting paper but argues not very practical (not very knowledgable) reviewer 2: this is the most thorough and knowledable review, and here the authors like the scope of the paper and its interest to ICLR.  Recommendation: going mainly by reviewer 2, i vote to accept this as a poster
This paper proposes a graph pooling mechanism based on adaptive edge scores that are then fed into a min cut procedure.  Reviewers acknowledged that this is an important topic of study, but all agreed that the current manuscript does not provide enough evidence about the significance and novelty of their proposed approach.  The AC recommends rejection at this time, and encourages the authors to build from the reviewers feedback to improve upon their current work. 
While the reviewers appreciated the new methodology and presentation of the paper the reviewers were concerned about the experimental section. Specifically they wanted to see optimization outside of penalized logP and QED, which are now viewed by the community as toy molecule optimization tasks (e.g., Penalized logP can always be improved by just adding a longer chain of carbon atoms). The authors responded that this would have taken too long to run Guacamol tasks in the rebuttal phase as all methods would need to be rerun for all tasks, but this is not true: many methods e.g., Ahn et al., 2020, already have reported these results and could be directly compared against (as this paper is near state of the art this would have been a convincing comparison). Another odd thing about the experimental setup is that the authors compare with Ahn et al., 2020 only for constrained property prediction. However Ahn et al., 2020 achieves a Penalized logP of 31.40 whereas the proposed method only achieves 13.95. It s suspicious that this result is missing in Table 2 of the current paper. If the authors are able to improve their work beyond Ahn et al., 2020 and related recent work on Guacamol and othe real world tasks, the paper will make a much stronger submission.
The paper proposes learning a hash function that maps high dimensional data to binary codes, and uses multi index hashing for efficient retrieval. The paper discusses similar results to "Similarity estimation techniques from rounding algorithms, M Charikar, 2002" without citing this paper. The proposed learning idea is also similar to "Binary Reconstructive Embedding, B. Kulis, T. Darrell, NIPS 09" without citation. Please study the learning to hash literature and discuss the similarities and differences with your approach.  Due to missing citations and lack of novelty, I believe the paper does not pass the bar for acceptance at ICLR.   PS: PQ and its better variants (optimized PQ and cartesian k means) are from a different family of quantization techniques as pointed out by R3 and multi index hashing is not directly applicable to such techniques. Regardless, I am also surprised that your technique just using hamming distance is able to outperform PQ using lookup table distance.  
Well motivated and well written, with extensive results. The paper also received positive comments from all reviewers. The AC recommends that the paper be accepted.
All the reviewers shared the concerns about the novelty and the quality of the results. Comparisons with some SOTA results are missing, and the inclusion of deblurring/denosing tasks is not convincing. The authors carefully addressed these issues in the rebuttal but the reviewers didn’t change their mind afterwards. After carefully examining the results in the paper, the AC agrees with the reviewers that the improvement on image quality, if any, seems to be too small to warrant a publication. 
Thanks for the discussion with reviewers, which improved our understanding of your paper significantly. However, we concluded that this paper is still premature to be accepted to ICLR2020. We hope that the detailed comments by the reviewers help improve your paper for potential future submission.
Analyzing class wise adversarial vulnerability of models is an interesting direction to pursue. However, the authors should consult the references pointed out in the reviews to put their contributions in the right perspective. Overall, the lines of inquiry explored in this paper are of interest but, as some of the reviewers point out, there are improvement in the methodology that still need to be addressed before this paper is ready for publication. (I very much recommend that the authors do build on this feedback and revise the paper, as it will be a valuable contribution then.) 
The paper discusses audio source separation with complex NNs.  The approach is good and may increase an area of research.  But the experimental section is very weak and needs to be improved to merit publication.
The paper proposes a method to escape saddle points by adding and removing units during training. The method does so by preserving the function when the unit is added while increasing the gradient norm to move away from the critical point. The experimental evaluation shows that the proposed method does escape when positioned at a saddle point   as found by the Newton method. The reviewers find the theoretical ideas interesting and novel, but they raised concerns about the method s applicability for typical initializations, the experimental setup, as well as the terminology used in the paper. The title and terminology were improved with the revision, but the other issues were not sufficiently addressed.
The paper provides a careful, reproducible empirical comparison of 5 graph neural network models on 9 datasets for graph classification. The paper shows that baseline methods that use only node features (either counting node types, or summing node features) can be competitive. The authors also provide some guidelines for ways to improve reproducibility in empirical comparisons of graph classification.  The authors responded well to the issued raised during review, and updated the paper during the discussion period. The reviewers improved their score, and while there were reservations about the comprehensiveness of the set of experiments, they all agreed that the paper provides a solid empirical contribution to the literature.  As machine learning becomes increasingly popular, papers that perform a careful empirical survey of baselines provide an important sanity check that future work can be built upon. Therefore, this paper, while not covering all possible graph neural network questions, provides an excellent starting point for future work to extend.  
This paper presents work on open world object detection.  The main idea is to use fixed per category semantic anchors.  These can be incrementally added to when new data appear.  The reviewers engaged in significant discussion around the paper with many iterations of improvements to the paper.  Initial concerns regarding zero shot learning were addressed, as were remarks on presentation and claims.  In the end the reviewers were split on this paper.  I recommend to accept the paper on the basis of the semantic topology ideas and the thorough experimental results.  The remaining concern centered around the evaluation protocol used in the paper, which follows that in the literature (e.g. Joseph et al. CVPR 21).  While this is not a fatal flaw, it is an issue with how this genre of methods is evaluated.  It would be good to add discussion to the final paper to highlight this as an opportunity for future work in the field to address.  Specifically, as a reviewer noted "after detecting "unknown" objects in T1, the (hypothetical) annotation process provides boxes for ALL objects of some new classes instead of only for those that have been correctly detected (localized and marked "unknown")."
The paper proposes several modifications to vision transformers: multiscale features, a variant of factorized attention, and "dynamic position bias". The proposed architecture with these modifications achieves strong results on classification, detection, and segmentation.  After considering the authors  responses, all reviewers are positive about the paper (reviewer K7wS mentioned upgrading to weak accept, but apparently forgot to do so). Main pros include clean architecture and strong empirical results. The main con is the somewhat limited novelty.  Overall, I recommend acceptance. While each of the proposed modifications might not be that unique, they are reasonably new in the context of transformers and their combination makes for a clean architecture that performs very well in practice.
The paper presents a RL planning algorithms where a policy selects a reachable state. The empirical evaluation shows promising results in some environments. While all the reviewers agreed that the state planning RL is a relevant and promising direction, the reviewers expressed concerns with the rigor, significance of the results, and incremental novelty.  To improve them paper the authors should:    Bring the theoretical foundation in the main text, and add more rigorous analysis, including the limitations of the method.   The readability of the figures needs to be improved. The legend on the figures is too small and colors are too similar that renders the figures unreadable and confusing.   If the authors  goal is to develop a method for interpretable RL, then some results and analysis need to address the interpretability of method.
Compressing BERT is a practically important research direction. Our main concern on this submission is on its practical value. Comparing with MobileBERT in the literature, NAS BERT does not show advantages on any aspect: latency, prediction performance, or model size (less important), while being much more costly to build because of NAS. MobileBERT just simply narrowed the original BERT models (8x narrower than BERT large). So it is hard to convince the readers that adaptive size or NAS is interesting or matters. On the research side, this paper have some interesting points on designing the search space, but overall the novelty of this paper is limited, as all of the reviewers pointed out. It is also worth noticing that the claim of "task agonistic" in this paper does not fully hold: in the downstream tasks, the soft labels of the teacher model are required to train the compressed model. To be fully "task agonistic", the results on downstream tasks should be solely based on training with the ground truth labels, as in the MobileBERT paper. Once following the exact task agnostic experimental protocol, the reported performance in this paper may be significantly lower. 
The paper introduces a method called DeepTLF that handles heterogeneous tabular data by using GBDT as an encoder for a DNN.  The paper is clearly written and the method works as intended.  There is however the issue of novelty (raised by Q6we). The method indeed relies of the capacity of GBDT to represent the data, the internal node values are used as features to train a downstream neural network. This process is straightforward, which is good from an application perspective, though the paper offers limited insights to the community from a scientific perspective.  Another reviewer concern was that of incompleteness of experiments and lack of certain details (reviewers vaip and gWeP). This was answered in the rebuttal, which the reviewers acknowledged, however, the authors did not provide a revised version of the manuscript, when ICLR in fact allowed (and actually encouraged) revised versions to be submitted by Nov 22. Without a revised version, it is difficult for the reviewers to assess whether the text in the final manuscript will actually accurately reflect the changes they suggested. This justifiably caused two of the reviewers to keep their original scores (they explicitly stated the lack of an updated manuscript as the reason).  Given lack of an update, coupled with the issue of novelty, I conclude the paper is not ready to be accepted in its current form.
This paper has been withdrawn by the authors.
This paper tackles the problem of how to adapt a model from a source to a target domain when both data is not available simultaneously (even unlabeled) to a single learner. This is of relevance for certain privacy preserving applications where one setting would like to benefit from information learned in a related setting but due to various factors may not be willing to directly share data. The proposed solution is a transfer alignment network (TAN) which consists of two autoencoders (each trained independently on the source and the target) and an aligner which has the task of mapping the latent codes of one domain to the other.   All three reviewers expressed concerns for this submission. Of greatest concern was the experimental setting. The datasets chosen were non standard and there was no prior work to compare against directly so the results presented are difficult to contextualize. The authors have responded to this concern by specifying the existing domain adaptation benchmarks are more challenging and require more complex architectures to handle the “more complex data manifolds”. The fact that existing benchmark datasets may be more complex the the dataset explored in this work is a concern. The authors should take care to clarify whether their proposed solution may only be applicable to specific types of data. In addition, the authors claim to address a new problem setting and therefore cannot compare directly to existing work. One suggestion is if using new data, report performance of existing work under the standard setting to give readers some grounding for the privacy preserving setting. Another option would be to provide scaffold results in the standard UDA setting but with frozen feature spaces. Another option would be to ablate the choice of L2 loss for learning the transformer and instead train using an adversarial loss, L1 loss etc. There are many ways the authors could both explore a new problem statement and provide convincing experimental evidence for their solution. The AC encourages the authors to revise their manuscript, paying special attention to clarity and experimental details in order to further justify their proposed work. 
The paper proposes a novel black box attack aiming to fool a particular detector model. All reviewers see problems in the claims, the experiments etc and all argue for rejection. The authors did not provide a rebuttal to clarify any of the questions of the reviewers. Thus this is a clear reject.
The paper presents a solution to generating molecule with three dimensional structure by learning a low dimensional manifold that preserves the geometry of local atomic neighborhoods based on Euclidean distance geometry.   The application is interesting and the proposed solution is reasonable. The authors did a good job at addressing most concerns raised in the reviews and updating the draft.   Two main concerns were left unresolved: one is the lack of novelty in the proposed model, and the other is that some arguments in the paper are not fully supported. The paper could benefit from one more round of revision before being ready for publication.   
The paper presents a novel strategy for statistically motivated feature selection i.e. aimed at controlling the false discovery rate. This is achieved by extending knockoffs to complex predictive models and complex distributions; specifically using a variational auto encoder to generate conditionally independent data samples with the same joint distribution.   The reviewers and ACs noted weakness in the original submission related to the clarity of the presentation, relationship to already published work, and concerns about the correctness of some main claims (this mostly seems to have been fixed after the rebuttal period). There are additional concerns about a thorough evaluation of the claimed results, as the ground truth is unknown. The authors (and reviewers) also note a similar paper submitted to ICLR with the same goal but implemented using GANs. Nevertheless, there remain significant concerns about the clarity of the presentation.
This paper presents an approach for scalable autoregressive video generation based on a three dimensional self attention mechanism. As rightly pointed out by R3, the proposed approach ’is individually close to ideas proposed elsewhere before in other forms ... but this paper does the important engineering work of selecting and combining these ideas in this specific video synthesis problem setting.’  The proposed method is relevant and well motivated, and the experimental results are strong. All reviewers agree that experiments on the Kinetics dataset are particularly appealing. In the initial evaluation, the reviewers have raised several concerns such as performance metrics, ablation study, training time comparison, empirical evaluation of the baseline methods on Kinetics, that were addressed by the authors in the rebuttal.  In conclusion, all three reviewers were convinced by the author’s rebuttal, and AC recommends acceptance of this paper – congratulations to the authors!
Based on the paper, reviewers  comments and discussions, and the responses, the meta reviewer would like to suggest the authors to improve the paper and resubmit.
This work proposes a hybrid autoregressive and adversarial model for sound synthesis (including but not limited to speech), conditioned on various types of control signals. Although recent adversarial approaches have gained favor over previously popular autoregressive approaches in this domain, because of their ability to produce audio signals much more quickly, the authors argue that these models tend to introduce certain types of artifacts which stem from an inability to learn accurate pitch and periodicity. They propose to address this by reintroducing some degree of autoregression, without compromising too much on inference speed.  Reviewers praised the presentation of this work, the thoroughness of the experimental evaluation, and the audio examples provided. A few concerns were also raised regarding related work and the clarity of some parts of the paper, which the authors have taken the time to address. After the discussion phase, all reviewers chose to recommend acceptance, and I will follow their recommendation.
The paper gives a novel algorithm for transfer learning with label distribution shift with provably guarantees. As the reviewers pointed out, the pros include: 1) a solid and motivated algorithm for a understudied problem 2) the algorithm is implemented empirically and gives good performance. The drawback includes incomplete/unclear comparison with previous work. The authors claimed that the code of the previous work cannot be completed within a reasonable amount of time. The AC decided that the paper could be accepted without such a comparison, but the authors are strongly urged to clarify this point or include the comparison for a smaller dataset in the final revision if possible. 
The paper was found to be well written and conveys interesting idea. However the AC notices a large body of clarifications that were provided to the reviewers (regarding the theory, experiments, and setting in general) that need to be well addressed in the paper. 
This paper has some interesting ideas and is an incremental improvement over previous work. However, it needs further revisions and polishing. The relation to prior work is a bit unclear. Since you mention POMDPs, what would be an equivalent version of your method in POMDPS? Why not compare your algorithm with a state of the art method for small discrete problems? It is also a bit unclear why training a model to predict beliefs would be faster than just calculating them (after all the data must come from somewhere)..
The paper demonstrates that transformer architectures can be trained to compute solutions of linear algebra problems with high accuracy. This is an interesting direction and, as the the reviews and the discussion show it is a "good data point and insightful", as one reviewer puts it. I fully agree with this but also agree with one other reviewer in that this is "yet another" application of a known transformer architecture. The author should place the model into context and provide some perspective. Without, the motivation behind solving the specific set of linear algebra problems considered is a bit unclear. For instance, could a transformer now learn to solve corresponding ML problems? Moreover, the dimensions of the considered matrices are rather small, and the generalization to larger dimension appear to be tricky.
This paper shows how multiple tasks can be encoded in a single neural network without the need for explicit modular construction for each task. The idea is very interesting and the research work presented is of high quality.   All the reviewers underline their interest in the presented work. However, there is a deviation in the reviewers  score with half voting  for acceptance and the other half for rejection. The main concern of the fellow reviewers with the below acceptance threshold score was the difficulty in grapsing the theory of the research presented due to the lack of important content from the main manuscript due to space limitations. The authors have an extended supplementary material that covers the whole magnitude of their work.   I understand the reviewers  concern on how such a dense presentation does not do justice and harms the presented effort itself. However, given the edits the authors added to address the issue rasied and the interest and potential of this work   acknowledged by all the reviewers and myself I recommend acceptance. This is a work of a quality I would like to keep seeing in ICLR. 
This paper proposes Adversarial Feature Desensitization (AFD) as a defense against adversarial examples. Specifically, following the spirit of GAN and Adversarial Domain Adaptation, an adversarial discriminator is introduced to distinguish clean and perturbed inputs at the representational level.   This paper receives 3 reject and 1 accept recommendations. On one hand, though the proposed method shares some similarity with the Feature Scattering method at a high level, most of the reviewers still find the proposed method is interesting. The AC also agrees that the paper s organization and typos does not warrant a rejection.   On the other hand, the reviewers have also raised a few concerns. (i) A more careful discussion on the scalability of the proposed method is needed. (ii) Experiments are mostly focused on small datasets, while results on ImageNet is lacking, which makes the paper less convincing. The authors claim that they are trying to at least run Tiny ImageNet experiments; however, this set of results are not provided by the end. (iii) A more detailed analysis and visualization on the learned difference between the distributions of benign and adversary representation is needed, since a discriminator is learned here.   The rebuttal unfortunately did not fully address the reviewers  main concerns. On balance, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere. 
This paper proposes a neural network architecture that represents each neuron with input and output embeddings. Experiments on CIFAR show that the proposed method outperforms baseline models with a fully connected layer.  I like the main idea of the paper. However, I agree with R1 and R2 that experiments presented in the paper are not enough to convince readers of the benefit of the proposed method. In particular, I would like to see a more comprehensive set of results across a suite of datasets. It would be even better, although not necessary, if the authors apply this method on top of different base architectures in multiple domains. At the very least, the authors should run an experiment to compare the proposed approach with a feed forward network on a simple/toy classification dataset. I understand that these experiments require a lot of computational resources. The authors do not need to reach SotA, but they do need to provide more empirical evidence that the method is useful in practice.  I also would like to see more discussions with regards to the computational cost of the proposed method. How much slower/faster is training/inference compared to a fully connected network?  The writing of the paper can also be improved. There are many a few typos throughout the paper, even in the abstract.   I recommend rejecting this paper for ICLR, but would encourage the authors to polish it and run a few more suggested experiments to strengthen the paper.
This paper uses Bayesian optimization with neural networks for neural architecture search.  One of the contributions is a path based encoding that enumerates every possible path through a cell search space. This encoding is shown to be surprisingly powerful, but it will not scale to large cell based search spaces or non cell based search spaces. The availability of code, as well as the careful attention to reproducibility is much appreciated and a factor in favor of the paper.  In the discussion, it surfaced that a comparison to existing Bayesian optimization approaches using neural networks would have been possible, while the authors initially did not think that this would be the case. The authors promised to include these comparisons in the final version, but, as was also discussed in the private discussion between reviewers and AC, this is problematic since it is not clear what these results will show. Therefore, the one reviewer who was debating about increasing their score did in the end not do so (but would be inclined to accept a future version with a clean and thorough comparison to baselines).   All reviewers stuck with their score of "weak reject", leaning to borderline. I read the paper myself and concur with this judgement. I recommend rejection of the current version, with an encouragement to submit to another venue after including a comparison to BO methods based on neural networks.
This article investigates the optimization landscape of shallow ReLU networks, showing that for sufficiently narrow networks there are data sets for which there is no descent paths to the global minimiser. The topic and the nature of the results is very interesting. The reviewers found that this article makes important contributions in a relevant line of investigation and had generally positive ratings. The authors  responses addressed questions from the initial reviews, and the discussion helped identifying questions for future study departing from the present contribution.  
This paper exposes a method for video compression based on multi head models. The reviewers seem to agree that the results are interesting, and worth publishing. On the other hand, there are many concerns raised on the quality of the writing, with grammatical mistakes and confusing parts. The motivation for the multi head models, as well as its novelty, has been questioned in all reviews. Although the authors rebuttal has lead some reviewers to increase their score, it s still very concerning that authors needed to explain the main point of the paper to each reviewers. I think that the authors should polish this paper, taking into account the reviewers feedback, which would make a stronger paper, and submit it again in a future venue. I therefore recommend reject for this paper.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    novel approach to audio synthesis   strong qualitative and quantitative results   extensive evaluation   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    small grammatical issues (mostly resolved in the revision).   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
This paper introduces a new method for diffusion based generative modeling through a Brownian bridge formulation, where the data and latent variable can be coupled. They extend their method to mixtures of diffusion bridges and spatially correlated processes that go beyond the factorial diffusion processes used in prior work.  We thank the authors for engaging with the reviewers and addressing many of their detailed concerns. While reviewers agreed that the proposed theory and methodology were novel and interesting, there are no small or large scale experiments or empirical comparisons to the relevant prior work. In the absence of theoretical justification (bound or proof) as to why the proposed diffusion bridge mixture transport method would result in better performance, more empirical comparisons and evaluations are needed. Additionally, several reviewers found the presentation confusing and overly complex, including the notation, writing, and figures. Given the lack of experimental results and concerns over presentation, I’m inclined to reject this paper.
The paper introduces group equivariant self attention networks constructed by defining positional encoding that are invariant to the group action considered. This is related to equivariance in set networks . The work is sound and the idea of infusing the inductive bias via the positional encoding is  interesting and leads to improvement  results when comparing transformers with equivariance and without it, nevertheless more work needs to be done to bridge the gap in terms of performance with CNNs as pointed by the reviewers.  Authors made an admirable efforts in the rebuttal and in the revision of the paper clarifying most of the reviewers questions and concerns.   Accept   
The paper proposes a method to generate attention masks to interpret the performance of RL agents. Results are presented on a few ATARI games. Reviewers unanimously vote for rejecting the papers. R1, R3 give a score of 5, whereas R4, R5 give a score of 4. Their concerns are best explained in their own words:   R1 says, "The use of attention maps to analyze and explain deep neural networks is not new in itself, and learning attention maps to improve vision tasks is not new either."  R3 says, "the analysis of the learned attention masks seems selective. Some automatic metrics or systematic studies of different game categories (shooting, maze like, and ball and paddle) may shed light on the learned attention s general property."  R5 says, "I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive"  In their rebuttal, to address R1 s concern authors suggested that the use of attention on both value and policy networks is novel. This is not sufficient, because it does not show why such attention maps are more useful than ones proposed by prior work. As suggested by reviewers, a systematic study or a human study clearly showing that the proposed method adds more interpretability is critical. However, this is missing.  In response to R3, the authors provided experiments on more games. But this is not the point   because it s not about the number of environments in which experiments are provided, but rather the nature of the analysis that is performed. Finally, R5 comments that it s unclear whether attention actually provided interpretability or not.   Due to the lack of convincing analysis that demonstrates the utility of the proposed method in advancing the understanding of decisions made by RL agents, I recommend that the paper be rejected.  
This submission aims to improve adversarial training by making it involve also layer wise (instead of only input wise) perturbations. This is an interesting idea and it is accompanied by an interesting ODE based perspective on the resulting dynamics. However, as the comments and reviews detail, the current manuscript misses the discussion of very relevant previous work, does not specify important details of the approach (e.g., how to bound the extent of the perturbations used), and relies on weak primitives (FGSM vs PGD).   The consensus is that this would be an interesting and valuable contribution but only after addressing the above shortcomings. 
This paper proposed a new experience replay approach, applicable to deep RL methods. Two reviewers suggested acceptance and two did rejection. The first negative reviewer R1 raised a concern on continuous vs. discrete issue, but AC thinks that the authors  response is not fully convincing enough.  The second negative reviewer R2 pointed out that the reported performance of SAC is poor compared to the existing implementation (although authors claim a different set of hyperparameters is used), which AC thinks is a critical weakness to judge the value of the experiments. Two other positive reviewers (even R4) shows mixed opinions. Overall, AC thinks this is a borderline paper, a bit toward rejection.
The paper presents a novel architecture, reminescent of mixtures of experts, composed of a set of advocates networks providing an attention map to a separate "judge" network. Reviewers have several concerns, including lack of theoretical justification, potential scaling limitations, and weak experimental results. Authors answered to several of the concerns, which did not convinced reviewers. The reviewer with the highest score was also the least confident, so overall I will recommend to reject the paper.
The main contribution of this paper is a nearly linear time algorithm for learning Bayesian networks with a known structure when an epsilon fraction of the samples are contaminated. The model assumes that the directed graph is known and the goal is to estimate a vector of length m that describes the conditional distribution of any node for any configuration if its parents. Let N be the number of samples and let d be the number of nodes. Prior work gave an algorithm that runs in time N d^2 time. This is now improved to roughly Nd time under natural conditions on the "balancedness" and the "minimum parental configuration probability". The algorithm itself is simple, and is a more direct reduction to robust mean estimation.   The reviewers had somewhat differing opinions. The pros are that it s a basic problem, the algorithm is clean and the ingredients in the improved running time could have further applications. The negative is that there are no experiments, even synthetic ones, to demonstrate practicality. Overall it still seems that there is enough excitement about the work to merit acceptance. 
Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. A significant concern is that the definition of privacy used here is not adequately justified. This opens up issues of: 1) possible attacks, 2) privacy guarantees that are not worst case, among others. 
This paper studies the effect of various data augmentation methods on image classification tasks. The authors propose the structural similarity as a measure of the magnitude of the various types of data augmentation noise they consider and argue that it is outperforms PSNR as a measure of the intensity of the noise. The authors performed an empirical analysis showing that speckle noise leads to improved CNN models on two subsets of ImageNet. While there is merit in thoroughly analysing data augmentation schemes for training CNNs, the reviewers argued that the main claims of the work were not substantiated and the raised issues were not addressed in the rebuttal. I will hence recommend rejection of this paper. 
This paper provides a novel and non trivial method for approximating the eigenvectors of the Laplacian, in large or continuous state environments. Eigenvectors of the Laplacian have been used for proto value functions and eigenoptions, but it has remained an open problem to extend their use to the non tabular case. This paper makes an important advance towards this goal, and will be of interest to many that would like to learn state representations based on the geometric information given by the Laplacian.   The paper could be made stronger by including a short discussion on why the limitations of this approach. Its an important new direction, but there must still be open questions (e.g., issues with the approach used to approximate the orthogonality constraint). It will be beneficial to readers to understand these issues.
Reviewers agreed on the value of theoretical contribution, especially the surprising conclusion that the weight tied and untied RNTK are identical. The empirical results were updated in response to reviewer s suggestion. I believe this would be of interest to ICLR audience.
The paper proposes a fast, nearly linear time, algorithm for finding a sparsifier for general directed and undirected graphs that approximately preserves the spectral properties of the original graph. The reviewers appreciated the main contribution of the paper, but they were concerned about the correctness and clarity of the paper, as well as the relevance of the contribution to machine learning. Following the discussion with the authors, the reviewers still felt that these concerns had not been fully addressed by the authors  responses and the subsequent revision of the paper. After taking these concerns into account as well as evaluating the paper relative to other ICLR submissions, I recommend reject.
The paper defines a methodology to discover unknown classes in a semi supervised learning setting, based on: i) defining a proper representation based on self supervision on all samples; ii) defining equivalence classes on the unlabelled samples, based on ranking statistics; iii) training supervised heads aimed to predict the labels (when available) and the equivalence class indices (when unlabelled).   All reviewers agree that the ranking statistics based heuristics is a quite innovative element of the paper. The extensive and careful experimental validation, with the ablation studies, establishes the merits of all ingredients.   Therefore, I propose acceptance of this paper. 
It is a simple but good idea to consider the choice of mini batch size as a multi armed bandit problem. Experiments also show a slight improvement compared to the best fixed batch size.  The main concerns from the reviewers are that (1) treating the choice of hyper parameters as a bandit problem is known and has been exploited in different context, and this paper is limited to the choice of the mini batch size, (2) the improvement in the test error is not significant. The authors  feedback did not solve the concerns raised by R2.  This paper conveys a nice idea but as the current form it falls slightly below the standard of the ICLR publications. One direction for improvement, as suggested by the reviewer, would be extending the idea for a wider hyper parameter selection problems.
This paper caused a lot of discussions before and after the rebuttal. The concerns are related to the novelty of this paper, which seems to be relatively limited. Since we do not have a champion among positive reviewers, and the overall score is not high enough, I cannot recommend its acceptance at this stage. 
This paper presents a broad exploratory analysis of the geometry of token representations in large language models, with a focus on isotropy and manifold structure, and reveals some surprising findings that help explain past observations.  Pros:   Clear and surprising analytical findings concerning a broad and widely used family of models.  Cons:   The paper is a fairly broad exploratory analysis, with no single precise claim that ties together every piece of the work.  I thank both the authors and reviewers for an unusually productive discussion.
This paper proposes a platform for benchmarking and evaluating reinforcement learning algorithms.  While reviewers had some concerns about whether such a tool was necessary given existing tools, reviewers who interacted with the tool found it easy to use and useful. Making such tools is often an engineering task and rarely aligned with typical research value systems, despite potentially acting as a public good. The success or failure of similar tools rely on community acceptance and it is my belief that this tool surpasses the bar to be promoted to the community at a top tier venue.  
The paper proposes to augment the conditional GAN discriminator with an attention mechanism, with the aim to  help the generator, in the context of image to image translation. The reviewers raise several issues in their reviews. One theoretical concern has to do with how the training of the attention mechanism (which seems to be collaborative) would interact with the minimax, zero sum nature of a GAN objective; another with the discrepancy in how the attention map is used during training and testing. The experimental results were not significant enough, and the reviewers also recommend additional experiment results to clearly demonstrate the benefit of the method. 
The paper introduces an interesting idea of using different rates of learning for low level vs high level computation for meta learning. However, the experiments lack the thoroughness needed to justify the basic intuition of the approach and design choices like which layers to learn fast or slow need to be further ablated.
Using ideas from mean field theory and statistical mechanics, this paper derives a principled way to analyze signal propagation through gated recurrent networks.  This analysis then allows for the development of a novel initialization scheme capable of mitigating subsequent training instabilities.  In the end, while reviewers appreciated some of the analytical insights provided, two still voted for rejection while one chose accept after the rebuttal and discussion period.  And as AC for this paper, I did not find sufficient evidence to overturn the reviewer majority for two primary reasons.  First, the paper claims to demonstrate the efficacy of the proposed initialization scheme on multiple sequence tasks, but the presented experiments do not really involve representative testing scenarios as pointed out by reviewers.  Given that this is not a purely theoretical paper, but rather one suggesting practically relevant initializations for RNNs, it seems important to actually demonstrate this on sequence data people in the community actually care about.  In fact, even the reviewer who voted for acceptance conceded that the presented results were not too convincing (basically limited to toy situations involving Cifar10 and MNIST data).  Secondly, all reviewers found parts of the paper difficult to digest, and while a future revision has been promised to provide clarity, no text was actually changed making updated evaluations problematic.  Note that the rebuttal mentions that the paper is written in a style that is common in the physics literature, and this appears to be a large part of the problem.  ICLR is an ML conference and in this respect, to the extent possible it is important to frame relevant papers in an accessible way such that a broader segment of this community can benefit from the key message.  At the very least, this will ensure that the reviewer pool is more equipped to properly appreciate the contribution.  My own view is that this work can be reframed in such a way that it could be successfully submitted to another ML conference in the future.
While a lot of previous work on emergent communications studies discrete protocols, this work explores a continuous and audio based channel for learning multi agent communication. Reviewers have commented positively on the novelty of the topic. At the same time, there are a number of concerns raised with respect to experimental design and implementation (6auy) and general approach of the topic which, as reviewers t576 and 42Xh point,  doesn t really go deep into the analysis and understanding of the particular experimental setup and findings. So, unfortunately as the papers stands I cannot recommend acceptance at this time. However, given that continuous communication in emergent communication is a somewhat overlooked topic, I would encourage the authors to use the reviewers  feedback and strengthen their manuscript.
The paper focuses on providing generalization bounds for SGD for functions that are invariant under scaling. The paper s analysis is based on the stability framework but instead focuses on a metric that is based on the anglular distance as compared to the euclidean distance.  Overall the reviewers found the paper to be interesting and the results to be useful. However the reviewers found the paper to be significantly lacking in terms of its presentation. In particular a clear exposition of the central object of the paper, i.e. normalized loss function was missing as well as clear comparisons between the presented results and existing results. I recommend the authors to motivate their results better and contrast their presented results with existing results to fully highlight the impact of their presented result. Hopefully the suggestions made by the reviewers in terms of presentation will be helpful to the authors towards improving the paper.
The paper proposes a hierarchical Bayesian model over multiple data sets that                                                       has both data set specific as well as shared parameters.                                                                            The data set specific parameters are further encouraged to only capture aspects                                                     that vary across data sets by an addition mutual information contribution to the                                                    training loss.                                                                                                                      The proposed method is compared to standard VAEs on multiple data sets.                                                                                                                                                                                                 The reviewers agree that the main approach of the paper is sensible. However,                                                       concerns were raised about general novelty, about the theoretical justification                                                     for the proposed loss function and about the lack of non trivial baselines.                                                          The authors  rebuttal did not manage to full address these points.                                                                                                                                                                                                      Based on the reviews and my own reading, I think this paper is slightly                                                             below acceptance threshold.
This work proposes new learning algorithms that fine tune ("tailor") a model at test time using unsupervised objectives. This formulation allows for introducing an inductive bias into the model that might improve generalization on unseen data. The proposed algorithm is demonstrated on two example tasks.  The reviewers like the topic and also find the proposed approach to be interesting. However, they are unconvinced by the current empirical evaluation of the method. Additional experimental evaluation could improve our understanding of the proposed method and help contrast it to previously proposed techniques. Given these reviews I recommend rejecting the paper at this time.
This paper proposes a novel problem of polymer retrosynthesis, and a method to solve it. The authors formally define the polymer retrosynthesis optimization problem as a constrained problem to identify the monomers and the unit polymer, with the recursive and stability constraints. Further, since the main challenge with polymer retrosynthesis is the extremely scarce training data, the authors propose a domain adaptation technique that can utilize a single step retrosynthesis model trained on a large amount of data. The authors also use Retro* [Chen et al. 20] for synthesizability check of the monomers. The proposed method, PolyRetro, is validated against few naive baselines for top k recovery performance, and is shown to outperform them.   All reviewers found the problem of polymer retrosynthesis tackled to be important as well as novel, and the paper to be very well written. However, all reviewers had a common concern on the limited technical novelty and meaningless baselines that makes it difficult to evaluate the significance of the results. Some of the reviewers were also concerned with the insignificant performance gain with the proposed domain adaptation technique (PolyRetro vs. PolyRetro USPTO in Figure 4), and its limited applicability to a condensation polymerization. The authors provided new results with more baselines, which fine tune the single step retrosynthesis model (MLP, seq to seq) trained on USPTO.  The below is the summary of pros and cons:  **Pros**   The tackled polymer retrosynthesis problem is novel and practically important.    The proposed problem formulation and constraints are interesting and make sense.   The paper is well written and easy to follow even for non domain experts.  **Cons**   The proposed solution with recursive and stability constraints is rather straightforward, as well as the use of Retro* for screening out the monomers.    The domain adaptation technique, which is advertised as an important contribution to combat extreme data scarcity, is both straightforward and yields small performance gain.   The baselines in the original version of the paper are simply meaningless strawmans, and the new baseline (seq2seq retro) in Section D of the Appendix seems quite strong, making it difficult to validate the effectiveness of the proposed method.  The paper received split reviews, with three leaning toward acceptance and one leaning toward rejection. After the interactive discussion period with the authors, the reviewers had an in depth discussion, where all reviewers agreed that the technical novelty or contribution to the general machine learning field, or general applicability to polymer synthesis is limited. The reviewers did not reach a consensus, which makes the paper a borderline case, and after the discussion with the program chairs, we decided to reject the paper due to the unresolved concerns.     I believe that the proposed problem specific solution is adequate, although it has little technical novelties, since this is an application paper. However what is more problematic is the inconclusive experimental validation results due to lack of meaningful baselines. I suggest the authors to compare against seq2seq retro + Retro* in order to properly validate the effectiveness of the proposed method. Also, results in Figure 7, or the polymerization examples in Section A of the appendix should be incorporated into the main paper. I also suggest that the authors drop domain adaptation from the title since it constitutes a small part of the method and thus is misleading. 
This paper provides a novel generalization bound for neural networks using knowledge distillation. In particular, they argue that  "test error <  training error + distillation error + distillation complexity" where the distillation complexity is typically much smaller than the original complexity of the neural network. This is motivated by the empirical findings that neural networks can typically be significantly compressed in practice using KD without losing too much accuracy.    I found this result novel and the direction is very promising. This is a clear accept for ICLR. 
This paper proposes a novel technique to learn a disentangled latent space using VAEs and semi supervision. The technique is based on a careful specification of the joint distribution where the labels inform a factorisation of the distribution over continuous latent factors. The technique allows for inference, generation, and intervention in a tractable way.  The paper is well written, the formulation is original, and the experiments convincing. There were some confusions that were mostly resolved during the discussion.    In addition to the expert reviews attached, I would like to remark that I too find the formulation interesting and elegant. And if I may add to the discussion, oiVAE (output interpretable VAEs) by Ainsworth et al presented at ICML18 is a related piece of work that did not occur to me earlier, but which the authors could still relate to (I d certainly enjoy reading about the authors  views on that line of work). 
Summary: The authors propose to approximate operations on graphs, roughly speaking by approximating the graph locally around a collection of vertices by a collection of trees. The method is presented as a meta algorithm that can be applied to a range of problems in the context of learning graph representations.  Discussion: The reviews are overall positive, though they point out a number of weaknesses. One was unconvincing experimental validation. Another, more conceptual one was that this is a  unifying framework  rather than a novel method. Additionally, there were a number of minor points that were not clear. However, the authors have provided additional experiments that the reviewers consider convincing, and were able to provide sufficient clarification.  Recommendation: The reviewer s verdict post discussion favors publication, and I agree. The authors have convincingly addressed the main concerns in discussion, and novelty is not a necessity: Unifying frameworks often seem an end in themselves, but this one is potentially useful and compellingly simple. 
The paper proposes a new attentional pooling mechanism that potentially addresses the issues of simple attention based weighted averaging (where discriminative parts/frames might get disportionately high attentions). A nice contribution of the paper is to propose an alternative mechanism with theoretical proofs, and it also presents a method for fast recurrent computation. The experimental results show that the proposed attention mechanism improves over prior methods (e.g., STPN) on THUMOS14 and ActivityNet1.3 datasets. In terms of weaknesses: (1) the computational cost may be quite significant. (2) the proposed method should be evaluated over several tasks beyond activity recognition, but it’s unclear how it would work.   The authors provided positive proof of concept results on weakly supervised object localization task, improving over CAM based methods. However, CAM baseline is a reasonable but not the strongest method and the weakly supervised object recognition/segmentation domains are much more competitive domains, so it s unclear if the proposed method would achieve the state of the art by simply replacing the weighted averaging attentional pooling with the proposed attention mechanism. In addition, the description on how to perform attentional pooling over images is not clearly described (it’s not clear how the 1D sequence based recurrent attention method can be extended to 2 D cases). However, this would not be a reason to reject the paper.   Finally, the paper’s presentation would need improvement. I would suggest that the authors give more intuitive explanations and rationale before going into technical details. The paper starts with Figure 1 which is not really well motivated/explained, so it could be moved to a later part. Overall, there are interesting technical contributions with positive results, but there are issues to be addressed. 
All of the reviewers find this paper to contain interesting ideas. Originally, clarity was a major issue, although a few issues remain (see the comments of reviewer 3). The reviewers believe that the paper has been substantially improved from its original form, however there is still room for improvement: more comprehensive comparisons to existing work (reviewer 1), careful ablations (reviewer 3), etc. With a little bit of polish, this paper is likely to be accepted at another venue.  I am certainly not penalizing you for anonymously sharing your code on Github, as this was specifically requested by reviewer 1. 
The paper presents an extension of recent implicit representations for view synthesis, such as NeRF. The presented formulation accepts an image set as input at test time, and can thus in principle be applied to new scenes. The idea is sound, but reviewers had concerns with the presentation and the experimental results. The work is primarily evaluated on the simplistic ShapeNet domain, which a number of reviewers found unconvincing. Concerns remain even after the authors  responses, and the AC agrees that the work can benefit from further investment before it is published.
The paper proposes a method that aims to combine the strenghts of VAEs and GANs.  The paper establishes an interesting bridge between GANs and VAEs. The experimental results are encouraging, even though only relatively small datasets were used. It is encouraging that the method results in better reconstructions then ALI, a related method.  Some reviewers think that the paper contains limited novelty compared to the wealth of recent work on this topic (e.g. ALI/BiGAN). The paper s contribution is seen as incremental; e.g. the training is very similar to InfoGAN. Also, the claims of better sample quality over ALI seem insufficiently supported by the data.
This paper studies how to improve the worst case subgroup error in overparameterized models using two simple post hoc processing techniques. All reviewers were positive about the paper, though R5 questioned the novelty of the paper which built heavily on a few previous papers (in particular, it builds heavily on Sagawa et al. 2020a,b). The AC is satisfied with the authors`  response clarifying the novelty. Given that this topic is quite timely and of interest to the ICLR community, and that this paper presented a clean investigation on it, the AC recommends acceptance.
Reviewers appreciated the care and substantial effort that went into the paper, for instance: AR3) I think it s of good value for the community to see and discuss the paper in the conference. AR4) would be quite valuable for the senior members of the community to read and be familiar with.  The main argument for rejection is the the analysis done in the paper is not typical of ICLR research.  Arguably, the paper could fall under the topic "societal considerations of representation learning including fairness, safety, privacy", but this does not apply because the subject of analysis is the conference ICLR, not representation learning.   I support this argument.  The reviewers posed a good number of questions and issues with the paper, and largely these were addressed well by the authors.  In some cases they addressed the issues properly, and others they argued their case.  For instance AR2 says  "think the ACs decision process is too simplified" and the response summed up as "our ability to do multi factor studies is limited by the size of our dataset".  An important one of these discussions is as follows: AR4)  But since the AC are not identified as biased, and the papers are anonymous, it is not clear what is the mechanism suggested by the authors of how these biases manifest themselves. Authors)  <extensive points>  .... we find the idea that anonymity does not genuinely exist to be entirely plausible. I would argue that neither party can claim to have won this argument, and I am not really sure how it can be resolved.  Fortunately, though, no evidence for gender bias in ACs was found.  In conclusion, the paper is not topical to ICLR material, and the reviewer consensus is Reject.  However, the paper is both valuable and interesting to the community, and it has seen substantial improvement through the review process and a lot of the issues defended well.    The paper should be brought to the attention of the various committees and made available somehow at the conference and acknowledged as a useful publication. 
All reviewers agreed that this work on OOD and pseudo labeling presents interesting and strong results. The authors’ rebuttal has addressed some of reviewers’ concerns. Based on the current review and discussion, there are still several major concerns towards the expensive computational cost introduced by the clustering method, the lack of discussion around how the proposed work can be incorporated into SSL methods, and the sensitivity towards the selection of K.
This paper introduces a newly collected dataset of natural language interactions between a tourist and a guide for localization and navigation.  The paper also includes baseline experiments with a reasonably novel approach.  The task is well motivated (although an open question remains due to GPS, comment by reviewer 1), but the description of the dataset and collection, approach and experiments were not ideal in the first version of the paper. Much of the information was pushed to the appendix and it was hard to follow the paper without going back and forth, and even then some points were missing. Authors rewrote parts of the paper to address these concerns, but there are still some open questions. For example, is it possible to have sub tasks, given the task is complex and may not be easy to accomplish as a whole? Or could simple LSTM be another baseline (the final review of the third reviewer)?   
The paper has major presentation issues. The rebuttal clarified some technical ones, but it is clear that the authors need to improve the reading substantially, ,so the paper is not acceptable in its current form.
This paper examined physics inspired inductive biases in neural networks, in particular Hamiltonian and Lagrangian dynamics. The work separated the benefits arising from incorporating energy conservation, the symplectic bias, the coordinate systems, and second order dynamics.  Through a set of experiments, the paper showed the most important factor for improved performance in the test domains was the second order dynamics, and not the more common explanation of energy conservation or the other factors. The increased generality of this approach was demonstrated with better predictions on Mujoco tasks that did not conserve energy.  All reviewers liked the insights provided by the paper.  They agreed that the paper clearly laid out several hypotheses and systematically tested them.  The reviewers found the experiments thoughtful and the results compelling.  The reviewers also pointed out several aspects of the document that could be improved, including additional formalism clarifications (reviewer nLbj), baseline algorithms (reviewer wu5x), and domains (reviewers 7KKB,SW9u).  The reviewers found the author s response satisfactory but were disappointed that a revised paper was not ready to read. The reviewers want the final paper to include the modifications that were promised in the author response.  All four reviewers indicated to accept this paper which contributes novel insights that simplify and generalize physics inspired neural networks. The paper is therefore accepted.
With an 8 6 6 rating all reviewers agreed that this paper is past the threshold for acceptance.  The  quality of the paper appears to have increased during the review cycle due to interactions with the reviewers. The paper addresses issues related to the quality of heterogeneous data sources. The paper does this through the framework of graph convolutional networks (GCNs). The work proposes a data quality level concept defined at each vertex in a graph based on a local variation of the vertex. The quality level is used as a regularizer constant in the objective function. Experimental work shows that this formulation is important in the context of time series prediction.  Experiments are performed on a dataset that is less prominent in the ML and ICLR community, from two commercial weather services Weather Underground and WeatherBug; however, experiments with reasonable baseline models using a "Forecasting mean absolute error (MAE)" metric seem to be well done.  The biggest weakness of this work was a lack of comparison with some more traditional time series modelling approaches. However, the authors added an auto regressive model into the baselines used for comparison. Some more details on this model would help.  I tend to agree with the author s assertion that: "there is limited work in ICLR  on data quality, but it is definitely one essential hurdle for any representation learning model to work in practice. ".  For these reasons I recommend a poster.  
I thank the authors for their submission and active participation in the discussions. This papers is borderline with reviewers WXXr and eK4b leaning towards acceptance and reviewers f6jT and FV5x leaning towards rejection. On the positive side, reviewers remarked that the paper is interesting [FV5x] and novel [FV5x,f6jT,eK4b,WXXr]. However, there all reviewers found some flaws with respect to the execution and empirical validation [FV5x], specifically around lacking baselines [FV5x,WXXr] and some ablations [f6jT,WXXr]. I side with the comment made by reviewers FV5x as well as WXXr that a comparison to stronger baselines (UCB DrAC) is warranted. Therefore, I recommend that this paper is not ready for publication at this point and that it will benefit greatly from another iteration with stronger empirical results. I want to very strongly encourage the authors to further improve their paper based on the reviewer feedback.
There is a consensus among reviewers that the paper should not be accepted. No rebuttal was provided, so the paper is rejected. 
This paper introduces a novel idea, and demonstrates its utility in several simulated domains. The key parts of the algorithm are (a) to prefer keeping and using samples in the ER buffer where the corresponding rho_t, using the current policy pi_t, are not too big or small and (b) preventing the policy from changing too quickly, so that samples in the ER buffer are more on policy.    They key weakness is not better investigating the idea of making the ER buffer more on policy, and the effect of doing so. The experiments compare to other algorithms, but do not sufficiently investigate the use of both Point 1 and Point 3. Further, the appendix contains an investigation into parameter sensitivity and gives some confidence intervals. However, the presentation of this is difficult to follow, and so it is difficult to gauge the sensitivity of Ref ER. With a more thorough experimental section, better demonstrating the results (not necessarily running more things), the paper would be much stronger.   For more context, the authors rightly mention "It is commonly believed that off policy methods (e.g. Q learning) can handle the dissimilarity between off policy and on policy outcomes. We provide ample evidence that training from highly similar policy experiences is essential to the success of off policy continuous action deep RL." Q learning can significantly suffer from changing the state sampling distribution. However, adjusting sampling in the ER buffer using rho_t does not change the state sampling distribution, and so that mismatch remains a problem. Changing the policy more slowly (Point 3) could help with this more. In general, however, these play two different roles that need to be better understood. The introduction more strongly focuses on classifying samples as more on or off policy, to solve this problem, rather than the strategy used in Point 3. So, from the current pitch, its not clear which component is solving the issues claimed with off policy updates.   Overall, this paper has some interesting results and is well written. With more clarity on the roles of the two components of Ref ER and what they mean for making the ER buffer more on policy, in terms of both action selection and state distribution, this paper would be a very useful contribution to stable control. 
The paper presents an interesting view of ResNets and the findings should be of broad interest. R1 did not update their score/review, but I am satisfied with the author response, and recommend this paper for acceptance. 
The paper proposes to learn disentangled trends and seasonal representations of time series for forecasting tasks. It shows separating the representation learning and downstream forecasting task to be a more promising paradigm than the standard end to end supervised training approach for time series forecasting.   During the post rebuttal phase, there were interactions from all the reviewers, and reviewer KrXv raised the score. The reviewers think the contrastive learning method is novel and the added experiments have strengthened the paper.  The authors are encouraged to include more standard datasets (M5) in the final version.  Based on the above reasons, I am recommending accepting this paper.
The authors develop stochastic variational approaches to learn Bayesian "structure distributions" for neural networks. While the reviewers appreciated the updates to the paper made by the authors, there will still a number of remaining concerns. There were particularly concerns about the clarity of the paper (remarking on informality of language and lack of changes in the revision with respect to comments in the original review), and the fairness of comparisons. Regarding comparisons, one reviewer comments: "I do not agree that the comparison with DARTS is fair because the authors remove the options for retraining in both DARTS and DBSN. The reason DARTS trains using one half of the data and validate on the other is that it includes a retraining phase where all data is used. Therefore fair comparison should use the same procedure as DARTS (including a retraining phrase). At the very least, to compare methods without retraining, results of DARTS with more data (e.g., 80%) for training should be reported." The authors are encouraged to continue with this work, carefully accounting for reviewer comments in future revisions.
This paper proposes a weighted balanced accuracy metric to evaluate the performance of imbalanced multiclass classification. The metric is based on a one versus all decomposition from multi class to binary, and then aggregating the metrics on the binary classification sub problems in a weighted manner. The authors hope to argue that the new metric is more flexible for evaluating classifiers in the imbalanced and importance varying setting.  The reviewers agree that the proposed framework is simple and applicable to an important problem. Somehow the novelty and significance of the work is pretty limited, as many related metrics (e.g. micro/macro averaged metrics) exist in the literature. The authors are encouraged to think about stronger reasoning on how useful the "new" metric is. The experiments are also not convincing nor complete enough to verify the benefits of the proposed metric. 
This paper studies the performance of second order algorithms on training multi layers over parameterized neural networks. The authors propose an algorithm based on the Gram Gauss Newton method, tensor based sketching techniques, and preconditioning to train such a network, whose runtime is subquadratic in the width of the neural network. While some reviewers provide some weak support, none of them are in strong support, even after the author s response. I think one of the reasons is the lack of empirical experiments. Since the main claim of this paper is an efficient second order algorithm, some experiments are necessary to back up this claim. Unfortunately, the authors did not try to add such an experiment during the rebuttal. I would suggest the authors add such experiments in the revision.
This paper aims at improving AAEs with an intervention loss. While the topic is important, the reviewers agree that    The paper has poor clarity,   The related work is not adequately put into perspective,   There are concerns with technical correctness,   Experimental evidence is lacking,  As the authors have not addressed any of these concerns, the paper can not be accepted in its current form.
Quoting a reviewer for a very nice summary:  "In this work, the authors suggest a new point of view on generalization through the lens of the distribution of the per sample gradients. The authors consider the variance and mean of the per sample gradients for each parameter of the model and define for each parameter the Gradient Signal to Noise ratio (GSNR). The GSNR of a parameter is the ratio between the mean squared of the gradient per parameter per sample (computed over the samples) and the variance of the gradient per parameter per sample (also computed over the samples). The GSNR is promising as a measure of generalization and the authors provide a nice leading order derivation of the GSNR as a proxy for the measure of the generalization gap in the model."  The majority of the reviewers vote to accept this paper. We can view the 3 as a weak signal as that reviewer stated in his review that he struggled to rate the paper because it contained a lot of math.
This paper presents a new method to predict the performance of deep neural networks. It evaluates the method on three different networks: LeNet, AlexNet, and VGG16 under two different frameworks, TensorFlow and TensorRT.  Reviewer 2 thought that the results were promising but comparison with other approaches was weak (PerfNet being the only baseline). They also asked for motivation for the selected architecture as well as raised a number of points for clarification. R2 was also concerned that the single baseline appeared to have not yet been published. The authors clarified this in their response (it was published in ACM RACS, obtaining results directly from those authors).  Reviewer 1 said that the experiments were extensive, but did not find the approach novel (“a normal application of ResNet”). They suggested NAS as a motivating application rather than stopping at predicting execution time. The authors agreed with the importance of predicting execution time in NAS.  Reviewer 3 agreed with Reviewer 1’s assessment of lacking novelty and technical contribution. They also pointed towards NAS, where many methods are already using neural networks to predict execution time. They were also disappointed by the reduced set of architectural elements considered. The authors responded to R3’s comments, but R3 was still not convinced of novelty.  This looks like a fairly straightforward rejection on the basis of not enough technical merit. The authors are encouraged to explore their approach in the context of NAS as per R1’s suggestion.
The proposal is a scheme for using implicit matrix vector products to exploit curvature information for neural net optimization, roughly based on the adaptive learning rate and momentum tricks from Martens and Grosse (2015). The paper is well written, and the proposed method seems like a reasonable thing to try.  I don t see any critical flaws in the methods. While there was a long discussion between R1 and the authors on many detailed points, most of the points R1 raises seem very minor, and authors  response to the conceptual points seems satisfactory.  In terms of novelty, the method is mostly a remixing of ideas that have already appeared in the neural net optimization literature. There is sufficient novelty to justify acceptance if there were strong experimental results, but in my opinion not enough for the conceptual contributions to stand on their own.  There is not much evidence of a real optimization improvement. The per epoch improvement over SGD is fairly small, and (as the reviewers point out) probably outweighed by the factor of 2 computational overhead, so it s likely there is no wall clock improvement. Other details of the experimental setup seem concerning; e.g., if I understand right, the SGD training curve flatlines because the SGD parameters were tuned for validation accuracy rather than training accuracy (as is reported). The only comparison to another second order method is to K FAC on an MNIST MLP, even though K FAC and other methods have been applied to much larger scale models.   I think there s a promising idea here which could make a strong paper if the theory or experiments were further developed. But I can t recommend acceptance in its current form. 
The authors propose a simple addition to adversarial training methods that improves model performance without significantly changing the complexity of training.  The initial reviews raised some questions about whether experiments were sufficiently extensive, but these issues were resolved during the rebuttal and discussion period, resulting in a strong consensus that the paper should be published.
This paper explores the idea of using meta learning for acquisition functions. It is an interesting and novel research direction with promising results.   The paper could be strengthened by adding more insights about the new acquisition function and performing more comparisons e.g. to Chen et al. 2017. But in any case, the current form of the paper should already be of high interest to the community 
Two reviewers are negative on this paper while the other one is slightly positive. Overall, the paper does not make the bar of ICLR and thus a reject is recommended.
This paper builds on a promising line of literature developing connections between Gaussian processes and deep neural networks.  Viewing one model under the lens of (the infinite limit of) another can lead to neat new insights and algorithms.  In this case the authors develop a connection between convolutional networks and Gaussian processes with a particular kind of kernel.  The reviews were quite mixed with one champion and two just below borderline.  The reviewers all believed the paper had contributions which would be interesting to the community (such as R1: "the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own" and R2: "I really like the idea of authors that kernels based on convolutional networks might be more practical compared to the ones based on fully connected networks").  All the reviewers found the contribution of the covariance function to be novel and exciting.  Some cited weaknesses of the paper were that the authors didn t analyze the uncertainty from the model (arguably the reasoning for adopting a Bayesian treatment), novelty in appealing to the central limit theorem to arrive at the connection, and scalability of the model.  In the review process it also became apparent that there was another paper with a substantially similar contribution.  The decision for this paper was calibrated accordingly with that work.  Weighing the strengths and weaknesses of the paper and taking into account a reviewer willing to champion the work it seems there is enough novel contribution and interest in the work to justify acceptance.  The authors provided responses to the reviewer concerns including calibration plots and timing experiments in the discussion period and it would be appreciated if these can be incorporated into the camera ready version.
This submission got 1 reject and 3 marginally below the threshold. The concerns in the original reviews include (1) lack of theoretical justification. The motivation and claim are from empirical observation; (2) the performance improvement is minor compared with the existing methods; (3) some experiment settings and details are not explained clearly. Though the authors provide some additional experiments to the questions about the experiments, reviewers still keep their ratings. The rebuttal did not address their questions. AC has read the paper and all the reviews/discussions. AC has the same recommendation as the reviewers. The major concerns are (1) the theoretical justification is not clear. The additional explanation given by the authors in their rebuttal, i.e., the prediction becomes sharper and thus the model generalization ability can be improved, is not justified. (2) the experiments are not very convincing and can be further improved in the following two aspects: (1) the motivation experiments should be conducted in a consistent manner, instead of using simplified EL in some cases; (2) the effectiveness of EL should be more significant otherwise it is not clear whether the claim is true or not. At the current status of this submission, AC cannot recommend acceptance for the submission.
The paper tackles the key question of achieving high prediction performances with few labels. The proposed approach builds upon Contrastive Predictive Coding (van den Oord et al. 2018). The contribution lies in i) refining CPC along several axes including model capacity, directional predictions, patch based augmentation; ii) showing that the refined representation learned by the called CPC.v2 supports an efficient classification in a few label regime, and can be transferred to another dataset; iii) showing that the auxiliary losses involved in the CPC are not necessarily predictive of the eventual performance of the network.  This paper generated a hot discussion. Reviewers were not convinced that the paper contributions are sufficiently innovative to deserve being published at ICLR. Authors argued that novelty does not have to lie in equations, and that the new ideas and evidence presented are worth.   The area chair thinks that the paper raises profound questions (e.g., what auxiliary losses are most conducive to learning a good representation; how to divide the computational efforts among the preliminary phase of representation learning and the later phase of classifier learning), but given the number of options and details involved, these results may support several interpretations besides the authors .   The authors might also want to leave the claim about the generality of the CPC++ principles (e.g., regarding audio) for further work   or to bring additional evidence backing up this claim.   In conclusion, this paper contains brilliant ideas and I hope to see them published with a strengthened analysis of its components. 
This paper suggests an extension of previous implicit bias results on linear networks to a tensor formulation and arguably weakens some of the assumptions of previous works (e.g. loss going to zero is replaced with initialization assumptions). The reviewers were all positive about this work, saying it is clearly written and an original significant contribution. There were a few issues raised (e.g. the novelty of the proof techniques) and the authors responded. The reviewers did not clarify if this response satisfied these concerns, but did not change their positive scores. I will take this to indicate they still recommend acceptance.
This paper was near the borderline, but ultimately, calibrating with the acceptance criteria applied to submissions across the conference, we didn t find sufficient enthusiasm among the reviewers to accept the paper.  Two reviewers put it just above the bar for acceptance, on the strength of its results.  A third reviewer finds the results to be a small improvement over other work, and finds the definitions of class, content, and style used by the authors to be confusing.  The AC agrees with the 3rd reviewer that it is more natural to define (for the class of faces) the identity to be the content and the facial pose to the the style.    Unfortunately, acceptance to ICLR required a stronger case than the reviewers presented for this paper. The remaining concerns which swayed the AC s opinion included:   concern that this was an incremental extension of LORD   the reliance on the nature of transformations applied in the algorithm   lack of any enthusiastic reviewer championing acceptance for the paper.  
The paper presents an asymptotic analysis of the convergence of the last iterate of mSGD and Adagrad. This result extends previous work providing stronger results under weaker assumptions. Even if these topics received less attention from the community, they are key problems in stochastic optimization.  The reviewers and I had several doubts about the proofs and relation with previous work. However, the rebuttal phase essential acted as a minor revision process. In fact, the authors fixed all the issues, convincing the reviewers (and me) that the results are novel, correct, and interesting.  For the above reasons, I recommend the acceptance of this paper.
This work formulates the Adaptive Mesh Refinement (AMR) problem used in solving Finite Element Method (FEM) as an MDP, and suggests an RL based solution for it. Most reviewers agree that this is a novel problem and the solution is promising. There are, however, several issues raised by our reviewers, who have expertise ranging from ML to computational methods to solve PDEs. Some of the concerns are:    As this is not a theoretical work, the burden of proof is on the empirical evaluations. Some reviewers found the experiments very small and not convincing enough.   The paper does not compare with the state of the art AMR methods.   The detail of how the problem is formulated as an MDP can be improved.  Given that four out of five reviewers are on the negative side, unfortunately I cannot recommend acceptance of this paper in its current form. Nevertheless, I believe this is a promising application of RL. I d like to encourage the authors to consider the reviews in order to improve their work, and resubmit it to another venue.
This paper focuses on avoiding overfitting in the presence of noisy labels. The authors develop a two phase method called pre stopping based on a combination of early stopping and a maximal safe set. The reviewers raised some concern about illustrating maximal safe set for all data sets and suggest comparisons with more baselines. The reviewers also indicated that the paper is missing key relevant publications. In the response the authors have done a rather through job of addressing the reviewers comments. I thank them for this. However, given the limited time some of the reviewers comments regarding adding new baselines could not be addressed. As a result I can not recommend acceptance because I think this is key to making a proper assessment. That said, I think this is an interesting with good potential if it can outperform other baselines and would recommend that the authors revise and resubmit in a future venue.
This paper focuses on investigating the relations between the heterophily and over smoothness problem. However, the relationship is not clear.  The over smoothness problem considers the features and the adjacency matrix, while the heterophily incorporates the adjacency matrix and the labels. They have different views on the graph. It may not be treated as the same coin. Besides, the stacked aggregations lead to indistinguishable node representations and poor performance in the over smoothing problem. The same phenomenon appears in the heterophily problem because the features in different classes are falsely mixed, leading to indistinguishable nodes [2]. They have the same phenomenon but different origins. It may be not a necessity to combine these two problems.  Besides, MADGap[1] is proposed to evaluate the over smoothness problem. It is unreliable to use the accuracy and the degree to measure this problem. Therefore, in section 3, the relations between node degrees and the homophily ratio cannot infer the relations between the heterophily and over smoothness problem.  As a result, the authors should carefully re organize their paper and results.   A suggestion is to pack the submission as a new method to learn from heterophily instead of trying to make such a close relationship with over smoothing.    [1] Measuring and Relieving the Over smoothing Problem for Graph Neural Networks from the Topological View. AAAI 2020   [2] Beyond homophily in graph neural networks: Current limitations and effective designs. NeurIPS 2020
Though the approach is not terribly novel, it is quite effective (as confirmed on a wide range of evaluation tasks). The approach is simple and likely to be useful in applications. The paper is well written.  + simple and efficient + high quality evaluation + strong results   novelty is somewhat limited 
This paper proposes a testing procedure to determine whether a policy is better than another policy with respect to long term treatment effects. The reviewers found the problem interesting and saw a lot of value in this work. One of the key concerns was the lack of clarity throughout the paper. The reviews helped the authors actively revise the paper, improving the paper s overall readability throughout the discussion phase. However, the reviewers did not change their ratings. While I agree that this work has merits, since there are many legibility issues, I cannot recommend its acceptance at this stage.   
The paper defines a "local data matrix" (inspired from local Fisher matrix) and uses it to obtain a foliation in the data space. This provides a lens to view the data space from model s perspective. While the idea is interesting, reviewers have two main concerns from the reviewers which are not fully addressed in the author response:  (i) The method works with partially trained model (1 epoch for MNIST) and it s not clear how the observations made in the paper extend to fully trained models,  (ii) The motivation and application of the proposed model centric view of data space needs more work   it will be good to think of some applications where this view can help.   I encourage the authors to consider the suggestions from the reviewers (e.g, R3 suggested label smoothing for (i)), and submit a revised version to a future venue. 
This paper proposes a spanning tree based graph generation framework for molecular graph generation, which is an interesting problem. The tree based approach is efficient and relatively effective in molecular graph generation tasks, and the empirical results are convincing. There were some concerns during the initial reviews, but all of them have been addressed during the discussion phase. Thus, I recommend this work be accepted.
This submission proposes a statistically consistent saliency estimation method for visual model explainability.  Strengths:  The method is novel, interesting, and passes some recently proposed sanity checks for these methods.  Weaknesses:  The evaluation was flawed in several aspects.  The readability needed improvement.  After the author feedback period remaining issues were:  A discussion of two points is missing: (i) why are these models so sensitive to the resolution of the saliency map? How does the performance of LEG change with the resolution (e.g. does it degrade for higher resolution?)? (ii) Figure 6 suggests that SHAP performs best at identifying "pixels that are crucial for the predictions". However, the authors use Figure 7 to argue that LEG is better at identifying salient "pixels that are more likely to be relevant for the prediction". These two observations are contradictory and should be resolved.  The evaluation is still missing some key details for interpreting the results. For example, how representative are the 3 images chosen in Figure 7? Also, in section 5.1 the authors don t describe how many images are included in their sanity check analysis or how those images were chosen.  The new discussion section is not actually a discussion section but a conclusion/summary section.  Because of these issues, AC believes that the work is theoretically interesting but has not been sufficiently validated experimentally and does not give the reader sufficient insight into how it works and how it compares to other methods. Note also that the submission is also now more than 9 pages long, which requires that it be held to a higher standard of acceptance.  Reviewers largely agreed with the stated shortcomings but were divided on their significance. AC shares the recommendation to reject.
There was some support for the ideas presented, but this paper was on the borderline, and ultimately not able to be accepted for publication at ICLR.  Concerns raised included level of novelty, and clarity of the exposition to an ML audience.
We thank the authors (and reviewers) for engaging in a detailed and constructive discussion, and providing a revised version of the paper after the initial round of reviews.  Regarding quality, the work is technically correct and the amount of experiments significant. However, as highlighted by reviewers 2 and 3, some important questions remain unanswered, in particular 1) more empirical evidence to support the claim that the UMAP loss is a relevant for neural networks, and 2) more comparison with existing approaches (beyond t SNE).  Regarding clarity, the paper is overall clear and pleasant to read. However, after the revision round, all details about the proposed methods have been moved to the annex. While the initial version was criticized for the opposite reason (all experiments were in a annex), the balance may not be found yet; e.g., the equation for the UMAP loss, which is at the core of the paper, would certainly find its place in the main part of the manuscript for an ICLR paper.  The originality is the weakest aspect of the paper (besides the lack of comparison with related work). As mentioned by several reviewers, plugging the UMAP loss to a differentiable model is nowadays an idea that lacks originality. What would be important to justify that such a "straightforward" idea makes it to ICLR would be to demonstrate convincingly that it outperforms existing alternative approaches.  Finally, regarding the significance of the work, it is limited by the lack of thorough comparison with existing method. On the other hand, if the method is implemented in a fast and easy to use package, it may find its public as illustrated by the positive evaluation of Reviewer 1 from a potential user point of view.
Several approaches can be used to feed structured data to a neural network, such as convolutions or recurrent network. This paper proposes to combine both roads, by presenting molecular structures to the network using both their graph structured and a serialized representation (SMILES), that are processed by a framework combining the strenth of Graph Neural Network and the sequential transformer architecture.  The technical quality of the paper seems good, with R1 commenting on the performance relative to SOTA seq2seq based methods and R3 commenting on the benefits of using more plausible constraints. The problem of using data with complex structure is highly relevant for ICLR.  However, the novelty was deemed on the low side. As a very competitive conference, this is one of the key aspects necessary for successful ICLR papers. All reviewers agree that the novelty is too low for the current (high) bar of ICLR. 
This paper presents a tensor diagram view of the multi headed self attention (MHSA) mechanism used in Transformer architectures, and by modifying the tensor diagram, introduces a strict generalization of MHSA called the Tucker head self attention (THSA) mechanism. While there is some concern regarding the incremental nature of the proposition, the identification of where to usefully add the additional parameter that converts from MHSA to THSA was nontrivial, and the experimental results on the performance benefits across multiple tasks is convincing.
The paper addresses a video generation setting where both initial and goal state are provided as a basis for long term prediction. The authors propose two types of models, sequential and hierarchical, and obtain interesting insights into the performance of these two models. Reviewers raised concerns about evaluation metrics, empirical comparisons, and the relationship of the proposed model to prior work.  While many of the initial concerns have been addressed by the authors, reviewers remain concerned about two issues in particular. First, the proposed model is similar to previous approaches with sequential latent variable models, and it is unclear how such existing models would compare if applied in this setting. Second, there are remaining concerns on whether the model may learn degenerate solutions. I quote from the discussion here, as I am not sure this will be visible to authors [about Figure 12]: "now the two examples with two samples they show have the same door in the middle frame which makes me doubt the method learn[s] anything meaningful in terms of the agent walking through the door but just go to the middle of the screen every time."
Between a reject, an accept, and a borderline accept, this is truly a borderline paper, though I d lean slightly on the side of accepting it. The most negative review raises issues of weak baselines, along with several more minor issues. The authors rebut this reasonably well, arguing several differences from the setting used in the suggested baseline papers. It is a little hard to follow who is correct between the review and the rebuttal, though the rebuttal makes reasonably convincing arguments. Other than the baselines most of the issues raised by the negative review are more minor and can be easily fixed in a revision. The specific issues mentioned are mostly not raised in either of the more positive reviews.  The borderline (positive) review is by far the most detailed, but overall praises the paper and mostly suggests fixes in terms of better positioning the paper. Overall both the positive and borderline positive reviews make persuasive arguments as to the paper s conceptual merits, which outweigh some more minor issues.
This paper proposes a model that can learn predicates (symbolic relations) from pixels and can be trained end to end.  They show that the relations learned generate a representation that generalizes well, and provide some interpretation of the model.  Though it is reasonable to develop a model with synthetic data, the reviewers did wonder if the findings would generalize to new data from real situations.  The authors argue that a new model should be understood (using synthetic data) before it can reasonably be applied to natural data.  I hope the reviews have shown the authors which areas of the paper need further explanation, and that the use of a synthetic dataset needs to strong justification, or perhaps show some evidence that the method will probably work on real data (e.g. how it could be extended to natural images).
The authors propose an RL based approach, “Rewriting by Generating (RBG)”, to solve large scale capacitated vehicle routing problems (CVRPs): such problems are NP hard in general and are ubiquitous. The RL agent consists of a "Generator" and "Rewriter". In generation, the graph is sub divided into several regions and in each region, an RL algorithm runs to get the best (or near optimal) route. The rewriter then patches these near optimal sub solutions together using “hierarchical RL”.  The paper is generally well written.   One main concern is related to generalizability: the authors respond that their approach can work for other NP hard combinatorial optimization problems such as knapsack. The authors are encouraged to do a systematic study of several such (related) problems where their approach can work. It was also a concern that the overall approach of partitioning the input instance and rewriting the CVRP solution by merging regions and recomputing routes, is also employed by commercial OR solvers. The authors are encouraged to do a careful comparison (and perhaps melding) with such available solvers, to get a hybrid “OR + ML” improvement. It is also suggested that the authors include several different constraints from real world VRP (e.g., heterogeneous vehicle costs, costs of missed shipment, route limits, upper bounded number of vehicles etc.).  
The paper introduces a graph neural network for molecules which takes into account motif level relationships. The paper received borderline reviews, with three reviewers voting for reject, and one for accept.  After the rebuttal, the reviewers did not change their scores. Overall, it seems that the paper has some merit, with good experimental results. Nevertheless, it suffers from two issues (i) the positioning with respect to other motif based approaches is not clear enough, making the novelty hard to assess; (ii) there is a lot of room for improvement in terms of clarity. Therefore, the area chair follows the majority of the reviewers  recommendations and recommends a reject.
This paper explores the addition of feedback connections to popular CNN architectures. All three reviewers suggest rejecting the paper, pointing to limited novelty with respect to other recent publications, and unconvincing experiments. The AC agrees with the reviewers.  
The paper applies the Go Explore algorithm to text based games and shows that it is able to solve text based game with better sample efficiency and generalization than some alternatives.  The Go Explore algorithm is used to extract high reward trajectories that can be used to train a policy using a seq2seq model that maps observations to actions.  Paper received 1 weak accept and 2 weak rejects.  Initially the paper received three weak rejects, with the author response and revision convincing one reviewer to increase their score to a weak accept.  Overall, the authors liked the paper and thought that it was well written with good experiments. However, there is concern that the paper lacks technical novelty and would not be of interest to the broader ICLR community (beyond those that are interested in text based games).  Another concern reviewers expressed was that the proposed method was only compared against baselines with simple exploration strategies and that baselines with more advanced exploration strategies should be included.  The AC agrees with above concerns and encourage the authors to improve their paper based on the reviewer feedback, and to consider resubmitting to a venue that is more focused on text based games (perhaps an NLP conference).
The reviewers had a number of concerns:  not state of the art recommend analysis and comparison with [1] Temporal Shift Module writing needs to be improved appreciate the motivation for the paper, but needs more extensive experimentation.  Need larger scene related datasets.  We hope you find the reviewers  comments helpful as you revise the work.
This paper received three borderline reviews (2+ / 1 ) and one positive review.  Having read through the reviews and author responses, the AC recommends the paper to be accepted.  The method, while simple, is proven experimentally to be effectively and will add to the body of work on key point localization.   The authors are requested to add their additional baselines in the response text to the revision of their paper if it has not already been done. 
The paper proposes a weakly supervised learning algorithm, motivated by its application to histopathology. Similar to the multiple instance learning scenario, labels are provided for bags of instances. However instead of a single (binary) label per bag, the paper introduces the setting where the training algorithm is provided with the number of classes in the bag (but not which ones). Careful empirical experiments on semantic segmentation of histopathology data, as well as simulated labelling from MNIST and CIFAR demonstrate the usefulness of the method. The proposed approach is similar in spirit to works such as learning from label proportions and UU learning (both which solve classification tasks). http://www.jmlr.org/papers/volume10/quadrianto09a/quadrianto09a.pdf https://arxiv.org/abs/1808.10585  The reviews are widely spread, with a low confidence reviewer rating (1). However it seems that the high confidence reviewers are also providing higher scores and better comments. The authors addressed many of the reviewer comments, and seeked clarification for certain points, but the reviewers did not engage further during the discussion period.  This paper provides a novel weakly supervised learning setting, motivated by a real world semantic segmentation task, and provides an algorithm to learn from only the number of classes per bag, which is demonstrated to work on empirical experiments. It is a good addition to the ICLR program.
The reviewers brought up significant concerns that were not resolved by the authors  responses. The concerns are too significant for the paper to be accepted at this time.
This paper addresses an important application in genomics, i.e. the prediction of chromatin structure from nucleotide sequences.  The authors develop a novel method for converting the nucleotide sequences to a 2D structure that allows a CNN to detect interactions between distant parts of the sequence.  The reviewers found the paper innovative, interesting and convincing.  Two reviewers gave a 7 and there was one 6.  The 6, however, indicated during rather lengthy discussion that they were willing to raise their scores if their comments were addressed.  Hopefully the authors will address these comments in the camera ready version.  Overall a solid application paper with novel insights and technical innovation.  
This paper tackles an interesting problem: "How should we evaluate models when the test data contains noisy labels?". This is a particularly relevant question in the medical imaging domain where expert annotators often disagree with each other. The paper proposes a new metric "discrepancy ratio" which computes the ratio how often the model disagrees with humans to how often humans disagree with each other. The paper shows that under certain noise models for the human annotations the discrepancy ratio can exactly determine when a model is more accurate than humans, whereas commonly used baselines such as comparing with the majority vote do not have this property. Reviewers were satisfied with the author rebuttal, particularly with the clarification that the goal of the metric is to accurately determine when model performance exceeds that of human annotators, and not to better rank models. The metric should be quite useful, assuming users are cautious of the limitations described by the authors.
The paper proposes to improve generated images via a post processing update procedure guided by gradients from a robust classifier.  After the author response and discussion, all reviewers agree that the paper is below the acceptance threshold.  Reviewer concerns include limited technical novelty and missing experimental comparison to relevant baselines.
The paper addresses an important problem of selecting inputs to drive an inductive program synthesis process. This is an important problem because inductive synthesis relies on carefully chosen inputs to ensure that the chosen inputs can provide sufficient information about what the desired program is. This paper proposes an approach where instead of simply asking the user to provide a set of input/output examples to the synthesizer, the user interacts with a query network that queries the user on the output of specific inputs and these input/output examples are then fed into an existing program synthesis engine.    I think the ideas in the paper are very original and I agree with reviewer WAY8 that this paper should be accepted. I think the original version of this paper had several issues that led to the low scores from the other reviewers, but the paper improved significantly with the review process.   That said, I do think that some of the concerns of the other two reviewers are valid, and some additional steps could be taken to address them. For example, the paper follows a long tradition in ML of making assumptions that are questionable but that make the math work nicely (e.g. choosing to represent things as gaussian distributions, or assuming independence for things that are clearly not independent). We are usually ok with such shortcuts if they are properly acknowledged and the resulting method proves to work well empirically, but the original set of experiments in the paper was extremely minimal. That said, the experiments added through the rebuttal process give me more confidence that even with the mathematical shortcuts, the method still works well.
This paper exams the role of mutual information (MI) estimation in representation learning. Through experiments, they show that the large MI is not predictive of downstream performance, and the empirical success of  methods like InfoMax may be more attributed to the inductive bias in  the choice of architectures of discriminators, rather than accurate MI estimation. The work is well appreciated by the reviewers. It forms a strong contribution and may motivate subsequent works in the field.  
The paper provides a deep learning technique aimed for tabular data via a unified view of factorization machines and other DNN approaches. The reviews are overall positive when discussing the provided technique, the motivation behind it and the writing. However, there are major concerns related to the experiments. The most dominant one is that of significance, meaning the advantage of the provided method when compared to existing literature. Other claims such as unclear details or different methods of reporting might be possible to resolve via minor edits, but this concern was not resolved in the rebuttal period. Before the paper can be published in a venue such as ICLR, it should provide a clearer comparison against previous works showing exactly where it improves upon them. At its current state, it doesn’t seem to be ready.
The reviewers find the work to address an interesting and important problem but have several critical concerns about its insufficient treatment of prior work in this area,  lack of novelty in relation to the body of existing literature.
The paper proposes an algorithm for learning a transport cost function that accurately captures how two datasets are related by leveraging side information such as a subset of correctly labeled points. The reviewers believe that this is an interesting and novel idea. There were several questions and comments, which the authors adequately addressed. I recommend that the paper be accepted.
The paper proposes a very interesting decomposition of the neural tangent kernel, which promises to decouple effects of the parameters and data. The authors illustrate the effects of this decomposition by considering pruning strategies for initialization. While the approach looks promising, the current paper is somewhat premature: The only "hard"  theoretical result, Theorem 1, is a direct consequence of the decomposition.  Its consequences for  training discussed in the subsequent paragraph involve quite a few approximations, yet the effects  of these approximations remain unclear. This general, high level tone is kept when discussing the  initializations.   Finally, the N(0,1) response to Reviewer 3 worries me.
This paper proposes incorporating adversarial training on real images to improve the stability of GAN training. The key idea relies on the observation that GAN training already implicitly does a form of adversarial training on the generated images and so this work proposes adding adversarial training on real images as well. In practice, adversarial training on real images is performed using FGSM and experiments are conducted on CelebA, CiFAR10, and LSUN reporting using standard generative metrics like FID.  Initially all reviewers were in agreement that this work should not be accepted. However, in response to the discussion with the authors Reviewer 2 updated their score from weak reject to weak accept. The other reviewers recommendation remained unchanged. The core concerns of reviewers 3 and 1 is limited technical contribution and unconvincing experimental evidence. In particular, concerns were raised about the overlap with [1] from CVPR 2019. The authors argue that their work is different due to the focus on the unsupervised setting, however, this application distinction is minor and doesn’t result in any major algorithmic changes. With respect to experiments, the authors do provide performance across multiple datasets and architectures which is encouraging, however, to distinguish this work it would have been helpful to provide further study and analysis into the aspects unique to this work   such as the settings and type of adversarial attack (as mentioned by R3) and stability across GAN variants.   After considering all reviewer and author comments, the AC does not recommend this work for publication in its current form and recommends the authors consider both additional experiments and text description to clarify and solidify their contributions over prior work.  [1] Liu, X., & Hsieh, C. J. (2019). Rob gan: Generator, discriminator, and adversarial attacker. CVPR 2019. 
This paper presents a new technique for modifying neural network structure, and suggest that this structure provides improved robustness to black box attacks, as compared to standard architectures. The paper is very thorough in its experimentation, and the method is simple and quite easy to understand. It also raises some important questions about adversarial examples.   However, there are serious concerns regarding the evaluation methodology. In particular, the authors claim "black box robustness" but do not test against any query based attacks, which are known to perform better against gradient masking based adversarial defenses. Furthermore, it is not clear why one would expect adversarial examples to transfer between models representing two completely different functions (i.e. from a standard model to a random mask model). So, the gray box evaluation is much more informative and, unfortunately, random mask seems to provide little to no robustness in this setting.  Given how fundamental sound and convincing evaluation is for proposed defense methods, the submission is not ready for publication yet. In particular, the authors are urged to (a) evaluate on stronger black box attacks, and (b) compare to a baseline that is known to be non robust, (e.g. JPEG encoding or SAP), to verify that these results are actually due to black box robustness and not simply obfuscation.
Thank you for submitting you paper to ICLR. The consensus from the reviewers is that there are some interesting theoretical contributions and some promising experimental support. However, although the paper is moving in the right direction, they believe that it is not quite ready for publication.
The reviewers found found the paper well motivated and well written, they found both the theoretical contributions limited in novelty and the experiments too rudimentary to be insightful.
This paper did experimental studies on how DPSGD and SSGD converge in different tasks. Some concerns were raised regarding the clarity, some unjustified claims, baselines and etc, and partially addressed after the rebuttal and discussions. However, some critical concerns remains. The reviewers agreed that the paper would be more appealing if these concerns can be well addressed.
This paper introduces a student teacher method for learning from labels of varying quality (i.e. varying fidelity data). This is an interesting idea which shows promising results.  Some further connections to various kinds of semi supervised and multi fidelity learning would strengthen the paper, although understandably it is not easy to cover the vast literature, which also spans different scientific domains. One reviewer had a concern about some design decisions that seemed ad hoc, but at least the authors have intuitively and experimentally justified them.
The paper considers generalization analysis of SGD using stability analysis. The authors argued the use of normalized version of the loss function, and angle wise stability. However, the reviewers pointed out that both motivation and novelty of the current work are not strong enough for it to be accepted by ICLR.
This paper presents theoretical results showing the conditional generative models cannot be robust. The paper also provide counter examples and some empirical evidence showing that the theory is reflected in practice. Some reviewers doubt how much of the theory holds in reality, but still they think that this paper could be a useful for the community. After the rebuttal period, R2 increased their score and it seems that with the current score the paper can be accepted.
I recommend acceptance based on the reviews. The paper makes novel contributions to learning one hidden layer neural networks and designing new objective function with no bad local optima.   There is one point that the paper is missing. It only mentions Janzamin et al in the passing. Janzamin et al propose using score function framework for designing alternative objective function. For the case of Gaussian input that this paper considers, the score function reduces to Hermite polynomials. Lack of discussion about this connection is weird. There should be proper acknowledgement of prior work. Also missing are some of the key papers on tensor decomposition and its analysis  I think there are enough contributions in the paper for acceptance irrespective of the above aspect.  
This paper is nowhere near standards for publication anywhere.
This paper presents a method to formulate learning of causally disentangled representation as a part of the encode decoder framework. Although the reviewers agree that the paper presents some interesting ideas, they feel the paper is not ready for publication yet.  In particular, I encourage the authors to take the feedback of reviewer R2 into account, which is quite detailed and provides substantive ways of improving the work. After all, I recommend rejection.  
The paper propose a new quantization friendly network training algorithm called GQ (or DQ) net. The paper is well written, and the proposed idea is interesting. Empirical results are also good. However, the major performance improvement comes from the combination of different incremental improvements. Some of these additional steps do seem orthogonal to the proposed idea. Also, it is not clear how robust the method is to the various hyperparameters / schedules. For example, it seems that some of the suggested training options are conflicting each other. More in depth discussions and analysis on the setting of the regularization parameter and schedule for the loss term blending parameters will be useful.
The authors focus on the conditional generation of molecular conformations (i.e. 3D cartesian atom positions) from a given molecular graph. They formulate the generation via diffusion probabilistic models.  Conformations are generated by a reverse diffusion process from isotropic Gaussian noise to molecular conformations. This diffusion process is learned from data using a SE(3) invariant formulation of the diffusion process. The authors work directly with atomic positions (i.e. a point cloud) instead of interatomic distances or an intermediate bond geometry representation. Experimental evaluations show state of the art results according to COV/MAT metrics on GEOM Drugs and GEOM QM9 datasets.  Strengths:    High technical novelty: first generative model for molecular conformation generation based on a diffusion framework   Very clearly written paper.   Impressive empirical results with state of the art results on GEOM Drugs and GEOM QM9 datasets.  Weaknesses:    Most of the weaknesses reported by the reviewers seem to have been addressed in the rebuttal.  The idea of the work is highly novel. The authors propose the first generative model for molecular conformation generation based on a diffusion framework. This paper brings together recent ideas and methods (e.g. diffusion, SE(3) equivariance) to the established task of molecular conformation generation with impressive empirical results. All the reviewers agree on acceptance with high scores. I recommend the authors to look at the reviewers  comments to improve the paper for the camera ready version
This paper makes the following contributions   1) it shows that one reason behind the attributions being more interpretable for adversarial robust models is that for these models, the gradient with respect to the input is more closely aligned with the normal direction to a close decision boundary. 2) Using the previous fact, the authors devise two new attribution methods   BSM and BIG   which can be used to get more reliable explanations from even a normal (non robust) model. While the reviewers agree that the premise of this paper is interesting, some concerns remain post the rebuttal. More specifically, some reviewers opine that the AGI and BIG methods are somewhat similar, and other reviewers are not very convinced about some of the details e.g., the generalization of the orthogonality of SM to the decision boundary from the binary classification case (section 3.1) to the more general case of ReLU Net s multi class classifiers (section 3.2). Given this, we are unable to accept this paper at this time. We hope the authors find the reviewer feedback useful.
The reviewers raised a number of major concerns including the incremental novelty of the proposed (if any) and insufficient and unconvincing experimental evaluation presented. The authors did not provide any rebuttal. Hence, I cannot suggest this paper for presentation at ICLR.
This paper introduces a new type of language model, the GNN LM, which uses a graph neural network to allow a language model to reference similar contexts in the training corpus in addition to the input context.  The empirical results are good, and the model sets a new SOTA on the benchmark Wikitext 103 corpus, as well as improving over strong baselines on two other language modeling datasets (enwiki8 and Billion Word Benchmark).  The main drawback, as noted by one reviewer, is the computational expense of the method with significant slowdowns compared to the baseline.  Two reviewers voted strong accept, with a third raising several concerns.  The largest concern was the lack of comparison to prior work, especially prior retrieval based methods on two datasets.  The authors responded with an ablation study comparing their method to KNN LM and showed their proposed GNN LM performs better.  Other concerns raised by the reviewer were the paper s lack of clarity (the authors should address the reviewers questions during the next revision) and incremental technical contribution.  Another reviewer highlighted the paper s novelty, and this AC agrees it is sufficient for publication.  Overall, the method is an interesting, if expensive, extension of retrieval based language models, and the empirical results support its effectiveness.
This paper proposes a new benchmark that compares performance of deep reinforcement learning algorithms on the Atari Learning Environment to the best human players.  The paper identifies limitations of past evaluations of deep RL agents on Atari. The human baseline scores commonly used in deep RL are not the highest known human scores.  To enable learning agents to reach these high scores, the paper recommends allowing the learning agents to play without a time limit.  The time limit in Atari is not always consistent across papers, and removing the time limit requires additional software fixes due to some bugs in the game software.  These ideas form the core of the paper s proposed new benchmark (SABER). The paper also proposes a new deep RL algorithm that combines earlier ideas.   The reviews and the discussion with the authors brought out several strengths and weaknesses of the proposal.  One strength was identifying the best known human performance in these Atari games.   However, the reviewers were not convinced that this new benchmark is useful.  The reviewers raised concerns about using clipped rewards, using games that received substantially different amounts of human effort, comparing learning algorithms to human baselines instead of other learning algorithms, and also the continued use of the Atari environment. Given all these many concerns about a new benchmark, the newly proposed algorithm was not viewed as a distraction.  This paper is not ready for publication. The new benchmark proposed for deep reinforcement learning on Atari was not convincing to the reviewers.  The paper requires further refinement of the benchmark or further justification for the new benchmark.
The authors propose to use genetic algorithms to learn variational autoencoders (VAEs) with discrete latent spaces. Specifically they employ natural evolution strategies (NES) to avoid backpropagating gradients through discrete variables. Experiments show how the proposed approach is competitive with the current state of the art to train discrete VAEs.  Some concerns arose from the review and discussion phases, these included confusion around the justification and derivation of NES for VAEs in the presentation and the limitation of the experiments. Authors were responsive and provided the reviewers the needed clarifications, an updated presentation in the revised paper and additional experimental results which ultimately were successful in raising the reviewers  scores towards full acceptance.
This paper proposes a novel representation for pose authoring, and was uniformly lauded by all reviewers.  The AC concurs this paper is far above the threshold for acceptance at ICLR.
This submission proposes an RL method for learning policies that generalize better in novel visual environments. The authors propose to introduce some noise in the feature space rather than in the input space as is typically done for visual inputs. They also propose an alignment loss term to enforce invariance to the random perturbation.  Reviewers agreed that the experimental results were extensive and that the proposed method is novel and works well.  One reviewer felt that the experiments didn’t sufficiently demonstrate invariance to additional potential domain shifts. AC believes that additional experiments to probe this would indeed be interesting but that the demonstrated improvements when compared to existing image perturbation methods and existing regularization methods is sufficient experimental justification of the usefulness of the approach.  Two reviewers felt that the method should be more extensively compared to “data augmentation” methods for computer vision tasks. AC believes that the proposed method is not only a data augmentation method given that the added loss tries to enforce representation invariance to perturbations as well. As such comparisons to feature adaptation techniques to tackle domain shift would be appropriate but it is reasonable to consider this line of comparison beyond the scope of this particular work.  Ac agrees with the majority opinion that the submission should be accepted.
The reviewers have agreed this work is not ready for publication at ICLR.
This paper proposes a Role Diversity metric, meant to quantify how different roles are in a multi agent RL setting. There s actually three versions of this metric, or three aspects (the distinction is not entirely clear to this area chair).  The reviewers are generally not very enthusiastic about the paper, with scores hovering at or just below the acceptance threshold. There has been extensive discussion between reviewers and authors, but there a sense that there is confusion about the exact purpose and contribution of the paper. This is reinforced by the authors  "letter to area chair", which outlines several ways the reviewers have not gotten the message. Reading the paper, it appears to me that the root cause is that the authors are indeed not communicating clearly what the paper contributes and why. It is, after all, the authors  responsibility that the reviewers understand the work. My own impression is that the text is dense and not particularly easy to get through. Perhaps the authors are simply trying to cram too many contributions into a single conference paper? This is a classic error which leads to hard to read papers. In addition to this, there is a lingering concern about the generalizability of the proposed methods.  I think the authors need to work more on their presentation, and perhaps reconsider which parts to include in their paper and exactly which measure they want to send, before they submit to another venue.
Many concerns raised by the reviewers have been addressed by the authors, sometimes through additional experiments. The reviewers have updated their scores in response, and all now recommend acceptance.  Like Reviewer 4, I think that the relation to nested dropout (Rippel et al. 2014) needs to be acknowledged and discussed appropriately, so I encourage the authors to carefully consider the reviewers  most recent comments about this when preparing the final version of the manuscript.  I disagree somewhat with Reviewer 3 that the motivation provided for this work is insufficient; controlling the quality/speed trade off at inference time seems like a compelling application. So does progressive generation, as suggested by Reviewers 3 & 4. I appreciate that this is highly subjective, however. Perhaps a few more concrete examples of practical situations where such trade offs are useful could be mentioned in the introduction.
This work has triggered engaged discussions between authors and reviewers and also among reviewers. These conversations have highlighted the potential weaknesses of the contribution, namely that the work is a proof of concept experimentally (although arguably for ethical reasons) and that the overall theoretical contribution is not strong.  Despite the merit of this work, and given the strong expectations of ICLR, I don t feel that this work can be endorsed for publication at ICLR 2022.
This paper provides an approach for weakly supervised learning by label noise correction and OOD sample removal. Overall, all reviewers agree paper is simple and approach makes sense. The experiments are solid with results on Webvision and ImageNet Mini (there were initial concerns but rebuttal handled some of those concerns). AC agrees with reviewers and recommends acceptance. 
This paper introduced a log barrier based regularization method to reduce the dynamic range of data types in neural networks. As pointed out by the reviewers, there are many technical issues. The authors agree with the reviewers in the rebuttal, though claimed that they are fixed in the revised version of the paper.  Experimental results are not convincing. It is not clear how the proposed method is evaluated. Accuracy of MobileNet using the proposed method is quite significantly lower compared to previous works. The work needs additional results/comparisons with other highly relevant papers on fixed point training.  There are also many clarity issues that need to be fixed.
The manuscript concerns a mutual information maximization objective for dynamics model learning, with the aim of using this representation for planning / skill learning. The central claim is that this objective promotes robustness to visual distractors, compared with reconstruction based objectives. The proposed method is evaluated on DeepMind Control Suite tasks from rendered pixel observations, modified to include simple visual distractors.   Reviewers concurred that the problem under consideration is important, and (for the most part) that the presentation was clear, though one reviewer disagreed, remarking that the method is only introduced on the 5th page. A central sticking point was whether the method would reliably give rise to representations that ignore distractors and preferentially encode task information. (I would note that a very similar phenomenon to the behaviour they describe has been empirically demonstrated before in Warde Farley et al 2018, also on DM Control Suite tasks, where the most predictable/controllable elements of a scene are reliably imitated by a goal conditioned policy trained against a MI based reward). The distractors evaluated were criticized as unrealistically stochastic, that fully deterministic distractors may confound the procedure; while a revised version of the manuscript experimented with *less* random distractors, these distractors were still unpredictable at the scale of more than a few frames.  While the manuscript has improved considerably in several ways based on reviewer feedback, reviewers remain unconvinced by the empirical investigation, particularly the choice of distractors. I therefore recommend rejection at this time, while encouraging the authors to incorporate criticisms to strengthen a resubmission.
The idea studied here is fairly incremental and the empirical evaluation could be improved.
The paper extends the analysis of Telgarsky (2013) and Gunasekar et al. (2018) to the Breman proximal point algorithm and to mirror descent. Upper and lower bounds show a dependency on the condition number of the distance generating function used in the Bregman divergence.  The paper received lukewarm reviews, also because the topic does not seem to be a good match for this community. In fact, none of the algorithms analyzed seem to be commonly used as optimization algorithms for deep learning, despite of the applications mentioned by the authors.  So, I didn t take into account the concerns about the relevance of the results for deep learning people, the complains about missing references from the OMD literature, and the supposed restricted setting.  However, even ignoring the above issues, the paper seems to fall squarely on the borderline. Hence, I carefully read it.  It seems to me that the analysis heavily builds on previous work, in particular the seminal paper of Telgarsky (2013) and the Fenchel Young trick in Ji&Telgarsky (2019). The part on the Bregman divergence is novel, but technically speaking it is also straightforward for people in this sub community. For example, Lemma B.3 is very well known to any optimization person. Moreover, the curvature of the Bregman divergence is exactly the term one would expect to appear. So, the upper bound seems to be incremental compared to past work and it does not really add much to our understanding of this problem. The matching lower bound is probably the only truly interesting result. However, it still does not exclude the possibility to achieve a better margin when measuring it in a different way. Indeed, measuring the margin according to the (dual) norm appearing in the strong convexity definition of Bregman divergence is not completely justified, but rather it seems a way to make the analysis work coherently.  Overall, given the overall lukewarm reviews and my evaluation of the limited novelty of the theoretical results, I recommend rejecting this paper.
Dear authors,  While the reviewers appreciated your analysis, they all expressed concerns about the significance of the paper. Indeed, given the plethora of GAN variants, it would have been good to get stronger evidence about the advantages of the Dudley GAN. Even though I agree it is difficult to provide a clean comparison between generative models because of the lack of clear objectives, the LL on one dataset and images generated is limited. For instance, it would have been nice to show robustness results as this is a clear issue with GANs.
This paper studies the transfer of representations learned by deep neural networks across various datasets and tasks when the network is pre trained on some dataset and subsequently fine tuned on the target dataset. On the theoretical side the authors analyse two layer fully connected networks. In an extensive empirical evaluation the authors argue that an appropriately pre trained networks enable better loss landscapes (improved Lipschitzness). Understanding the transferability of representations is an important problem and the reviewers appreciated some aspects of the extensive empirical evaluation and the initial theoretical investigation. However, we feel that the manuscript needs a major revision and that there is not enough empirical evidence to support the stated conclusions. As a result, I will recommend rejecting this paper in the current form. Nevertheless, as the problem is extremely important I encourage the authors to improve the clarity and provide more convincing arguments towards the stated conclusions by addressing the issues raised during the discussion phase.
This paper is good but at a borderline. One reviewer increased the score during the discussions. However, no reviewer was in strong favor. So that this paper is still a borderline one, and it is up to the SAC to decide.
This paper proposes a method for transferring an NLP model trained on one language a new language, without using labeled data in the new language.   Reviewers were split on their recommendations, but the reviews collectively raised a number of concerns which, together, make me uncomfortable accepting the paper. Reviewers were not convinced by the value of the experimental setting described in the paper—at least in the experiments conducted here, the claim that the model is distinctively effective depend on ruling out a large class of models arbitrarily. it would likely be valuable to find a concrete task/dataset/language combination that more closely aligns with the motivations for this work, and to evaluate whether the proposed method is genuinely the most effective practical option in that setting. Further, the reviewers raise a number of points involving baseline implementations, language families, and other issues, that collectively make me doubt that the paper is fully sound in its current form.
The paper introduces a new variant (SREDA Boost) of a variance reduced method SEDRA for nonconvex strongly concave min max optimization. Given that SEDRA is already optimal in the worst case, the proposed modification is intended to improve practical performance of the method, by relaxing conditions needed at initialization and allowing larger step sizes. While the reviewers appreciated the main ideas of the paper, they shared concerns about the significance of the paper s technical contributions, which were ultimately not addressed by the authors in the rebuttal phase. 
This paper extends recent multi step dynamic programming algorithms to reinforcement learning with function approximation.  In particular, the paper extends h step optimal Bellman operators (and associated k PI and k VI algorithms) to deep reinforcement learning.  The paper describes new extensions to DQN and TRPO algorithms.  This approach is claimed to reduce the instability of model free algorithms, and the approach is tested on Atari and Mujoco domains.   The reviewers noticed several limitations of the work.  The reviewers found little theoretical contribution in this work and they were unsatisfied with the empirical contributions.  The reviewers were unconvinced of the strength and clarity of the empirical results with the Atari and Mujoco domains along with the deep learning network architectures.  The reviewers suggested that simpler domains with a simpler function approximation scheme could enable more through experiments and more conclusive results.  The claim in the abstract of addressing the instabilities was also not adequately studied in the paper.  This paper is not ready for publication.  The primary contribution of this work is the empirical evaluation, and the evaluation is not sufficiently clear for the reviewers.
This paper presents a model based posterior sampling algorithm in continuous state action spaces theoretically and empirically. The work is interesting and the authors provide numerical evaluations of the proposed method. But the reviewers find the contribution of the work limited. 
The paper presents an interesting empirical analysis showing that increasing the batch size beyond a certain point yields no decrease in time to convergence. This is an interesting finding, since it indicates that parallelisation approaches might have their limits. On the other hand, the study does not allow the practitioners to tune their hyperparamters since the optimal batch size is dependent on the model architecture and the dataset. Furthermore, as also pointed out in an anonymous comment, the batch size is VERY large compared to the size of the benchmark sets. Therefore, it would be nice to see if the observation carries over to large scale data sets, where the number of samples in the mini batch is still small compared to the total number of samples. 
The paper considers the setting of bi level optimization and proposes a quasi Newton scheme to reduce the cost of Jacobian inversion, which is the main bottleneck of bi level optimization methods. The paper proves that the proposed scheme correctly estimates the true implicit gradient. The theoretical results are supported by numerical experiments, which are encouraging and show that the proposed method is either competitive with or outperforms the Jacobian Free method recently proposed in the literature.  Even though the reviews expressed some initial concerns regarding the empirical performance of the proposed method, the authors adequately addressed those concerns and provided additional experiments. Thus, a consensus was reached that the paper should be accepted.
The paper studies the problem of satisfying group based fairness constraints in the situation where some demographics are not available in the training dataset. The paper proposes to disentangle the predictions from the demographic groups using adversarial distribution matching on a "perfect batch" generated by a clustered context set.  Pros:   The problem of satisfying statistical notions of fairness under "invisible demographics" is a new and well motivated problem.   Creative use of recent works such as DeepSets and GANs applied to the fairness problem.   Cons:   Makes a strong assumption that the clustering of the context set will result in a partitioning that has information about the demographics. This requires at the very least a well behaved embedding of the data w.r.t. the demographic groups, and a well tuned clustering algorithm (where optimal tuning is difficult in practice on unsupervised problems)   but at any rate, as presented, the requirements for a "perfect batch" is neither clear nor formalized.   Lack of theoretical guarantees.   Various concerns in the experimental results (i.e. proposed method does not clearly outperform other baselines, high variance in experimental results, and other clarifications).  Overall, the reviewers agreed the studied problem is new, interesting and relevant to algorithmic fairness; however, there were numerous concerns (see above) which were key reasons for rejection. 
This was a borderline paper, but in the end two of the reviewers remain unconvinced by this paper in its current form, and the last reviewer is not willing to argue for acceptance. The first reviewer s comments were taken seriously in making a decision on this paper. As such, it is my suggestion that the authors revise the paper in its current form, and resubmit, addressing some of the first reviewers comments, such as discussion of utility of the methodology, and to improve the exposition such that less knowledgable reviewers understand the material presented better. The comments that the first reviewer makes about lack of motivation for parts of the presented methodology is reflected in the other reviewers comments, and I m convinced that the authors can address this issue and make this a really awesome submission at a future conference.  On a different note, I think the authors should be congratulated on making their results reproducible. That is definitely something the field needs to see more of.
The paper proposes an interesting idea (using "reliable" samples to guide the learning of "less reliable" samples). The experimental results and detailed analysis show clear improvement in object detection, especially small objects.  On the weak side, the paper seems to focus quite heavily on the object detection problem, and how to divide the data into reliable/less reliable samples is domain specific (it makes sense for object detection tasks, but it s unclear how to do this for general scenarios). As the authors promise, it will make more sense to change the title to "Feature Intertwiner for Object Detection" to alleviate such criticisms.   Given this said, I think this paper is over the acceptance threshold and would be of interest to many researchers. 
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The paper tackles an interesting and relevant problem for ICLR: guided image modification of images (in this case of facial attributes).   The proposed method is in general well explained (although some details are lacking)   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The training set of faces and associated attributes were annotated using a pre trained model which introduced a bias into the annotations used for training the method.   The experimental results weren t convincing. The qualitative results showed no clear advantage of the proposed method and the quantitative comparison to StarGAN only considered two attribute manipulations and only found a statistically significant different in performance for one of those. The second weakness was the key determining factor in the AC s final recommendation.   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  There were no major points of contention and no author feedback.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
This manuscript proposes an extension of semi supervised learning to the federated setting. The contributions include a thorough evaluation of performance and some method extensions.   There are four reviewers. One reviewer points out a name leakage issue in the code that was missed and suggests deks rejection. The area chair has chosen not to desk reject the paper. Three other reviews agree that the manuscript addresses an interesting and timely issue   indeed, label acquisition is a significant issue in federated learning. Three reviewers agree to reject the paper   raising concerns about novelty compared to existing methods, some details of the evaluation, and some lack of clarity. The authors provide a good rebuttal addressing many of these issues. However, the reviewers are unconvinced that the method is sufficiently novel after reviews and discussion. Authors are encouraged to address the highlighted concerns for future submission of this work.
The paper presents a method for future trajectory generation. The main contribution is in proposing a technique for data augmentation in the latent space which encourages prediction of trajectories that are both plausible, but also different from the training set. The results clearly show superior performance on standard benchmarks. The evaluation is thorough and ablations show that the proposed innovation matters.   R2, R3, R4 recommend that the paper be accepted with scores 6, 8, and 6 respectively. R1 recommends the paper be rejected with a score of 5. The main concern of reviewers are:   R1: " In summary, the paper suffers from lack of a clear justification of the proposed contributions, unfair evaluations, and questionable significance of the results." The authors addressed this concern in their rebuttal.    R2: "Some other points remain still open such as the limited focus on Trajectron in evaluations." Since trajectron is a recent SOTA, I think this is not a big concern. Authors compare against other baseline methods too.   R4: Comparison to Mercat, Jean, et al., ICRA 2020 is missing. The authors mention that their code is unavailable and therefore cannot compare.    R4: "underlying reasons for the success of different components (classification of latent intent and hallucinative latent intent) are hard to explain". I agree with this and this is also my major concern which I detail below.   The paper proposes to find diverse trajectories by generating two latent vectors: z, z . The first h time steps are generated by latent vector z and the remainder using z . The generated trajectory is evaluated by a discriminator that ensures plausibility. The latent vectors are chosen to be discrete and a classifier is trained to recognize z from ground truth trajectories. To encourage diverse trajectories, authors use a loss that encourages mis classification of the latent variable inferred from the generated trajectory. Since the generated trajectory cannot be classified well, it is assumed to be different from the training set.   This formulation is rather adhoc. If the trajectory is indeed different from the training distribution, then it will also fool the discriminator. If it doesnot, then it s not very different. The mis classification, is akin to encouraging high entropy in the z space inferred from predicted trajectories. With this view, it is possible that there is no need to generate two latent vectors z, z , but simply generate one and use the entropy penalty. I would love to see this experiment and see the authors demystify their method. It would also lead to significant changes in writing. Even now, writing needs improvement. Due to the proposed method being a adhoc trick, that is not well justified, I would normally not recommend acceptance. However, the empirical results are strong, tilting the recommendation to acceptance.
In this paper, the authors study "team zero sum games", where two teams are facing each other with opposite objective.   The main result is that the complexity of finding equilibrium is CLS, hence probably not polynomial. This result is obtained via a reduction to some congestion games.  Three reviewers gave a mild positive score (6) while the fourth one had more concerns. I tend to agree with the first three reviewers, with a  a personal opinion around 5 6. The paper is interesting, but could benefit from polishing here and there (I acknowledge that the related work section is more precise after discussion).   This said, I also kind of agree with the last reviewer in the sense that the result of this paper is a bit narrow (also not really surprising, but we cannot always have breathtaking results), and I am also not sure that most of the ICLR community will be interested by this kind of result. This is not really a criticism, but this paper is really borderline, and this is what makes it fall into the rejection pile.  For instance, I think this paper would be more suited to some other conferences, more concerned about games and computations for instance (or even a journal).
The paper presents quite a simple idea to transfer a policy between domains by conditioning the orginal learned policy on the physical parameter used in dynamics randomization.  CMA ES then finds the best parameters in the target domain. Importantly, it is shown to work well,  for examples where the dynamics randomization parameters do not span the parameters that are actually changed, i.e., as is likely common in reality gap problems.  A weakness is the size of the contribution beyond UPOSI (Yu et al. 2017), the closest work. The authors now explicitly benchmark against this, with (generally) positive results. AC: It would be ideal to see that the method does truly help span the reality gap, by seeing working sim2real transfer.  Overall, the reviewers and AC are in agreement that this is a good idea that is likely to have impact. Its fundamental simplicity means that it can also readily be used as a benchmark in future sim2real work. The AC recommend it be considered for oral presentation based on its simplicity, the importance of the sim2real problem, and particularly if it can be demonstrated to work well on actual sim2real transfer tasks (not yet shown in the current results). 
This work addresses the problem of learning representations from noisy expert demonstrations in in adversarial imitation learning. The authors build on top of GAIL, which utilizes a discriminator to model a "pseudo" reward from demonstrations. In this work, the discriminator is replaced with an auto encoder. The authors hypothesis is that using an auto encoder helps in 2 ways: 1) denoising expert trajectories for more "robust" learning; 2) using the reconstruction error (instead of binary classification loss) to distinguis experts from samples provides more informative signal for reward learning.   **Strengths** on a global perspective this work is well motivated a novel algorithmic variant of GAIL is proposed  thorough experimental evaluation  **weaknesses** The manuscript doesn t clearly distinguish between adversarial imitation learning algorithms (like GAIL) and "true" inverse reinforcement learning algorithms. This makes it unclear what the real goal of the proposed method is. The ultimate goal of adversarial IL is to learn a policy (by inferring a pseudo reward at "train" time which is then never used again), while the primary goal of IRL is to learn a reward function at train time, which can then be used at test time. The manuscript motivates the algorithm by saying it will have a more informative signal for learning reward functions, but the algorithm itself is an adversarial IL algorithm which primary goal is to learn a policy from demonstrations. Overall, makes the evaluation and analysis confusing. Ideally, the authors would have focussed on the question "Does the reconstruction error lead to better policies?" (through better pseudo reward modeling)   or would have extended an IRL method.  Second, the motivation is that the autoencoder helps with more "robust" learning, but it s unclear to me that the evaluation really shows that learning is more robust (also because "robustness" is not clearly defined)  The experimental evaluation is a bit of a mixed bag, and it s a unclear why the new algorithm performs better on non noisy data (when compared to baselines), but not less so on the noisy data.  **Summary** Overall, this work provides a promising direction, however in it s current form the manuscript is not yet ready for publication.
This paper provide an explanation why contrastive learning methods like SimSiam avoid collapse without negative samples. As the authors claimed, this is indeed a timely work for understanding the recent success in self supervised learning (SSL). The key idea in this submission is to decomposes the gradient into a center vector and residual vector which respectively correspond to de centering and de correlation. Such an explanation is interesting and novel. The empirical results are solid and convincing. During the rebuttal stage, the concerns from the reviewers are well resolved, and the writing of the new version is significantly better than the original one.
A lot of work has appeared recently on recurrent state space models. So although this paper is in general considered favorable by the reviewers it is unclear exactly how the paper places itself in that (crowded) space. So rejection with a strong encouragement to update and resubmission is encouraged. 
Both R1 and R2 suggested that Conceptors (Jaeger, 2014) had previously explored learning transformations in the context of reservoir computing. The authors acknowledged this in their response and added a reference. The main concern raised by the reviewers was lack of novelty and weak experiments (both the MNIST and depth maps were small and artificial). The authors acknowledged that it was mainly a proof of concept type of work. R1 and R2 also rejected the claim of biological plausibility (and this was also acknowledged by the authors). Though the authors have taken great care to respond in detail to each of the reviewers, I agree with the consensus that the paper does not meet the acceptance bar.
AR1 is is concerned that the only contribution of this work is  combining second order pooling with with a codebook style assignments. After discussions, AR1 still maintains that that the proposed factorization is a marginal contribution. AR2 feels that the proposed paper is highly related to numerous current works (e.g. mostly a mixture of existing contributions) and that evaluations have not been improved. AR3 also points that this paper lacks important comparisons for fairly evaluating the effectiveness of the proposed formulation and it lacks detailed description and discussion for the methods.  AC has also pointed several works to the authors which are highly related (but by no means this is not an exhaustive list and authors need to explore google scholar to retrieve more relevant papers than the listed ones):  [1] MoNet: Moments Embedding Network by Gou et al. (e.g. Stanford Cars via Tensor Sketching: 90.8 vs. 90.4 in this submission, Airplane: 88.1 vs. 87.3% in this submission, 85.7 vs. 84.3% in this submission) [2] Second order Democratic Aggregation by Lin et al. (e.g. Stanford Cars: 90.8 vs. 90.4 in this submission) [3] Statistically motivated Second order Pooling by Yu et al (CUB: 85%) [4] DeepKSPD: Learning Kernel matrix based SPD Representation for Fine grained Image Recognition by Engin et al. [5] Global Gated Mixture of Second order Pooling for Improving Deep Convolutional Neural Networks by Q. Wang et al. (512D representations) [6] Low rank Bilinear Pooling for Fine Grained Classification  by S. Kong et al. (CVPR I believe). They get some reduction of size of 10x less than tensor sketch, higher results than here by some >2% (CUB), and all this obtained in somewhat more sophisticated way.  The authors brushed under the carpet some comparisons. Some methods above are simply better performing even if cited, e.g. MoNet [1] uses sketching and seems a better performer on several datasets, see [2] that uses sketching (Section 4.4), see [5] which also generates compact representation (8K). [4] may be not compact but the whole point is to compare compact methods with non compact second order ones too (e.g. small performance loss for compact methods is OK but big loss warrants a question whether they are still useful). Approach [6] seems to also obtain better results on some sets (common testbed comparisons are essentially encouraged).   At this point, AC will also point authors to sparse coding methods on matrices (bilinear) and tensors (higher order) from years 2013 2018 (TPAMI, CVPR, ECCV, ICCV, etc.). These all methods can produce compact representations (512 to 10K or so) of bilinear or higher order descriptors for classification. This manuscript fails to mention this family of methods.  For a paper to be improved for the future, the authors should consider the following:   make a thorough comparison with existing second order/bilinear methods in the common testbed (most of the codes are out there on line)   the authors should vary the size of representation (from 512 to 8K or more) and plot this against accuracy   the authors should provide theoretical discussion and guarantees on the quality of their low rank approximations (e.g. sketching has clear bounds on its approximation quality, rates, computational cost). The authors should provide some bounds on the loss of information in the proposed method.   authors should discuss the theoretical complexity of proposed method (and other methods in the literature)  Additionally, the authors should improve their references and the story line. Citing  (Lin et al. (2015)) in Eq. 1 and 2 as if they are the father of bilinear pooling is misleading. Citing (Gao et al. (2016)) in the context of polynomial kernel approximation in Eq. 3 to obtain bilinear pooling should be preceded with earlier works that expand polynomial kernel to obtain bilieanr pooling. AC can think of at least two papers from 2012/2013 which do derive bilinear pooling and could be cited here instead. AC encourages the authors to revise their references and story behind bilinear pooling to give unsuspected readers a full/honest story of bilinear representations and compact methods (whether they are branded as compact or just use sketching etc., whether they use dictionaries or low rank representations).  In conclusion, it feels this manuscript is not ready for publication with ICLR and requires a major revision. However, there is some merit in the proposed direction and authors are encouraged to explore further.
There was some interest in the ideas presented, but this paper was on the borderline and ultimately not able to be accepted for publication at ICLR.  The primary reviewer concern was about the level of novelty and significance of the contribution. This was not sufficiently demonstrated.
This paper studies two layer graph convolutional networks and two layer multi layer perceptions and develops quantitative results of their effect in signal processing settings. The paper received 3 reviews by experts working in this area. R1 recommends Weak Accept, indicating that the paper provides some useful insight (e.g. into when graph neural networks are or are not appropriate for particular problems) and poses some specific technical questions. In follow up discussions after the author response, R1 and authors agree that there are some over claims in the paper but that these could be addressed with some toning down of claims and additional discussion. R2 recommends Weak Accept but raises several concerns about the technical contribution of the paper, indicating that some of the conclusions were already known or are unsurprising. R2 concludes "I vote for weak accept, but I am fine if it is rejected." R3 recommends Reject, also questioning the significance of the technical contribution and whether some of the conclusions are well supported by experiments, as well as some minor concerns about clarity of writing. In their thoughtful responses, authors acknowledge these concerns.  Given the split decision, the AC also read the paper. While it is clear it has significant merit, the concerns about significance of the contribution and support for conclusions (as acknowledged by authors) are important, and the AC feels a revision of the paper and another round of peer review is really needed to flesh these issues out. 
The goal in this submission is to find interpretable samples discriminating two probability distributions. In order to tackle this task the authors propose to use a sliced variant of the Bures distance (where the slicing is implemented via a one rank tensor) and the associated witness function, and illustrate the idea in the discrimination task of fake and real images, and in the detection of covariate shift.   Interpretable discrimination of probability measures with witness functions is a hot topic of machine learning with a large number of applications and available tools (including linear time ones in the sample size, and methods capable of handling independence testing, goodness of fit testing, relative tests among others, beyond the considered two sample setting).   1)The motivation of the paper and the efficiency of the proposed method compared to available baselines are not clear; the relevance of the demos is questionable.  2)Unfortunately, the submission also lacks mathematical contributions: for instance  i)Is the proposed divergence a semi metric or metric, and under what conditions on the domain and the kernel? ii)Does the proposed estimator converge, under what assumptions, and how quickly (rates)?  The contribution represents a potentially interesting idea, but significantly more work is needed before publication.
This paper investigates the tasks used to pretrain language models. The paper proposes not using a generative tasks ( filling in  masked tokens), but instead a discriminative tasked (recognising corrupted tokens). The authors empirically show that the proposed method leads to improved performance, especially in the "limited compute" regime.   Initially, the reviewers had quite split opinions on the paper, but after the rebuttal and discussion phases all reviewers agreed on an "accept" recommendation. I am happy to agree with this recommendation based on the following observations:   The authors provide strong empirical results including relevant ablations. Reviews initially suggested a limitation to classification tasks and a lack of empirical analysis, but those issues have been addressed in the updated version.    The problem of pre training language model is relevant for the ML and NLP communities, and it should be especially relevant for ICLR. The resulting method significantly outperforms existing methods, especially in the low compute regime.    The idea is quite simple, but at the same time it seems to be a quite novel idea. 
There was discussion of this paper, and the accept reviewer was not willing to argue for acceptance of this paper, while the reject reviewers, specifically pointing to the clarity of the work, argued for rejection. There appear to be many good ideas related to wavelets, and hopefully the authors can work on polishing the paper and resubmitting.
The authors did a nice job of responding to the concerns of reviewers during the discussion phase which increased reviewer scores. Because of this I will vote to accept.   The authors should carefully edit the paper for typos, grammatical errors, and style errors. Some examples:   Abstract: Make this one paragraph without a line break   End of 1st paragraph in Intro: "So there is an urge"  > "So there is an urgent"   Start of 3rd paragraph in Intro: "State of the art cryptographic"  > "The state of the art cryptographic"   Last paragraph of 2.1: "To solve above"  > "To solve the above"   End of 2.3: "Compared to the light weight InstaHide and TextHide, MPC and HE are of advantages in the security guarantees so far."  > "Compared to the light weight methods InstaHide and TextHide, MPC and HE provide much stronger security guarantees."  I also urge the authors to please double check the reviewer comments when preparing a newer version to ensure all concerns are taken into account.
The paper got generally positive scores of 6,7,7. The reviewers found the paper to be novel but hard to understand. The AC feels the paper should be accepted but the authors should revise their paper to take into account the comments from the reviewers to improve clarity.
The paper presents a continuous framework for GNNs based on neural diffusion PDE and is an evolution of a previous method (GRAND). The main novelty appears to be the additional source term, which the author show to be beneficial in reducing the oversmoothing effect typical in deep GNNs. While novelty is somewhat limited, the paper provided detailed theoretical and experimental assessment of the idea. Overall, the reviewers liked the approach and expressed some questions/concerns that were satisfactorily addressed in the rebuttal. We recommend acceptance.
Reviewers are in agreement that the paper is below the acceptance threshold. Main concerns focus around novelty, experiments, and justification of the paper s main claims.
Reviewers were in agreement but borderline.  The paper has a nice hypothesis and develops the work using two realistic datasets, Wikipedia and Code.  One reviewer was initially more negative but changed their views based on the authors improvements to the paper. The idea is fairly simple, but does require modellers come up with the structural features.  There was discussion that more down stream tasks are needed to highlight the approach.  Moreover, more datasets should be experimented with.  In all, experiments are good but improvement is easily done.
This paper proposes a new learning procedure for quantizing neural networks. Basically, DQA method proposed in this paper uses attention to obtain a linear combination of the existing network quantization techniques and uses it to pursue more efficient quantization.  Overall, it seems the submission was written in haste, so there are many typos and errors. Above all, the motivation that it can be applied to various existing techniques could not be proved experimentally at all since it only covers one somehow obsolete work. In addition, as in [1], it seems necessary to quantize not only weights but also activations, or to verify in lightweight networks such as MobileNetV2 rather than ResNet.  [1] Cluster Promoting Quantization with Bit Drop for Minimizing Network Quantization Loss, ICCV 2021
Dear authors,  The reviewers all appreciated the question you are asking and the study of the impact of each layer is definitely an interesting one.  They were however uncertain about the actual metrics you used to emphasize your points. Further, as you noted, there were quite a few presentation issues that led to skepticism of the reviewers, despite them spending quite a bit of time reading the paper and engaging in discussion.  Hence, I regret to inform you that your work is not yet ready for publication. A more focused analysis would be a great addition to the questions you raise.
This is a nice but very narrow study of domain invariance in a microscopic imaging application.  Since the problem is very general, the paper should include much more substantial context, e.g. discussion of various alternative methods (e.g. the ones cited in Sun et al. 2017).  In order to contribute to the broader ICLR community, ideally the paper would also include application to more than just the one task.
This submission proposes an approach to pre train general purpose image and text representations that can be effective on target tasks requiring embeddings for both modes. The authors propose several pre training tasks beyond masked language modelling that are more suitable for the cross modal context being addressed, and also investigate which dataset/pretraining task combinations are effective for given target tasks.  All reviewers agree that the empirical results that were achieved were impressive.  Shared points of concern were:   the novelty of the proposed pre training schemes.   the lack of insight into the results that were obtained.  These concerns were insufficiently addressed after the discussion period, particularly the limited novelty. Given the remaining concerns and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance. 
Pros   Shows alternative strategies to train low rank factored weight matrices for recurrent nets.  Cons   Minor modifications (and gains) over other forms of regularization like L2.   Results are only on an ASR task, so it’s not entirely clear how they’ll work on other tasks.  As pointed out by the reviewers, unless the authors show that the techniques generalize well to other tasks, and larger datasets it hard to accept it to the main conference. The AC, therefore, recommends that the paper be rejected. 
Four experts reviewed this paper and rated the paper below the acceptance threshold. The reviewers raised many concerns regarding the paper, mainly the lack of empirical studies and clarity. Some reviewers also suggested the authors better positioning the paper in the literature by discussing more related works. The rebuttal did not address all concerns. Considering the reviewers  concerns, we regret that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The manuscript extends the popular "RL as inference" framework with a generalized divergence minimization perspective. The authors observe that most policy optimization can be thought of as minimizing a reverse KL divergence, which has potentially undesirable mode seeking properties. The authors propose a particle based scheme wherein samples generated via Langevin dynamics are used for learning.  Several reviewers found the ideas presented interesting, and cited potential novelty and high potential for tackling an important problem. Unfortunately, all reviewers found major shortcomings, from presentation ("messy" presentation, lack of definition of notation and inconsistent use, issues around motivation and logical flow, vague and imprecise use of language, etc.). Several reviewers also had more fundamental criticisms, notably Uu6f who helpfully provided quite actionable feedback on the presentation. Unfortunately, discussion ended with the reviews: the authors offered no rebuttal or updates. The AC considers this a missed opportunity.  The AC concurs with, first and foremost, the concerns around presentation. The current state of the manuscript makes it difficult to parse apart the contribution being made, and in light of all 4 reviewers recommending rejection either strongly or weakly and with no rebuttals or responses put forth, I have no basis to recommend anything other than rejection.
The paper aims at understanding why self supervised/contrastive learning methods  transfer well when used as pretraining for fine tuning downstream tasks  (compared to e.g., supervised pretraining based on the cross entropy loss). Three reviewers recommend acceptance, whereas one reviewer recommends borderline rejection, arguing the take home message of the paper is not very clear. While this is a legitimate concern, the AC agrees with the majority that the paper does shed light on the differences between supervised and self supervised pretraining (based on interesting empirical findings) and recommends acceptance.
All reviewers agree that the contribution of this paper, a new way of training neural nets to execute Monte Carlo Tree Search, is an appealing idea.  For the most part, the reviewers found the exposition to be fairly clear, and the proposed architecture of good technical quality.  Two of the reviewers point out flaws in implementing in a single domain, 10x10 Sokoban with four boxes and four targets.  Since their training methodology uses supervised training on approximate ground truth trajectories derived from extensive plain MCTS trials, it seems unlikely that the trained DNN will be able to generalize to other geometries (beyond 10x10x4) that were not seen during training.  Sokoban also has a low branching ratio, so that these experiments do not provide any insight into how the methodology will scale at much higher branching ratios.  Pros: Good technical quality, interesting novel idea, exposition is mostly clear.  Good empirical results in one very limited domain. Cons: Single 10x10x4 Sokoban domain is too limited to derive any general conclusions.  Point for improvement: The paper compares performance of MCTSnet trials vs. plain MCTS trials based on the number of trials performed.  This is not an appropriate comparison, because the NN trials will be much more heavyweight in terms of CPU time, and there is usually a time limit to cut off MCTS trials and execute an action.  It will be much better to plot performance of MCTSnet and plain MCTS vs. CPU time used.
I would like to commend the authors on their work engaging with the reviewers and for working to improve training time. However, there is not enough support among the reviewers to accept this submission. The reviewers raised several important points about the paper, but I believe there are a few other issues not adequately highlighted in the reviews that prevent this work from being accepted:  1. [premises] It has not been adequately established that "large batch training often times leads to degradation in accuracy" inherently which is an important premise of this work. Reports from the literature can largely be explained by other things in the experimental protocol. Even the framing of this issue has become confused since, although it may be possible to achieve the same accuracy at any batch size with careful tuning, this might require using (at worst) the same number of steps as the smaller batch size in some cases and thus result in little to no speedup. For example see https://arxiv.org/abs/1705.08741 and recent work in https://arxiv.org/abs/1811.03600 for more information. Even Keskar et al. reported that data augmentation eliminated the solution quality difference between their larger batch size and their smaller batch size experiments which indicates that even if noisiness from small batches serving to regularize training other regularization techniques can serve just as well.  2. [baseline strength] The appropriate baseline is standard minibatch SGD w/momentum (or ADAM or whatever) algorithm with extremely careful tuning of *all* of the hyperparameters. None of the popular learning rate heuristics will always work and other optimization parameters need to be tuned as well. If learning rate decay is used, it should also be tuned especially if one is trying to measure a speedup. The submission does not provide a sufficiently convincing baseline.  3. [measurement protocol] The protocol for measuring a speedup is not convincing without more information on how the baselines were tuned to achieve the same accuracy in the fewest steps. Approximating the protocols in https://arxiv.org/abs/1811.03600 would be one alternative.  Additionally there are a variety of framing of issues around hyperparameter tuning, but, because they are easier to fix, they are not as salient for the decision.  
This paper provides approximation results for functions that can be represented by hybrid quantum classical circuits. It is felt that venues such as QIP would be a more suitable venue, and perhaps some experiments/simulations could be added.
This paper proposes an extension of mixup (a data augmentation method) to k mixup using optimal transport. The idea is to select randomly at each iteration  two subsets of  k samples and compute the optimal transport solution. Each pairs of samples assigned by the optimal transport plan will then be used to perform mixup and promote smoothness in the prediction function. The authors also provide some theoretical results about preservation of the clusters. Finally numeric experiment show the interest of k mixup on toy and real life dataset classification and study the effect of k and the $\alpha$ parameter (of the $\beta$ distribution).  All reviewers found the paper interesting and acknowledge that it leads to some performance improvements in practice. But they had several concerns that lead to low scores. The justification of the method an more specifically the link with the theoretical findings was found lacking, indeed the result make sens fr a large $k$ which is not was is done in practice (but experiments also show a decrease sometimes for large $k$). One interesting discussion  between the proposed approach and minibatch OT is also missing. In addition the reviewers found the numerical experiments interesting but regret that some mixup approaches have not been compared and also noted a small gap in performance for the proposed approach (with no variance reported). Also the Adversarial robustness measure is now considered weak in the literature and those results could have been made stronger with more modern adversaries. Their final concern was the fact that the method now has two parameters that needs tuning and that can have a large impact on the performance for limited gain. The authors did a detailed reply a,d edition of the paper that was very appreciated by the reviewers but that did not change their opinion that this paper still deserves some more work before being accepted.  For these reasons the AC recommend to reject the paper but strongly suggests that the authors take into account the reveiwers  comments before resubmitting to a ML venue.
This work proposes a GAN architecture that aims to align the latent representations of the generator with different interpretable degrees of freedom of the underlying data (e.g., size, pose).  Reviewers found this paper well motivated and the proposed method to be technically sound. However, they cast some doubts about the novelty of the approach, specifically with respect to DMWGAN and MADGAN. The AC shares these concerns and concludes that this paper will greatly benefit from an additional reviewing cycle that addresses the remaining concerns.  
This paper introduces an autoencoder architecture that can handle sequences of data, and attempts to automatically disentangle multiple static and dynamic factors.  Quality:  The main idea is relatively well motivated.  However the motivation for the particular technical choices made seems a little lacking, and the complexity of the proposed model put a lot of strain on the experiments.  A lot of important updates were made by the authors in the rebuttal period, however I feel the number of changes are a lot to ask the reviewers to re evaluate.  Clarity:  The English of the paper isn t great, including the title (should be "Using an ..." or "Using the ...").  The intro is clear enough, but belabors a relatively simple point about how an image model can t model factors in video.  There were some concerning parts where major issues seemed to be glossed over.  E.g. "FHVAE model uses label information to disentangle time series data, which is different setup with our FAVAE model."  As far as I understand, they both are trained from unsupervised data.   Originality:  This paper does a good job of citing related work, but seems incremental in relation to the FHVAE.  But the main problem is that the proposed method makes a lot of changes from a standard time series VAE, and the limited number of experiments means it s hard to say what the important factor in this model s performance is.  Significance:  Ultimately it s hard to say what the takeaway from this paper is.  The authors motivated and evaluated a new model, but the work wasn t done in a systematic enough way to make an strong conclusions.  What conclusion were asserted seem specious and overly general, e.g. " Since dynamic factors have the same time dependency, these models cannot disentangle dynamic factors.".  Why not?  Why can t a dynamic model learn the time scales of each of its factors automatically? 
This paper provides a new perspective on deep networks by showing that NPK is composed of base kernels and their dependence on the architecture is explicitized. It is further shown that learning the gates can perform better than random gates.  While the paper provides interesting understanding neural networks, it is unclear what practical benefit can be drawn from it. On the architectures considered such as FC, ResNet and CNN (btw, it seems restricted to 1 D), it will be important to show that such insights lead to new models or learning algorithms that improve upon the standard practice in deep learning (or get very close to).  It is debatable whether drawing such a nontrivial insight alone warrants publication at ICLR, while "nontrivial" itself is a subjective judgement.  I understand people differ in their opinions, and the NTK paper has been impactful.  Unfortunately since there are quite a few other papers that are stronger, I have to recommend not accepting this paper to ICLR this time.
The paper tries to show that many of the state of the art interpretability methods are brittle and do not provide consistent stable explanations. The authors show this by perturbing (even randomly) the inputs so that the differences are imperceptible to a human observer but the interpretability methods provide completely different explanations. Although the output class is maintained before and after the perturbation it is not clear to me or the reviewers why one shouldn t have different explanations. The difference in explanations can be attributed to the fragility of the learned models (highly non smooth decision boundaries) rather than the explanation methods. This is a critical point and has to come out more clearly in the paper.
After engaging in some good interactive discussions all but one reviewer settled on a rating of marginal accept. The most negative reviewer didn t really provide a clear enough explanation of what was lacking in the work. The other reviewers felt that the observed gains for this multi task learning framework were clear enough that the work is worthy of some attention by the community. The AC recommends acceptance, but one may consider this recommendation as a just past the line for acceptance recommendation.
Dear authors,  All reviewers liked your work. However, they also noted that the paper was hard to read, whether because of the notation or the lack of visualization.  I strongly encourage you to spend the extra effort making your work more accessible for the final version.
This paper provides empirical results for one class classification problems. The studied problem is important and the reviewers admire the challenge of this paper. However, the empirical results are not still insightful enough to provide practical recommendations.  Some of the questions raised by the reviewers could potentially be answered by the authors, but we did not receive any feedback unfortunately.  Given that there is essentially no technical novelty in this paper, it cannot be accepted for ICLR.
This work looks at ways to fill in incomplete data, through two different energy terms. Reviewers find the work interesting, however it is very poorly written and nowhere near ready for publication. This comes on top of poorly stated motivation and insufficient comparison to prior work. Authors have chosen not to answer the reviewers  comments. We recommend rejection.
The paper proposes an intriguing approach for "individual treatment effect" estimation from an observational dataset.  The approach is developed for multiple discrete actions (beyond binary treatments as typically studied in ITE literature) and discrete outcomes (a special case compared to related literature). The idea is to use the "direct method" (i.e. learn a probabilistic classifier using the observed dataset) and sample imputed outcomes for all unobserved action outcomes. Then, learn a probabilistic classifier that fits the observed+imputed dataset well, and iterate the procedure. This intriguing idea seems to converge empirically on a few different problems, and sampling the imputations rather than using deterministic imputations seems to be an important detail. Proof of convergence is however shown for deterministic imputations. The generalization error bound (Theorem 1) also does not show adequate motivation for the proposed method   even with infinite data (n >infinity), the excess risk could scale with the empirical risk of the returned model on the imputed dataset. Without an additional step proving that empirical risk on hat{D} (the imputed dataset) converges to 0 during successive iterations of the procedure, the generalization error bound is incomplete.  Consider the example of Figure 1, but where customer A has arrived to the system twice. So, the dataset contains {x1, $2, 1} and {x1, $3, 0}. When constructing the imputed dataset, the first data point would create 2 regression examples {x1, $1, ..} and {x1, $3, ..} while the second data point would create 2 regression examples {x1, $1, ..} and {x1, $2, ..}. Now, if the two {x1, $1, ..} examples have different imputation labels sampled from the model, this sets up an unrealizable learning problem and the empirical risk on hat{D} cannot be 0 for any predictor. In this toy example, we might know that we should "collapse" the two data points (e.g., de duplicate the dataset to only have unique x s with aggregated action outcomes across all observations) in the original data set and only create one set of imputed labels   but similar unrealizability can happen for x s that are "close" to each other that no model has capacity to label them differently.  The strength of the paper is its intriguing approach to ITE estimation. It is a form of an iterative S learner (vanilla S learners have been widely used in ITE estimation). The low point of the paper is this weakness in theory and analysis   it is unclear if the proposed procedure with sampling imputations (which seems to be important for empirical performance) is even a consistent algorithm. The paper would be much stronger with a more rigorous analysis of when the method will reliably work, and importantly, its limitations   such a study will help practitioners know when to use self training over direct method, targeted max likelihood, S learners, etc. 
The paper proposes to extend mirror descent to sampling with stein operator when the density is defined on a constrained domain and non euclidean geometry. All reviewers agreed on the novelty and the merits of the paper. Accept
The authors propose to extend model based/model free hybrid methods (e.g., MVE, STEVE) to stochastic environments. They use an ensemble of probabilistic models to model the environment and use a lower confidence bound of the estimate to avoid risk. They found that their proposed method yields state of the art performance over previous methods.  The valid concerns by Reviewers 1 & 4 were not addressed by the authors and although the authors responded to Reviewer 3, they did not revise the paper to address their concerns. The ideas and results in this paper are interesting, but without addressing the valid concerns raised by reviewers, I cannot recommend acceptance.
The paper presents a personalized federated learning approach using a mixture of global and local models. Four reviewers evaluated this paper; one of the reviewers is luke warm (6) while the rest of the reviewers pretty negative to this work (3, 3, 3). The reviewers pointed out many weaknesses, especially about novelty, motivation, contribution, presentation, etc. Most importantly, although the idea of a "mixture of experts" makes sense, it is not clear what the real technical contribution of this paper is in terms of federated learning.  Considering all the comments by the reviewers, I believe that this paper is not ready yet for publication. The authors need to improve the novelty and technical soundness of the proposed direction to convince the readers including reviewers. 
The paper proposes the use of GANs to match the joint distribution of features to the product of their marginals for ICA. The approach is totally plausible but reviewers have complaints about lack of rigor and analysis in terms of (i) mixing conditions under which the proposed GAN based approach will work, given that ICA is ill posed for general nonlinear mixing  (ii) comparison with prior work on linear and PNL ICA.  Further, in most scenarios where GANs are used, one of the distributions is fixed (say, the real distribution) and the other is dynamic (fake distribution) trying to come close to the fixed distribution during optimization. In the proposed method, the discriminator encodes the distance b/w joint and product of marginals which are both dynamic during the learning. It might be useful to comment whether or not it has any implications wrt increased instability of training, etc. 
Pruning is an important problem in practice. The angle of this study is also interesting. The key concept proposed by this submission is called the "utility imbalance" of the weights.  There are many concerns raised by the reviewers. Let us summarize some of them here: (1) hard to follow even for the domain experts; (2) the definition and motivation on "utility imbalance" are unclear ; (3) loss landscape visualizations are too much simplified to be informative.  There are also lots of concerns on writings. The rebuttal did help clarifying some details. However, most of the concerns still remain. We hope the detailed comments from the reviewers will be useful for the authors to polish this work. 
This paper proposes an MLP based neural network specifically designed for speech processing. The proposed Split & Glue layer is used to capture multi resolution speech characteristics. The method achieved better performance in both command recognition and speech enhancement tasks.  Two major concerns raised by the reviewers: The proposed split & glue layer is similar to convolution. Although the authors revised the paper with more clarification on the differences, the op is equivalent to frame wise convolution which has been explored in speech literature. This limits the novelty of the paper. The experimental justifications are relatively simple and limited. On the voice command and speech enhancement tasks presented in the paper, stronger and better baselines would be more convincing to justify the benefit of the proposed method. Moreover, testing on large scale ASR tasks instead of the relatively simple voice command task would be more convincing.    The decision is mainly based on the limited novelty and experimental justification.
The paper proposes a novel variational inference framework for knowledge graphs which is evaluated on link prediction benchmark sets and is competitive to previous generative approaches. While the idea is interstnig and technically correct, the originality of the contribution is limited, and the paper would be clearly improved by providing a clearer motivation for using generative models instead of standard methods and a experimental demonstration of  the benefits of using a generative instead of a discriminative model,  especially since the standard method perform slightly better in the experiments. Overall, the work is slightly under the acceptance threshold. 
This paper studies the task of learning a binary classifier from only unlabeled data. They first provide a negative result, i.e., they show it is impossible to learn an unbiased estimator from a set of unlabeled data. Then they provide an empirical risk minimization method which works when given two sets of unlabeled data, as well as the class priors.   The four submitted reviews were unanimous in their vote to accept. The results are impactful, and might make for an interesting oral presentation.
The paper presents a simple and effective convolution kernel for CNNs on spherical data (convolution by a linear combination of differential operators). The proposed method is efficient in the number of parameters and achieves strong classification and segmentation performance in several benchmarks. The paper is generally well written but the authors should clarify the details and address reviewer comments (for example, clarity/notations of equations) in the revision.  
This paper proposes a mechanism for fast sampling from the posterior over the weights of the last layer of neural network, by approximating the logits as a Gaussian through equation 8. This is based on earlier work by MacKay, but has some new empirical investigations. This is a very difficult case.  In its favor: * Despite the main ideas coming from older work by MacKay, they are interesting and relevant and worth re surfacing. * The experiments demonstrate some improvements in OOD detection over a diagonal Laplace approximation to the last layer, and is competitive in performance with a KFAC Laplace last layer approximation but much faster at test time. * The authors provided early and thoughtful responses and actively tried to have a discussion with reviewers. It is a pity that the reviewers did not participate in this discussion.   Concerns: * While interesting, it is unclear if the proposed method actually has much practical utility in its current form. The method is presented as a fast approach for uncertainty in Bayesian deep networks. But Eq. 8 requires such significant computations to form that whatever is gained by the fast sampling may not make up for the cost of forming the approximation itself. This computational burden is why the approach is only applied to a last layer. Table 2 and some of the surrounding discussion helps with alleviating these concerns and is most appreciated. But many basic questions persist: (i) do we really need many samples to achieve good performance, especially from a posterior over only a last layer? (we see the KL divergence decreases, but what about performance on interesting problem as a function of sample size?) (ii) in terms of total runtime accuracy would this be competitive with using Bayesian methods over all the parameters, even if these methods are taking fewer samples? It would be easy to try. (iii) Besides OOD detection, how does this approach generally affect accuracy or calibration? (iv) How would this method compare to a basic baseline like retraining the last layer several times and ensembling? (v) could anything be done to significantly accelerate the computations in forming Eq. 8? While not all of the answers to these questions need to be favorable to the Laplace bridge for acceptance, it would certainly improve the paper to at least address most of the questions explicitly. At the end of the paper, an online setting is mentioned, which I think would be amenable to this approach   it could be good to explore this direction. * It is disappointing that the reviewers did not communicate with the authors, despite commendable efforts from the authors. However, the paper continued to lack a clear champion. Given the persisting lukewarm reception of reviewers,  and some of the practical concerns above, it would help to have some "stand out" result, especially since the methodology, while interesting and relevant, is not new. That does raise some expectations for the experiments.  This is not an easy case. The paper has merits. And it s possible some of the concerns could be addressed by simply more clearly rationalizing design decisions (why would we use this approach in its current form over full Bayesian methods, which are now quite fast, with fewer posterior samples?).  At the same time it s clear the paper in its current form is not resonating with reviewers, and there are concerns about the practical applicability and limitations. It s on the borderline. Having some stand out results could really help this paper realize its potential.
The manuscript describes a method for improving the computational efficiency of randomized ensemble double Q learning for continuous action RL, by using a small ensemble of Q functions equipped with dropout and layer normalization, achieving matched sample efficiency at considerably less computational cost.  Reviewers praised the method s simplicity and achievement of its stated objective of reducing the computational cost of deploying ensemble Q functions. In general, the paper was found to be easy to understand and well written. Several expressed concern about the lack of interrogation of why this combination of dropout and layer norm worked so well and an overall lack of novelty. Other miscellaneous criticisms were well addressed in rebuttal and extensive new analyses in the Appendix were noted by several reviewers as adding much to the work.  In the AC s opinion, this is an example of a simple but non obvious combination of well known ideas that works very well. The review process has improved the level of empirical rigor that has gone into understanding the properties and trade offs of this method. I m happy  to recommend acceptance, though would echo reviewers concerns that dubbing the method "Dr.Q" will lead to confusion and would strongly urge adopting another name for the camera ready.
This paper proposes a new method for post training quantization, achieving very good results. After the author s response, all the reviewers were positive. There were some issues regarding clarity, and about explaining why the methods work better than just optimizing the loss, but I think the reviewers were eventually satisfied.  Following some info after the author s response phase, I ll just ask the authors to verify their published code works with publicly available PyTorch packages, so their method could be easily used.
This paper proposes a GAN based framework for image compression.  The reviewers and AC note a critical limitation on novelty of the paper i.e., such a conditional GAN framework is now standard. The authors mentioned that they apply GAN for extreme compression for the first time in the literature, but this is not enough to justify the novelty issue.  AC thinks the proposed method has potential and is interesting, but decided that the authors need new ideas to publish the work. 
Main content:  This paper provides a unified way to provide robust statistics in evaluating the reliability of RL algorithms, especially deep RL algorithms. Though the metrics are not particularly novel, the investigation should be useful to the broader community as it compares seven specific evaluation metrics, including  Dispersion across Time (DT): IQR across Time ,  Short term Risk across Time (SRT): CVaR on Differences ,  Long term Risk across Time (LRT): CVaR on Drawdown ,  Dispersion across Runs (DR): IQR across Runs ,  Risk across Runs (RR): CVaR across Runs ,  Dispersion across Fixed Policy Rollouts (DF): IQR across Rollouts  and  Risk across Fixed Policy Rollouts (RF): CVaR across Rollouts . The paper further proposed ranking and also confidence intervals based on bootstrapped samples, and compared against continuous control and discrete actions algorithms on Atari and OpenAI Gym.     Discussion:  The reviews clearly agree on accepting the paper, with a weak accept coming from a reviewer who does not know much about this subarea. Comments are mostly just directed at clarifications and completeness of description, which the authors have addressed.     Recommendation and justification:  This paper should be accepted due to its useful contributions toward doing a better job of measuring performance of RL.
This paper analyzes the extent to which parameterized layers within a CNN can be replaced by parameter free layers, with specific focus on utilizing max pooling as a building block.  After the author response and discussion, all reviewers favor accepting the paper.  The AC agrees that its empirical results open a potentially interesting discussion on network design.
The paper proposes a new method for subgraph similarity search by learning embeddings via a GNN based approach to reflect the edit distance between subgraphs. Reviewers highlighted that the paper proposes an intuitive and promising approach to an interesting problem and provides a good balance between theoretical and empirical results. However, reviewers raised also concerns regarding the significance of technical contributions, limited analysis (e.g, performance on large scale graphs, baselines, evaluation) and comparison to related work. After author response and discussion, reviewers did not come to a full agreement with two reviewers indicating weak acceptance and two reviewers indicating (weak) reject. Taking rebuttal and discussion into account, I agree with the viewpoint that the paper is not yet ready for acceptance at ICLR as it would require an additional revision to fully address the raised concerns. However, I encourage the authors to revise and resubmit their manuscript based on the feedback from this reviewing round.
This is a borderline paper.  The reviewers are happy with the simplicity of the proposed method and the fact that it can be applied after training; but are concerned by the lack of theory explaining the results.  I will recommend accepting, but I would ask the authors add the additional experiments they have promised, and would also suggest experiments on imagenet.
This paper proposes a software package to ease and provide a standard way for Hessian related computation, both for loss analysis and second order optimization. I think all reviewers agree with the usefulness of this work but differ in their assessment whether this work is ready for publication. Given the emphasize on providing a software package I share the view that careful testing and support of usability is important. While there is quite some spread in the scores I think in this case the average score is an appropriate way to compensate for subjective differences between reviewers. I think it is justified to encourage the authors to invest a bit more work to turn this into a fully convincing contribution.  
The paper proposes an interesting approach that leverages shared dynamics across causal systems for improved joint causal discovery.  The reviewers and AC all agree that the approach is interesting, promising and that the paper is well written.   While theoretical validation would be an exciting thing to have, it is perfectly acceptable for the paper to focus on an empirical study. But in this case, it is very important to provide convincing evaluation experiments.  As several reviewers have pointed out, in order to convincingly demonstrate the value of the approach, it would be very important for the experiments to go beyond noiseless systems.  We strongly encourage the authors to address this point as this will significantly strengthen the significance of their contributions.
This paper proposes a novel framework for tractably learning non eucliean embeddings that are product spaces formed by hyperbolic, spherical, and Euclidean components, providing a heterogenous mix of curvature properties.  On several datasets, these product space embeddings outperform single Euclidean or hyperbolic spaces. The reviewers unanimously recommend acceptance.
This paper provides further insight into using RL for active learning, particularly by formulating AL as an MDP and then using RL methods for that MDP. Though the paper has a few insights, it does not sufficiently place itself amongst the many other similar strategies using an MDP formulation. I recommend better highlighting what is novel in this work (e.g., more focus on the reward function, if that is key). Additionally, avoid general statements like “To this end, we formalize the annotation process as a Markov decision process”, which suggests that this is part of the contribution, but as highlighted by reviewers, has been a standard approach. 
The reviewers have not supported the acceptance of this paper where the key weakness is that the study of the proposal neglect effect is not sufficient (see the reviews for the details). I agree with the assessment of the reviewers and recommend rejecting the paper in its current form.
Three reviewers recommended rejection, and there was no rebuttal.
The authors received reviews from true experts and these experts felt the paper was not up to the standards of ICLR.   Reviewer 3 and Reviewer 1 disagree as to whether the new notion of generalization error is appropriate. I think both cases can be defended. I think the authors should aim to sharpen their argument in this regard.Several reviewers at one point remark that the results follow from standard techniques: shouldn t this be the case? I believe the actual criticism being made is that the value of these new results do not go above and beyond existing ones. There is also the matter of what value should be attributed to technical developments on their own. On this matter, the reviewers seem to agree that the derivations lean heavily on prior work. 
The paper proposes a new solution for cross domain correspondence in control, which combines GANs and cycle consistency, and separates shifts in observation space and in action space. The paper targets unpaired data / simulations, and discovers alignment of state by enforcing that domains are mappable.  The paper was received well by reviewers, who pointed out several strengths: a strong contribution on a fundamental problem, and an interesting formulation; a well written and well positioned paper; This compensates minor weaknesses, in particular the fact that transfer has been tested between two different simulated environments.   The reviewers unanimously suggested acceptance, the AC concurs.
Pros: + Clear, well written paper that tackles an interesting problem. + Interesting potential connections to other approaches in the literature such as Carreira Perpiñán and Wang, 2014 and Taylor et al., 2016. + Paper shows good understanding of the literature, has serious experiments, and does not overstate the results.  Cons:   Theory only addresses gradient descent, not stochastic gradient descent.   Because the optimization process is similar to BFGS, it would make sense to have an empirical comparison against some second order method, even though the proposed algorithm is more like standard backpropagation.  This paper is a nice first step in an interesting direction, and belongs in ICLR if there is sufficient space. 
The paper received mixed reviews. Reviewers were concerned about the clarity of the presentation, including both the analyses of gradients and the approach. Some reviewers also suggested improvements to the experiments. The two reviewers who gave a rating of 6 liked the gradient perspective to the long tailed recognition, but their concerns seemed to overshadow their excitement. AC suggested authors improving the paper, especially its clarity and empirical part, following Reviewers  comments and submitting the paper elsewhere. 
This paper explores training CNNs with labels of differing granularity, and finds that the types of information learned by the method depends intimately on the structure of the labels provided.  Thought the reviewers found value in the paper, they felt there were some issues with clarity, and didn t think the analyses were as thorough as they could be. I thank the authors for making changes to their paper in light of the reviews, and hope that they feel their paper is stronger because of the review process.
The paper proposes All SMILES VAE which can capture the chemical properties of small molecules and also optimize the structures of these molecules. The model achieves significantly performance improvement over existing methods on the Zinc250K and Tox21 datasets.   Overall it is a very solid paper   it addresses an important problem, provides detailed description of the proposed method and shows promising experiment results. The work could be a landmark piece, leading to major impacts in the field. However, given its potential,  the paper could benefit from major revisions of the draft. Below are some suggestions on improving the work: 1. The current version contains a lot of materials. It tries to strike the balance between machine learning methodology and details of the application domain. But the reality is that the lack of architecture details and some sloppy definitions of ML terms make it hard for readers to fully appreciate the methodology novelty.   2. There is still room for improvement in experiments. As suggested in the review, more datasets should be used to evaluate the proposed model. Since it is hard to provide theoretic analysis of the proposed model,  extensive experiments should be provided.   3. The complexity analysis is not fully convincing. Some fair comparison with the alternative approaches should be provided.   In summary, it is a paper with big potentials. The current version is a step away from being ready for publication. We hope the reviews can help improve the paper for a strong publication in the future. 
This paper analyzes existing approaches to program induction from I/O pairs, and demonstrates that naively generating  I/O pairs results in a non uniform sampling of salient variables, leading to poor performance. The paper convincingly shows, via strong evaluation, that uniform sampling of these variables can much result in much better models, both for explicit DSL and implicit, neural models. The reviewers feel the observation is an important one, and the paper does a good job providing sufficiently convincing evidence for it.  The reviewers and AC note the following potential weaknesses: (1) the paper does not propose a new model, but instead a different data generation strategy, somewhat limiting the novelty, (2) Salient variables that need to be uniformly sampled are still user specified, (3) there were a number of notation and clarity issues that make it difficult to understand the details of the approach, and finally, (4) there are concerns with the use of rejection sampling.  The authors provided major revisions that address the clarity issues, including an addition of new proofs, cleaner notation, and removal of unnecessary text. The authors also included additional results, such as KL divergence evaluation to show how uniform the distribution is. The authors also described the need for rejection sampling, especially for Karel dataset, and clarified why the Calculator domain, even though is not "program synthesis", still faces similar challenges. The reviewers agreed that not having a new model is not a chief concern, and that using rejection sampling is a reasonable first step, with more efficient techniques left for others for future work.  Overall, the reviewers agreed that the paper should be accepted. As reviewer 1 said it best, this paper "is a timely contribution and I think it is important for future program synthesis papers to take the results and message here to heart".
This paper presented a domain transportation perspective on optimizing recommender systems. The basic motivation is to view recommendation as applying some form of intervention, implying a distributional shift after the recommendation/intervention. Distribution shift brings tremendous difficulty to traditional causal inference or missing data theory perspective of recommender systems as it violates the distributional overlapping assumption: in simple terms, if the model recommends radically different set of items, there isn t much you can say about its generalization ability; on the other hand, if the model only recommends items that it already observed during training (no distribution shift at all), it would inherent all the biases which already exist in the data. To that end, this paper proposed a domain transportation perspective by introducing a Wasserstein distance constrained risk minimization to find interventions that can best transport the patterns it learns from the observed domain to the post intervention domain.  The paper received overall borderline scores. All the reviewers acknowledged that the proposed perspective is novel and has the potential to spark a new direction for future work. The reviewers raised concerns, ranging from the bounds in the paper, sensitivity of the optimization w.r.t. the hyperparameter, to some relevant but missing baselines. The authors provided very detailed response and revised the paper quite substantially to address most of the feedback. I also read the paper myself given the borderline scores, and I think the authors did a reasonably good job improving the paper and I agree this paper provides an interesting and novel perspective on viewing recommendation, though I also agree with one reviewer that the idea of "partially extrapolation" can be further explored.   My major complaint is around experimental evaluation. It seems to me that only the semi synthetic experiment actually makes sense in this context (where the measure is based on the unobserved relevance as opposed to observed click), as the traditional random split on clicks evaluation would inevitably favor models with little distributional shift (the training and test data essentially come from the same distribution, maybe not so with a sequential setting but still close). Furthermore, the inclusion of Yahoo R3 and Coat dataset is even more confusing, as the associated test set implies random exposure which is certainly not what this paper aims to address, unless I am missing something in which case more clarification would be nice.   My overall assessment of the paper is still leaning towards positive but I also wouldn t be too upset if this paper doesn t end up making it. However, if accepted, I do want the authors to carefully revise the presentation of the experimental results for the final version.
This paper proposed a way to combine LSTMs with Fast weights for associative inference.  While reviewers had concerns about comparison with Ba et al., and experimental results, the authors addressed all the concerns and convinced the reviewers. The revision strengthened the paper significantly. I recommend an accept.
This paper addresses data sanitization, using a KL divergence based notion of privacy. While an interesting goal, the use of average case as opposed to worst case privacy misses the point of privacy guarantees, which must protect all individuals. (Otherwise, individuals with truly anomalous private values may be the only ones who opt for the highest levels of privacy, yet this situation will itself leak some information about their private values).  
The paper received two borderline accept recommendations and one accept recommendation from three reviewers with low confidence and a reject recommendation from an expert reviewer.   Although all reviewers found that the paper addresses an important and challenging problem of semantically constraining adversarial attacks as opposed to constraining them artificially by an artificial norm ball. However, during the discussion phase it has been pointed out that there were some important weaknesses indicating that the paper may need one more evaluation round.  The meta reviewer recommends rejection based on the following observations.   In terms of evaluation, while it is understandable the authors were unable to compare to Gowal et al. due to the lack of publicly available implementation, showing Song et al. s adversarials hurt performance and and are farther than the image manifold has been found puzzling, as this was done by Song et al. only to keep human prediction the same while changing model prediction. Furthermore, the paper did not contain a user study similar to Song et al. for a fair comparison Finally, the discussion revealed that the comparison to "norm bounded adversarial inputs" may not have clarified whether this experiment faithfully demonstrates an advantage for the contribution as the norm could be contained to a point where accuracy is not reduced, and the discussion on the certified defense being "broken" was inconclusive.
All reviewers gave this paper a score of 1. The AC recommends rejection.
The reviews were a bit mixed, and there was some concern on the usefulness and actual novelty of this work. On one hand, the authors did a nice job in visualizing their findings and conducting a wealth of interesting experiments. On the other hand, the submission suffers severely from hand waving definitions and arguments. Many terms were not precisely defined, various hyperparameters were not thoroughly investigated, and yet conclusions were made based on indirect experimental results. The AC agrees with the reviewers that it is not very clear how this work would impact the field. For exploratory work like this one, there is also a great danger that one may simply overfit the observations and squeeze conclusions from thin air. It would be more convincing if the authors could largely quantify their definitions and results. For example: what do we mean by human aligned? robust / nonrobust feature? (this definition depends on the perturbation size hence needs more elaboration.) Is there any way to quantify the results in Fig 2, including the impact of epsilon? Should these adversarial examples be called universal if their ASR falls below what threshold? Are (some of) the conclusions (e.g. translation invariance, semantic) a direct consequence of the perturbation being universal? At this stage this work would be an excellent workshop paper but a bit more rigor would be needed for publishing at the ICLR conference. 
The paper proposes a method to identify informative latent variables by thresholding based on the conditional generative model. While the exposition of the paper has substantially improved during the discussion period, some major concerns remain after the discussion among the reviewers. In particular, the problem considered in the paper has a very limited scope. Moreover, the evaluation of the methods needs to be improved. The paper could benefit from discussing how it situates in the broader context.
The paper proposes a new approach to continual learning with known task boundaries that is scalable and highly performant, while preserving data privacy.  To mitigate forgetting the proposed approach restricts gradient updates to fall in the orthogonal direction to the gradient space that are important for the past tasks. The main novelty of the approach is to estimate these subspaces by analysing the activations for the inputs linked for each given task.   All reviewers give accepting scores. R2, R3 and R4 strongly recommend accepting the paper, while R1 considers it borderline.  The authors provided an extensive response carefully considering all reviewers  comments. New experiments were introduced (training time analysis and comparisons with expansion based methods), and several clarifications were added.  All reviewers agree that the paper is well written and its literature review adequate.  The main concern of R1 was the similarities with OGD (Farajtabar et al. 2020). R1 considered the authors’ response acceptable. R2, R3 and R4 consider the contribution well motivated and significant and highlight its simplicity. The AC agrees with this assessment.  The empirical evaluation covers most of the typical benchmarks in CL. Very strong results are reported on a variety of tasks both in terms of performance and memory efficiency, as agreed by R2, R3 and R4.  Overall the paper makes a strong contribution to the field of CL. 
This article studies gradient optimization for classification problems with shallow networks with smooth activations, obtaining convergence and generalisation results under a separability assumption on the data. The results are obtained under much less stringent requirements on the width of the network than other related recent works. However, with results on convergence and generalisation having been established in other previous works, the reviewers found the contribution incremental. The responses clarified some of the distinctive challenges with the logistic loss compared with the squared loss that has been considered in other works, and provided examples for the separability assumption. Overall, the article makes important contributions in the case of classification problems. However, with many recent works addressing challenging problems in a similar direction, the bar has been set quite high. As pointed out by some of the reviewers, the contribution could gain substantially in relevance and make a more convincing case by addressing extensions to non smooth activations and deep models. 
The paper considers neural importance sampling (that is, importance sampling with a trained flow proposal) and its application to high energy physics. The two contributions of the paper are: (a) a methodological improvement in the training of the proposal; (b) a description of a software library that implements the framework.  All reviewers were critical of the paper and recommended rejection. The main issue raised was that the methodological contribution was not novel or significant enough, and not sufficiently evaluated. The authors disagreed with the reviewers that the methodological contribution was not significant enough, but they acknowledged that the first version of the paper did not present the contribution clearly; consequently, they submitted a heavily revised second version following the reviewers  feedback.  Although it seems that the second version is an improvement over the first one, it s clear that the paper requires a second round of reviewing to ascertain whether it satisfies the requirements for acceptance. At this stage, the consensus among reviewers remains that the paper should be rejected. For that reason, I cannot recommend acceptance to ICLR. I sincerely hope the reviewers  feedback will be useful to the authors for a future submission to a different venue.
The authors propose a new dataset, namely ImageNet NOC, for evaluating robustness of image classifiers to corruptions. The dataset may be viewed as an alternative to ImageNet C which uses a different set of corruptions. To derive this set of corruptions, the authors first develop a notion of similarity between two corruptions, and then propose an iterative algorithm to build a set of corruptions which, intuitively, is sufficient to cover the larger set of corruptions (i.e., enjoys *high coverage*), and assigns a similar importance to each such corruption (i.e., is *balanced*). Then, the authors argue that ImageNet NOC is superior to ImageNet C as it achieves a higher degree of balance and coverage.  The reviewers found this to be a borderline paper. The reviewers appreciated the introduced metric and agree that there is no point in evaluating on corruptions which are perfectly correlated. In addition, the systematic approach for generating a set of relevant corruptions is seen as a step in the right direction. The reviewers appreciated the author response and were engaged in the discussion. As it currently stands the reviewers are not convinced that the paper is ready for acceptance. To improve the manuscript the authors could extend Tables 3 and 4 with a wider range of models and investigate qualitative differences between models robust on one dataset, but not on the other. Furthermore, there should be a more detailed discussion of stability and computational properties of algorithm 1. In addition, the authors should provide strong arguments as to why is it not sufficient to add additional corruptions to ImageNet C and compute a weighted score instead. The latter suggestion could lead to an iterative improvement of the current set of benchmarks and place more emphasis on the methodology. I suggest the authors to incorporate the reviewer s feedback and place more emphasis on the methodology around algorithm 1, rather then on introducing another dataset which is likely to be superseded as soon as we add a couple more corruptions in the mix. 
Dear authors,  All reviewers pointed out the fact that your result is about the expressivity of the big network rather than its accuracy, a result which is already known for the literature.  I encourage you to carefully read all reviews should you wish to resubmit this work to a future conference.
Strong paper on hierarchical RL with very strong reviews from people expert in this subarea that I know well. 
The paper studies the introduction of a variant of batch normalization (BN) to train deep neural network. The underlying idea is a two step approach for per sample based normalization, relying on augmenting the computational graph to handle "several samples" nodes.  The reviewers have mentioned that the idea of altering the computational graph is interesting and potentially novel. Yet, the numerical experiments were not enough precise or solid to back up the claims by the authors, that their proposed BN alternative is of practical interest. It was also raised that the paper lacks theoretical supports: no formal analysis, most explanations are ad hoc, etc.
In this work, the authors develop an improved generalization bound for stochastic optimization algorithms. Reviewers agree that the theoretial results are significant. Several reviewers had concerns about the lack of experimental validation, which the authors addressed during the discussion phase. Other more minor concerns were also adequately addressed by the authors. The final recommendation is therefore to accept.
The paper proposes to lear molecular descriptors that account for the 3D structure of molecules. This is done by using first a "Hamiltonian Engine" that runs a brief simulation, predicting the structure of the small molecule by minimizing a learned potential energy, and second, a message passing algorithm that uses the predicted structure as input. The reported experimental results show state of the art performance.  Strengths:  1   Relevant contribution through the Hamiltonian Engine.  2   Strong empirical results.  Weaknesses:  3   Some reviewers mentioned that the readability of the paper could be improved.  I recommend the authors to also take into account the concerns of AnonReviewer1 to improve the paper.
Dear authors,  Based on the comments and your rebuttal, I am glad to accept your paper at ICLR.
This submission proposes a method to explain deep vision models using saliency maps that are robust to certain input perturbations.  Strengths:  The paper is clear and well written.  The approach is interesting.  Weaknesses:  The motivation and formulation of the approach (e.g. coherence vs explanation and the use of decoys) was not convincing.  The validation needs additional experiments and comparisons to recent works.  These weaknesses were not sufficiently addressed in the discussion phase. AC agrees with the majority recommendation to reject.
The paper provides an improved analysis of the finite time convergence rate of double Q learning under more reasonable step size rules, comparing to previous work by Xiong et al., 2020.  Understanding the convergence behavior of double Q learning is an obviously interesting theoretical topic and all reviewers appreciate the authors’ improved analysis.    Several reviewers questioned the sample complexity in terms of the dependence on L (thus |S||A|); In the latest revision, the authors claimed they now refined the dependence from O(L^6) to O(L). This major change is yet to be further reviewed since the authors did not leave any clue on why/how such an improvement was attained.  Another outstanding concern relates to the theoretical comparison of the rates between double Q learning and Q learning, which remains clueless. It’s unclear whether the bound in this paper is sharp enough and whether/when double Q learning is provably inferior than Q learning.   Therefore, I am not recommending acceptance at this time, though I encourage the authors to resubmit with a more conclusive theoretical analysis.  
This paper proposes a meta learning method with a latent feature space with a special structure of orthogonality and low rankness. This paper is well written, and the use of the orthogonal low rank embedding for meta learning is interesting. The experimental results (including additional experiments in the author response) demonstrate the effectiveness of the proposed method. The author response addressed some concerns of the reviewers. However, the novelty of the proposed method is not high enough.
I appreciate that the authors are refuting a technical claim in Poole et al., however the paper has garnered zero enthusiasm the way it is written. I suggest to the authors that they rewrite the paper as a refutation of Poole et al., and name it as such.
This manuscript describes a continual learning approach where individual instances consist of sequences, such as language modeling. The paper consists of a definition of a problem setting, tasks in that problem setting, baselines (not based on existing continual learning approaches, which the authors argue is to highlight the need for such techniques, but with which the reviewers took issue), and a novel architecture.  Reviews focused on the gravity of the contribution. R1 and R2, in particular, argued that the paper is written as though the problem/benchmark definition is the main contribution. R2 mentions that in spite of this, the methods section jumps directly into the candidate architecture. As mentioned above, several reviewers also took issue with the fact that existing CL techniques are not employed as baselines. The authors engaged with reviewers and promised updates, but did not take the opportunity to update their paper.  As many of the reviewers  comments remain unaddressed and the authors  updates did not materialize, I recommend rejection, and encourage the authors to incorporate the feedback they have received in a future submission.
This paper aims to study the convergence of deep neural networks training via a control theoretic analysis. This is a very interesting approach to establish theoretical understanding of deep learning. However, there are several concerns raised by the reviewers:  1.	The contribution of this paper is limited. The results simply follow from standard optimal control. It is not clear what new insight the paper provides. 2.	There are already quite a few works on control theoretic analysis of deep learning. This paper did not do a good job on presenting its novelty and difference with existing works. 3.	The experimental part is weak. It only involves small data set and very simple networks.  Based on these, I am not able to recommend acceptance for the current manuscript. But the authors are encouraged to continue this research. 
The paper proposes a  communicate then adapt  framework for decentralized optimization, with both theoretical and empirical analysis. The reviewers  main concern is the comparison in theory with prior methods like the GT DAdam. The convergence to a stationary point of GT DAdam seems to be faster than the proposed method in the important non convex optimization. The reviewers are not convinced by the strong claim that  communicate then adapt  is better than  adapt then communicate  as such  adapt then communicate  method can also achieve same or better rates, possibly with less hyper parameter tuning. I would suggest the authors to make more proper comparison with related methods.
This paper proposes a GAN based approach to producing poisons for neural networks.  While the approach is interesting and appreciated by the reviewers, it is a legitimate and recurring criticism that the method is only demonstrated on very toy problems (MNIST and Fashion MNIST).  During the rebuttal stage, the authors added results on CIFAR, although the results on CIFAR were not convincing enough to change the reviewer scores; the SOTA in GANs is sufficient to generate realistic images of cars and trucks (even at the ImageNet scale), while the demonstrated images are sufficiently far from the natural image distribution on CIFAR 10 that it is not clear whether the method benefits from using a GAN.   It should be noted that a range of poisoning methods exist that can effectively target CIFAR, and SOTA methods (e.g., poison polytope attacks and backdoor attacks) can even target datasets like ImageNet and CelebA.
The paper is well written and easy to follow. The experiments are adequate to justify the usefulness of an identity for improving existing multi Monte Carlo sample based gradient estimators for deep generative models. The originality and significance are acceptable, as discussed below.  The proposed doubly reparameterized gradient estimators are built on an important identity shown in Equation (5). This identity appears straightforward to derive by applying both score function gradient and reparameterization gradient to the same objective function, which is expressed as an expectation. The AC suspects that this identity might have already appeared in previous publications / implementations, though not being claimed as an important contribution / being explicitly discussed. While that identity may not be claimed as the original contribution of the paper if that suspicion is true, the paper makes another useful contribution in applying that identity to the right problem: improving three distinct training algorithms for deep generative models. The doubly reparameterized versions of IWAE and reweighted wake sleep (RWS) further show how IWAE and RWS are related to each other and how they can be combined for potentially further improved performance.   The AC believes that the paper makes enough contributions by well presenting the identity in (5) and applying it to the right problems. 
This paper presents an efficient RNN architecture that dynamically switches big and little modules during inference. In the experiments, authors demonstrate that the proposed method achieves favorable speed up compared to baselines, and the contribution is orthogonal to weight pruning.  All reviewers agree that the paper is well written and that the proposed method is easy to understand and reasonable. However, its methodological contribution is limited because the core idea is essentially the same as distillation, and dynamically gating the modules is a common technique in general. Moreover, I agree with the reviewers that the method should be compared with more other state of the art methods in this context. Accelerating or compressing DNNs are intensively studied topics and there are many approaches other than weight pruning, as authors also mention in the paper. As the possible contribution of the paper is more on the empirical side, it is necessary to thoroughly compare with other possible approaches to show that the proposed method is really a good solution in practice. For these reasons, I’d like to recommend rejection.  
It is important to have good stable and trustworthy algorithms.  Though I am unconvinved that the C DQN algorithm proposed here is the final word (and I suppose this is not controversial, and the authors might agree), the ideas presented here are sufficiently interesting to be disseminated and discussed more widely.  All reviewers recommended accepting the paper, and I ll follow their lead.  That said, the paper can still be improved, and the authors are encouarged to carefully consider the feedback provided by the reviewers.  In particular, it is good to be clear about which parts are principled, and which parts are somewhat heuristic or arbitrary, and could therefore presumably be improved in future work.  In fact, doing so clearly could make the paper _more_ rather than less impactful.  In any case, it seems good to include this paper at the conference, to highlight the questions and partial answers given here, and to inspire more discussion.
The paper studies the global convergence for policy gradient methods for linear control problems.  Multiple reviewers point out strong concerns about the novelty of the results.
The authors propose a novel and elegant way for learning parameterized aggregation functions and show that their approach can achieve good performance on several datasets (in many cases outperforming other state of the art methods). This is also appreciated by most of the reviewers. However, there have been several issues regarding the description of the proposed approach and the conducted experiments. These have been partly resolved in the rebuttal phase but should be more carefully assessed in another iteration of reviews.   More specifically: Experiments regarding learning of a single LAF versus multiple LAF should partly be included in the main paper (e.g. Figure 4 showing the performance for different numbers of LAFs). When constructing deep sets in this setting with a similar number of aggregation function it appears not very sensible to me to incorporate the same aggregation function multiple times but one would rather include a set of different fixed aggregation functions (these could be derived from the proposed LAFs). The experiments would also benefit from including set transformers as baselines (set transformers are discussed in the paper but not considered in the experiments as the authors argue that this is an orthogonal approach; while I agree that the goal of set transformers is different, I think there would be big value in understanding how these approaches compare and/or can be combined).  Beyond that I think 	a brief discussion of the related topic of learning pooling operations (e.g., in CNNs) is warranted.   Some reviewers also find that their concerns are only partially addressed in the rebuttal (e.g., regarding the extension from sets to vectors and applications in which the achieved performance differences are bigger).  One point which didn’t come up in the reviews but I would want to see addressed in a future version of the paper is an extended discussion of Figure 4. While there are cases were LAF clearly performs better, there are also cases, where Deep Sets outperform (this seem to be the cases in which the used aggregation units match the considered task). As LAFs can in theory represent these aggregation function it still seems challenging to learn the correct form of the aggregation function — I would appreciate deeper insights an analysis of this aspect. An immediate heuristic solution for many applications for improving performance thus might be to combine LAFs and standard aggregators.  In summary, the submitted paper has big potential but should be carefully revised and the experiments should be extended before the paper is accepted.
The paper presents an approach to predict relations between node pairs in heterogeneous graphs, with application to recommendation and knowledge base completion.   The author s approach is to compute similarities between subgraphs that are neighborhoods of nodes where the relation holds or not to score a relation. The authors use graph neural networks to scores these subgraphs. The type of subgraphs that are considered are pairs of nodes, 3  and 4  cyles to make inference and training tractable. The paper lies in the stream of work that combines logical reasoning and neural network, even though in that particular instance it mostly combines graph mining techniques and neural networks.   The reviewers unanimously liked the presentation of the paper and the high empirical performance. The rebuttal addressed most of the remaining concerns.
The focus of the submission is to define divergences on discrete probability measures. Particularly, the authors propose a common generalization of the well known concept of maximum mean discrepancy and kernel Stein discrepancy.  As summarized by the reviewers the submission is in a rather preliminary form: 1)The work lacks motivation. 2)Literature review (there are 4 references in total) and numerical illustrations are missing. 3)The submission lacks proper mathematical formulation/rigor. I highly recommend the authors to not submit similar draft manuscripts in the future.
The paper s primary contributions are: * Contrary to previous claims, the authors empirically show that inheriting the weights after pruning can be beneficial when using *larger* fine tuning learning rates than previously done. * As an explanation, the authors provide suggestive results showing that pruning breaks dynamical isometry, which they claim explains why larger learning rates are needed. * They propose a regularization based technique to recover dynamical isometry on modern residual CNNs.  Generally, reviewers were positive about the ideas in the paper, however, even after the rebuttal 3/4 reviewers did not find the arguments were clear or strongly supported yet. One issue that came up several times is a request for more investigation of StrongReg+pruning. At this time, I have to recommend rejection, but I encourage the authors to follow up on the reviewers suggestions and submit to a future venue.
The reviewers agree that the results are promising and there are some interesting and novel aspect to the formulation. However, two of the reviews have raised concerns regarding the exposition and the discussion of previous work. The paper benefits from a detailed description of soft Q learning, PCL, and off policy actor critic algorithms, and how SAC is different from those. Instead of differentiating against previous work by saying soft Q learning and PCL are not actor critic algorithms, discuss the similarities and differences and present empirical evaluation.
This paper investigates the problem of building a program execution engine with neural networks. While the reviewers find this paper to contain interesting ideas, the technical contributions, scope of experiments, and the presentation of results would need to be significantly improved in order for this work to reach the quality bar of ICLR.
This paper proposes a cyclical training scheme for grounded visual captioning, where a localization model is trained to identify the regions in the image referred to by caption words, and a reconstruction step is added conditioned on this information. This extends prior work which required grounding supervision.   While the proposed approach is sensible and grounding of generated captions is an important requirement, some reviewers (me included) pointed out concerns about the relevance of this paper s contributions. I found the authors’ explanation that the objective is not to improve the captioning accuracy but to refine its grounding performance without any localization supervision a bit unconvincing   I would expect that better grounding would be reflected in overall better captioning performance, which seems to have happened with the supervised model of Zhou et al. (2019). In fact, even the localization gains seem rather small: “The attention accuracy for localizer is 20.4% and is higher than the 19.3% from the decoder at the end of training.” Overall, the proposed model is an incremental change on the training of an image captioning system, by adding a localizer component, which is not used at test time. The authors  claim that “The network is implicitly regularized to update its attention mechanism to match with the localized image regions” is also unclear to me   there is nothing in the loss function that penalizes the difference between these two attentions, as the gradient doesn’t backprop from one component to another. Sharing the LSTM and Language LSTM doesn’t imply this, as the localizer is just providing guidance to the decoder, but there is no reason this will help the attention of the original model.   Other natural questions left unanswered by this paper are:   What happens if we use the localizer also in test time (calling the decoder twice)? Will the captions improve? This experiment would be needed to assess the potential of this method to help image captioning.   Can we keep refining this iteratively?   Can we add a loss term on the disagreement of the two attentions to actually achieve the said regularisation effect?  Finally, the paper [1] (cited by the authors) seems to employ a similar strategy (encoder decoder with reconstructor) with shown benefits in video captioning.  [1] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruction network for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7622–7631, 2018.  I suggest addressing some of these concerns in a revised version of the paper.
The paper proposes a nice and easy way to regularize spectral graph embeddings, and explains the effect through a nice set of experiments. Therefore, I recommend acceptance.
This work suggests an extension of diffusion based generative models, where both the forward and reverse process have learnable parameters (rather than just the reverse process). This is then applied to speech synthesis, with high fidelity audio generated in very few sampling steps compared to what is typical for this class of models. The proposed model is specifically compared to other diffusion based approaches for speech synthesis in terms of inference speed.  Reviewers highlighted the novelty of the idea and the convincing experimental results. Concerns were raised about the accessibility and clarity of the presentation (structure, too many technical details), lack of a related work section, and the methodology used to compare the proposed model against baselines. The authors have attempted to address these issues, and two reviewers raised their scores as a result. All reviewers now recommend acceptance.  I am therefore recommending acceptance as well, but I would like to encourage the authors to polish the presentation further, in order to make the work maximally accessible to a wide audience.
The paper proposes a new approach for linked view clustering based on chained non negative matrix factorization. Reviewers highlighted that paper proposes a novel and interesting approach to an important problem. However, reviewers raised also significant concerns regarding clarity of presentation (motivation, general approach, contributions, scope) as well as the experimental evaluation. Reviewers raised also concerns regarding justification of the approach being a novel paradigm. After author response and discussion, all reviewers and the AC agree that the paper is not yet ready for publication due to the aforementioned issues.
This paper gives explicit hyperparameter gradients for several models with convex losses.  The idea is well motivated and clearly presented, but because it s relatively incremental, it needs a more systematic experimental section, or at least a stronger characterization of its scope and limitations.  I would also recommend an investigation of more expressive hyperparameterizations (like in Maclaurin et al 2015) and/or an investigation of overfitting on the validation set.
The work presents a theoretical analysis of data augmentation, presenting evidence that data augmentation enlarges the smaller the singular values of the network Jacobian. Based on this theory the authors present a method for selecting a subset of training data to use with augmentation that decently approximates performance of training w/ augmentation on the full dataset. Reviewers overall agreed that the theoretical analysis was interesting, and did not find any flaws (though it is worth noting that the theory is restricted to additive perturbations). However, multiple reviewers found the presented experiments unconvincing, and questioned the stated motivation. The AC agrees with reviewers that most simple augmentations are not prohibitive in training speed. Certainly training on less data with a fixed epoch budget would require less compute time, but this is has nothing to do with augmentation and instead is a result of fewer steps taken in training. In the rebuttal, the authors argued that training on Imagenet is prohibitive with a single GPU (taking 2 weeks to do full training). However, given the authors claim their method speeds up training by a factor of 6.3x, then reducing ImageNet training from 2 weeks to 2 days would be a more convincing application of their method and would strengthen the work.
The paper considers the problem of learning a new task with few examples by using related tasks which can exploit shared representations for which more data is available. The paper proves a number of interesting (primarily theoretical) results.
This paper addresses the question of how to solve image super resolution, building on a connection between sparse regularization and neural networks. Reviewers agreed that this paper needs to be rewritten, taking into account recent work in the area and significantly improving the grammar. The AC thus recommends rejection at this time. 
All reviewers agree in their assessment that this paper is not ready for acceptance into ICLR and the authors did not respond during the rebuttal phase.
The authors present a hierarchical explanation model for understanding the underlying representations produced by LSTMs and Transformers.  Using human evaluation, they find that their explanations are better, which could lead to better trust of these opaque models.  The reviewers raised some issues with the derivations, but the author response addressed most of these.  
The authors propose the Bures metric (a distance between covariance matrices of the last feature layer of a discriminator) as an extra loss to mitigate mode collapse. The metrics bears some similarity to the covariance term in FID, and builds upon a number of GAN papers that augment GAN losses with differences in covariances between real and generated data. As the reviewers noted, the authors did an admirable job of performing an apples to apples comparison with other GAN alternatives, and use a number of metrics to demonstrate their results. Unfortunately, the most extensive comparisons usedthe DCGAN architecture, which is now 2 3 years too old for a potential reader to ascertain how well the proposed method would work on her problem. Moreover, the reviewers identified discrepancies in the baselines of those the experiments, as the numbers reported in this paper seemed to indicate poorer performance and the numbers reported in the original papers.  During the rebuttal phase, the authors demonstrated that these methods also perform well with using ResNet architectures on CIFAR 10 and STL 10, and the method is competitive with more recent models. As noted by the reviewers, however, these new comparisons are not as extensive and controlled as those that used DCGAN. Furthermore, results on more difficult datasets, such as ImageNet, are missing.  Had the extensive experiments used ResNets instead of DCGAN, or if the authors demonstrated promising results on ImageNet, I would recommend acceptance. Unfortunately, I think the audience for this paper in 2020 2021 would be relatively limited, so I have to recommend rejection.
The authors introduce a method that improves the representation learned by RL agents, making them more robust to visual distractions. In particular, their approach proposes to use mutual information between two views as a proxy for that objective. This is clearly a borderline paper that required many discussions among the reviewers and the authors. The reviewers mention that the approach is novel, addresses an important problem of robustness in RL and some of the experiments provided are impressive. On the other hand, the reviewers point out that the baselines seem to achieve lower results than previously reported, writing could be improved and some of the results don t show significant improvement over baselines.   Given that some of the results cause confusion around the evaluation protocol (it s still not 100% clear why the performance of baselines is lower than expected) and other doubts expressed by the reviewers, I encourage the authors to continue working on the paper and resubmit. I believe that with a little bit of extra work and clarifications this can be a very strong submission.
This paper addresses the setting of imitation learning from state observations only, where the system dynamics under which the demonstrations are performed differs from the target environment. The paper proposes to circumvent this dynamics shift with an algorithm whereby the target policy is trained to imitate its own past trajectories, re ranked based on the similarity in state occupancies as judged by a WGAN critic.  The reviewers found the paper to be clearly written and enjoyable. The paper improved considerably through reviewers feedback. Notably, a behavior cloning from observations (BCO) baseline was added, which was stronger than the authors expected but still helped highlight the strength of the proposed method by comparison. R1 had a particularly productive multiple round exchange, clarifying the description of previous work, clarifying the details of the proposed procedure and strengthening the presentation of empirical evidence.  This work compellingly addresses an important problem, and in its final form is a polished piece of work. I recommend acceptance.
The paper proposes a new approach to target propagation that performs well when used in RNNs on sequence modelling. The paper falls into something of an uncanny valley, where it is different enough from the original TP to lose some of its motivation ("biological plausibility"), and is now directly competing with backprop. Claims about outperforming backprop require EXTREMELY thorough and rigorous experimental evidence. Without meaning to cast any doubt on the authors work, there have simply been a lot of papers over the years that saw some improvements over backprop in some setting, that have not generalised or even been reproducible.
This paper introduces an HRL method that uses slow features to define subgoals (or abstract states), which can then be used by goal conditioned policies. It is said that such an approach allows for efficient exploration. Most reviewers are recommending the acceptance of this paper, they found the method interesting and they think it introduces interesting ideas that are not that common to the HRL literature. Thus, I’m recommending the acceptance of this paper.  I’d still encourage the authors to take the reviewers comments into consideration when preparing the final version of the paper. Specifically, it would be useful to explicitly discuss the “chicken and egg problem” and the fact that the agent has access to a function defining the distance to the goal before the goal was observed for the first time. Some baselines have the same assumption, but it is somewhat weird to discuss exploration in this setting without further clarifications.  
This paper presents several theoretical results linking deep, wide neural networks to GPs.  It even includes illuminating experiments.  Many of the results were already developed in earlier works. However, many at ICLR may be unaware of these links, and we hope this paper will contribute to the discussion. 
The paper proposes an interesting framework for visualizing and understanding GANs, that will be of clear help for understanding existing models and might provide insights for developing new ones. 
Reviewers concurred that this is an interesting paper with contributions worthy of publication. The authors also provided many details in the rebuttal which makes the paper even more strong.
This paper proposes a voice conversion framework, ClsVC, which is based on disentanglement of speaker and content information in some latent space.  The authors introduce two classification constraints (a common speaker classifier and an adversarial classifier) to improve the separation of the two embeddings.  Experimental results are reported on a few voice conversion tasks with objective and subjective scores.  Reviewers have reservation about the novelty of the work which is not considered overwhelmingly significant given existing techniques. The theory and arguments on the claimed effectiveness of the disentanglement of speaker and content also raise concerns, which need to be further verified.  The experimental results need to be more convincing.  Lastly, the exposition needs significant improvements.  The authors  rebuttal answers some of the comments but a few major concerns still stand.  This paper can not be accepted given its current form.
The paper presents an explicit memory that directly contributes to more efficient exploration. It stores trajectories to novel states, that serve as training data to learn to reach those states again (through iterative sub goals).   The description of the method is quite clear, the method is not completely novel but has some merit. Most weaknesses of the paper come from the experimental section: too specific environments/solutions, lack of points of comparisons, lacking some details.  We strongly encourage the authors to add additional experimental evidence, and details. In its current form, the paper is not sufficient for publication at ICLR 2019.  Reviewers wanted to note that the blog post from Uber ("Go Explore") did _not_ affect their evaluation of this paper.
The paper proposes a new method for adversarial attacks, MarginAttack, which finds adversarial examples with small distortion and runs faster than the CW baseline, but slower than other methods. The authors provide theoretical guarantees and a broad set of experiments.   In the discussion, a consistent concern has been that, experimentally, the method does not perform noticeably better than previous approaches. The authors mention that the lines are too thick to reveal the difference. It has been pointed out that this might be related to the way the experiments are conducted, but the proposed method still does better than other methods. AnonReviewer1 mentions that the assumptions needed for the theoretical part might be too strong, meaning that the main contribution of the paper is in the experimental side.   The comparisons with other methods and the assumptions made in the theorems seem to have caused quite some confusion and there was a fair amount of discussion. Following the discussion session, AnonReviewer1 updated his rating from 5 to 6 with high confidence.   The referees all rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below the acceptance threshold.    Although the paper seems to propose valuable ideas, and it appears that the discussion has clarified many questions from the initial submission, the paper has not provided a clear, convincing, selling point at this time. 
The paper proposes recasting robust optimization as regularizer for learning representations by neural networks, resulting e.g. in more semantically meaningful representations.   The reviewers found that the claimed contributions were well supported by the experimental evidence. The reviewers noted a few minor points regarding clarity that seem to have been addressed. The problems addressed are very relevant to the ICLR community (representation learning and adversarial robustness).  However, the reviewers were not convinced by the novelty of the paper. A big part of the discussion focused on prior work by the authors that is to be published at NeurIPS. This paper was not referenced in the manuscript but does reduce the novelty of the present submission. In contrast to the current submission, that paper focuses on manipulating the learned manipulations to solve image generation tasks, whereas the current paper focuses on the underlying properties of the representation. Since the underlying phenomenon had been described in the earlier paper and the current submission does not introduce a new approach / algorithm, the paper was deemed to lack the novelty for acceptance to ICLR.   
Main content: Paper is about training low precision networks to a high accuracy.  Discussion: reviewer 2: impressive results, main questions are around some clarity in the experiments tried, but sounds like authors addressed most of this in rebuttal. reviewer 1: well written paper, but authors think some technical details could be clarified.  reviewer 3:  well written but experimental section could be improved. Recommendation: all reviewers are in consensus, well written paper but some experiments/technical details could be improved. i vote poster.
While the reviewers agree that the paper contains interesting ideas and the method is elegant, it unfortunately does not meet the bar for acceptance. I strongly encourage the authors to revise their paper, in particular using the numerous comments made throughout the discussion phase; for example:  * It is important that the authors polish their work, in particular for the updates provided (e.g. Figure 3, see EFwa)  * Reviewers pointed the lack of updates on important claims by the authors (in particular the claim regarding clustering vs decision trees, see EFwa, the comments on the lack of diverse datasets, see meXP, )  * Some answers might have gained in clarity, such as the reply to EFwa on the application and conclusions following Wilcoxon sign test.
The paper discusses the dynamics of training neural nets and how they are related to features that are robust and predictive (following Ilyas et al). The reviewers had many comments regarding the presentation of the claim and the validity of the empirical results, as well as their unclear practical implications. The authors have improved the writing somewhat but reviewers still thought the manuscript should be substantially improved so that the claims are clearer and empirical validation is more convincing.  The authors are also encouraged to discuss their results in the context of results on  inductive bias of deep learning (e.g., results on NTK, rick regimes, margin maximization etc).    
The paper proposes a Bayesian neural network model for tensor factorization, with particular focus on streaming data. The key contribution is the streaming posterior inference of the deep TF models.  The combinations of online tensor factorization, Bayesian NN with sparsity priors, posterior inference is new and interesting.  However, there are many approximation steps, and the quality of the approximation and convergence of algorithm are not well justified.   
This work proposes a new regularization method for weakly supervised localization based on counting. Reviewers agree that this is an interesting topic but the experimental validation is weak (qualitative, lack of baselines), and the contribution too incremental. Therefore, we recommend rejection.
This paper presents a GAN training algorithm motivated by online learning. The method is shown to converge to a mixed Nash equilibrium in the case of a shallow discriminator. In the initial version of the paper, reviewers had concerns about weak baselines in the experiments, but the updated version includes comparisons against a variety of modern GAN architectures which have been claimed to fix mode dropping. This seems to address the main criticism of the reviewers. Overall, this paper seems like a worthwhile addition to the GAN literature.
The authors present an approach to learn node embeddings by minimising the mincut loss which ensures that the network simultaneously learns node representations and communities. To ensure scalability, the authors also propose an iterative process using mini batches.   I think this is a good paper with interesting results.  However, I would suggest that the authors try to make it more accessible to a larger audience (2 reviewers have indicated that they had difficulty in following the paper). For example, while Theorem 1 and Theorem 2 are interesting they could have been completely pushed to the Appendix and it would have sufficed to say that your work/results are grounded in well proven theorems as mentioned in 1 and 2.   I agree that the authors have done a good job of responding to reviewers  queries and addressed the main concerns. However, since the reviewers have unanimously given a low rating to this paper, I do not feel confident about overriding their rating and accepting this paper. Hence, at this point I will have to recommend that this paper cannot be accepted. This paper has good potential and the authors should submit it to another suitable venue soon.
This paper studies the use of natural language explanations during the training of an agent for odd one out tasks. Experiment results show that using quality explanation as abstract information about object properties helps with the agent performance, as compared with the vanilla method.  Strengths:   Experiment results are conducted thoroughly to support the major claims made by the paper   The problem is well motivated and has an important implication  Weakness:   There has been extensive discussion about whether the paper lacks a more formal and rigorous definition of "explanation" as considered in the scope of this paper.    Concerns are raised regarding the gaps between the broad claims in the paper and the restricted experiment settings
This paper reveals that popular data poisoning systems, Fawkes and LowKey, fail to effectively protect user privacy in facial recognition. The methods to defend against poisoning attacks are quite simple you can either adaptively tune the face recognition models or just wait for more advanced facial recognition systems. Given these “disappointed” findings from the technical solution side, this paper further argues that legislation may be the only viable solution to prevent abuses of facial recognition.  Overall, all the reviewers highly appreciate the comprehensive and rigorous evaluations provided in this paper and enjoy reading it. The biggest concern is raised by the Reviewer 6s7m, given this work fails to discuss/compare to previous works on Facial identity anonymizing and the technical contribution is incremental. During the discussion period, all other reviewers reach a consensus that 1) facial identity anonymizing is not relevant; and 2) this work make enough contributions and is worthy to be heard by the general community; the Reviewer 6s7m still hold the opposite opinion, but is okay for accepting this paper anyway.   In the final version, the authors should include all the clarification provided in the discussion period.
This paper develops a meta learning approach for few shot object detection. This paper is borderline and the reviewers are split. The problem is important, albeit somewhat specific to computer vision applications. The main concerns were that it was lacking a head to head comparison to RepMet and that it was missing important details (e.g. the image resolution was not clarified, nor was the paper updated to include the details). The authors suggested that the RepMet code was not available, but I was able to find the official code for RepMet via a simple Google search: https://github.com/jshtok/RepMet Reviewers also brought up concerns about an ICCV 2019 paper, though this should be considered as concurrent work, as it was not publicly available at the time of submission. Overall, I think the paper is borderline. Given that many meta learning papers compare on rather synthetic benchmarks, the study of a more realistic problem setting is refreshing. That said, it s unclear if the insights from this paper would transfer to other machine learning problem settings of interest to the ICLR community. With all of this in mind, the paper is slightly below the bar for acceptance at ICLR.
 Pros:   This is an interesting and relevant topic   It is well motivated and mostly clear  Cons:   The motivation, large amounts of data such as occur in lifelong learning, is not well examined in the evaluation which focuses on quite small problems.  For an example of work which addresses the lifelong memory management issue (though does not learn a memory management policy) see [1].   In general the evaluation is not adequate to the claims.   Reviewer 2 is concerned with the use of a bi directional RNN for the comparison of memory entries since it may overfit to order.   Reviewer 1 is somewhat concerned with novelty over other memory management schemes.  [1] Scalable Recollections for Continual Lifelong Learning. https://arxiv.org/pdf/1711.06761.pdf
All reviewers agreed that the idea proposed by the paper is interesting and is well motivated for handling long tailed recognition problems.  As suggested by the reviewers, it seems important that the limitations the paper be addressed in the final version of the paper.
As per R3: This paper presents a novel approach for doing hierarchical deep RL (HRL) on UMDPs by: (a) use of hindsight experience replay at multiple levels;  combined with (b) max T timesteps at each level. By effectively learning from missed goals at multiple levels, it allows for fairly  data efficient learning and can (in principle) work for an arbitrary number of levels. HRL is an important open problem.  The weaknesses described reviewers include limited comparisons to other HRL methods; its applicaiton to fairly simple domain; its still unclear what the benefit of > 4 levels is, and what the diminishing returns are wrt to the claim of working for an arbitrary number of levels.  R1(5) and R3(7)  stand by their scores. R1(5) still has some remaining concerns regarding some experiments not being done across all tasks, an older version of the HAL algo baseline being used, and  lack of insight regarding >  4 levels.  Based on the balance of the reviewers comments and the AC s own reading of the paper and results,  and the importance of the problem, the AC leans towards accept.  Using Hindsight Exp Replay across multiple levels is a simple but interesting idea, and the terminate after T steps is an interesting heuristic to make this effective. While the paper does not give insight for large (> 4) levels, it does make for an interesting framework that will inspire further work.  The AC recommends that the claims regarding an "arbitrary number of levels" be significantly toned down. 
This paper received 2 marginally below and 1 marginally above ratings. We discussed the paper with the reviewers and there was broad consensus that 1) the paper lacked clarity; 2) multiple modeling choices were debatable (e.g., ordering or embedding of neurons and convolution over neurons!!) and not sufficiently justified (and these choices will critically impact the conclusions drawn from the analysis); 3) we were not convinced by the relevance of the synthetic data to reflect a meaningful biological process; 4) we did not see any meaningful knowledge gained for biology from this whole analysis. My recommendation is thus to reject this paper.
The paper proposes three algorithms for the sparse PCA problem, where one imposes the additional constraint that the vectors have a small number of non zero entries. The proposed algorithms run in polynomial time and achieve provable approximation guarantees on the accuracy and sparsity. The reviewers identified the following strengths of the contributions: the algorithms are simple and have different strengths; the theoretical results are sound and perhaps even surprising; the presentation is clear. The reviewers identified the following weaknesses of the contributions: the running times of the proposed algorithms are high and they may not scale to large datasets, which significantly limits their application to machine learning datasets; the experimental evaluation is insufficient and it does not compare with some of the state of the art algorithms; the algorithmic novelty is limited. After weighing these strengths and weaknesses as well as evaluating the paper relative to other ICLR submissions, I recommend reject.
This is a very interesting paper which extends natural gradient to output space metrics other than the Fisher Rao metric (which is motivated by approximating KL divergence). It includes substantial mathematical and algorithmic insight. The method is shown to outperform various other optimizers on a neural net optimization problem that s artificially made ill conditioned; while it s not clear how practically meaningful this setting is, it seems like a good way to study optimization. I think this paper will be of interest to a lot of researchers and could open up new research directions, so I recommend acceptance as an Oral. 
The paper s strength is in that it shows the log likelihood objective is lower bounded by a GAN objective plus an entropy term. The theory is novel (but it seems to relate closely to the work https://arxiv.org/abs/1711.02771.) The main drawback the reviewer raised includes a) it s not clear how tight the lower bound is; b) the theory only applies to a particular subcase of GANs   it seems that the only reasonable instance that allows efficient generator is the case where Y   G(x)+\xi where \xi is Gaussian noise. The authors addressed the issue a) with some new experiments with linear generators and quadratic loss, but it lacks experiments with deep models which seems to be necessary since this is a critical issue. Based on this, the AC decided to recommend reject and would encourage the authors to add more experiments on the tightness of the lower bound with bigger models and submit to other top venues.  
The paper proposes a tensor based extension to graph convolutional networks for prediction over dynamic graphs.   The proposed model is reasonable and achieves promising empirical results. After discussion, it is agreed that while the problem of handling dynamic graphs is interesting and challenging, the proposed tensor method lacks novelty, the theoretical analysis is artificial, and the empirical study does not cover enough benchmarks.   The current version of the paper is not ready for publication. Addressing the issues above could lead to a strong publication in the future. 
The paper extends the results in Yarotsky (2017) from Sobolev spaces to Besov spaces, stating that once the target function lies in certain Besov spaces, there exists some deep neural networks with ReLU activation that approximate the target in the minimax optimal rates. Such adaptive networks can be found by empirical risk minimization, which however is not yet known to be found by SGDs etc. This gap is the key weakness of applying approximation theory to the study of constructive deep neural networks of certain approximation spaces, which lacks algorithmic guarantees. The gap is hoped to be filled in future studies.   Despite the incompleteness of approximation theory, this paper is still a good solid work. Based on fact that the majority of reviewers suggest accept (6,8,6), with some concerns on the clarity, the paper is proposed as probable accept. 
The manuscript proposes to analyze the learning dynamics of deep networks with separable data. A variety of results are provided under various assumptions.  The reviewers and AC note the assumptions required for the analysis are quite strong, and perhaps too strong to provide useful insight into real problems. Reviewers also cite issues with writing and the breadth of the title (this was much improved after rebuttal).
This paper introduces a new variant of autoencoders with an topological loss term.  The reviewers appreciated part of the paper and it is borderline. However, there are enough reservations to argue for it will be better for the paper to updated and submitted to next conference.  Rejection is recommended.  
This paper aims to explain the pretraining effectiveness of masked language model, based on the concept of  diversity of classes. They empirically study how a diversity regularizer, based on this theory can improve model performance, as an empirical support.  Before rebuttal, reviewers consistently found the empirical study rather preliminary, while authors, through rebuttals, argue the theoretical study should be highlighted as their main contribution, and expressed concerns that the lack of empirical rigor should not be a ground to reject. We agree with these concerns, but rebuttals and discussions failed to convince reviewers that assumptions and evaluations are proper for connecting the proposed theory to potential impacts in pretrained language model scenarios. Revising to make this connection clearer would address the reviewer disagreements in the future.
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * The motivation of the problem is presented well * The architecture is simple and potentially applicable to real world applications  Cons: * The novel methodological contribution is limited to non existant * Comparison against other relevant baselines is missing, and the baseline is not strong * The evaluation methodology does not follows standard practice in IR, and thus it is difficult to analyze and compare results * Paper is hard to read and requires proofreading  Considering these pros and cons, my conclusion is that this paper is not up to the standards of ICLR.
This paper introduces the idea of "empathy" to improve learning in communication emergence. The reviewers all agree that the idea is interesting and well described. However, this paper clearly falls short on delivering the detailed and sufficient experiments and results to demonstrate whether and how the idea works.  I thank the authors for submitting this research to ICLR and encourage following up on the reviewers  comments and suggestions for future submission. 
The authors did a good job responding to reviewer concerns.   While the reviewers still consider the method described in the paper to not be especially novel, at least one is impressed by the practicality.  imo the authors  attention detailed ablations and analysis post review makes the paper worth including in the conference.
This paper investigates tradeoffs between preserving accuracy on clean samples and increasing robustness on adversarial samples by using transformations and majority votes. Observations on the distribution of the induced softmax show that existing methods could be improved by leveraging information from that distribution to correct predictions, as confirmed by experiments. The problem space is important and reviewers find the approach interesting. Authors have provided some necessary clarifications during rebuttal and additional experiments. While some reservations remain, this paper s premise and its experimental results appear sufficiently interesting to justify an acceptance recommendation.
This paper analyses generalization ability of graph neural network from the aspect of the distance between the test data point and the training data point, where the labels of a part of the vertexes are observed as the training data and a test data point is selected from the remaining vertexes. The theoretical result indicates that if the training data "cover" the whole vertexes of the graph, then the test accuracy will be better. This theoretical finding is supported by some numerical experiments.   The problem this paper considers is interesting and would be worth investigation. However, the theoretical results presented in the paper are based on quite strong assumptions, and the statement of the theorem is not well exposed.     First, the paper assumes that a distortion map is obtained by training and the training procedure can produce zero training errors. Although these assumptions are far from obvious in practice, the paper lacks justification of these assumption. Hence, these assumptions seem to be made only for the sake of proof.     Second, the constants appeared in the theorems are not correctly specified. How different constants are correlated is not properly exposed.    As for the experiments, they are not so strong: only Cora is experimented and training data size is small.   For these reasons, this paper is not sufficiently matured to appear in ICLR.
This work proposes a novel network structure, spatial dependency networks that is introduced as an alternative to convolutional neural networks. This new architecture is used successfully to get state of the art performance for a number of common image generation benchmarks when compared with non autoregressive approach (even much larger CNNs). There is a lot of useful feedback in the reviews themselves: a thing to consider in the final version is the fact that the authors had motivated SDNs as drop in replacements for CNNs, but do experiments mostly in VAE like settings. This is a point that was raised by multiple reviewers and is clearly something that should be dealt with as explicitly as possible.  While there are legitimate reasons to be wary of the increased computation time, I tend to side with the authors that baselines that are being compared with SDNs are likely to have more optimized primitives. From the inference numbers presented in the rebuttal, it doesn t appear like the speed issues are insurmountable.   Given the high quality of writing, the excellent performance on image density modeling, the various ablations and understanding of the disentangling effects, I think this is an interesting piece of work that the field would benefit from.
This paper constitutes interesting progress on an important problem.  I urge the authors to continue to refine their investigations, with the help of the reviewer comments; e.g., the quantitative analysis recommended by AnonReviewer4.
This paper presents a novel idea of transferring gradients between tasks to improve multi task learning in neural network models. The write up includes experiments with multi task experiments with text classification and sequence labeling, as well as multi domain experiments. After the reviews, there are still some open questions in the reviewer comments, hence the reviewer decisions were not updated. For example, the impact of sequential update in pairwise task communication on performance can be analyzed. Two reviewers question task relatedness and the impact of how and when it is computed could be good to include in the work. Baselines could be improved to reflect reviewer suggestions.
This paper proposes a graph neural network based approach for scaling up imitation learning (e.g., of swarm behaviors). Reviewers noted key limitations in the discussion of related work, size of the proposed contribution in terms of model novelty, and evaluation / comparison to strong baselines. Reviewers appreciated the author replies which resolved some concerns but agree that the paper is overall not ready for publication.
The paper got mixed ratings. However, keeping in mind the low confidence of some of the reviewers, the paper needed an additional look. The AC himself went over the paper. The paper presents an interesting formalism for private information retrieval. As reviewers have pointed out the formalism is based on several existing ideas on utility privacy tradeoff.  The use of GANs for enforcing privacy is also not new. The rebuttal did not convince some of the reviewers about novelty which seems reasonable given the area and literature in it.   Overall, the paper needs to consolidate all ideas of Adversarial training for privacy and compare and contrast with the proposed approach to make it compelling for publication. 
The paper considers a problem of weak mean estimation under a differential privacy like constraint. Specifically, estimating the signs of a (sparse) mean, and not the actual values.   The reviewers brought up a number of concerns, including the weak privacy guarantee (a type of average case privacy). Other lesser concerns include inaccuracies in comparisons with the literature and lack of interest in the algorithm/method itself.  As there was no response from the authors, there was little further discussion afterwards, and the reviewers remained in their opinion to reject the paper.
This paper introduces a GAN based framework for inferring human category representations. The reviewers agree that the idea is interesting and well motivated, and the results are promising. The technical contribution is not significant, but nevertheless the paper combines existing ideas in an interesting way. The reviewers would also like to see some more work towards the direction of investigation of the results and extraction of insights, without which the paper feels somehow incomplete.
This is a very interesting paper which discusses practical issues and solutions around deploying RL on real physical robotic systems, specifically involving questions on the use of raw sensory data, crafting reward functions, and not having resets at the end of episodes.  Many of the issues raised in the reviews and discussion were concerned with experimental details and settings, as well as relation to different areas of related work. These were all sufficiently handled in the rebuttal, and all reviewers were in favour of acceptance.
This paper presents a graph neural network model to predict the severity of depression symptoms from text. It proposes to construct a graph with word nodes and use schema structure to capture the context information in the text. A schema encoder is introduced for modeling the constructed graphs.  Strength:   Interesting application domain and clear motivation   Experiment results demonstrate the effectiveness of the method   Paper writing is clear and easy to follow  Weakness:   Technical novelty of the method is limited   Experiment comparison with some recent work is missing   More in depth analysis on the method are needed   Some details of the method pipeline require further elaboration
This paper proposes an importance sampling estimator for probabilities of observations of SDEs. The proposed approach has several advantages over conventional methods: it does not require an SDE solver, it has lower gradient variance, and shows nice results with a Gaussian process representation of the function. Reviewers were somewhat split on this paper, with some concerns that experiments were limited. On balance, however, the paper makes several nice contributions, the experiments are in line with related works, and the authors did a good job of clarifying Theorem 1 in the rebuttal. We note that Reviewer K19Y changed their opinion to accept (although they forgot to update the score). Please carefully account for all reviewer comments in the final version.
This paper presents an approach, FedMix, for federated learning using mixture of experts (MoE). The basic idea is to learn an ensemble of models and user specific combination weights (mixing proportions).  The reviewers appreciated the MoE formulation for federated learning. However, there were multiple concerns from the reviewers, which include lack of clarity (regarding the variational lower bound that is being used), significant communication cost and privacy concerns (the server can infer the users  label distributions), weak experimental results, and lack of any theoretical support (which isn t that big an issue if the paper were stronger on other aspects). The author feedback was considered and the reviewers engaged in some discussions (also with the authors). In the end, however, the reviewer were still not convinced that the paper is ready to be published in its current state. Based on my own reading of the paper, the reviews, and the author response, I agree with this assessment.  The authors are advised to take into account the feedback from the reviewers and resubmit to another venue.
The paper addresses an important problem (preventing catastrophic forgetting in continual learning) through a novel approach based on the sliced Kramer distance. The paper provides a novel and interesting conceptual contribution and is well written. Experiments could have been more extensive but this is very nice work and deserves publication.
After reading the reviews and rebuttal and looking over the paper, I feel that the results are indeed strong, and the paper could have an impact in terms of exploiting the relationship among action dimensions. Maybe the only detail that I would add is that going through the example given in Fig 1 completely could be useful as it might not be perfectly obvious how (e.g. considering a simple mixing function like summation) one retrieves the q values for someone not familiar with this particular topic. 
Reviews for this paper were quite mixed (7744), and none were exactly borderline. All reviews were detailed and informative, as was the rebuttal. The main criticisms were (1) lack of detail in the experiments, and some missing evaluation (2) missing related work, (3) overall lack of polish (mentioned among positive reviews too), and (4) some unsubstantiated claims. Positively, reviewers praise the novelty, dataset, the demo, and some reviewers found the experiments mostly convincing.  Ultimately this is still a borderline decision. The rebuttal does appear to address many of the claims about missing evaluation, and the complaints about polish can be easily addressed. I think the unsubstantiated claims are reasonably rebutted too. Related work doesn t seem to be addressed in the rebuttal.
Because of strong support from two of the reviewers I am recommending accepting this paper. However, I believe reviewer 1 s concerns should be taken seriously. Although I disagree with the reviewer that a general "framework" method is a bad thing, I agree with them that additional experiments would be valuable.
All of the reviewers found some aspects of the formulation and experiments interesting, but they found the paper hard to read and understand. Some of the components of the technique such as the state screening function (SSF) seem ad hoc and heuristic without much justification. Please improve the exposition and remove the unnecessary component of the technique, or come up with better justifications.
The work focuses on a new method for sampling hyper parameter based on an "Population Based Training" schedule that tend to sample more often configurations that gave good performances in the past. The authors have conducted experiments to verify the superior of their method, especially for the effectiveness and generalisability.  Pros:   simple method that can be implemented without much effort,   good empirical performances on Imagenet,   paper well organised and written.  Cons:   lack of explanation about the DensNet121 performance degradation [partially addressed in the rebuttal],   additional simple experiments in Section 4.4 were recommended to evaluate the generality of the method [addressed in Table 5],   empirical validation seems not sufficient [partially addressed in the rebuttal],   similarity with respect to prior art, such as the focal loss [partially addressed in the rebuttal],   clarification of the randomisation strategy in experiments [addressed in the rebuttal].  Despite most of the issues being addressed, the reviewers decided that this paper would benefit more work to be accepted for the conference this year.
The reviewers highlighted that the application in the paper is interesting, but note a lack of new methodology, and also highlight serious flaws in the testing methodology. Specifically, the reviewers are discouraged by the straightforward reuse of Siamese networks without clear modifications. Further, the testing setup might be unfairly easy, since chemical families are represented in both training and test sets, while in true application of the method would be exposed to previously unseen chemical families.  The authors did not participate in the discussion, and address concerns. The reviewer consensus is a rejection.
This work was the subject of significant back and forth (between authors and reviewers, but also between reviewers & myself) due to the wide range of opinions. Two of the reviewers have found this work below the bar: they have provided multiple reasonings that I would rather not repeat here. The third reviewer found this work more compelling and argued for its acceptance. My attempts at reaching a consensus have yielding the following conclusions:    * There s agreement that one shot generation is indeed a challenging task   * Some of the results are indeed impressive, but many results are not compelling.   * The rebuttal addressed some of the concerns (e.g. visualization of latents), but some issues are unaddressed (e.g. more motivation, explanation of why the proposed method works better)   * One of the reviewers has argued rather forcefully that the work doesn t quite do domain adaptation in the typically understood sense. Moving beyond definitions of domain adaptation, the same reviewer was not very convinced by the quality of the results themselves.   * The reviewer most positive about this work agrees that this work only explores a limited form of domain transfer. They argued that some of the potential applications of this work do make the submission interesting.   Fundamentally, the discussion did not necessarily resolve the differences in opinion one way or another. Ultimately, all 3 reviewers believe that it would fine if this work was not accepted to ICLR at this time, despite some of the interesting results and promise. Given the discussion and this mildest consensus, I am inclined to recommend rejection too. I do think there s a substantial amount of constructive feedback in the reviews that would make a subsequent revision of this work quite a bit better.
The paper proposes an interesting hypothesis about deep nets  generalization behavior inside RL methods: it suggests that the nets  implicit regularization favors a particular form of degeneracy, in which there is excessive aliasing of state action pairs that tend to co occur. It proposes a new regularizer to mitigate this problem. It evaluates the hypothesis and the regularizer empirically, and it provides suggestive derivations to motivate both.  The reviewers praised the comprehensive empirical analysis, the insights into learning, and the combination of empirical and theoretical evidence. The authors participated responsively and helpfully in the discussion period, and addressed any concerns raised by the reviewers.  This is a strong paper: it derives and motivates a novel hypothesis about an important problem, and analyzes this hypothesis both mathematically and experimentally.
While the reviewers find the experiments in the paper somewhat interesting, they find that the paper does not sufficiently address whether the limitations shown for models in this paper translate to larger models and other, more realistic, tasks, or an artifact of the setup considered in the paper.  Overall the takeaways seem unclear from the paper and I believe it is not ready for acceptance.  Addressing the issues raised by reviewers and having a more clear discussion on connections to existing results will help the paper.
Initial reviews of this paper cited some concerns about a lack of comparison to SOTA and baselines, and also some debate over claims of what is (or is not) "biologically plausible."  However, after extensive back and forth between the authors and reviewers these issues have been addressed and the paper has been improved.  There is now consensus among authors that this paper should be accepted.  I would like to thank the reviewers and authors for taking the time to thoroughly discuss this paper.
The paper proposes a new method for the problem of learning under instance dependent noise (IDN). The idea is to construct a variational approximation to the ideal training objective, which involves learning a single scalar C(x) per instance. In turn, each such scalar is treated as an additional parameter to be learned by the network.  Reviewers generally found the basic idea of the proposal to be interesting and novel, with the response clarifying some initial questions on the design of the network to learn C(x). The paper is also well written, and presents experiments on image and text classification benchmarks. Some concerns were however raised:  (1) _Limited theoretical justification_. There is limited formal analysis of when the proposed method can work well.  (2) _Lack of comparison to IDN baselines_. The original submission did not include any IDN baselines as comparison. The revision included results of the method of (Zhang et al.,  21a), which is on par or better than the proposed method; it seems that this baseline really ought to have been included in the original submission, but it is appreciated that these have been added. A related concern was the marginal gains over the GCE method on the CIFAR datasets.  (3) _Sufficiency of learning a single parameter_. The paper learns a single scalar per sample. Several reviewers were unsure on the sufficiency of this parameter to capture the underlying noise distribution.  For (1), the authors acknowledge theoretical analysis as an important future direction. This is perfectly reasonable, but does then require weighting more any issues with the the conceptual and empirical contributions of the paper.  For (2), the response clarified that most of these operate either in the binary setting, or require auxiliary information. This is a valid motivation for the present work; it would however be more compelling to include results in a binary setting, to better understand the strengths and weaknesses compared to existing proposals. The response also clarified the present method does not claim to improve upon state of the art performance, but rather proposes a simple method which has additional applications (as shown in Appendix E). This is a reasonable claim; however, to my taste, there is insufficient discussion of the PLC method (Zhang et al.,  21a), and what new conceptual information the present work offers.  For (3), the response argued that the present results already demonstrate the efficacy of using a single parameter, and that using multiple parameters can be studied in future work. One reviewer was not convinced of the efficacy being shown in some of the results in Appendix E. It could strengthen the work if there is an empirical analysis of when the single parameter assumption starts to break down; e.g., perhaps under increasing levels of CCN noise?  Overall, the paper has interesting ideas and some nice analyses. At the same time, there was clear scope for improvement in the original submission. This was partially addressed in the revision, but given that several domain experts retain reservations (particularly in regards to comparisons against prior IDN works), it is encouraged that the authors incorporate the above comments for a future version of the paper.
This paper proposes a novel dataset of bouncing balls and a way to learn the dynamics of the balls when colliding. The reviewers found the paper well written, tackling an interesting and hard problem in a novel way. The main concern (that I share with one of the reviewers) is about the fact that the paper proposes both a new dataset/environment *and* a solution for the problem. This made it difficult the for the authors to provide baselines to compare to.  The ensuing back and forth had the authors relax some of the assumptions from the environment and made it possible to evaluate with interaction nets.  The main weakness of the paper is the relatively contrived setup that the authors have come up with. I will summarize some of the discussion that happened as a result of this point: it is relatively difficult to see how this setup that the authors have and have studied (esp. knowing the groundtruth impact locations and the timing of the impact) can generalize outside of the proposed approach. There is some concern that the comparison with interaction nets was not entirely fair.  I would recommend the authors redo the comparisons with interaction nets in a careful way, with the right ablations, and understand if the methods have access to the same input data (e.g. are interaction nets provided with the bounce location?).   Despite the relatively high average score, I think of this paper as quite borderline, specifically because of the issues related to the setup being too niche. Nonetheless, the work does have a lot of scientific value to it, in addition to a new simulation environment/dataset that other researchers can then use. Assuming the baselines are done in a way that is trustworthy, the ablation experiments and discussion will be something interesting to the ICLR community.
This paper proposes a new type of activations function based on q calculus. The reviewers found that the papers is significantly lacking in its presentation, in clarity, and in its experimental evaluation. The motivation of the method raises several significant questions to the reviewers, and the proposed method is not sufficiently compared to existing approaches for (noisy) activation functions. After reviews, the authors have failed to present any updates to their paper.
I would like to thank the authors for having managed a thorough discussion despite the complexity of the task at hand (e.g. BEvM). during discussion, the reviewers clearly converged to accepting the paper, praising the importance of the problem tackled and the setup put in place to effectively tackle the challenge at hand.  All this makes the paper an important contribution and a clear accept (and an enjoyable read), for which I can only recommend a further polish before camera ready to follow the latest inclusions.  AC.
The paper proposes a method of training latency limited (wait k) decoders for online machine translation. The authors investigate the impact of the value of k, and of recalculating the transformer s decoder hidden states when a new source token arrives. They significantly improve over state of the art results for German English translation on the WMT15 dataset, however there is limited novelty wrt previous approaches. The authors responded in depth to reviews and updated the paper with improvements, for which there was no reviewer response. The paper presents interesting results but IMO the approach is not novel enough to justify acceptance at ICLR.  
Three reviewers recommend rejection. After a good rebuttal, the first reviewer is more positive about the paper yet still feels the paper is not ready for publication. The authors are encouraged to strengthen their work and resubmit to a future venue.
 The paper considers the risk sensitive RL by exploiting entropic risk. The major contribution of this paper is providing the theoretical guarantees for the proposed risk senstive value iteration with function approximation.   The major concern of this paper is the similarity to the existing work in (Fei et al., 2020). I encourage the authors to reorganize the paper and emphasize the differences to highlight the major contribution. 
The paper proposes a black box algorithm for MRF training, utilizing a novel approach based on variational approximations of both the positive and negative phase terms of the log likelihood gradient (as R2 puts it, "a fairly creative combination of existing approaches").   Several technical and rhetorical points were raised by the reviewers, most of which seem to have been satisfactorily addressed, but all reviewers agreed that this was a good direction. The main weakness of the work is that the empirical work is very small scale, mainly due to the bottleneck imposed by an inner loop optimization of the variational distribution q(v, h). I believe it s important to note that most truly large scale results in the literature revolve around purely feedforward models that don t require expensive to compute approximations; that said, MNIST experiments would have been nice.   Nevertheless, this work seems like a promising step on a difficult problem, and it seems that the ideas herein are worth disseminating, hopefully stimulating future work on rendering this procedure less expensive and more scalable.
This paper presents some interesting new ideas on training binary neural networks. However, as many reviewers point out the study is quite limited by their experimental section, and some technical issues were raised. These are criticisms that remain largely unaddressed after the author response, hence the paper is not ready for publication at its current form.
AR2 is concerned about the marginal novelty, weak experiments and very high complexity of the algorithm. AR3 is concerned about lack of theoretical analysis and parameter setting. AR4 is concerned that the proposed method is useful in very restricted settings and the paper is incremental.  Unfortunately, with strong critique from reviewers regarding the novelty, complexity, poor presentation and restricted setting, this draft cannot be accepted by ICLR.
Positional encoding of the input coordinates using Fourier basis [as described in (1)] is a common tool in the context of multilayer perceptrons (MLP). The author propose to replace the Fourier basis with one on manifolds M (2), such as the classical spherical harmonics (M S^2), the Fourier basis on M SO(3) or on M S^2 x S^2.  MLPs form an important tool of our times. Unfortunately, as it is elaborated by the reviewers the Fourier basis of the investigated manifolds are widely studied and the presented results are well known; the submission lacks novelty.
The paper addresses the problem of recovering a graph structure from empirical observations. The proposed approach consists of formulating the problem as an inverse problem, and then unrolling a proximal gradient descent algorithm to generate a solution.   Whereas the paper has definitely some merit, it received borderline reviews, with three borderline rejects and one borderline accept. The reviewers have appreciated the clarifications and discussions provided by the rebuttal, and one reviewer went up from reject to borderline reject. More precisely, this reviewer agrees that the paper has become stronger, but he/she believes that the paper requires additional experimental work (see section "After rebuttal" from his/her review). Another active reviewer during the rebuttal/discussion stage was not convinced by the rebuttal, after raising issues about identifiability. The area chair agrees that solving the identifiability issue is not a key requirement for this paper; however, this raises legitimate questions about the guarantees/properties of the returned solutions.  Overall, this is a borderline paper, which introduces an interesting idea, but which requires additional experimental work and discussions about the properties of the solutions. Unfortunately, the area chair agrees with the majority of the reviewers and follows their recommendation. The two previous points should be addressed if the paper is resubmitted elsewhere.
3 reviewers recommend accept, 1 rates the paper marginally above acceptance. The authors provided satisfactory answers to criticism   all in all this is a paper worth accepting at ICLR. Please make sure that criticism in the reviews is adequately addressed in the final version, e.g. include various experimental results in the rebuttal, add the symbols in sec 3.2 & 3.3 to fig. 2, add a related discussion on ablations when the model is fully trained, etc.
This paper explores the emergence of language in environments that demand agents communicate, focusing on the compositionality of language, and the cultural transmission of language.  Reviewer 1 has several suggestions about new experiments that are possible. The AC does think there is value in many of the suggested experiments, if not to run, then just to acknowledge their possibility and leave for future work. The reviewers also point to some previous work that is very similar.  E.g. "Ease of Teaching and Language Structure from Emergent Communication", Funshan Li et al
This paper presents a method to improve the calibration of neural networks on out of distribution (OOD) data.  The authors show that their method can be applied post hoc to existing methods and that it improves calibration under distribution shift using the benchmark in Ovadia et al. 2019.  However, reviewers felt that the theoretical justification for why this works is unclear (see detailed comments by R1 and R4), and some of the choices are not well justified. Revising the paper to address these concerns with additional theoretical and/or empirical justifications should improve the clarity and strengthen the paper.  I encourage the authors to revise and resubmit to a different venue. 
This paper presents a very cool setup for multi task learning for learning fixed length representations for sentences.  Although the authors accept the fact that fixed length representations may not be suitable for complex, long pieces of text (often, sentences), such representations may be useful for several tasks.  They use a significantly large scale setup with six interesting tasks and show that learning generic representations for sentences across tasks is useful than learning in isolation.  Two out of three reviewers presented extensive critique of the paper and there s thorough back and forth between the reviewers and the authors.  The committee believes that this paper will add positive value to the conference.
The paper treats a relevant and challenging problem in sequential learning scenarios   how to detect distributional change over time when the pre  and post change distributions are not known up to certainty. All reviewers more or less acknowledge that the paper presents a new approach towards solving this inference problem, where the high level idea is to approximately learn the pre  and post change distribution parameters online using gradient descent and then apply well known tests for change detection (e.g., the Shiryaev or CUSUM rules) with these assumed to be the pre  and post change parameters.  However, beyond the concerns expressed by the reviewers, my finding after going through the manuscript myself is that the presentation of the paper s results leaves a lot to be desired in terms of clarity of exposition, comprehensiveness of performance benchmarking and comparison to existing approaches. Despite some of the reviwers  scores being revised upwards, the overall evaluation of the paper according to me is not adequate to merit acceptance, as per the concerns listed below.   1. There are two settings assumed in the paper (beginning of Sec. 2): (a) a completely Bayesian one, with the pre  and post change distributional parameters drawn from a prior \cal{F} and the change time lambda drawn from a prior pi, and (b) a minmax one, where everything is the same as in (a) except that there is no prior over the change time lambda. However, it is not at all clear, in the algorithm design of the paper, where the prior \cal{F} over the distributions is used in computing (or approximating) conditional probabilities such as P[lambda | v_alpha   n].  2. There seem to be meaningless (or ill defined) expressions in the paper s crucial portion motivating the algorithm design, such as P(X_t ~ f_{theta_0} | v_alpha   n), P(X_t ~ f_{theta_1} | v_alpha   n). It is hard to understand what the event "X_t ~ f_{theta_0}" even means   I find it impossible to relate it to a sample path property. This leads me to question the validity of the technical development in the paper.  3. Another undefined term is "r quickly" in eq. (4); I had to dig through the classical work of Lai, and Tartakovsky Veeravalli to get a formal definition for this term. This is not to be expected of a paper that attempts to develop a new change point detection procedure from scratch, especially to an audience (ICLR) that may largely be unfamiliar with classicalt change detection theory.   4. There are several technical statements made without adequate formal proof, e.g., "Given the optimal stopping time \nu, it s possible to evaluate the posterior distribution of the change point P(lambda t | v_alpha n), which in turn is a good classifier of the pre and post change observation". What the precise meaning of the term "classifier" is, what its "goodness" is, and how exactly it is related to the posterior distribution of lambda given the value of v_alpha, is formally not spelt out for a paper that largely uses formal probability language to develop its main results.  5. While I understand that the final algorithm to detect the change involves several approximations and heuristics along the way, which may very well be intuitively appealing, I do not understand (even after repeated passes over the submission) several key aspects   a concern also expressed by Reviewer 3. Why is it reasonable to assume that the conditional distribution of the change time lambda given the algorithm s stop time v_alpha would be logistic, and with the specific parameters mu and s given in the section "Distribution Approximation"? Moreover, it is hard to discern from the crucial Section 3.2 why the functions f_0^n, f_1^n should be useful in practice as proxies to the actual expected log likelihood ratios under the true parameters   despite Lemma 2 showing that they converge to the true expectations (again, the sense in which this convergence occurs is omitted leading to imprecision in the statement), the rates as a function of n, t_2 may be slow. I agree in this regard with the same concern voiced by Reviewer 1, and do not see a satisfactory explanation to it in the paper s discussion.  6. Comparison to literature. Contrary to the general picture painted in the paper about the lack of sufficient investigation of the "unknown pre and post change parameter" case, there does seem to be a rigorous body of work existing in this line that is not discussed in the manuscript. For instance, "SEQUENTIAL CHANGE POINT DETECTION WHEN THE PRE  AND POST CHANGE PARAMETERS ARE UNKNOWN", Lai and Xing, 2009, and "A BAYESIAN APPROACH TO SEQUENTIAL SURVEILLANCE IN EXPONENTIAL FAMILIES", Lai Liu Xing, 2009, are both works that address this very setting and in a comprehensive manner with theoretical guarantees. What the current manuscript does, in the context of both these works, is highly unclear. Is it trying to suggest an approximate way of computing the natural posterior distribution of the change time lambda given all data up to now, using the proxy P(lambda | v_alpha   n), or using a completely different approach altogether, is not adequately discussed at all, which makes the motivating arguments for the algorithm vague.  7. Finally, but in no lesser measure, the Experimental Results section features a rather narrow set of (two) scenarios for which it presents numerics. For a paper that claims to demonstrate "experimental results (over a wide variety of settings)" [from the author response], this is quite telling as it renders the argument in favor of the paper s approach quite weak. Here again, for the first (synthetic) setting, I do not understand the relevance of the neural network adopted to fix the parameters of a Gaussian distribution. Moreover, the reported distributions of the "regretted detection delay" seem to be quite wide for all the approaches compared (unknown params, adaptive, GLR), precluding a reasonable comparison of their performance. The author(s) would do well to expand the scope of both synthetic and non synthetic experiments to show the validity of their approach, and in each case carry out many more independent trials than just 500 for more accurate benchmarks.  I do note that more experimental results have been reported in the appendix, but I would presume that they have more value being in the main body after the algorithm design is explained in a more succinct and clearer manner. This can only come about by a significant rewriting and reorganizing of the paper, which I am confident the author(s) can carry out in order to make this into a much stronger submission. I wish the author(s) good luck on this, and hope to see the strengths of this new approach brought out in a more impactful manner in the next revision. 
Reviewers agree that the proposed method is interesting and achieves impressive results. Clarifications were needed in terms of motivating and situating the work. Thee rebuttal helped, but unfortunately not enough to push the paper above the threshold. We encourage the authors to further improve the presentation of their method and take into accounts the comments in future revisions.
The authors have addressed several of the issues raised by the reviewers, and they are strongly encouraged in include the additional experiments, and sections, that they propose, in a revised submission. The reviewers also recognized the novelty and extend of applications the proposed methodology has. Nevertheless, the paper would significantly benefit from a rigorous and thorough comparison to related work, placing it well within the context of the literature brought up by reviewers. Experimental comparisons to competitors, even if the latter address more restrictive settings, would strengthen the paper. Most importantly, the authors should consider including a comprehensive related work section, that convincingly discusses and compares to related/adjacent methods.
The paper proposes an architecture to learn over sets, by proposing a way to have permutations differentiable end to end, hence learnable by gradient descent. Reviewers pointed out to the computational limitation (quadratic in the size of the set just to consider pairwise interactions, and cubic overall). One reviewer (with low confidence) though the approach was not novel but didn t appreciate the integration of learning to permute with a differentiable setting, so I decided to down weight their score. Overall, I found the paper borderline but would propose to accept it if possible.
This is an intriguing work that introduces a novel sparse training technique. The core insight is a novel reparametrization or sparsity pattern based on the so called butterfly matrices that enables fast training and good generalization. The theory is solid and useful. Most importantly, the method is novel and is likely to become impactful. Understanding better what contributes to the excellent performance is an interesting question for future work. In agreement with all the reviewers, it is my pleasure to accept the work.
This paper proposes a network quantization method which is based on kernel level quantization. The extension from layer level to kernel level is straightforward, and so the novelty is somewhat limited given its similarity with HAQ. Nevertheless, experimental results demonstrate its efficiency in real applications. The paper can be improved by clarifying some experimental details, and have further discussions on its relationship with HAQ.
This paper is well written, addresses and interesting problem, and provides an interesting solution.
Investigating using other sensory inputs in our agents, and the impact on exploration is fascinating. We all want to see agents that use more sensory information.  As it stands the paper has several issues that require significant revision, most notably: (1) the polish, quality of the writing and clarity of the text is low, (2) the empirical results are based on 3 runs at this number we might not have enough data to form valid estimates of the std dev the error bars are not defined (see Henderson et al 2018), (3) in the ablation studies the hyper parameters are not tuned (as far as the text suggests) meaning the ablations results might be not representative of the utility of the method, (4) many missing details like hyper parameter tuning, number of runs in some cases, and reasonable descriptions of experiment protocols and baselines, (5) unsupported claims of causality.  Some of the issues were first raised during the discussion period, so another reviewer was brought in and provided a high quality review with many constructive comments. All reviewers reached clear agreement at the end of the discussion period. 
This paper studies the problem of multi agent meta learning. It can be viewed as extending Al Shedivat et al. (2018) by incorporating the dynamics of other agents. The reviewers praised clear writing and theory. There were two main concerns. The first concern is the novelty when compared to Al Shedivat et al. (2018). The second concern are experiments, which could be more ambitious and are not always clearly described.  The reviews of this paper were borderline and this was not enough to get accepted.
The SAC wrote a very good meta review and I just copy and paste it here. I completely agree with the SAC that the contribution of the paper due to the similarity to MME and MCD. Hopefully adding data augmentation to MCD and providing empirical results on new tasks can shed some lights to the community.    Based on a request from the authors, the SAC read the paper and the reviews and engaged two additional expert reviewers to provide an additional assessment.  The paper addresses semi supervised learning (SSL) and makes two contributions: 1) a method called χ model, which combines data augmentation with a two headed network that has an adversarial loss between the heads and the feature backbone; 2) an empirical evaluation on classification and regression SSL tasks.  As pointed out by reviewer 6Dgh, the proposed model is very similar to two existing methods: MME and MCD. These related works were not mentioned in the submission. During the rebuttal the authors compared to MME but not MCD.  Similarity to MME: The core idea in MME (Min Max Entropy) is to introduce a min max game between the head and backbone to regularize the model. The authors point out that one difference is that X model uses two heads and regression loss, while MME uses a single head and entropy loss. They also have other components like data consistency regularization, which are borrowed from prior work (FixMatch). They further point out that MME was evaluated and motivated for unsupervised domain adaptation whereas their paper evaluates on the same distribution (regular SSL). In general, there is a lot of overlap and borrowed techniques between UDA and SSL methods, but the experimental benchmarks tend to be different.  Similarity to MCD: A larger concern is that the proposed method is actually more similar to MCD than MME. In Eq.5, the data is augmented into two ways and two different output heads are applied. If the data is not augmented this way, the formulation is the same as MCD. The authors did not discuss this point, even though reviewers pointed out the similarity. From the perspective of the technical approach, the novelty wrt MCD appears to be mainly in adding the data augmentation. They do show in the paper that this type of min max regularization is effective in various regression tasks, which is a good empirical contribution. In the ablation studies of the data augmentation part, the difference over MCD is not very large, but the augmentation provides good gains in some experiments (although MCD is not mentioned, in Table 2 and 5, the "χ model (w/o data aug.)" is likely essentially MCD.)  Overall, the strength of the paper appears to be in adding data augmentation to MCD and providing empirical results on new tasks showing that it works on same distribution test sets and on regression tasks. The technical contribution seems somewhat limited, if accepted, the paper should include a clear discussion w.r.t MCD in the method and experiment sections. Furthermore, some baselines may potentially be missing, e.g. an MDD based (similar to MCD) UDA method for regression can be used as a baseline (Regressive Domain Adaptation for Unsupervised Keypoint Detection, CVPR 21).
The present work addresses the problem of opponent modeling in multi agent learning settings, and propose an approach based on variational auto encoders (VAEs). Reviewers consider the approach natural and novel empirical results area presented to show that the proposed approach can accurately model opponents in partially observable settings. Several concerns were addressed by the authors during the rebuttal phased. A key remaining concern is the size of the contribution. Reviewers suggest that a deeper conceptual development, e.g., based on empirical insights, is required.
This paper proposes to reduce the number of variational parameters for mean field VI. A low rank approximation is used for this purpose. Results on a few small problems are reported.  As R3 has pointed out, the main reason to reject this paper is the lack of comparison of uncertainty estimates. I also agree that, recent Adam like optimizers do use preconditioning that can be interpreted as variances, so it is not clear why reducing this will give better results.  I agree with R2 s comments about missing the "point estimate" baseline. Also the reason for rank 1,2,3 giving better accuracies is unclear and I think the reasons provided by the authors is speculative.  I do believe that reducing the parameterization is a reasonable idea and could be useful. But it is not clear if the proposal of this paper is the right one. Due to this reason, I recommend to reject this paper. However, I highly encourage the authors to improve their paper taking these points into account.
The reviewers have improved their scores after the rebuttal, and I agree that the work has value. It proposes a model driven data augmentation approach to environment invariant graph representation. Just like most data augmentation works in graph representation learning, the approach relies on graph proposal generator. The work has an implicit mechanism hidden in the graph generator (the authors  reply to a reviewer that "we target the extrapolation via a new invariant learning approach that is agnostic to specific GNN models" is a misunderstanding of why & how these types of methods work). The submission could significantly improve the introduction by properly describing the work w.r.t. other OOD efforts in graph representation learning. While the tasks of different works may be different (e.g., graph classification vs node classification), it is important to emphasize what each different approach brings to the table (rather than just state that the tasks are different). I hope the authors take this opportunity to improve the introduction. This work is more similar to the former OOD graph representation works than IRM & REX, since (without modeling assumptions) both IRM and REX require the support of the environments in test to be a subset of the ones in training. The present work does not need this support assumption since it uses an implicit mechanism in the graph generator.  The toy example in Section 3.1 is informative, thank you. The theory looks solid and easy to understand. The technical novelty is limited, since once the input graph has been decomposed into a set of ego graphs the definitions, formulations, and theory are all straightforward adaptations from the respective versions for IID data.   Minor:   In Assumption 2, m() should be defined inside the assumption.
This paper introduces an approach to model explainability on high dimensional data by: (1) first mapping inputs to a smaller set of intelligible latent features, and then (2) applying the Shapley method to this set of latent features. Several methods are considered for (1), and empirical results are examined across several settings.  Reviewers were mixed in their views   one reviewer was in favor of acceptance, and three were against.   Some concerns were addressed by authors in the discussion period but remaining issues include: concerns over the faithfulness of the approach;  missing comparisons to other related methods such as CaCE; and  a desire for more in depth discussion of the pros and cons of the different methods considered for (1).
This paper considers how to learn the structure of deep network by beginning with a simple network and then progressively adding layers and filters as needed. The paper received three reviews by expert working in this area. R1 recommends Weak Reject due to concerns about novelty, degree of contribution, clarity of technical exposition, and experiments. R2 recommends Weak Accept and has some specific suggestions and questions. R3 recommends Weak Reject, also citing concerns with experiments and writing. The authors submitted a response that addressed many of these comments, but R1 and R3 continue to have concerns about contribution and the experiments, while R2 maintains their Weak Accept rating. Given the split decision, the AC also read the paper. While we believe the paper has significant merit, we agree with R1 and R3 on the need for additional experimentation, and believe another round of peer review would help clarify the writing and contribution. We hope the reviewer comments will hep authors prepare a revision for a future venue.
This paper on extending MLNs using NNs is borderline acceptable: one reviewer is strongly opposed, although I confess I don t really understand their response to the rebuttal or see what the issue with novelty is (a position shared by the other reviewers). I m not sure how to weigh this review, but there is not a lot of signal in favour of rejection aside from the rating.  The remaining two reviews are in favour of acceptance, with their enthusiasm only bounded by the lack of scalability of the method, something they appreciate the authors are upfront about. My view is this paper brings something new to the table which will interest the community, but doesn t oversell the result.  Given the distribution of papers in my area, this one is just a little too borderline to accept, but this is primarily a reflection of the number of high quality papers reviewed and the limited space of the conference. I have no doubt this paper will be successful at another conference, and it s a bit of a shame we were not in a position to accept it to this one.
We thank the authors for their detailed answers and for providing an updated version of the paper addressing several of the issues raised by the reviewers, including new experimental results.  The paper is technically correct. The comparison with other methods is thorough and includes ablation studies clarifying the contributions of different aspects of the proposed method. One aspect that has been moderately addressed in the new version is the comparison between the "learned lambda" of the paper with a "tuned lambda" suggested by a reviewer. The authors added results where lambda is set to a particular value, however it would be more interesting relevant to consider a real "tuned lambda", i.e., a scalar parameter, shared by all vertices, that is optimized during training; the goal being to clarifying the benefit (if any) of parameterizing lambda as a function of the node, as opposed to a value shared by all nodes.  The paper is clearly written, particularly the revised version.  The novelty is the weakest aspect of the paper. While the specific problem of learning with noisy labels with GCNN may be new, the field of learning with noisy labels in general, and of using label propagation from clean labels to guide the prediction of uncertain labels, has been proposed before, and mentioned in the reviews. The specific instantiation of this idea to the GCNN framework is novel.  The significance of the work is rather positive. The revised version contains results on two real world datasets, where the proposed method outperforms several existing ones. As mentioned by a reviewer, this paper may inspire other researchers to explore in more depth the specific problems of learning with noise on graphs with GCNN, and to exploit the knowledge of a limited set of clean labels which may have practical importance to reduce human annotation efforts.  In summary, the paper proposes a novel model and demonstrates its potential to address a possibly important problem. Although the reviewers did not update their reviews, the authors  responses and updated version correctly addresses several of the initial concerns. The limited conceptual novelty compared to existing work did however not convince us to recommend acceptance, given the high selectivity of the conference.
This paper aims to study the dimension of the Class Manifolds (CM) which are defined as the region classified as certain classes by a neural network. The authors develop a method to measure the dimension of CM by generating random linear subspaces and compute the intersection of the linear subspace with CM. All reviewers agree that this is an interesting problem and worth studying.   However, there are major concerns. One question raised by several reviewers is that the goal of this paper is to analyze the dimension of the region that has the same output for the neural network; while the method and analysis are for a single datum. It is not clear if the obtained result is what the paper really aimed at. Another issue is the experimental results are different from that of local analysis. The dimension estimated by using the method in this paper is much higher.   Based on these, I am not able to recommend acceptance. But the authors are highly encouraged to continue this research. 
This work seeks to describe the heavy tail phenomenon observed for deep networks learned with SGD. The work presents proof of a relationship between curvature, step size, batch size, and a heavy tail weight distribution. The proofs assume a quadratic optimization problem and the authors speculate that the results may also be relevant for non convex deep learning settings. On the positive side the reviewers agreed that this work is one of the first, if not the first, to try to theoretically describe a poorly understood phenomenon in deep learning. On the less positive side, the reviewers believe that the proofs developed in this paper are for an idealized setting that is too different from the settings under which deep models are trained. As such, even though the authors provide some (somewhat mixed) experimental results to support the claim of relevance to deep learning, the reviewers were not convinced. Given that the stated goal of the work is to attempt to explain this phenomenon in deep models, the majority view is that this work, while promising, needs further development to convincingly claim some relevance to the original phenomenon being studied.
This is an interesting application area, but the quality of the presentation and experimental work here is not sufficient for acceptance. The numerical ratings from reviewers are just not high enough to warrant acceptance. 
This paper addresses stochastic semantic segmentation with a two step approach: a standard segmentation network learned with cross entropy serves as a guide to calibrate a second refinement network to generate diverse predictions while their expectation matches the calibration model.   The reviewers acknowledge the paper merits , e.g. the decoupling between the segmentation and generation networks. However, they also highlight serious concerns on the the clarity of the presentation, and the need for a consolidated evaluation.   The AC carefully reads the paper and the discussion among authors and reviewers. Despite improvements in paper presentation, the AC still considers that the paper would benefit from clarifications, e.g. the fact that the paper does not address calibration, and that stronger baselines as those mentioned by reviewers are needed for fully validating the approach.   Therefore, the AC recommends rejection. 
In this paper dense layers in deep neural networks representing policies are replaced by tensor regression layers, also by a scattering layer, and second order optimization is considered. The paper does not have a single consistent message, and combines different techniques for unclear reason. Important related work is not cited. The presentation was found unclear by the reviewers. 
This paper builds on top of Cycle GAN ideas where the main idea is to jointly optimize the domain level translation function with an instance level matching objective. Initially the paper received two negative reviews (4,5) and a positive (7). After the rebuttal and several back and forth between the first reviewer and the authors, the reviewer was finally swayed by the new experiments. While not officially changing the score, the reviewer recommended acceptance. The AC agrees that the paper is interesting and of value to the ICLR audience.
The paper presents a new problem: open set single domain generalization, where only one source domain is available and unknown classes and unseen target domains increase the difficulty of the task. To tackle this challenging problem, this paper designs a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. Then, the paper proposes a cross classifier consistency regularization that minimizes the multi binary classifier s output and one vs all multi class classifier s output.   The proposed OS SDG is an interesting and realistic problem. However, since it is way more challenging, the optimal solution to it remains elusive. Some reviewers think the method might be heuristic and lack theoretical guarantees. Nevertheless, the results are promising and the paper makes a first step toward the challenging OS SDG problem. Another concern is that the CCR loss needs more ablation studies to further analyze its role. Though the authors have added more explanation of this part, I suggest the authors put more ablation studies in the final supplementary document.   Overall, the paper is novel and interesting.  I would recommend acceptance of this paper given its novelty and impressive performance, but I highly suggest the authors add more ablation studies in the final supplementary, as suggested by the reviewers.
There s precious little work asking existential questions about adversarial examples, and so this work is most welcome. The work connects with deep results in probability to make simple and transparent claims about the inevitability of adversarial examples under some assumptions. The authors have addressed the key criticisms of the authors around clarity.
This paper proposes an approach to probabilistic time series forecasting based on combining autoregressive deep learning models with normalizing flows. In terms of strengths, time series forecasting is a fundamental problem. The proposed approach is a reasonable combination of existing model components that provides a flexible, end to end trainable framework for multivariate probabilistic forecasting. The experiments are well conducted and the results outperform recently published methods. While the reviewers raised a number of questions, all of the reviewers agree that their questions have be answered satisfactorily by the authors during the discussion and the paper should be accepted. The authors should be sure to incorporate the reviewer suggestions and author responses into the final paper. 
This paper empirically evaluates the performance (in time and accuracy) of randomized signatures for time series, an idea that was developed theoretically in a series of recent paper. While reviewers acknowledge that implementing and testing this idea is relevant, they also consider that the lack of methodological and theoretical novelty, combined with the fact that the experimental results do not convincingly show that randomized signatures outperform existing methods on a variety of tasks, puts the paper below the acceptance bar.
The paper presents a multi scale extension of the hourglass network. As the reviewers point out, the paper is below ICLR publication standard due to low novelty (i.e., multi scale extension is not a new idea) and significance (i.e., not a significant performance gain against the state of the art method or other baselines).
The article studies universal approximation for the restricted class of equivariant functions, which can have a smaller number of free parameters. The reviewers found the topic important and also that the approach has merits. However, they pointed out that the article is very hard to read and that more intuitions, a clearer comparison with existing work, and connections to practice would be important. The responses did clarify some of the differences to previous works. However, there was no revision addressing the main concerns.  
The average review ratings for this paper is somewhat borderline. The paper provides mathematical characterizations on when ReLU neural networks are injective. The paper has very nice ideas, but the reviewers also pointed out several key concerns:   1. “Given that the DSS condition takes exponential time to check, how do you check for injectivity of a given network?” 2. “But after you train a network using some training dataset, these matrices are no longer random or generic. How do you ensure that the network is still injective?” 3. “With leaky ReLU or flow model, global injectivity is automatically satisfied for non degenerate weight matrices, and in most applications, we don t see much difference”  I think points 2&3 are particularly important here. It seems that for practitioners, if injectivity is a key concern, then one can just use leaky relu with well conditioned weight matrices that guarantee injectivity. Note that well conditioning is easy to check and relatively easy to enforce. It’s unclear to the AC why one has to stick to particularly the relu activation and the recipe provided by corollary 2 and the paragraphs below Corollary 2. (It also seems to the AC that Corollary 2’s construction is fundamentally similar to the using leaky relu, but the AC is not quite sure.) Given that a much easier workaround (using leaky relu and full rank matrices) is available and is widely used in prior works (when it’s necessary), the AC, unfortunately, does not see that the paper could have a strong impact on the ML community and does not think the experimental results are sufficient to justify that this is a better idea than using leaky relu. In the AC’s opinion, the paper might be more compelling in a math venue.   
This paper proposes a new batching strategy for training deep nets. The idea is to have the properties of sampling with replacement while reducing the chance of not touching an example in a given epoch. Experimental results show that this can improve performance on one of the tasks considered. However the reviewers consistently agree that the experimental validation of this work is much too limited. Furthermore the motivation for the approach should be more clearly established.
This paper show that in several different neural network  architectures, recurrent   networks that share parameters over iterations have comparable  performance and similar features to feed forward networks of the same "effective depth".    Reviewers initially had some reservations about novelty and  generalizability to deeper SOTA networks.  These were successfully addressed by the authors and all reviewers feel the paper is above the bar due to the importance of the area, and that this paper brings together many important insights that, while many may have been known  before, had not previously been all brought together before.  The maze  task was also considered a useful task for the field.  I agree that the paper makes a worthwhile contribution and am in favor of  acceptance.
The paper describes a constrained optimization strategy for optimizing on an intersection of two manifolds.  Unfortunately, the paper suffers from generally weak presentation quality, with the technical exposition seriously criticized by two out of the three reviewers.  (The single positive review is too short and devoid of content to be taken seriously.  Even there, concerns are expressed.) This paper requires substantial improvement before it could be considered for publication.
The paper presents a masking strategy to introduce the locality bias into the vision transformers. The experiments show the effectiveness of considering such inductive bias. The reviewers agreed on the importance of the research question and the simplicity of the algorithm. MaiT also has a straight forward sparse attention extension that performs on the complexity of $O(n)$ rather than $O(n^2)$.  The reviewers also listed some common concerns of the paper:  (1) The novelty of such a masking approach is relatively low. I don t think the ALS or the soft masking adding too much contribution to that. Similar ideas have been explored in a number of papers.  (2) Reviewers also raise concerns about the experiments. Inductive biases often help more in small settings (fewer parameters and FGLOPs) and gain less in the large settings. When comparing with the STOA models, I think this is basically the trend shown in the paper as well. While I appreciate the authors’ efforts in including more comparisons, I have to say I really don’t think the performance gain is significant enough especially in the large settings. Needless to say that there are many other ways of encoding the same locality bias into the model.  Based on the reviewers  judgements and my own opinion, I therefore recommend rejection of this paper.
The reviewers raised a number of concerns including the usefulness of the presented dataset given that the collected data is acted rather than naturalistic (and the large body of research in affective computing explains that models trained on acted data cannot generalise to naturalistic data), no methodological novelty in the presented work, and relatively uninteresting application with very limited real world application (it remains unclear whether having better empathetic dialogues would be truly crucial for any real life application and, in addition, all work is based on acted rather than real world data). The authors’ rebuttal addressed some of the reviewers’ concerns but not fully (especially when it comes to usefulness of the data). Overall, I believe that the effort to collect the presented database is noble and may be useful to the community to a small extent. However, given the unrealism of the data and, in turn, very limited (if any) generalisability of the presented to real world scenarios, and lack of methodological contribution, I cannot recommend this paper for presentation at ICLR.
This paper presents a number of techniques to improve the existing non autoregressive end to end TTS model   FastSpeech. These techniques include replacing the teacher forcing with ground truth targets and using a variation adaptor to introduce auxiliary information such as duration, energy and pitch.  The experiments show that the proposed Fastspeech 2 model is faster in training  compared to the existing FastSpeech model and meanwhile can still achieve high quality synthesized speech.  The work reported in the paper is essentially about system improvement over FastSpeech but has it value in the speech community given the current interest in non autoregressive rapid TTS.  On the other hand, concerns are also raised regarding the complexity of the pipeline and the significance of the novelty. The authors  rebuttal is good and has addressed most of the concerns.  Overall, this is an interesting paper and can be accepted.  
The submission proposes a new GAN based method for translating from semantic maps of (synthetic) images/videos (from computer graphics) to photo realistic images/videos with the aid of edge maps. The main innovation is the inclusion of edge maps to the generator, where the edge maps are initially computed using the spatial Laplacian operator, and later output from their DNED network. According to the authors, the edge map allows them to generate images with fine details and to generate output images at higher resolutions.  The authors use their method to generate both single images as well as videos.   The submission received relatively low scores (2 rejects and 1 weak reject).  This was unchanged after the rebuttal (the authors did not submit a revised version of their paper).  The reviewers voiced concerns about the following: 1. Limited novelty All of the reviewers indicated that they felt the novelty of the proposed approach of not high as the work seemed to make only small modifications on prior work.  In the author response, the authors provided some details on where they felt their innovation to be.  The paper can be improved by building on those and having experiments/examples to probe those claims in more detail.   2. Application to other datasets The proposed method is demonstrated only on two datasets of driving scenarios (Cityscapes and Synthia).  It is unclear how the method will generalize to other types of inputs.  Experiments on other datasets will demonstrate whether the proposed approach can work well for other types of images.  3. The overall quality of the writing.   The overall quality of the writing is poor and hard to follow in places. The paper should also include more discussion of domain adaptation in the related work section.  It s possible that with improved writing that situates the work and explains the novel aspects of the work better, that the concern about limited novelty will be partially alleviated.   The paper also needs an editing pass as there are many grammar/spelling/capitalization issues.  Page 2: "We make Three"  > "We make three" Page 3: "as can bee seen in fig 3.2"  > "as can be seen in ..." (it s unclear which figure "fig 3.2" refers to, as figures are labeled Figure 1, Figure 2, etc) Page 4: Equation (3), symbol e is not explained (it is presumably the edge map) Page 7: "bellow"  > "below"  Overall, there are interesting elements in this paper and the reviewers noted that the generated results look good.  However, the paper will need to be improved considerably.   The authors are encouraged to improve their work and submit to an appropriate venue. 
The paper investigates out of distribution detection for regression tasks.  The reviewers raised several concerns about novelty of the method relative to existing methods, motivation & theoretical justification and clarity of the presentation  (in particular, the discussion around regression vs classification).   I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue. 
The paper presents a method for coarse and fine inference for question answering.  It originally measured performance only on WikiHop and then later added experiments on TriviaQA.  The results are good.  One of the concerns regarding the paper was the novelty of the work, and lack of enough experiments.  However, the addition of TriviaQA results allays some of that concern.  I d suggest citing the paper by Swayamdipta et al from last year that attempted coarse to fine inference for TriviaQA:  Multi Mention Learning for Reading Comprehension with Neural Cascades.  Swabha Swayamdipta, Ankur P. Parikh and Tom Kwiatkowski.  Proceedings of ICLR 2018.  Overall, there is relative consensus that the paper is good with a new method and some strong results.
The paper studies the problem of reinforcement learning under certain constraints on action sequences. The reviewers raised important concerns regarding (1) the general motivation, (2) the particular formulation of constraints in terms of action sequences and (3) the relevance and significance of experimental results. The authors did not submit a rebuttal. Given the concerns raised by the reviewers, I encourage the authors to improve the paper to possibly resubmit to another venue.
This work mainly applies wav2vec 2.0 to multilingual speech recognition and lacks of novelty.  The various pre training and fine tuning mix match are specific to the speech recognition task. As suggested by reviewers, it is recommended to resubmit to a speech conference.  Also the paper lacks comparisons to SOTA on one of the well studied task (i.e. BABEL) in the speech field.  The main factor for the decision is lack of novelty.
This paper presents U WILDS, an extension of the multi task, large scale domain shift dataset WILDS. The authors propose an extensive array of experiments evaluating the ability of a wide variety of algorithms to leverage the unlabelled data to address domain shift problems. The vision behind sounds quite ambitious and convincing to me, namely, the proposed U WILDS benchmark would be a useful and well motivated resource for the ML community, and their experiments were very comprehensive. Although they did not introduce any new methods in this paper, U WILDS significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real world unsupervised adaptation.  The clarity, vision and significance are clearly above the bar of ICLR. While the reviewers had some concerns on the novelty, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to strongly accept this paper for publication! Please include the additional rebuttal discussion in the next version.
This work proposes a novel metric for measuring calibration error in classification models.  Pros: * Novel calibration metric addressing limitations of previously used metrics such as ECE  Cons: * Limited experimental validation on CIFAR 10/CIFAR 100 only * Unclear impact beyond proposing a new calibration metric * Unclear value of using the proposed UCE metric for regularization and OOD detection  All reviewers appreciate the aim of the work to produce a calibration metric that addresses shortcomings of commonly used existing metrics such as expected calibration error (ECE), which is known to be sensitive to discretization choices.  However, all reviewers remain in doubt whether the proposed metric (uncertainty calibration error, UCE) is truly a better metric of calibration than previous proposals.  This doubt comes from two sources: 1. limited experiments that do not convincingly show the usefulness of UCE; and 2. interpretability of UCE not being as intuitive to the reviewers.  The experiments also use UCE as regularizer but the benefit of doing so over simple entropy regularization is not clear.  Overall the work is well motivated and written and the proposed UCE measure is interesting.  However, the reviewers remain unconvinced of the claimed benefits and the potential impact for measuring or improving calibration.
This paper improves MoCo based contrastive learning frameworks by enabling stronger views via an additional divergence loss to the standard (weaker) views. Three reviewers suggested acceptance, and one did rejection. Positive reviewers found the proposed method is novel and shows promising empirical results. However, as pointed out by the negative reviewer, the paper should have clarified about computational overheads of the method compared to the baseline (MoCo) in several aspects, e.g., their effective batch sizes or training costs, for the readers’ better understanding. As the concern was not fully resolved during the discussion phase, AC is a bit toward for reject. AC thinks the paper would be stronger if the authors include more ablations (and the respective discussions) regarding this point, e.g., CLSA multi (and  single) vs. MoCo v2 under the same training time, both at early epochs (~200; as reported in the author response) and longer epochs (after convergence; ~1000 and even more). 
All reviewers seems in favour of accepting this paper, witht he majority voting for marginally above acceptance threshold.  The authors have taken special heed of the suggestions and improved the clarity of the paper.  From examination of the reviews, the paper achieves enough to warrant publication.  My recommendation is therefore to accept the manuscript. 
The reviewers were clearly excited by the novel application of group theory to the problem of composition, and think the core idea is good.  However, the reviewers also expressed concern about the clarity of the paper, stating that in several places examples might help. Reviewers were also interested in seeing the work tied to real world applications, and how the work expands our existing knowledge about the composition of learned representations.  I hope their suggestions will help the authors to turn this into a stronger, clearer paper.
This work proposes new initialization and layer topologies for training a priori sparse networks. Reviewers agreed that the direction is interesting and that the paper is well written. Additionally the theory presented on the toy matrix reconstruction task helped motivate the proposed approach. However, it is also necessary to validate the new approach by comparing with existing sparsity literature on standard benchmarks. I recommend resubmitting with the additional experiments suggested by the reviewers.
This paper presents a package for "Dynamic Fine grained Structured Sparse Attention Mechanism" (DFSSATTEN), which aims to improve the computational efficiency of attention mechanisms by leveraging the specific sparse pattern supported by sparse tensor cores of NVIDIA A100. DFSSATTEN shows theoretical and empirical advantage in terms of performance and speedup compared to various baselines, with 1.27~1.89x speedup over the vanilla attention network across different sequence lengths.  Reviewers praised the simplicity of the method and the clean code implementation. Speeding up attention mechanisms is an important problem is leveraging sparse tensor cores for attention speedup is a sensible idea. The practical speedups are significant (1.27~1.89x over the vanilla attention across different sequence lengths). However, they also pointed out some weaknesses: the fact that the proposed method is very specific to the particular sparse pattern offered by NVIDIA A100, and not easily generalizable to other future hardware; the fact that the method focuses on inference acceleration and not training from scratch (not completely clear in the paper), which limits its scope; and the fact that the method still has O(N^2) complexity (it still requires the computation of QK^T, which has quadratic memory and computation cost), and therefore it does not really address the quadratic bottleneck of transformers, unlike other existing work in efficient transformers for long sequences.   I tend to agree with the reviewers and, even though the package can be potentially useful to other researchers, the scope seems limited and the paper seems a bit thin to deserve publication at ICLR.  Other comments and suggestions:   When talking about linear transformers, you should cite [1], which predates Performers   It is not clear to me why 1:2 and 2:4 are called "fine grained *structured* sparsity"   Citations for the systems in Tab 4 are missing   When comparing to other methods, it would be include to include their Pareto curves since those methods have tradeoffs in terms of sparsity / approximation error (or downstream accuracy).  [1] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret (https://arxiv.org/abs/2006.16236)
This is a nice paper on the classical problem of universal approximation, but giving a direct proof with good approximation rates, and providing many refinements and ties to the literature.  If possible, I urge the authors to revise the paper further for camera ready; there are various technical oversights (e.g., 1/lambda should appear in the approximation rates in theorem 3.1), and the proof of theorem 3.1 is an uninterrupted 2.5 page block (splitting it into lemmas would make it cleaner, and also those lemmas could be useful to other authors).
All the reviewers agree that the paper studies an important and interesting problem. However the reviewers felt the paper is still in preliminary stages, with incorrect derivations, missing comparisons/references,  and writing. While the authors updated the paper during the discussion stage addressing some of the concerns, the paper still needs more work in adding appropriate comparisons and in presenting the concepts more clearly. Hence I believe the paper is not yet ready for publication and encourage authors to continue their work.
This paper presents a framework for adversarial robustness by incorporating local and global structures of the data manifold. In particular, the authors use a discriminator classifier model, where the discriminator tries to differentiate between the original and adversarial spaces and the classifier aims to classify between them. The authors implement the proposed approach on several datasets and the experimental results demonstrate performance improvements. The idea of using the global data manifold into addressing robustness of the learning model is interesting. However, the technical contribution and novelty have not been explained very well.
A deep Bayesian generative model is presented for multi omics integration, using fused Gromov Wasserstein regularization between latent representations of the data views. The method removes several non trivial and practically important restrictions from an earlier method BayRel, enabling application in new setups, while still performing well.  Reviewers discussed the paper with the authors, resolving misunderstandings of the differences from earlier work (esp. BayReL). The authors reported more extensive experiments in the rebuttal, though not comparisons. The main remaining weakness is that the contributions are in a very narrow field, or at least aplications have only been demonstrated in the narrow field of multi omics data analysis. And even within that field, only in a narrow subfield. In a machine learning venue that is restrictive. Another issue is computational efficiency. The final decision then depends on how much weight we place on the novel contributions vs these weaknesses.
Strengths: This paper develops a method for learning the structure of discrete latent variables in a VAE.  The overall approach is well explained and reasonable.  Weaknesses: Ultimately, this is done using the usual style of discrete relaxations, which come with tradeoffs and inconsistencies.  Consensus: The reviewers all agreed that the paper is above the bar.
The paper received borderline ratings due to concerns regarding novelty and experimental results/settings (e.g. zero shot learning). On my side, I believe that the proposed method would need more evaluations on other benchmarks (e.g., SUN, AWA1 and AWA2) for both ZSL and GZSL settings to make the results more convincing. Overall, none of the reviewers championed this paper and I would recommend weak rejection.
This paper proposes a new optimization method for ReLU networks that optimizes in a scale invariant vector space in the hopes of facilitating learning. The proposed method is novel and is validated by some experiments on CIFAR 10 and CIFAR 100. The reviewers find the analysis of the invariance group informative but have raised questions about the computational cost of the method. These concerns were addressed by the authors in the revision. The method could be of practical interest to the community and so acceptance is recommended.
This paper considers the problem of identification of causal effects under the unsupervised domain adaptation setting. The authors assume the invariance of the causal structure and use it to regularize the predictor of causal effects. The method is interesting and looks effective, although this assumption may not hold always true (e.g., in some domains, some causal influences may disappear, leading to extra conditional independence relations). Hope the authors will update the paper to address the concerns raised by the reviewers, especially to conduct a sensitivity analysis of the framework to misspecification of the causal structure and make the motivation for the used evaluation metrics clear, and also provide a more thorough review of related work.
The paper received highly diverging scores: 5 (R1) ,9 (R2), 4(R3). Both R1 and R3 complained about the comparisons to related methods. R3 suggested some kNN and GP baselines, while R1 mentioned concurrent work using deepnets for trafffic prediction.  R3 is real expert on field. R2 and R1, not so. R2 review very positive, but vacuous. Rebuttal seems to counter R1 and R3 well.  It s a close all but the AC is inclined to accept since it s an interesting application of (graph based) deepnets.
The paper proposes the use of a state distribution estimation objective with a classic behavioral cloning objective for imitation learning.  The submission also proposes the use of a continuous normalizing flow training technique coined "denoising normalizing flow" to learn the state distribution. The authors experimentally validate their method on several MuJoCo continuous control benchmarks. The theorem 4.1 does validate the fact that this proposed objective is can be maximized by the target policy. However, the technical contributions (proposal of new objective and the denoising normalizing flow method) are marginal compared to previous work (e.g., SoftFlow or Energy Based Imitation Learning).  The empirical validation is lacking more extensive comparison with PWIL or NDI, which are more recent methods attempting to address the challenges described in the submission.  I m recommending this paper for rejecting for this conference.
The reviewers initially gave scores of 1,1,3 citing primarily weak empirical results and a lack of theoretical justification.  The experiments are presented on synthetic examples, which is a great start but the reviewers found that this doesn t give strong enough evidence that the methods developed in the paper would work well in practice.  The authors did not submit an author response to the reviewers and as such the scores did not change during discussion.  This paper would be significantly strengthened with the addition of experiments on actual problems e.g. related to drug discovery which is the motivation in the paper.
The reviewers all agree this is a well written and interesting paper describing a novel black box adversarial attack.   There were missing relevant references in the original submission, but these have been added.  I would suggest the authors follow the reviewer suggestions on claims of generality beyond CNN; although there may not be anything obvious stopping this method from working more generally, it hasn t been tested in this work.   Even if you keep the title you might be more careful to frame the body in the context of CNN s.
Main content:  Blind review #2 summarizes it well:  Summary: This paper deals with the representation degeneration problem in neural language generation, as some prior works have found that the singular value distribution of the (input output tied) word embedding matrix decays quickly. The authors proposed an approach that directly penalizes deviations of the SV distribution from the two prior distributions, as well as a few other auxiliary losses on the orthogonality of U and V (which are now learnable). The experiments were conducted on small and large scale language modeling datasets as well as the relatively small IWSLT 2014 De En MT dataset.  Pros: + The paper is well written with great clarity. The dimensionality of the involved matrices (and their decompositions) are clearly provided, and the approach is clearly described. The authors also did a great job providing the details of their experimental setup. + The experiments seem to show consistent improvements over the baseline methods (at least the ones listed by the authors) on a relatively extensive set of tasks (e.g., of both small and large scales, of two different NLP tasks). Via WT2 and WT103, the authors also showed that their method worked on both LSTM and Transformers (which it should, as the SVD on word embedding should be independent of the underlying architecture). + I think studying the expressivity of the output embedding matrix layer is a very interesting (and important) topic for NLP. (e.g., While models like BERT are widely used, the actual most frequently re used module of BERT is its pre trained word embeddings.)     Discussion:  The reviewers agree that it is a very well written paper, and this is important as a conference paper to illuminate readers.  The one main objection is that spectrum control regularization was previously proposed and applied to GANs (Jiang et al ICLR 2019). However the authors convincingly point out that the technique is widely used, not only for GANs, and that application to neural language generation has quite different characteristics requiring a different, new approach: "our proposed prior distributions as shown in Figure 2 in our paper are fundamentally different from the singular value distributions learned using their penalty functions (See Figure 1 and Table 7 in Jiang et al.’s paper). Figure 1 in their paper suggests that their penalty function, i.e., D optimal Reg, will encourage all the singular values close to 1, which is well aligned with their motivation for training GAN. However, if we use such penalty function to train neural language models, the learned word representations will lose the power of modeling contextual information, and can result in much worse results than the baseline methods."     Recommendation and justification:  I concur with the majority of reviewers that this paper is a weak accept. Though not revolutionary, it is well written, has usefully broad application, and is supported well empirically.
The paper investigates a very interesting problem of the connections between adversarial detection and adversarial classification. Theoretically, the authors show that one can always (ideally) construct a robust classifier from a robust detector that has equivalent robustness, and vice versa. This theorem is only correct without considering the computational complexity. However, the authors did not provide any approximate results of the reduction steps to verify the feasibility of the theorems in practice, which is the main concern of all reviewers. So we can say the paper is a reminder to the community we need to be careful about the detection results but did not provide any evidence to say they are overclaimed (only a conjecture based on the theorem in the paper) which greatly limits the contribution of the paper. Due to the competitiveness of ICLR, I cannot recommend accepting it.
The authors propose a novel approach to a dialog based  automated medical diagnosis, and present promising empirical results. The focus of this work is on robustness and reliability besides just the accuracy of diagnosis, which appears to be an important aspect in medical applications. The paper is clearly written and well motivated. However, in there are still several concerns raised by the reviewers, and the paper may require a bit of extra work to be ready for publication.
The paper proposes  a min/max reformulation for JKO gradient flows appealing the variational formulation of f divergences. This would alleviate the need of an explicit density. All reviewers pointed out the limited novelty in the work and the limited experimentation.   We encourage authors to add a theoretical analysis to their work and further strengthening of the experimental section with high dimensional experiments, and to resubmit the work on an upcoming venue.
The paper proposes a way to analyze overfitting to non relevant parts of the state space in RL and proposes a framework to measure this type of generalization error. All reviewers agree that the formulation is interesting and useful for practical RL.
This paper proposes a method for hierarchical decision making where the intermediate representations between levels of the hierarchy are interpretable. I personally really like this general direction, as did most of the reviewers. Unfortunately, it was felt that, even after discussion, this paper is not ready for publication. To summarize the general spirit of the objection to this paper, all reviewers found that the experimental section was faulty and did not match the claims of the paper. Specifically, criticism here surrounded first, the choice of experimental setting, which was not considered to be the best for testing interpretable hierarchical decision making approaches; and second, the choice of comparison/baselines, which did not give sufficient security that the results produced by the proposed approach were sufficiently impressive.  I am satisfied that the reviewers considered the paper fairly, gave constructive criticism, and took onboard the author feedback. As a result, I am recommending rejection. I nonetheless think that the high quality feedback provided here will enable the authors to prepare follow up experiments that may show their method in a more robust positive light, and encourage them to submit to a future conference, once armed with such results.
In this paper, the authors extended Q learning with UCB exploration bonus by Jin et al. to infinite horizon MDP with discounted rewards without accessing a generative model, and proved nearly optimal regret bound for finite horizon episodic MDP. The authors also proved PAC type sample complexity of exploration, which matches the lower bound up to logarithmic factors. Overall this is a solid theoretical reinforcement learning work.  After author response, we reached a unanimous agreement to accept this paper.
The main contribution of the paper is a novel parametrization of normalizing flows using ideas from optimal transport theory. This new parametrization allows viewing a normalizing flow as the gradient of a convex function, which leads to an efficient method for gradient estimation for likelihood estimation, using only access to convex optimization solvers. The paper is overall well written and provides a clean analysis. Theoretical results from the paper are supported by experiments. The paper was overall viewed favorably by the reviewers. 
Dear authors,  The reviewers appreciated the insights provided by your paper and the strong results. Congratulations. I encourage you to address the other points raised to make your final submission as complete as possible.
The author proposes a object oriented probabilistic generative model of 3D scenes.  The model is based on the GQN with the key innovation being that there is a separate 3D representation per object (vs a single one for the entire scene).  A scene volume map is used to prevent two objects from occupying the same space. The authors show that using this model, it s possible to learn the scene representation in an unsupervised manner (without the 3D ground truth).   The submission has received relatively low scores with one weak accept and 3 weak rejects.  All reviewers found the initial submission to be unclear and poorly written (with 1 reject and 3 weak rejects initially).  The initial submission also failed to acknowledge prior work on object based representations in the 3D vision community.  Based on the reviewer feedback, the authors greatly improved the paper by reworking the notation and the description of the model, and included a discussion of related work from 3D vision.  Overall, the exposition of the paper was substantially improved.  Some of the reviewers recognize the improvement, and lifted their scores.    However, the work still have some issues: 1. The experimental section is still weak The reviewers (especially those from an computer vision background) questioned the lack of baseline comparisons and ablation studies, which the authors (in their rebuttal) felt to be unnecessary. It is this AC s opinion that comparisons against alternatives and ablations is critical for scientific rigor, and high quality work aims not to just propose new models, but also to demonstrate via experimental analysis how the model compares to previous models, and what parts of the model is necessary, coming up with new metrics, baselines, and evaluation when needed.  It is the AC s opinion that the authors should attempt to compare against other methods/baselines when appropriate.  For instance, perhaps it would make sense to compare the proposed model against IODINE and MONet.  Upon closer examination of the experimental results, the AC also finds that the description of the object detection quality to be not very precise.  Is the evaluation in 2D or 3D?  The filtering of predictions that are too far away from any ground truth also seems unscientific.    2. The objects and arrangements considered in this paper is very simplistic.    3. The writing is still poor and need improvement. The paper needs an editing pass as the paper was substantially rewritten.  There are still grammar/typos, and unresolved references to Table ?? (page 8,9).   After considering the author responses and the reviewer feedback, the AC believe this work shows great promise but still need improvement.  The authors have tackled a challenging and exciting problem, and have provided a very interesting model.  The work can be strengthened by improving the experiments, analysis, and the writing.  The AC recommend the authors further iterate on the paper and resubmit.  As the revised paper was significantly different from the initial submission, an additional review cycle will also help ensure that the revised paper is properly fully evaluated.  The current reviewers are to be commended for taking the time and effort to look over the revision. 
The authors present an approach for learning graph embeddings by first fusing the graph to generate a new graph with encodes structural information as well as node attribution information. They then iteratively merge nodes based spectral similarities to  obtain coarser graphs. They then use existing methods to learn embeddings from this coarse graph and progressively refine the embeddings to finer graphs. They demonstrate the performance of their method on standard graph datasets.   This paper has received positive reviews from all reviewers. The authors did a good job of addressing the reviewers  concerns and managed to convince the reviewers about their contributions. I request the authors to take the reviewers suggestions into consideration while preparing the final draft of the paper and recommend that the paper be accepted.
This paper presents a new reinforcement learning based approach to device placement for operations in computational graphs and demonstrates improvements for large scale standard models.  The paper is borderline with all reviewers appreciating the paper even the reviewer with the lowest score. The reviewer with the lowest score is basing the score on minor reservation regarding lack of detail in explaining the experiments.  Based upon the average score rejection is recommended. The reviewers  comments can help improve the paper and it is definitely recommended to submit it to the next conference.  
Perturbation based methods often produce artefacts that make the perturbed samples less realistic. This paper proposes to corrects this through use of an inpainter.  Authors claim that this results in more plausible perturbed samples and produces methods more robust to hyperparameter settings.  Reviewers found the work intuitive and well motivated, well written, and the experiments comprehensive. However they also had concerns about minimal novelty and unfair experimental comparisons, as well as inconclusive results. Authors response have not sufficiently addressed these concerns. Therefore, we recommend rejection.
This work propose to learn hierarchical skill representations that, as opposed to prior work, consist of both discrete and continuous latent variables. Specifically, this work proposes to learn 3 level hierarchy via a hierarchical mixture latent variable model from offline data. For test time usage and adaptation on down stream tasks, the manuscript proposes two ways of utilizing the learned hierarchy in RL settings.   **Strengths** A novel method to learn hierarchical representations with mixed (discrete/continuous) latent variables is proposed Detailed experimental evaluation, and baseline comparisons, show promising results  **Weaknesses** There were various clarity issues as pointed out by the reviewers (fixed in rebuttal phase) The related work was missing relevant work, and the proposed framework was not connected well to existing work (fixed during rebuttal)  **Rebuttal** The authors significantly updated the manuscript based on the feedback of the reviewers, and improved both clarity of the manuscript (method+experiments) as well as the exposition of the proposed framework with respect to related work.  **Summary** After the rebuttal, all reviewers agree that this is a good paper that should be accepted. Thus my recommendation is accept.
This work presents a novel method h to learn object dynamics from unlabelled videos and shows its benefits on causal reasoning and future frame prediction.  This paper received 4 positive reviews and 1 negative review. In the rebuttal, the authors have addressed most of the concerns. AC feels this work is very interesting and deserves to be published on ICLR 2022. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make other necessary changes.
This paper is a computational linguistic study of the semantics that can be inferred form text corpora given parsers (which are trained on human data) are used to infer the verbs and their objects in text. The reviewers agreed that the work was well executed, and that the experiments comparing the resulting representations to human data were solid. The method employed has little or no technical novelty (in my opinion, not necessarily a flaw), and it s not clear what tasks (beyond capturing human data) representations could be applied to (again, not a problem if the goal is to develop theories of cognition).   The first draft of the work missed important connections to the computational linguistics literature, where learning about  affordances for verbs  (referred to as  selectional preferences ) has long been an important goal. The authors did a good job of setting out these connections in the revised manuscript, which the reviewers appreciated.   The work is well executed, and should be commended for relating ideas from different sub fields in its motivation and framing. But my sincere view is that it does not meet the same standards of machine learning or technical novelty met by other papers at this conference. It is unclear to me what the framing in terms of  affordance  adds to a large body of literature studying the semantics of word embeddings, given various syntactically and semantically informed innovations.  It feels to me like this work would have been an important contribution to the literature in 2013, but given the current state of the art in representation learning from text and jointly learning from text and other modalities, I would like to have seen some attempt to incorporate these techniques and bridge the gap between the notion of affordance in text/verbs (selectional preference) and Gibson s notion of object affordance (what you can do physically with an object) in experiments and modelling, not just in the discussion. Such a programme of research could yield fascinating insights into the nature of grounding, and the continuum from the concrete, which can be perceived and directly experienced, to the abstract, which must be learned from text. I encourage the authors to continue in this direction. An alternative is to consider submitting the current manuscript to venue where the primary focus is cognitive modelling, and accounting for human, behavioural data, and where there is less emphasis on the development of novel methods or models.  For these reasons, and considering the technical scope of related papers in the programme, I cannot fairly recommend acceptance in this case. 
This paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a tree structured autoencoder.  I side with the two more confident reviewers, and argue that this paper doesn t offer sufficient evidence that this method is useful in the proposed setting. I m particularly swayed by R1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed. I m also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a tree structured decoder encourages the model to generate grammatical sentences—I see no reason why this should be the case in the setting described here, and the paper doesn t seem to offer evidence to back this up.
The paper uses graph kernels to perform local convolutions and achieve better expressiveness than classical GNNs. The paper received three borderline reviews. The area chair found the feedback to be consistent and constructive and agrees with most statements made by the reviewers. Overall, the idea has some interest (even though there are other works who also propose hybrid approaches between graph kernels and GNNs, as noted in the paper). Nevertheless, there is a lot of room for improvement regarding the experimental validation and the results are not very convincing (yet?). The datasets used in the paper have been traditionally used for evaluating GNNs but they have strong limitations due to their small size and it is often hard to draw conclusions from them. If the method does not suffer from scalablity issues, it is likely that more interesting results could be obtained by using ZINC or MOLHIV datasets, which are larger and often provide statistically significant results.  Overall, these issues may require a major revision and unfortunately, the area chair believes that the paper is not ready for publication.
Mixup is very helpful when the training sample is scarce or has weak supervision. The paper studies how to adapt mixup to positive and unlabeled (PU) learning, a representative weakly supervised learning problem. By studying the specific properties of PU learning, the authors propose the concept of marginal pseudo negative instances, which are more likely to be positive but actually annotated by negative. A novel mixup variant has been proposed for PU learning by mixuping the marginal pseudo negative instances with the positive instance around the classification boundary. The effectiveness has been empirically shown.
The paper studies two aspects of personalized federated learning: (1) Clients having their own labeling scheme. (2) Domain heterogeneity across clients. They propose a way to collaborate across clients by similarity matching. The key novelty is to measure similarity of client pairs, based on on how much their representation layer agrees (measured with cosine similarity). A second novelty is a low rank factorization of model weights. Empirical evaluations show wins on MNIST, CIFAR10, 100.    Reviewers had various grave concerns. On the method side, they were concerned that thee is not enough theoretical insight and analysis of the proposed approach, esp. the kernel factorization and its effect.  On the empirical side, they were concerned that comparisons were not made with most recent baselines. There was a large number of PFL approaches published in 2021. e.g. FedBN.  Among these, its worth noting pFedHN (ICML2021) which actually discussed the case of heterogeneous (permuted) labels (their Sec 3.3).  In a discussion, reviewers appreciated the responses by the authors, the additional experiments and ablation studies. Unfortunately however, they found that the paper is not ready for publication in ICLR.
Pros:   novel, general idea for hard exploration domains   multiple additional tricks   ablations, control experiments   well written paper   excellent results on Montezuma  Cons:   low sample efficiency (2B+ frames)   unresolved questions (non episodic intrinsic rewards)   could have done better apples to apples comparisons to baselines  The reviewers did not reach consensus on whether to accept or reject the paper. In particular, after multiple rounds of discussion, reviewer 1 remains adamant that the downsides of the paper outweigh its good points. However, given that the other three reviewers argue strongly and credibly for acceptance, I think the paper should be accepted.
Summary:  This paper introduces a different, interesting definition of safety in RL. The paper does a nice job of showing success with empirical results and providing bounds. I think it provides a nice contribution to the field.  Discussion: The reviewers agree this paper should be accepted. The initial points brought up against the paper have been successfully addressed or mitigated. 
This paper aims to address the catastrophic overfitting issue in single step adversarial training. Specifically, this paper finds that 1) using larger random noise initialization and 2) avoiding clipping adversarial perturbations are the two keys for stabilizing single step adversarial training.   Overall, the reviewers find this paper is well written and the empirical results look promising. The reviewers originally misunderstood certain technical details of this paper, but got clarified in the discussion period.  However, the biggest concern shared by the reviewers is that the motivation of using larger random noise initialization and avoiding clipping adversarial perturbation is pretty unclear they all fail to (either empirically or theoretically) understand how and why these two techniques are helpful to preventing catastrophic overfitting. Given the main contribution of this paper is a revisiting of existing techniques, it is a legitimate concern from the reviewer side for demanding the in depth empirical analysis or the theoretical proof to help them better understand the proposed method; otherwise, the novelty contribution of this paper may get trivialized.   I encourage the authors to delve deeper into the proposed method and make a stronger submission next time.
There was a consensus among all the reviewers that the methodological contribution is not significant enough for publication at ICLR. In short, the main contribution of the paper is to include spatial modeling into a deep temporal point process model. However, to do that, they just use a well known method (KDE) on top of a methodology that is very similar to Du et al., KDD 2016.   In addition, in the original submission, the specific functional form for KDE was independent on time, as highlighted by the reviewers, which basically separates spatial and temporal modeling. Unfortunately, further experiments performed by the authors failed to show performance benefits on making it dependent on time.  The authors also claim that an additional contribution is the sampling method, however, this seems to thin of a contribution for a full paper.
This paper presents Yformer to perform long sequence time series forecasting based on a Y shaped encoder decoder architecture. Inspired by the U Net architecture, the key idea of this paper is to improve the prediction resolution by employing skip connection and to stabilize the encoder and decoder by reconstructing the recent past. The experiment results on two datasets named ETT and ECL partially showed the effectiveness of the proposed method.  Reviewers have common concerns about the overall technical novelty, presentation quality, and experiment details. The authors only provided a rebuttal to one reviewer and most concerns from the other three reviewers were not addressed in the rebuttal and discussion phase. The final scores were unanimously below the acceptance bar.   AC read the paper and agreed that, while the paper has some merit such as an effective Yformer model for the particular problem setup, the reviewers  concerns are reasonable and need to be addressed in a more convincing way. The weaknesses are quite obvious and will be questioned again by the next set of reviewers, so the authors are required to substantially revise their work before resubmitting.
There has been a recent focus on proving the convergence of Bayesian fully connected networks to GPs. This work takes these ideas one step further, by proving the equivalence in the convolutional case.  All reviewers and the AC are in agreement that this is interesting and impactful work. The nature of the topic is such that experimental evaluations and theoretical proofs are difficult to carry out in a convincing manner, however the authors have done a good job at it, especially after carefully taking into account the reviewers’ comments.  
This paper investigates an improvement to the direct feedback alignment (DFA) algorithm where the "backward weights" are learned instead of being fixed random matrices. The proposed approach essentially applies the technique of DFA to Kolen Pollack learning. While reviewers found the paper reasonably clear and thought the experiments were acceptable, there were significant concerns about the novelty of the approach and the fact that the proposed approach was a straightforward combination of existing ideas. Further, the paper could have done a better job situating (and applying) the proposed method to DFA variants that have been proposed since the original DFA paper came out.
Motivated by the possibility of Neural Architecture Search on domains beyond computer vision, this paper introduces a new search space and search method to improve neural operators. It applies the technique to problems in vision and text.  Reviewer 1 found the paper interesting and liked the motivation of considering different tasks in NAS. However, they found some aspects of the paper confusing and, like other reviewers, thought that the baselines were weak. The authors clarified some points, and R1 said that some, but not all, concerns were resolved. The reviewer improved their score by a point but still was not in favour of acceptance.  Reviewer 2 thought the paper was interesting but questioned its main claim: that it was proposing a search space over novel operators. They argued that what was discovered was similar to convolution and therefore not much had been gained. They questioned the significance of the ablation studies: there were a lot of them, but they focused on relatively simple tasks like MNIST and CIFAR 10. They also asked some clarifying questions which were answered by the authors. Pushing back on the point about the smaller scale of the experiments (in a general reply to all reviews), the authors said that the goal of their work was not advancing computer vision, but to push NAS beyond computer vision and simple search spaces to new application domains.  Reviewer 3 liked that the paper gave a good overview of the NAS problem and thought that it was ambitious. They also thought the approach was novel and promising. Like R2’s comment, R3 seemed disappointed that the search was over “reparameterized convolutions”. In fact, they thought that the paper was overselling its contribution. They pointed out that performance was still far from state of the art on the various benchmarks. The authors argued against this view of “reparameterizing convolutions” and claimed that the search space was, in fact, much larger than that of DARTS. R3 read and responded to the rebuttal, appreciating the response but ultimately thought that the search space wasn’t clear and comparisons fell short. Reviewer 4 shared similar pros & cons as have been pointed out by the other reviewers. They thought that operator search was limiting and that the paper should also consider topology. The authors responded to this, saying that they intentionally fixed the topology. Searching beyond operators was out of scope. R4 responded to the rebuttal though still had some remaining concerns both in terms of motivation and execution.  Multiple reviewers said that they would have considered the paper more favourably had an updated paper been submitted, addressing some of the original concerns. As it stands, all of the reviewers think that the paper has some merits but none believe after considering the author response, that the paper is ready for acceptance. I see no reason to overrule the consensus.
The covered topic is timely and of potential impact for many application domains, such as drug design. The paper is well written and presentation is clear. The proposed approach seems to have some degree of originality. Experimental results seem to be generally good, and in the rebuttal the authors have provided further experimental support to their main claim.  There are however some issues that have not been solved by the author’s rebuttal. I think two of them are the most important and related:   i) significance of contribution: although the authors have tried in the rebuttal to explain how the proposed approach differs from related papers, it seems that there are still doubts about the amount of innovation introduced by the paper. This issue could have been mitigated by SOTA experimental results in presence of a proper model selection, that, however does not seem to be the case here (see next point);  ii) model selection: the authors did not clearly explain the model selection procedure in the rebuttal. This is an important issue since it is often easy to get good results by picking the best run a posteriori. Unfortunately in the literature there are highly cited papers where model selection is not performed in a proper way and reviewers very often reject papers just looking at numbers without looking at how the numbers were obtained. So, I believe it is important to accept only papers where model selection is properly done and properly explained, so to allow for reproducibility of experiments. 
The paper proposes an optimization framework that automatically adapts the learning rates at different levels of a neural network  based on hypergradient descent.  The AC and reviewers all found the approach interesting and promising and appreciate the author feedback.  We strongly encourage the authors to incorporate the points and additional results provided in their response to the reviewers.  Additionally, some concerns remain to be addressed regarding initialization of hyper parameters of combination weights. Specifically it would be important to further investigate the impact of such initialization on the optimization performance. Furthermore, additional experiments with other network optimizers would be valuable as pointed out in the reviews.
This paper goes beyond the NTK setting in analyzing optimization and generalization in ReLU networks. It nicely generalizes NTK by showing that generalization depends on a family of kernels rather than the single NTK. The reviewers appreciated the results. One thing that is missing is a clear separation between NTK results and the ones proposed here. Although it is ok to defer this to future work, a discussion of this point in the paper would be helpful.
This paper revisits the problem of influence maximization and suggests using graph neural networks to estimate an upper bound on the influence, which can then be used to find good seed sets. The paper gives a variety of experimental evidence that the methods improve on various algorithms in the literature. There was a wide variation in opinions. Some reviewers felt that the overall idea was not particularly novel, as methods that combine graph embeddings and reinforcement learning to solve influence maximization have already been proposed in the literature. Additionally some reviewers felt that the experiments were missing important comparisons, particularly to learning based methods, without which it is difficult to argue that these methods really do advance the state of the art.
The paper proposes a method for object detection by predicting category specific object probability and category agnostic bounding box coordinates for each position that s likely to contain an object. The proposed idea is interesting and the experimental results show improvement over RetinaNet and other baselines. However, in terms of weakness, (1) conceptually speaking it s unclear whether the proposed method is a big departure from the existing frameworks; and (2) although the authors are claiming SOTA performance, the proposed method seems to be worse than other existing/recent work. Some example references are listed below (more available here: https://paperswithcode.com/paper/foveabox beyond anchor based object detector).   [1] Scale Aware Trident Networks for Object Detection https://arxiv.org/abs/1901.01892  [2] GCNet: Non local Networks Meet Squeeze Excitation Networks and Beyond https://arxiv.org/abs/1904.11492  [3] CBNet: A Novel Composite Backbone Network Architecture for Object Detection https://arxiv.org/abs/1909.03625  [4] EfficientDet: Scalable and Efficient Object Detection https://arxiv.org/abs/1911.09070  References [3] and [4] are concurrent works so shouldn t be a ground of rejection per se, but the performance gap is quite large. Compared to [1] and [2] which have been on arxiv for a while (+5 months) the performance of the proposed method is still inferior. Despite considering that object detection is a very competitive field, the conceptual/technical novelty and overall practical significance seem limited for ICLR. For a future submission, I would suggest that a revision of this paper being reviewed in a computer vision conference, rather than ML conference. 
This paper offers an alternative formulation of demographic parity, named GDP, which makes it amenable to easier computation when the sensitive attribute is continuous. Analytically, the paper relates GDP to other notions, offers ways to estimate GDP from data, and establishes the convergence of these estimators. Experimentally, the paper adds the estimated GDP as a learning regularizer and establishes the accuracy fairness tradeoff that results by using this method versus others.  The need to handle continuous sensitive attributes is well motivated since they are ubiquitous. The direction of the paper is thus very pertinent. The experimental exploration of the paper is also strong, though reviewers initially raised questions of clarity of the relationship of GDP with adversarial debiasing. These are mostly addressed by the authors. One weakness of the paper that largely remains is whether the paper offers new conceptual insights. Indeed, demographic parity is simply a notion of independence between an algorithm’s output and sensitive attributes. Other independence metrics are dismissed in the paper as unreliable to compute. However, one reviewer correctly raises the concern that *under similar regularity conditions* to the ones establishing the convergence of the kernel GDP estimator, it is also possible to establish convergence of other independence metrics, that would equally capture demographic parity. Another reviewer also points out that such convergence would follow using standard non parametric statistics techniques. Smoothed estimators of mutual information are indeed available in the literature, with convergence guarantees even in the high dimensional regime. The authors do not satisfactorily address this, casting doubt on the overall significance of the contribution.  That said, given the strong motivation behind the paper and the overall promise of the methodology, it may be worth sharing with the community. The authors are urged to address the above. Additionally, they are urged to be transparent about what the theory offers and what it doesn’t. For instance, the convergence results of GDP only tell us that we can use these estimators to audit the fairness of existing models. In other words, although the paper is touted as showing that GDP can be successfully used for learning, the evidence there is purely empirical: there is no learning guarantee simultaneously on the accuracy and fairness of GDP penalized risk minimization.
This work develops a methodology for exploration in deep Q learning through Thompson sampling to learn to play Atari games.  The major innovation is to perform a Bayesian linear regression on the last layer of the deep neural network mapping from frames to Q values.  This Bayesian linear regression allows for efficiently drawing (approximate) samples from the network.  A careful methodology is presented that achieves impressive results on a subset of Atari games.  The initial reviews all indicated that the results were impressive but questioned the rigor of the empirical analysis and the implementation of the baselines.  The authors have since improved the baselines and demonstrated impressive results across more games but questions over the empirical analysis remain (by AnonReviewer3 for instance) and the results still span only a small subset of the Atari suite.  The reviewers took issue with the treatment of related work, placing the contributions of this paper in relation to previous literature.  In general, this paper shows tremendous promise, but is just below borderline.  It is very close to a strong and impressive paper, but requires more careful empirical work and a better treatment of related work.  Hopefully the reviews and the discussion process will help make the paper much stronger for a future submission.  Pros:   Very impressive results on a subset of Atari games   A simple and elegant solution to achieving approximate samples from the Q network   The paper is well written and the methodology is clearly explained  Cons:   Questions remain about the rigor of the empirical analysis (comparison to baselines)   Requires more thoughtful comparison in the manuscript to related literature   The theoretical justification for the proposed methods is not strong
The authors presented a Federate Learning algorithm which constructs the global model layer wise by matching and averaging hidden representations. They empirically demonstrate their method outperforms existing federated learning algorithms  This paper has received largely positive reviews. Unfortunately one reviewer wrote a very short review but was generally appreciative of the work. Fortunately, R1 wrote a detailed review with very specific questions and suggestions. The authors have addresses most of the concerns of the reviewers and I have no hesitation in recommending that this paper should be accepted. I request the authors to incorporate all suggestions made by the reviewers. 
The paper builds on ideas in test time adaptation and test time normalization to improve performance under covariate shift. Concretely, the paper proposes (i) alpha BN, a method to calibrate batch statistics by mixing source and target statistics and (ii) test time adaptation using the CORE loss (which was proposed by Jin et al., 2020). The authors compare the the proposed approach to existing approaches on multiple benchmarks.   The reviewers found the idea interesting and appreciated the additional ablations. The main concerns were around novelty (as the idea is closely related to prior work in test time adaptation and normalization) and hyperparameter selection (e.g. how to choose alpha in practice).  Overall, the reviewers and I felt that the current version falls slightly below the acceptance threshold. I encourage the authors to revise and resubmit to another venue.   Minor comment about Appendix C (this didn t affect the score, just a suggestion for future revisions):  I think it might be interesting to include other alternatives to cross entropy that downweight easy examples, cf. focal loss https://arxiv.org/abs/1708.02002 and https://arxiv.org/abs/2002.09437. I m curious to see if CORE and focal loss consistently outperform cross entropy.
This paper focuses on using reference objects for long distance estimation by introducing a novel dataset and an attention based learning framework. While the presentation flows well and the methodology is practically useful, it is only marginally significant and novel. Some of the practical data augmentation aspect raise some question on whether the process wouldn t confuse the network   the authors provide empirical evidence of the contrary in their response, although I find the principled argument to be somewhat lacking.
The paper modifies DPMs by replacing the denoising L2 losses with GANs to learn the iterative denoising process. This leads to excellent results using a small number of refinement steps. In some sense, this also takes away one of the key advantages of DPMs over GANs, which is DPMs minimize a well defined objective function. Nevertheless, the results are convincing, but not spectacular. I am not convinced that we should continue to report training FID on CIFAR 10. I would have like to see class conditional ImageNet results. Also, it is not clear whether the proposed technique provides additional gains on top of SoTA GANs. Overall, I recommend acceptance as a spotlight.
This paper experiments with what is required for a deep neural network to be similar to the visual activity in the ventral stream (as judged by the brainscore benchmark). The authors have several interesting contributions, such as showing that a small number of supervised updates are required to predict most of the variance in the brain activity, or that models with randomized synaptic weights can also predict a significant portion of the variance. These different points serve to better connect deep learning to important questions in neuroscience and the presence of the paper at ICLR would create good questions. The discussion between authors and reviewers resulted in a unanimous vote for acceptance, and the authors already made clarifications to the paper. I recommend acceptance.
The reviewers agree that the paper is addressing an interesting problem (cold start for representation learning on dynamic graphs). However, the proposed methods can be improved by proposing more novel ideas. At the moment, the proposed methods is a combination of GCN model for node classification and GAE model for link prediction. In this case, some analysis or theoretical justification may make the paper more interesting. Furthermore, the reviewers think the experiments can be improved. For instance, results on more datasets, more comparison methods and a different setup will strengthen the paper. 
This paper regards video understanding as an image classification task, and reports promising performance against state of the arts on several standard benchmarks. Though the method is quite simple, it achieves good results. The visualization in this paper also provides good insight. All reviewers give positive recommendations for this paper.
In line with recent work in the NAS literature, the authors consider a weak NAS performance strategy to filter out bad architectures and narrow down the exploration to the most promising region of the search space. The authors propose to estimate weak predictors progressively by learning a series of weak predictors that can connect towards the best architectures. The authors provided a number of additional experiments during rebuttal, addressing most of the reviewers  comments convincingly and further showing the strong performance of their method. However, the authors should relate their work to Bayesian optimization, which comes in many flavors, and black box optimization techniques in general as their work shows a number of similarities, but is less principled.
The reviewers are not convinced by a number of aspects: including originality and clarity. Whereas the assessment of clarity and originality may be somewhat subjective (though the connections between margin based loss and negative sampling is indeed well known), it is pretty clear that evaluation is very questionable. This is not so much about existence of more powerful factorizations  (e.g., ConvE / HolE) but the fact that the shown baselines (e.g., DistMult) can be tuned to yield much better performance on these benchmarks.  Also, indeed the authors should report results on cleaned versions of the datasets (e.g., FB15k 237).  Overall, there is a consensus that the work is not ready for publication.  Pros:   In principle, new insights on standardly used methods would have been very interesting  Cons:   Evaluation is highly problematic   At least some results do not seem so novel / interesting; there are questions about the rest (e.g., assumptions)   The main advantage of sq loss methods is that it enables the alternating least squares algorithm, does not seem possible here (at least not shown) 
This paper makes the important, albeit somewhat unsurprising, finding, that cell based NAS search spaces, and in particular the DARTS search space, include some operations that are much better than others. Reducing the search space to these allows even random architectures to yield good performance, similarly to the findings of "Designing Network Design Spaces", https://openaccess.thecvf.com/content_CVPR_2020/html/Radosavovic_Designing_Network_Design_Spaces_CVPR_2020_paper.html  This paper received mostly positive scores (5,6,6,8). While I agree with the negative reviewer that it would be good to study this on other benchmarks as well, I follow the positive reviewers in recommending acceptance. I encourage the authors to fix the remaining typos (there are still many) and to open source their code. This would increase the paper s impact a lot.  Finally, I would like to ask the authors to avoid protraying the misconception that we don t need large and powerful search spaces. In fact, as already hinted on in Section 6, we *do* need larger and more exciting search spaces in order to discover entirely novel architectures. Also the multi objective nature of NAS is not to be undervalued, so the take away of the paper should *not* be that we should design NAS benchmarks with really small & strong search spaces, but that, given a specific problem and objective, it may be prudent to evaluate whether the whole power of a given NAS search space is needed or whether it can be reduced to its essential parts.
This paper presents an efficient secure aggregation algorithm in federated learning scenarios, which employs sparse random secure sharing clients. Four experienced reviewers left valuable comments on this paper, and three of them are unfortunately negative to this work (4, 4, 3) while one reviewer is slightly on the positive side.   The reviewers are generally positive about the main idea and the direction for this work, but they are not convinced of its mathematical soundness and practical benefits; the theoretical analysis and mathematical proof has been conducted only for simplified models while their practical advantage is not clear enough. Also, even the most positive reviewer (R3) is concerned about the novelty of the proposed approach. Although the concerns raised in the original reviews have been partially clarified during the discussion phase, there still remain several critical limitations, which makes this paper require (probably) multiple rounds of revision before publication and this AC has a reservation for accepting this paper.
This paper proposes a differentiable version of CEM, allowing CEM to be used as an operator within end to end training settings. The reviewers all like the idea   it is simple and should be of interest to the community. Unfortunately, the reviewers also are in consensus that the experiments are not sufficiently convincing. We encourage the authors to expand the empirical analysis, based on the reviewer s specific comments, and resubmit the paper to a future venue.
This paper proposes to improve VAE/GAN by performing variational inference with a constraint that the latent variables lie on a sphere. The reviewers find some technical issues with the paper (R3 s comment regarding theorem 3). They also found that the method is not motivated well, and the paper is not convincing. Based on this feedback, I recommend to reject the paper.
This work is effectively an extension of progressive nets, where the task ID is not given at test time. There were several concerns about novelty of this work and the evaluation being insufficient. There was a reasonable back and forth between the reviewers and authors, and the reviewers are all aligned with the idea that this work would need a substantial rewrite in order to be accepted at ICLR.
The reviewers appreciated the treatment of the topic of certifiable robustness done in this work and although they had a number of concerns, I feel they were adequately addressed by the authors.
The submission presents a semi parametric approach to motion synthesis. The reviewers expressed concerns about the presentation, the relationship to existing work, and the scope of the results. After the authors  responses and revision, concerns remain. The AC also notes that the submission is 10 pages long. The AC recommends rejecting the submission.
This paper addresses the problem of few shot classification across multiple domains. The main algorithmic contribution consists of a selection criteria to choose the best source domain embedding for a given task using a multi domain modulator.   All reviewers were in agreement that this paper is not ready for publication. Some key concerns were the lack of scalability (though the authors argue that this may not be a concern as all models are only stored during meta training, still if you want to incorporate many training settings it may become challenging) and low algorithmic novelty. The issue with novelty is that there is inconclusive experimental evidence to justify the selection criteria over simple methods like averaging, especially when considering novel test time domains. The authors argue that since their approach chooses the single best training domain it may not be best suited to generalize to a novel test time domain.   Based on the reviews and discussions the AC does not recommend acceptance. The authors should consider revisions for clarity and to further polish their claims providing any additional experiments to justify where appropriate. 
This paper uses energy based model to interpret standard discriminative classifier and demonstrates that energy based model training of the joint distribution improves calibration, robustness, and out of distribution detection while generating samples with better quality than GAN based approaches. The reviewers are very excited about this work, and the energy based perspective of generative and discriminative learning. There is a unanimous agreement to strongly accept this paper after author response.
The reviewers all appreciated the novel concept behind the work. I agree with this, I think the principles behind the work are novel and interesting, and I would encourage the authors to improve the validation of this method and publish it in the future.  However, reviewers also raised a number of issues with the current paper: (1) the evaluation appears a bit preliminary, and could be improved significantly with additional datasets and more ablations/comparisons; (2) it s not clear if the improvements from the method are especially significant; (3) the writing could be improved (I do see that the authors made a significant number of changes and improved parts of the paper in response to reviewer concerns to a degree). Probably the writing issues could be fixed, but the skepticism about the experiment results seems harder to address, and while I recognize that the authors made an effort to point some existing ablations in the paper that do address parts of what the reviewers raised, I do think that in the balance the experimental results leave the validation of the work as somewhat borderline.  While less important for the decision, I found that the paper is somewhat overselling the contribution in the opening   while the particular concept of using gradients as features in this way is interesting, similar ideas have been proposed in the past, and the paper would probably be better if it was more clearly positioned in the context of prior work rather than trying to present a new "framework" like this. It kind of feels like it s biting off too much in the opening, and then delivering a comparatively more modest (but novel and interesting!) technical component.
This paper studies constructing text2text transformer models that are good at zero shot task generalization via multi task learning over a diverse set of NLP tasks. One main contribution of the work is to create prompt templates for various NLP tasks (that are of different task formats) such that all tasks can be framed into text2text learning format and that is "natural" to the pretrained T6 model. The paper conducts extensive experiments to demonstrate the promising zero shot generalization ability of such multi task learner.  Strength:   Important problem setup that has broad applications   Extensive experiments to validate the claims   Useful resources are developed for the problem  Weakness:   Good to study the effect of using different combination of training tasks on the downstream zero shot generalization, which can shed some light on the usefulness of upstream tasks   Justification of "true zero shot learning" capability would require further experiments on analyzing the data overlap between MTL datasets (and also T5 pertaining task data) and the unseen task data.   Some more discussion on the task split and categorization will be helpful.
Reviewers had several concerns about the paper, primary among them being limited novelty of the approach. The reviewers have offered suggestions for improving the work which we encourage the authors to read and consider.
This paper provides an interesting benchmark for multitask learning in NLP. I wish the dataset included language generation tasks instead of just classification but it s still a step in the right direction. 
The paper proposes an interesting idea of inserting Gaussian convolutions into ConvNet in order to increase and to adapt effective receptive fields of network units. The reviewers generally agree that the idea is interesting and that the results on CityScapes are promising. However, it is hard not to agree with Reviewer 3, that validation on a single dataset for a single task is not sufficient. This criticism is unaddressed. 
The paper shows how meta learning contains hidden incentives for distributional shift and how a technique called context swapping can help deal with this. Overall, distributional shift is an important problem, but the contributions made by this paper to deal with this, such as the introduction of unit tests and context swapping, is not sufficiently clear. Therefore, my recommendation is a reject.
This is an interesting paper that provides modeling improvements over several strong baselines and presents SOTA on Squad.  One criticism of the paper is that it evaluates only on Squad, which is somewhat of an artificial task, but we think for publication purposes at ICLR, the paper has a reasonable set of components.
This paper analyzes analyze the fairness of Integrated Gradient based attribution methods. The authors exploit SHAP and BShap, two approaches based on the theory of Shapley Values, as the reference of "fair" methods. Specifically, they present an "attribution transfer" phenomenon in which the Integrated Gradients are affected by some sharply fluctuated area across the integration path, thereby deviating from the  fair  attribution methods. To avoid the attribution transfer issue, they further propose Integrated Certainty Gradients (ICG) method, where the integration path does not pass through the original fluctuated input space. Experiments are performed to demonstrate the advantages of ICG in avoiding attribution transfer. While the basic premise of the work is interesting, many conceptual details remain unclear and experimental evaluation can also be improved (please see detailed reviewer comments below). Given this, we are unable to recommend acceptance at this time. We hope the authors find the reviews helpful.
The article is easy to read, of interest for the community, and provide some advance towards understanding the implicit bias of gradient descent. The results and the methodology for the rank 1 case are very interesting and convincing. Yet, some results could be made more explicit and the comments by the reviewers should be addressed for the camera ready paper, in particular the one on the organization. 
This paper proposed a mixup inference (MI) method, for  mixup trained models, to better defend adversarial attacks.  The idea is novel and is proved to be effective on CIFAR 10 and CIFAR 100.  All reviewers and the AC agree to accept the paper.
This paper proposes a novel differentiable digital signal processing in audio synthesis. The application is novel and interesting. All the reivewers agree to accept it. The authors are encouraged to consider the reviewer s suggestions to revise the paper.
The article proposes an approach to alter the approximate posterior distribution in order to remove some of the information (unlearning). The approach is applicable when the approximate posterior is obtained via (stochastic gradient) MCMC methods. Unlearning is done by shifting the approximate true posterior by some value delta, which is found via optimisation with an influence function.  The approach is novel, tackles an important problem and is mathematically sound. Reviewers have highlighted some of its limitations. In particular, the modified posterior is obtained by translating the original posterior, without changing its shape; many aspects of the data may therefore not be forgotten. The authors have partially addressed this concern in their response and provided additional experiments. Although there is still disagreement amongst reviewers, I recommend acceptance.   A Minor comment: I would not present MCMC merely as a "machine learning algorithm" (p.1), nor as a "sampling based Bayesian inference method" (p.1 and p.2). MCMC is a generic approach to approximate high dimensional integrals and obtain samples approximately sampled from some target distribution, dating back from the work of Metropolis (1953) and Hastings (1970). Its application to Bayesian inference/machine learning problems came much later, see e.g. the excellent review of C. Robert and R. Casella: A short History of MCMC: Subjective recollections from incomplete data. Statistical Science, 2011.
The topic of macro actions/hierarchical RL is an important one and the perspective this paper takes on this topic by drawing parallels with action grammars is intriguing. However, some more work is needed to properly evaluate the significance. In particular, a better evaluation of the strengths and weaknesses of the method would improve this paper a lot.
We appreciate the authors for engaging in discussions with the reviewers and providing further experimental results to clarify and address the concerns raised by them in their original reviews, leading to changes in some of the recommendations.  While the (revised) paper with the clarifications and new results incorporated are more ready for publication, some outstanding concerns should preferably be addressed to further enhance its quality.  The authors are highly recommended to take into consideration all the comments and suggestions of the reviewers to further revise their paper to make it a scholarly work to contribute to the ICLR and the more general ML community.
In this paper, authors introduce two properties of feature representations, namely local alignment and local congregation, and show how these properties can be predictive of downstream performance. The paper has a heavier focus on providing theoretical statements using these properties but authors also empirically evaluate their suggested method.  **Strong Points**:   The paper is well written and easy to follow.     The proposed concepts (local alignment and local congregation) are intuitive.     The theoretical statements and their proofs are correct.     The proposed metric shows some advantage against a few baselines.     Prior work on feature representations and transferability are discussed.   **Weak Points**:     **The connections to prior work on K nearest neighbors and linear classifiers are not properly discussed.** This is very important because authors assume that the network that outputs the feature representations is trained on a different data and they reduced the analysis to that of a binary linear classifier. Hence, all classical learning theory results on binary classifiers apply in this setting. Furthermore, KNN methods and analysis can be simply applied on the features as well. In light of this and the lack of discussion on this matter, the significance of the theoretical and empirical results are not clear.     **The main proposed properties could be improved further**. It looks like the defined properties (local alignment and local congregation) could be improved by merging them into one property about separability of data? The current properties are sensitive to scaling which is undesirable given that classification performance is invariant to scaling of the features. It seems like local congregation is mostly capturing the scale so some normalized version of local alignment might be able to capture the main property of interest.     **The theoretical results in their current form are not very significant.** One limiting factor on the theoretical results is that since the analysis is done only on the classification layer, it does not say anything about the relationship of the upstream and downstream tasks. But perhaps the most important limitation is that the properties are defined based on the downstream task distribution as opposed to downstream training data. That makes it difficult to measure them in practical settings where we have a limited number of data points. Classical results on learning theory avoid this and only use measures that depend on the given training set.     **The empirical evaluation could benefit from stronger baselines** Authors mentioned "We therefore consider only baselines that make minimal assumptions about the pre trained feature representation and the target task" and hence avoided comparing to many prior methods. However, I think the appropriate approach would be to compare the performance of the proposed method to strong baselines but then explain how they differ in terms of their assumptions, etc. Moreover, there are other simple heuristic baselines to consider, eg. K NN (which is not computationally expensive in the few shot settings) or a classifier that is trained by initializing it to be the sum of feature vectors in the first class (assuming binary classification) minus sum of feature vectors in the second class and doing a few SGD updates on it. Therefore, I believe authors could improve the empirical section significantly by taking these suggestions into account.   **Final Decision Rationale**:  This is a borderline paper. While the paper has a nice combination of theoretical and empirical contributions, both theoretical and empirical contributions have a lot of room for improvement (and a clear path to get there) as pointed above. In particular, I believe having either strong theoretical contributions or strong empirical contributions would have been enough for acceptance and I hope authors would take the above suggestions into account and submit the improved version of this work again!
The paper provides a nice approach to optimizing marginals to improve exploration for RL agents.  The reviewers agree that its improvements w.r.t. the state of the art do not merit a publication at ICLR.  Furthermore, additional experimentation is needed for the paper to be complete.
The paper provides a constrained mutual information objective function whose Lagrangian dual covers several existing generative models. However reviewers are not convinced of the significance or usefulness of the proposed unifying framework (at least from the way results are presented currently in the paper). Authors have not taken any steps towards revising the paper to address these concerns. Improving the presentation to bring out the significance/utility of the proposed unifying framework is needed.
Four experts reviewed this paper and all recommended rejection. There was no rebuttal. The reviewers raised many concerns regarding the paper, such as missed citations, lack of comparison with related methods, and some presentation issues. Considering the reviewers  concerns, we regret that the paper cannot be recommended for acceptance at this time.  The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The reviewers have uniformly had significant reservations for the paper. Given that the authors did not even try to address them, this suggests the paper should be rejected.
This paper proposes to combine FMs and GNNs. All reviewers voted reject, as the paper lacks experiments (eg ablation studies) and novelty. Writing can be significant improved   some information is missing. Authors did not respond to reviewers questions and concerns. For this reason, I recommend reject.  
This paper presents a more complex version of the grammar VAE, which can be used to generate structured discrete objects for which a grammar is known, by adding a second  attribute grammar , inspired by Knuth.  Overall, the idea is a bit incremental, but the space is wide open and I think that structured encoder/decoders is an important direction.  The experiments seem to have been done carefully (with some help from the reviewers) and the results are convincing.
Description: The paper presents a patch based 3D representation of man made shapes that can be computed with deep learning and used directly in existing CAD applications. This representation is based off a deformable parametric template with Coons patches. Results in sketch based modelling tasks shows comparable results with STOA  Strengths:   The patch based representation provide several advantages: compact, sparse, interpretable, consistent and easily editable.   Can infer the right template, and thus does not require manually created templates  Weaknesses   Limited evaluation restrained to mostly sketch based modelling, and missing evaluation against a few STOA methods  The paper has introduced a very impactful new representation for 3D shapes and has strong technical novelty. I recommend, as reviewers have suggested, more in depth quantitative evaluation (against other work and ablation studies)
The paper describes a cool application of online learning from bandit feedback   creating personalized, adaptive typing interfaces for users with sensorimotor impairments. The problem is well motivated   the interface can observe users  gaze (e.g. via a webcam image), predict a character as an action, and bandit feedback can be collected by observing whether users use the backspace key after the interface s action. Prior work showed that gaze to text can be less burdensome than typing, but this can quickly become untrue the more mistakes the interface makes. So, the goal is to personalize the interaction policy so that it makes fewer mistakes than the default interaction policy trained using a fixed dataset of expert demonstrations.  The high point of the paper is the empirical user study with 12 60 participants   the study convincingly demonstrates that indeed a simple bandit algorithm can improve over the default interface; moreover, users exhibit intriguing co adaptation patterns with the adaptive interfaces. These findings may prove to be an interesting point for future studies in user co adaptation.  The low point of the paper is its algorithmic development. There is a vast literature on bandit/RL algorithms, and incorporating human feedback into their operation (the paper rightly cites TAMER, COACH, etc.) but it is very unclear why any one of these algorithms could not be used for the paper s application. COACH (human feedback gives an explicit view of the action s advantage   which in the contextual bandit setting exactly matches the paper s assumptions) seems particularly appropriate. Although the algorithm proposed in the paper is simple, how applicable is it in any other context? how does it compare to COACH/etc.? when should we prefer this algorithm over others? Furthermore, given that X2T trains a reward model from observed user behavior, a natural baseline would use an epsilon greedy strategy (fraction of the time, pick actions greedily according to the reward model)   this might isolate the benefit of the approximately Boltzmann exploration being conducted on top of the reward estimates in Eqn 2. Finally, since X2T trains a reward model per user it could be particularly informative to visualize what the models have learned to illustrate qualitatively how X2T is personalizing across its user base.  The paper could have a much bigger impact if the authors can figure out some creative way to enable the broader research community to work on this problem domain. A testbed or environment (like RecSim for content recommendation https://github.com/google research/recsim) with configurable but realistic reward models could allow researchers to test several bandit algorithms, MDP vs CB formulations, other ways to interpret user feedback etc. 
This is a well written paper that shows how to use optimal transport to perform smooth interpolation, between two random vectors sampled from the prior distribution of the latent space of a deep generative model. By encouraging the marginal of the interpolated vector to match the prior distribution, these interpolated distribution preserving random vectors in the latent space are shown to result in better image interpolation quality for GANs. The problem is of interest to the community and the resulted solutions are simple to implement.   As pointed out by Reviewer 1, the paper could be made clearly more convincing by showing that these distribution preservation operations also help perform interpolation in the latent space of VAEs, and the AC strongly encourages the authors to add these results if possible.   The AC appreciates that the authors have added experiments to satisfactorily address his/her concern:  "Suppose z_1,z_2 are independent, and drawn from N(\mu,\Sigma), then t z_1 + (1 t)z_2 ~ N(\mu, (t^2+(1 t)^2)\Sigma). If one lets y | z_1, z_2 ~ N(t z_1 + (1 t)z_2, (1 t^2 (1 t)^2)\Sigma) as the latent space interpolation, then marginally we have y ~ N(\mu, \Sigma). This is an extremely simple and fast procedure to make sure that the latent space interpolation y is highly related to the linear interpolation t z_1 + (1 t)z_2 but also satisfies  y ~ N(\mu, \Sigma)."  The AC strongly encourages the authors to add these new results into their revision, and highlight "smooth interpolation" as an important characteristic in addition to "distribution preserving." A potential suggestion is changing "Distribution Preserving Operations" in the title to "Distribution Preserving Smooth Operations." 
The submission proposes a  co natural  gradient update rule to precondition the optimization trajectory using a Fisher information estimate acquired from previous experience. This results in reduced sensitivity and forgetting when new tasks are learned.   The reviews were mixed on this paper, and unfortunately not all reviewers had enough expertise in the field. After reading the paper carefully, I believe that the paper has significance and relevance to the field of continual learning, however it will benefit from more careful positioning with respect to other work as well as more empirical support. The application to the low data regime is interesting and could be expanded and refined in a future submission.   The recommendation is for rejection.
The proposed “input forgetting” problem is interesting, and the reflective likelihood can come to be seen as a natural solution, however the reviewers overall are concerned about the rigor of the paper. Reviewer 2 pointed out a technical flaw and this was addressed, however the reviewers remain unconvinced about the theoretical justification for the approach. One suggestion made by reviewer 1 is to focus on simpler models that can be studied more rigorously. Alternatively, it could be useful to focus on stronger empirical results. The method works in the experiments given, but for example in the imbalanced data experiments, only MLE is compared to as a baseline. I think it would be more convincing to compare against stronger baselines from the literature. If they are orthogonal to the choice of estimator, then it would be even better to show that these baselines + RLL outperforms the baselines + MLE. Alternatively, you mention some challenging tasks like seq2seq, where a convincing demonstration would greatly strengthen the paper. While the paper is not yet ready in its current form, it seems like a promising approach that is worth further exploration.
This paper introduces a new way to estimate gradients of expectations of discrete random variables by introducing antithetic noise samples for use in a control variate.  Quality:  The experiments are mostly appropriate, although I disagree with the choice to present validation and test set results instead of training time results.  If the goal of the method is to reduce variance, then checking whether optimization is improved (training loss) is the most direct measure.  However reasonable people can disagree about this.  I also think the toy experiment (copied from the REBAR and RELAX paper) is a bit too easy for this method, since it relies on taking two antithetic samples.  I would have liked to see a categorical extension of the same experiment.  Clarity:  I think that this method will not have the impact it otherwise could because of the authors  fearless use of long equations and heavy notation throughout.  This is unavoidable to some degree, but 1) The title of the paper isn t very descriptive 2) Why not follow previous work and use \theta instead of \phi for the parameters being optimized? The presentation has come a long way, but I fear that few besides our intrepid reviewers will have the stomach.  I recommend providing more intuition throughout.  Originality:  The use of antithetic samples to reduce variance is old, but this seems like a well thought through and non trivial application of the idea to this setting.  Significance:  Ultimately I think this is a new direction in gradient estimators for discrete RVs.  I don t think this is the last word in this direction but it s both an empirical improvement, and will inspire further work.
The paper provides a theoretical analysis of the recent and popular Generative Adversarial Imitation Learning (GAIL) approach. Valuable new insights on generalization and convergence are developed, and put GAIL on a stronger theoretical foundation. Reviewer questions and suggestions were largely addressed during the rebuttal.
The submission evaluates the relationship between (logarithmic) Dice loss and cross entropy loss, arguing for a similar decomposition into ground truth and "hidden label marginal biases."  The submission received mixed reviews, with two reviewers voting for rejection, and two feeling that it is marginally above the acceptance threshold.  Setting aside the numerical scores, there are reasons to believe that this submission, while interesting, has shortcomings that limit its relevance to the wider ICLR community.  These include   Very many losses have been proposed for imbalanced classification / (medical) image segmentation, such as Jaccard and Tversky index or ranking measures, although admittedly Dice is probably the most popular in the medical imaging literature due to historic reasons.  Arguably, Dice is less well behaved from a theoretical perspective compared to other options (e.g. it does not even form a metric), and may not be the most relevant point of departure for a representation learning conference.  The literature review misses many relevant papers on such losses, including papers that specifically are focused on the relationship between Dice and cross entropy, e.g. Eelbode et al., IEEE TMI 2020 and citations therein.   The empirical results do not show substantially improved results compared to baselines.  On the balance, this does not cross the threshold for acceptance to a competitive venue such as ICLR.
The paper proposes Variational Beta Bernoulli Dropout,, a Bayesian method for sparsifying neural networks. The method adopts a spike and slab pior over parameter of the network. The paper proposes Beta hyperpriors over the network, motivated by the Indian Buffet Process, and propose a method for input conditional priors.  The paper is well written and the material is communicated clearly. The topic is also of interest to the community and might have important implications down the road.  The authors, however, failed to convince the reviewers that the paper is ready for publication at ICLR. The proposed method is very similar to earlier work. The reviewers think that the paper is not ready for publication.
The paper under review provides a theoretical analysis for contrastive representation learning. The paper proposes a guarantee on the performance (specifically upper and lower bounds) without resorting to previously used conditional independence assumptions. Throughout, the theoretical results and assumptions are supported by experiments.   After a lively discussion, and after changes made to the paper in the revision stage, all four reviewers recommend this paper for acceptance.    Reviewer tWSB appreciates that the paper makes weaker assumptions than prior work (i.e., not assuming conditional independence), but raises a number of serious concerns on the theoretical results: The review questions whether assumption 4.6 used in the theory can be true, and whether the bound is vacuousness. The authors argue that this assumption was used in prior work, point out that only some of their results rely on this assumption, and that the assumption is compatible with the theory. The response of the authors partly resolved the reviewers concern and the reviewer raised their score.    Reviewer bTLa finds the idea of understanding contrastive learning for intra class samples interesting, but finds some key assumptions too strong, a critique similar to that raised by reviewer tWSB. The authors responded and the reviewer increased their score, and mentioned that most concerns were addressed. The response partially resolved the reviewers concern, and the reviewer now also recommends acceptance.   I recommend to accept the paper. Understanding contrastive learning better is an important problem, and based on my own reading, I agree with the reviewers that the paper contributes to the understanding of contrastive learning. Two reviewers had concerns about unrealistic assumptions, but those have been largely resolved in the discussion.
The paper introduces a game theoretic framework to improve our understanding of dropout. All reviewers appreciated the contribution of the paper. While they had a number of questions/suggestions, almost all of them were adequately addressed. Three reviewers are satisfied and recommend acceptance, while a lone reviewer is on the fence, he/she admits he/she is less knowledgeable about game theory. Overall, I think this paper makes a solid contribution to ICLR.
This paper proposes a method for inspecting and interpreting the visual representations learned by self supervised methods.  The method is conceptually simple and intuititive, the authors assume that concept labels for the images are available, and then go on to learn a mapping between the learned image vectors and the human provided descriptions of the images. The key insight is to learn a reverse mapping, i.e., to map label vectors to representation vectors. Specifically, feature vectors are quantized using k means to obtain clusters;  images are labeled (automatically) with a diverse set of concepts from expert models trained with supervision on external data sources, and  a linear model is trained  to map concepts to clusters, measuring the mutual information between the representation and human interpretable concepts.  Reviewers raised some questions regarding the relation of the approach to topic models, the difference between reverse probing and linear probing, implementation details and computation. The authors addressed reviewers comments convincingly with additional experiments and/or explanations.
The main remaining criticism of the paper is reproducibility, i.e., "it is nearly impossible to verify the correctness of the result in the paper or to reproduce any of these results" (AC). We generally agree with this statement. While the authors do provide some details in the paper, reviewers, AC, SAC, and PCs agree that this is insufficient. Further points that came up in our discussions were the simplicity of the baselines and the choice of testing to demonstrate that the approach really works. Our impression is that the work lacks a rigorous experimental evaluation. These considerations led to the decision in the end.
This paper addresses the problem of learning representation of 3D point clouds and introduces an interesting approach of concentric spherical GNN with the property of rotationally equivariant. It shows some promising results on point cloud classification under SO(3) transformations and on predicting electronic state density of graphene allotropes. The reviews suggest that, while it does not suffer from any major flaws, the paper has a fairly large number of minor issues that add up to make it subpar for publication. The proposed approach have several hyperparameters, but the authors do not seem to be up front about how the parameters are selected except for stating that they use "standard tuning techniques"   this is not a satisfactory answer and appears to be dodging the question. Many technical details and specific choices could use more thorough explanation and analysis. The distinction of the proposed approach in relation to the large body of existing literature could be more clearly spelled out. Collectively, these issues made the contribution of this paper less clear.
The paper focuses on supervised and self supervised learning. The originality is to formulate the self supervised criterion in terms of optimal transport, where the trained representation is required to induce $K$ equidistributed clusters. The formulation is well founded; in practice, the approach proceeds by alternatively optimizing the cross entropy loss (SGD) and the pseudo loss, through a fast version of the Sinkhorn Knopp algorithm, and scales up to million of samples and thousands of classes.  Some concerns about the robustness w.r.t. imbalanced classes, the ability to deliver SOTA supervised performances, the computational complexity have been answered by the rebuttal and handled through new experiments. The convergence toward a local minimum is shown; however, increasing the number of pseudo label optimization rounds might degrade the results.   Overall, I recommend to accept the paper as an oral presentation. A more fancy title would do a better justice to this very nice paper ("Self labelling learning via optimal transport" ?).  
There are several concerns with the brittleness and reproducibility of the proposed approach and experiments.
The only positive reviewer has not decided to step forward to champion the paper. All others have had a negative first impression which has not sufficiently changed after the answers from authors. My recommendation is based on the data that I have available: unfortunately for the authors the need of more clarity throughout and compelling results cannot be ignore/resolved with the info at hand.
This paper decouples the adversarial training of a domain adaptation model with the detector learning process, and is able to disentangle the features of foreground and background when performing adaptation.  State of the art results on four different domains/tasks are presented with significant improvement. Reviewers are unanimous that the submission is acceptable.  Reviewer PJVS is the most authoritative and experienced reviewer, and notes the paper clarity is impaired, and that the paper is immodest in various places and overclaims what is otherwise supported by the results therein.  The AC concurs with this view.  However the paper is acceptable in present form for ICLR, and the AC advises the authors to revise as discussed in the rebuttal and response to the rebuttal.
All three reviewers felt the paper should be rejected and no rebuttal was offered. So the paper is rejected.
This paper shows how constraining the representation to be invariant to augmentation shrinks the hypothesis space to improve generalization more than just introducing additional samples through augmentation. I agree with the reviewers that this is a novel, intuitive, and interesting finding. However, there were many technical and clarity issues with the original submission. These were partially addressed by the authors in the rebuttal. The reviewers appreciated the authors  efforts and commitment in the rebuttal, but my conclusion from our discussion that this paper requires another round of revisions. I hope the authors would follow the reviewer s comments, improve the paper, and re submit.
This paper received a majority voting of rejection. During the internal discussion, all reviewers insisted their original scores. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the initial recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.  **Research Problem**  In this paper, the authors consider a novel scenario that feature selection in the contrastive setting, where an extra *background* dataset is utilized to remove the background noisy features. However, this problem can be easily handled with a fully supervised feature selection method, where the samples in the *target* datasets are annotated as 1 and the samples in the *background* datasets are annotated as 0. Therefore, the research problem addressed in this paper is not novel. Reviewer UFq8 and ft7b held the same opinion.   **Technical Points**  The technical part could be more informative. The whole framework is based on auto encoder based self reconstruction, where the feature selection is finished by the recent CAE model. In my eyes, the major contribution of this paper lie in learning $g_z$, the background representation function. To achieve this, the authors proposed three strategies, *joint*, *pretraining* and *gates*. The *pretraining* idea does not involve any information from the target dataset, where the background representation function is a general one and it has no relationship with the target dataset. I believe the concept of background should be defined based on the target dataset. The *joint* idea suffers from the information leak, which was pointed by the authors. We can also see the inferior performance of the joint model, comparing with two other models. Unfortunately, the philosophy of *gates* is unclear.  **Experimental Evaluation**  (1) The authors only compared with one supervised method on the semi synthetic dataset. No results of supervised methods on real world datasets were reported. (2) The performance with different numbers of selected features were not reported.
This paper proposes a new wavelet based model to represent textures. The model incorporates a wide range of statistics, by computing covariances between rectified wavelets coefficients, at different scales, phases and positions. The model can synthesize textures that have a similar quality to state of the art texture models using CNN structure. Qualitative results are shown to demonstrate the effectiveness of the model.  The paper studies an important problem in computer vision and neuroscience, which is texture modeling. However, many important related works are missing. After rebuttal, three of four reviewers champion accepting the work because the proposed wavelet based texture model, which produces competitive synthesis with much less parameters than the CNN based model, will be beneficial to the fields of computer vision and neuroscience. One reviewer has critical comments on this paper because the paper lacks a comparison again more recent works both quantitatively and qualitatively. However, during rebuttal, the authors expressed their disagreement with it and pointed out that the goal of the paper is to bridge the gap between the classical work of Portilla and Simoncelli (2000), and the CNN based models and to find what statistics are needed to describe the geometric structures in natural textures. Their discussion didn t reach an agreement after rebuttal. After an internal discussion, AC recommends accepting the paper but urges the authors to improve their paper by taking into account all the suggestions from reviewers, especially include the discussion or comparison with those related works mentioned in the rebuttal.
This paper studies the problem of learning better video text representation learning with an application to video text retrieval. It proposes a key innovation: it uses a new generative task of cross captioning that addresses issues with contrastive learning by learning to reconstruct a sample’s text representation as a weighted combination of a video support set, using a novel objective function using video set bottlenecks. It uses pre training based on YouTube video ASR pairs, and shows empirical results where the proposed method outperforms multiple SOTA methods.  The authors have addressed the feedback of the reviewers, especially with the following improvements:    Experiments were run on more datasets   Relevant work pointed out by the reviewers were added   Concerns regarding technical details were clarified 
This paper proposes a regularization for IRL based on empowerment. The paper has some good results, and is generally well written. The reviewers raised concerns about how the approach was motivated; these concerns have largely been addressed from the reframing of the algorithm from the perspective of regularization. Now, all reviewers agree that the paper is somewhat above the bar for acceptance. Hence, I also recommend accept. There are several changes that the authors are strongly encouraged to incorporate in the final version of the paper (based on discussion between the reviewers):   The claim that empowerment acts as a regularizer in the policy update is a fairly complicated interpretation of the effect of the algorithm. It relies on an approximation derived in the appendix that relates the proposed objective with an empowerment regularized IRL formulation. The new framing makes much more sense. However, the one sentence reference to this section of the appendix in the main paper is not appropriate given that it is central to the claims of the paper s contribution. More discussion in the main text should be included.   There are still some parts of the implemented algorithm that could introduce bias (using a target network in the shaping term which differs from the theory in Ng et al. 1999), but this concern could be remedied by a code release. The authors are strongly encouraged to link to the code in the final non blind submission, especially since IRL implementations tend to be quite difficult to get right.   The authors said they would change the way they bold their best numbers in their rebuttal. The current paper does not make the promised change, and actually adopts different bolding conventions in different tables which is even more confusing. The numbers should be bolded in a consistent way, bolding the numbers with the best performance up to statistical significance.
This paper presents an extensive evaluation of two language models: GPT 3 and UnifiedQA on 57 tasks. The results demonstrate that these models are still far from expert level accuracy and do not know when they are wrong.  I think this is an interesting paper that provides useful insights into the capability of large scale language models. The authors also plan to release their dataset and have addressed some of the concerns from the reviewers to improve the paper during the rebuttal period.
The authors  provide a discussion of Cover s Theorem in the setting of equivariance.  The reviewers consider the work well explained and interesting, especially after the revisions, and so I will vote to accept.
In this paper, the authors propose to add recurrence to pre trained language models such as GPT 2 or BERT. The idea is similar to the compressive transformer paper: a small module is added to the network, and used to compress the representations from the previous chunk of data from the sequence to a single vector. Then, this vector is added to the keys and values of the self attention module when processing the next chunk. The main contribution of the paper is to show that this technique can be added to pre trained models at fine tuning time.  The main concerns regarding the paper are technical novelty and limited empirical results. The idea of adding recurrence to transformers was previously explored in compressive transformer, and many previous work have considered adding modules with small number of parameters at fine tuning time. Moreover, I do not believe that the empirical section is strong enough to justify the acceptance of the paper, as the method is only evaluated on two language modeling tasks (and one early experiment on HotpotQA). The baselines are weak, and thus, the results are not convincing. For these reasons, I weakly recommend to reject the paper, and encourage the authors to make the empirical section stronger.
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
This paper proposes orthogonalising loss gradients with respect to neural network parameters to speed up optimization and improve performance.  The reviewers are unanimous in recommending rejection of the paper. They highlight the following issues: * weak baselines, which make it difficult to judge the contribution of this paper empirically * lack of discussion of relevant literature and existing techniques * arbitrary choices in the design of the algorithm, not backed up by theory or convincing arguments  The reviewers acknowledge the author response, but remain largely unconvinced of the merit of the proposed approach. I see no special reasons to disregard the reviewer assessments, and I therefore recommend not accepting this paper.
The paper received two accepts and 1 marginally above acceptance recommendations. The authors provided satisfactory answers, mostly on clarifying the unsupervised learning methodology, in conjunction with the MAA recommendation. I recommend the paper be accepted as poster.
This paper studies the convergence of gradient descent ascent (GDA) dynamics in a specific class of non convex non concave zero sum games that the authors call "hidden zero sum games". Unlike general min max games, these games have a well defined notion of a "von Neumann solution". The authors show that if the hidden game is strictly convex concave then vanilla GDA converges not merely to local Nash, but typically to this von Neumann solution.  The paper received four high quality reviews and was discussed extensively during the author rebuttal phase. From an application angle, the authors  replies did not convince the reviewers on the relevance of this paper to GANs, and one of the original "accept" recommendations was downgraded to a "reject" because of this. On the theory side, the novelty over Vlatakis Gkaragkounis et al. (2019) is not clear and the reviewers found the writing often confusing or hard to connect with practice. The reviewer with the most positive recommendation did not champion the paper post rebuttal. In the end, the consensus was that the work shows significant promise, but it requires refocusing before appearing at a top tier conference.
The main problem as flagged by reviewers is the lack of formal evidence that the approach is a right one to carry out. Decision tree induction has early been the subject of formal studies in ML, whether in statistics (Friedman et al.) or ML (Kearns et al.). It is a bit sad that a new approach that relies on a much different standpoint on the problem and modelling of tree classification (Section 3, R2), with experimental results recognized by reviewers (R3, R4) is not accompanied by formal analyses on par with SOTA for related approaches (R3, R1). I would strongly suggest the authors fit in a few more Lemmata, either to follow up on specific problems (R1). The paper would tremendously benefit from extensive connections with the existing theory, be it from the generalization and overfitting standpoint (R2, remark #6) or the choice of the appropriate best contender using the boosting literature. Decision was taken not to accept the paper but I would very strongly encourage the authors to revise the draft. 
All reviewers gave an accept rating: 9, 7 &6. A clear accept   just not strong enough reviewer support for an oral.
The paper presents a general view of supervised learning models that are jointly trained with a model for embedding the labels (targets), which the authors dub target embedding autoencoders (TEAs).  Similar models have been studied before, but this paper unifies the idea and studies more carefully various components of it.  It provides a proof for the specific case of linear models and a set of experiments on disease trajectory prediction tasks.  The reviewer concerns were addressed well by the authors and I believe the paper is now strong.  It would be even stronger if it included more tasks (and in particular some "typical" tasks that more of the community is focusing on), and the theoretical part is to my mind not a major contribution, or at least not as large as the paper implies, because it analyzes a much simpler model than anyone is likely to use TEAs for.
This paper proposes blockwise masked attention mechanisms to sparsify Transformer architectures, the main motivation being reducing the memory usage with long sequence inputs. The resulting model is called BlockBERT. The paper falls in a trend of recent papers compressing/sparsifying/distilling Transformer architectures, a very relevant area of research given the daunting resources needed to train these models.  While the proposed contribution is very simple and interesting, it also looks a rather small increment over prior work, namely Sparse Transformer and Adaptive Span Transformer, among others. Experiments are rather limited and the memory/time reduction is not overwhelming (18.7 36.1% less memory, 12.0 25.1% less time), while final accuracy is sometimes sacrificed by a few points. No comparison to other adaptively sparse attention transformer architectures (Correia et al. EMNLP 19 or Sukhbaatar et al. ACL 19) which should as well provide memory reductions due to the sparsity of the gradients, which require less activations to be cached. I suggest addressing this concerns in an eventual resubmission of the paper.
This paper proposes the use of federated learning to the application of steering wheel prediction for autonomous driving. While the application is new and interesting, the reviewers felt that the approach and results were mostly empirical. I suggest that the authors improve the conceptual/algorithmic contribution of the paper in a revised draft. Another suggestion is to include a better explanation of hyper parameter optimization used in the experiments. I hope that the reviewers  constructive comments will help the authors revise the draft adequately for submission to a future venue!
The paper focuses on multi agent reinforcement learning applications in network systems control settings. A key consideration is the spatial layout of such systems, and the authors propose a problem formulation designed to leverage structural assumptions (e.g., locality). The authors derive a novel approach / communication protocol for these settings, and demonstrate strong performance and novel insights in realistic applications. Reviewers particularly commended the realistic applications explored here. Clarifying questions about the setting, experiments, and results were addressed in the rebuttal, and the resulting paper is judged to provide valuable novel insights.
The reviewers agree that this paper has some interesting ideas. However, they believe it needs more work before it is ready for publication, especially so with regards to presentation (SDEs as GANs) and the experiments (backpropagating through the solver rather than using the adjoint dynamics). These would significantly strengthen the paper, but would probably require another round of reviews.
Reviewers generally found the RKHS perspective interesting, but did not feel that the results in the work (many of which were already known or follow easily from known theory) are sufficient to form a complete paper. Authors are encouraged to read the detailed reviewer comments which contain a number of critiques and suggestions for improvement.
This work provides a formal framework for discussing membership inference attacks (MIA). It then examines existing attacks and proposes some new ones. The attacks are evaluated on several datasets.  The framework mostly formalizes the types of information and error types that an attack may use and is presented as the main contribution of this work. However the presented formalizations do not appear to contribute significantly beyond the existing work on MIAs. The new attacks may be of interest and, according to the presented experiments, (mildly) improve on some of the existing MIAs. At the same time, as presented, the discussion of the the benefits of the new attacks is relatively short and reviewers did not find the results to be sufficiently convincing. Therefore I cannot recommend acceptance for this work in its current form.
The paper makes a reasonable contribution to extracting useful features from a pre trained neural network.  The approach is conceptually simple and sufficient evidence is provided of its effectiveness.  In addition to the connection to tangent kernels there also appears to be a relationship to holographic feature representations of deep networks.  The authors did do a reasonable job of providing additional ablation studies, but the paper would be improved if a clearer study were added to investigate applying the technique to different layers.  All of the reviewer comments appear worthwhile, but AnonReviewer2 in particular provides important guidance for improving the paper.
The reviewers have issues with novelty and quality of exposition. I recommend rejection.
Although the reviewers acknowledge that the paper is well written and easy to follow, they found that the contributions of the paper are not enough to be accepted at ICLR. Some concerns from the reviewers are as follows:   1. Assumption 3 is very strong and uncommon. It is not easy to verified even for over parameterized setting.  2. Both the theoretical and experimental results are not sufficient. No improvement in theoretical results compared to the previous work. Moreover, the performance of the method is no better than the baselines, which are themselves much weaker than state of the art results. 3. Motivation for small batch training, advantages over K FAC, the practicality of SLIM QN, and novelty compared to L BFGS are questionable. 4. The method is essentially LBFGS with momentum and damping of the hessian, hence its novelty is questionable.  5. The authors emphasize that "we are trying to design a practical QN method with light compute/memory cost, especially when applied to large scale NNs". Any method that has 20 40 times as much memory requirement as SGD cannot be said to have light memory cost.   Based on the above concerns, the paper is not ready for the publication at this moment. The authors should consider to improve the paper by addressing the reviewers  comments and implementing their suggestions and resubmit this paper in the future venues.
This is a high quality paper, clearly written, highly original, and clearly significant. The paper gives a complete analysis of SGD in a two layer network where the second layer does not undergo training and the data are linearly separable.  Experimental results confirm the theoretical suggestion that the second layer can be trained provided the weights don t change sign and remain bounded. The authors address the major concerns of the reviewers (namely, whether these results are indicative given the assumptions). This line of work seems very promising.
Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera ready submission.
The paper proposes an approach to defining/tackling the question of separating "style" and "content" of images, and introduces a novel way to learn representation that disentangle these aspects of images. I think it offers some new ideas. The reviewers were split on the evaluation. Among the chief concerns with the initial submission were a problematic formulation of the objective, missing comparisons and analysis, and questions about novelty of the architecture (in particular w.r.t. AdaIn). I think the rebuttal/revision have addressed these fairly well. I do agree with R2 that some flaws remain, in particular the analysis could be more thorough/complete, and the paper could then be stronger. 
This paper proposes to perform unsupervised grammar induction over image text pairs and used shared structure between the modalities to improve grammar induction on both sides. Authors find the paper clear, creative and interesting and recommend acceptance without hesitation.
Thanks to the authors for submitting the paper and providing further explanations and experiments. This paper aims to ensure robustness against several perturbation models simultaneously. While the authors  response has addressed several issues raised by the reviewers, the concern on the lack of novelty remains. Overall, there is not enough support among the reviewers for the paper to be accepted.
There is no author response for this paper. The paper addresses the issue of catastrophic forgetting in continual learning. The authors build upon the idea from [Zheng,2019], namely finding gradient updates in the space perpendicular to the input vectors of the previous tasks resulting in less forgetting, and propose an improvement, namely to use principal component analysis to enable learning new tasks without restricting their solution space as in [Zheng,2019].  While the reviewers acknowledge the importance to study continual learning, they raised several concerns that were viewed by the AC as critical issues: (1) convincing experimental evaluation   an analysis that clearly shows how and when the proposed method can solve the issue that [Zheng,2019] faces with (task similarity/dissimilarity scenario) would substantially strengthen the evaluation and would allow to assess the scope and contributions of this work; also see R3’s detailed concerns and questions on empirical evaluation, R2’s suggestion to follow the standard protocols, and R1’s suggestion to use PackNet and HAT as baselines for comparison;  (2) lack of presentation clarity   see R2’s concerns how to improve, and R1’s suggestions on how to better position the paper.  A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs clarifications, more empirical studies and polish to achieve the desired goal. 
The reviewers agree this paper is not good enough for ICLR.
This paper presents an approach to enforce statistical fairness notions using adversarial networks. The reviewers point out several issues of the paper, including 1) their approach does not provably enforce criteria such as demographic parity, 2) lack of novelty and 3) poor presentation.
The review phase was very constructive, where reviewers raised several opportunities for improvements. The authors did a very good job in their rebuttal, which led some reviewers to change their opinion in a positive direction. Overall, reviewers agree that this is the borderline paper with remaining concerns about the weak experimentation.  The paper was again discussed by the Area Chair and Program chairs.  Due to the competitive nature of the conference and the high bar of experimental evaluations expected by empirical papers, the paper was finally rejected.  We hope authors will use the feedback from the reviews and make a stronger submission in near future. 
The paper is interested in the Lipschitz constant estimation of deep equilibrium models. The estimation of this constant provides us the ability to certify classification decisions and understand robustness as well as has important bearings on the generalization ability of a neural network.  Overall a solid theoretical contribution with rigorous theory in a well written paper.  
The paper is about learning policies in RL while ensuring safety (avoid constraint violations) during training and testing.   For this meta review, I ignore Reviewer #3 because that review is useless. The discussion between the authors and Reviewer #1 was useful.  Overall, the paper introduces an interesting idea, and the wider context (safe learning) is very relevant. However, I also have some concerns. One of my biggest concerns is that the method proposed here relies heavily on linearizations to deal with nonlinearities. However, the fact that this leads to approximation errors is not being acknowledged much. There are also small things, such as the (average) KL divergence between parameters, which makes no sense to me because the parameters don t have distributions (section 3.1).   In terms of experiments, I appreciate that the authors tested the proposed method on multiple environments. The results, however, show that safety cannot be guaranteed. For example, in Figure 1(c), SDDPG clearly violates the constraints. The figures are also misleading because they show the summary statistics of the trajectories (mean and standard deviation). If we were to look at individual trajectories, we would find trajectories that violate the constraints. This fact is brushed under the carpet in the evaluation, and the paper even claims that "our algorithms quickly stabilize the constraint cost below the threshold". This may be true on average, but not for all trajectories. A more careful analysis and a more honest discussion would have been useful. In the robotics experiment, I would like to understand why we allow for any collisions. Why can t we set $d_0 0$, thereby disallowing for collisions. The threshold in the paper looks pretty arbitrary.  Again, the paper states that  "Figure 4a and Figure 4b show that the Lyapunov based PG algorithms have higher success rates". This is a pretty optimistic interpretation of the figure given the size of the error bars.   There are some points in the conclusion, I also disagree with: 1) "achieve safe learning": Given that some trajectories violate the constraints, "safe" is maybe a bit of an overstatement 2) "better data efficiency": compared to what? 3) "scalable to tackle real world problems": I disagree with this one as well because for all experiments you will need to run an excessive number of trials, which will not be feasible on a real world system (assuming we are talking about robots).  Overall, I think the paper has some potential, but it needs some more careful theoretical analysis (e.g., effect of linearization errors) and some better empirical analysis.   Additionally, given that the paper is at around 9 pages (including the figures in the appendix, which the main paper cites), we are supposed to have higher standards on acceptance than an 8 pages paper.  Therefore, I recommend to reject this paper.
This clearly written paper extends the Kronecker factored approximate curvature optimizer to recurrent networks.  Experiments on Penn Treebank language modeling and training of differentiable neural computers on a repeated copy task show that the proposed K FAC optimizers are stronger than SGD, Adam, and Adam with layer normalization. The most negative reviewer objected to a lack of theoretical error bounds on the approximations made, but the authors successfully argue that obtaining such bounds would require making assumptions that are likely to be violated in practice, and that strong empirical performance on real tasks is sufficient justification for the approximations.  Pros: + "Completes" K FAC training by extending it to recurrent models. + Experiments show effects of different K FAC approximations.  Cons:   The algorithm is rather complex to implement. 
R4 recommends acceptance while R2 is lukewarm and R1 argues for rejection to revise the presentation of the paper. As we unfortunately need to reject borderline papers given the space constraints, the AC recommends "revise and resubmit".
This works relates adversarial robustness and Lipschitz constant regularization. After the rebuttal period reviewers still had some concerns. In particular it was felt that Theorem 1 could likely be deduced from known results in optimal transport, and it would be nice to make this connection explicit. There were still concerns about scalability. The authors are encouraged to continue with this work, considering the above points in future revisions. 
This paper introduces a generative model termed generalized energy based model (GEBM).  The goal is modelling complex distributions supported on low dimensional manifolds, while offering more flexibility in refining the distribution of mass on those manifolds. The key idea is presented as parametrizing the base measure (called a generator in the paper) and the density with respect to this base measure separately. Figure 1 of the paper sketches the idea on a very clear toy example.  The pros: * Flexibility: Decomposing the full problem as learning the support and learning the density on this support  * Theoretical justification * Introducing the KALE objective * Comparative empirical results with GANs show the additional benefits. Empirically, the framework outperforms GAN with the same complexity. * Clear written paper  The lack of a comparison with GANs has been raised as a concern.  The authors have satisfactorily answered key questions and others raised during rebuttal and added several new references. They have also improved the narrative and included an additional experiment to contrast GEBM and GANs in response to AnonReviewer2, also provided more detail  on how the energy function (class) is chosen.  
This paper proposes to use meta learning to design MCMC sampling distributions based on Hamiltonian dynamics, aiming to mix faster on set of problems that are related to the training problems. The reviewers agree that the paper is well written and the ideas are interesting and novel. The main weaknesses of the paper are that (1) there is not a clear case for using this method over SG HMC, and (2) there are many design choices that are not validated. The authors revised the paper to address some aspects of the latter concern, but are encouraged to add additional revisions to clarify the points brought up by the reviewers. Despite the weaknesses, the reviewers all agree that the paper exceeds the bar for acceptance. I also recommend accept.
This paper studies the problem of out of distribution (OOD) detection for semantic segmentation.  Reviewers and AC agree that the problem might be important and interesting, but the paper is not ready to publish in various aspects, e.g.,  incremental contribution and less motivated/convincing experimental setups/results.  Hence, I recommend rejection.
The authors address the problem of robust reinforcement learning. They propose an adversarial perspective on robustness. Improving the robustness can now be seen as two agent playing a competitive game, which means that in many cases the first agent needs to play a mixed strategy. The authors propose an algorithm for optimizing such mixed strategies.   Although the reviewers are convinced of the relevance of the work (as a first approach of Bayesian learning to reach mixed Nash equilibria, which is useful not only for robustness but for any problem that can be formulated as zero sum game requiring a mixed strategy), they are not completely convinced by the work in current state. Three of the reviewers commented on the experiments not being rigorous and convincing enough in current form, and thus not (yet!) being able to recommend acceptance to ICLR. 
In general there is agreement under reviewers that the ideas/method presented are somewhat interesting/promising but also that the paper lacks a lot of clarity. Reviewers agree that the paper needs more work (on the method) and more extensive experiments to be convincing, and that in its current form it is not mature enough for publication at ICLR.
The paper proposed an efficient way of generating graphs.  Although the paper claims to propose simplified mechanism, the reviewers find that the generation task to be relatively very complex, and the use of certain module seems ad hoc. Furthermore, the results on the new metric is at times inconsistent with other prior metrics. The paper can be improved by addressing those concerns concerns. 
An algorithm for learning prototype based nearest neighbor regression model is presented. This algorithm minimizes an MSE on training examples w.r.t. the prototype centers and the prototype outputs by a block coordinate descent. The main contribution is the optimization algorithm finding the prototypes. Major concerns in the reviews include missing mathematical rigor, poor description of the experiments, and unclear novelty. From my own reading I would like to add that the main theoretical contribution (Theorem 1) makes assumptions that are beyond any reasonable constraint, in particular as we know for more than 40 years, that such kind of assumptions are superfluous for many, many other algorithms.  In summary, a clear reject.
This papers addresses the problem of creating sentiment lexicon for a resource limited language (Amharic). This task is time consuming and requires skilled annotators. Hence the authors propose a method for constructing this automatically from News corpora. They start with a seed list of sentiment bearing words and then add new words to this list based on their PPMO scores with existing words.   While the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below:  1) Lack of novelty: The work has very few new ideas 2) Lack of comparison with existing work: Several missing citations have been pointed out by the reviewers 3) Weak experiments: The experimental section needs to be strengthened with more comparisons to existing work as well as proving the results for at least one more language.  4) Organisation of the paper: The paper needs to be restructured for better presentation. In particular,  the Results and Discussions section does not really contain any discussions. 5) Grammatical errors: Please proofread the paper thoroughly and fix all grammatical and typo errors.  Based on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted.  
This paper proposes a reinforcement learning approach to clustering time series data. The reviewers had several questions related to clarity and concerns related to the novelty of the method, the connection to RL, and experimental results. While the authors were able to address some of these questions and concerns in the rebuttal, the reviewers believe that the paper is not quite ready for publication.
Thank you for your submission to ICLR.  The reviewers ultimately have mixed opinions on this paper, but reading in a bit more depth I don t feel that the critical comments raised by the sole negative reviewer really raise valid points.  Specifically, the fact that this reviewer directly asks e.g. for comparisons to Levine and Feiz 2019, when the paper (before its revisions) contains an entire section devoted to exactly this comparison, strikes me as not sufficient for a thorough review.  However, while I m thus going to recommend the paper for acceptance (it does present a notable, if somewhat minor, advance upon the state of the art in randomized smoothing), I also feel the paper is generally rather borderline for more straightforward reasons.  Specifically, given the _very_ narrow focus of the proposed improvements (improvements to the bounds of randomized smoothing, for L0 perturbations, for Top k accuracy), I ultimately don t think the paper presents that significant an advance in the field.  The paper could go other way, thought definitely not doing so due to the issues that the sole critical reviewer takes.
This paper studies the average convergence rate for first order methods on random quadratic optimization problems. Specifically it is a follow up to work of Pedregosa and Scieur. They study the expected spectral distribution (e.s.d.) of the objective s Hessian and show asymptotic guarantees that work under some assumptions. In comparison to Pedregosa and Scieur, the main takeaway is that you only need to know the distribution at the edges as opposed to the entire spectrum in order to get the same improved convergence. However some reviewers felt that the contributions were oversold, and for example that Assumption 1 is quite restrictive.
This paper studies change point detection in time series using a multiscale neural network architecture which contains recurrent connections across different time scales.   Reviewers were mixed in this submission. They found the paper generally clear and well written, and the idea of adding a multiscale component to the model interesting. However, they also pointed out weaknesses in the related work section and found the experimental setup somewhat limited. In particular, the paper provides little to no analysis of the learnt features. Taking these assessments into consideration, the AC concludes this submission cannot be accepted at this time. 
This paper introduces ATC, which is a contrastive learning on observations separated in time, to learn representations that do not need to take rewards into consideration. These learned representations allow, for the first time, a real disentanglement between representation learning and control, as the agent can simply load such a representation, “freeze it”, and still recover performance of end to end deep reinforcement learning agents.  Overall, all reviewers agree this is a promising direction. Nevertheless, there has been extensive discussion (with the authors and privately) about the significance of the reported results due to the small number of seeds. On one hand, there’s the argument that there is a wide range of experiments and that should compensate for a small number of seeds in individual experiments. On the other hand, there are experiments with as little as two seeds (e.g., DMControl multi env) and this can be seen at most as anecdotal evidence. There’s also the argument that we, as a community, should be striving for more reliable and meaningful experiments in reinforcement learning. Moreover, there have been concerns about how “variance” is being reported (max and min performance) and, although the authors replied to that, an alternative plotting was never shown.  Importantly, at this point it is not clear how many seeds were used in each experiment (Figures 6, 7, 9, 11, 12, 13 do not report the number of seeds used). It is said that each curve represents a minimum of 3 random seeds, but that is very informal and not that useful. Exactly stating the number of seeds would be the right thing to do, not to mention that in the rebuttal it is said that 8 game pretraining for Atari multi env uses 2 seeds, contradicting the original claim. Also, sometimes, different methods, in the same experiment, are  “averaged” across different numbers of seeds (“DMLab offline   ATC is 4 seeds, PC and CPC are 2 seeds each”). This is particularly problematic because of the small number of seeds and potentially high variance. Reporting the max over 4 numbers drawn from a Gaussian distribution is very likely to lead to a larger number than when reporting the max over 2 numbers drawn from the same Gaussian distribution.   I do acknowledge the effort to increase the number of seeds during the rebuttal phase, but it is hard to accept a paper with unknown results. We have very little evidence to believe that going from 2 seeds to 5 seeds is not going to change the results. The reviewers couldn’t agree on the variance of this process as well. Some say the variance of PPO is low between runs when using the same hyper parameters while others mention papers (e.g., Deep RL that matters) that show how much variance one can have across these methods. Thus, I cannot accept this paper conditioned on more seeds being added to the final version because we don’t know what the results will look like. Since this paper is mostly an empirical study, it should have thorough experiments and a careful analysis of the results, but the small number of seeds prevents that in my opinion. Thus, as difficult as it is given the promising direction of the paper, I’m recommending its rejection. I strongly encourage the authors to increase the number of runs in their experiments and to use a more standard measure of variability (e.g., standard error, standard deviation) when reporting their results. This will then be a very strong submission for a future conference. 
This work presents an algorithm   graph structured reinforcement learning (GSRL)  to address the problem of exploration in sparse reward settings. The core elements of this work are 1) to build a state transition graph from experienced trajectories in the replay buffer; 2) learn an attention module that chooses a goal from a subset of nodes in the graph and 3) policy learning via DDPG using "related trajectories", where trajectories that are related to the generated goal are sampled from the replay buffer.  Pros:   all reviewers agree that the idea/work is interesting and valuable to the community   reviewers appreciate the theoretical graph based foundation/motivations  Cons:   clarity: the manuscript still remains hard to follow. Many critical components for understanding are in the appendix.    One of the key steps in this work is the discretization of the state/action space for graph construction. However, this is not mentioned very clearly, which creates a lot of confusion given that you re considering continuous control domains.    Furthermore, the group selection part and training the attention module is expressed in an overly complex manner. Without the reviewers inquiries it would have been impossible to decode the technical details of this key contribution, and unfortunately it remains hard to read/follow.    while the ablation experiments (impact of discretization, group selection ..) are appreciated, but it is not clear on which environment they were generated (average across all? or only one of them?).   do you use DQN and DDPG? There are some conflicting statements in your paper, namely first you say "We use off policy algorithms named DQN (Mnih et al., 2013) for discrete action space and DDPG (Lillicrap et al., 2015) for continuous action space", then in the experiments you say "to demonstrate the real performance gain of our GSRL we set the policy network with DDPG for GSRL and all baselines".     I agree with the reviewers that it s not clear why the chosen baselines are very relevant   there seem to be other more relevant baselines.    the significance of the attention module is not very clear, and is not analysed properly. What does it really learn? some form of deeper analysis would be useful here. How would a version that simply picks the most uncertain state in the graph? The ablation graph presented is not very convincing.   Overall, I believe that this work will make a valuable contribution in the future, with an iteration to improve clarity and better show case the significance of the attention module.
The paper focuses on the Catastrophic Overfitting problem of adversarial training of FGSM. One reviewer gave a score of 6 and the other three reviewers gave negative scores. The authors failed to address or clarify (no rebuttal provided) how perturbation distribution and robustness are linked (four reviewers all agree on this). Other issues include unclear motivation, limited experiments validation, and lack of theoretical analysis. Thus, the current version of the paper cannot be accepted to ICLR.
The authors present an adaptive model that learns a good policy by adversarial training, focusing on the setting where the query budget is very small. Some experiments are carried out to validate the proposed method. The reviewers  opinions turned out to be split on this paper. On one hand, all reviewers appreciated the idea of the problem and recognized its importance. On the other hand, there are have been multiple concerns regarding readability (but that has improved during the discussion) and about the empirical validation/evaluation. Based on the above, as well as my own reading, I believe this paper contains interesting ideas but, as it currently stands, is not ready for publication.
The focus of this paper is to analyze an end to end network to reconstruct matrices originating from non Euclidean data which are corrupted. The authors present an untrained network for this task. In the review period the reviewers raised a variety of concerns including concerns about novelty of the paper with respect to existing work, technical depth and clarity. The authors did not respond to these concerns. Therefore, I recommend rejection.
This paper presents a simple method for improving molecular optimization with a learned model. The method operates by repeatedly feeding generated molecules back through an encoder decoder pair trained to maximize a desired property. Reviewers liked the simplicity of the method, and found it interesting but ultimately there were concerns about the metrics used to evaluate the method. Reviewers 3 and 4 both noted issues with the log P (and penalized log P) metric, noting that it is possible to artificially increase both metrics in a way that isn t useful in practice. During the discussion phase, Reviewer 4 constructed a specific example where simply adding long carbon chains to a molecule would yield a linear increase the penalized log P metric, and noted that the "best molecules" found by the method in Figure 3 also have extremely long carbon chains (long carbon chains are not generally desirable for drug discovery).  I recommend the authors resubmit after finding a better way to evaluate that their method generates molecules with more useful properties for drug discovery.
This paper studies the number of linear regions of a multi layer ReLU network and gives a new upper bound. Reviewers concern about the writing and the results are incremental compared with previous results.
The paper proposes an original and interesting alternative to GANs for optimizing a (proxy to) Jensen Shannon divergence for discrete sequence data. Experimental results seem promising. Official reviewers were largely positive based on originality and results. However, as it currently stands, the paper still makes false claims that are not well explained or supported, in particular its repeated central claim to provide a "low variance, bias free algorithm" to optimize JS.  Given that these central issues were clearly pointed out in a review from a prior submission of this work to another venue (review reposted on the current OpenReview thread on Nov. 6), the AC feels that the authors had had plenty of time to look into them and address them in the paper, as well as occasions to reference and discuss relevant related work pointed in that review. The current version of the paper does neither. The algorithm is not unbiased for at least two reasons pointed out in discussions: a) in practice a parameterized mediator will be unable to match the true P+G, at best yielding a useful biased estimate (not unlike how GAN s parameterized discriminator induces bias). b) One would need to use REINFORCE (or similar) to get an unbiased estimate of the gradient in Eq. 13, a key detail omitted from the paper. From the discussion thread it is possible that authors were initially confused about the fact that this fundamental issue did not disappear with Eq. 13 (they commented "most important idea we want to present in this paper is HOW TO avoid incorporating REINFORCE. Please refer to Eq.13, which is the key to the success of this."). But rather, as guessed by a commentator, that a heuristic implementation, not explained in the paper, dropped the REINFORCE term thus effectively trading variance for bias.  On December 4th authors posted a justification confirming heuristically dropping the REINFORCE terms when taking the gradient of Eq. 13, and said they could attach detailed analysis and experiment results in the camera ready version.  However if one of the "most important idea" of the paper is how to avoid REINFORCE (as still implied and highlighted in the abstract), the AC finds it worrisome that the paper had no explanation of when and how this was done, and no analysis of the bias induced by (unreportedly) dropping the term.   The approach remains original, interesting, and potentially promising, but as it currently stands, AC and SAC agreed that inexact theoretical over claiming and insufficient justification and in depth analysis of key heuristic shortcuts/tradeoffs (however useful) are too important for their fixing to be entrusted to a final camera ready revision step. A major revision that clearly adresses these issues in depth (both in how the approach is presented and in supporting experiments) will constitute a much more convincing, sound, and impactful research contribution.  
This paper considers the problem of hardware and software co design for neural accelerators. Specifically, it looks at hardware and the software compiler that maps DNN to hardware. It employs Bayesian Optimization (BO) to perform joint search over hardware and software design parameters in an alternating manner. To handle black box constraints that cannot be evaluated without performing simulations, the method uses constrained BO algorithms.   The paper talks about two technical challenges: 1) Black box constraints. There is a lot of literature on constrained BO. 2) Semi discrete design variables. The paper didn t propose any generic solution. There are some recent papers to handle mixed variables that may be useful. https://arxiv.org/abs/1907.01329 https://arxiv.org/abs/1906.08878  BO methodology is justified. There is recent work on hardware and software co design for neural accelerators and should be taken into account for both qualitative and quantitative comparison.   Overall, my assessment is that the paper in its current form lacks technical novelty for acceptance.
The paper studies an interesting problem, but as pointed out by reviewers, the presentation of the problem statement and contributions need to be improved.
The paper considers a lower bound complexity for the convex problems. The reviewers worry about whether the scope of this paper fit in ICLR, the initialization issues, and the novelty and some other problems.
The paper proposes a learning framework for Hypergraphs. The proposed method can be viewed as generalisation of GraphSAGE to hyper graphs. Though the paper emphasises that there is significant differences between Hypergraphs and Graphs and hence new methods are required. However, the proposed methods are not significantly different than that used for Graphs. Thus the novelty seems to be limited and hence it is difficult to strongly argue for acceptance. 
The authors propose an architecture that uses a curriculum and multi task distillation to gain higher performance without forgetting. The paper is largely a smart composition of known methods, and it requires keeping data from all tasks to do the distillation, so it is not truly a scalable continual learning approach. There were a lot of concerns about clarity in the manuscript, but many of these have been assuaged by an update to the paper. This is a borderline paper, but the author s rebuttal and update probably tip it towards acceptance. 
The major complaint about this paper was the lack of a proper comparison to previous work, both theoretically and empirically. Also, a study of the tradeoff between the accuracy and running time would significantly help this paper. Ultimately these were the main reasons for deciding to not accept the paper. The reviewers did think the algorithm was new and interesting, and so hopefully by addressing the complaints above, a future version of the paper could be more influential.
The paper works towards analysis to understand the difference   and primarily the lack thereof   between different pruning methods. The central observation is that the convolutional filters in a layer are not strongly correlated and   if the weights of the layer are taken as a matrix   then the covariance matrix is block diagonal.   Extending this objective the regime of a large number of filters, then the matrix is approximately diagonal and all weights are   approximately Gaussian and i.i.d. The point of this analysis is that under this assumption, norm based metrics, particluarly $\ell_1$ and $\ell_2$, behave quite similarly.  The pros of this paper are the extensive evaluation and   after revisions   relatively clear text. The core analysis is nice to have elaborated in detail in the community.  The primary con of this paper is, as the reviewers point out, that there are limited conclusions to take away from this work. Specifically, a plausible default hypothesis is that different pruning criteria result in different pruning decisions. From the results in this paper, that still seems to hold with   exception of the norm based metrics. So, while this work does demonstrate that these norm based metrics are relatively similar   a nice clarification to see in the community   the work offers limited comment on the broader space of pruning metrics.  My recommendation is Reject. Despite the strong empirical evaluation, the ultimate results offer limited clarification on the similarity of pruning metrics. 
This paper presents an understudied bias known to exist in the learning patterns of children, but not present in trained NN models.  This bias is the mutual exclusivity bias: if the child already knows the word for an object, they can recognize that the object is likely not the referent when a new word is introduced.  So that is, the names of objects are mutually exclusive.   The authors and reviewers had a healthy discussion. In particular, Reviewer 3 would have liked to have seen a new algorithm or model proposed, as well as an analysis of when ME would help or hurt.  I hope these ideas can be incorporated into a future submission of this paper.
This paper presents work on temporal logic representations in neural networks.  The paper builds on work on Neural Logic Machines (Dong et al.), adding temporal quantification.  The main positives to the method are this contribution of the temporal reasoning layers (e.g. iii in Fig. 2).  This layer provides an interesting extension of existing literature in the area.  The main concerns raised by the reviewers were the following:   Contribution of the paper over previous NLM techniques   Relation to graph neural networks and other message passing techniques   Use of hand crafted initial features and resultant comparisons to baselines   Empirical validation  After reading the authors  responses the reviewers reconsidered their positions and engaged in discussion.  While the reviewers appreciate the addition of temporal reasoning layers as an interesting contribution, there is still concern over the magnitude of this contribution and its effectiveness.  Additional points raised in the discussion include   Lack of evaluation on standard, challenging datasets and comparisons to state of the art   Ablation study in response (Fig. 6) does not consider absolute removal of temporal layers (main contribution).  Overall, while the paper does contribute an interesting inductive bias for learning with temporal data, the current evaluation is limited in terms of its effectiveness at the classification tasks in the experiments.  Based on the concerns raised in the initial reviews and subsequent discussion, it was determined that the paper is not ready for publication in ICLR.  
This paper proposed a method for adaptive network compression at inference time. However, the paper contains various issues raised by the reviewers that needs to be addressed.
This paper tackles the difficult problem of learning to segment objects from an image using no supervision during training. The paper is clearly written and a new synthetic dataset is made available. Unfortunately, the reviewers raised a number of issues with the submission (missing citations and comparison to relevant related work / additional baselines  + ablation studies / missing empirical evaluation of the proposed method on standard dataset beyond the toy dataset proposed by the authors). The paper received 1 reject, 2 marginal rejects and 1 accept but even the positive reviewer agreed that these were limitations. The authors also conceded to these limitations and initiated experiments that are starting to address the reviewers  comments. At this time, the results of these experiments remain incomplete and hence most reviewers agree that the paper should go through another round of reviews before it is publishable. I thus recommend this paper be rejected in the hope that a subsequent revision will make it a much stronger contribution.
Four reviewers have reviewed and discussed this submission. After rebuttal, two reviewers felt the paper is below acceptance threshold. Firstly, Rev. 1 and Rev. 2 were somewhat disappointed in the lack of analysis regarding non linearities despite authors suggested this was resolved in the revised manuscript, e.g. Rev. 2 argued the paper without such an analysis is too similar to existing  linear  models, e.g. APPNP, SGC, and so on. While Rev. 3 was mildly positive about the paper, they also noted that combining several linear operators is somewhat trivial. Overall, all reviewers felt there is some novelty in the proposed regularization term but also felt that contributions of the paper could have been stronger. While AC sympathizes with this submission and hopes that authors can improve this work, in its current form it appears marginally below the acceptance threshold.
This paper introduces a pair of related regularization oriented techniques for fine tuning pretrained transformer models for NLP tasks, and shows that both are more efficient and more effective than prior work in thorough experiments on a wide range of tasks. The techniques are motivated by the idea of  representational collapse , which is defined as drops in the ability of a linear model trained on an input representation to solve tasks _other than_ the one being trained on.  Pros:   The new method is demonstrated to be broadly efficient and effective on a wide range of tasks.  Cons:   It s not clear why  representational collapse  warrants a new term, or whether it s desirable in general.   The motivations for some of the precise technical decisions behind the new methods are unclear.
The paper considers the problem of private data sharing under local differential privacy.   (1) it assumes having access to a public unlabeled dataset for learning a VAE, so it reduces the dimensionality in a more meaningful way than simply running PCA. (2) the LDP guarantee is coming from the standard Laplace mechanism and Randomized Responses. (3) then the authors propose how to learn a model based on the privately released (encoded) data which exploits the knowledge of the noise distribution.  None of these components are new as far as I know, nor were they new in the context of differential privacy. For example, the use of a publicly available data for DP was considered in:     Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 363–378. Springer, 2013.  (they called it Semi Private Learning...)    Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., & Talwar, K. (2017). Semi supervised knowledge transfer for deep learning from private training data. In ICLR 17.  The idea of integrating out the noise by leveraging the known noise structure were considered in:    Williams, O., & McSherry, F. (2010). Probabilistic inference and differential privacy. Advances in Neural Information Processing Systems, 23, 2451 2459.    Balle, B., & Wang, Y. X. (2018). Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising. In International Conference on Machine Learning (pp. 394 403).  And many subsequent work.  The contribution of this work is in combining these known pieces (without citing some of the earlier work) to achieve a reasonably strong set of experimental results (for LDP standard).  I believe this is the first experimental study that uses VAE for the dimension reduction, however, this alone is not sufficient to carry the paper in my opinion; especially since the setting is now much easier, with access to a public dataset.  The reviewers question the experiments are baselines are usually not using a public dataset as well as the practicality of the proposed method.   Also, connections to some of the existing work on private data release (a.k.a., private synthetic data generation) were note clarified. For these reasons, there were not sufficient support among the reviewers to push the paper through.   The authors are encouraged to revise the paper according to the suggestions and resubmit in the next appropriate venue.
The paper proposes a novel loss for segmentation tasks, which incorporates reasoning about topological accuracy of predicted segmentations vs. ground truth. All reviewers, after the rebuttal period, recommend acceptance, and I agree   it s an interesting paper, offering a potentially useful and clearly novel building block for training segmentation models.
This paper has been reviewed by three reviewers, two scoring it borderline leaning towards an accept, and one scoring it as accept. The key criticism from reviewers is the lack of novelty (*the technical novelty is limited due to the adoption of existing methods without substantial changes (pseudolabeling for latent distribution, adversarial learning for domain generalization)*) and limited technical analysis (*Theoretical analysis is weak due to the use of existing conclusion from Sicilia et al., 2021*). On the other hand, authors argue that the problem they address is new (*learn the generalized representation for time series data, which is a new and challenging problem*).  As it stands, AC sees this paper as borderline leaning towards reject for the above reasons even though the evaluations are interesting.
This paper provides a learning theoretic account of domain generalization in which domains themselves are treated as data, generated from some domain generating distribution. All of the reviewers were positive about this approach and found it interesting. There were, however, a couple of critiques raised by reviewers that lead me to recommend that it is rejected:    the theory provided in this paper does not remotely apply to the datasets that are used in the experiments. While, I agree with one of the author responses that DG benchmarks exist with many domains, DomainBed has very few domains, and it s not clear that their theory is a remotely satisfactory account of the experimental results presented in the paper.   Despite some back and forth on the wording and positioning of the paper, I think the writing still does not give enough credit to worst case analyses of DG.
This paper builds upon recent iterative refinement approaches NMT with an evaluator model that controls the termination of the translation process, yielding a “rewriter evaluator framework” for multi pass decoding. Their approach is an alternative to the policy network used in Geng et al (EMNLP 2018). The main delta wrt previous studies is that the evaluator offers this framework the capability of flexibly controlling the termination. While the idea behind the rewriter evaluator framework is sensible and well described, and the proposed method achieves significant performance improvement against reported baselines, reviewers pointed out some concerns with the baselines and model optimization details. More analysis of the termination procedure against the RL based model of Geng et al. 2018 could shed some light on why the proposed approach is better. Some analysis testifying how many iterations the model uses for translating one sentence, and what factors could affect the iteration number, such as sentence length, would greatly improve the paper. A second weakness pointed out by reviewers is related to the results of WMT’15 En De reported in Table 1, where the reported baseline numbers seem to be weaker than expected. As pointed out by one the reviewers, pre trained checkpoints on English >German (available at https://github.com/pytorch/fairseq/tree/master/examples/translation) exist which achieve much higher sacre BLEU than the reported baseline. I found the authors’ answer not very convincing regarding this point. Therefore, I recommend rejection. I suggest the authors, in future iterations of their work, address some of the issues pointed out by the reviewers and re implement their method following the settings in (Ott et al., 2019) to get more convincing results. 
The paper considers the case where policies have been learned in several environments   differing only according to their transition functions. The goal is to achieve a policy for another environment on the top of the former policies. The approach is based on learning a state dependent combination (aggregation) of the former policies, together with a "residual policy". On the top of the aggregated + residual policies is defined a Gaussian distribution. The approach is validated in six OpenAI Gym environments. Lesion studies show that both the aggregation of several policies (the more the better, except for the computational cost) and the residual policy are beneficial.   Quite a few additional experiments have been conducted during the rebuttal period according to the reviewers  demands (impact of the quality of the initial policies; comparing to fine tuning an existing source policy).  A key issue raised in the discussion concerns the difference between the sources and the target environment. It is understood that "even a small difference in the dynamics" can call for significantly different policies. Still, the point of bridging the reality gap seems to be not as close as the authors think, for training the aggregation and residual modules requires hundreds of thousands of time steps   which is an issue in real world robotics.  I encourage the authors to pursue this promising line of research; the paper would be definitely very strong with a proof of concept on the sim to real transfer task.
The paper first aims to propose a new controllable Pareto multi task learning framework to find pareto optimal solutions. But after the revision according the comments, the paper claims to find finite Pareto stationary solutions. But the paper still can not prove their proposed method can find the Pareto stationary solutions. Even if they can find the  Pareto stationary solutions, they can not guarantee find the pareto front which is conflict with the experiments and claims. There are major flaws in the paper.
The paper demonstrates that test error of image classification models can be accurately estimated using samples generated by a GAN. Surprisingly, this relatively simple proposed method outperforms existing approaches including ones from recent competitions. All reviewers agree this is a very interesting finding, even though theoretical analysis is lacking. Given the importance of the problem of predicting generalization, I recommend acceptance.
The submission received split reviews: two reviewers recommended accepts, and the other two rejects. The AC went through the reviews, responses, and discussions carefully. The AC appreciates the authors  effort during the response period and agreed that the revision has addressed some of the concerns of the reviewers. However, a few key issues are not fully addressed. This includes results on additional, more complex object categories; discussion on why the performance of the proposed method is not even as comparable as BSP Net (Table 4); and others.  Further, while the authors have significantly refactored the paper to address the concern on presentation and clarity, the changes are too major for the reviewers to review during the response period (the reviewers are expected to check minor updates, but not review a new paper during the response period).    Considering all pros and cons, the AC recommends rejection. The authors are encouraged to revise the paper for the next venue.
The paper has been actively discussed in the light of the authors’ response. Following a strong consensus across the reviewers, the paper is recommended for rejection. Even though the paper was, overall, found quite clear, theoretically sound and tackling a relevant problem for the ICLR community, they listed several concerns that remained unclarified after the rebuttal, e.g.,  * Important baselines missing (e.g., high dimensional BO baselines), a concern unanimously shared across the reviewers. As an example, the fact that using trust regions could benefit HOZOG should be demonstrated empirically. The same goes for the fact that HOZOG is a better strategy compared to TurBO. Such a statement warrants an empirical validation * Further discussion about the non convexity and local optima concerns (raised by reviewer 4) * Limitation to continuous hyperparameters.  This list, together with the detailed comments of the reviewers, highlight opportunities to improve the manuscript for a future resubmission.
This paper presents a nice approach to domain adaptation that improves empirically upon previous work, while also simplifying tuning and learning. 
The paper questions the use of cross entropy loss for classification tasks and shows that using squared error loss can work just as well for deep neural networks. The authors conduct extensive experiments across ASR, NLP, and CV tasks. Comparing cross entropy to squared error loss is certainly not novel, but the conclusions of the paper, backed by a lot of experimental evidence, are certainly thought provoking.   I would have liked to see a bit more analysis into the results of the paper, and perhaps a bit more theoretical justification. That said, the paper will be of interest to the community, given the ubiquity of classification tasks. 
Inferring latent trajectory from noisy Ca time series is an important and timely problem and the current study shows some progress in the inference problem. Although the proposed model has some originality, there are remaining issues rendering the manuscript not ready for publication yet. Reviewers raised issues on readability, lack of details, statistics of experiments, Ca time constant estimation, effect size, and lack of comparison. Through an extensive discussion and revisions, I m happy to see the manuscript was greatly improved in readability and additional statistics were provided. However, the Gaussian LFADS  performance at exactly chance raises red flags, effect size is small, and the significance of the scientific findings remain weak. The model is presented as a variational ladder autoencoder system with 2 layers, but the shallow latent representation is tied to the continuous approximation of the point process likelihood. Hence, I view the model as an extension of LFADS rather than a flat hierarchical VAE.  Overall, this paper has a potential of becoming a solid contribution for statistical neuroscience, once the above shortcomings are addressed.
The paper proposes to use transformers to do lossless data compression. The idea is simple and straightforward (with adding n gram inputs). The initial submission considered one dataset, a new dataset was added in the rebuttal. Still, there is no runtime in the experiments (and Transformers can take a lot of time to train). Since this is more an experimental paper, this is crucial (and the improvements reports are very small and it is difficult to judge if there are significant). Overall, there was a positive discussion between the authors and the reviewers. The reviewers commented that concerns have been addressed, but did not change the evaluation which is  unanimous reject.  
The paper studies finding winning tickets with limited supervision. The authors consider a variety of different settings. An interesting contribution is to show that findings on small datasets may be misleading. That said, all three reviewers agree that novelty is limited, and some found inconsistencies and passages that were hard to read: Based on this, it seems the paper doesn t quite meet the ICLR bar in its current form. 
This paper is an intriguing study of agents that can give explanations (contrastive) of their actions via symbolic representation such as dialog.  Agents can also allow users to argue against the agents  decisions. I am extremely impressed by the quality of the reviewer comments and discussions.  It is also interesting that the reviewers have formed two camps of thought on the paper: One camp consists of R3 and R5 who are in agreement in vociferously criticizing the weak points in the paper.  The other camp consists of R1, R2, and R4, who champion the merits of what they see as strong points.  Notably, all reviewers have fairly high confidence values   only one confidence score of 3 and all others are 4.  It was a borderline case and not an easy decision. In the end the program committee decided that the paper in its current form does not quite meet the bar, and would benefit from another revision (see e.g., R4 comments).  We think that the work is interesting, and encourage the authors to address the reviewers  comments and resubmit the work to another venue. 
Three reviewers recommend Reject. Two reviewers recommend Accept although do not champion the paper. I believe the paper develops an interesting idea to better estimate the location of the inducing inputs in sparse GP models. However, I still think the paper would benefit from another careful revision and therefore I do not recommend Acceptance at this stage. I agree with reviewers that 1) currently the method is unable to estimate the covariance between two data points. This is important in applications of GPs for uncertainty quantification such as Bayesian optimisation. For example, including a BayesOpt example would clearly strengthen the paper. 2) empirical evaluation lacks simple baselines, e.g. Titsias (2009). The authors claim that Titsias (2009) does not scale and that s why they don t care for it. Even if this is true, including an example that helps to better compare against this method at a different scale might strengthen the model proposed here.
This paper received two clear accept, one accept, one borderline accept and one reject review. R4 identified that the paper falls short in discussing recent works from CVPR and ECCV 2020 on the image inpainting and completion tasks which also tackle challenging scenarios in these tasks. The authors improve their related work section with these more recent works while pointing out that the task still remains unsolved and they propose an effective technique towards the solution. The meta reviewer recommends acceptance based on the following observations.   The submission proposes a GAN architecture for image inpainting using co modulation, which is similar to the weight modulation in StyleGAN2 but is conditioned on both the input image and the stochastic variable instead of only the stochastic variable. The main novelty of co modulation appears to be interesting as well as being generalisable to different tasks. The approach is shown to perform well in the image painting with large scale missing pixels and some image to image translation tasks. Furthermore a new metric P IDS/U IDS is proposed to evaluate the perceptual fidelity of inpainted images. 
This paper performs event extraction from Amharic texts. To this end, authors prepared a novel Amharic corpus and used a hybrid system of rule based and learning based systems. Overall, while all reviewers admit the importance of addressing low resource language and the value of the novel Amharic corpus, they are not satisfied with the quality of the current paper as a scientific work.  Most importantly, although the attempt of even extraction might be new on Amharic, there have been many works on other languages. It should be clearly presented what are the non trivial language specific challenges on Amharic and how they are solved, otherwise it seems just an engineering of existing techniques on a new dataset. Also, all reviewers are fairly concerned about the presentation and clarity of the paper. Unfortunately, no revised paper is uploaded and we cannot confirm how authors  response is reflected. For those reasons, I would like to recommend rejection.  
This paper presents an update to the method of Franceschi 2017 to optimize regularization hyperparameters, to improve stability.  However, the theoretical story isn t so clear, and the results aren t much of an improvement.  Overall, the presentation and development of the idea needs work.
This work shows interesting potential applications of known machine learning techniques to the practical problem of how to devise a retina prosthesis that is the most perceptually useful. The paper suffers from a few methodological problems pointed out by the reviewers (e.g., not using the more powerful neural network encoding in the subsequent experiments of the paper), but is still interesting and inspiring in its current state.
Reviewers generally appreciate the contributions of the paper, namely the horocycle neuron, Poisson neuron, and the universal approximation properties. However, there are concerns, especially by R4 and R5, that the presentation is confusing, lacks clarity, and should be substantially improved.  Note: Theorem 1.7 in (Helgason, 1970) is proved explicitly for the case n 2, not for general n as claimed in (9).  Thus the Laplacian eigenspace motivation needs to be re written/re examined.
This paper theoretically studied one of the fundamental issue in CycleGAN (recently gained much attention for image to image translation). The authors analyze the space of exact and approximated solutions under automorphisms.  Reviewers mostly agree with theoretical value of the paper. Some concerns on practical values are also raised, e.g., limited or no surprising experimental results. In overall, I think this is a boarderline paper. But, I am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image to image translation.  
This paper introduces modifications that allow to make the training of contrastive learning based models practical. The goal of the paper is very interesting, and the motivation clear. This paper tackles a very important issue with recent unsupervised feature learning methods. However, while the goal is great, the present submission does not provide time improvements on par with the ambitions of this work. As noted by R2, many other hacks could be used in conjunction with the current work to scale this goal to the extreme, yielding time improvements which would be of a more impressive magnitude. In its current form, this paper unfortunately doesn’t meet the bar of acceptance. Given the interesting scope of this work, I strongly encourage the authors to take the feedback from reviews and discussions into account and submit to another venue.
This paper presents a decentralized version of the CEM technique, where an ensemble of CEM instances run independently from one another and each performs a local improvement of its own sampling distribution. The paper shows that the proposed technique can alleviate the problem of centralized CEM related to converging to a local optimum. The paper includes a theoretical analysis and simulation experiments that show some benefits of the proposed technique over centralized CEM.  The key criticisms from the reviewers include the straightforward nature of the proposed idea, which limits the technical contribution of the paper, as well as the limited improvements over centralized CEM in the simulation experiments.   In summary, this is a borderline paper. While the paper is well written and the proposed approach is clearly explained, the lack of strong empirical results that show a pronounced improvement of decentralized CEM coupled with the incremental nature of the idea of decentralized CEM makes me lean toward a rejection.
The main contribution of this paper is that it points out incorrect claims in the literature of multi agent RL and provides new insight on the failure modes of current methods. Specifically, this paper investigates the inconsistency problem in LOLA (meaning it assumes the other agent as a naive learner, thus not converging to SFPs in some games). It then shows problems with two fixes in the literature: 1) HOLA addresses the inconsistency problem only when it converges; otherwise, HOLA does not resolve the issue. 2) GCD does not resolve the issue although it claims to do so. This paper then proposes a method COLA that fixes the inconsistency issue, which outperforms HOLA when it diverges. Reviewers generally agree that the insight from this work is interesting and important for the field. However, there were some concern on both the theory and the experiments. While the updated version addresses some of the concerns, it also made significant changes to both the theoretical and the empirical sections, and would benefit from another round of close review. Thus, I think the current version of this work is borderline.
The paper proposes and studies a new SO(2) equivariant convolution layer for vehicle and pedestrian trajectory prediction. The experiments are detailed and demonstrate the effectiveness of the approach in relation to non equivariant models.
This paper proposes to further distill token embeddings via what is effectively a simple autoencoder with a ReLU activation. All reviewers expressed concerns with the degree of technical contribution of this paper. As Reviewer 3 identifies, there are simple variants (e.g. end to end training with the factorized model) and there is no clear intuition for why the proposed method should outperform its variants as well as the other baselines (as noted by Reviewer 1). Reviewer 2 further expresses concerns about the merits of the propose approach over existing approaches, given the apparently small effect size of the improvement (let alone the possibility that the improvement may not in fact be statistically significant). 
The paper provides an analysis of the well known method of Iterative Magnitude Pruning (IMP) for DNN compression. The problem tackled is undoubtably an important one, and IMP is likely one of the most known solutions for DNN compression. As such, there is no doubt that the paper is well motivated. In addition to the motivated task, the reviews indicate that the paper is well written and provides a thorough review of the related literature, making the paper easy to read and follow. The main weakness of the paper seems to its novelty, as it seems that similar analyses have been done in the past. This issue was raised by the reviews and remained after the correspondence with the authors:   WMeJ: “As I described previously the consistent references and experimental structure borrowed from existing work hinder the novelty of the work”, dL1d: “While the paper introduces inspiring findings on how SLR (or CLR) help IMP, most components are from existing techniques”.  Given the discussion and concerns related to the novelty of the paper, I feel that the paper requires too major of a revision to be accepted, either improving its core analysis, or presenting it in a better way that clearly distinguishes it from previous art.
PAPER: This paper introduces a new method to learn joint representations from multimodal data, with potentially missing data. The primary novelty builds from the idea of semi supervised VAE, introducing the concept of bi directional information flow, which is termed “mutual supervision”. This approach brings the same advantages of semi supervised VAE to the multimodal setting, allowing the cross modal interactions to be modeled in the latent space.  DISCUSSION: The discussion brought many important issues, addressed by both reviewers and authors. In general, it seems that most reviewers appreciate the technical novelty of the paper, related to the mutual supervision. While some concerns were expressed about the similarity with semi supervised VAE (Joy et al., 2021), I would agree with other reviewers and the authors that the extension is not straightforward. Bi directional information flow is a worthwhile novelty in itself. One reviewer also mentioned a concern about previous work on multimodal generative models; previous work on the same topic should not preclude new papers, as long new technical ideas are proposed. The final observation is about modeling more than 3 modalities. This is effectively a challenge with the proposed idea and should be acknowledged in the paper, but it is also an issue for many other approaches. New research will be needed to study 3+ modalities, but it should be seen as a future work direction. SUMMARY: Based on the reviews, discussion and personal reading of the paper, I lean towards acceptance of this paper. The paper introduces a new technical idea (bi directional information flow, aka mutual supervision) which enables multimodal representation learning with missing data. The authors should revise their paper to acknowledge potential limitations of the approach (e.g., complexity challenges with 3+ modalities), but the idea is very interesting and worth publication.
This paper tackles the problem of feature interactions identification in black box models, which is an important problem towards achieving explainable AI/ML. The authors formulate the problem under the multi armed bandit setting and propose a solution based on the UCB algorithm. This simplification of the problem leads to a computationally feasible solution, for which the authors provide several theoretical analyses. The importance of the learned interactions is showcased in a new deep learning model leveraging these interactions, leading to a reduction in model size (thereby competing against pruning methods) as well as an improvement in accuracy (thereby competing against generalization methods). Although the proposed approach essentially builds on the specific UCB algorithm, it could likely be extended/modified to other (potentially more efficient) bandit strategies. A drawback of this work resides in the experiments being entirely synthetics. In order to close the gap with practice, experiments on real datasets of higher dimensionality should be conducted.
The paper studies the problem of estimating high quality prediction intervals for deep regression models. The paper argues that one (relatively under studied) avenue to improve these intervals is to accurately estimate conditional coverage   traditional PIs only reason about marginal coverage. The paper argues that in the presence of heteroskedastic errors or model mis specification, conditional coverage can be dramatically different than marginal coverage. Concrete examples for each of these cases would be useful to establish the claim   a synthetic experiment later in the paper illustrates the gap using heteroskedastic errors. The paper introduces a "Confidence Assessment" module that estimates the probability that the model s confidence interval is correct. In spirit, this is akin to learning a calibrated probabilistic classifier. Theoretical analysis shows that the CA module can provably assess the reliability of the confidence intervals while jointly training the confidence interval method   some reviewers appreciated the rigor in this analysis.  However, the reviewers also pointed out that the main message of the paper is muddled, and the confusion spills over into the experimental execution of the paper. Many of the complaints about baselines and experiment setup can be traced back to this confusion. There are several claims in the paper:   Conditional coverage estimation is useful. The synthetic experiments demonstrate this sufficiently.   The CA module achieves conditional coverage estimation reliably and efficiently. There are missing baselines (e.g., other approaches implementing a probabilistic classifier) in the experiments to establish this claim. The authors added an experiment to address this, but reviewers are concerned that the baseline classifier is unnecessarily handicapped (e.g., training a new coverage model from scratch instead of using existing learned features). Reviewers also note that there are missing metrics   the existing metrics can plausibly be gamed by simply outputting the marginal coverage estimate.   Incorporating CA module leads to better prediction intervals. Some experiments suggest that this is not the case, and that there is negligible improvement (the lambda_2   0 setting that the authors describe). On the other hand, it is heartening to note that adding the CA module did not adversely affect the quality of the prediction intervals either.  Since a two stage procedure (estimate intervals, followed by estimating CA module) is empirically inferior to joint training, reviewers rightly ask for some insight into why estimating conditional coverage jointly would reliably lead to prediction intervals that are more precise on average. The theoretical analysis in the paper applies to the 2 stage procedure too (proving that the 2nd stage CA module indeed estimates the reliability of the confidence intervals); so there is some missing insight on why joint training could be beneficial.  A clearer message, making weaker claims and experiments that clearly back those claims will make the paper stronger. For example, (softening claims about CA module:) The paper introduces one ad hoc procedure (CA module) and shows that it is fit for purpose. No claim that it is efficient relative to baselines, but it still needs to justify why CA module should be preferred compared to any other probabilistic classification approach. (softening claims about better intervals:) joint training works better than stage wise training (which, by definition, leaves the prediction intervals unaffected). Unclear as to why that should happen in general; two special cases are mis specification and heteroskedasticity.  
The paper proposes a method for selecting a group of pretext tasks out of a set of candidates in order to optimize self training for downstream performance. The method relies on Hilbert Schmidt Independence Criterion (HSIC) and uses a few data samples to select weights for the given set of tasks The paper demonstrates that using the method for task selection can result in learning better representation for downstream tasks improving accuracy on speech, speaker and emotion recognition.  The reviewers had concerns mostly about the strength of the empirical results. In particular, they felt that the baselines are not strong enough. To the authors credit, the paper was augmented with some of the missing experiments that the reviewers pointed out (e.g., wav2vec plus naive task selection), but that did not persuade reviewers to change their recommendations.  The paper still misses the point that self supervised learning approaches can benefit from training larger models that result in better results. These comparisons are missing from the paper. It is established in other work that findings such as the use of pretext tasks often do not carry over to larger scales. Furthermore, the idea of pretraining a model specific to a downstream task is not inline of the philosophy of self supervised training that aims to train a single model that can be used for many different tasks.
This paper proposes a new variant of a stochastic gradient Langevin dynamics sampler that relies on two key ideas: approximation of the target density with a simpler function (as in [Deng, 2020]) and the parallel simulation of many chains. The authors also prove that their approach can be theoretically more efficient than a single chain algorithm.  The reviewers see the contribution as significant although they did raise some concerns regarding the clarity of the paper. Since these concerns do not appear to be major, I recommend acceptance but I advise the authors to address the comments of the reviewers to maximize the impact of the paper.
The paper makes a significant contribution in the rather sparse and challenging field of convergence analyses of actor critic style algorithms, under the linear MDP structural assumption, showing that there is a natural bias towards being high entropy. As one of the reviewers points out, although it is unlikely that the strategy actually proposed is amenable to implementation, the paper nevertheless provides a clean and novel analysis of convergence of learning by eschewing the usual mixing time type assumptions often found in the theoretically oriented RL literature. Based on this strength of the paper, I am glad to recommend its acceptance.
This paper is overall clearly written, and the proposed approach of performing clustering on the space of persistence diagrams can be a significant contribution.  However, during the discussion, the reviewers share the concern about insufficient empirical evaluation. In particular, datasets are limited and (Lacombe et al. 2018) is not included as a comparison partner, although the authors had a chance to include it in the author response phase. Since this point is crucial, I will reject the paper.  Addressing these points will largely improve the paper, and also reviewers put a great effort to give detailed reviews for the paper. So I hope the authors take the reviews into consideration for further revision of the paper.
This paper present a way to fully binarize a BERT model. The authors convincingly demonstrate that a naive binarization results in large quality losses and then propose amendments. It is pretty impressive that it is possible to get a fully binarized model to work at all. At the same time, the quality losses are still significant and in practice one might prefer to use distillation (as long as the hardware doesn t require binarization). One could also envision combinations of the proposed technique (perhaps in the 1 1 4 setting) with distillation.
This paper studies the "shortcut" learning phenomenon in CNNs and proposes a simple and effective strategy (white paper) to alleviate specific shortcut patterns (e.g. "black squares" in the image). The proposed scheme is verified empirically and shown to improve over some existing solutions. All reviewers appreciate the simplicity of the idea, which allows its quick implementation and reproduciblity. However, reviewers y5Su and C42n believe the notion of shortcuts as studied in this paper are not only very limited, but also artificial. Consequently, they raise doubts about practical relevance/significance of the method for real world datasets with natural shortcuts. Based on these concerns, I suggest authors to identify a real setting (non artificial data) where, alongside their synthetic shortcuts, can show the practical effectiveness of the proposed can.
I agree with the reviewers  positive comments about the paper. The BREEDS approach to generating benchmarks seems to be a useful one and addresses an important problem in the space. This approach could be the start of a nice direction of inquiry that will give us new insights into subpopulation shift. And most of the reviewers  negative concerns were addressed by the revision.
The paper presents an action conditioned video prediction method that combines previous losses in the literature, such as, perceptual, adversarial and infogan type of losses. The reviewers point out the lack of novelty in the formulation, as well as the lack of experiments that would verify its usefulness in model based RL. There is no rebuttal thus no ground for discussion or acceptance.
This paper proposes a feature selection method to identify features for downstream supervised tasks, focused on addressing challenges with sample scarcity and feature correlations.  The proposed approach is highly motivating in biological and medical applications.  Reviewers pointed out various strengths including potential high impacts in biomedical applications, technical novelty and significance, and comprehensive and illustrative experiments.  The authors adequately addressed major concerns raised by reviewers.
This paper presents an approach to model based reinforcement learning in high dimensional tasks. The approach involves learning a latent dynamics model, and performing rollouts thereof with an actor critic model to learn behaviours. This is extensively evaluated on 20 visual control tasks.  This paper was favourably received, but there were concerns around it being incremental (relative to PlaNet and SVG). The authors highlighted the differences in the rebuttal, clarifying the novelty of this work.   Given the interesting ideas presented, and the convincing results, this paper should be accepted.
This paper studies how recurrent neural networks, and more specifically GRUs, store and access information. The authors analyze the solution obtained by gradient descent to the variable delay copy memory task for discrete sequences. They use concepts from dynamical systems, such as slow manifold, to understand the behavior of the learned model. Finally, based on this analysis, the authors propose a synthetic solution to a simplified version of the delay copy memory task.  Overall, while the scores for the paper are rather positive, I still have concerns about the paper, based on the reviews and discussion. I do not believe that these concerned were well addressed by the authors in their rebuttal. First, I tend to agree that the paper is somewhat lacking novelty and insightful findings (reviewers TN5R, afLb, MToe). For example, I think that tools from dynamical systems are mostly useful to analyze RNNs when the input is constant (Jordan et al., 2019). In the case of the copy task, this corresponds to the "delay" period, where in practice the hidden state is almost constant. This behavior is easily explained by the value of the update gate, close to 1. I thus agree that other hypotheses than slow manifold should be discussed to explain how GRUs store and access information, and that the benefits of using dynamical systems is not obvious. Moreover, I believe that previous solutions to the copy task (eg, from Henaff et al.) could be extended to the variable setting by adding a gating mechanism to these solutions. In particular, Henaff et al. claimed that LSTM could solve this task empirically, while the authors claim otherwise.  Second, after reading the revised version a couple of times, I still find the paper hard to follow (MToe, afLb, TN5R). For example, I think that the concept of slow manifold is not introduced properly, and in particular, how it applies to the learned solution is not clear. More generally, I found the sections regarding how information is stored and accessed a bit confusing. Finally, I think that the studied task is simple, and probably does not provide strong insight about the working of recurrent networks. Specifically, LSTMs tend to perform similarly or slightly better than GRUs on many tasks, while the authors claim that this architecture cannot solve the studied task.
The authors present a method to learn representations of 3D atomic structure. They consider two cases: "small" and "large" molecules based on a metric that takes the spatial extension and number of atoms in the molecule into account. Small molecules are represented by an interatomic distance map. Large molecules are represented by a "sinusoidal function based absolute position encoding method". Both settings make use of a transformer architecture on top of the initial representation. The authors also introduce a subsampling step to select a subset of points/atoms and aggregate information from these.  Experimental results are shown for datasets relating to small molecule property prediction, protein ligand binding and a dataset from material science on metalorganic compounds.  Strengths:    interesting modification of transformer architecture dedicated to the chemical compound.  Weaknesses:     Poor presentation of methods with respect to prior work   Limited technical novelty. The distance map representation for small molecules and the sinusoidal function based absolute position encoding method for larger molecules have previously been proposed. Many components are built upon the design of PointNet++(Qi et al., 2017b) without significant modifications. The proposed "3D Transformer" is very similar to an attention based PointNet++ that is specially designed for molecular data.   Experiments are applied only on classification tasks.  All reviewers voted for rejection. I recommend the authors to address the limitations listed above by improving the presentation with respect to prior work, clarifying more the novelty of the methods and including a more diverse range of experiments.
This paper a new method of constructing graph neural networks for the task of reasoning to answer IQ style diagrammatic reasoning, in particular including Raven Progressive Matrices. The model first learns an object representation for parts of the  image and then tries to combine them together to represent relations between different objects of the image. Using this model they achieve SOTA results (ignoring a parallely submitted paper) on the PGM and Raven datasets. The improvement in SOTA is subtantial.  Most of the critique made for the paper is on writing style and presentation. The authors seem to have fixed several of these concerns in the newly uploaded version of the paper. I will further request the authors to revise the paper for readability. However, since the paper presents both an interesting modeling and improved empirical results, I recommend acceptance.
The paper describes a system for learning rules in a quasi NL format: roughly Horn clauses where a predicate p(X1,...,Xk) is replaced by a natural language pattern interleaving ground tokens and variables.  The method is to propose ground sentences   using one of several task specific approaches   use anti unification of pairs to variabilize, and then find a minimal theory from these proposed pairs by reduction to maxsat.  Pros:    QNL is a neat idea, and makes symbolic rule learning possible to some NLP tasks    The use of maxsat is novel in rule learning AFAIK  Cons:    Unification is a highly simplified model of the NL task of cross document co reference    It s unclear if maxsat process will work in the presence of noise, or how well it scales    The datasets use clean text generated from templates or synthetic grammars    Experimentally, the generality of the system is not well demonstrated, because there are differences in how it is applied: eg a subset of short examples for scan, input engineering ($TRUE, $FALSE) for RuleTaker, plus the "heuristics for filtering invalid rules generated by anti unification”    It s not clear if this work really speaks to the main "point" of the SCAN and RuleTaker datasets.  These are both the kind tasks that symbolic systems would be expected to do well, and are used as ANN benchmarks because ANNs perform in unexpected ways: worse than one would expect for SCAN, and better for RuleTaker.  They are important for understanding ANNs but I m not certain what the research benefit is of using them for symbolic methods as a benchmark.
Thank you for your first (hopefully of many!) submissions to ICLR. This work describes a method for allowing nodes to be processed concurrently instead of sequentially, allowing for a reduction in computation time. The reviewers identified a number of concerns about the paper (lack of citations and baselines, an additional experiments demonstrating scale, and a number of clarifications and motivation in the text). The authors addressed the majority of these concerns due the rebuttal. I m afraid a promise of a revised manuscript is not a sufficient substitute for the reviewers seeing a revised manuscript, and due the nature of the feedback, a revision is needed, which the reviewers have not seen to check their concerns are fully addressed. Therefore, at this stage, unfortunately, I recommend rejection.
This paper is interested in finding salient areas in a deep learning image classification setting. The introduced method relies on masking images using Gaussian Gaussian light and shadow (GLAS) and estimating its impact on output.  As noted by all reviewers, the paper is too weak for publication in its current form:   Novelty is very low.   Experimental section not convincing enough, in particular some metrics are missing.   The writing should be improved.
This paper is right on the borderline. It questions the utility of episodic training from a novel perspective, driven by a comparison to NCA, with thorough experiments. The hypothesis that more pairwise comparisons per batch/episode benefit learning is also quite interesting, but some reviewers didn’t feel this was convincingly presented.  Prototypical networks are indeed a popular method for FSL, but I do as well think that NCA is more closely related to matching networks, and that it makes more sense for that to be the focus of experimentation. Matching networks involve more direct pairwise comparisons, and so a leave one out baseline with this model would probably be a useful comparison.  While I appreciate the desire to focus on a fundamental aspect of FSL and not chase state of the art, I think that it’s important to show where one should go from here. That is, as the reviewers pointed out there are many mechanisms beyond vanilla PNs that have yielded better results than those presented in this paper. I don’t think matching SOTA is necessary here, but it would be nice to show that the insights here complement other mechanisms in FSL. 
This paper introduces a prompting technique for eliciting factual knowledge from frozen pretained transformer LMs. The key idea is to modify the embeddings produced by the embedding layer before they are passed to the first attention layer and the paper investigates several different design choices. The Reviewers all agree that the paper tackles an important problem with interesting methods, that it is well written and has strong results. The main concerns, raised by Reviewer jddf, were about clarifying the connections to the robust optimization literature and evaluating on OOD relations. The former has been addressed in the revised version. While the latter point remains valid, I find that the paper in its current state has enough useful experiments and analysis to warrant publication. The authors have clarified most of the other points raised by the reviewers in their rebuttal.
I am recommending a poster for this paper.  There was considerable discussion and much author response.  The reviews were good (after taking author response and paper revision into account) with one out of three being enthusiastic. There was a concern that the basic idea was technically mis represented as the inductive bias is being placed in the decoder rather than prior. But I am convinced that it is a reasonable idea to place bias in the decoder and that idea is worth publication.  Personally I think the paper would be much stronger with better empirical evaluation.  I find a focus on MNIST (or fashion MNIST) unconvincing. Results on CelebA should be accompanied by sample image generations.  I would rather see downstream task metrics based on learned features.  This paper cannot be put in the same class as recent results on unsupervised learning of image features for downstream tasks. It remains an open question as to whether this paper provides any contribution in that arena.
This paper was reviewed by three experts. After the author response, R2 and R3 recommend rejecting this paper citing concerns of experimental evaluation and poor quality of the manuscript. All three reviewers continue to have questions for the authors, which the authors have not responded to. The AC finds no basis for accepting this paper in this state. 
The paper has been actively discussed, both during and after the rebuttal phase. I am thankful for the active communication that took place between the authors and some of the reviewers.  The paper was, overall, found quite clear, with an interesting methodology (especially the introduction of a forecasting step) and a solid large scale experimental evaluation. As a result, it is recommended for acceptance.  However, several concerns remained after the rebuttal phase and we strongly encourage the authors to try to improve the following aspects of their submission: * Clarify as much as possible (notably in the light of the ablation studies further added in the paper) the importance & impact of the BO component (which cast some doubts among the some reviewers on its necessity to get good performance) * Transparently discuss the choice of, _and the robustness with respect to_, the “hyper hyperparameters” of the proposed method (e.g., k, tau, tau’, kappa, tau_max, mini batch size of validation set,...). Such an in depth discussion is essential to fully demonstrate the practical value of the method.  
The paper proposes a new RL algorithm (MIRL) in the control as inference framework that learns a state independent action prior.  A connection is provided to mutual information regularization.  Compared to entropic regularization, this approach is expected to work better when actions have significantly different importance.    The algorithm is shown to beat baselines in 11 out of 19 Atari games.  The paper is well written.  The derivation is novel, and the resulting algorithm is interesting and has good empirical results.  A few concerns were raised in initial reviews, including certain questions about experiments and potential negative impacts of the use of nonuniform action priors in MIRL.  The author responses and the new version were quite helpful, and all reviewers agree the paper is an interesting contribution.  In a revised version, the authors are encouraged to   (1) include a discussion of when MIRL might fail, and   (2) improve the related work section to compare the proposed method to other entropy regularized RL (sometimes under a different name in the literature), for example the following recent works and the references therein:     https://arxiv.org/abs/1705.07798     http://proceedings.mlr.press/v70/asadi17a.html     http://papers.nips.cc/paper/6870 bridging the gap between value and policy based reinforcement learning     http://proceedings.mlr.press/v80/dai18c.html
The paper proposed a new architecture called Regional to Local attention for the vision transformers. The idea is easy to understand, the model adopts the pyramid structure and adds a regional to local attention instead of using the global attention. The architecture is well motivated and the paper is generally well written.  The main concerns from the reviewers are mostly clarification questions. The authors did a good job addressing them. Apart from those, most reviewers raise the novelty issue of such architecture, which I would think is a drawback of this paper.  I am leaning towards the acceptance of this paper mainly because of its experimental results. It is the best in my batch and I think there is a significant improvement over the previous approaches.
The paper has some interesting points in extending IRM to regret minimization, and extending to structured environments.  I can see the writing has been improved in the revision.  The main criticism arises from the experiment, which can be improved in several aspects.   The reviews have been quite detailed and helpful.
The use of SARAH for Policy optimization in RL is novel, with some theoretical analysis to demonstrate convergence of this approach. However, concerns were raised in terms of clarity of the paper, empirical results and in placement of this theory relative to a previous variance reduction algorithm called SVRPG. The author response similarly did not explain the novelty of the theory beyond the convergence results of what was given by the paper on SVRPG.  By incorporating some of the reviewer comments, this paper could be a meaningful and useful contribution.
This paper presents a neural version of individual refinement (IR) architecture for improving the expressiveness of GNN in terms of isomorphism tests. As IR is the dominant approach of practical graph isomorphism tests, adapting IR to GNN is a novel and important idea. As IR suffers from the exponential number of branches, the paper adapts particle filtering algorithms to sample K paths to approximate the full version of the IR algorithm. Simulation and real world datasets are used to demonstrate the improvement over base GNN.  Strengths:  + The paper is well written and easy to follow.   + The originality of the paper is high since it is both technically rich. Adapting individual refinement (IR) to Neural and GNNs is a novel and important contribution. The designed algorithm does improve over base GNNs.  + The particle filtering algorithm is an elegant and low complexity realization of the IR algorithm.   Weaknesses:    PF GNN is mainly evaluated on synthetic datasets, while only three real world datasets are employed in total. It is not thus entirely clear how effective the proposed model is in real world scenarios. The authors added a new real world dataset, OGB molhiv, during the discussion period.     The major weakness is the scalability and practical complexity. The designed model is T times deeper and larger than the base model, and K path sampling needs K times larger memory for parallel computation. Although the reviewers still have some concerns, such as “sampling method may judge isomorphic graphs as non isomorphic”, the reviewers appreciate the author’s hard work on partially addressing the problem during the discussion period. We encourage the author to continue to improve the paper along this direction.
This paper presents a way of using multigrid techniques to parallelize GRU networks across the time dimension. Reviewers are uniformly in favor of accepting the paper.  The main strength is that the paper provides a new perspective on dealing with long input sequences by parallelizing RNNs across time. The main weaknesses are around the experiments: only CPU experiments are run, and sequences are not very long (max 128 length). All in all, though, it provides an interesting perspective that should be valuable to the community.
The paper developed log abstract transformer, square abstract transformer and sigmoid tanh abstract transformer to certifiy robustness of neural network models for audio. The work is interesting but the scope is limited. It presented a neural network certification methods for one particular type of audio classifiers that use MFCC as input features and LSTM as the neural network layers. This thus may have limited interest to the general readers.   The paper targets to present an end to end solution to audio classifiers. Investigation on one particular type of audio classifier is far from sufficient. As the reviewers pointed out, there re large literature of work using raw waveform inputs systems. Also there re many state of the art systems are HMM/DNN and attnetion based encoder decoder models. In terms of neural network models, resent based models, transformer models etc are also important. A more thorough investigation/comparison would greatly enlarge the scope of this paper. 
The paper aims to address the mode collapse issue in GANs by training multiple generators and forcing them to be diverse.  Reviewers agree that the proposed solution is not novel and has disadvantages such as increased parameters due to multiple generator models. The authors do not provide convincing arguments as to why the proposed approach should work well. The experiments presented also fail to demonstrate this. The results are limited to smaller MNIST and CIFAR10 datasets. Comparisons with approaches that directly address the mode collapse problem are missing.
This work propose a method for learning a Kolmogorov model,  which is a binary random variable model that is very similar (or identical) to a matrix factorization model. The work proposes an alternative optimization approach that is again similar to matrix factorization approaches.  Unfortunately, no discussion or experiments are made to compare the  proposed problem and method with standard matrix factorization; without such comparison, it is unclear if the proposed is substantially new, or a reformation of a standard problem. The authors are encouraged to improve the draft to clarify the connection matrix factorization and standard factor models. 
This paper proposes a counterfactual explanation method, termed DISSECT, for image classification. While previous work is concerned with generating one single counterfactual, DISSECT aims to produce multiple counterfactuals, with each illustrating one possible way the class label could be altered. Intermediate images between the benign example and the counterfactuals are also generated to show how the decision boundary is crossed. The reviewers find the idea novel, the presentation clear, and the empirical evaluation thorough.  However, there are concerns regarding whether the method will generalize to other domains because it relies on a strong generative model.  In addition, there is no human subject study to show whether and how much the method really help an end user.
With the advent of non recurrent sequence processing models, it has become costumary to augment input tokens with positional embeddings providing implicit positional information. Despite their potentially crucial role in modern architectures, such positional embeddings are rarely addressed in analytical studies. The current paper provides a systematic investigation of positional embeddings, characterized in terms of properties such as monotonicity, translation invariance, and symmetry. These properties are studies for different positional embeddings using language models fine tuned on two separated benchmarks, with an emphasis on visual analysis.  The authors provided an impressive rebuttal, adding many of the experiments required by the reviewers. The latter are still somewhat split about the paper. I lean towards the positive side. I find that some of the criticism, while valid, is not really granting a rejection, especially after the authors  clarifications. In particular, one reviewer assumed that the authors claim that symmetry should be a property of an ideal positional embedding, whereas the authors are rather studying whether it is an important property of them, in light of the previous literature. Some claims about the results being "interesting" or "surprising" enough might depend on what the reader is looking for in the paper. I think that many readers in the "black box NLP" community will find the methods and analyses presented in this paper interesting and useful. 
This paper improves on the efficiency of prior work that uses homomorphic encryption to perform privacy preserving inference. There are two main concerns raised by the reviewers. First, multiple reviewers (and I) found this paper difficult to read. Multiple pieces of the problem are not clearly presented especially with respect to the technical contributions. This was fixed in part in the rebuttal but more could still be done here. But more importantly, three reviewers raise concerns about the evaluation methodology, especially with respect to comparisons to prior work. On top of this, there are valid criticisms raised by the reviewers about if the contribution here is that significant when compared to prior work. (This is something that both more clear writing and more careful experiments could hep address.) Taken together I do not believe this paper is yet ready for publication.
Thanks to the authors for the revision and discussion. This paper provides a neural architecture search (NAS) method, called Evolutionary Neural hybrid agents (Evo NAS), which combines NN based NAS and Aging EVO. While the authors  response addressed some of the reviewers  comments, during discussion period there is a new concern that the idea proposed here highly overlaps with the method of RENAS, which stands for Reinforced Evolutionary Neural Architecture Search. Reviewers acknowledge that this might discount the novelty of the paper. Overall, there is not sufficient support for acceptance.
The paper proposed to use  normalizing flow to model point processes. However, the reviews find that the paper is incremental. There have been several works using deep generative models to temporal data, and the proposed method is a simple combination of well established existing works without problem specific adaptation. 
All of the reviewers agree that the paper presents strong experimental results on continuous control benchmarks. The reviewers raised concerns regarding the analysis of the behavior of the algorithm, the possible impact of the technique, and requested more references and comparison with related work. The paper has significantly improved since the initial submission, but still not able fully satisfactory to the reviewers, partly due to the large extent of the changes needed. 
This paper provides a near optimal analysis of the unadjusted Langevin Monte Carlo (LMC) algorithm with respect to the W2 distance. The main statement is that the mixing time is ~ d^{1/2}/eps under standard assumptions. The authors also give a nearly matching lower bound under these assumptions. The reviewers agreed that this is an interesting contribution obtained via non trivial techniques. The consensus recommendation is to accept the paper.
this work adapts cycle GAN to the problem of decipherment with some success. it s still an early result, but all the reviewers have found it to be interesting and worthwhile for publication.
All the reviewers recommend rejecting the submission. There is no basis for acceptance.
This paper studies the effect of importance weighting in three model classes: linear models, linearized networks, and wide fully connected networks, and show that under certain assumptions, gradient descent for training an overparameterized model converges to the same ERM interpolator regardless of the reweighting scheme. The reviewers acknowledge that this paper had good exposition and writing in general, but they were in consensus that the initial version of this paper includes many inaccurate overclaiming statements. In summary, after discussions, the reviewers would like the authors to:    revise the abstract and the introduction, specifically, adding appropriate qualifiers on the neural networks, losses, full gradient descent training, etc (Reviewers hLzT, M2hT, fZgx)   address the discrepancy between theory and experiments, e.g. the inconsistency of the loss chosen in theory and experiments, the requirement that the widths of the neural networks need to large (Reviewers wG4N, fZgx)   add experimental results for early stopping (Reviewer hLzT)   empirically verify that the final solutions of ERM, DRO, and IW initialized at the same \theta^0 are very similar (Reviewer M2hT)   discuss the novelty compared to Sagawa et al in the paper (Reviewer wG4N)  thus, this submission needs a major revision, and is not ready for acceptance in its current form. We encourage the authors to revise accordingly, and resubmit in the future.
This paper proposes a Residual Energy based Model for text generation.  After rebuttal and discussion, the reviewers all converged on a vote to accept, citing novelty and interestingness of the approach.  Authors are encouraged to revise to address reviewer comments.
Although the proposed method shows sota results, it is a simple combination of two existing methods, a bit of Bayesian + domain generalization.  It seems that the total improvement by the proposed method is just the sum of improvements by Bayesian and by domain generalization.  No synergy between Bayesian and domain generalization is observed.  I personally doubt that the Bayesian treatment of the domain generalization loss is not essential. The derivation in Sec. 2 is unnecessarily complicated.  In derivation from eq(3) to eq(5), the authors first "extend" (3) to (4), which is not appropriate ((4) can hold even if $p(y_{\zeta}|x_{\zeta})$ is highly diverse. ).  After that the authors apply Jensen to come back to an appropriate form (5), which is a weighted sum of distances (which is an appropriate criterion). If they start from Eq.(5), the proposed objective is simply the sum of the standard ELBO (2) and a natural domain invariance loss (5).  For non Bayesian treatment of the domain invariant loss, you could simply replace the KL by Lp norm between $y_s$ and $y_\zeta$.  I expected that by answering to the question by Reviewer 1 the authors would prove synergy between Bayesian and domain generalization.  But the authors just excused that   The feature distributions  are unknown without Bayesian formalism, leading to an intractable $L_I$. Therefore, we do not conduct the experiment with only the domain invariant loss on both the classifier and the feature extractor.  I don t really understand what the authors mean, but the authors should have explained why you cannot replace the KL with non Bayesian Lp loss.   
This paper uses an extension of HoloGAN for few shot recognition and novel view synthesis. All but one reviewer gave a final rating of accept. These reviewers were concerned that the submitted version of this work had not adequately placed this work in context with prior art. However, during the discussion these concerns seem to have been addressed sufficiently. The most negative reviewer was not impressed by the quality of the generated images; however these are relatively new methods and the few shot recognition aspect of this work is also part of the contribution. Accounting for all reviews and the discussion the AC recommends accepting this work as a poster. 
The paper focuses on extracting the underlying dynamics of objects in video frames, for background/foreground extraction and video classification. Generally speaking, the presentation of the paper should be improved. Novelty should be clarified, contrasting the proposed approach with existing literature. All reviewers also agree the experimental section is also too weak in its current form. 
All reviewers appreciate the framework described in the paper and say it is a "useful tool", a "flexible, efficient, extensible, and secure visual analysis framework"  and "in full fledged form may help the productivity while building visual analysis applications."  However, the reviewers also point to significant shortcomings in terms of fit to ICLR, e.g. "looks more like a technical report than a research paper", "key contributions of the VideoFlow should be only counted as engineering efforts rather than any novelty in the scientific or research perspective", or "major concern is that if it is appropriate for ICLR to publish this tutorial which may be regarded as an endorsement to this software".  The authors did not reply to the reviewers comments.  Overall the paper does not seem to contain sufficient scientific contributions for being accepted.
The paper proposes a method to control dynamical systems described by a partial differential equations (PDE). The method uses a hierarchical predictor corrector scheme that divides the problem into smaller and simpler temporal subproblems. They illustrate the performance of their method on 1D Burger’s PDE and 2D incompressible flow. The reviewers are all positive about this paper and find it well written and potentially impactful. Hence, I recommend acceptance of this paper.
The submission has two issues, identified by the reviewers; (1) the description of the proposed method was found to be confusing at times and could be improved, and (2) the proposed transitional skills were not well motivated/justified as a solution to the problem the authors propose to solve.
This paper introduces a differentiable yarn level model of fabrics. The model is more detailed and physically realistic than proposed in earlier work, which may allow for applications to manufacturing guidance and textile design. The paper is generally well written and contains detailed problem formulation and derivations. Experiments show it is possible to successfully learn a control policy and material parameters using the differentiable model.
The paper proposes a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling.  The reviewers and AC note the following potential weaknesses: (1) limited novelty and (2) preliminary experimental results.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
The paper proposes a novel approach to graph representation learning. In particular, a graph auto encoder is proposed that aims to better capture the topological structure by utilising a neighbourhood reconstruction and a degree reconstruction objective. An optimal transport based objective is proposed for the neighbourhood reconstruction that optimises the 2 Wasserstein distance between the decoded distribution and an empirical estimate of the neighbourhood distribution. An extensive experimental analysis is performed, highlighting the benefits of the proposed approach on a range of synthetic datasets to capture structure information. The experimental results also highlight its robustness across 9 different real world graph datasets (ranging from proximity oriented to structure oriented datasets).  Strengths:   The problem studied is well motivated and the method proposed is well placed in the literature.   The method is intuitive and the way that the neighbourhood information is reconstructed appears novel.   The empirical comparisons are extensive.   Weaknesses:    Some of the choices in matching neighborhoods seem a bit arbitrary and not sufficiently justified.    The scalability of the proposed method is questionable. The method has a high complexity of O(Nd^3) (where N is the number of nodes and d is the average node degree). The authors address this problem by resorting to the neighborhood sampling method (without citing the prior art), which is only very briefly discussed in the paper.    The reviewers have also expressed concerns about the fixed sample size q. The question of how the neighbour sampling is handled when a node has less than q neighbours remains unanswered.
The present paper addresses the problem of imitation learning in multi modal settings, combining vision, language and motion. The proposed approach learns an abstract task representation, and the goal is to use this as a basis for generalization. This paper was subject to considerable discussion, and the authors clarified several issues that reviewers raised during the rebuttal phase. Overall, the empirical study presented in the paper remains limited, for example in terms of ablations (which components of the proposed model have what effect on performance) and placement in the context of prior work. As a result, the depth of insights is not yet sufficient for publication.
The paper considers a procedure for the generation of adversarial examples under a black box setting. The authors claim simplicity as one of the main selling points, with which reviewers agreed, while also noting that the results were impressive or "promising". There were concerns over novelty and some confusion over the contribution compared to Guo et al, which I believe has been clarified.  The highest confidence reviewer (AnonReviewer2), a researcher with significant expertise in adversarial examples, raised issues of inconsistent threat models (and therefore unfair comparisons regarding query efficiency), missing baselines. A misunderstanding about comparison against a concurrent submission to ICLR 2019 was resolved on the basis that the relevant results are mentioned but not originally presented in the concurrent submission.   While I disagree with AnonReviewer2 that results on attacking a particular image from previous work (when run against the Google Cloud Vision API) would be informative, the reviewer has remaining unaddressed concerns about the fairness of comparison (comparing against results reported in previous work rather than re run in the same setting), and rightly points out that as many variables should be controlled for as possible when making comparisons. Running all methods under the same experimental setting with the same *collection* of query images is therefore appropriate.   The authors have not responded to AnonReviewer2 s updated post rebuttal review, and with the remaining sticking point of fairness of comparison with respect to query efficiency I must recommend rejection at this point in time, while noting that all reviewers considered the method promising; I thus would expect to see the method successfully published in the near future once issues of the experimental protocol have been solidified.
The paper proposes a transformer model of code that leaves "holes" at points of generation at which the model is uncertain. The model is evaluated on C# and Python programs and outperforms existing techniques.   The reviewers found the Grammformer model and the RegexAcc evaluation metric to be useful and interesting. The experimental results are also compelling. Given this, I recommend acceptance. Please make sure to incorporate the feedback in the reviews and the additional experimental results into the final version.
Dear authors,  I have carefully read the reviews, rebuttals, and the subsequent discussion. Most reviews provide high quality feedback, and the reviewers  combined opinion is strongly oriented towards recommending rejection. I have to concur with this recommendation. While essentially all reviewers agreed that the paper is well written, they also raised several key concerns. I will reiterate (and further elaborate) on some of them here:  1) I do not agree with the authors  claim that there is a significant difference between client sampling in FL and data sampling in SGD in terms of the underlying mathematics; at least not in the present form. The mathematical formulation of both problems is the same. While in SGD it is possible to access local information and construct more powerful sampling strategies using this information, it is also possible to forgo using this information and propose simpler data agnostic strategies. Such strategies have been studies in the SGD literature before. I recommend the literature on *arbitrary sampling* pioneered by Richtarik and Takac ("Iteration complexity of randomized block coordinate descent methods for minimizing a composite function", Mathematical Programming, 2014) in the context of randomized coordinate descent. The same sampling approach was later adopted for SGD. The paper by Csiba and Richtarik (Importance sampling for minibatches, JMLR 2018) suggested by one of the reviewers is relevant here as it adopts the arbitrary sampling approach to (variance reduced) SGD. Such an arbitrary sampling paradigm is more general than the unbiased sampling strategy you study here (indeed, arbitrary sampling includes also biased samplings). The work of Chen et al mentioned by a reviewer is also more relevant than you appreciate. This work also mentions a couple decomposition statements. They work with the arbitrary sampling framework in the FL setting   and this seems more general than your framework since it includes biased samplings as well. Note that they then proceed to compute the optimal sampling out of all samplings, while you do not attempt to theoretically capture what samplings are best. This prior works thus addresses a similar problem, and goes deeper in this aspect. Also, the parameters $v_i$ in their Lemma 1 are simply just statistics of the sampling   and are unrelated to the data. Also, Lipschitz smoothness constant of the aggregate loss over all local data samples is not hard to estimate, and is needed anyway to set the stepsize correctly. So, your comments about the unavailability of these quantities in the FL setting seem incorrect.  2) Theorem 1 is indeed just a simple calculation / observation rather than a result. I agree with the reviewer who said that the value of this simple observation, without a deeper study of its consequences, and an explanation of how the consequences lead to new results that are in some sense interesting, is quite limited. As mentioned by one reviewer, sophisticated methods are often non monotonic: they do *not* attempt to greedily reduce some simple potential (e.g., distance to the minimizer) as such a strategy may be suboptimal from a total convergence point of view. This observation by the reviewer seems to have been misunderstood by the authors. This limits the impact of Theorem 1 as Theorem 1 assumes that one is interested in a greedy method.  3) The bounded variance and bounded dissimilarity assumption *are* strong. The suggestion by one of the reviewers to consider a work on more accurate ways of modeling stochastic gradients (Khaled et al) was appropriate. I suggest the authors read the paper to see detailed reasoning explaining why the types of assumptions you make in this paper are problematic. The fact that some other papers use such problematic assumptions, even if they are well known, is not evidence that these assumptions are not problematic. It is merely evidence that many papers share the same issue. I also have to oppose the authors  view that non peer reviewed work should not be brought up. In my view, this is a deeply problematic and unscientific attitude to research that is available online. Peer review does not imply correctness, and vice versa.   4) Experimental comparison with any other methods is missing. Why do you not compare with the optimal sampling strategy of Chen et al, for example? Does your framework suggest a better strategy in some sense? If yes, show it. If no, then in what sense are your sampling strategies interesting? In any case, all have been considered in the arbitrary sampling framework before as far as I can see.  5) There are more issues with have been identified by the reviewers. I strongly recommend the authors to take all of them seriously in their revision.  In summary, this is a solid paper. However, it has some serious issues and for this reason I cannot recommend it for acceptance. Having said that, I thank the authors for their submission and wish them best of luck in future research with this project.  Area Chair
Strengths  The paper proposes to include exploration for the PETS (probabilistic ensembles with trajectory sampling) approach to learning the state transition function. The paper is clearly written.  Weaknesses  All reviewers are in agreement regarding a number of key weaknesses: limited novelty, limited evaluation, and aspects of the paper are difficult to follow or are sparse on details. No revisions have been posted.  Summary  All reviewers are in agreement that the paper requires significant work and that it is not ready for ICLR publication. 
This paper makes an interesting connection between the density matching in imitation learning and reaching the goal state in goal oriented reinforcement learning. Reviewers generally expressed that the paper proposes an interesting approach, but some aspects of the paper need room for improvement. By reinforcing the experiments that address the reviewers’ various concerns, this paper will make a good contribution towards reinforcement learning research.
This paper introduces a way of generating adversarial speech samples to attack an ASR system based on speech synthesis.  The proposed approach, by the name of Speech Synthesizing based Attack (SSA) , does not rely on real speech to create adversasial samples but rather create them purely from text using  a conditional variational auto encoder.  Experiments are conducted on three datasets and the results appear to support the effectiveness of the proposed approach.   All reviewers consider the paper technically sound and well written. Overall, the work is interesting. It pursues another possibility for a threat model on ASR and may inspire related work in the community. Reviewers also raised a number of concerns most of which have been cleared by the authors  rebuttal.  However, there are two major ones standing. One is the perceptual quality of the synthesized speech and the other is the justification of the application scenarios in the real world.  In the follow up MOS experiments, there seems to be a noticeable difference (4.09 vs 3.39) which means the synthesized speech does sacrifice quality.  From the real world application perspective, the scenarios proposed by the authors seem to be a bit contrived.  These two drawbacks are considered significant and need further investigation and justification.
There reviewers unanimously recommend rejecting this paper and, although I believe the submission is close to something that should be accepted, I concur with their recommendation.  This paper should be improved and published elsewhere, but the improvements needed are too extensive to justify accepting it in this conference. I believe the authors are studying a very promising algorithm and it is irrelevant that the algorithm is a relatively obvious one. Ideally, the contribution would be a clear experimental investigation of the utility of this algorithm in realistic conditions. Unfortunately, the existing experiments are not quite there.  I agree with reviewer 2 that the method is not particularly novel. However, I disagree that this is a problem, so it was not a factor in my decision. Novelty can be overrated and it would be fine if the experiments were sufficiently insightful and comprehensive.  I believe experiments that train for a single epoch on the reduced dataset are absolutely essential in order to understand the potential usefulness of the algorithm. Although it would of course be better, I do not think it is necessary to find datasets traditionally trained in a single pass. You can do single epoch training on other datasets even though it will likely degrade the final validation error reached. This is the type of small scale experiment the paper should include, additional apples to apples baselines just need to be added. Also, there are many large language modeling datasets where it is reasonable to make only a single pass over the training set. The goal should be to simulate, as closely as is possible, the sort of conditions that would actually justify using the algorithm in practice.  Another issue with the experimental protocol is that, when claiming a potential speedup, one must tune the baseline to get a particular result in the fewest steps. Most baselines get tuned to produce the best final validation error given a fixed number of steps. But when studying training speed, we should fix a competitive goal error rate and then tune for speed. Careful attention to these experimental protocol issues would be important. 
This paper proposes a new approach, which combines offline reinforcement learning with learning in simulation. There were different views on the paper among the reviewers and we had quite a lot of discussions. As a consequence, there were still serious concerns remaining, e.g., whether the results are significant enough, whether there are clear advantages of the proposed mehtod over directly using offline RL methods. It is not justified whether the proposed framework can use offline data more efficiently or better reduce the gap between mismatched simulators and offline data. The reviewer who gave the highest score decided not to champion the paper. Considering all the discussions, we believe the paper is not ready for publication at ICIR yet.
The reviewers were excited by this work, which focuses on lifelong RL in non stationary, non episodic environments.  They found the approach compelling with exciting results on the tested domains.  However, even the more positive reviewers were concerned with the somewhat narrow scope of evaluation, which makes the paper somewhat less ambitious.    In response to the reviews, the authors added extra experiments, clarifying text, and requested details that provide more depth and insight to the paper.  Still, the approach and paper is somewhat narrowly focused, but it does yield insights that should be useful for future works that solve this problem in a more general manner.
This paper tackles the challenge of continual learning. It approaches the problem by combining a Gaussian Mixture Model (GMM) to model concepts in a latent space and and a decoder system to generate new data points for pseudo rehearsal and maintenance of previous information. When new concepts arrive, the GMM can be updated with rehearsal serving to prevent forgetting. The authors show competitive results on incremental learning of MNIST and FMNIST.  The scores were mostly below threshold, with one above threshold (5,3,5,6). The reviewers generally agreed the approach was interesting and they appreciated the theoretical treatments. However, there were a number of concerns, the central ones being the lack of clarity and the lack of convincing empirical demonstrations of scalability. The authors attempted to address the concerns, but they were not able to show good performance on larger datasets. The suggested this was due to the complexity of the encoding model, but they were unable to demonstrate this concretely. The reviewers  scores did not change, though, and the consensus was that this paper was not quite ready for publication. Given these considerations, and an average final score of 4.75, a decision of reject was reached.
This paper studies text style transfer which aims to edit a given sentence to possess a desired style value (e.g., positive sentiment) while keeping all other styles and content unchanged. The paper specifically focuses on a challenging setting where besides the target style (e.g., sentiment) to transfer, there exists confounding attributes (e.g., product category) that correlate with the target style, making it hard to change only the target style while preserving the other. The proposed approach is to learn an invariant/unbiased style classifier using Invariance Risk Minimization (IRM), together with an orthogonal classifier for monitoring style independent changes (e.g., product category), to supervise the generator training. The main concerns are on the experiments   it s suggested to include experiments on other styles besides sentiment; human evaluation and/or other metrics are needed for more convincing comparison; it s also encouraged to experiment with large language models (e.g., GPT 2, BART) besides the small LSTM/CNN networks as in the present work.
This paper proposes an algorithm for offline RL, that consists in solving a finite MDP derived from a fixed batch of transitions. The initial reviews were overall positive, and the concerns raised at this stage were nicely addressed by the rebuttal and the revision from the authors. The final discussion led to the consensus that this paper should be accepted at ICLR.
Motivated by GANs, the authors study the convergence of stochastic subgradient                                                      descent on convex concave minimax games.                                                                                            They introduced an improved "anchored" SGD variant, that provably converges                                                         under milder assumptions that the base algorithm.                                                                                   It is applied to training GANs on MNIST and CIFAR 10, partially showing                                                             improvements over alternative training methods.                                                                                                                                                                                                                         A main point of criticism that the reviewers identify is the strength of the                                                        assumptions needed for the analysis.                                                                                                Furthermore, the experimental results were deemed weak as the reported scores                                                       are far away from the SOTA, and only simple baselines were compared against.  
The paper proposes a cycle consistent GAN architecture with measuring the reconstruction error of time series for anomaly detection.  The paper aims to address an important problem, but the current version is not ready for publication. We suggest the authors consider the following aspects for improving the paper: 1. The novelty of the proposed model: motivate the design choices and compare them with state of art methods 2. Evaluation: formalize the target anomalies and identify datasets/examples where the proposed model can significantly outperform existing solutions.   
This paper propose an approach to probabilistic meta learning for Bayesian optimization. The goal is to accelerate Bayesian optimization under the assumption that multiple related tasks require optimization. In terms of strengths, the paper addresses an important problem as it has applications to efficiently optimizing hyper parameters over multiple related data sets or models. In terms of weaknesses, the proposed approach is closely related to neural processes, but this connection was not made in the original paper. The authors were unfortunately not able to provide additional insights or results regarding this point during the limited discussion period and as a result, the novelty of the method at the core of this approach is in question. The reviewers also note that the experimental designs and comparisons performed are limited. For some smaller problems, more standard baseline methods like multitask GPs should be applied. The authors have also not compared to a number of other recent methods aimed at scalable transfer learning for hyper parameter optimization, as detailed in the comments of Reviewer 1. The reviewers agree that the paper is not yet ready for publication.
This paper presents a different method for learning autoencoders with discrete hidden states (compared to recent discrete like VAE type models). The reviewers in general like the method being proposed and are convinced that there is worth to the underlying proposal. However there are several shared complaints about the setup and writing of the paper.    Several reviewers complained about the use of qualitative evaluation, particularly in the "Deciphering the latent code" section of the paper.   One reviewer in particular had significant issues with the experimental setup of the paper and felt that there was insignificant quantitative evaluation, particularly using standard metrics for the task (compared to the metric introduced in the paper).   There were further critiques about the "procedural" nature of the writing and the lack of formal justifications for the ideas introduced. 
The reviews are of good quality. The responses by the authors are commendable, but ICLR is selective and reviewers continue to feel that important choices in the research are not sufficiently clear and fully justified.
Based on the Bayesian approach to filtering problem, the paper proves that RNN are universal approximators for the filtering problem.  Two reviewers, however, have doubts about the novelty and difficulty to get the result. Although I do not fully agree that Reviewer3 that the proof is just "DNN can fit anything"   it is not this case, but the concerns of Reviewer2 are more strong, especially about the usage of the term "recurrent neural network". The paper is purely theoretical and does not have any numerical experiments, which probably makes it too weak for ICLR in this form. However, I encourage the authors to continue to work on the subject, since the approach looks very interesting but it still very far from practice.
This paper demonstrated interesting observations that simple transformations such as a rotation and a translation is enough to fool CNNs. Major concern of the paper is the novelty. Similar ideas have been proposed before by many previous researchers. Other networks trying to address this issue have been proposed. Such as those rotation invariant neural networks. The grid search attack used in the experiments may be not convincing. Overall, this paper is not ready for publication.
The paper studies improving model for abductive natural language inference task. Specifically, they introduce information interaction layers and the joint softmax focal loss.   On positive notes, their method shows persuasive empirical gains. However, reviewers found (1) the technical novelty of the approach to be limited (reviewer croc, 3Vwo, W1Sp), (2) approaches (especially focal loss) not well motivated (reviewer hk5y), (3) there are limited take away from the paper (reviewer imYG, hk5y) and (4) claims not well supported and experimental details missing (reviewer hk5y). The reviewers further provided detailed comments that would be helpful for authors to improve the paper. Because of such limitations, in its current form, the paper is not ready for publication.
The paper presents a new approach to learn separate class invariant and class equivariant latent representations, by training on labeled (and optional additional unlabelled) multi class data. Empirical results on MNIST and SVHN show that the method works well. Reviewers initially highlighted the following weaknesses of the paper: insufficient references and contrasting with related work (given that this problem space has been much explored before),  limited novelty of the approach, limited experiments (MNIST only). One reviewer also mentioned a sometimes vague, overly hyperbolic, and meandering writeup.  Authors did a commendable effort to improve the paper based on the reviews, adding new references, removing and rewriting parts of the paper to make it more focused, and providing experimental results on an additional dataset (SVHN). The paper did improve as a result. But while attenuated, the initial criticisms remain valid: the literature review and discussion remains short and too superficial. The peculiarities of the approach which grant it (modest) originality are insufficiently (theoretically and empirically) justified and not clearly enough put in context of the whole body of prior work. Consequently the proposed approach feels very ad hoc. Finally the additional experiments are a step in the right direction, but experiments on only MNIST and SVHN are hardly enough in 2018 to convince the reader that a method has a universal potential and is more generally useful. Given the limited novelty, and in the absence of theoretical justification, experiments should be much more extensive, both in diversity of data/problems, and in the range of alternative approaches compared to, to build a convincing case. 
This paper improves upper bound estimates on Lipschitz constants for neural networks by converting the problem into a polynomial optimization problem.  The proposed method also exploits sparse connections in the network to decompose the original large optimization problem into smaller ones that are more computationally tractable. The bounds achieved by the method improve upon those found from a quadratic program formulation.  The method is tested on networks with random weights and networks trained on MNIST and provides better estimates than the baselines.  The reviews and the author discussion covered several topics.  The reviewers found the paper to be well written.  The reviewers liked that tighter bounds on the Lipschitz constants can be found in a computationally efficient manner.  They also liked that the method was applied to a real world dataset, though they noted that the sizes of the networks analyzed here are smaller than the ones in common use.  The reviewers pointed out several ways that the paper could be improved.  The authors adopted these suggestions including additional comparisons, computation time plots, error bars, and relevant references to related work.  The reviewers found the discussion and revised paper addressed most of their concerns.  This paper improves on existing methods for analyzing neural network architectures and it should be accepted.
This is a borderline case and it s quite difficult to decide the recommendation. The paper works on a critically important problem, namely removing or reducing the in distribution accuracy drop when we need to also take the out of distribution accuracy into account. The proposed method is simple and it works, which is great. However, as the reviewers discussed, the demonstrated applications are not very representative, and the authors should consider more popular setups of few short learning and even other forms of domain generalization. Furthermore, adversarial examples are also OOD (in most cases, since the ID manifolds are thin films and the attacks can easily go out of the ID manifolds), it would be great if adversarial accuracy can be incorporated as a case of OOD accuracy. Since there is still room for improvement, we hope the paper would benefit from a cycle of revisions for a re submission.
The reviewers had a hard time fully identifying the intended contribution behind this paper, and raised concerns that suggest that the experimental results are not sufficient to justify any substantial contribution with the level of certainty that would warrant publication at a top venue. The authors have not responded, and the concerns are serious, so I have no choice but to reject this paper despite its potentially valuable topic.
The paper proposes an upsampling layer design for converting layouts to images. Three reviewers rate the paper below the bar, while one reviewer rates the paper marginally above the bar. The main concern that several reviewers raise is the novelty. Particularly, R1 and R3 point out that the proposed method shares great similarity to CARAFE [Wang et al. ICCV 2019]. The AC agrees with the reviewers. 
The paper proposes monotonic splines as an improvement on current approaches to parametrising quantiles in distributional RL. The idea is an obvious, natural improvement on what exists, and yields improved experimental results.
This paper presents an explanation based learning approach that learns from both observations (examples) and explanations paired with examples. It proposes to learn an interpreter that can map from natural language sentences to examples. The authors also develop an evaluation environment and protocols for the tasks.  Strengths:   The proposed idea is intuitive and seems general   The benchmark dataset is useful resource for future research  Weakness:   The motivation of the present problem setup needs more justification   The close connection to the line of work on explanation based learning (especially recent ones in modeling natural language explanations) are not thoroughly discussed and compared.   Experiments beyond the game like datasets will help validate the claims better and justifies that the problem setup has real world applications
The paper proposed a non autoregressive attention based encoder decoder model for text to sepectrogram using attention distillation. It is shown to bring good speedup to conventional autoregressive ones. The paper further adopted VAE for the vocoder training which trains from scratch although performs worse than existing method (e.g. ClariNet).   The main concerns for this paper come from the unclear presentation: * As the reviewer pointed out, there re some misleading claims that the speedup gains was obtained without the consideration of the full context (i.e. not including the whole inference time). * The paper failed to clear present the architectures developed/used in the paper and the differences from those used in the literature. The reviewers suggested the use of diagram to aid the presentation. * The two contributions are unbalanced presented. Due to the complexities involved, it s better to explain things in more details.  The authors acknowledged the reviewers comments during rebuttal, but did not make any changes to the paper.
This paper proposes a method to solve high dimensional, continuous robotic tasks offering a trajectory optimization and a distill policy. The paper is well written and the work is promising. It is very relevant for the robotics and RL communities.
This paper proposes a VAE based model which is able to perform musical timbre transfer.   The reviewers generally find the approach well motivated. The idea to perform many to many transfer within a single architecture is found to be promising. However, there have been some unaddressed concerns, as detailed below.   R3 has some methodological concerns  regarding negative transfer and asks for more extended experimental section.  R1 and R2 ask for more interpretable results and, ultimately, a more conclusive study. R2 specifically finds the results to be insufficient.  The authors have agreed with some of the reviewers  feedback but have left most of it unaddressed in a new revision. That could be because some of the recommendations require significant extra work.  Given the above, it seems that this paper needs more work before being accepted in ICLR. 
This paper studies the problem of defending deep neural network approaches for image classification from physically realizable attacks. It first demonstrates that adversarial training with PGD attacks and randomized smoothing exhibit limited effectiveness against three of the highest profile physical attacks. Then, it proposes a new abstract adversarial model, where an adversary places a small adversarially crafted rectangle in an image, and develops two approaches for efficiently computing the resulting adversarial examples. Empirical results show the effectiveness. Overall, a good paper. The rebuttal is convincing.
While there was some support for the ideas presented, the majority of the reviewers did not think the submission was ready for presentation at ICLR. Concerns raised included that the experiments needed more work, and the paper needs to do a better job of distinguishing the contributions beyond those of past work.
This paper investigates an important problem, i.e., the fairness of the learned representation in deep metric learning, which is relatively under explored by the research community. Observing that the existing metric learning approaches become less fair when trained on an imbalanced dataset, the authors propose finDML to benchmark previous methods on multiple imbalanced datasets with three newly proposed metrics.  Further, a PARADE module is adapted into this problem to tackle the fairness issue.   The paper is meticulously written of good structure, and well motivated by experimental findings. The authors have a deep and thorough discussion with reviewers, through which the mixed preliminary ratings became all positive, with most concerns well addressed. AC found no ground for rejection and thus recommended acceptance. Authors shall integrate all response material into the next revision.
R1 and R3’s  main concern was that the work was not actually outperforming existing work and therefore its advantages were unclear. R2 brought up several questions on the experiments and asked for clarification with respect to previous work. R3 had several other detailed questions for the authors. The authors did not provide a response.
This paper presents a Feature Propagation (FP) method for dealing with missing features in graph learning tasks. The FP method is based on minimization of the Dirichlet energy and leads to a diffusion type differential equation on the graph. Empirical results demonstrated the effectiveness. However, after rebuttal major concerns still remain on the novelty and siginificance, in particular, the connection with label propogation should be better elaborated, which is crucial to understand the contributions of this paper. Considering that, I can t recommend accept the current manuscript. The authors are encouraged to further improve for a more solid publication in the future.
This paper presents an approach to domain adaptation in reinforcement learning. The main idea behind this approach, DARC, is to modify the reward function in the source domain so that the learned policy is optimal in the target domain. This is achieved by learning a classifier that learns to discriminate between the data from the source domain and those from the target domain.   Overall, reviewers appreciated the intuitiveness of the approach as well as its formal analysis. They had some concerns with respect to experiments, which was sorted out in the author response period. Given the overall positive reviews, I recommend accepting the paper.  
The authors consider the problem of predicting DNA folding patterns.                                                                They use a range of simple, linear models and find that a bi LSTM architecture                                                      yielded best performance.                                                                                                                                                                                                                                               This paper is below acceptance.                                                                                                     Reviewers pointed out strong similarity to previously published work.                                                               Furthermore the manuscript lacked in clarity, leaving uncertain eg details about                                                    experimental details. 
The authors develop a new technique for training neural networks to be provably robust to adversarial attacks. The technique relies on constructing a polyhedral envelope on the feasible set of activations and using this to derive a lower bound on the maximum certified radius. By training with this as a regularizer, the authors are able to train neural networks that achieve strong provable robustness to adversarial attacks.  The paper makes a number of interesting contributions that the reviewers appreciated. However, two of the reviewers had some concerns with the significance of the contributions made: 1) The contributions of the paper are not clearly defined relative to prior work on bound propagation (Fast Lin/KW/CROWN). In particular, the authors simply use the linear approximation derived in these prior works to obtain a bound on the radius to be certified. The authors claim faster convergence based on this, but this does not seem like a very significant contribution.  2) The improvements on the state of the art are marginal.  These were discussed in detail during the rebuttal phase and the two reviewers with concerns about the paper decided to maintain their score after reading the rebuttals, as the fundamental issues above were not   Given these concerns, I believe this paper is borderline   it has some interesting contributions, but the overall novelty on the technical side and strength of empirical results is not very high.
This paper proposes a new dimensionality reduction technique that tries to preserve the global structure of the data as measured by the relative distances between triplets. As Reviewer 1 noted, the construction of the TriMap algorithm is fairly heuristic, making it difficult to determine how TriMap ought to behave “better” than existing dimensionality reduction approaches other than through qualitative assessment. Here, I share Reviewer 2’s concern that the qualitative behavior of TriMap is difficult to distinguish from existing methods in many of the figures.  
The paper describes a VAE model for individual protein families that uses phylogenetic trees through an Ornstein Uhlenbeck process on latent space. They also use a sequence likelihood which does not factorize over positions. The paper claims these two advances represent a more expressive and efficient model of protein evolution and apply it to ancestral sequence reconstruction.  Strengths:    The technical novelty of relaxing independent sites is interesting and important   The use of a tree structured OU process over latent space is novel and natural for this problem setting   The exposition of the model itself is easy to follow and well written   A statistically well grounded approach  Weaknesses:    The evaluations do not enable strong conclusions to be reached yet. More careful ablations and comparisons are needed to understand implications of this work for scalability, representation learning, and evolutionary modeling   Lack of computational cost details  A majority of reviewers voted with high confidence for acceptance. The only reviewer voting for rejection did not respond to the author s rebuttal and did not give strong arguments for rejection. The authors are encouraged to address the reviwers  comments and improve the paper for the camera ready version.
The paper studies the problem of evaluating representations and proposes two new metrics: surplus description length and epsilon sample complexity.  Pros:   A good overview of existing methods and their corresponding weaknesses (i.e. sensitivity to dataset size and insensitivity to representation quality and computational complexity).   The proposed procedures seem to be well supported conceptually.   Has an efficient implementation.  Cons:   While the theoretical results in the Appendix are appreciated and do provide some insight into the procedures, they more or less seem straightforward and don t answer some important questions (i.e. what is the sample size necessary in terms of epsilon? is it exponential in some dimension?).    More insight could have been provided into where the noisy measurements come from in these metrics as there appear to be many components in the calculations that could be contributing to the noise (i.e. dataset distribution, dataset size, bootstrap samples, probe initializations, etc).   The methods make an assumption that the performance is monotonic in the dataset size, which is often not the case (i.e. there is a subfield regarding removing noisy label examples to improve performance; moreover there are investigations in the active learning literature that suggest sometimes performance degrades with more training data).   It appears that the proposed metrics are based on data efficiency (i.e. least number of samples to get obtain a desired performance). However, such may have more of a dependence on the distribution of the data and how the examples are chosen (i.e. they can be actively chosen) moreso than the actual representation. This may or may not be an issue but may deserve at least some discussion.  Overall, the reviewers appreciated the new methods proposed and how they relate and improve upon previous methods; however, as currently presented, most were unconvinced about its significance which was a key reason for rejection.  
The authors propose a new MLP Mixer like architecture called Cycle MLP which has two main advantages with respect to MLP Mixer: (i) it’s applicable to varying input image sizes, and (ii) linear computational complexity. The authors present competitive results on image classification, object detection and segmentation.  The reviewers felt that both (i) and (ii) are key issues in the current MLP Mixer based models. The reviewers also appreciated the simplicity of the idea and the execution of the empirical evaluation. During the rebuttal and discussion phase the authors provided compelling evidence for the issues pointed out in the initial review.   Given that MLP Mixer based architectures are becoming increasingly popular, I believe that these contributions will be of great interest to the ICLR community and I will recommend acceptance.
All reviewers recommend reject and there is no rebuttal. There is no basis on which to accept the paper.
Pros: + Interesting perspective on training deep networks  Cons:   Not a lot of practical significance: why would one want to use this algorithm over standard methods like ResNets or highway networks given that the proposed algorithm is more complex than established methods? 
The paper proposes a conditional generative adversarial network with an auxiliary discriminative classifier for conditional generative modeling. The auxiliary discriminative classifier can provide the discrepancy between the joint distribution of the real data and labels and that of the generated data and labels to the generator by discriminatively predicting the label of the real and generated data. Experiment results are provided to demonstrate the effectiveness of the proposed idea.  The current paper receives mixed ratings after rebuttal (5, 6, 5, 8). Except that one reviewer (the Reviewer uPwH) will champion the paper with a score of 8, the concerns of the other three reviewers remain. To be specific, even though Reviewer ebJs assigns a score of 6, he/she doesn’t champion the paper because additional experiments requested are not provided by the authors, including (i) training on more datasets or higher resolutions, (ii) visualizing feature norm and grad norm as done in ReACGAN, (iii) experiments on ADC GAN without unconditional GAN loss. The Reviewer DPgR pointed out that the paper might have a novelty issue because it bears some similarities with other works but it lacks a discussion in the revision. Additionally, Reviewer mZT7 pointed out that the authors didn’t provide a revised paper during the rebuttal, thus leading to a difficulty to assess the quality of the final paper. As a result, AC thinks that the paper is not ready to publish at the current stage and recommends a rejection.  The AC urges the authors to revise their paper according to the comments provided by the reviewers, and resubmit their work in a future venue.
The reviewers lean to accept, and the authors clearly put a significant amount of time into their response. I will also lean to accept. However, the comments of reviewer 2 should be taken seriously, and addressed if possible, including an attempt to cut the paper length down.
The paper studies subsampling techniques necessary to handle large graphs with graph convolutional networks.  The paper introduces two ideas: (1) preprocessing for GCNs (basically replacing dropout followed by linear transformation with linear transformation followed by drop out); (2) adding control variates based on historical activations.  Both ideas seem useful (but (1) is more empirically useful than (2), Figure 4*). The paper contains a fair bit of math (analysis / justification of the method).  Overall, the ideas are interesting and can be useful in practice. However, not all reviewers are convinced that the methods constitute a significant contribution.  There is also a question whether the math has much value (strong assumptions   also, from interpretation, may be too specific to the formulation of Kipf & Welling making it a bit narrow?).  Though I share these feelings and recommend rejection, I think that the reviewers 2 and 3 were a bit too harsh, and the scores do not reflect the quality of the paper.  *Potential typo: Figure 4   should it be CV +PP rather than CV?  + an important problem + can be useful in practical applications + generally solid and sufficiently well written   significance not sufficient   math seems not terribly useful  
The paper addresses an important problem of supervised learning for predicting graph connectivity using both node features and the overall graph structure. The paper is clearly written, and the presented approach produces promising results on synthetic data. However, all reviewers agree that the paper could be improved by including more comparison with prior art and related work discussion, and strengthening empirical results by including real life  data and more through evaluation; they also find the novelty and significance of the proposed approach somewhat limited. We hope the authors will use the suggestions of the reviewers to further improve the paper. 
This paper proposes RTFM, a new model in the field of language conditioned policy learning. This approach is promising and important in reinforcement learning because of the difficulty to learn policies in new environments.   Reviewers appreciate the importance of the problem and the effective approach. After the author response which addressed some of the major concerns, reviewers feel more positive about the paper. They comment, though, that presentation could be clearer, and the limitations of using synthetic data should be discussed in depth.  I thank the authors for submitting this paper.
The paper proposes a margin based adversarial training procedure. The paper is lacking in terms of proper dicussion of related literature e.g. similarity and differences to MMA, the "theoretical" discussion on page 5 is incomplete as there is no way how one can estimate the perturbed samples to do the analysis (the authors seem to implicitly already assume that the adversarial samples lie on the decision boundary) and the underlying assumptions are not clearly stated, the reported robust accuracies  (see https://github.com/fra31/auto attack for a leaderboard of adversarial defenses) on MNIST and CIFAR10 are worse than that of MMA which are in turn worse than SOTA. Thus this paper is below the bar for ICLR.
The paper describes an approach for automatically generating CAD sketches, including both the primitives that describe the drawing, as well as the constraints that describe relationships between the primitives that need to be maintained even if the primitives are changed. This is an important problem that is starting to receive a lot of attention from the literature.   Overall, the paper is very well executed and the results are quite compelling.   There were some concerns about the relationship with the work by Willis et al. and other papers that were published around the time when this paper was submitted. There is still some novelty in this paper relative to those works as argued in appendix H, but it would have been really good to have a more quantitative comparison. However, the authors pointed out that this work was concurrent as opposed to prior work as per the ICLR reviewer guidelines.   Overall, given the quality of this paper and the guidance given in the ICLR reviewer guide, most reviewers agree with the meta reviewer that this paper should be accepted (the lowest reviewer still indicated it is above the acceptance threshold). However, there is some discomfort around not having an explicit comparison with very closely related work that ultimately was published before this paper.
The initial reviews were a bit split. R4 was slightly positive, R3 was slightly negative, and both R1 and R2 voted for rejection. The main issue was lack of proper comparisons with the SOTA methods and missing references. In the rebuttal, the authors added additional experiments as requested, but R1 and R2 were not convinced by the new results. In particular, R1 pointed out that even the unsupervised setup in [4] achieved 0.89 AUC, outperforming 0.86 as reported in the paper. The AC agrees with R1 and R2 that the paper cannot be published until more thorough comparisons are conducted. 
The authors attempt to unify graph convolutional networks and label propagation and propose a model that unifies them. The reviewers liked the idea but felt that more extensive experiments are needed. The impact of labels needs to be specially studied more in depth.
Two knowledgeable and confident reviewers suggest rejection, while one not confident reviewer suggests acceptance. I agree with the confident reviewers. All reviewers also point out that the paper is confusingly written and difficult to understand.
This paper proposes video level 4D CNNs and the corresponding training and inference methods for improved video representation learning. The proposed model achieves state of the art performance on three action recognition tasks.  Reviewers agree that the idea well motivated and interesting, but were initially concerned with positioning with respect to the related work, novelty, and computational tractability. As these issues were mostly resolved during the discussion phase, I will recommend the acceptance of this paper. We ask the authors to address the points raised during the discussion to the manuscript, with a focus on the tradeoff between the improved performance and computational cost.
The paper proposed a meta learning approach that learns from demonstrations and subsequent RL tasks. The reviewers found this work interesting and promising. There have been some concerns regarding the clarity of presentation, which seems to be addressed in the revised version. Therefore, I recommend acceptance for this paper.
This paper proposes a new algorithm to solve the discrete optimal transport problem, consisting of showing how the well known Douglas Rachford algorithm can be efficiently applied, and also providing a convergence rate. Secondly, the paper gives an efficient implementation suitable for GPUs.  The paper would be a bit weak on just the DR contribution alone, so the efficient implementation, and experiments, are significant. However, the reviewers were not consistently happy with the experiments.  Two big issues raised by reviewers were wanting more experiments (having comparison experiments with real world data), and wanting to compare with variants of the baseline algorithm (Sinkhorn), such as the log transformed version which is more stable.  Looking at the revision, I think the reviewers have done a good job adding experiments.  Overall, for a paper with strong theoretical components, I think the computational aspects are strong.  As for (not) comparing with the log transformed Sinkhorn and other more robust methods, the authors argue that this implementation would be slower.  I agree with reviewers that it would be nice to have these comparisons, but the authors  argument is plausible and I don t find it grounds to reject the paper.  Overall, it seems there is some evidence that this is a worthwhile method, and there are no theoretical concerns other than presentation issues. Thus I think it would be a benefit to the community to accept this paper.
The authors propose to combine ideas from SDEs and time series modeling with stochastic optimal control to present a framework for modeling continuous time stochastic dynamics. The reviewers are in agreement that there are several good ideas presented here and that the interface of the perspectives the authors combine toward their proposed framework is an interesting one to explore. One referee mentions valid concerns in confusing points of the details in the presentation, and the positive reviewers echoed these concerns. In particular, more details and clearer exposition are needed for the decomposition into the subproblems and the problem of many hyper parameters. Nonetheless, my overall impression after a careful read of the paper and discussion is that these concerns are addressable and are to a degree ameliorated by the author response, and that they may be viewed as limitations outweighed by the merits of the novel ideas presented here. I emphasize that all reviewers were surprisingly consistent in their assessment of the shortcomings, and I encourage the authors to take these constructive criticisms seriously in preparing a final version of this paper.
This paper proposes a method for having a meta deep learning model generate the weights of a main model given a proposed architecture.  This allows the authors to search over the space of architectures efficiently.  The reviewers agreed that the paper was very well composed, presents an interesting and thought provoking idea and provides compelling empirical analysis.  An exploration of the failure modes of the approach is highly appreciated.  The lowest score was also of quite low confidence, so the overall score should probably be one point higher.  Pros:   Very well written and composed   "Thought provoking"   Some strong experimental results   Analysis of weaker experimental results (failure modes)  Cons:   Some weak results (also in pros, however)
There was some debate between the authors and an anonymous commentator on this paper.  The feeling of the commentator was that existing work (mostly from the PL community) was not compared to appropriately and, in fact, performs better than this approach.  The authors point out that their evaluation is hard to compare directly but that they disagreed with the assessment.  They modified their texts to accommodate some of the commentator s concerns; agreed to disagree on others; and promised a fuller comparison to other work in the future.  I largely agree with the authors here and think this is a good and worthwhile paper for its approach.  PROS: 1. well written 2. good ablation study 3. good evaluation including real bugs identified in real software projects 4. practical for real world usage  CONS: 1. perhaps not well compared to existing PL literature or on existing datasets from that community 2. the architecture (GGNN) is not a novel contribution
The paper proposes a variational family of distributions for posterior estimation in sequential latent variable models. The paper does so by extending variational recurrent neural networks so as to use a variational mixture posterior and capture more realistic multi modalities in the data.   During the review process, it was suggested to improve the clarity of the paper, provide results on an additional dataset and a visualization of the latent distributions. I commend the authors for addressing these issues satisfactorily.  Overall, the paper is well motivated and well written. However, when considering the novelty of the paper, although none of the reviewers raised this issue, I believe the paper heavily relies on previously proposed ideas and therefore, its contribution can be seen as incremental. Additionally, something important to highlight is in section 2, with regards to deep state space models (SSMs). The authors make a rather strong claim with regards to assumptions on the variational distribution. However, one can find out of the box implementations where this is not the case, see e.g. https://pyro.ai/examples/dmm.html that implements deep Markov models with posteriors based on inverse autoregressive flows. A comparison with such approaches may be also required.  
The reviewers agree that the idea of introducing structural biases in the attention mechanism is interesting but the results and presentation right now is not convincing. Improvements are seen on only some datasets and the comparisons are not exact. A reject.
This paper studies the use of a graph neural network for drug to drug interaction (DDI) prediction task (an instance of a link prediction task with drugs as vertices and interaction as edges). In particular, the authors apply structured prediction energy networks (SPEN) and model the dependency structure of the labels by minimising an energy function. The authors empirically validate the proposed approach against feedforward GNNs on two DDI prediction tasks. The reviewers feel that understanding drug drug interactions is an important task and that the work is well motivated. However, the reviewers argued that the proposed methodology is not novel enough to merit publication at ICLR and that some conclusions are not supported by the empirical analysis. For the former, the benefits of the semi supervised design need to be clearly and concisely presented. For the latter, providing a more convincing practical benefit would greatly improve the manuscript. As such, I will recommend the rejection of this paper at the current state.
The paper describes a novel learning scenario where there are many related tasks, some seen at test time, and some seen only at training time, where additionally the task labels can be hidden or present.  This approach generalizes both a "relational setting" (where auxiliary task labels could be used as features) and a "meta setting" (where new tasks need to be solved in a zero shot setting using data from related tasks only).  The idea behind the method is to do MTL with a common representation and a set of task specific heads, and build a graph where (1) tasks are nodes associated with the parameters of their task specific "heads" and (2) edges link examples to tasks with known labels.  A GNN method is then used to find regularities in the graph.  Pros    The setting is innovative and the approach is novel    The experimental results are strong  Cons    Some of the terminology seems awkward and/or strained (eg "knowledge graph" for the task example graph)
 Pro:    Interesting approach to tie together reinforcement Q learning with CNN for prediction and reward function learning in predicting downstream effects of chemical structures, while providing relevant areas for decision making.  Con:   Datasets are small, generalizability not clear.   Performance is not high (although performance wasn t the goal necessarily)   Sometimes test performance is higher than training performance, making results questionable.   Should include comparison to other wrapper based combinatorial approaches.   Too targeted an appeal/audience (better for chemical journal)
The paper presents both theoretical analysis (based upon lambda stability) and experimental evidence on stability of recurrent neural networks. The results are convincing but is concerns with a restricted definition of stability. Even with this restriction acceptance is recommended. 
As the reviewers point out, the paper seems to be below the ICLR publication bar due to low novelty and limited significance. 
This paper received mostly positive reviews. The reviewers praised the strong performance when compared with previous work. Also, the evaluation clearly shows the benefit of the proposed contributions in terms of performance. Most concerns raised by reviewers were properly addressed in the rebuttal.  Lack of comparison to several previous works has been noted in a comment, but the authors clarified this concern, stating that the current work is a “large deviation from prior works”. The authors promised to include the missing references into the comparison.  Given the reviews, comments, and author s answers, I suggest acceptance.
This paper presents an interesting analysis of mixup, discussing when it works and when it fails. The theory is further illustrated with small but intuitive examples, which facilitates understanding the underlying phenomena and verifies correctness of the predictions made by the theory. The submission has received three reviews with high variance ranging from 3 to 8: mn55 favoring rejection while eGEK recommending accept. I read all the reviews and authors  response. Unfortunately, mn55 did not follow up to express how convinced they are with author s reply, but I do find the responses to mn55 very solid and convincing. In concordance with eGEK, I do find the provided analysis important and helpful, and the presentation of the theory through concrete examples very compelling.
I thank the authors and reviewers for the lively discussions about the paper. All reviewers indicated that the work has merits and novelty however there were concerns about showing the benefits of the proposed method experimentally specially on malware applications. Given all, I think the paper needs a bit more work to be accepted.     AC
The paper proposes a new analysis of the optimization method called entropy sgd which seemingly leads to more robust neural network classifiers. This is a very important problem if successful. The reviewers are on the fence with this paper. On the one hand they appreciate the direction and theoretical contribution, while on the other they feel the assumptions are not clearly elucidated or justified. This is important for such a paper. The author responses have not helped in alleviating these concerns. As one of the reviewers points out, the writing needs a massive overhaul. I would suggest the authors clearly state their assumptions and corresponding justifications in future submissions of this work.
This paper aims at answering an interesting question that puzzles the whole community of deep learning: why CNNs perform better than FCNs? The authors show that CNNs can solve the k pattern problem much more efficiently than FCNs, which partially contributes to the answer of the question.  Pros: 1. Studies an interesting question on DNNs. 2. Constructs a specific problem, the k pattern problem, so that CNNs can solve much more efficiently than FCNs.  Cons: 1. The analysis is only a very limited answer to the question. It only shows that CNNs are more efficient than FCNs on a very specific problem, which is of little interest to the community. On the one hand, people want to see the advantage of CNNs on more common problems, perhaps the image recognition problem (The AC understands that analyzing this problem is nearly impossible. It is just for hinting the choice of problems to analyze)? On the other hand, maybe others can find another specific problem that FCNs can solve much more efficiently than CNNs. If so, the value of this paper will be totally gone. The authors did not exclude such a possibility (Nonetheless, it is still a computational "separation" between CNNs and FCNs :)). 2. Reviewer #4 pointed out an issue in the proof. The response from the authors, though looked promising, did not fully convince the reviewer (in the confidential comment). Reviewer #3 also raised a question on the bounded stepsize. The authors should address both issues.  Overall, since the problem studied is of great interest to the community and the analysis is mostly sound, the AC recommended acceptance.
The paper introduces a new locality aware importance weighted sampling procedure for distributed training of GNNs. While the paper is interesting, the reviewers raised some fundamental concerns about it.  The focus on the paper is on scalable methods and the experiments or only run on medium size datasets(<2m nodes). For such a paper larger scale experiments are expected.  Furthermore, the novelty of the paper is limited.  Overall, the paper is below the high acceptance bar of ICLR.
This paper proposes a new approach to domain adaptation based on sub spacing, such that outliers are filtered out. While similar ideas have been used e.g. in multi view learning, their application to domain adaptation makes it a novel and interesting approach.   While the above is considered by the AC an adequate contribution to ICLR, the authors are encouraged to investigate further the implications of the assumptions made, in a way that the derived criteria seem less heuristic, as R1 pointed out.  There had been some concerns regarding the experiments, but the authors have been very active in the rebuttal period and addressed these concerns satisfactorily. 
The paper proposes improvements on the area of neural network inference with homomorphically encrypted data. Existing applications typically have high computational cost, and this paper provides some solutions to these problems. Some of the improvements are due to better "engineering" (the use of the faster SAEL 3.2.1 over CryptoNet). The idea of using pre trained AlexNet features is new, but pretty standard practice. The presentation has been greatly improved in the updated version, however the paper could benefit from additional discussions and experiments. For example, when a practitioner wants to solve a new problem with some design need (e.g. accuracy, latency vs. bandwidth trade off), what network modules should be used and how should they be represented? To summarize, the problem considered is important, however, as pointed out by the reviewers, both the empirical and the theoretical results appear to be incremental with respect to the existing literature. 
All of the reviewers agree that the experimental results are promising and the proposed activation function enables a decent degree of quantization. However, the main concern with the approach is its limited novelty compared to previous work on clipped activation functions.  minor comments:   Even though PACT is very similar to Relu, the names are very different.   Please include a plot showing the proposed activation function as well. 
The reviewers were not convinced about the significance of this work. There is no empirical or theoretical result justifying why this method has advantages over the existing methods. The reviewers also raised concerns related to the scalability of the proposal. Since none of the reviewers were enthusiastic about the paper, including the expert ones, I cannot recommend acceptance of this work.
Summary:  The authors propose to predict a neural network classifier s generalization performance by measuring the proportion of parameters that can be pruned to produce an equivalent network (in terms of training error). Experimental and theoretical evaluation are provided.   Discussion: The overall opinion in reviews was that the idea is potentially interesting, but needs to be pursued further before publication, and that the empirical evaluation in particular was lacking. That was followed by a detailed discussion, in which authors were able to address a number of concerns, and have provided helpful additional experiments.  Recommendation: This is a potentially interesting paper that is not quite there yet. Although reviewers have raised scores in discussion, the case for acceptance would still be hard to make. I recommend to reject.  It looks like a reasonable  amount of additional work will turn this from what is now on the weak end of borderline into a potentially strong submission, especially given the thoughtful and thorough feedback from reviewers. The next top tier conference deadline is not far away, and I encourage the authors to incorporate the feedback fully and resubmit soon. That being said, I agree with reviewers that the theory provided is, at present, not strong. Also, a point that still seems to require work is the relation between prunability and the use of dropout.  Note to authors and chairs: AnonReviewer3 explicitly stated in discussion that they would raise their score from 5 to 6, but the change was not recorded in the system. My recommendation assumes their score is 6.
Paper was well written and rebuttal was well thought out and convincing.  The reviewers agree that the paper showed BNNs were good (relatively speaking) at resisting adversarial examples. Some question was raised about whether the methods would work on larger datasets and models. The authors offered some experiments in this regard in the rebuttal to this end. Also, a public comment appeared to follow up on CIFAR and report correlated results. 
This paper proposes to use GMM as the latent prior distribution of GAN. The reviewers unanimously agree that the paper is not well motivated, explanations are lacking and writing needs to be substantially improved. 
This paper presents a study of methods for tabular data imputation. In particular, the authors compare deep learning methods with k NN based approaches. Experiment results demonstrate k NN to be competitive with deep learning methods.  Reviewers have concerns about the characteristics of datasets used in the experiments and hyperparameters used for evaluation, which I agree with. They also think that the main contribution of comparing k NN and deep learning methods is not strong enough for acceptance to ICLR. I recommend rejecting this paper.
I fully agree with strong positive statements in the reviews.  All reviewers agree that the paper introduces a novel and elegant twist on standard RL, wherein one agent proposes a sequence of diverse tasks to a second agent so as to accelerate the second agent s learning models of the environment.  I also concur that the empirical testing of this method is quite good.  There are strong and/or promising results in five different domains (Hallway, LightKey, MountainCar, Swimmer Gather and TrainingMarines in StartCraft). This paper would make for a strong poster at ICLR.
Thanks to the reviewers and the authors for an interesting discussion. The reviewers are mixed, learning toward positive, but a few shortcomings were left unaddressed: (i) Turning the task into a mention pair classification problem ignores the mention detection step, and synergies from joint modeling are lost. (ii) Lee et al. (2018) has been surpassed by some margin by BERT and spanBERT, models ignored in this paper. (iii) Several approaches to aggregating structured annotations have already been introduced, e.g., for sequence labelling tasks. [0] Overall, the limited novelty, the missing baselines, and the missing related work lead me to not favor acceptance at this point.   [0] https://www.aclweb.org/anthology/P17 1028/
This paper proposes a channel pruning approach based one shot neural architecture search (NAS). As agreed by all reviewers, it has limited novelty, and the method can be viewed as a straightforward combination of NAS and pruning. Experimental results are not convincing. The proposed method is not better than STOA on the accuracy or number of parameters. The setup is not fair, as the proposed method uses autoaugment while the other baselines do not. The authors should also compare with related methods such as Bayesnas, and other pruning techniques. Finally, the paper is poorly written, and many related works are missing.
This paper studies trainable deep encoders/decoders in the context of coding theory, based on recurrent neural networks. It presents highly promising results showing that one may be able to use learnt encoders and decoders on channels where no predefined codes are known.  Besides these encouraging aspects, there are important concerns that the authors are encouraged to address; in particular, reviewers noted that the main contribution of this paper is mostly on the learnt encoding/decoding scheme rather than in the replacement of Viterbi/BCJR. Also, complexity should be taken into account when comparing different decoding schemes.  Overall, the AC leans towards acceptance, since this paper may trigger further research in this direction. 
This paper considers a new and practical setting of meta learning for out of domain task adaptation where a pretrained model exists but the original meta training data is not available. The authors incorporate several ideas including deep ensembles, adversarial training and uncertainty based step sizes, and achieve competitive performance under this particular setting.  The combination of various methods appears complicated, but the authors provide detailed ablation study to show the effectiveness of each component empirically. During rebuttal and discussion, they addressed many of the concerns from the reviewers. As pointed out by a reviewer, their proposed method would have a value in the domain adaptation area beyond meta learning.  The remaining concern is on the somewhat ad hoc combination of multiple methods and lack of a clear single solution for addressing the OOD few shot learning problem. Nonetheless, the proposed methods show a convincing empirical improvement on the vanilla MAML baseline in the experiments.
The paper studies how suboptimal conditioning sets create suboptimal variational approximations in variational inference with amortization in state space models.  While the point made about the role of the conditioning set is not a new one, the point was carried out further and  more clearly in this paper than previous works. Addressing a couple of issues would  make the paper stronger:    Really boiling down in the experiments to know for what models/data   the "full" approach would add value would provide concrete guidance   to the community.     Notation choices in the paper are rough. For example, Appendix A.2   reads like a type mismatch since the w on the left is a function of   z but is also equal to a function of z and C.     Adding a more detailed description of the complement of C in the   main text
AR1 is concerned about the overlap of this paper with Gama et al., 2018 as well as lack of theoretical analysis and poor results on REDDIT 5k and REDDIT 5B datasets. AR2 reflects the same concerns (lack of clear cut novelty over Zou & Lerman, 2018, Game, 2018. AR3 also points the same issue re. lack of theoretical results. The austhors admit that Zou and Lerman, 2018, and Gama, 2018, focus on stability results while this submission offers empirical evaluations.  Unfortunately, reviewers did not find these arguments convincing. Thus, at this point, the paper cannot be accepted for publication in ICLR. AC strongly encourages authors to develop their theoretical  edge  over this crowded market of GCNs and scattering approaches.
This paper combines ideas from student teacher training and multi view learning in a simple but clever way.  There is not much novelty in the methods, but promising results are given across several tasks, including realistic NLP tasks.  The improvements are not huge but are consistent.  Considering the limited novelty, the paper should include some more convincing analysis and insight on why/when the approach works. Given the intersting results, the committee recommends this for workshop track.
Good premise:  What unsupervised training supports IR?  This is a key question for IR and is a focus for papers in TREC 2019 Deep Learning Track, for instance.  Also, historically, empirical work in the IR community is a very high standard.   One reviewer says the contrastive loss for learning Siamese Transformers is not new, and prior experimental work was listed.  Several reviewers suggested extensions to the empirical work, some of which was subsequently done.  Results are "promising" according to one reviewer, but not strong enough.  Another reviewer says a different use context is needed since its hard to compete with efficient BM2t in its own terms.  The authors made some good changes to their paper: updated intro and related work, extended results and discussion, but the 4 reviewers remained in agreement, a reject.  However, some reviewers felt this was a good contribution, so with further empirical work and polish it should be good.
This paper introduces an objective for representation learning that captures "controllable elements" in the environment (i.e., things that are affected by the agent s actions). In their reviews and discussion, the reviewers agreed this idea was intuitive, well motivated, and the paper well written. However, multiple reviewers raised concerns about the evaluation and the extent to which LCER is truly an improvement over PI SAC. Although many of the reviewer s concerns were addressed in the rebuttal period, at the end of the discussion they were still unconvinced or confused about how much LCER really helps over PI SAC. Based on this, my assessment is that this paper is a promising piece of work, and that with some more controlled comparisons (see suggestion below) it would be a useful contribution to the literature. However, given that the claims are not fully supported as it currently stands, I recommend rejection.  Specific suggestion to improve the paper: based on reading the paper and the discussion, it seems to me (as per the authors  own statement in response to Reviewer uWv6) that the most valid/controlled comparison between LCER and PI SAC is in Figure 4, where LCER w/ $\beta 0.1$ "can be seen as a variant of PI SAC with the same embedding choices as LCER" (author s words). However, when taking into account the error bars of the training curves, other values of $\beta$ are only clearly better than $\beta 0.1$ in 1/3 environments (D.walker walk). This does not make for a particularly convincing result that LCER is better than PI SAC. To improve the paper, I d encourage the authors to run further well controlled comparisons such as this in a larger number of environments. If they can show via such controlled comparisons that LCER is generally better than PI SAC (i.e. LCER w/ $\beta 0.1$) then that would be a much more compelling demonstration of LCER s superiority.
Reviewers are unanimous that this is a reject. A "class project" level presentation. Errors in methodology and presentation. No author rebuttal or revision
This paper focuses attacks on federated learning. The reviewers had the following concerns:   The assumption of knowledge of batch indices is unrealistic in an HFL setting   The setup only works when doing a single epoch (I believe the authors claim that it is applicable in more general settings, but evidence to that effect has not been provided)   The novelty of the approach is somewhat limited.   The description of the algorithm and comparison to prior work could be clearer.  I raised the question of whether the reviewers would be more positive if there were no claimed results on HFL. They still did not seem positive enough to justify acceptance (due to the other reasons mentioned above).
This paper proposes using object centered graph neural network embeddings of a dynamical system as approximate Koopman embeddings, and then learning the linear transition matrix to model the dynamics of the system according to the Koopman operator theory. The authors propose adding an inductive bias (a block diagonal structure of the transition matrix with shared components) to limit the number of parameters necessary to learn, which improves the computational efficiency and generalisation of the proposed approach. The authors also propose adding an additional input component that allows for external control of the dynamics of the system. The reviewers initially had concerns about the experimental section, since the approach was only tested on toy domains. The reviewers also asked for more baselines. The authors were able to answer some of the questions raised during the discussion period, and by the end of it all reviewers agreed that this is a solid and novel piece of work that deserves to be accepted. For this reason I recommend acceptance.
The paper proposes to get universal adversarial examples using few test samples. The approach is very close to the Khrulkov & Oseledets, and the abstract for some reason claims that it was proposed independently, which looks like a very strange claim. Overall, all reviewers recommend rejection, and I agree with them.
This submission has been assessed by three reviewers who scored it as 3/3/3. The main criticism includes lack of motivation for sections 3.1 and 3.2, comparisons to mere regular self attention without encompassing more works on this topic, a connection between Theorem 1 and the rest of the paper seems missing. Finally, there exists a strong resemblance to another submission by the same authors which is also raises the questions about potentially a dual submission. Even excluding the last argument, lack of responses to reviewers does not help this case. Thus, this paper cannot be accepted by ICLR2020.
This paper proposes a method of multi agent reinforcement learning that separately deals with the risk associated with uncertainties of the other agents and the risk associated with the uncertainties of the environment. This allows for example to be agent wise risk seeking and environment wise risk averse.  The proposed approach is largely heuristic with little theoretical justifications.  The experimental results are promising but not sufficiently convincing, given the lack of formalism.  Further improvement on clarity might complement the lack of formalism or theoretical justifications.
This paper addresses the problem of estimating treatment responses involving a continuous dosage parameter.  The basic idea is to learn a GAN model capable of generating synthetic dose response curves for each training sample, which then facilitates the supervised training of an inference model that estimates these curves for new cases.  For this purpose, specialized architectures are also proposed for the GAN, which involves a multi task generator network and a hierarchical discriminator network.  Empirical results demonstrate improvement over existing methods.  While there is always a chance that reviewers may underappreciate certain aspects of a submission, the fact that there was a unanimous decision to reject this work indicates that the contribution must be better marketed to the ML community.  For example, after the rebuttal one reviewer remained unconvinced regarding explanations for why the proposed method is likely to learn the full potential outcome distribution.  Among other things, another reviewer felt that both the proposed DRGAN model, and the GANITE framework upon which it is based, were not necessarily working as advertised in the present context.
The paper provides a new covariant approach to 3D molecular generation motivated by the desire handle compounds with symmetries. To this end, the method uses equivariant state representations for autoregressive generation, built largely from recently proposed covariant molecular networks (comorant), and integrating such representations within an existing actor critic RL generation framework (Simm et al). The selection of focal atom, element to add, and the distance are realized in an equivariant manner while the compound valuation remains invariant to rotation. The approach is clean and well executed. The authors added additional experiments (e.g., RMSD demonstrating stability of generated compounds) to further reinforce the case for the method.  
This paper introduces a recurrent neural network approach for learning diffusion dynamics in networks. The main advantage is that it embeds the history of diffusion and incorporates the structure of independent cascades for diffusion modeling and prediction. This is an important problem, and the proposed approach is novel and provides some empirical improvements.  However, there is a lack of theoretical analysis, and in particular modeling choices and consequences of these choices should be emphasized more clearly. While there wasn t a consensus, a majority of the reviewers believe the paper is not ready for publication.  
The reviewers pointed out that the claims made in this submission have already appeared (in even stronger forms) before, to which the authors seem to agree. Therefore, this submission is not ready for publication in its current form. 
The authors take two algorithmic components that were proposed in the context of discrete action RL   priority replay and parameter noise   and evaluate them with DDPG on continuous control tasks. The different approaches are nicely summarized by the authors, however the contribution of the paper is extremely limited. There is no novelty in the proposed approaches, the empirical evaluation is inconclusive and limited, and there is no analysis or additional insights or results. The AC and the reviewers agree that this paper is not strong enough for ICLR.
This work is novel, reasonably clearly written with a thorough literature survey. The proposed approach also empirically seems promising. The paper could be improved with a bit more discussion about the sensitivity, particularly as a two timescale approach can be more difficult to tune.
I have to agree with the reviewers here and unfortunately recommend a rejection.  The methodology and task are not clear. Authors have reformulated QA in SQUAD as as ranking and never compared the results of the proposed model with other QA systems. If authors want to solve a pure ranking problem why they do not compare their methods with other ranking methods/datasets.
Main content:  Blind review #1 summarizes it well:  This paper presents a new reading comprehension dataset for logical reasoning. It is a multi choice problem where questions are mainly from GMAT and LSAT, containing 4139 data points. The analyses of the data demonstrate that questions require diverse types of reasoning such as finding necessary/sufficient assumptions, whether statements strengthen/weaken the argument or explain/resolve the situation. The paper includes comprehensive experiments with baselines to identify bias in the dataset, where the answer options only model achieves near half (random is 25%). Based on this result, the test set is split into the easy and hard set, which will help better evaluation of the future models. The paper also reports the numbers on the split data using competitive baselines where the models achieve low performance on the hard set.     Discussion:  While the authors agree this is an important direction, there are reservations concerning the small size of the dataset, that have not been fully addressed.     Recommendation and justifcation:  I still believe this paper should be accepted as the existing datasets for reading comprehension are inadequate and it is important for the field not to be climbing the wrong hill.
This paper suggests a method for defending against adversarial examples and out of distribution samples via projection onto the data manifold. The paper suggests a new method for detecting when hidden layers are off of the manifold, and uses auto encoders to map them back onto the manifold.   The paper is well written and the method is novel and interesting. However, most of the reviewers agree that the original robustness evaluations were not sufficient due to restricting the evaluation to using FGSM baseline and comparison with thermometer encoding (which both are known to not be fully effective baselines).   After rebuttal, Reviewer 4 points out that the method offers very little robustness over adversarial training alone, even though it is combined with adversarial training, which suggests that the method itself provides very little robustness. 
The paper presents an unsupervised method for learning Full Waveform Inversion in geophysics, by combining a differentiable physics simulation with a CNN based inversion network.  The reviewers agreed that the paper was well written and described an important advance but were concerned about limited novelty and a potential sim2real gap. The authors responded to their critique with significant new experiments and clarified the novelty of their method relative to prior work.  Based on the author responses, I recommend acceptance.
The paper extends the work on randomized smoothing for certifiably robust classifiers developed in prior work to a weaker specification requiring that the set of top k predictions remain unchanged under adversarial perturbations of the input (rather than just the top 1). This enables the authors to achieve stronger results on robustness of classifiers on CIFAR10 and ImageNet (where the authors report the top 5 accuracy).  This is an interesting extension of certified defenses that is likely to be relevant for complex prediction tasks with several classes (ImageNet and beyond), where top 1 robustness may be difficult and unrealistic to achieve.  The reviewers were in consensus on acceptance and minor concerns were alleviated during the rebuttal phase.  I therefore recommend acceptance.
While it’s commonly acknowledged that the paper is well written, the reviews are a bit split: R3 and R1 are mildly positive/negative, respectively, R2 and R4 both voted for reject. R2 asked many questions regarding experiments, which were addressed in the details in the rebuttal. R4 raised 6 questions regarding the bound, and the authors only answered some of them in the rebuttal. R4 felt “the method is lacking in a theoretic proof of a strict bound, which is the primary contribution of the paper”. Both R1 and R4 pointed out the proposed algorithm is not practical as expected, especially the results on larger scale such as ImageNet are missing.   The AC cannot agree with the authors’ argument that the contribution of the paper is “a conceptual framework that it is possible to certify a watermark for neural networks” in responding to such criticisms. It’s indeed very important for this conceptual framework to be proven valuable through thorough experiments and solid comparisons. 
The 4 reviewers all had a consistent view of this paper:  concern that the scope of the work was overstated (paper claims, without evidence, to apply in more generality than the 1 example scenario shown); concern about the difficulty of implementing this approach (1 TSNN required for each rendered viewpoint); and lack of examples showing how the method performs under more challenging scenarios.  The AC encourages the authors to revise the work in response to the reviews.  That would involve additional experimentation and examples, and some attention to revising the manuscript.   After two of the reviewers complained of lack of clarity in the algorithm description, the authors replied, "We explain our algorithm in the paper; the reader can refer to our code for implementation details."  I hope the authors can be more responsive to the readers  concerns than that in their revisions.
The paper introduces a model agnostic heuristic for batch active learning.  There was an agreement among the reviewers that it s a good approach to try and report about, but the paper was ultimately rejected after calibration.  There were two concerns raised in the reviews, and the authors are encouraged to address them in a revision:  1) Several reviewers commented on issues with readability, affecting the paper s reproducibility (see reviews for details).  The reviewers would have also liked to see more evidence of empirical robustness to various choices made.  2) For the paper to be compelling, it should either compare with gradient based approaches like BADGE (Ash et al. 2019) or include experiments with a representation where BADGE can t be applied (to support the model agnostic distinction the authors are making).  The core motivation is the same, with both approaches trying to explicitly incorporate predictive uncertainty and sample diversity, and it would be interesting to see a comparison.
This is a borderline paper. The scores were initially below the bar. The novelty of the work is limited and there are strong claims in the paper that should be revised. The authors can also do a better job in positioning their work with respect to the existing results. However, the authors managed to address several questions/concerns of the reviewers and convince them to raise their scores. I would strongly recommend the authors to address the rest of the reviewers  comments, especially those related to strong claims and connection to related work, and further improve their work in preparing its final draft.
This paper received 5 quality reviewers, where 3 of them rated 5 and 2 rated 3. While this work has merits, many concerns are raised by various reviewers. The AC agrees with the reviewers that this paper is not ready for publication at its current form.
The reviewers concerns regarding novelty and the experimental evaluation have been resolved accordingly and all recommend acceptance. I would recommend removing the term "unsupervised" in clustering, as it is redundant. Clustering is, by default, assumed to be unsupervised.  There is some interest in extending this to non vision domains, however this is beyond the scope of the current work.
This paper proposes a deep network architecture for learning to predict depth from images with sparsely depth labeled pixels.   This paper was subject to some discussion, since the authors felt that the approach was interesting and the problem well motivated. Some of the concerns about experimental evaluation (especially from R1) were resolved due the author s rebuttal, but ultimately the reviewers felt the paper was not yet ready for publication. 
The reviewers unanimously recommend rejecting this submission and I concur with this recommendation. The submission essentially introduces a regularization technique to solve the alleged problem of Adam getting worse out of sample error for typical image classification problems, e.g. training ResNets on ImageNet. Reviewers raised a variety of issues with the submission. Some found the experiments unconvincing, some were concerned that the submission duplicated closely related work without engaging with and citing that work, and some were concerned by what they viewed as insufficient analysis and comparisons. To me, the most severe issue with the submission is that the experimental evidence for its claims is not sufficiently convincing and the problem it purports to solve has not been convincingly demonstrated, making the work hard to motivate. The other issues raised by the reviewers are less damaging in my view.  Although this is a meta review and not a full de novo review, I would be remiss to not raise a few of the severe issues I see with the results that makes it hard for them to be convincing. The Adam results in table 1 are far weaker than they should be, raising questions about the experiments as a whole. For example, https://arxiv.org/abs/2102.06356 reports 76.4% top 1 accuracy for ResNet 50 on ImageNet with Adam without increasing the epsilon parameter to a larger value as Choi et al. 2019 did (who also report good Adam results for ResNet 50 on ImageNet). This should also lead us to question one premise of the paper that there is some problem with adaptive optimizers for image classification.  Ok, but perhaps LAWN helps validation error even if there is no gap between SGD and Adam? Sadly, to demonstrate this subordinate claim, LAWN would have to be compared carefully with state of the art regularization techniques and compared with results that use any optimizer, not just Adam. With modern regularization techniques, it isn t hard to get 77%+ top 1 validation accuracy on ImageNet with ResNet 50. See, for example https://arxiv.org/abs/2010.01412v1 which gets 77.5% in 100 epochs and as high as 79.1 with longer training. Since LAWN is claiming to improve generalization, it must be compared with other regularization techniques. It is a type error to primarily compare it with optimizers so even if there weren t concerns with the performance of the existing baselines, there would need to be additional comparisons.  The claims about fixing issues that arise at large batch sizes are prima facie problematic since there isn t strong evidence of an actual problem at the batch sizes considered in the submission.
The paper proposed a shot conditional form of episodic fine tuning approach for few shot image classification. There were a number of concerns raised, e.g., there lacks of sufficient comparison with SOTA baselines, the justification on the significance of shot aware approach is not entirely convincing, and incremental contributions in both novelty and improvements. While some of these issues were improved in the rebuttal, the revision remains not satisfied by the reviewers. Overall, I think the paper has some interesting idea, but is still not ready for publication. 
The paper proposes a method for learning a latent dynamics model for videos. The main idea is to learn a latent representation and model the dynamics of the latent features via residual connection motivated by ODE. The architectural choice of residual connection itself is not new as many prior works have employed "skip connections" in hidden representations but the notion of connecting this with ODE and factoring time as input into the residual function seems a new idea. The experimental results show the promise of the proposed method on moving MNIST, KTH, and BAIR datasets. The experiments on different frame rates are also nice.  In terms of weakness, the evaluation is performed on relatively simple domains (e.g., moving MNIST and KTH) with static backgrounds and the improvement on BAIR dataset (which is not considered as a difficult benchmark) in terms of FVD is not as clear. For the BAIR dataset, it s unclear how the proposed method will handle the interactions between the robot arm and background objects due to the modeling assumption (i.e., static background). In this sense, content swap results on BAIR dataset look quite anecdotal, and the significance is limited. For improvement, I would suggest adding evaluations on other challenging domains, such as Human 3.6M (where human motions are much more uncertain compared to KTH) and other Robot datasets with more complex robot object interactions. Overall, the paper proposes an interesting architecture with promising results on relatively simple datasets, but the advantage over existing SOTA methods on challenging benchmarks is unclear yet. 
This paper proposes a new approach to solve mixed discrete continuous action RL problems, based on embedding actions into a latent space so that standard continuous control algorithms (like TD3) can be applied. Experiments over standard discrete continuous benchmarks demonstrate the superiority of the proposed approach vs. existing baselines.  There is overall a strong consensus of all reviewers towards acceptance, especially after the discussion period where the authors were able to submit several revisions addressing most of the questions and concerns raised in the original reviews, in particular w.r.t. the quality and relevance of the results.  I believe this submission could be improved along two axes though:  1. As pointed out by some reviewers, the current environments are somewhat simple. A more realistic robotic task for instance could be a good fit for such an algorithm. That being said, as the authors pointed out, this may require custom development due to the lack of existing public environment with the proper setup.  2. As a reviewer mentioned, there is no "Related Work" section, and although previous work is discussed in the Introduction, I consider that it remains limited, and more previous work should have been discussed. Here are some pointers regarding relevant work I am aware of:   Hierarchical Approaches for Reinforcement Learning in Parameterized Action Space (https://arxiv.org/abs/1810.09656)   Neural Ordinary Differential Equation Value Networks for Parametrized Action Spaces (https://openreview.net/forum?id 8WKd467B8H)   Improving Action Branching for Deep Reinforcement Learning with A Multi dimensional Hybrid Action Space (https://ipsj.ixsq.nii.ac.jp/ej/index.php?action pages_view_main&active_action repository_action_common_download&item_id 199976&item_no 1&attribute_id 1&file_no 1&page_id 13&block_id 8)   Distributed Reinforcement Learning with Self Play in Parameterized Action Space (https://cgdsss.github.io/pdf/SMC21_0324_MS.pdf)   Discrete and Continuous Action Representation for Practical RL in Video Games (https://arxiv.org/abs/1912.11077)   Multi Pass Q Networks for Deep Reinforcement Learning with Parameterised Action Spaces (https://arxiv.org/abs/1905.04388)  In particular, I believe the last one (MP DQN) should have been one of the baselines, since it is supposed to be an improvement over the P DQN algorithm (that is one of the baselines used here). I encourage the authors to try and incorporate it for the final version (at the very least, it should be cited).  In spite of these concerns, I still recommend acceptance since the combination of action space embedding with mixed discrete continuous actions is novel and non trivial, and the empirical validation is convincing enough in its current state.
Taking all reviews and the work in consideration, unfortunately the work does not present the breadth it needs to sustain the claims it makes. In particular, there work requires to analyse more architectures/variations of datasets with different properties and to provide more careful ablation studies that shows the efficiency of the 3 different proposed methods. Potentially removing one of this methods in order to give more space to analyse the others that seem more promising. 
DictFormer is a method to reduce the redundancy in transformers so they can deployed on edge devices. In the method, a shared dictionary across layers and unshared coefficients are used in place of weight multiplications. The author proposed a l1 relaxation to train the non differentiable objective  to achieve both higher performance and lower parameter counts.  All reviewers ended up giving the paper a score of 6 after increasing their scores during discussions. While the results are strong (better performance at much lower parameter counts), the paper is not clearly written. Several reviewers noted that the paper is difficult to understand and has a few unresolved points. For example, the method also ended up performing better than the base transformer model that DictFormer is supposed to compress. There seems to be a lack of understanding about what part of the model delivered the improvements. One reviewer said that this is potentially a great paper that deserves to be better explained. The basic concept of sharing a dictionary across layers should be simple enough to explain well and deserve a better explanation than eq 5.   The authors promise to release the code, which would be necessary for a full dissemination of this work. I recommend accept.
The paper proposes a model to defend against multiple lp norm attacks by classifying those attacks. The reviewers raised several concerns about the methodologies. Furthermore, it s not clear how the proposed algorithm can deal with an unseen attack (e.g., only trained on l1, l_infty attacks but encounter l2 attack in the testing phase). The assumption that the attack types are known beforehand is restricted.  
 This paper focuses on the problem of robustness in the network with random loss of neurons.  However, reviewers had issues with insufficient clarity of the presentation, and lack of discussion about closely related dropout approach.     
This paper presents an approach for online continual learning where only a single pass over each task s data is allowed. Instead of the oft used softmax classification setting in continual learning, the paper proposes to use the generative setting based on the nearest class mean (NCM). The paper claims that it avoids the logits bias problem in the softmax classifier and helps combat catastrophic forgetting.  While the reviewers found the basic idea interesting, there were concerns about novelty and lack of clarity regarding the reasons for improved performance. In particular, there are several aspects from existing work that are leveraged in this paper (e.g, replay, metric learning loss, combination of generative and discriminative classification, etc) but the paper lacks in establishing which of these components affect the performance and in what ways.  The authors and reviewers engaged in detailed discussions; however, the reviewers were still unsatisfied and did not change their assessment. Based on my own reading of the paper as well as going through the reviews and discussions, I too concur with their assessment. It would be a stronger paper if the paper could shed more light on the above aspects as well as address the other concerns raised by the reviewers. However, in the current shape, it is not ready for publication.
This submission proposes a variant of population based training (PBT) for hyperparameter selection/evolution, aimed at addressing drawbacks of existing variants (e.g. the coupling of the choice of checkpoint with the choice of hyperparameters). Reviewers generally agreed that the paper is interesting and covers an important topic, and the evaluation does show improvements over existing PBT variants. On the other hand they also raised a few important issues:  1. The `hoptim` library is claimed as a primary contribution of the work, but it is not clear from the manuscript what benefits this library offers over existing software. When claiming a library as a main contribution, it is helpful to provide a more thorough description of the software and its benefits, and/or ideally a link (anonymized for review) to the software. The authors did respond by providing a brief description of the benefits of the library, mitigating this issue somewhat. However it s still difficult to discern how/whether to weigh the open source library as a main contribution of the paper.  2. The evaluation is not very convincing: the differences are small and error margins are not provided for the neural network based experiments, meaning that any differences could be due to noise. The authors fairly point out that it is difficult to perform multiple runs of these experiments as the resource requirements are large, and they have done 20 runs of the Rosenbrock experiment with smaller compute requirements. But the reviewers were not convinced that the Rosenbrock experiment reflects the method s application to neural network hyperparameter selection; the problems are too different. The submission would be significantly stronger if it included results over multiple runs of an "intermediate" sized experiment on a problem involving a neural network demonstrating that ROMUL outperforms competing approaches by a statistically significant margin.  3. The proposed approach is ultimately heuristic. This is not necessarily a problem if there are strong empirical results demonstrating the efficacy of the proposed heuristic, but in this case the empirical results didn t convince (see point 2).  Given these concerns raised by reviewers, the submission is not quite ready for ICLR. I hope the authors will consider resubmitting the paper after improving it based on the reviewers  feedback.
This is a promising idea, without enough empirics to substantiate its potential utility, and also with a lack of clarity on the importance of the outlined task itself (fold based rather than structure based conditional sequence generation). There remain concerns about the lack of a more comprehensive comparison to methods for structure to sequence (e.g. Ingraham was added during the revision but only in a limited capacity), or easy generalizations of them, and about the quality of some of the presented results. Additionally, the concern about sensitivity to rotational invariances, and related issues wrt the fixed size cubic grid were not satisfactorily addressed. As a side note, the quality of the manuscript in terms of scholarliness of presentation was overall lacking.
Pros:   a method that obtains convergence results using a using time dependent (not fixed or state dependent) softmax temperature.  Cons:   theoretical contribution is not very novel   some theoretical results are dubious   mismatch of Boltzmann updates and epsilon greedy exploration   the authors seem to have intended to upload a revised version of the paper, but unfortunately, they changed only title and abstract, not the pdf   and consequently the reviewers did not change their scores.  The reviewers agree that the paper should be rejected in the submitted form.
This work proposes use of two pre trained FST models to explicitly incorporate semantic and strategic/tactic information from dialog history into non collaborative (negotiation) dialog systems. Experiments on two datasets from prior work show the advantage of this model in automated and human evaluation. While all reviewers found the work interesting, they made many suggestions regarding the presentation. Author (s) rebuttal included explanations and changes to the presentation. Hence, I suggest acceptance as a poster presentation.
All reviewers agree that the contributions of this paper are not significant, and the paper does not compare well with many of the existing works. Authors did not respond.
All the reviewers agree that the paper presents an interesting idea, and the main concern raised by the reviewers was the clarity of the paper. I believe that the authors have improved the presentation of the paper after rebuttal, however, I still believe that the paper woudl require another round of reviews before being ready for publication, in order to properly assess its contributions. 
This method proposes a primal approach to minimizing Wasserstein distance for generative models. It estimates WD by computing the exact WD between empirical distributions.  As the reviewers point out, the primal approach has been studied by other papers (which this submission doesn t cite, even in the revision), and suffers from a well known problem of high variance. The authors have not responded to key criticisms of the reviewers. I don t think this work is ready for publication in ICLR. 
The paper proposes new techniques for improving the generalization ability of deep learning models for Knowledge Tracing (KT). Instead of designing more sophisticated models, the paper investigates simple data augmentation techniques that can be applied to train existing models. In particular, three different augmentation strategies are proposed based on replacement, insertion, and deletion in the training data. These strategies are then applied with appropriate regularization loss ensuring consistency and monotonicity in the training process. Extensive experiments are performed using three popular neural models for KT and four publicly available datasets. Overall, the paper studies an interesting problem in an important application domain of online education. The results are promising and open up several exciting follow up research directions to explore more complex data augmentation techniques for KT.  I want to thank the authors for actively engaging with the reviewers during the discussion phase and sharing their concerns about the quality of the reviews.  The reviewers generally appreciated the paper s ideas; however, there was quite a bit of spread in the reviewers  assessment of the paper (scores: 4, 5, 6, 7). In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should incorporate the reviewers  feedback to better position the work w.r.t. the existing literature on data augmentation and state of the art results, better motivate the data augmentation strategies in the context of educational applications possibly through additional data analysis, and add more ablation studies w.r.t. the hyperparameters associated with data augmentation. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers  feedback when preparing future revisions of the paper. 
This paper presents a new DDPM model based on solving differential equations on a manifold.  The resulting numerics appear to be favorable, with faster performance than past models.  Most of the reviews thought the main result was of interest and were impressed with the performance.  Reviewer c9bY points out some challenging issues and analytical questions that remain unanswered in the text; they also have some simpler textual revisions that seem less important.  In general, this paper has the misfortune of receiving reviews whose confidence appears to be low.  While partially this is a byproduct of the noisy machine learning review system, the difficulty of the text itself is substantial and made the paper less than approachable; the authors are encouraged to continue to revise their text based on feedback from as many readers as possible.  That said, the authors were quite responsive to reviewer comments during the rebuttal phase, which significantly improved the text.  Overall this is a borderline case, and the AC also had some difficulty following details of this technically dense paper.  Given the positive *technical* assessments of the work and at least one reviewer defending the paper s clarity, the AC is willing to give this paper the benefit of the doubt.
This paper makes a contribution in the literature of cooperative multi agent reinforcement learning by proposing a decentralized and communication efficient training framework under a fully observable setting. The paper first defines the homogeneous or permutation invariant subclass of Markov games (homogeneous MG), where it is proved that sharing policy parameters does not loose optimality. The paper then proposes an actor critic algorithm for the homogeneous MG. The proposed approach is empirically supported. The reviewers had originally raised concerns or confusions, but no major concerns remain after discussion.   Overall, the paper studies an interesting and practically relevant setting, providing new insights and solid basis for policy sharing that has been used in the literature without much understanding.
This paper studies the role of topology in designing adversarial defenses. Specifically , the authors study defense strategies that rely on the assumption that data lies on a low dimensional manifold, and show theoretical and empirical evidence that such defenses need to build a topological understanding of the data.  Reviewers were initially positive, but had some concerns pertaining to clarity and limited experimental setup. After a productive rebuttal phase, now reviewers are mostly in favor of acceptance, thanks to the improved readibility and clarity. Despite the small scale experimental validation, ultimately both reviewers and AC conclude this paper is worthy of publication.  
The paper presents a quantization method that generates per layer hybrid filter banks consisting of full precision and ternary weight filters for MobileNets. The paper is well written. However, it is incremental. Moreover, empirical results are not convincing enough. Experiments are only performed on ImageNet. Comparison on more datasets and more model architectures should be performed.
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
This paper introduces Noisy Feature Mixup: an extension of input mixup and manifold mixup to all layers of a neural net, for the purpose of improving robustness and generalization in supervised learning. Experimental validation supports the increased robustness to attacks on the input data. The reviewers find the paper well written and they appreciate the theoretical analysis as well as the empirical results. The reviewers did not identify any big problems, and their minor concerns were sufficiently addressed in the author reponse. I m therefore happy to recommend accepting this paper.
This paper proposes a way to construct group equivariant neural networks from pre trained non equivariant networks. The equivarification is done with respect to known finite groups, and  can be done globally or layer wise. The authors discuss their approach in the context of the image data domain. The paper is theoretically sound and proposes a novel perspective on equivarification, however, the reviewers agree that the experimental section should be strengthened and connections with other approaches (e.g. the work by Cohen and Welling) should be made clearer. The reviewers also had concerns about the computational cost of the equivarification method proposed in this paper. While the authors’ revision addressed some of the reviewers’ concerns, it was not enough to accept the paper this time round. Hence, unfortunately I recommend a rejection.
This paper proposes to unroll power iterations within a Slow Feature Analysis learning objective in order to obtain a fully differentiable slow feature learning system. Experiments on several datasets are reported.   This is a borderline submissions, with reviewers torn between acceptance and rejection. They were generally positive about the clarity and simplicity of the presentation, whereas they raised concerns about the relative lack of novelty (especially related to the recent SpIN model), as well as the current limitations of the approach on large scale problems. Reviewers also found authors to be responsive and diligent during the rebuttal phase. The AC agrees with this assessment, and therefore recommends rejection at this time, encouraging the authors to resubmit to the next conference cycle after addressing the above points. 
This paper presents a novel methodology for performing meta learning for gradient based hyperparameter optimization.  The approach overcomes limitations (scaling, e.g.) of previous methods through distilling the gradients of the hyperparameters.  The paper received 4 reviews, of which all were positive (6, 6, 8, 8).  The reviewers appreciated the technical clarity of the paper and found the proposed approach sensible, novel, technically sophisticated and effective.  The main concerns were regarding the comprehensiveness of the experiments and technical presentation of the dataset distillation.  It seems that the reviewers found the author response (lots of results were added) satisfactory regarding these points.  Thus the recommendation is to accept.
This paper proposes a general framework to use MT to solve structural prediction problems. The method is well developed and be verified in an arrange of tasks including entity recognition, relation classification, event extraction, semantic role labelling, coreference resolution and dialog state tracking and achieves new state of the art in some of these tasks.   Further experiments also suggest the method is especially effect for low resource scenario, if the label semantics can be used appropriately.Further experiments also suggest the method is especially effect for low resource scenario, if the label semantics can be used appropriately. All reviewers agreed to accept the paper and gave very positive comments.  Some reviewers pointed out that the methods do not improve the performance significantly in some of the tasks.  And more analysis is wanted (by reviewer1).
This work analyses the impact of mini batch size on the variance of the gradients during SGD, in the context of linear models. It shows an inverse relationship between the variance of the gradient and the batch size for such models, under certain assumptions. Reviewers generally agree that the work is theoretically sound. However, all reviewers believe that the contributions of this work are limited. This concern was not adequately addressed during the discussion phase and led to the ultimate decison to reject.
The paper proposes a planning framework that uses a transformer based architecture as an attention mechanism that guides the search of a traditional sample based planner (e.g., RRT*). More specifically, features extracted from a sliding window over the 2D search space serve as input to a transformer that produces a mask indicating where to draw samples from. By constraining the search space for the sample based planner, the method reduces the time required for planning. The method is compared to both traditional and learning based planners on different 2D navigation tasks and found to improve sample complexity (and, in turn, computation time), while also being capable of generalizing to unseen and real world maps.  The manner by which the method combines the advantages of sample based planning with an attentional mechanism as a way to constrain the sampling process is interesting. As the reviewers emphasize, the experimental evaluation shows that this approach results in performance gains over both traditional (sample based) and learning based planners, while also being able to scale to larger maps as well as better generalize to out of distribution settings (compared to learning based methods). These results support the value of both the overall approach as well as the architectural components (e.g., the transformer and the use of positional encoding). The reviewers initially raised a few concerns with the paper, the most notable of which are the need to include preprocessing in the overall computation time, the accuracy of some of the claims in the paper (e.g., with regards to generalizability), generalization to higher dimensional domains, and the performance on the Dubins car domain. The authors responded to each of the reviews and updated the submission to address many of these concerns. However, questions still remain regarding whether or not the approach can be adapted to state/configuration spaces with more than two dimensions, something that traditional planners are readily capable of, and the unconvincing results on the Dubins car domain.  Overall, the paper proposes an interesting approach to an important problem that is relevant to the robotics and machine learning communities. The paper makes promising contributions to improve the efficiency of planning, however the significance of these contributions needs to be made clearer.
Paper proposes and demonstrates a method to reconstruct 3d shape for a tree, from drone data.  While the reviewers all appreciated to work, all felt there were many shortcomings of the paper with respect to an ICLR audience: (a)  no machine learning novelty (b)  highly interactive data processing method (c) only one example processed tree shown (d) inadequate connections with relevant literature on 3d reconstruction, both general purpose, and examples applied to vegetation. (e) incomplete presentation of the method:   no ablation studies, no listing of the times required for individual steps of the processing.  In view of these concerns, we have decided to reject the paper.  But we hope you find the reviewers  comments helpful, and make use of them in a revision of the work. 
The paper presents the Stein gradient estimator, a kernelized direct estimate of the score function for implicitly defined models. The authors demonstrate the estimator for GANs, meta learning for approx. inference in Bayesian NNs, and approximating gradient free MCMC. The reviewers found the method interesting and principled.  The GAN experiments are somewhat toy ish as far as I am concerned, so I d encourage the authors to try out larger scale models if possible, but otherwise this should be an interesting addition to ICLR.
The paper is very interesting and novel, and all reviewers are of the same opinion.  The main concern, however, is on the experimental section that is limited to image classification benchmarks and that some critical comparisons are missing (e.g. clarify factors that play key role in improvement, more computation and therefore more free parameters, how about non discriminative tasks, etc).  The heterogeneity question is in my opinion only partially answered by the authors but I also feel proper handling of this matter would require a proper multi task setup and different target for the work. I also personally find applicability of the approach quite limited, I encourage the authors to further improve their work as I feel that with a proper revision would make a nice contribution for the community.
There is a consensus that the contribution is not strong enough to effectively argue for an important novel lead which would justify publication at ICLR.   Authors have also not engaged with the reviewers.  For these rejections, this paper cannot be endorsed for publication at ICLR 2022.
The paper attacks an interesting problem: accurately estimating uncertainties in action value estimates in offline RL. It proposes a method based on ensembles of Q functions, where we alternately train an ensemble to estimate Q(s,a) for the current policy, and then adjust our policy based on the mean and uncertainty in this ensemble. By choosing mean + \beta * [standard deviation] as the basis for our policy updates, we can be either conservative (\beta < 0) or optimistic (beta > 0). The paper analyzes the ensemble training using the Gaussian process (NTK) view of deep nets.  The largest weakness of the paper is a lack of rigor in its analysis. While its main topic is uncertainty in Q estimates, the paper does not specify a valid probabilistic model on which such uncertainty estimates could be based. The theorems analyze only a part of the algorithm (policy evaluation), and don t take into account the interplay between this evaluation and any policy updates. The theorems also do not show that the computed output distribution is relevant to the actual uncertainty of the algorithm; e.g., they do not describe a prior for which the ensemble approximates the correct posterior (nor any other similar notion). Despite these omissions, the theorems are nonetheless presented as providing a reason to trust the output of the algorithm.  On the other hand, there definitely is valuable material in the paper; the experiments are interesting (and would be even more interesting if we could compare to some notion of a correct answer for at least the small ones), and the intuition and analysis could be enlightening if presented more clearly and formally, with a better description of the connection between theory and practice. Unfortunately, the paper as written doesn t enable the reader to accurately understand and assess the contributions.
This is a pretty nice paper, but it suffers a bit from being in an  uncanny valley  between application and research.  The approach clearly has been made, and derives from, the application under consideration.  However, the application is not a real application, and rather is a simplified simulation. That s okay, but it means that the application here is not the real goal.  So, our attention should go to the solution technique.  Unfortunately, this seems rather specific, exploiting known structure for the specific problem at hand, and lacking other reasonable baselines one could imagine.  So, this is not really an application paper, as the application in the paper is a proxy.  But this is also not really an algorithm paper, as the algorithm is not clearly shown to be generalisable to other settings.  And this is also not a theory paper that tells us something general and meaningful.  These are just observations   this is not criticism per se.  But it means I struggle a little to find something meaningful to learn from this paper, that could be applied elsewhere.  This, in addition to the overall recommendations by the reviewers, unfortunately lead me to reject the paper in its current form.  I want to thank the authors for engaging with the discussion, and hope they have found it interesting and rewarding, despite the outcome this time around.
While reviewers find this paper interesting, they raised number of concerns including the novelty, writing, experiments, references and clear mention of the benefit. Unfortunately, excellent questions and insightful comments left by reviewers are gone without authors’ answers. 
The authors have proposed an approach for directly learning a spatial exploration policy which is effective in unseen environments. Rather than use external task rewards, the proposed approach uses an internally computed coverage reward derived from on board sensors. The authors use imitation learning to bootstrap the training and then fine tune using the intrinsic coverage reward. Multiple experiments and ablations are given to support and understand the approach. The paper is well written and interesting. The experiments are appropriate, although further evaluations in real world settings really ought to be done to fully explore the significance of the approach. The reviewers were divided, with one reviewer finding fault with the paper in terms of the claims made, the positioning against prior art, and the chosen baselines. The other two reviewers supported publication even after considering the opposition of R1, noting that they believe that the baselines are sufficient, and the contribution is novel. After reviewing the long exchange and discussion, the AC sides with accepting the paper. Although R1 raises some valid concerns, the authors defend themselves convincingly and the arguments do not, in any case, detract substantially from what is a solid submission.
To tackle the problem of classification under input dependent noise, the authors proposed the posterior transition matrix (PTM) to achieve statistically consistent classification. Specifically the information fusion approach was developed to fine tune the noise transition matrix. Experiments demonstrated the effectiveness of the proposed approach.  I would like to thank the authors for the detailed feedback to the initial reviews and also further feedback to the reviewers  additional questions. Many concerns were clarified by the feedback, and the additional experiments still demonstrate the effectiveness of the proposed method.  The issue of data augmentation still remains, which should be at least experimentally investigated, but the contribution of the current manuscript is still valuable to be presented as ICLR2022.
This paper studies to what extent adversarial training affects the properties of adversarial examples in object classification.  Reviewers found the work going in the right direction, but agreed that it needs further evidence/focus in order to constitute a significant contribution to the ICLR community. In particular, the AC encourages authors to relate their work to the growing body of (mostly concurrent) work on robust optimization and adversarial learning. For the above reasons, the AC recommends rejection at this time. 
This work presents a theory for building scale equivariant CNNs with steerable filters. The proposed method is compared with some of the related techniques . SOTA is achieved on MNIST scale dataset and gains on STL 10 is demonstrated. The reviewers had some concern related to the method, clarity, and comparison with related works. The authors have successfully addressed most of these concerns. Overall, the reviewers are positive about this work and appreciate the generality of the presented theory and its good empirical performance. All the reviewers recommend accept.
The paper proposes a general framework to reason about fine grained distribution shifts, evaluating a large set of different approaches in a variety of settings. All reviewers recommend acceptance. While concerns were raised, including questions about the generality of the framework, unsurprising “tips”, and unclear take home messages, all reviewers find the work strong, with an elegant formulation, and useful insights. The AC agrees with the reviewers that this work addresses a very important problem, proposes an interesting unified framework and benchmark for domain shift analysis, and should be a valuable tool for the community to pursue further research in this area.
The is a borderline paper with the reviewers split in their recommendations.  The decision is therefore not easy.  The work is promising, but a key concern is that the contribution appears incremental: the paper proposes to alternate between kickstarting, which is itself not entirely new as an idea, with a simple instance transfer heuristic.  The resulting method is straightforward, which can be considered a strength, but there is no serious technical justification beyond intuitive motivation.  Rather than present technical analysis, the paper focuses more on intuitively delivering the proposal then evaluating it.  This would be acceptable if the empirical outcomes were undeniably impressive, but the outcomes, though positive, are not overwhelming.  The experimental evaluation is limited in scope, considering only the simplest MuJoCo environments and a benchmark racing simulator.  The authors responded to some of the criticisms forcefully, and were comprehensive in their rebuttal, but if the support for the proposed method is to be entirely intuitive and empirical, one would have expected a more comprehensive evaluation where transfer was used to solve more impressively difficult problems.  Overall, I think this work would be better served by adding a technical analysis that validates the significance of the instance transfer heuristic, combined with a broader empirical study that tackles more challenging domains. 
The authors develop a novel technique to train networks to be robust and accurate while still being efficient to train and evaluate. The authors propose "Robust Dynamic Inference Networks" that allows inputs to be adaptively routed to one of several output channels and thereby adjust the inference time used for any given input. They show   The line of investigation initiated by authors is very interesting and should open up a new set of research questions in the adversarial training literature.  The reviewers were in consensus on the quality of the paper and voted in favor of acceptance. One of the reviewers had concerns about the evaluation in the paper, in particular about whether carefully crafted attacks could break the networks studied by the authors. However, the authors performed additional experiments and revised the paper to address this concern to the satisfaction of the reviewer.  Overall, the paper contains interesting contributions and should be accepted.
The paper introduces a learning framework for solving incompressible Navier Stokes fluid using a physics informed loss formulation. The PDE is solved on a grid, and the model, implemented via convolutions and a U Net, is trained to minimize the NS residual. The model is trained on a variety of randomized contexts, in a way that allows training to explore a large number of configurations. The paper presents original contributions compared to previous Physics informed framework (discrete formulation, conditioning on the domain conditions, …). All the reviewers agree that the detailed rebuttal provides answers to their questions and that the contribution is significant, they all have a positive assessment of the paper.
This paper proposes a recurrent architecture based on a recursive gating mechanism. The reviewers leaned towards rejection on the basis of questions regarding novelty, analysis, and the experimental setting. Surprisingly, the authors chose not to engage in discussion, as all reviewers seems pretty open to having their minds changed. If none of the reviewers will champion the paper, and the authors cannot be bothered to champion their own work, I see no reason to recommend acceptance.
This paper suggests stabilizing the training of GANs using ideas from control theory. The reviewers all noted that the approach was well motivated and seemed convinced that that the problem was a worthwhile one. However, there were universal concerns about the comparisons with baselines and performance over previous works on Stabilizing GAN training and the authors were not able to properly address them.
This paper introduces the multiple manifold problem   in a simple setting there are two data manifolds representing the positive and negative samples, and the goal is to train a neural network (or any predictor) that separates these two manifolds. The paper showed that this is possible to do with a deep neural network under certain assumptions   notably on the shape of the manifold and also on the ability of the neural network to represent certain functions (which is harder to verify, and only verified for a 1 d case in the paper). The optimization of neural network falls in the NTK regime but requires new techniques. Overall the question seems very natural and the results are reasonable first steps. There are some concerns about clarity that the authors should address in the paper.
The authors have described a navigation method that uses co grounding between language and vision as well as an explicit self assessment of progress. The method is used for room 2 room navigation and is tested in unseen environments. On the positive side, the approach is well analyzed, with multiple ablations and baseline comparisons. The method is interesting and could be a good starting point for a more ambitious grounded language vision agent. The approach seems to work well and achieves a high score using the metric of successful goal acquisition. On the negative side, the method relies on beam search, which is certainly unrealistic for real world navigation, the evaluation metric is very simple and may be misleading, and the architecture is quite complex, may not scale or survive the test of time, and has little relevance for the greater ML community. There was a long discussion between the authors and the reviewers and other members of the public that resolved many of these points, with the authors being extremely responsive in giving additional results and details, and the reviewers  conclusion is that the paper should be accepted. 
This submission proposes a new paradigm for modelling temporal point processes by using deep learning to learn to mix log normal distributions in order to directly model the conditional distribution of event time intervals themselves.  Strengths of the paper:  Introduces a new modelling paradigm that can lead to further research in this direction, for an important problem.  Extensive experimentation validates the approach quantitatively.  Easy to read.  Weaknesses:  Several reviewers wanted more details on how the mixing parameter K was tuned. This was adequately addressed during the discussion period.  The reviewer consensus was to accept this submission. 
This paper investigates emergence of language from raw pixels in a two agent setting. The paper received divergent reviews, 3,6,9. Two ACs discussed this paper, due to a strong opinion from both positive and negative reviewers. The ACs agree that the score "9" is too high: the notion of compositionality is used in many places in the paper (and even in the title), but never explicitly defined. Furthermore, the zero shot evaluation is somewhat disappointing. If the grammar extracted by the authors in sec. 3.2 did indeed indicate the compositional nature of the emergent communication, the authors should have shown that they could in fact build a message themselves, give it to the listener with an image and ask it to answer. On the other hand, "3" is also too low of a score. In this renaissance of emergent communication protocol with multi agent deep learning systems, one missing piece has been an effort toward seriously analyzing the actual properties of the emergent communication protocol.  This is one of the few papers that have tackled this aspect more carefully. The ACs decided to accept the paper. However, the authors should take the reviews and comments seriously when revising the paper for the camera ready.
I agree with the reviewers that this work is not well presented, and it seriously lacks rigor and experimental support. The writing of this work also needs significant improvement. The authors made many claims without offering rigorous proofs, and hand waved their argument throughout without strong empirical support. In the end, the authors  response did not address the reviewers  concerns satisfactorily and no one is excited enough to defend the current draft. Please consider revising your draft according to the reviewers  comments.
Word vectors are well studied but this paper adds yet another interesting dimension to the field.
The submission evaluates maximum mean discrepancy estimators for post selection inference. It combines two contributions: (i) it proposes an incomplete u statistic estimator for MMD, (ii) it evaluates this and existing estimators in a post selection inference setting.  The method extends the post selection inference approach of (Lee et al. 2016) to the current u statistic approach for MMD.  The top k selection problem is phrased as a linear constraint reducing it to the problem of Lee et al.  The approach is illustrated on toy examples and a GAN application.  The main criticism of the problem is the novelty of the paper.  R1 feels that it is largely just the combination of two known approaches (although it appears that the incomplete estimator is key), while R3 was significantly more impressed.  Both are senior experts in the topic.  On the balance, the reviewers were more positive than negative.  R2 felt that the authors comments helped to address their concerns, while R3 gave detailed arguments in favor of the submission and championed the paper.  The paper provides an additional interesting framework for evaluation of estimators, and considers their application in a broader context of post selection inference.
This paper focuses on how to improve video text retrieval via using additional user comments, and uses an attention mechanism to filter out the irrelevant comments. The main contribution is a context adapter module that allows learning from the auxiliary modality through an attention mechanism. The reviewers appreciated the overall idea s intuition and well written paper, but they also felt that the technical novelty is incremental, and that the treatment of user comments should be more intuitive via the dialogue thread structure. There were also concerns about the applicability of the context adapter module to more realistic scenarios with much longer videos, where the number of comments is very large, and where number of distractor comments is larger than the non distractor ones.
The paper proposes to study the impact of normalizing the gradient for each layer before applying existing techniques such as SG + momentum, Adam or AdaGrad. The study is done on a reasonable number of datasets and, after the reviewers  comments, confidence intervals have been added,  although Table 1 puts results in bold but many of these results are not statistically significant.  The paper, however, lacks a proper analysis of the results. Two main things could be improved:   Normalization does not always have the same effect but the reasons for it are not discussed. This needs not be done theoretically but a more thorough analysis would have been appreciated.   There is no hyperparameter tuning, which means that the results are heavily dependent on which hyperparameters were chosen. Thus, it is hard to draw any conclusion.  Regarding the seemingly conflicting remarks of the two reviewers, it all depends on what the paper is trying to achieve. If it tries to show that is it state of the art, then comparing to state of the art algorithms on every dataset is crucial. If it tries to study the impact of one specific change, in this case layer normalization, on the optimization, then comparing to the vanilla version is fine. The paper seems to try to address the latter so it is OK if it is not compared to all the state of the art algorithms. However, proper tuning of existing methods is still required.  Ultimately, a better understanding of layer normalization could be useful but the paper is not convincing enough to provide that understanding. There is no need to increase the number of datasets but it should rather focus on designing setups to test and validate hypotheses.
This paper deals with segmentation of time series. The paper has received quite detailed reviews and the approach seems to have several interesting aspects (interesting architecture choice, stepwise classification approach, ability of capturing long range dependencies). However, there is a consensus that the paper would definitely benefit from a further iteration before publication in ICLR or in any other similar venue. The authors in their final response have already identified the improvement points raised by the reviewers. In addition to these, I believe it would be helpful to put the contributions better into perspective with existing literature. I think all these this would require a major rewrite and I encourage the authors to make a fresh submission in a future venue.
Strengths of the paper:  Based on previous work suggesting that radial basis features can help defend against adversarial attacks, the paper proposes a concrete method for incorporating them in deep networks.  The paper evaluates the method on multiple datasets, including MNIST and  ISBI International Skin Imaging Collaboration (ISIC) Challenge.  Weaknesses:  Reviewers 2 and 3 felt that the paper was not clearly written, and cited several concrete questions about the method that could not be understood from the paper.  There were additional concerns of lacking comparison to existing methods, and Reviewer 1 pointed out that a competing method gave higher performance, although this was not reported in the present submission.  Points of contention:  The authors did not provide a response to the reviewer concerns.  Consensus:  All reviewers recommended that the paper be rejected, and the authors did not provide a rebuttal.
This work is on stochastic convex optimization (SCO) in shuffle differential privacy (DP) models. In SCO, a learner receives a convex loss function L: Theta x X  > Reals, where Theta is a d dimensional vector of parameters and X is a set of data points. The objective is to use samples x1, x2, …, xn to find a parameter theta that minimizes the loss E_{x ~ D}[L(theta,x)], where the distribution D on X is unknown. The shuffle models considered are a ``sequential" model where the analyzer operates in rounds (and where a new set of users participate in a local DP protocol in every round), and a new, stronger "full" model in which the analyzer can request a specific subset of users to participate in a round, which in particular allows users  data to be queried more than once. This work shows that in the full model, one can develop excess population loss bounds matching the known best possible bounds in centralized DP; it is also shown that even the weaker sequential model offers improved excess population loss bounds over the best possible bound of sqrt(d/n) in the local setting.  The reviewers appreciated the novelty and technical depth of this work (despite concerns about part of the work being taking “off the shelf” results).
This paper extends the information bottleneck method to the unsupervised representation learning under the multi view assumption. The work couples the multi view InfoMax principle with the information bottleneck principle to derive an objective which encourages the representations to contain only the information shared by both views and thus eliminate the effect of independent factors of variations. Recent advances in estimating lower bounds on mutual information are applied to perform approximate optimisation in practice. The authors empirically validate the proposed approach in two standard multi view settings. Overall, the reviewers found the presentation clear, and the paper well written and well motivated. The issues raised by the reviewers were addressed in the rebuttal and we feel that the work is well suited for ICLR. We ask the authors to carefully integrate the detailed comments from the reviewers into the manuscript. Finally, the work should investigate and briefly establish a connection to [1].  [1] Wang et al. "Deep Multi view Information Bottleneck". International Conference on Data Mining 2019 (https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.5)
The authors study the problem of (insufficient) generalization in gossip type decentralized deep learning. Specifically, they establish an upper bound on the square of the consensus parameter distance, which the authors identify as a key quantity that influences both optimization and generalization. This upper bound (called the critical consensus distance) can be monitored and controlled during the training process via (e.g.) learning rate scheduling and tweaking the amount of gossip. A series of empirical results on decentralized image classification and neural machine translation are presented in support of this observation.  Initial reviews were mixed. While all reviewers liked the approach, concerns were raised about the novelty of the results, the lack of theoretical depth, and the mismatch between theory and experiments. Overall, the idea of tracking consensus distance to control generalization seems to be a practically useful concept.  During the discussion phase the authors were been able to (convincingly, in the area chair s view) respond to a subset of the criticisms.   Unfortunately, concerns remained regarding the mismatch between the theoretical and empirical results, and in the end the paper fell just short of making the cut.   The authors are encouraged to carefully consider the reviewers  concerns while preparing a future revision.
This paper addresses the challenge of time complexity in aggregating neighbourhood information in GCNs. As we aggregate information from larger hops (deeper neighbourhoods) the number of nodes can increases exponentially thereby increasing time complexity. To overcome this the authors propose a sampling method which samples nodes layer by layer based on bidirectional diffusion between layers. They demonstrate the effectiveness of their approach on 3 large benchmarks.  While the ideas presented in the paper were interesting the reviewers raised some concerns which I have summarised fellow:  1) Novelty: The reviewers felt that the techniques presented were not very novel and is very similar to one existing work as pointed out by R4 2) Writing: The writing needs to be improved. The authors have already made an attempt towards this but it could be improved further 3) Comparisons with baselines: R4 has raised some concerns  the settings/configurations used for the baseline methods. In particular, the results for the baseline methods are lower than those reported in the original papers. I have read the author s rebuttal for this but I am not completely convinced about it. I would suggest that the authors address this issue in subsequent submissions  Based on the above reasons I recommend that the paper cannot be accepted.    
The paper studies dyna style MBRL in a resource limited setting. It is evaluated on an acrobat task where it shows very promising results.  The reviewers appreciated the extensive replies, but they did not fundamentally change their opinion. In particular:   Lack of formal problem statement and definitions   The experiment on a single task (and that being a non standard version) isn t sufficient to demonstrate the general merits of the method  While the ideas are very promising, the paper cannot be published in its current form. We d hence like to highly encourage the authors to revise the paper and to re submit at a different venue.
The authors consider a parameter server setup where the learner acts a server communicating updated weights to workers and receiving gradient updates from them. A major question then relates in the synchronisation of the gradient updates, for which couple of *fixed* heuristics exists that trade off accuracy of updates (BSP) for speed (ASP) or even combine the two allowing workers to be at most k steps out of sync. Instead, the authors propose to learn a synchronisation policy using RL. The authors perform results on a simulated and real environment. Overall, the RL based method seems to provide some improvement over the fixed protocols, however the margin between the fixed and the RL get smaller in the real clusters. This is actually the main concern raised by the reviewers as well (especially R2)   the paper in its initial submission did not include the real cluster results, rather these were added at the rebuttal. I find this to be an interesting real world application of RL and I think it provides an alternative environment for testing RL algorithms beyond simulated environments.   As such, I’m recommending acceptance. However, I do ask the authors to be upfront with the real cluster results and move them in the main paper. 
Strengths: Paper uses an efficient inference procedure cutting inference time on intermediate frames by 53%, & yields better accuracy and IOU compared to the one recent closely related work.  The ablation study seems sufficient and well designed. The paper presents two feature propagation strategies and three feature fusion methods. The experiments compare these different settings, and show that interpolation BMV is indeed a better feature propagation.  Weaknesses: Reviewers believed the work to be of limited novelty. The algorithm is close to the optical flow based models Shelhamer et al. (2016) and Zhu et al. (2017). Reviewer asserts that the main difference is that the optical flow is replaced with BMV, which is a byproduct of modern cameras.  R3 felt that there was Insufficient experimental comparison with other baselines and that technical details were not clear enough.  Contention: Authors assert that Shelhamer et al. (2016) does not use optical flow, and instead simply copies features from frame to frame (and schedules this copying). Zhu et al. (2017) then proposes an improvement to this scheme, forward feature warping with optical flow. In general, both these techniques fail to achieve speedups beyond small multiples of the baseline (< 3x), without impacting accuracy.  Consensus: It was disappointing that some of the reviewers did not engage after the author review (perhaps initial impressions were just too low). However, after the author rebuttal R1 did respond and held to the position that the work should not be accepted, justified by the assertion that other modern architectures that are lighter weight and are able to produce fast predictions.  
The paper proposes to maximizing the mutual information to optimize the bin for multiclass calibration. The idea, technique, and presentation are good. The paper solves some multiclass calibration  issues. The author should revise the paper according the reviewer s comments before publish.
This work presents a method for generating an (approximately) minimal adversarial perturbation for neural networks. During the discussion period, the AC raised additional concerns that were not originally addressed by the reviewers. The method is an iterative first order method for solving constrained optimization problems, however when considered as a new first order optimization method the contribution seems minimal. Most of the additions are rather straightforward e.g. using a line search at each step to determine the optimal step size and the reported gains over PGD are unconvincing. PGD can be considered as a "universal" first order optimizer [1], as such we should be careful that the reported gains are substantial and not just a question of tuning. Given that using a line search at each step increases the computational cost by a multiplicative factor, the comparison with PGD should take this into account.  The AC notes several plots in the Appendix show PGD having better performance (particularly on restricted Imagenet), and for others there remain questions on how PGD is tuned (for example the CIFAR 10 plots in Figure 5). One of two things explains the discrepancies in Figure 5: either PGD is finding a worse local optimum than FAB, or PGD has not converged to a local optimum. There needs to be provided experiments to rule out the second possibility, as this is evidence that PGD is not being tuned properly. Some standard things to check are the step size and number of steps. Additionally, enforcing a constant step size after projection is an easy way to improve the performance of PGD. For example, if the gradient of the loss is approximately equal to the normal vector of the constraint, then proj(x_i+ lambda * g) ~ x_i will result in an effective step size that is too low to make progress.  Finally, it is unclear what practical use there is for a method that finds an approximately minimum norm perturbation. There are no provable guarantees so this cannot be used for certification. Additionally, in order to properly assess the security and reliability of ML systems, it is necessary to consider larger visual distortions, occlusions, and corruptions (such as the ones in [2]) as these will actually be encountered in practice.   1. https://arxiv.org/pdf/1706.06083.pdf 2. https://arxiv.org/abs/1807.01697
While there are some interesting and novel aspects in this paper, none of the reviewers recommends acceptance.
This work investigates an algorithm to learn representations of Lie groups. It first learns a representation of the Lie algebra by enforcing the Jacobi identity using known structure coefficients. Then obtains the group representation via matrix exponentiation. The paper also proposes a Poincaré equivariant neural network, and applies this model to an object tracking task. The paper is well motivated, the derivations could be more clearly presented but are otherwise sound. The experimental results are promising but rather limited in scope at the time.
The paper proposes a novel approach to detect outliers using Optimal transport. the authors prove a very interesting relation between Outlier robust OT and solving OT with a  thresholded loss. Numerical experiments show that the proposed approach indeed work for outlier detection.   The paper had mixed reviews and the comments and changes from the authors were appreciated. The comments about recent (and contemporary) references were not taken into account in the final decision following ICLR guidelines.   One major concern that appeared during discussion was the fact that one important claimed contribution is the ability to perform outlier detection, the proposed method is never evaluated or compared to the numerous existing outlier detection methods. It works on a toy example and seem to provide a robust way to train a robust GAN but the experiments are very limited. Also the claim from the authors that the method scales are not really true. The proposed approach requires solving an exact OT of complexity O(N^3log(N)), while one can use an approximated entropic solver on the thresholded loss it does not solve the ROBOT problem anymore and the relations between the problem does not exist anymore in this case (or are more similar to UOT).  The concerns detailed above and the limited novelty of the contributions (most of the formulations proposed in the paper are already existing in the literature) suggest that the paper in its current iteration  is too borderline for being accepted in a selective venue such as ICLR. The method and the relations uncovered are interesting and the AC encourages the authors to continue work on the proposed method and provide more detailed experiments illustrating and comparing the method to baselines for outlier detection. 
This paper proposes a method for hierarchical reinforcement learning that aims to maximize mutual information between options and state action pairs. The approach and empirical analysis is interesting. The initial submission had many issues with clarity. However, the new revisions of the paper have significantly improved the clarity, better describing the idea and improving the terminology. The main remaining weakness is the scope of the experimental results. However, the reviewers agree that the paper exceeds the bar for publication at ICLR with the existing experiments.
Thank you for your submission to ICLR.  This paper presents a technique for image synthesis based on stochastic differential equations and a diffusion model.  This looks to be a very nice idea with good results.  After discussion, the reviewers converged and all agreed that the paper is ready for publication the most negative reviewer raised their score after the author rebuttal, from a weak reject to weak accept.  The rebuttal clearly and concisely addressed several concerns of the reviewers.  I m happy to recommend accepting the paper.
This paper tests out some straightforward data augmentation strategies on the protein inputs to the transformer used in the TAPE paper.  Overall, there is insufficient intellectual merit to warrant publication at ICLR. As a side note, the quality of the manuscript in terms of scholarliness of presentation was overall lacking.
The work presents a method to back propagate and visualize bias distribution in network as a form of explainability of network decisions. Reviewers unanimous reject, no rebuttal from authors. 
The reviewers have raised relevant concerns that preclude acceptance and the authors have not provided a response. At this time, all reviewers concur that this paper should be rejected and I agree.
This paper proposes to use Graph Convolutional Networks (GCNs) in Bayesian optimization for neural architecture search. While the paper title includes multi objective, this component appears to only be a posthoc evaluation of the Pareto front of networks evaluated using a single objective search   this could be performed for any method that evaluates more than one network. Performance on NAS Bench 101 appears to be very good.   In the private discussion of reviewers and AC, several issues were raised, including whether the approach is compared fairly to LaNAS and whether the GCN will predict well for large search spaces. Also, unfortunately, no code is provided, making it unclear whether the work is reproducible. The reviewers unanimously agreed on a weak rejection score.  I concur with this assessment and therefore recommend rejection. 
This paper proposes an expansion based approach for task free continual learning, using a Bayesian nonparametric framework (a Dirichlet process mixture model).  It was well reviewed, with reviewers agreeing that the paper is well written, the experiments are thorough, and the results are impressive. Another positive is that the code has been released, meaning it’s likely to be reproducible.  The main concern shared among reviewers is the limited novelty of the approach, which I also share. Reviewers all mentioned that the approach itself isn’t novel, but they like the contribution of applying it to task free continual learning. This wasn’t mentioned, but I’m concerned about the overlap between this approach and CURL (Rao et al 2019) published in NeurIPS 2019, which also deals with task free continual learning using a generative, nonparametric approach. Could the authors comment on this in their final version?  In sum, it seems that this paper is well done, with reproducible experiments and impressive results, but limited novelty. Given that reviewers are all satisfied with this, I’m willing to recommend acceptance.   
The paper addresses generalized zero shot learning (test data contains examples from both seen as well as unseen classes) and proposes to learn a shared representation of images and attributes via multimodal variational autoencoders.  The reviewers and AC note the following potential weaknesses: (1) low technical contribution, i.e. the proposed multimodal VAE model is very similar to Vedantam et al (2017) as noted by R2, and to JMVAE model by Suzuki et al, 2016, as noted by R1. The authors clarified in their response that indeed VAE in Vedantam et al (2017) is similar, but it has been used for image synthesis and not classification/GZSL. (2) Empirical evaluations and setup are not convincing (R2) and not clear   R3 has provided a very detailed review and a follow up discussion raising several important concerns such as (i) absence of a validation set to test generalization, (ii) the hyperparameters set up; (iii) not clear advantages of learning a joint model as opposed to unidirectional mappings (R1 also supports this claim). The authors partially addressed some of these concerns in their response, however more in depth analysis and major revision is required to assess the benefits and feasibility of the proposed approach. 
This paper proposed a method to train quantized supernets which can be directly deployed without retraining. A main concern is that there is limited novelty. The proposed method looks like a combination of well known techniques. Experimental results are promising. However, it is not clear if the comparisons are fair and if all the methods are using the same setup. It is desirable to have additional analysis and ablation studies. The writing can also be improved.
The reviewers and AC note the following potential weaknesses: 1) the proof techniques largley follow from previous work on linear models 2) it’s not clear how signficant it is to analyze a one neuron ReLU model for linearly separable data. 
I thank the authors for their submission and active participation in the discussions. This papers is borderline. On the positive side, reviewers emphasized this is a well written [ovqB,1zPe] and sound paper [BUDa] with good theoretical [td5N,ovqB,1zPe] and empirical [BUDa,td5N,ovqB] results. On the negative side, reviewers remarked clarity [KyZj,AVki], incremental with respect to Tasse et al (2020) [KyZj], relatively restricted Boolean task algebra [td5N], toyish nature of the environments considered [ovqB], and some missing details [1zPe]. During discussion, the sentiment seems to be somewhat lukewarm with none of the reviewers strongly favoring acceptance or rejection. It seems the main remaining concern is around the toyish nature of the environments used in this paper. I acknowledge that and I believe the authors could include experiments on more complex environments. However, I also give the authors credit for addressing most of the reviewer s concerns during rebuttal and for presenting a solid empirical and theoretical result that the research community can build upon in the future. I am therefore recommending acceptance of this paper and highly encourage the authors to further improve their paper based on the reviewer feedback.
This paper was referred to the ICLR 2021 Ethics Review Committee based on concerns about a potential violation of the ICLR 2021 Code of Ethics (https://iclr.cc/public/CodeOfEthics) raised by reviewers. The paper was carefully reviewed by two committee members, who provided a binding decision. The decision is "Significant concerns (Do not publish)". Details are provided in the Ethics Meta Review. As a result, the paper is rejected based Ethics Review Committee s decision .  The technical review and meta reviewing process moved proceeded independently of the ethics review. The result is as follows:  This paper considers sparse (L0) attacks against binary images analysis systems, in particular OCR.  The major concern of the reviewers seems to be similarity to other methods in the literature, but reviewers did not specify any specific methods to compare to.  Because it was not possible for reviewers to address such vague concerns, and because I believe the authors did a good job differentiating their work in the rebuttal, I think the paper is of good merit.  
This paper proposes a scalable approach for graph learning from data. The reviewers think the approach appears heuristic and it is not clear the algorithm is optimizing the proposed sparse graph recovery objective. 
This one was really on the fence.  After some additional rounds of discussion post rebuttal with the reviewers I think the general consensus is that it s a good paper and almost there but not quite ready for acceptance at this time.  A detailed list of issues and concerns below.  PROS: 1. good idea: an additional loss term that enforces semantic constraints on the network output (like exactly 1 output element must be 1). 2. well written generally 3. a nice variety of different experiments  CONS: 1. paper organization.  The authors start with the axioms they would like a semantic loss function to obey, then provide a general definition, then show it does obey the axioms.  The general definition is intractable in a naive implementation.  The authors use boolean circuits to tractably solve the problem but this isn t discussed enough and it s unreasonable to expect readers to just give a pass on it without some more background.  I personally would prefer an organization that presented the motivation (in english) for the loss definition; then the  definition with a description of its pieces and why they are there; then a short discussion of how to implement such a loss in practice using boolean circuits (or if this is too much put it in the appendix); and a pointer to the axiomatization in an appendix.  2. related to 1, I didn t see anything which discussed the training time of this approach.  Given that the semantic loss has to be computed in a more involved way than usual, it s not clear whether it is practical.
Reviewers found the problem statement having merit, but found the solution not completely justifiable. Bandit algorithms often come with theoretical justification because the feedback is such that the algorithm could be performing horribly without giving any indication of performance loss. With neural networks this is obviously challenging given the lack of supervised learning guarantees, but reviewers remain skeptical and prefer not to speculate based on empirical results. 
This paper presents an interesting strategy of curriculum learning for training neural networks, where mini batches of samples are formed with a gradually increasing level of difficulty.   While reviewers acknowledge the importance of studying the curriculum learning and the potential usefulness of the proposed approach for training neural networks, they raised several important concerns that place this paper bellow the acceptance bar: (1) empirical results are not convincing (R2, R3); comparisons on other datasets (large scale) and with state of the art methods would substantially strengthen the evaluation (R3); see also R2’s concerns regarding the comprehensive study; (2) important references and baseline methods are missing – see R2’s suggestions how to improve; (3) limited technical novelty   R1 has provided a very detailed review questioning novelty of the proposed approach w.r.t. Weinshall et al, 2018.   Another suggestions to further strengthen and extend the manuscript is to consider curriculum and anti curriculum learning for increasing performance (R1).  The authors provided additional experiment on a subset of 7 classes from the  ImageNet dataset, but this does not show the advantage of the proposed model in a large scale learning setting.  The AC decided that addressing (1) (3) is indeed important for understanding the contribution in this work, and it is difficult to assess the scope of the contribution without addressing them.  
While there was some support for this paper, there was not enough support to accept it for publication at ICLR.  The following concern is characteristic of the concerns raised by the reviewers: "The "main contribution of this paper is hard to discern, but the ideas presented are interesting." Other reviewers said it was "hard to read" and not ready for publication. 
This paper presents an approach to use spatio temporal self similarity (STSS) as a feature for a convolutional neural network for video understanding. The proposed approach extracts STSS as a descriptor capturing similarities between local spatio temporal regions, and adds conventional layers such as soft argmax, fully connected layers, and conv. layers on top of it.  On one hand, all of the reviewers agree that the novelty of the paper is limited. On the other hand, most of the reviewers (except R1) appreciated thoroughness of the experiments and ablations. In the end, the reviewers gave 3 marginally above the acceptance threshold ratings and 1 marginally below the threshold rating.  The AC views this paper as a borderline paper. None of the reviewers are excited about the paper, and it is a typical "Nice experiments with limited novelty" (by R1) paper. The concept of the STSS itself was already proposed in prior studies as mentioned in the paper and by the reviewers, and this paper  engineers  a new way to take advantage of STSS without further theoratical or conceptual justifications on why it should work. The newly added Kinetics and HMDB results in the rebuttal are nice, but the impact of STSS seems to be minimal in these results.  Overall, the AC find the paper slighly lacking to be considered for ICLR.
This was a difficult paper to decide, given the strong disagreement between reviewer assessments.  After the discussion it became clear that the paper tackles some well studied issues while neglecting to cite some relevant works.  The significance and novelty of the contribution was directly challenged, yet I could not see a convincing case presented to mitigate these criticisms.  The paper needs to do a better job of placing the work in the context of the existing literature, and establishing the significance and novelty of its main contributions.
Although the reviewers appreciated the novelty of this work, they unanimously recommended rejection.  The current version of the paper exhibits weak presentation quality and lacks sufficient technical depth.  The experimental evaluation was not found to be sufficiently convincing by any of the reviewers.  The submitted comments should help the authors improve their paper.
This paper attempts to improve adversarial imitation learning (GAIL) by encouraging the discriminator to focus on task dependent features.  An advantage of this paper is that it not only improves upon GAIL, but it is doing so after first demonstrating and analyzing an existing issue.   On the other hand, the presentation of the paper and breadth of experiments could be significantly improved further than the updated version. It would also be necessary to clarify whether the baseline is vanilla PG or D4PG.  A major point for discussion was the selection of the invariance set. The ablation studies and explanation provided during the rebuttal period towards this point are helpful, but somehow we still do not have the full picture to understand well how this method compares to existing literature. 
This paper studies the information theoretic complexity for emergent languages when pairs of neural networks are trained to solve a two player communication game. One of the primary claims of the paper was that under common training protocols, networks were biased towards low entropy solutions. During the discussion period, one reviewer shared an ipython notebook investigating the experiments shown in Figure 1. There it was discovered that low entropy solutions were only obtained for networks which were themselves initialized at low entropy configurations. When networks are initialized at high entropy configurations, the converged solution would remain high entropy. This experiment raises questions about the validity of the claim that there was "pressure" towards low entropy solutions to the task. Therefore, a more careful analysis of the phenomenon is required. 
The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. The resulting idea is simple but also surprising that it works so well. All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. In addition, they identified the following strengths of the paper:   The experiments are exhaustive, identifying many domains where the approach can be applied   The presented results are compelling   The paper is well written   The paper introduces a new problem setup that has not been studied before  I agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.  
This paper proposes a method to resolve "language drift," where a pre trained X >language model trained in an X >language >Y pipeline drifts away from being natural language. In particular, it proposes to add an auxiliary training objective that performs grounding with multimodal input to fix this problem. Results are good on a task where translation is done between two languages.  The main concern that was raised with this paper by most of the reviewers is the validity of the proposed task itself. Even after extensive discussion with the authors, it is not clear that there is a very convincing scenario where we both have a pre trained X >language, care about the intermediate results, and have some sort of grounded input to fix this drift. While I do understand the MT task is supposed to be a testbed for the true objective, it feel it is necessary to additionally have one convincing use case where this is a real problem and not just the artificially contrived. This use case could either be of practical use (e.g. potentially useful in an application), or of interest from the point of view of cognitive plausibility (e.g. similar to how children actually learn, and inspired by cognitive science literature).  A concern that offshoots from this is that because the underlying idea is compelling (some sort of grounding to inform language learning), a paper at a high profile conference such as ICLR may help re popularize this line of research, which has been a niche for a while. Normally I would say this is definitely a good thing; I think considering grounding in language learning is definitely an important research direction, and have been a fan of this line of work since reading Roy s seminal work on it from 15 years ago. However, if the task used in this paper, which is of questionable value and reality, becomes the benchmark for this line of work I think this might lead other follow up work in the wrong direction.  I feel that this is a critical issue, and the paper will be much stronger after a more realistic task setting is added.  Thus, I am not recommending acceptance at this time, but would definitely like the authors to think hard and carefully about a good and realistic benchmark for the task, and follow up with a revised version of the paper in the future.
All reviewers agreed that the contribution is too limited for the paper to be published. I encourage the authors to take the reviews into account when improving their work.
This paper aims to make Stackelberg Deep Deterministic Policy Gradients practical and efficient. The main contributions are an analysis which suggests terms involving the Hessian can be dropped and a block diagonal approximation to an expensive matrix inversion.  Several reviewers who voted for rejection expressed concerns about the soundness of the theoretical arguments. The response provided by the authors did help alleviate some of the reviewers’ concerns but still left significant doubts. While some of the remaining concerns could be due to a misunderstanding of the deterministic setting it is up to the authors to convince the reviewers that their arguments are sound. Given the current scores and the low confidence of the reviewer voting for acceptance, I recommend rejecting the paper in its current form.
The paper is motivated by the observed similarity between learned filters at the low layers of a convolutional neural network and oriented Gabor filters. It proposes to replace the lower layers with dual tree wavelet packet transforms, which yield fixed oriented frequency selective features. Instead of learning filters from scratch, it proposes to learn only a scalar importance for each of these features, reducing the number of learned parameters. Experiments with the AlexNet architecture on ImageNet indicate that this modification does not reduce performance, but does significantly reduce the number of parameters. The paper argues that this modification also improves the interpretability (and in the case of complex dual tree wavelets, potentially the invariance properties) of the low level features.   Pros and cons:  [+/ ] As the paper clearly argues, replacing learned filters with wavelet packet transforms improves the interpretability of the low layers of a convolutional network. While other works have pursued similar ideas, limiting the conceptual novelty, at a technical level this is the first work to use the dual tree complex wavelet transform for this purpose. The DT CWT may have mathematical advantages. The paper and rebuttal argue that it it is conceptually cleaner (“sparser”, since the transform is generated by a single filter) although there may not be a greater reduction in the number of trainable parameters.  [+] The additional per channel weights are redundant in terms of the representation capacity of the network, but may effectively introduce sparse regularization (see work on the “Hadamard parameterization” in implicit sparse regularization), allowing the network to select relevant wavelet features.  [+] The paper is well organized and cleanly written. The authors revision has done a good job of addressing all clarity concerns of reviewers.   [ ] Several reviewers raised concerns about the limited scope of the experiments: the paper only replaces a single layer of a particular architecture (AlexNet) and evaluates on one particular dataset (ImageNet).   [ ] The main proposed benefit of this modification is in the interpretability of the network and its potential amenability to mathematical analysis. This claim would be stronger if the paper either 1. showed the benefit by exhibiting some rudimentary mathematical theory for this network or 2. used this idea to demonstrate networks that are significantly more interpretable, say by replacing all learned convolutional layers with DT CWPT.   The paper’s reviews were split. All reviewers appreciate the paper’s clean exposition of a reasonable idea, and note the novelty of using dual tree wavelets in this context. However, reviewers express concerns about the paper’s significance: it could do more to show how replacing the lowest layer with DT CWPT yields new insights, and do more to demonstrate (both rhetorically and experimentally) the generality of its ideas. Based on the bulk of the reviews, as written the paper falls slightly below the threshold for acceptance. 
The reviewers recommended a rejection. The authors of the paper did not respond.
The manuscript proposes a framework for imposing priors on the feature extraction in deep visual processing models. The core contribution of this manuscript is the systematic formulation and investigation of how different, distinct feature priors leads to complementary feature representations that can be combined to provide more robust data representations. The manuscript uses early work on co training and also the more recent work on self supervision and self training. Experiments are performed with classical shape  and texture biased models, and show that diverse feature priors are able to robustly create a set of complementary data views.  Positive aspects of the manuscript includes: 1. The topic of this paper, creating and combining robust, generalizable and diverse feature representations, is of high relevance; 2. Positive results from co training of groups image classification models designed to focus on shape but not texture or vice versa.  There are also several major concerns, including: 1. The ensemble results presented in section 3.2 are generated using very primitive ensembling techniques; 2. The absence of spurious correlation in unlabelled data assumption be presented more cautiously; 3. Definition of feature prior; 4. Analysis on another domain aside from image classification.  During the rebuttal period, the Authors provided additional experiments using a more sophisticated method (“stacking”), and additional discussion of where spurious correlations are likely to occur. The manuscript has high rating variance. Some reviewers think that the manuscript lacks the technical novelty, and the results presented are the results of an empirical study. The focus of this manuscript is on two natural feature priors (i.e., shape and texture). It would strengthen the manuscript if the Authors can provide further analysis to emphasise the generality of the proposed framework that it could accommodate any two feature priors as long as they are sufficiently diverse.
This paper proposes an input dependent baseline function to reduce variance in policy gradient estimation without adding bias. The approach is novel and theoretically validated, and the experimental results are convincing. The authors addressed nearly all of the reviewer s concerns. I recommend acceptance.
This paper studies the reasons for failure of trained neural network models on out of distribution tasks. While the reviewers liked the theoretical aspects of the paper, one important concern is about the applicability of these insights to real datasets. The authors added an appendix to the paper showing results on a real dataset that mitigates this concern to an extent. Further, there are interesting insights in the paper to merit acceptance.
This paper proposes a method for interpreting structured output model. All the reviews are negative. The reviewers find the paper difficult to read, and lacking in novelty, technical contribution and empirical evaluation.
Overall the reviewers like the ideas in this paper.  It calls out some of the issues with the current line of thinking in the ML/AI community.  There were some concerns, but overall this paper offers a new way to think about, present, and question efficiency results.  This could be quite infulential.  I think this is interesting enough to warrent publicaiton.
The paper introduces a procedure to control the churn (i.e. differences in the predictive model due o retraining) using distillation.  This is a strong paper, with novel technique which is clearly presented, and is backed by sound theory. The experimental results were also deemed extremely convincing by reviewers TJ4g and pZBb.   Reviewer nqfu raised a question about the similarities between churn reduction and domain adaptation. The authors have addressed this by pointing out similarities to their work but also noting that, in the settings mentioned by the reviewer, alternative approaches such as completely retraining the model might be more appropriate. This part of the rebuttal is convincing.  Reviewer TJ4g has pointed out several points of improvement, to which the authors have responded adequately.  All in all, this paper is ready for and deserving of acceptance.
This paper proposes a multi task version of Gradient Boosted Machines (GBMs). The paper proposes a learning algorithm that adaptively adjusts the learning rate per task. Empirical evaluation is carried out on two datasets with the method implemented in the LightGBM framework.  The reviewers thought that the paper is not very clear. They were not ready to accept the paper claims based on the current version. In particular, the algorithms are hard to follow, the empirical evaluation is not easy to follow and there are missing comparisons to related work. The authors did not offer a response to the reviews.
This paper introduces a convolution where the kernel is parametrised continuously over time (in the context of recurrent networks) to address vanishing gradients issues, by using another neural network to generate the kernels. This is a meaningful idea, addressing an important problem. The paper is well written and clear. The idea is novel (parametrised kernel already exist, but the way it s used here is new). The experimental section is solid, although some reviewers suggests it could be extended with more baselines. All reviewers recommend to accept the paper, therefore I also recommend accept.
The paper shows convergence results for RMSprop in certain regimes. The reviews are uniformly positive about this paper and I recommend acceptance.
A conceptually and technically highly innovative paper which reinforces an existing powerful connection between the critical set of two layer ReLU networks and suitable convex programs with cone constraints. The reviewers are in strong consensus that the paper is sound and has merits for publication.
This paper develops a novel continual meta reinforcement learning algorithm that focuses on learning sequential tasks without revisiting previous tasks. The setting is compelling, and the method is well developed with good empirical results. The initial version of the paper included a variety of issues, especially lack of clarity in some aspects and the contributions, that were remedied through discussion with the reviewers and subsequent revisions. The discussion among the reviewers seems to have settled on leaning toward a weak accept overall, with one low score that should be dismissed claiming lack of novelty (which isn t correct   the paper certainly is sufficiently novel).  There do remain some concerns by two reviewers that although "the paper has enough meat to be accepted, ... [it] needs more careful and well thought out ablations and analysis to be truly valuable." Although the authors have revised the paper to address this issue of a precise analysis, adding material into the appendices with some changes to the main text, they are encouraged to make certain that these aspects are integrated and clear throughout.
None of the reviewers recommended this paper. There were concerns that it is hard to draw meaningful conclusions from the experimental work due to the comparisons provided.  While the design of the block masking + contrastive learning proposed in this paper was rated as potentially being quite important, there remained some concern that subsequent tokenization steps could be problematic for "spatial heavy" datasets.  The AC recommends rejection.
This paper advances the idea that recent “influence estimation” methods for supervised learning cannot be trivially applied to GANs. Based on Hara et al.’s method, the authors propose a novel influence estimation for GANs, and an evaluation scheme based on popular GAN evaluation methods, exploiting the fact that they are differentiable with respect to their input data. The paper demonstrates empirically that the proposed influence estimation method correlates to true influence. It also shows that removing “harmful” instances using the average log likelihood, Inception Score, and Frechet Inception Distance versions of the proposed metric improves the quality of generated examples.  All reviewers were positive about the paper. R2 pointed out that it was well written and appreciated the detailed analysis. They thought it thoroughly explained the similarities between it and the most closely related recent work (Hara et al. and Koh & Liang). Concerns expressed by the reviewer were: the amount of samples needed to be removed to obtain a statistically significant result, lack of qualitative results, and an outdated baseline for anomaly detection. The reviewer also stated that they had some concerns with practical applicability and would like to see more GAN metrics, like Precision & Recall. The authors added qualitative results to the paper which partially satisfied the reviewer.  R1 also thought that the paper was well written and contributed to the interpretability of GAN training. Like R2, they pointed out the lack of visual examples (addressed in rebuttal), and asked for more insight into what kind of characteristics make a data point influential. They also requested that the authors add a metric that trades fidelity and diversity like P&R. The reviewer originally felt that the paper was below the bar, because it was “like a story without a satisfying conclusion”. However, the authors responded with additional analysis which satisfied the reviewer, and they upgraded their score by two points.  R3 also found the paper well written and interesting, like the other reviewers. The reviewer raised some similar concerns as the other reviewers (e.g. qualitative results), as well as the scalability of the method to relevant architectures, which I thought was surprising that the other reviewers didn’t mention. The authors responded that they believe their method succeeded in improving diversity of the generated samples but not their visual quality. This is an important point. The additions in Appendix D have addressed the main concerns of R1 and R2, as well as R3’s concern about lack of visual analysis. R1 seems quite convinced now, and R2, though not changing their score, was already in favour of acceptance. It is an interesting finding that “harmful” instances seem to come from regions of distributional mismatch.  I would like to see a fidelity diversity tradeoff like P&R added to a paper, and a discussion of this work in relation to DeVries et. al “Instance Selection” that appears to be similarly motivated though executed differently. I think one major thing holding back this paper is the scale of the experimental analysis (Gaussians & MNIST); I hope the authors can scale the method in future work.
This paper explores a very challenging problem of biased label selection and its effect on graph neural networks. It highlights that GNNs are indeed vulnerable to this issue, and then proposes a regularizer to reduce the learning of spurious correlations from the node embeddings. All of the reviews agree that the problem is relevant and important, but that there are still some outstanding issues.  It’s unclear the degree to which this problem occurs in the real world. It is also important to establish the effectiveness of the method across a range of datasets. The four datasets presented in the paper (and the rebuttal) are a good start, but the reviewers feel that more is still needed to present a convincing argument.  On the theory side, the reviewers are concerned about the linearity assumptions in the theory, and how this will translate into the more realistic nonlinear setting. Even though the authors state that they do not rely on a causal model, the paper and their responses really do seem to point in that direction. This could simply be a clarity issue, in which case I would encourage the authors to revisit this framing this to avoid confusion.  Overall, the paper is promising, but the reviewers feel that more work is needed to provide a comprehensive and convincing case. 
This paper extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. All reviewers agree that this approach is interesting (verification and validation) and experiments are solid. One of the reviewers raised concerns are promptly answered by authors, raising the average score to accept.  
The paper builds upon parametric distributionally robust optimization (PDRO) and proposes ratio PDRO (R PDRO) where the ratio of the worst case distribution and training distribution is parameterized by a discriminative network. This has a benefit over PDRO which needs to do generative modeling of worst case distribution. The paper empirically demonstrates R PDRO improves over existing methods on group robustness problems. Reviewer are overall positive about the paper, and have appreciated the significance of the problem, writing clarity, and thorough empirical evaluation. There were some minor questions which have been adequately addressed by the authors.
The authors propose a simple method to estimate the accuracy of a classifier on an unlabeled dataset given an in distribution validation set. In extensive experiments the authors show that the proposed method is significantly more accurate than previous methods and other baselines.   The reviewers are quite consistent in their judgement, just the weighting of the different aspects is different. After the rebuttal four out of five reviewers recommend acceptance.  Strong points:   simplicity of the method   strong experimental results for various tasks and domain shift problems  Weak points:   there is no clear theoretical statement when the method is supposed to work   the discussion in Section 3.1 is pretty obvious and seems a bit like a waste of space whereas the motivation for the actual method is very short  While I agree with the reviewers that there is little theoretical justification for the method, the strong experimental results on various datasets, tasks and different domain shifts make this paper interesting for a large audience. Thus this paper is a nice contribution to ICLR and I recommend acceptance. However, I strongly recommend to the authors to add more motivation in Section 4 and add a limitation section where the cases are discussed where the method is definitely not working. Section 3 is pretty obvious and could be significantly shortened or integrated into the limitations section.   One case which is highly relevant for this limitations section is the provable asymptotic overconfidence of neural networks which is discussed in  Hein et al, Why ReLU networks yield high confidence predictions far away from the training data and how to mitigate the problem, CVPR 2019  This would definitely lead to a failure of the presented method as all predictions would get a score above the threshold. I would also assume that the method would predict high accuracy values for out of distribution tasks which are semantically similar e.g. training on CIFAR10 and then using CIFAR100 as unlabeled dataset. In that context it would be interesting to evaluate OOD aware classifiers using ATC such as discussed in  Hendrycks et al, Deep Anomaly Detection with Outlier Exposure, ICLR 2019  Also it would be helpful to understand better the influence of the classifier performance on the original task on the performance of ATC on unlabeled data.
This paper proposes a new type of Polynomial NN called Ladder Polynomial NN (LPNN) which is easy to train with general optimization algorithms and can be combined with techniques like batch normalization and dropout.  Experiments show it works better than FMs with simple classification and regression tasks, but no experiments are done in more complex tasks. All reviewers agree the paper addresses an interesting question and makes some progress but the contribution is limited and there are still many ways to improve.
An interesting model, for an interesting problem but perhaps of limited applicability   doesn t achieve state of the art results on practical tasks. Paper has other limitations, though the authors have addressed some in rebuttals.
This paper is full of ideas. However, a logical argument is only as strong as its weakest link, and I believe the current paper has some weak links. For example, the attempt to tie the behavior of SGD to free energy minimization relies on unrealistic approximations. Second, the bounds based on limiting flat priors become trivial. The authors in depth response to my own review was much appreciated, especially given its last minute appearance. Unfortunately, I was not convinced by the arguments. In part, the authors argue that the logical argument they are making is not sensitive to certain issues that I raised, but this only highlights for me that the argument being made is not very precise.  I can imagine a version of this work with sharper claims, built on clearly stated assumptions/conjectures about SGD s dynamics, RATHER THAN being framed as the consequences of clearly inaccurate approximations. The behavior of diffusions can be presented as evidence that the assumptions/conjectures (that cannot be proven at the moment, but which are needed to complete the logical argument) are reasonable. However, I am also not convinced that it is trivial to do this, and so the community must have a chance to review a major revision.
This paper presents a new benchmark task for models similar to CLIP for evaluating how visual word forms interfere with the visual recognition of objects in images when the former are superimposed on the latter ones. Specifically, by superimposing words belonging to different categories  (e.g., hypernyms vs basic labels) the authors study the misclassification rates of CLIP under different degrees of varying similarity between the original and superimposes labels.   All reviewers agreed that this is a novel and interesting study which, by productively using insights from cognitive science literature on language biases, aims at shedding light on the inner workings of a popular artificial model. The main concern raised by reviewer P83Y was regarding the claims around misclassification rates. Indeed, since CLIP was not taught (e.g., by fune tuning or few shot prompting) which of the two labels (i.e., the written or the visual) is the correct one, it s not fair to assess its performance on this way. While this is strictly true, the experimental protocols presented in Sections 4.3/4/5 are still a valid way to assess representational inference. Moreover, the authors have followed P83Y suggestions and incorporated a few shot prompting experiment in Section 4.6.  All in all, I think this will make for an addition to the ICLR program and thus I m recommending accepting this paper.  (Minor comment: WKSS rightly pointing that this paper has, at best, a loose connection to compositionality. The authors changed compositionality  > representations which is a better fit, so please make sure to change the title also in Openreview when prompted.)
The paper presents a new method for detection of backdoor attacks under strong limitations such as the lack of access to training data and the reference benign model. Its main idea is to utilize a new expected transferability statistic that can be used for detection in broad range of application domains. The effectiveness of the proposed approach is demonstrated experimentally.
The paper discusses an approach for privacy preservation in the context of multi task classification. All reviewers struggled to follow the paper and had fundamental questions about the motivation, methods and technical contributions. Unfortunately there was no feedback from the authors to help support the submission.
This paper proposes using a tensor train low rank decomposition for compressing neural network parameters.  However the paper falls short on multiple fronts 1)lack of comparison with existing methods 2) no baseline experiments. Further there are concerns about correctness of the math in deriving the algorithms, convergence and computational complexity of the proposed method.  I strongly suggest taking the reviews into account before submitting the paper it again. 
Strengths: * Well written paper * Strong empirical results on three benchmarks * Interesting approach of producing semantically augmented LMs using dependency parses to extract svo triples, and finding coreferences between them across multiple sentences  Weaknesses: * None of the reviewers seem particularly excited about the paper * Stronger baseline comparisons would have improved the paper * Authors re define a lot of terminology, but the novelty of the method is more from the type of graph used to initialize their method, which seems to be a function of OpenIE triplets
Paper proposes a meta learning approach to interactive segmentation. After the author response, R2 and R3 recommend rejecting this paper citing concerns of limited novelty and insufficient experimental evaluation (given the popularity of this topic in computer vision). R1 does not seem be familiar with the extensive literature on interactive segmentation and their positive recommendation has been discounted. The AC finds no basis for accepting this paper.  
The authors propose a method for training agents in FPS games, and achieve good results in a VizDoom setting. The method combines a number of different components and ideas, and it is not clear which of these are crucial to the success. In particular, ablations of the method are missing, as well as more runs to test variability and diversity. In addition, the paper is not all that easy to read. Reviewers had a number of partly overlapping concerns, of which I ve tried to distil the main ones above. While the empirical results are promising, it is clear that much more work is needed to distil this method into generalizable knowledge.
The paper introduces an interesting family of two player zero sum games with tunable complexity, called Erdos Selfridge Spencer games, as a new domain for RL.  The authors report on extensive empirical results using a wide variety of training methods, including supervised learning and several flavors of RL (PPO, A2C, DQN) as well as single agent vs. multi agent training.  The reviewers also appear to agree that the method appears to be technically correct, clearly written, and easy to read.  A drawback of the paper is that it does not make a *significant* contribution to the field.  In combing through the reviewer comments, none of them identify a significant contribution.  Even in the text of the paper, the authors do not anywhere claim to have made a significant contribution. As the paper is still interesting, the committee would like to recommend this for the workshop track.  Pros:         Interesting domain with tunable complexity         High quality extensive empirical results         Writing is clear  Cons:         Lacks a significant contribution         Appears to overlook self play, the dominant RL training paradigm for decades (multiagent training appears to be related but different)         Per Reviewer3, "I remain unconvinced that these games are good general tests for Deep RL"
Although all reviewers had many positive comments on the paper, and the authors engaged nicely in the discussion period, at the moment there is a consensus among the reviewers that the central claims of the paper (related to minimal representations / information bottleneck) are not adequately supported by the current experiments. In particular, there were concerns that performance gains could be due to diversity of predictors, rather than minimal representations, which would need to be addressed. It s suggested that the reviewers take all of these comments and discussion into account when preparing a revised version of the paper.
Two knowledgeable reviewers were positive 7 and very positive 10 about this paper, considering it an important contribution that illuminates previously unknown aspects of two classic models, namely RBMs and Hopfield networks. They considered the work very well developed, theoretically interesting and also of potential practical relevance. A third reviewer initially expressed some reservations in regard to the inverse map from RBMs to HNs and the experiments. Following the authors  responses, which the reviewer found detailed and informative, he/she significantly raised his/her score to 7, also emphasizing that he/she hoped to see the paper accepted. With the unanimously positive feedback, I am recommending the paper to be accepted. 
This paper presents a method for extracting "knowledge consistency" between neural networks and  understanding their representations.   Reviewers and AC are positive on the paper, in terms of insightful findings and practical usages, and also gave constructive suggestions to improve the paper. In particular, I think the paper can gain much attention for ICLR audience.    Hence, I recommend acceptance.
The paper proposes to regularize via a family of structured sparsity norms on the weights of a deep network.  A proximal algorithm is employed for optimization, and results are shown on synthetic data, MNIST, and CIFAR10.  Pros: the regularization scheme is reasonably general, the optimization is principled, the presentation is reasonable, and all three reviewers recommend acceptance.  Cons: the regularization is conceptually not terribly different from other kinds of regularization proposed in the literature.  The experiments are limited to quite simple data sets.
This paper looks at stochastic and Markov potential games. Its different results, including the sample complexity ones, are overall interesting and relevant for the community.  This said, we had an intense discussion as several of the aforementioned results    actually, closely related results, not the exact same one   already appeared elsewhere (in a ArXiv preprint, that has been publicly submitted at a previous conference). We do believe that there is no ethical/plagiarism issue here, however, it remained the question of "paternity" of these results.  We have decided to give the paternity of the sample complexity to the first paper (the ArXiv preprint) that proved it. We can therefore only credit to this paper the improvements in the sample complexity results (as they are not exactly similar).  However, this had an impact on the reviewers (and also mine and the SAC one) evaluation of the paper, when some substantial parts of the paper are "discarded".  Nonetheless, we think that this paper has its merits, although it does not reach the ICLR bar in its current form. We strongly encourage the authors to work on a revised version   incorporating the different comments of the reviewers   and to resubmit it at a further venue.
The authors design a framework to estimate the uncertainties in the predictions of gradient boosting models, for both classification and regression. The framework contains several methods, some that use sub sampling on data to calculate the estimation, and some that use sub sampling on the trees within one single gradient boosting model (i.e. virtual ensemble) to calculate the estimation. The different methods reveal the trade off between faster calculation and good uncertainty estimation. The authors conduct extensive empirical study to demonstrate the validity of the designed framework.  The reviewers agree that the paper is well written on a very important topic of machine learning in practice. The authors have done a great job addressing the comments from the reviewers, including the comparison to random forest, and adding more motivating examples. The reviewers believe that the work marks a good starting point for addressing this important topic. Nevertheless, the reviewers have some concerns that the results are promising but not impressive yet, and the performance of the virtual ensemble is a bit discouraging.  
This paper recognizes that several common sub problems studied in RL, such as meta RL and generalization in RL, can be cast as POMDPs. Using this observation, the authors evaluate how a straightforward approach to deal with POMDPs using a recurrent neural network compares to more specialized approaches. The reviewers agree that the research question studied in this paper is very interesting. However, after careful deliberation, I share the view of reviewer 2WFY that the results insufficiently support the claims made in the paper. In particular, I view the main claim from the abstract "We find that a careful architecture and hyperparameter decisions yield a recurrent model free implementation that performs on par with (and occasionally substantially better than) more sophisticated recent techniques in their respective domains."  as insufficiently supported. The main issue with the experiments is that only a small number of simple domains are considered. As Luisa points out in the public comments, variBAD dominates recurrent baselines when more complex tasks are considered, while on simpler domains such as the Cheetah Vel domain considered in this paper, it performs similar to a recurrent model free baseline. In the rebuttal the authors have added a more complex domain to address this, showing that a recurrent model free baseline outperforms an off policy version of variBAD. However, I view these results as inconclusive, as only a single complex domain is considered and they appear to contradict previous results with on policy variBAD. For these reasons, I don t think the work in its current form is ready for publication at ICLR. But I want to encourage the authors to work out this direction further. In particular, adding more complex domains and also considering the on policy variBAD method, can make this work stronger.
This paper proposes an unsupervised graph learning method [Iterative Graph Self Distillation (IGSD)] by iteratively performing self distillation to contrast graph pairs under different augmented views. This idea is then extended to semi supervised setting where via a supervised contrastive loss and self training. The method is empirically evaluated on some semi supervised graph classification and molecular property prediction tasks, and has achieved promising results.  Reviewers agree that the method is interesting and the paper is well written. The biggest concern from reviewers related to experimental evaluations of the method. The authors responded to this and included additional experiments. Although the reviewers appreciate the provided results and explanations, at the end they were not convinced about the empirical assessments. In particular, R1 s post rebuttal comment indicates concerns about the reported performance of GCKN, which is different from the published one in Table 1 of GCKN paper. I encourage authors to improve on these experimental discrepancies and resubmit. 
This paper introduces a pruning criterion which is similar to magnitude based pruning, but which accounts for the interactions between layers. The reviewers have gone through the paper carefully, and after back and forth with the authors, they are all satisfied with the paper and support acceptance.
This work raised quite a few questions, and left the reviewers somewhat divided. The authors have done their best to answer these questions, conducting additional experiments where needed.  The close relation of this work to Mask Predict (Ghazvininejad et al. 2019) was noted by several reviewers. Although the current version of the manuscript addresses this, the introduction still frames Mask Predict as an iterative model, and does not explicitly make the connection between GLAT and single iteration Mask Predict. My impression is that this understates the relationship between these models somewhat.  Taking single iteration Mask Predict as a baseline, the proposed extension is fairly simple, and seemingly effective, which is a potentially impactful combination. However, the manuscript is still held back by presentation issues (including but not limited to spelling and choice of words), and I concur with Reviewer 2 that the connection with curriculum learning should be elucidated not just in words, but with supporting experimental analysis.  Regarding training cost, given that training for GLAT seems to be more costly for the same number of training iterations, a comparison where the total compute budget is held constant could be interesting   though I appreciate that this is not a key point of the paper, as the authors point out (whereas inference cost is).  I believe the changes made by the authors in response to the reviewers  comments are substantial enough that they merit a further review cycle, and may still fall short of the reviewers  expectations in some aspects. Therefore, I will not recommend acceptance, though I want to add that this was a tough call to make. I would also like to encourage the authors to resubmit their updated manuscript.
The authors propose an approach for anomaly detection in the setting where the training data includes both normal and anomalous data.  Their approach is a fairly straightforward extension of existing ideas, in which they iterate between clustering the data into normal vs. anomalous and learning an autoencoder representation of normal data that is then used to score normality of new data.  The results are promising, but the experiments are fairly limited.  The authors argue that their experimental settings follow those of prior work, but I think that for such an incremental contribution, more empirical work should be done, regardless of the limitations of particular prior work.
The paper argues that adversarial training increases inter class similarities, therby increasingly the misclassification of some classes and lowering accuracy parity across classes. It proposes to combine existing adversarial training methodologies, PGD AT and TRADES, with a maximum entropy term to improve the classification fairness while remaining robust.  While they agree that the problem is timely and important, the reviewers identify the following issues that place the current iteration of the paper below the bar of acceptance: the comparison to other works on fair robust training and accuracy parity is incomplete; experimental evaluation is conducted only on CIFAR10, making the generalizability of the paper s claims about performance unclear; and the proposed methodology has low technical novelty.
Three experts reviewed this paper and all recommended rejection. The rebuttal did not change the reviewers  recommendations. The reviewers was not excited by the proposed probabilistic framework and raised many concerns regarding the comparison with baselines and competing methods, limited size of datasets, and limited scope of one dataset for one task. Considering the reviewers  concerns, we regret that the paper cannot be recommended for acceptance at this time.  The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper deals with the under sensitivity problem in natural language inference tasks.  An interval bound propagation (IBP) approach is applied to predict the confidence of the model when a subsets of words from the input text are deleted.  The paper is well written and easy to follow.  The authors give detailed rebuttal and 3 of the 4 reviewers lean to accept the paper.
The paper addresses an interesting problem, namely how to evolve effective weight and activation update rules for online learning of a recurrent network. The work focuses on two specific tasks: character sequence memorisation and prediction. Two approaches based on meta gradients and evolutionary strategies are explored. Unfortunately the paper is missing some important related works. Moreover, presentation needs to be improved, as well as experimental assessment should be expanded both in terms of tasks and in terms of comparable models presented in the literature.
This paper proposes an autoregressive flow based network, Flowtron, for TTS with style transfer. It integrates the Tacotron architecture with the flow based generative model.  Extensive experiments are carried out in a controlled manner and the results show that the proposed Flowtron framework can achieve comparable MOS scores to the SOTA TTS models and is good at generating speech with different styles. All reviewers consider the work interesting.  There are concerns raised on technical details which mostly have been cleared by the authors  rebuttal. The exposition also has been greatly improved based on the reviewers  suggestions and questions.  Overall, this is an interesting paper and I would recommend acceptance. 
The paper introduces a new method for 3d point cloud generation based upon auto encoders and GANs.  Two reviewers voted for accept and one reviewer for outright reject. Both authors and reviewers posted thorough responses. Based upon these it is judged best to not accept the paper in the present. The authors should take the feedback into account in a an updated version of the paper.  Rejection is recommended.  
All reviewers recommend rejection, and I m following this recommendation.
This submission proposes an approach for fusing representations at multiple scales to improve object detection systems. Reviewers thought the paper was well written and showed positive results on COCO, a common object detection benchmark. However, reviewers agreed that there was not sufficient methodological novelty or empirical improvement over existing approaches to warrant acceptance at ICLR: several prior works have addressed multiscale fusion and reviewers did not find the evaluation/ablations sufficient to demonstrate the approach yielded substantial improvements over these existing approaches. I hope the authors will consider resubmitting the paper after refining it based on the reviewers  feedback.
After the rebuttal phase, all reviewers give borderline scores (leaning slightly positive, one of these noted in the comment rather than final review). While the reviewers recognize the potential merit of the contribution (efficiency while preserving effectiveness), support for acceptance is not sufficient. The major concerns include novelty (shared by multiple reviewers) and the limited experimental settings.
There is no author response for this paper. The paper addresses the affective analysis of video sequences in terms of continual emotions of valence and arousal. The authors propose a multi modal approach (combining modalities such as audio, pose estimation, basic emotions and scene analysis) and a multi scale temporal feature extractor (to capture short and long temporal context via LSTMs) to tackle the problem. All the reviewers and AC agreed that the paper lacks (1) novelty, as the proposed approach is a combination of the existing well studied techniques without explanations why and when this could be advantageous beyond the considered task, (2) clarity and motivation   see R2’s and R3’s concerns and suggestions on how to improve. We hope the reviews are useful for improving the paper. 
The paper focuses on zero shot capability of BERT like models. The key contribution boils down to a novel prompting techniques that effectively ensembles predictions made for [MASK] tokens inserted at different places.  Reviewers B5Rv and 9k3X voted for rejecting mostly on the grounds that the contribution is not significant enough for ICLR. In particular, there are already existing works show that null prompting works, and other works that suggest that using multiple prompts works. While these insights have not been combined before, it is to some extend incrementally.  On the positive side, the multi null prompting strategy is a genuinely useful tool. I think it is likely to find applications in different NLP applications as an effective way to generate ensemble predictions. The paper has also many carefully carried out experiments that will likely help guide future efforts in designing effective prompting strategies.  On the whole, I am recommending rejection. I know this is a disappointing result. Thank you for your submission, and I hope the reviews will help improve your paper.
This paper presents a method for using transformer models to perform approximate Bayesian inference, in the sense of approximating the posterior predictive distribution for a test example.  This seems similar to doing amortized variational inference using a transformer model.  The reviewers all found the paper to be clearly written, interesting, novel and compelling.  Two of the reviewers found the results "impressive".  There is some concern of over claiming (is it really Bayesian?, are the authors making too broad statements based on very simple case studies?).  The presented method is also not scalable O(n^2), so the setting is restricted to very small datasets and models.   However, the reviewers didn t seem especially concerned by this.  The reviews were mixed but leaning positive (8, 6, 5) and the positive reviews are more substantial.  Therefore the recommendation is to accept, but please incorporate the reviewer feedback and additional discussion about related methods (discussion below) into the camera ready.
This paper received 3 reviews with mixed initial ratings: 9,4,5. The main concerns of R1 and R3, who gave unfavorable scores, included lack of novelty and hence limited value of this work for the ML community. At the same time, R5 strongly advocates for acceptance and mentions meaningful contributions in the context of the specific application, including the new dataset. In response to that, the authors submitted a new revision and provided detailed responses to each of the reviews separately, which did not change the position of the reviewers. The AC agrees with R1 and R3 that, even though the biometrics related contributions are relevant, the scope of this work is too narrow and application driven for presentation in the main track of ICLR. As a result, the final recommendation is reject.
All reviewers have carefully reviewed and discussed this paper. They are in consensus that this manuscript merits a strong revision. I encourage the authors to take these experts  thoughts into consideration in revising their manuscript.
The paper surveys existing differentially private data synthesis methods, and introduces an algorithm that learns both a generator and a classifier in a differentially private mode.  The problem is highly timely and important. Results are promising.  Main remaining concerns after discussion between the reviewers and the authors are:    reason why the proposed scheme can give better classification accuracy, should be clarified more    unclarity on conclusions that can be drawn from the experiments. The revised version has improved on this somewhat.  One explanation for the problems was suggested to be that the paper tries, at the same time, to both present a new method and be a survey. Is hard to do in a short paper, and as a result, the paper lacks focus. At the very least, more work is needed.  The authors are encouraged to continue their work on this important problem, and the review comments hopefully help in that. 
This paper proposes a compressed sensing (CS) method which employs deep image prior (DIP) algorithm to recovering signals for images from noisy measurements using untrained deep generative models.  A novel learned regularization technique is also introduced. Experimental results show that the proposed methods outperformed the existing work. The theoretical analysis of early stopping is also given. All reviewers agree that it is novel to combine the deep learning method with compressed sensing. The paper is well written and overall good. However the reviewers also proposed many concerns about method and the experiments, but the authors gave no rebuttal almost no revisions were made on the paper. I would suggest the author to consider the reviewers  concern seriously and resubmit the paper to another conference or journal.
The submission suggests reducing the parameters in a conv lSTM by replacing the 3 gates in the standard LSTM with one gate. The idea is to get a more efficient convolutional LSTM and use it for video prediction. Two of the reviewers found the manuscript and description of the work difficult to follow and the justification for the proposed method lacking. Additionally, the contribution of this submission feels rather thin, and the experimental results are not very convincing: the absolute training time is too coarse of a measurement (and convergence may depend on many factors), and the improvements over PredNet seem somewhat marginal.  Finally, I agree with the reviewer that mentioned that a proper comparison with baselines should be done in such a way that the number of parameters is comparable (if #params is a main claim of the paper!). It is entirely plausible that if you reduce the number of parameters in PredNet by 40% (in some other way), its performance would also benefit.  With all this in mind, I do not recommend this paper be accepted at this time.
Strengths:      Solid experiments    The paper is well written  Weaknesses:    The findings are not entirely novel and not so surprising, previous papers (e.g., Brevlins et al (ACL 2018)) have already  suggested that LM objectives are preferable and also using LM objective for pretraining is already the  standard practice (see details in R1 and R3).   There is a consensus between the two reviewers who provided detailed comments and engaged in discussion with the authors.  
This paper introduces a multi domain self supervised representation learning method. Its objective consists of three terms: the first two terms are identical to SimCLR and the last one is to minimize the similarity of pairs across different datasets which is similar to the second term of SimCLR. In the experiment, it tests the methods across multiple common datasets. The method is simple but results are pretty good at the multi domain setting. It seems to demonstrate the importance of domain clustering and moving the domains apart. However, there are several important questions the paper may need more clarification on:  1. What is the definition of the domain? How to determine the pair of data is from different domains? What is the motivation/theory that you used to choose those datasets as different domains in your experiment?  2. Is there any of the public datasets that would cover multiple domains?  Without solving these questions, I think it would constrain the future research/adoption of the method.
This paper evaluates interpretation methods of neural networks on time series data. The reviewers find some values in this work, but were also consistently concerned with the main theme and novelty of this work. The authors have actively responded to reviewer comments, but the reviewers were not convinced with the major contributions and novelty. Thus the work is not ready for ICLR.
This paper propose two new neural network (NN) architectures, namely TNN, and SQANN. The paper claims that these networks are resistant to catastrophic forgetting, are interpretable, and are highly accurate. While the reviewers agree that the idea of making neurons reflect training data is novel, some concerns remain post rebuttal. Most of the reviewers opine that the statements of theorems are unclear, confusing, and hard to interpret (even after the rebuttal and update), thus making it hard to appreciate the contributions of this work. Given this, we are unable to recommend this paper for acceptance at this time. We hope the authors find reviewer feedback useful.
In my opinion, this paper is borderline (but my expertise is not in this area) and the reviewers are too uncertain to be of help in making an informed decision.
This paper analyzes popular metric based few shot learning (FSL) methods from the perspective of computational geometry. Namely, viewing prototypical networks as Voronoi diagrams (VDs). This lends itself to incorporating extensions based on the recently proposed CIVD that allows for multiple centers per cell. The paper then discusses various aspects of the FSL pipeline (data augmentation, feature transformations, geometries and representations), referred to as heterogeneities, that can be efficiently ensembled via a cluster to cluster VD (CCVD). The resulting model produces state of the art results.  Initial concerns from the reviewers pointed to a potential lack of novelty (since it can be seen as applying existing ideas to FSL), lack of self containment in the main paper, weak positioning in the context of other FSL methods (which ones can be interpreted under the VD framework) and a potentially impractical computational complexity. The discussion period settled these issues, with the paper receiving several updates, and the reviewers all ended up recommending acceptance.  Personally, I would like to see an addition to Figure 1 with the resulting decision boundaries from CIVD and CCVD. I think that this would greatly improve the ability of the reader to reason about the approach intuitively. Also, as a minor comment, I think that the argmax below eqs 1 and 7 should either be an argmin, or the distances should be negated. Otherwise, I think this is a valuable contribution to the FSL literature.
Two reviewers are positive about this paper while the other reviewer is negative. The low scoring reviewer did not respond to discussions. I also read the paper and found it interesting. Thus an accept is recommended.
The paper investigates a variant of the "cross entropy method" (CME) for heuristic combinatorial optimization, based on stochastically improving a search distribution via policy optimization in a surrogate objective.  Unfortunately, the reviewers unanimously recommended rejection, noting that the significance of the contribution over CME remains far from clear and insufficiently supported by the given evidence.  The experimental evaluation was unconvincing to all of the reviewers, particularly since only one artificial problem (clique finding) was considered in the paper (with an additional problem, k medoid clustering, briefly and incompletely considered in the appendix).  Several additional concerns were raised about the experimental evaluation, which triggered lengthy author responses but really need to be properly handled in the paper itself:    The sensitivity of performance to the optimization algorithm is a concern and requires more detailed understanding so that reasonable choices can be made in practice.    The independence assumption between search components is an extreme simplification that limits the appeal and applicability of the proposed approach.  Even after author response, it remains unconvincing that an independent search distribution over subcomponents can be effective in challenging combinatorial spaces.  Concrete evidence on challenging problems would be a more effective evidence than discussion.    The comparisons omitted any tailored algorithms for the specific problems.  Even if the authors insist on only comparing to more "general purpose" methods, there is a large space of evolutionary and Bayesian optimization strategies that have been neglected from the comparison.  A justification is needed for such an omission (if indeed it is even justifiable).
This paper offers a novel method for semi supervised learning using GMMs.  Unfortunately the novelty of the contribution is unclear, and the majority of the reviewers find the paper is not acceptable in present form.  The AC concurs.
The paper provides a language for optimizing through physical simulations. The reviewers had a number of concerns related to paper organization and insufficient comparisons to related work (jax). During the discussion phase, the authors significantly updated their paper and ran additional experiments, leading to a much stronger paper.
The paper proposed to add the sliced Wasserstein distance between the distribution of the encoded training samples and a samplable prior distribution to the auto encoder (AE) loss, resulting in a model named sliced Wasserstein AE. The difference compared to the Wasserstein AE (WAE) lies in using the usage of the sliced Wasserstein distance instead of GAN or MMD based penalties. The idea of the paper is interesting, and a theoretical and an empirical analysis supporting the approach are presented. As reviewer 1 noticed, „the advantage of using sliced Wasserstein distance is twofold: 1)parametric free (compared to GANs); 2) almost hyperparameter free (compared to the MMD with RBF kernels), except setting the number of random projection bases.“ However, the empirical evaluation in the paper and concurrent ICLR submission on Cramer World AEs the authors refer to shows no clear practical advantage over the WAE, which leads to better results at least regarding the FID score. On the other hand, the Cramer World AE is based on the ideas presented in this paper (which was previously available on arxive) proving that the paper presents interesting ideas which are of value to the communty. Therefore, the paper is a bit boarderline, but I recommand to accept it. 
This paper studies whether the Bellman error is a good metric to reflect the quality of value function estimation, focusing on finite sample off policy data sets. Both theoretical analyses and empirical experiments have been provided, showing that the Bellman error is often not the right metric to consider. However, while I appreciate the authors  theoretical attempts, the current theoretical contributions are not deep/significant enough. As the reviewers mentioned, the failure of the direct use of BRM is not surprising given the insufficiency of data (namely, no algorithm can make predictions on completely unseen regions unless further modeling structure is present). The authors might want to further strengthen their theory along this important direction.
The method proposed in the paper for latent disentanglement and attribute conditional image generation is novel to the best of my understanding but reviewers (Anon1 and Anon3) have expressed concerns on the quality of results (CelebA images) as well as on the technical presentation and claims in the paper.  Given the novelty of the proposed method, I would *not* like to recommend a "reject" for this paper but the concerns raised by the reviewers on the quality of results and lack of quantitative results seem valid. Authors rule out possibility of any quantitative results in their response but I am not fully convinced   in particular, effectiveness of attribute conditional image generation can be captured by first training an attribute classifier on the generated images and then measuring how often the predicted attributes are flipped when conditioning signal is changed. There are also other metrics in the literature for evaluating generative models.  I would recommend inviting it to the workshop track, given that the work is novel and interesting but has scope for improvements. 
The paper argued some viewpoint about knowledge distillation quite interesting to me: the technically good KD might surprisingly be socially bad in helping outsiders "stealing" commercial models, even if the models are released as black boxes. Then the paper proposed a way called self undermining KD in order to turn a well trained model into a "nasty teacher" (i.e., an undistillable model), and by this way the commercial models and the corresponding intellectual properties for training them from insiders can be nicely protected.  Overall, the quality is quite high. The argument is very conceptually novel and the method is still technically novel. The idea of the method is simple but works for the purpose   that s great! Although the experimental significance seems not too impressive, the paper opens a door to a new world concerning model privacy instead of data privacy, and hence it is of social significance. In my opinion, the paper should have a potentially huge social impact to DL practitioners (and company owners), because KD is being used almost everywhere in the Internet industry to provide the standalone mode of Apps without clouds on personal devices. Based on the quality and the impact, I recommend to accept the paper as a spotlight presentation.
Three knowledgable reviewers give a positive evaluation of the paper. The decision is to accept.
The paper proposes iterative training strategies for learning teacher and student models. They show how iterative training can lead to interpretable strategies over joint training on multiple datasets. All the reviewers felt the idea was interesting, although, one of the reviewers had concerns about the experimentation.  However, there is a BIG problem with this submission. The author names appear in the manuscript thus disregarding anonymity.
This paper proposes a modification to the Transformer architecture in which the self attention and feed forward layer are merged into a self attention layer with "persistent" memory vectors. This involves concatenating the contextual representations with global, learned memory vectors, which are attended over. Experiments show slight gains in character and word level language modeling benchmarks.   While the proposed architectural changes are interesting, they are also rather minor and had a small impact in performance and in number of model parameters. The motivation of the persistent memory vector as replacing the FF layer is a bit tenuous since Eqs 5 and 9 are substantially different. Overall the contribution seems a bit thin for a ICLR paper. I suggest more analysis and possibly experimentation in other tasks in a future iteration of this paper.
This is an extremely interesting and timely paper regarding the approximation ability, with statistical consequences, of circuits and (computation bounded) Turing machines by feedforward networks and transformers. The paper has an interesting and valuable setting, and also many unusual ideas, together which can inspire a lot of future work.  Unfortunately, the reviewers had significant difficulties with the presentation and setting; the Transformer material in particular lacks clarity.  As such, the paper could use more time and polish.  Separately, I will recommend in the future that authors consider making use of the rebuttal and revision phase.  While it is not strictly required, it seems that in ICLR, scores shift quite a lot in those phase, and it has (for better or worse) become standard to have a thorough involvement in this phase.  It was difficult to cause score changes after the initial phase due to the lack of review responses.  That said, I sincerely hope the authors continue with this valuable line of work.
The reviewers were excited by the paper s theoretical contribution to continual learning, since that aspect of continual learning is underdeveloped.  However, all reviewers (including the most positive reviewer during discussions) expressed that the paper would benefit from revisions to improve the clarity and the thoroughness of comparisons in the paper.  The paper s focus on OGD is not necessarily an issue for it to be of use to the community, as mentioned as a negative point in one review that other reviewers disagreed with. The authors are encouraged to revise this paper incorporating the reviewers  suggestions.
This work proposed to insert backdoor into pre trained models, such that down streaming tasks can be attacked.   One of the main issue indicated by most reviewers is that some important and closely related works are missed and not compared, which also studied the backdoor attack to pre trained models. The authors argued in the rebuttal that these missed works require some instances of down streaming tasks, while the proposed method in this work doesn t. However, this difference could not be the reason to miss and not compare with them.  Besides, most reviewers also indicated the insufficient experiments, such as limited defense methods, and some experimental results are not well explained.   After reading the manuscript, reviews and discussions between reviewers and authors, I think this work is not ready for publication. The reviewers  comments are supposed to be helpful to improve this work.
All three reviewers recommend acceptance. The paper introduces an interesting study and insights on the connection between local attention and dynamic depth wise convolution, in terms of sparse connectivity, weight sharing, and dynamic weight. The reviews included questions such as the novelty over [Cordonnier et al 2020] and the connection to Multi scale vision longformer, which were adequately addressed by the authors. The findings in this paper should be interesting to the ICLR community.
The submission proposes a method to construct adversarial attacks based on deforming an input image rather than adding small peturbations.  Although deformations can also be characterized by the difference of the original and deformed image, it is qualitatively and quantitatively different as a small deformation can result in a large difference.  On the positive side, this paper proposes an interesting form of adversarial attack, whose success can give additional insights on the forms of existing adversarial attacks.  The experiments on MNIST and ImageNet are reasonably comprehensive and allow interesting interpretation of how the image deforms to allow the attack.  The paper is also praised for its clarity, and cleaner formulation compared to Xiao et al. (see below).  Additional experiments during rebuttal phase partially answered reviewer concerns, and provided more information e.g. about the effect of the smoothness of the deformation.  There were some concerns that the paper primarly presents one idea, and perhaps missed an opportunity for deeper analysis (R1).  R2 would have appreciated more analysis on how to defend against the attack.  A controversial point is the relation /  novelty with respect to Xiao et al., ICLR 2018.  As e.g. pointed out by R1: "The paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to ICLR 2018 [Xiao and al.]. This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.])."  On the balance, all three reviewers recommended acceptance of the paper.  Regarding novelty over Xiao et al., even ignoring the arguable precedence of the current submission, the formulation is cleaner and will likely advance the analysis of adversarial attacks.
This paper proposes adding a Dirichlet distribution as a wrapper on top of a black box classifier in order to better capture uncertainty in the predictions.  This paper received four reviews in total with scores (1,1,1,6).  The reviewer who gave the weak accept found the paper well written, easy to follow and intuitive.  The other reviewers, however, were primarily concerned about the empirical evaluation of the method.  They found the baselines too weak and weren t convinced that the method would work well in practice.  The reviewers also cited a lack of comparison to existing literature for their scores.  One reviewer noted that while the method addresses aleatoric uncertainty, it doesn t provide any mechanism for epistemic uncertainty, which would be necessary for the applications motivating the work.    The authors did not provide a response and thus there was no discussion. 
This paper studies optimal control with low dimensional representation.  The paper presents interesting progress, although I urge the authors to address all issues raised by reviewers in their revisions.
This paper studies the empirical performance of invertible generative models for compressive sensing, denoising and in painting. One issue in using generative models in this area has been that they hit an error floor in reconstruction due to model collapse etc i.e. one can not achieve zero error in reconstruction. The reviewers raised some concerns about novelty of the approach and thoroughness of the empirical studies. The authors response suggests that they are not claiming novelty w.r.t. to the approach but rather their use in compressive techniques. My own understanding is that this error floor is a major problem and removing its effect is a good contribution even without any novelty in the techniques. However,  I do agree that a more thorough empirical study would be more convincing. While I can not recommend acceptance given the scores I do think this paper has potential and recommend the authors to resubmit to a future venue after a through revision.
The reviewers agree that the work is high quality, clear, original, and could be significant.  Despite this, the scores are borderline. The reason is due to rough agreement that the empirical evaluations are not quite there yet. In particular, two reviewers agree that, in the synthetic experiments, the method is evaluated on data that is an order of magnitude too easy and quite far from the nature of real data, which has a much lower signal to noise ratio.  However, the authors have addressed the majority of the concerns and there is little doubt that the authors are capable of carrying out this new experiment and reporting its results. Even if the results are surprising, they should shed light on what seems to be an interesting new approach.
This paper discusses the promising idea of using RL for optimizing simulators’ parameters.   The theme of this paper was very well received by the reviewers. Initial concerns about insufficient experimentation were justified, however the amendments done during the rebuttal period ameliorated this issue. The authors argue that due to considered domain and status of existing literature, extensive comparisons are difficult. The AC sympathizes with this argument, however it is still advised that the experiments are conducted in a more conclusive way, for example by disentangling the effects of the different choices made by the proposed model. For example, how would different sampling strategies for optimization perform? Are there more natural black box optimization methods to use?  The reviewers believe that the methodology followed has a lot of space for improvement. However, the paper presents some fresh and intriguing ideas, which make it overall a relevant work for presentation at ICLR.
This paper considers "prototypes" in machine learning, in which a small subset of a dataset is selected as representative of the behavior of the models. The authors propose a number of desiderata, and outline the connections to existing approaches. Further, they carry out evaluation with user studies to compare them with human intuition, and empirical experiments to compare them to each other. The reviewers agreed that the search for more concrete definitions of prototypes is a worthy one, and they appreciated the user studies.  The reviewers and AC note the following potential weaknesses: (1) the specific description of prototypes that the authors are using is not provided precisely, (2) the desiderata was found to be informal, leading to considerable confusion regarding the choices that are made and their compatibility with each other, (3) concerns in the evaluation regarding the practicality and the appropriateness of the user study for the goals of the paper.  Although the authors provided detailed responses to these concerns, most of them still remained. Both reviewer 1 and reviewer 2 encourage the authors to define the prototypes defined more precisely, providing motivation for the various choices therein. Even though some of the concerns raised by reviewer 3 were addressed, it still remains to be seen how scalable the approach is for real world applications.  For these reasons, the reviewers and the AC feel that the authors would need to make substantial improvements for the paper to be accepted.
The paper introduces As ViT, an interesting framework for searching and scaling ViTs without training. Overall, the paper received positive reviews. On the other hand, R1 rated the paper as marginally below the threshold, raising concerns about search on small datasets and issues regarding the comparison in terms of FLOPS/accuracy with other methods. The authors adequately addressed these concerns in the rebuttal, and helped clarify other questions by R2 and R3. R1 did not participate in the discussion after the author response nor updated his/her review. The AC agrees with R2 and R3 that the paper passes the acceptance bar of ICLR, as the unified approach for efficient search/scaling/training is novel and should be interesting to the ICLR audience.
This paper proposes an extension of Gradient Episodic Memory (GEM) namely support examples, soft gradient constraints, and positive backward transfer. The authors argue that experiments on MNIST and CIFAR show that the proposed method consistently improves over the original GEM.  All three reviewers are not convinced with experiments in the paper. R1 and R3 mentioned that the improvements over GEM appear to be small. R2 and R3 also have some concerns without results with multiple runs. R3 has questions about hyperparameter tuning. The authors also appears to be missing recent developments in this area (e.g., A GEM). The authors did not provide a rebuttal to these concerns.  I agree with the reviewers and recommend rejecting this paper.
The paper makes novel explorations into how MPC and approximate DP / value function approaches, with value fn ensembles to model value fn uncertainty, can be effectively combined.  The novelty lies in exploring their combination. The experiments are solid. The paper is clearly written.  Open issues include overall novelty, and delineating the setting in which this method is appropriate.  The reviewers and AC are in agreement on what is in the paper. The open question is whether the combination of the ideas is interesting.   After further reviewing the paper and results. the AC believes that the overall combination of ideas and related evaluations that make a useful and promising contribution. As evidenced in some of the reviewer discussion, there is often a considerable schism in the community regarding what is considered fair to introduce in terms of prior knowledge, and blurred definitions regarding planning and control. The AC discounted some of the concerns of R2 that related more to discrete action settings and theoretical considerations; these  often fail to translate to difficult problems in continuous action settings.  The AC believes that R3 nicely articulates the issues of the paper that can be (and should be) addressed in the writing, i.e., to describe and motivate the settings that the proposed framework targets, as articulated in the reviews and ensuing discussion.  
This paper focuses on  once for all (OFA) network training towards developing accurate models for different hardware platforms and varying latency constraints. The paper proposes an approach to significantly reducing model search space and thus training costs without losing in predictive performance. The paper is well written, and the authors provide a thorough and convincing response to the concerns raised by some of the reviewers.
The paper proposes an unrolled algorithm to solve the l1 norm formulated dictionary learning problem, and focuses on the number of unrolling steps. It shows that it is better to limit the number of unrolling steps, and this leads to favorable performance over the alternating minimization baseline. The method can also be adapted to scale to very large datasets.  Most reviewers were positive or became positive after the rebuttals.  Reviewer njnY was still concerned about some issues, such as constraints and the choice of the l1 model over the l0 model; there also may have been confusion about unit sphere vs unit ball constraints.  However, given the recommendations of the other reviewers and my own opinion, I think the paper is a worthy contribution, and the point about not unrolling too deeply is an important one that is worth highlighting.
This paper introduces a further regularizer, retrospection loss, for training neural networks, which leverages past parameter states.  The authors added several ablation studies and extra experiments during the rebuttal, which are helpful to show that their method is useful. However, this is still one of those papers that essentially proposes an additional heuristic to train deep news, which is helpful but not clearly motivated from a theoretical point of view (despite the intuitions). Yes, it provides improvements across tasks but these are all relatively small, and the method is more involved. Therefore, I am recommending rejection.
Summary: The authors built on existing work of GP vine copula models. Some modifications are made, to conditional marginals and mixing. Applications to mutual information estimation are discussed and evaluated, and the approach is applied to joint neural/behavioral data.   Discussion: Strengths mentioned in the reviews are that the application is (from a neuroscience perspective) interesting, that estimating mutual information is an important problem, and that the paper is very well written. Weaknesses are the limited novelty (from a machine learning perspective), and weak empirical validation.   The authors have responded in detail, and were able to clarify a number of unclear points. Clearly, however, the main criticisms noted above are hard to address in discussion.  Despite the paper being overall clearly written, I agree with reviewers that it is hard to tell from abstract and introduction where the paper is going (even after modifications made by the authors in the course of the discussion); of the fairly long abstract, just about half a sentence relates to where the proposed model differs from previous work.    Recommendation: I recommend rejection. Despite some clearly positive aspects, the two main criticisms voiced by reviewers are serious: Weak validation and minimal  novelty from a machine learning perspective. I agree that the neuroscience application may be interesting, but requires more validation.  If the authors want to pursue this work further, I would suggest to perhaps consider first where to position the paper s focus. Estimation of mutual information is a problem that is both hard and important. Any progress here would be welcome, and simple usefulness could offset any lack of model novelty, but it would have to be carefully and comprehensively evaluated. On the other hand, a focus on neuroscience applications would require more emphasis on, and presumably more space in the paper for, relevant experiments.  
This paper suggests a novel defense against adversarial perturbations where during training a loss term is added which enforces similar feature representations. At test time: i) noise is added, ii) the feature loss is minimized  The authors report excellent results against AutoAttack but the problem is that AutoAttack expects a static, non randomized defense. Both is not the case for the defense proposed in the present paper. Therefore,  the evaluation with AutoAttack could significantly overestimate the actual robustness and the evaluation of the paper is therefore not valid. Thus adaptive attacks are needed, which are tailored to the defense mechanism, see e.g. Carlini et al, On Evaluating Adversarial Robustness, https://arxiv.org/abs/1902.0670.   As two reviewers noticed, the suggested "adaptive attack" in the paper is not properly attacking the whole defense mechanism by unrolling the test time optimization and using additionally EOT. Thus it is unclear at the moment if the method is really robust. Moreover, the inference time is significantly increased so that it is questionable if this approach is practically relevant. Therefore this paper is not ready for publication yet.
The authors propose a novel approach to using surrogate gradient information in ES. Unlike previous approaches, their method always finds a descent direction that is better than the surrogate gradient. This allows them to use previous gradient estimates as the surrogate gradient. They prove results for the linear case and under simplifying assumptions that it extends beyond the linear case. Finally, they evaluate on MNIST and RL tasks and show improvements over ES.  After the revisions, reviewers were concerned about:  * The strong (and potentially unrealistic) assumptions for the theorems. They felt that these assumptions trivialized the theorems. * Limited experiments demonstrating advantages in situations where other more effective methods could be used. The performance on the RL tasks shows small gains compared to a vanilla ES approach. Thus, the usefulness of the approach is not clearly demonstrated.   I think that the paper has the potential to be a strong submission if the authors can extend their experiments to more complex problems and demonstrate gains. At this time however, I recommend rejection.
This paper provides interesting discussions on the trade off between model accuracy and robustness to adversarial examples. All reviewers found that both empirical studies and theoretical results are solid. The paper is very well written. The visualization results are very intuitive. I recommend acceptance. 
The paper proposes a technique for incorporating prior knowledge as relations between training instances.  The reviewers had a mixed set of concerns, with one common one being an insufficient comparison with / discussion of related work. Some reviewers also found the clarity lacking, but were satisfied with the revision. One reviewer found the claim of the approach being general but only tested and valid for the VQA dataset problematic.  Following the discussion, I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to another venue.
All of the reviewers recommended rejecting this paper. There were concerns that the underlying research questions being probed were not expressed clearly enough. Reviewers were concerned that the experimental work was not sufficient to warrant acceptance. Other concerns included the technical depth of the paper, the degree to which related work was discussed, placed in context and compared with empirically. The AC recommends rejecting this paper.
This paper presents a variant of the WAE which uses a contrastive criterion to enforce the marginal distribution matching constraint. Experiments show faster convergence in terms of Wasserstein distance, more visually appealing samples, and better FID scores compared with other WAE models.  The original WAE framework leaves open the choice of approximation for enforcing marginal distribution matching, and the original paper gives two such algorithms. Therefore, it s pretty natural to replace this approximation with something else (such as the contrastive criterion used here), so a submission would need to show evidence that it s significantly better than other approaches. Reviewers have expressed various concerns about the experiments. None of them are major problems, but overall the method doesn t seem consistently better than other WAE methods; e.g., the FID score is worse than that of WAE GAN.  I encourage the authors to take the reviewers  comments into account in preparing the submission for future cycles.
This paper conducted theoretical analysis of the effect of batch normalisation to auto rate tuning. It provides an explanation for the empirical success of BN. The assumptions for the analysis is also closer to the common practice of batch normalization compared to a related work of Wu et al. 2018.  One of the concerns raised by the reviewer is that the analysis does not immediately apply to practical uses of BN, but the authors already discussed how to fill the gap with a slight change of the activation function. Another concern is about the lack of empirical evaluation of the theory, and the authors provide additional experiments in the revision. R1 also points out a few weaknesses in the theoretical analysis, which I think would help improve the paper further if the authors could clarify and provide discussion in their revision.  Overall, it is a good paper that will help improve our theoretical understanding about the power tool of batch normalization.
 This submission explores recent theoretical work by Shwartz Ziv and Tishby on explaining the generalization ability of deep networks. The paper gives counter examples that suggest aspects of the theory might not be relevant for all neural networks.  There is some uncertainty surrounding the results where mutual information is estimated empirically. Even state of the art estimation methods might lead to misleading empirical results. However, the submission appears to follow reasonable practice following previous work, making the reported results at least suggestive. They warrant reporting for further study and discussion.  The reviewers generally found the paper interesting enough for acceptance, however strong objections were posted by Tishby. A lengthy public exchange resulted between the groups of authors. Not every part of this exchange is resolved. It is not clear whether Tishby s group would be able to fix the full connected ReLU demonstration in this paper, or whether the authors of this submission have anything to say about Tishby s ReLU+convnet demonstration. By accepting this work, we are not declaring where this debate will end. However, we felt the current submission is a constructive part of ongoing discussion in the literature on furthering our theoretical understanding of neural networks.
AR1 is concerned about whether higher order interactions are modeled explicitly and if pi SGD convergence conditions can be easily satisfied. AR2 is concerned that basic JP has been conceptually discussed in the literature and \pi SGD is not novel because it was realized by Hamilton et al. (2017) and Moore & Neville (2017). However, the authors provide some theoretical analysis for this setting in contrast to prior works. AR1 is also concerned that the effect of higher order information has not been  disentangled  experimentally from order invariance. AR4 is concerned about  poor performance of higher order Janossy pooling compared to k  1 case and asks about the number of hyper parameters. The authors showed a harder task of computing the variance of a sequence of numbers in response.  On balance, despite justified concerns of AR2 about novelty and AR1 about experimental verification, the work appears to tackle an interesting topic.  Reviewers find the problem interesting and see some hope in the proposed solutions. On balance, AC recommends this paper to be accepted at ICLR. The authors are asked to update manuscript to reflect honestly weaknesses as expressed by reviewers, e.g. issue with effects of  higher order information  and  disentangled  from order invariance.
This paper introduces a quantum pyramidal circuit for the computation of orthogonal layers in neural networks and implements the algorithm on simulators and on a quantum computer to illustrate its effectiveness. It also obtains an O(n^2) classical algorithm for forward and backpropagation.  The reviewers generally found strength in the derivations and implementation on real quantum machines. Some reviewers regarded the contributions as strong and novel, while others expressed skepticism about the novelty and the robustness of the algorithm. Having read the paper in detail, I concur with the several reviewers who found the literature review of classical orthogonal NNs to be lacking. In particular, one reviewer highlights similarities with Householder reflections and Givens rotations, for which substantial literature already exists. Without a proper comparison to this existing work, it is not possible to properly assess the novelty or relative contributions of the current paper.  Beyond an extended discussion of related work, the paper would also benefit from improved experimental analysis. While the paper is framed around the quantum algorithm, the main contributions are described as a novel and efficient classical algorithm. This would indeed be a contribution of interest to the broader (non quantum) ICLR community, but there is no experimental evidence supporting the utility of the proposed methods. An analysis that compares the classical algorithm to the numerous prior works that parameterize orthogonal layers would be an essential addition. As it stands, I cannot recommend the paper for publication.
This paper considers adversarial attacks in deep reinforcement learning, and specifically focuses on the problem of identifying key steps to attack. The paper poses learning these key steps as an RL problem with a cost for the attacker choosing to attack.  The reviewers agreed that this was an interesting problem setup, and the ability to learn these attacks without heuristics is promising. The main concern, which was felt was not adequately addressed in the rebuttals, was that the results need to be more than just competitive with heuristic approaches.  The fact that the attack ratio cannot be reliably changed, even with varying $\lambda$ still presents a major hurdle in the evaluation of the proposed method.  For the aforementioned reasons, I recommend rejecting this paper.
The idea studied here is interesting, if incremental. The empirical results are not particularly stellar, but it s clear that the authors have done their best to provide reproducible and defensible results. A few sticking points: a) The use of the term  UCB , as mentioned in an anonymous comment, is somewhat misleading. "Approximate Confidence Interval" might be less controversial; b) there are a number of recent research results on exploration that are worth paying attention to (Plappert et al, O Donoghue et al.) and worth comparing to, and c) the theoretical results are not always justified or useful (e.g. Equation 9: the bound is trivial, posterior >  0 or 1). 
This paper examines under what conditions influence estimation can be applied to deep networks and finds that, among of items, that influence estimates are poorer for deeper architectures, perhaps due to poor inverse Hessian vector approximations for poor for deeper models. The authors provide an extensive experimental evaluation across datasets and architectures, and demonstrates the fragility of influence estimates in a number of conditions. Although the reviewers noted that these issues are now "folk knowledge", there has been less scientific effort in identifying these failures.  Of course, more theoretical understanding would help the community better understand where these fragilities lie, but the experimental evaluation is sufficiently strong to be of broad interest to the community.
This paper presents a fairly straightforward algorithm for learning a set of sub controllers that can be re used between tasks.  The development of these concepts in a relatively clear way is a nice contribution.  However, the real problem is how niche the setup is.  However, it s over the bar in general.
This paper presents Universal Transformers that generalizes Transformers with recurrent connections. The goal of Universal Transformers is to combine the strength of feed forward convolutional architectures (parallelizability and global receptive fields) with the strength of recurrent neural networks (sequential inductive bias). In addition, the paper investigates a dynamic halting scheme (by adapting Adaptive Computation Time (ACT) of Graves 2016) to allow each individual subsequence to stop recurrent computation dynamically.  Pros:  The paper presents a new generalized architecture that brings a reasonable novelty over the previous Transformers when combined with the dynamic halting scheme. Empirical results are reasonably comprehensive and the codebase is publicly available.  Cons: Unlike RNNs, the network recurs T times over the entire sequence of length M, thus it is not a literal combination of Transformers with RNNs, but only inspired by RNNs. Thus the proposed architecture does not precisely replicate the sequential inductive bias of RNNs. Furthermore, depending on how one views it, the network architecture is not entirely novel in that it is reminiscent of the previous memory network extensions with multi hop reasoning (  a point raised by R1 and R2). While several datasets are covered in the empirical study, the selected datasets may be biased toward simpler/easier tasks (  R1).   Verdict: While key ideas might not be entirely novel (R1/R2), the novelty comes from the fact that these ideas have not been combined and experimented in this exact form of Universal Transformers (with optional dynamic halting/ACT), and that the empirical results are reasonably broad and strong, while not entirely impressive (R1). Sufficient novelty and substance overall, and no issues that are dealbreakers. 
An interesting new approach for doing meta learning incorporating temporal convolution blocks and soft attention. Achieves impressive SOTA results on few shot learning tasks and a number of RL tasks. I appreciate the authors doing the ablation studies in the appendix as that raises my confidence in the novelty aspect of this work. I thus recommend acceptance, but do encourage the authors to perform the ablation experiments promised to Reviewer 1 (especially the one to "show how much SNAILs performance degrades when TCs are replaced with this method [of Vaswani et al.].")
The paper present an approach for defending for, and search for,  modularity  in neural networks, as a step to better interpretations of their functional structure. This is an interesting, and highly original approach, as recognised by the reviewers. However,  there was also some discussion about what exactly can be learned from the derived clusters/modules, and if and how they will lead to a better understanding of neural networks, or provide concrete ways of improving them.  While the authors addressed some issues during the review process, and provided additional results, the consensus (of all three reviewers) was finally that the paper did not reach the quality standards required by ICLR. I share this view  the paper provides a refreshing perspective, but I still am not convinced that I see a clear, compelling  use case  for their approach.  
In this paper the authors propose a wrapper feature selection method that selects features based on 1) redundancy, i.e. the sensitivity of the downstream model to feature elimination, and 2) relevance, i.e. how the individual features impact the accuracy of the target task. The authors use a combination of the redundancy and relevance scores to eliminate the features.   While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues: (1) all reviewers agreed that the proposed approach lacks theoretical justification or convincing empirical evaluations in order to show its effectiveness and general applicability   see R1’s and R2’s requests for evaluation with more datasets/diverse tasks to assess the applicability and generality of the proposed model; see R1’s, R4’s concerns regarding theoretical analysis;  (2) all reviewers expressed concerns regarding the technical issue of combining the redundancy and relevance scores   see R4’s and R2’s concerns regarding the individual/disjoint calibration of scores; see R1’s suggestion to learn to reweigh the scores; (3) experimental setup requires improvement both in terms of clarity of presentation and implementation   see R1’s comment regarding the ranker model, see R4’s concern regarding comparison with a standard deep learning model that does feature learning for a downstream task; both reviewers also suggested to analyse how autoencoders with different capacity could impact the results. Additionally R1 raised a concern regarding relevant recent works that were overlooked.  The authors have tried to address some of these concerns during rebuttal, but an insufficient empirical evidence still remains a critical issue of this work. To conclude, the reviewers and AC suggest that in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
Reading the paper and the reviews themselves, I found myself conflicted about this work:    Multiple reviewers commented that this is a rather incremental piece of work, given that it s a rather straightforward combination of existing losses/models.   On the other hand, there is admittedly value in (1) realizing that this combination is meaningful (2) understanding the meaningful ways in which these work or do not work with ablation studies.   I am not quite satisfied that the datasets and experiments in this work represent in any meaningful way real world noise. However, it does appear that the authors ran experiments on common benchmarks using common protocols so there s only so much that they themselves can be blamed for.   Tangentially, I am somewhat surprised about the relatively good ImageNet performance of this method. I suspect the combination of this being done with uniform noise rather than structured noise is helping quite a bit.  All in all, this work is certainly interesting enough, but the results are just not quite compelling enough to pass the bar.
This paper presents Non Markovian Predictive Coding (NPMC), a method for learning state representations in visual RL domains that can be used for planning. This work builds on recent work on PC3 (Shu et al. 2020) and PlaNet (Hafner et al. 2020). Concretely, NPMC replaces the image reconstruction objective in PlaNet with a noise contrastive estimation (NCE) objective for the latent dynamics model, an NCE objective between the images and representations, and an additional maximum likelihood objective for the latent dynamics.  Reviewers were in agreement that this paper tackles an important problem and appreciated the writing quality, the experiments that demonstrate effectiveness of NPMC in continuous control scenarios, and accompanying theoretical analysis. However, reviewers were on balance in consensus that this paper needs another iteration before it can appear. Aside from discussion of related work, the main weakness noted by reviewers is that the manuscript in its current form makes it clear how NPMC differs from closely related methods from a technical point of view, but does not make it sufficiently clear to what extent non Markovian predictive coding leads to improved planner performance. In particular, the paper lacks detailed comparisons to baselines, and reviewers were not sufficiently convinced by experiments that were added to the appendix after discussion.  The authors indicate that their contribution is that NPMC extends PC3 to RL tasks. The metareviewer appreciates that experimental comparisons can require creative thinking when baselines are not directly applicable to the tasks of interest, but would nonetheless like to encourage the authors to consider how they can improve their experiments.
This paper examines the evolution of densities of initial conditions under the multiplicative weights update rule for learning in two player zero sum games. Specifically, the authors estimate the differential entropy (DE) of a density of initial conditions as it evolves over time (what they call "uncertainty"), and they show that (a) as long as the density of states assigns sufficient mass to all strategies, its DE will increase; and (b) the density of states will get arbitrarily close to the boundary of the state space infinitely often (i.e., at least one pure strategy will be employed with arbitrarily small probability infinitely often). The authors also apply these results to a population like model of learning as well as an optimistic variant of the MWU protocol (the latter in the supplement).  The paper was extensively discussed during the review/rebuttal phase. While the reviewers appreciated the conceptual contributions of the paper, they also identified certain technical shortcomings that were only partially addressed by the authors. One of these issues concerned the possibility that the density of initial conditions may exhibit singularities, in which case the DE may fail to be well defined. As a result, one of the reviewers indicated an intent to downgrade their score from "8" to "3" due to concerns on the correctness of the results presented in the paper.  After discussing with both the authors and the reviewers, my view is that the merits of the paper outweigh its flaws, so I am making an "accept" recommendation. At the same time, there is a number of revisions that the authors will have to undertake in the camera ready version of their paper:  1. The authors need to be more careful with their assumptions and notation. The reviewers already indicated a number of glitches, most of them easily fixable (so they are not of particular concern). On the other hand, the issue of whether the initial density of states becomes singular or not is more subtle and led one of the reviewers to drastically change their evaluation of the paper.    The problem here is that the authors are not being precise in their assumptions for $g^1$ and its support, and this confusion remained throughout the discussion: the authors are looking at distributions that are "smooth with bounded support", but this does not exclude singularities. The counterexample given to the authors was a random variable $X$ supported on $\mathcal{X}   (0,1)$ with density $g(x)   1/(2\sqrt{x})$; this density has bounded support and it is smooth on its support, but it is not itself bounded. [There is an ambiguity here in whether the authors are considering the support to be closed or not.]    The issue for the initial density can be trivially fixed by asking that $g^1$ be itself bounded (or smooth over the closed support, or any other similar statement). However, even if this is assumed for $g^1$, the density at some later time $t$ could, a priori, become singular (incidentally, this is a problem that arises frequently in the study of densities that evolve over time, e.g., as in optimal transport). Thus, even an explicit assumption for $g^1$ does not suffice to ensure that $g^t$ does not develop singularities in future stages. [Incidentally, the authors  reply that the singularity has measure zero and therefore does not contribute to the integral misses the heart of the matter (and raises concerns about the authors  overall treatment of this question): the function $g(x)   (\log 2) \big/ (x \log^2x)$ has infinite differential entropy over $(0,1/2)$ even though it is a smooth density over $(0,1/2)$.]    To be clear, I do not believe that blow ups actuall occur in the authors  model, but there is still something that needs to be shown here. However, since it is impossible to check an argument or proof at this stage (and I do not think it would be fair to let this stand in the way of accepting the paper), the authors should instead revise their paper to add as an **explicit** assumption that $g^t$ has bounded support and is bounded over its support (or clarify whether they take the support to be closed or not).  1. Another concern revolves around the use of the word "uncertainty" to describe the basic premise of the paper. In the authors  model, this does not refer to uncertainty among the learners (all their observations are perfectly certain and deterministic), so it is not used in the sense that is standard in game theory and learning (cf. the classic works of Bertsekas, Dekel, Fudenberg, Tsitsiklis, and many others). Instead, the authors  use of the word seems to refer to some "outside spectator" who can only partially guess the players  initial conditions, and tries to guess the evolution of the players  mixed strategies (but still has full information about the learning model that players use, its parameters, etc.). However, this model is not fleshed out in sufficient detail by the authors, so the term "uncertainty" does not seem appropriate here.    During the rebuttal phase, the authors argued that the goal of their paper is "bringing the notion of DE to machine learning audience s attention as a measure of uncertainty, explaining how the change of DE is related to the Jacobian of the underlying dynamical systems" and they asked "that [the paper s] title remains as is". While I am sympathetic to the authors  request, the fact remains that the current title (and part of the discussion in the abstract) is not representative of the paper.    Given the authors  stated objective, the simplest solution would be to frame the paper as the "evolution of differential entropy under..." or the "evolution of spectator/observer uncertainty" or something of the sort. Both titles carry more information and, based on the authors  input, are more appropriate for the range of ideas the authors wish to convey – but simply saying "uncertainty" goes against the established terminology of the field.  Overall, I would urge the authors to avoid vague/ambiguous terminology and statements, and focus instead on exact mathematical definitions that are not open to interpretation. The ideas presented in the paper are interesting and fresh, so they deserve a likewise sharp and precise treatment.
This submission proposes an image generation technique for composing concepts by combining their associated distributions.   Strengths:  The approach is interesting and novel.  Weaknesses:  Several reviewers were not convinced about the correctness of the formulations for negation and disjunction.  The experimental validation of the disjunction and negation approaches is insufficient.  The paper clarity and exposition could be improved. The authors addressed this in the discussion but concerns remain.  Given the weaknesses, AC shares R3’s recommendation to reject.
This paper considers solving the minimax formulation of adversarial training, where it proposes a new method based on a generic learning to learn (L2L) framework. Particularly, instead of applying the existing hand designed algorithms for the inner problem, it learns an optimizer parametrized as a convolutional neural network. A robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The idea is using L2L is sensible. However, main concerns on empirical studies remain after rebuttal. 
The authors posit and investigate a hypothesis   the “lottery ticket hypothesis”   which aims to explain why overparameterized neural networks are easier to train than their sparse counterparts. Under this hypothesis, randomly initialized dense networks are easier to train because they contain a larger number of “winning tickets”. This paper received very favorable reviews, though there were some notable points of concern. The reviewers and the AC appreciated the detailed and careful experimentation and analysis. However, there were a couple of points of concern raised by the reviewers: 1) the lack of experiments conducted on large scale tasks and models, and 2) the lack of a clear application of the idea beyond what has been proposed previously.   Overall, this is a very interesting paper with convincing experimental validation and as such the AC is happy to accept the work.
This paper studies the control of symmetric linear dynamical systems with unknown dynamics. While the reviewers agree that this is an interesting topic, there are concerns that the assumptions are not realistic. Lack of experiments also stands out. I recommend the paper to workshop track with the hope that it will foster more discussions and lead to more realistic assumptions.
The paper proposes a method to embed graph nodes into a gaussian distribution rather than the standard latent vector embeddings. The reviewers concur that the method is interesting and the paper is well written especially after the opportunity to update.
This authors seek to improve upon previous work on randomized smoothing for certifiably robust models. They develop loss functions inspired by the notion of distinguishing hard and easy samples while training the base classifier that is randomly smoothed and conduct experiments evaluating their proposed losses on benchmark datasets.  While the reviewers agree that the paper contains interesting ideas, the paper in its current form is unacceptable for publication because: 1) Missing large scale experiments: All prior work on randomized smoothing report results on ImageNet, and this was seen as one of the main advantages of randomized smoothing. Since the authors do not report this, it brings into question the robustness and scalability of improvements obtained. 2) Computational complexity and improvements: The authors  approach has significant computational complexity and the final improvements obtained are marginal. This makes it difficult to justify the use of a more expensive method.
Quoting R3: "This paper studies the theoretical property of neural network s loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima."  There were split reviews, with two reviewers recommending acceptance and one recommending rejection.  During a robust rebuttal and discussion phase, both R2 and R3 s appreciation for the work was strengthened.  The authors also provided a robust response to R1, whose main concerns included (i) that the paper s analysis is limited to piecewise linear activation functions, (ii) technical questions about the difficulty of proving theorem 2, which appear to have been answered in the discussion, and (iii) concerns about the strength of the language employed.  On the balance, the reviewers were positively impressed with the relevance of the theoretical study and its contributions.  Genuine shortcomings and misunderstandings were systematically resolved during the rebuttal process.
The reviewers were split, with one of them leaning towards rejection, primarily due the (perceived) limited impact of the study. I tend to agree with the other reviewers that this paper provides an interesting and original framework for analysis of learning models, and while there are substantial shortcomings, they are outweighed by the positives (including the promise this approach may hold for analysis of learning in more realistic scenarios). I therefore recommend acceptance, if space in the proceedings allows.
Quality: The overall quality of the work is high.  The main idea and technical choices are well motivated, and the method is about as simple as it could be while achieving its stated objectives.  Clarity:  The writing is clear, with the exception of using alternative scripts for some letters in definitions.  Originality:  The biggest weakness of this work is originality, in that there is a lot of closely related work, and similar ideas without convergence guarantees have begun to be explored.  For example, the (very natural) U net architecture was explored in previous work.  Significance:  This seems like an example of work that will be of interest both to the machine learning community, and also the numerics community, because it also achieves the properties that the numerics community has historically cared about.  It is significant on its own as an improved method, but also as a demonstration that using deep learning doesn t require scrapping existing frameworks but can instead augment them.
The paper presents a method to stochastically optimize second order penalties and show how this could be applied to training fairness aware classifiers, where the linear penalties associated with common fairness criteria are expressed as the second order penalties.   While the reviewers acknowledged the potential usefulness of the proposed approach, all of them agreed that the paper requires: (1) major improvement in clarifying important points related to the approach (see R3’s detailed comments; R2’s concern on using the double sampling method to train non convex models; see R1’s and R3’s concerns regarding the double summation/integral terms and how this effects runtime), and (2) major improvement in justifying its application to fairness; as noted by R2, “there is no sufficient evidence why non convex models are actually useful in the experiments”. Given that fairness problems are currently studied on the small scale datasets (which is not this paper’s fault), a comparison to simpler methods for fairness or other applications could substantially strengthen the contribution and evaluation of this work. We hope the reviews are useful for improving and revising the paper.  
This paper presents a hierarchical memory for cross domain and few shot classification problems. The paper is well written, tackles an important topic, and the proposed approach which is an extension of VSM is interesting. Reviewer YEXZ has some concerns regarding comparison to a more proper baseline. I believe that the authors have adequately addressed this. Reviewer 2Ajk and g1Bf also have suggestions that the authors have incorporated in the revision. I recommend accepting this paper.
This paper addresses challenges in offline model learning, i.e., in the setting where some trajectories are given and can be used for learning a model, which in turn serves to train an RL agent or plan action sequences in simulation. A key issue in this setting is that of compounding errors: as the simulated trajectory deviates from observed data, errors build up, leading to suboptimal performance in the target domain. The paper proposes a distribution matching approach that considers trajectory sequence information and provides theoretical guarantees as well as some promising empirical results.  Several issues were raised by reviewers, including missing references, clarity issues, questions about limitations of the theoretical analysis, and limitations of the empirical validation. Many of the issues raised by reviewers were addressed by the authors during the rebuttal phase.  At the same time, several issues remain. First, the authors committed to adding results for additional tasks (initially deemed too easy or too hard to show differences). Even if the tasks show little separation between methods, these would be important data points to include as they support additional comparisons with prior and future work. The AC has to assess the paper without taking promised additional results into account. Second, questions about the results for Ant are not sufficiently addressed. The plot shows no learning. The author response mentions initialization but this is not deemed a sufficient explanation. Given the remaining questions, my assessment is that the quality and contribution of the submission are not yet ready for publication at the current stage.
The paper proposes PipeGCN, a system that uses pipeline parallelism to accelerate distributed training of large scale graph convolutional neural networks. Like some pipeline parallel methods (but unlike others), PipeGCN involves asynchrony in the sense that its features and feature gradients can be stale. The paper provides theoretical guarantees on the convergence of PipeGCN in the presence of this staleness, which is a nice contribution in itself. In discussion, the reviewers found the work to be well executed and sound. All reviewers recommended acceptance, and I concur with this consensus.
The paper introduces a novel way of doing IRL based on learning constraints. The topic of IRL is an important one in RL and the approach introduced is interesting and forms a fundamental contribution that could lead to relevant follow up work.
This paper proposes to consider value functions as explicit functions of policies, in order to allow generalization not only on the state(action) space, but also on the policy space. The initial reviews assessed that the paper was dealing with an important RL topic, but also raised many concerns about the position to previous works (PVN and PVF), the theoretical contributions and the experiments. The authors provided a rebuttal and revision that only partly addressed the initial concerns (check also the review of R3, updated to provide additional feedback following the author’s rebuttal). The final discussion led to the assessment that the paper is not ready for publication. Remaining concerns include clarity, claims being not fully supported by the experiments, theoretical aspects and missing baselines. 
This paper proposed an extension of the Monte Carlos Tree Search to find the optimal policy. The method combines A* and MCTS algorithms to prioritize the state to be explored. Compare with traditional MCTS based on UCT, A* MCTS seem to perform better.  One concern of the reviewers is the paper s presentation, which is hard to follow. The second concern is the strong restriction of assumption, which make the setting too simple and unrealistic. The rebuttal did not fully address these problems.  This paper needs further polish to meet the standard of ICLR. 
This paper studies the effect of various regularization techniques for dealing with noisy labels. In particular the authors study various regularization techniques such as distance from initialization to mitigate this effect. The authors also provide theory in the NTK regime. All reviewers have positive assessment about the paper and think is clearly written with nice contributions but do raise some questions about novelty given that it mostly follows the normal NTK regime. I agree that the paper is nicely written and well motivated. I do not think the theory developed here fully captures all the nuances of practical observations in this problem. In particular, with label noise this theory suggests that test performance is not dramatically affected by label noise when using regularization or early stopping where as in practice what has been observed (and even proven in some cases) is that the performance is completely unaffected with small label noise. I think this paper is a good addition to ICLR and therefore recommend acceptance but recommend the authors to more clearly articulate the above nuances and limitations of their theory in the final manuscript.  
The paper introduces a novel control based variational inference approach that learns latent dynamics in an *input driven* state space model. An optimal control solution (iLQR) is implicitly used as the recognition model which is fast and compact. Reviewers unanimously agree on the high quality writing and high significance of the work. This paper advances the horizon of nonlinear dynamical system models with unobserved input, an impactful contribution to the neuroscience and time series communities.
The authors propose a method for low resource domain adaptation where the number of examples available in the target domain are limited. The proposed method modifies the basic approach in a CycleGAN by augmenting it with a “content” (task specific) loss, instead of the standard reconstruction error. The authors also demonstrate experimentally that it is important to enforce the loss in both directions (target → source and source  > target). Experiments are conducted on both supervised as well as unsupervised settings. The main concern expressed by the reviewers relates to the novelty of the approach since it is a relatively straightforward extension of CycleGAN/CyCADA, but in the view of a majority of reviewers the work serves a useful contribution as a practical method for developing systems in low resource conditions where it is feasible to label a few new instances. Although the reviewers were not unanimous in their recommendations, on balance in the view of the AC the work is a useful contribution with clear and detailed experiments in the revised version. 
This paper considers an interesting theoretical question. However, it would add to the strength of the paper if it was able to meaningfully connect the considered model as well as derived methodology to the challenges and performance that arise in practice. 
Double coúnterfactual regret minimization is an extension of neural counterfactual regret minimization that uses separate policy and regret networks (reminiscent of similar extensions of the basic RL formula in reinforcement learning). Several new algorithmic modifications are added to improve the performance.   The reviewers agree that this paper is novel, sound, and interesting. One of the reviewers had a set of questions that the authors responded to, seemingly satisfactorily. Given that this seems to be a high quality paper with no obvious issues, it should be accepted.
There was a predominantly positive feedback from the reviewers so I recommend acceptance of the paper. It is well written and well motivated tackling an important problem: That in self supervised learning one might encode different invariances by default, even if some of these invariances are useful for downstream tasks (e.g. being rotation invariant may be detrimental to predicting if an image has the correct rotation on a phone). For this, they propose a simple, yet elegant approach and validate it on many downstream tasks. Given the recent interest in self supervised learning, this appears to be a relevant and interesting paper for the ICLR community.
This paper has been reviewed by four reviewers with three borderline scores leaning towards an accept and one clear reject. Reviewers have raised a number of issues. They feel that *the paper is borderline* as *the paper may not have great novelty* due to the use of low rankness even though it is used for the low rank tensor approximation and that *larger datasets* should be used to demonstrate the effectiveness of the proposed approach (even though there are no papers doing it on the large scale graphs to be fair).  Also, reviewers note that they would like to see more theoretical justifications rather than just to see authors *propose a method for the adversary scenario* without full theoretical analysis. For instance, reviewers xxhm and WHUo were seeking the novel theoretical analysis in the context of adversarial robustness rather than a statement that *the problem of recovering the data under gross error has gained much attention* followed by the list of prior papers and an outline of their findings.  While all reviewers agree that the empirical results look very promising, they also agree that the theoretical analysis needs an improvement. For the above reasons, however tempting, even if overlooking the reject score from the reviewer xxhm, it is difficult for AC to advocate for a clean accept.
In this paper, the authors present adversarial attacks by semantic manipulations, i.e., manipulating specific detectors that result in imperceptible changes in the picture, such as changing texture and color, but without affecting their naturalness. Moreover, these tasks are done on two large scale datasets (ImageNet and MSCOCO) and two visual tasks (classification and captioning). Finally, they also test their adversarial examples against a couple of defense mechanisms and how their transferability. Overall, all reviewers agreed this is an interesting work and well executed, complete with experiments and analyses. I agree with the reviewers in the assessment. I think this is an interesting study that moves us beyond restricted pixel perturbations and overall would be interesting to see what other detectors could be used to generate these type of semantic manipulations. I recommend acceptance of this paper. 
This paper extends the symbolic representation learning work of Konidaris et al. (2018) to be object centric, and generalize in this respect. All the reviewers agreed that this is an interesting problem, and that the approach is novel. Two reviewers gave positive evaluations (6,8), and one reviewer gave a mildly negative review (5), where the main critique is that the method still requires some human effort in designing the planning domain. While completely alleviating human efforts is definitely a good goal to pursue, I believe that it s too high a bar to ask for in the setting of limited data, and I believe that there are many real world problems where requiring some human effort is not too limiting.  Therefore I recommend acceptance.  Please take all reviewer comments into account when preparing the final version.
The manuscript centers on a critique of IRGAN, a recently proposed extension of GANs to the information retrieval setting, and introduces a competing procedure.   Reviewers found the findings and the proposed alternative to be interesting and in one case described the findings as "illuminating", but were overall unsatisfied with the depth of the analysis, and in more than one case complained that too much of the manuscript is spent reviewing IRGAN, with not enough emphasis and detailed investigation of the paper s own contribution. Notational issues, certain gaps in the related work and experiments were addressed in a revision but the paper still reads as spending a bit too much time on background relative to the contributions. Two reviewers seemed to agree that IRGAN s significance made at least some of the focus on it justifiable, but one remarked that SIGIR may be a better venue for this line of work (the AC doesn t necessarily agree).  Given the nature of the changes and the status of the manuscript following revision, it does seem like a more comprehensive rewrite and reframing would be necessary to truly satisfy all reviewer concerns. I therefore recommend against acceptance at this point in time.
The reviewers agree that the idea for dataset distillation is novel, however it is unclear how practical it can be. The paper has been significantly improved through the addition of new baselines, however ultimately the performance is not quite good enough for the reviewers to advocate strongly on its behalf. Perhaps the paper would be better motivated by finding a realistic scenario in which it would make sense for someone to use this approach over reasonable alternatives.
The paper proposes a new method for testing whether new data comes from the same distribution as training data without having an a priori density model of the training data. This is done by looking at the intersection of typical sets of an ensemble of learned models.   On the theoretical side, the paper was received positively by all reviewers. The theoretical results were deemed strong, and the ideas in the paper were considered novel. The problem setting was considered relevant, and seen as a good proposal to deal with the shortcoming of models on out of distribution data.   However, the lack of empirical results on at least somewhat realistic datasets (e.g. MNIST) was commented on by all reviewers. The authors only present a toy experiment. The authors have explained their decision, but I agree with R1 that it would be appropriate in such situations to present the toy experiment next to a more realistic dataset. This also means that the effectiveness of the proposed method in real settings is as of yet unclear. Although the provided toy example was considered clear and illuminating, the clarity of the text could still be improved.  Although the reviewers had a spread in their final score, I think they would all agree that the direction this paper takes is very exciting, but that the current version of the paper is somewhat premature. Thus, unfortunately, I have to recommend rejection at this point.   
This paper proposes an interesting new idea which creates an interesting discussion. 
 * Strengths  This paper presents a very interesting connection between GANs and robust estimation in the presence of corrupted training data. The conceptual ideas are novel and can likely be extended in many further directions. I would not be surprised if this opens up a new line of research.  * Weaknesses  The paper is poorly written. Due to disagreement among the authors and my interest in the topic, I read the paper in detail myself. I think it would be difficult for a non expert to understand the key ideas and I strongly encourage the authors to carefully revise the paper to reach a broader audience and highlight the key insights. Additionally, the experiments are only on toy data.  * Discussion  One of the reviewers was concerned about the lack of efficiency guarantees for the proposed algorithm (indeed, the algorithm requires training GANs which are currently beyond the reach of theory and finicky in practice). That reviewer points to the fact that most papers in the robustness literature are concerned with computational efficiency and is concerned that ignoring this sidesteps one of the key challenges. The reviewer is also concerned about the restriction to parametric or nearly parametric families (e.g. Gaussians and elliptical distributions). Other reviewers were more positive and did not see these as major issues.  * Decision  In my opinion, the lack of efficiency guarantees is not a huge issue, as the primary contribution of the paper is pointing out a non obvious conceptual connection between two literatures. The restriction to parametric families is more concerning, but it seems possible this could be removed with further developments. The main reason for accepting the paper (despite concerns about the writing) is the importance of the conceptual connection. I think this connection is likely to lead to a new line of research and would like to get it out there as soon as possible.  * Comments  Despite the accept decision, I again urge the authors to improve the quality of exposition to ensure that a large audience can appreciate the ideas.
The paper proposes a method for inference in models with GP priors and neural network likelihoods for multi output modelling, dealing with the problem of scalability and missing data. The paper builds upon previous work on inducing variables for scalability on GP models and inference networks for amortization (reducing the number of parameters to estimate) and dealing with missing data.  There are several concerns about the paper in terms of generality/flexibility of the approach, as the proposed model shares the NN parameters across tasks and the results on the small datasets do not show improvements wrt baseline such as GPAR. The authors’ comments provide somewhat satisfactory replies to these issues. Nonetheless, the major drawback of this paper is its novelty as the ideas on the paper have been explored extensively in the GP literature. Although the authors do make a case for scalability when using inference networks, there are other previous works that perhaps the authors are unaware of, for example, https://arxiv.org/abs/1905.10969   and even more sophisticated inference algorithms than can serve as truly state of the art competing approaches (for example based on stochastic gradient Hamiltonian Monte Carlo, https://arxiv.org/abs/1806.05490). 
The reviewers found the work interesting and sensible.  The application of latent space constrained autoencoders to wireless positioning certainly seems novel.  Applications can certainly be exciting additions to the conference program.  However, the reviewers weren t convinced that the technical content of the paper was sufficiently novel to be interesting to the ICLR community.  In particular, the reviewers seem concerned that there are no comparisons to more recent methods for dimensionality reduction and learning latent embeddings, such as variational auto encoders.  Certainly a comparison to more recent work constraining latent representations seems warranted to justify this particular approach.
The paper challenges claims about cross entropy loss attaining max margin when applied to linear classifier and linearly separable data. This is important in moving forward with the development of better loss functions.   The main criticism of the paper is that the results are incremental and can be easily obtained from previous work.   The authors expressed certain concerns about the reviewing process. In the interest of dissipating any doubts, we collected two additional referee reports.   Although one referee is positive about the paper, four other referees agree that the paper is not strong enough.     
While this paper was received pretty well, especially after the revision, reviewers still find it borderline and request further revisions which we cannot check in this short review cycle. Therefore, we encourage the authors to improve the paper and resubmit to a future venue. In particular, please take into account the reviewers  comment to improve the clarity of the paper. Particularly it is critical to clarify the function class you are working with (essentially polynomials) more clearly than what you currently do (i.e., your current gradient definition). It would be helpful for future work to clearly state that this function class is a shortcoming of your work, and that an interesting direction is to extend this to natural function classes in ML (e.g., logistic loss).
First, I d like to thank both the authors and the reviewers for extensive and constructive discussion. The paper proposes a generalization of SAC, which considers the entropy of both the current policy and the action samples in the replay pool. The method is motivated by better sample complexity, as it avoids retaking actions that already appear in the pool. The paper formulates a theoretical algorithm and proves its convergence, as well as a practical algorithm that is compared to SAC and SAC Div in continuous sparse reward tasks.  Generally, the reviewers found the method interesting. After rounds of discussion and revisions, the reviewers identified two remaining issues. Theoretical analysis still requires improvement and the positioning of the paper is not clear. Particularly, the method is motivated as an exploration method, and it should be evaluated as such, for example, by comparing to a more representative set of baseline methods. Therefore, I m recommending rejection, but encourage the authors to improve the work bases on the reviews, and submit to a future conference.
This paper extends the transformer model of Vashwani et al. by replacing the sine/cosine positional encodings with information reflecting the tree stucture of appropriately parsed data. According to the reviews, the paper, while interesting, does not make the cut. My concern here is that the quality of the reviews, in particular those of reviewers 2 and 3, is very sub par. They lack detail (or, in the case of R2, did so until 05 Dec(!!)), and the reviewers did not engage much (or at all) in the subsequent discussion period despite repeated reminders. Infuriatingly, this puts a lot of work squarely in the lap of the AC: if the review process fails the authors, I cannot make a decision on the basis of shoddy reviews and inexistent discussion! Clearly, as this is not the fault of the authors, the best I can offer is to properly read through the paper and reviews, and attempt to make a fair assessment.  Having done so, I conclude that while interesting, I agree with the sentiment expressed in the reviews that the paper is very incremental. In particular, the points of comparison are quite limited and it would have been good to see a more thorough comparison across a wider range of tasks with some more contemporary baselines. Papers like Melis et al. 2017 have shown us that an endemic issue throughout language modelling (and certainly also other evaluation areas) is that complex model improvements are offered without comparison against properly tuned baselines and benchmarks, failing to offer assurances that the baselines would not match performance of the proposed model with proper regularisation. As some of the reviewers, the scope of comparison to prior art in this paper is extremely limited, as is the bibliography, which opens up this concern I ve just outlined that it s difficult to take the results with the confidence they require. In short, my assessment, on the basis of reading the paper and reviews, is that the main failing of this paper is the lack of breadth and depth of evaluation, not that it is incremental (as many good ideas are). I m afraid this paper is not ready for publication at this time, and am sorry the authors will have had a sub par review process, but I believe it s in the best interest of this work to encourage the authors to further evaluate their approach before publishing it in conference proceedings.
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
The paper gives a method for generating contrastive explanations, in terms of user specified concepts, for an agent in a sequential decision making setting.   The reviewers found the paper to be a strong contribution to explainable AI and RL. There were some concerns about the writing, but the revisions have addressed most of these.   Overall, I am delighted to recommend acceptance. I urge the authors to incorporate the feedback in the reviews in the final version.
The paper addresses the problem of learning a teacher model which selects the training samples for the next mini batch used by the student model. The proposed solution is to learn the teacher model using policy gradient. It is an interesting training setting, and the evaluation demonstrates that the method outperforms the baseline. However, it remains unclear how the method would scale to larger datasets, e.g. ImageNet. I would strongly encourage the authors to extend their evaluation to larger datasets and state of the art models, as well as include better baselines, e.g. from Graves et al.
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The method presented here adapts an SGD preconditioner by minimizing particular cost functions which are minimized by the inverse Hessian or inverse Fisher matrix. These cost functions are minimized using natural (or relative) gradient on the Lie group, as previously introduced by Amari. This can be extended to learn a Kronecker factored preconditioner similar to K FAC, except that the preconditioner is constrained to be upper triangular, which allows the relative gradient to be computed using backsubstitution rather than inversion. Experiments show modest speedups compared to SGD on ImageNet and language modeling.  There s a wide divergence in reviewer scores. We can disregard the extremely short review by R2. R1 and R3 each did very careful reviews (R3 even tried out the algorithm), but gave scores of 5 and 8. They agree on most of the particulars, but just emphasized different factors. Because of this, I took a careful look, and indeed I think the paper has significant strengths and weaknesses.   The main strength is the novelty of the approach. Combining relative gradient with upper triangular preconditioners is clever, and allows for a K FAC like algorithm which avoids matrix inversion. I haven t seen anything similar, and this method seems potentially useful. R3 reports that (s)he tried out the algorithm and found it to work well. Contrary to R1, I think the paper does use Lie groups in a meaningful way.  Unfortunately, the writing is below the standards of an ICLR paper. The title is misleading, since the method isn t learning a preconditioner "on" the Lie group. The abstract and introduction don t give a clear idea of what the paper is about. While some motivation for the algorithms is given, it s expressed very tersely, and in a way that will only make sense to someone who knows the mathematical toolbox well enough to appreciate why the algorithm makes sense. As the reviewers point out, important details (such as hyperparameter tuning schemes) are left out of the experiments section.  The experiments are also somewhat problematic, as pointed out by R1. The paper compares only to SGD and Adam, even though many other second order optimizers have been proposed (and often with code available). It s unclear how well the baselines were tuned, and at the end of the day, the performance gain is rather limited. The experiments measure only iterations, not wall clock time.   On the plus side, the experiments include ImageNet, which is ambitious by the standards of an algorithmic paper, and as mentioned above, R3 got good results from the method.  On the whole, I would favor acceptance because of the novelty and potential usefulness of the approach. This would be a pretty solid submission of the writing were improved. (While the authors feel constrained by the 8 page limit, I d recommend going beyond this for clarity.) However, I emphasize that it is very important to clean up the writing. 
This paper studies the  behavior of training of over parametrized models. All the reviewers agree that the questions studied in this paper are important. However the experiments in the paper are fairly preliminary and the paper does not offer any answers to the questions it studies.  Further the writing is very loose and the paper is not ready for publication. I advise authors to take the reviews seriously into account before submitting the paper again. 
This paper proposed Selective Convolutional Unit (SCU) for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low “importance” and replace them by other ones which are in a similar fashion found to be important. To this end the authors propose the so called expected channel damage score (ECDS) which is used for channel selection. The authors also show the effectiveness of SCU on CIFAR 10, CIFAR 100 and Imagenet.  The major concerns from various reviewers are that the design seems the over complicated as well as the experiments are not state of the art. In response, the authors add some explanations on the design idea and new experiments of DenseNet BC 190 on CIFAR10/100. But the reviewers’ major concerns are still there and did not change their ratings (6,5,5). Based on current results, the paper is proposed for borderline lean reject.   
This work combines normalizing flows with conditional sampling. While there are connections to other works, the paper seems novel and applicable, and has nice experimental results. The authors did a good job clarifying the reviewers questions, and have addressed their major concerns. We appreciate the additional analyses added to the paper.
This paper applies methods inspired by neuroscience to analyze the inner workings of LSTM language models. In particular, a simple and clever approach is proposed, in which a sentence is presented in its observed context vs. a random one. The time for a unit activation to become similar in the two contexts is used as a probe of the timescale of contextual effects. The main results are that timescales increase with layer and that there are two classes of long timescale units with different graph theoretical properties. The functionality of syntax sensitive units previously identified in the literature is confirmed. Finally, the analysis is replicated for a character level model.  The paper received detailed and insightful reviews, and there was a lively (but always respectful) discussion between authors and reviewers.  Overall, the reviewers liked the topic of the paper and the overall methodology, however they had several issues with it. One of the issue pertained to the "holistic" approach to time in the paper, which is measured in number of tokens, rather than in terms of syntactic distance. More in general, there was a feeling that the paper was somewhat short on actual insights on the exact functional role of units in a linguistic context. The reviewer who assigned the most severe score was mostly concerned about one specific instance of this, namely the fact that the authors focus on syntax tracking and number agreement units whose scope should not really extend across sentences. Moreover, the reviewer was surprised that the syntax tracking units maintain information across longer distances than the number agreement units, that should, by definition, keep track of long distance relations.  I am divided. I welcome work that focuses on novel qualitative and quantitative analyses of an existing model. I wished there were clearer take home messages on how LSTMs process language, but I recognize that our knowledge of deep learning models is very preliminary, and I am thus not surprised that the conclusions are not entirely clear.  The reviewers raised important concerns, but I would not confidently claim that we know enough about the relevant units to be genuinely surprised by some of the results. For example, can we really say that number agreement units are only limited to clause internal agreement tracking? Couldn t it be, say, that we will discover in the future they also play a role in tracking discourse determined pronominal number (going out on a random limb, here, of course)?  Overall, I would like to see this at least as a poster at the conference, but I am assigning low confidence to my recommendation as I respect the reviewers  point of view. 
This paper investigates the identifiability of attention distributions in the context of Transformer architectures. The main result is that, if the sentence length is long enough, difference choices of attention weights may result in the same contextual embeddings (i.e. the attention weights are not identifiable). A notion of "effective attention" is proposed that projects out the null space from attention weights.   In the discussion period, there were some doubts about the technical correctness of the identifiability result that were clarified by the authors. The attention matrix A results from a softmax transformation, therefore each of its rows is constrained to be in the probability simplex   i.e. we have A >  0 (elementwise) and A1   1. In the present version of the paper, when analyzing the null space of T (Eqs. 4 and 5) this constraint on A is not taken into account. In particular, in Eq. 5 the existence of a \tilde{A} in the null space of T is not clear at all, since for (A + \tilde{A})T   AT to hold we would need to require, besides A >  0 and A1   1, that A + \tilde{A} >  0 and A1 + \tilde{A}1   1, i.e.  \tilde{A} <  A (elementwise) \tilde{A}1   0  The present version of the paper does not make it clear that the intersection of the null space of T with these two constraints is non empty in general   which would be necessary for attention not to be identifiable, one of the main points of the paper.   The authors acknowledged this concern and provided a proof. I suggest the following simplified version of their proof:  We re looking for a vector \tilde{A} satisfying  (1) \tilde{A}’*T   0 (to be in the null space of T)  and   (2) \tilde{A}’*1   0  (3) \tilde{A} >   A  (to make sure A + \tilde{A} are in the probability simplex).  Conditions (1) and (2) are equivalent to require \tilde{A} to be in the null space of [T; 1]. It is fine to assume this null space exists for a general T (it will be a linear subspace of dimension ds   dv   1).   To take into account condition (3) here’s a simpler proof: since A is a probability vector coming from a softmax transformation (hence it is strictly > 0 elementwise), there is some epsilon > 0 such that any point in the ball centered on 0 with radius epsilon is >   A.   Since the null space of [T; 1] contains 0, any point \tilde{A} in the intersection of this null space with the epsilon ball above satisfies (1), (2), and (3). This should work for any ds   dv > 1  and as long as A is not a one hot distribution (otherwise it collapses to a single point \tilde{A}   0).  I am less convinced about the justification to use an “effective attention” which is not in the probability simplex, though (not even in the null space of [T; 1] but only null(T)). That part deserves more clarification in the paper.   I recommend acceptance of this paper provided these clarifications are provided and the proof is included in the final version.   
This paper proposes a new training approach for deep neural interfaces. The idea is to bootstrap from critics of other layers instead of using the final loss as target. The method is evaluated of CIFAR 10 and CIFAR 100 and found to improve performance slightly upon Sobolev training while being simpler. The reviewers found the idea interesting but were concerned about the strength of the experimental results. The datasets are similar and the significance of the results is not clear. The revision submitted by the authors was only able to address some of these issues such as the evaluation protocol.
In the end, this paper essentially proposes a minor variation on an idea that 1) has been published before, 2) is not used extensively at all, and 3) seems applicable (in its current form) only on deterministic environments.  This, without additional insights or analyses, seems too marginal a contribution for acceptance.  The paper is not poorly executed, and the authors engaged well during discussion, for which I would like to thank them.  I would like to encourage the authors to consider the reviewers comments, and in particularly perhaps answer more clearly and directly what they are adding to the literature.  It could be that there is something particularly insightful in the detailed differences with past work, but this has not become sufficiently clear to me during this discussion phase.
This paper presents an analysis on different methods of noise injection in adversarial examples, using gaussian noise for example. There are important issues raised by reviewers 1 & 2 about some conclusions not being well supported by the experiments and the utility/importance of some conclusions. After a discussion among reviewers, as of now all 3 reviewers stand by the decision that substantial improvements, and analysis can be made in the paper. Thus, Im recommending a Rejection.
This paper proposes a meta learning based few shot federated learning approach to reduce the communication overhead incurred in aggregating model updates. The use of meta learning also gives some generalization benefits. The reviewers think that the paper has the following main issues (see reviews for more details): * Limited technical novelty   the paper seems to simply combine meta learning with federated learning * Not clear whether the communication overhead is actually reduced because the meta learning phase can require significant communication and computation. * The experimental evaluation, in particular, the data distribution, could have been more realistic.  I hope that the authors can use the reviewers  feedback to improve the paper and resubmit to a future venue.  
This paper introduces a few ideas to potentially improve the performance of neural ODEs on graph networks.  However, the reviewers disagreed about the motivations for the proposed modifications.  Specifically, it s not clear that neural ODEs provide a more advantageous parameterization in this setting than standard discrete networks.  It s also not clear at all why the authors are discussion graph neural networks in particular, as all of their proposed changes would apply to all types of network.  Another major problem I had with this paper was the assertion that the running the original system backwards leads to large numerical error.  This is a plausible claim, but it was never verified.  It s extremely easy to check (e.g. by comparing the reconstructed initial state at t0 with the true original state at t0, or by comparing gradients computed by different methods).  It s also not clear if the authors enforced the constraints on their dynamics function needed to ensure that a unique solution exists in the first place.
The paper received scores of 5,5,5, with the reviewers agreeing the paper was marginally below the acceptance threshold. The main issue, raised by both R2 and R3 was that connection between representation learning in deep nets and coding theory was not fully justified/made.  With no reviewer advocating acceptance, it is not possible to accept the paper unfortunately. 
This paper presents an heuristic method to detect periodicity in a time series such that it can handle noise and multiple periods.   All reviewers agreed that this paper falls off the scope of ICLR since it does not discuss any learning related question. Moreover, the authors did not provide any response nor updated manuscript addressing the reviewers remarks. The AC thus recommends rejection. 
This paper studies the problem of synthesizing adversarial examples that will succeed at fooling a classification system under unknown viewpoint, lighting, etc conditions. For that purpose, the authors propose a data augmentation technique (called "EOT") that makes adversarial examples robust against a predetermined family of transformations.  Reviewers were mixed in their assessment of this work, on the one hand highlighting the potential practical applications, but on the other hand warning about weak comparisons with existing literature, as well as lack of discussion about how to improve the robustness of the deep neural net against that form of attacks. The AC thus believes this paper will greatly benefit from a further round of iteration/review, and therefore recommends rejection at this time. 
The paper investigates the effect of focal loss on calibration of neural nets.  On one hand, the reviewers agree that this paper is well written and the empirical results are interesting. On the other hand, the reviewers felt that there could be better evaluation of the effect of calibration on downstream tasks, and better justification for the choice of optimal gamma (e.g. on a simpler problem setup).  I encourage the others to revise the draft and resubmit to a different venue.   
The work studied the problem of inserting backdoor into a deployed model through bit flip.  Some important concerns have been proposed by reviewers, including: the incorrect claim of the treat model, the potential defenses are only discussed but not validated, experimental setups and analysis (e.g., the sensitivity test of hyper parameters). Although the authors provided some responses, but all reviewers are not well convinced.   After reading the manuscript, reviews and discussions between reviewers and authors, I think this work is not ready for publication. The reviewers  comments are supposed to be helpful to improve this work.
This paper was reviewed by 3 expert reviews. While they all see value in the new task and dataset, they raise concerns (templated language, unclear what exactly are the new challenges posed by this task and dataset, etc) that this AC agrees with. To be clear, the lack of a fundamentally new model is not a problem (or a requirement for every paper introducing a new task/dataset), but make a clear compelling case for why people should work on the task is a reasonable bar. We encourage the authors to incorporate reviewer feedback and invite to the workshop track. 
This paper was very well received by the reviewers with solid Accept ratings across the board. The subject matter is quite interesting    mathematical reasoning in latent space, and it was suggested by a reviewer that this could be a good candidate for an oral. The AC agrees and recommends acceptance as an oral. Some of the intuitions of what is being done in this paper could be better visualized and presented and I encourage the authors to think carefully about how to present this work if an oral presentation is granted by the PCs.
This work considers the problem of calibrating a multi class classifier while preserving differential privacy. It proposes a method Accuracy Temperature Scaling, that aims to achieve consistency rather than calibration. The method is particularly easy to implement under the constraint of DP. The paper then evaluates  the calibration algorithm in the context of domain perturbation/shift and, as the authors demonstrate it outperforms adaptations of other technques to DP.  The strong sides of this work are  * the first work to study calibration in this setting (albeit that is also a result of the setting being of a relatively narrow interest) * proposes a new algorithm * evaluation on multiple benchmarks  The weaknesses * The method is not justified either by theoretical analysis or clear intuition * Evaluation of performance in the context of domain shift makes the the presentation somewhat confusing and experiments much more involved but is largely orthogonal to the problem of calibration  Overall the work has merits but also significant issues.
This paper presents a very interesting investigation. While deep neural networks are typically best in non private settings, the authors show that linear models with handcrafted features (ScatterNets) perform better in certain settings of the privacy parameter. The reviewers all found this to be important and insightful, with a thorough investigation, and I tend to agree, recommending acceptance.
This paper presents a method for out of distribution detection under the condition of access to only a few positive labeled samples. The main contribution as summarized by reviewers and authors is the new proposed benchmark and problem statement.   All reviewers are in agreement that this paper is not ready for publication in its current form. The main concern is around the validity of the problem statement. The reviewers seek more clarity motivating the proposed scenario. Though the authors argue that as few shot recognition is very difficult and may benefit from strategies like active learning, it is not directly clear how out of distribution detection is the best approach. In addition, R3 seeks clarification on the similarity to existing work.   Considering the unanimous opinions of the reviewers and all author rebuttal text, the AC does not recommend acceptance of this work. We encourage the authors to focus their revisions on the explanation and motivation of this new benchmark and submit to a future venue.  
The authors introduce a modification to CQL to use a weighting based on density estimates. In an idealized setting, they show that the estimate Q values bound the true Q values. Finally, they evaluate their proposed approach on a few benchmark offline RL tasks.  Generally, all reviewers felt that the results were too incremental. The theoretical result follows with light modifications from the CQL paper and even then, the implications of the result are unclear. The experimental results showed small improvements or comparable performance while requiring training a density estimator and introducing an additional hyperparameter. Furthermore, the set of tasks evaluated was limited and no comparisons to other methods than CQL were shown.   While I appreciate the effort the authors took to investigate this improvement, at this time, the paper falls below the bar and I recommend rejection.
This paper proposes an elegant approach to object detection where an encoder network reads in an image and a decoder network outputs coordinate and category information via a sequence of textual tokens. This method does away with several object detection specific details and tricks such as region proposals and ROI pooling. The paper received positive reviews from all reviewers who agreed that this formulation of object detection was novel and provided a new perspective that may transfer to other computer vision tasks. One common concern amongst reviewers was the slow inference time due to the sequential nature of the decoder   and this concern was a central point of discussion between the authors and reviewers. My takeaway from this discussion is that this model is certainly slower than traditional computer vision models that can generate boxes in parallel. The slowdown however, is image dependent. Less cluttered environments require shorter output sequences. Moreover, such a model can easily be applied to concept localization, e.g. "Locate the horses", in which cases one can expect fewer objects of the desired category, and hence acceptable inference speeds. Importantly, the contributions of this paper are noteworthy in spite of the proposed architecture having the drawback of being slow. Given this, I recommend accepting this paper for its merits.
The authors propose zero shot recommendations, a scenario in which knowledge from a recommender system enables a second recommender system to provide recommendations in a new domain (i.e. new users & new items). The idea developed by the authors is to transfer knowledge through the item content information and the user behaviors.  The initial assessment of the reviewers indicated that this paper was likely not yet ready for publication. The reviewers all recognized the potential usefulness of zero shot recommendations but argued that the implications of the proposed setup were somewhat unclear. Most notably, the reviewers raised the issue of how widely applicable this was in terms of distance between source and target domains (presumably the quality of the zero shot recommendations depends on the distance).   The reviewers also noted that this was an application paper. This is of course within the CFP, and recommender systems papers have been published at ICLR in the past (for example one of the initial Session based RecSys paper w. RNNs) but the potential audience for this work is somewhat lower at ICLR. I should also add that I agree with the authors that their model is novel, but it s very much tailored to this application and it was unclear to me how it might be impactful on its own. All in all, this did not play a significant role in my recommendation.  During the discussion, there were significant, yet respectful, disagreements between the authors and the reviewers. It also seems like perhaps the authors missed an important reply from reviewer hJB8 made available through their updated review (see "Reply to rebuttal"). So the discussion between reviewers and authors did not converge. Having said that, even the two most positive reviewers have scores that would make this paper a very borderline one (a 6 and a 5).   Further, I do find that reviewer s hJB8 arguments have merit and require another round of review. In particular, I think the role and effect of your simulated online scenario should be further discussed (note that I did read the new paragraph on it from your latest manuscript). For example, comparing to a baseline that can train with the data from this new domain would be useful even if at some point it ends up being an upper bound on the performance of your approach. I also found the question raised by the reviewer around the MIND results to be pertinent. Further characterizing pairs of domains in which the approach/works fails (even if empirically) would add depth to this paper.   All in all, this paper has interesting ideas and I strongly encourage the authors to provide a more thorough experimental setup that fully explores the benefits and limitations of their zero shot approach.
This paper presents an extensive empirical study to sentence level pre training. The paper compares pre trained language models to other potential alternative pre training options, and concludes that while pre trained language models are generally stronger than other alternatives, the robustness and generality of the currently available method is less than ideal, at least with respect to ELMO based pretraining.   Pros: The paper presents an extensive empirical study that offers new insights on pre trained language models with respect to a variety of sentence level tasks.     Cons: The primarily contributions of this paper is empirical and technical novelty is relatively weak. Also, the insights are based just on ELMO, which may have a relatively weak empirical impact. The reviews were generally positive but marginally positive, which reflect that insights are interesting but not overwhelmingly interesting. None of these is a deal breaker per say, but the paper does not provide sufficiently strong novelty, whether based on insights or otherwise, relative to other papers being considered for acceptance.  Verdict: Leaning toward reject due to relatively weak novelty and empirical impact.  Additional note on the final decision:  The insights provided by the paper are valuable, thus the paper was originally recommended for an accept. However, during the calibration process across all areas, it became evident that we cannot accept all valuable papers, each presenting different types of hard work and novel contributions. Consequently, some papers with mostly positive (but marginally positive) reviews could not be included in the final cut, despite their unique values, hard work, and novel contributions. 
This paper proposes O RAAC, an offline RL algorithm that minimizes the Conditional Value at Risk (CVaR) of the learned policy s return given a dataset by a behavior policy. The reviews are generally positive with most agreeing that the paper presents interesting empirical results.   The experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with more clearer safety settings.  Agreeably, asking for comparisons with unpublished results may be unfair, however, it would be recommended to authors to include additional comparisons with latest methods in Offline/Batch RL, including the ones which don t guarantee risk, such as CQL, BRAC, CSC.  Further, The theoretical properties of the proposed algorithm are largely unclear. It would help to analyze the effect of both convergence rates, and fixed points, further what is the effect of addition of risk, does the algorithm converge to a suboptimal solution or get there slower. Finally empirical reporting of cumulative number of failures (discrete count) during training as well as during evaluation would be very useful to practitioners.   Other relevant and concurrent papers to potentially take note of: Distributional Reinforcement Learning for Risk Sensitive Policies (https://openreview.net/forum?id 19drPzGV691) Conservative Safety Critics for Exploration (https://openreview.net/forum?id iaO86DUuKi)  I would recommend acceptance of the paper. I would strong encourage release of sufficiently documented and easy to use implementation.  Given the fact that the main argument is empirical utility of the method, it would be limit the impact of this work if readers cannot readily build on O RAAC.  
The reviewers of the paper are not very enthusiastic of the new model proposed, nor are they very happy with the experiments presented.   It is unclear from both the POS tagging and dependency parsing results where they stand with respect to state of the art methods that do not use RNNs.  We understand that the idea is to compare various RNN architectures, but it is surprising that the authors do not show any comparisons with other methods in the literature.  The idea of truncating sequences beyond a certain length is also a really strange choice.  Addressing the concerns of the reviewers will lead to a much stronger paper in the future.
This paper concerns the problem of defending against generative "attacks": that is, falsification of data for malicious purposes through the use of synthesized data based on "leaked" samples of real data. The paper casts the problem formally and assesses the problem of authentication in terms of the sample complexity at test time and the sample budget of the attacker. The authors prove a Nash equillibrium exists, derive a closed form for the special case of multivariate Gaussian data, and propose an algorithm called GAN in the Middle leveraging the developed principles, showing an implementation to perform better than authentication baselines and suggesting other applications.  Reviewers were overall very positive, in agreement that the problem addressed is important and the contribution made is significant. Most criticisms were superficial. This is a dense piece of work, and presentation could still be improved. However this is clearly a significant piece of work addressing a problem of increasing importance, and is worthy of acceptance.
The paper focuses on adversarial attacks for RL, which is an exciting understudied research direction, and can be of interest to the community. All the reviewers are (mildly) positive about the paper and the author competently replied to the concerns expressed by the reviewers. 
This paper proposes an alternating dialog model based on transformers and GPT 2, that model each conversation side separately and aim to eliminate human supervision. Results on two dialog corpora are either better than or comparable to state of the art. Two of the reviewers raise concerns about the novel contributions of the paper, and did not change their scores after authors  rebuttal. Furthermore, one reviewer raises concerns about the lack of detailed experiments aiming to explain where the improvements come from. Hence,  I suggest rejecting the paper.
The paper creates a dataset for exploration of RL for molecular design and I think this makes it a strong contribution to the community at the intersection of the two. For a methods focussed conference such as ICLR however, it may not be the best fit. Hence I would recommend submitting to a workshop track or targeting a more focussed venue such as a bioinformatics conference. 
The paper proposed a waveform to waveform music source separation system. Experimental justification shows the proposed model achieved the best SDR among all the existing waveform to waveform models, and obtained similar performance to spectrogram based ones. The paper is clearly written and the experimental evaluation and ablation study are thorough. But the main concern is the limited novelty, it is an improvement over the existing Wave U Net, it added some changes to the existing model architecture for better modeling the waveform data and compared masking vs. synthesis for music source separation.  
This paper studies physical "adversarial programs" that allow an attacker to control a machine learning model by placing transparent patches on top of an image. The reviewers are split on this paper: while some reviewers like the work, others are concerned about the practicality, novelty, or utility of the attack.  Starting with novelty, reviewers raise valid concerns about how this approach is similar to prior attacks that generate programs. The authors respond here, but the overall question remains unanswered and it is not clear which of the new pieces this paper introduces are responsible for the success. (Would prior techniques have sufficed? If not what part of prior methods makes this not the case?)  For utility, the paper does not make a clear case of why it would be easier for an adversary to place N~ 5 patches on top of an image as compared to other physical attacks (see especially Li et al. 2019 as a paper that deserves more than a sentence of comparison why is this approach easier?).  One final comment raised by many reviewers is the fact that the title and setup to this paper heavily lean on the "physical" component of the evaluation, and yet the paper does not demonstrate anything physical. The authors rebuttal that the word "towards" absolves them of responsibility for trying an attack in the real world does not convince me; either the paper should attempt this attack in the physical world (and say if it works or if it doesn t) or make it clear from the top that the attack is going to be digital from the start, but motivated by the physical world. Prior accepted papers that include physical world in the title (e.g., Kurakin et al., Athalye et al., Li et al.) don t solve the problem completely, but at least run experiments in the physical world.
The paper s main message is that some existing NLP techniques that claim to improve performance by the use of a knowledge graph may not achieve this improved performance because of the knowledge graph or at least the explanation given may be questionable.  This is thought provoking and it will incite the community to think more carefully about the real factors of improved performance.  The initial version of the paper was not well written, but the authors improved the writing significantly.  The paper includes a thorough empirical evaluation to support the main message.  I have read the paper and I believe that this work will be of interest to a diverse audience.
The manuscript proposes a simple technique for adaptive ensemble prediction. Unfortunately, several significant concerns were raised (by R2 and R3) that this AC agrees with. Both R2 and R3 asked fairly specific questions and requested follow up experiments, which have not been addressed. 
This paper introduces an FFT based loss function to enforce physical constraints in a CNN based PDE solver.  The proposed idea seems sensible, but the reviewers agreed that not enough attention was paid to baseline alternatives, and that a single example problem was not enough to understand the pros and cons of this method.
This paper proposes augmenting standard forward prediction techniques used for representation learning with backward prediction as well, termed "learning via retracing". The paper implements this idea in a Cycle Consistency World Model (CCWM) and demonstrates that CCWM improves performance of a Dreamer agent across a number of Control Suite tasks. The paper also proposes a way to detect "irreversible" transitions and exclude them from the backwards prediction step.  This paper generated mixed opinions, and the reviewers did not come to a consensus on whether it should be accepted or rejected. In particular, Reviewer VSAG maintained it should be accepted, while Reviewer NEVM maintained it should be rejected. The other reviewers did not reply; I thought the authors  responses to their questions were reasonable so I assume their concerns were addressed (additionally, I don t believe a comparison to PlayVirtual is a justifiable request per the [reviewing guidelines](https://iclr.cc/Conferences/2022/ReviewerGuide), as it is concurrent work).  The reviewers generally agreed that the cycle consistency idea proposed by the paper is interesting, well motivated, and borne out by the experimental results. I agree with these points. The main weakness of the paper, brought up by multiple reviewers, was the justification/motivation for the method for irreversibility detection. The authors clarified in the rebuttal that the motivation comes from the idea that temporally adjacent states with very different values will tend to be far apart in representation space. While I believe that is true, it s not at all clear to me that this necessarily *entails* irreversible transitions between those states. That said, my feeling is that this is approach is (1) not the main contribution of the paper and (2) empirically seems to work, based on the experiments, even if the motivation is unclear/unjustified. Therefore, I do not think the concern about irreversibility detection is grounds on its own for rejection.  Overall, I find this is a sensible approach to better representation learning in MBRL. I recommend acceptance as a poster.
The authors set out on an important question of whether abstract and culturally specific concepts like offensiveness can be detected in images. The novelty of this work comes in part from tackling this question and attempting to create a technology which can operationalize it.  However, despite the authors  insistence that offensiveness is not "just another label", in practice the work treats it very much that way and therefore does not present a compelling innovation either in modeling or in juxtaposition to other labeling tasks.  Known training and inspection techniques are used on existing representations and more powerful models with more training data generalize better. It is unclear what is novel in the approach or unique to offensiveness over other labels (including abstract ones).
The authors present a method to learn the expected number of time steps to reach any given state from any other state in a reinforcement learning setting.  They show that these so called dynamical distances can be used to increase learning efficiency by helping to shape reward.  After some initial discussion, the reviewers had concerns about the applicability of this method to continuing problems without a clear goal state, learning issues due to the dependence of distance estimates on policy (and vice versa), experimental thoroughness, and a variety of smaller technical issues.  While some of these were resolved, the largest outstanding issue is whether the proper comparisons were made to existing work other than DIAYN.  The authors appear to agree that additional baselines would benefit the paper, but are uncertain whether this can occur in time.  Nonetheless, after discussion the reviewers all appeared to agree on the merit of the core idea, though I strongly encourage the authors to address as many technical and baseline issues as possible before the camera ready deadline.  In summary, I recommend this paper for acceptance.
This paper aims for detecting not only clean OOD data, but also their adversarially manipulated ones. The authors propose a method for this goal, with no/marginal loss in clean test accuracy (say, Acc) and clean OOD detection accuracy (say, AUC), while existing methods for targeting the same goal suffers from low Acc and AUC. 3 reviewers are positive and 2 reviewers are negative. Reviewers and AC think that the proposed idea of merging a certified binary classifier for in versus out distribution with a classifier for the in distribution task is interesting. However, AC thinks that experimental results are arguable as pointed out by reviewers. For example, in CIFAR 10, the proposed method outperforms the baseline (GOOD) with respect to Acc and AUC, but often significantly underperforms it with respect to GAUC (guaranteed AUC) or AAUC (adversarial AUC). Then, the question is which metric is more important? It is arguable to say whether Acc is more important than GAUC or AAUC. But, at least, AC thinks that AUC and AAUC (or GAUC) are equally important as adversarially manipulated OOD data is nothing but another OOD data made from the original clean OOD data. Hence, the superiority of the proposed method over the baseline is arguable in the experiments, and AC tends to suggest rejection.   ps ... AC is also a bit skeptical on the motivation of this paper. What is the value of obtaining "guaranteed AUC"? It is not the "real/true" worst case OOD performance, as it varies with respect to the tested clean OOD data. Namely, it is the worst case OOD performance just in a certain "subset" of OOD data, i.e., adversarially manipulated OOD data made from a certain clean OOD data. Hence, AC is curious about what is the value of establishing such a "partial" lower bound (rather than "true" lower bound considering all possible OOD data). AC thinks that the problem setup studied in this paper (and some previous papers) looks interesting/reasonable at the first glance, but feels somewhat artificial after a deeper look.
The authors propose a new dataset, ChemistryQA which has complex questions requiring scientific and mathematical reasoning. They show that existing SOTA models do not perform well on this dataset thereby establishing the complexity of the dataset.   The reviewers raised several concerns as summarised below:  1) Writing is not very clear 2) The quality of the dataset is hard to judge as some crucial information about the dataset creation process is missing  3) The size of the dataset is small 4) some stronger QA baselines need to be included  Unfortunately the authors did not provide a rebuttal. Hence, its current form this paper cannot be accepted. 
This paper uses unsupervised learning to create useful representations to improve the performance of models in predicting protein ligand binding. After reviewers had time to consider each other s comments, there was consensus that the current work is too lacking in novelty on the modeling side to warrant publication in ICLR. Additionally, current experiments are lacking comparisons with important baselines. The work in its current form may be better suited for a domain journal. 
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with the reviewers that the paper has limited novelty as there are already previous studies on setting floating point configurations. Additionally, the particular hardware setting that the authors provide seems to rely on a fp32 FMA, which defeats the purpose of a low bit floating point(where smaller FMA could have been used).Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.  
The paper proposes a new way to tackle the trade off between disentanglement and reconstruction, by training a teacher autoencoder that learns to disentangle, then distilling into a student model. The distillation is encouraged with a loss term that constrains the Jacobian in an interesting way. The qualitative results with image manipulation are interesting and the general idea seems to be well liked by the reviewers (and myself).  The main weaknesses of the paper seem to be in the evaluation. Disentanglement is not exactly easy to measure as such. But overall the various ablation studies do show that the Jacobian regularization term improves meaningfully over Fader nets. Given the quality of the results and the fact that this work moves the needle in an important (albeit hard to define) area of learning disentangled representations, I think would be a good piece of work to present at ICLR so I recommend acceptance.
The paper proposes a multi scale network that uses DEQ models to incorporate samples at multiple resolutions. The authors also propose a training strategy to improve the performance of the model. The authors investigate the interest of the approach through ablation and explainability, weighing the value of hierarchical heritage, diversity modules, perturbation size, and regularization penalties.   The reviewers appreciated that the authors tackled the problem of incorporating multiple scales and the “impressive results” on CIFAR 10, CIFAR 100. The reviewers also expressed concerns regarding the computational assessment, in particular the additional computational/memory overhead of unrolling and what the authors mean by “explainability” in their experimental evaluation. The reviewers also made suggestions to organize the paper better.   The authors submitted responses to the reviewers  comments. After reading the response, updating the reviews, and discussion, the reviewers who took part in the discussion considered that they are “satisfied by the response” and the “major concerns have been addressed”.  The feedback provided was already fruitful and the final version should be already improved. The ablative analysis and comparison to baseline is careful and thorough.  Accept. Poster.
This paper suggests a new technique to utilize generative replay for continual learning. Specifically, the authors claim that even though the generated samples are imperfect (thus cannot be used as positive samples for old classes), they can still be used as negative samples for the current class. 3 reviewers are negative and 1 reviewer is positive. The main concerns of negative reviewers are (a) non ablated effects of baseline and proposed components, (b) insufficient analysis of negative replay, and (c) no assessment of generated data quality. The rebuttal provides an additional experiment to address the issue (a), but the reviewers and AC think the experiments should be better polished. Also, AC believes the issues (b) and (c) should be better analyzed. The rebuttal claims that issue (c) is not applicable as they generate samples on the latent space. However, the main motivation of the paper is the low quality of generated samples, and the paper should provide a quality measure to support their claim. For example, an update of the feature extractor may move the latent space generative replay to the wrong class (i.e., low quality), and thus one should not use it as positive but only as negative, as suggested in this paper. Here, the negative replay would increase the margin of current and old classes, enhancing the accuracy of the current class. To analyze the source of benefits (old vs. current classes), the authors could report the task wise accuracy trends, not only the overall accuracy. It would be a nice addition to the issue (b). Due to these unresolved concerns, AC tends to recommend rejection.
In this paper the authors view meta learning under a general, less studied viewpoint, which does not make the typical assumption that task segmentation is provided. In this context, change point analysis is used as a tool to complement meta learning in this expanded domain.    The expansion of meta learning in this more general and often more practical context is significant and the paper is generally well written. However, considering this particular (non)segmentation setting is not an entirely novel idea; for example the reviewers have already pointed out [1] (which the authors agreed to discuss), but also [2] is another relevant work. The authors are highly encouraged to incorporate results, or at least a discussion, with respect to at least [2]. It seems likely that inferring boundaries could be more powerful, but it is important to better motivate this for a final paper.    Moreover, the paper could be strengthened by significantly expanding the discussion about practical usefulness of the approach. R3 provides a suggestion towards this direction, that is, to explore the performance in a situation where task segmentation is truly unavailable.    [1] Rahaf et el. "Task Free Continual Learning".  [2] Riemer et al. "Learning to learn without forgetting by maximizing transfer and minimizing interference".   
This paper makes a thorough investigation on the idea of decoupling the input and output word embeddings for pre trained language models.  The research shows that the decoupling can improve the performance of pre trained LMs by reallocating the input word embedding parameters to the Transformer layers, while further improvements can be obtained by increasing the output embedding size.  Experiments were conducted on the XTREME benchmark over a strong mBERT.  R1&R2&R3 gave rather positive comments while R4 raised concerns on the model size.  The authors gave detailed response on these concerns but R4 still thought the paper is overclaimed because the experiments were only conducted in a multilingual scenario.  
Clarity: The paper is well written with illustrative figures.  Originality: The originality of the paper is relatively restricted, mainly due to the resemblance with the work [1]. However, there are important differences, that the authors nicely pointed out, and we encourage them to include these in the final version of the paper.  Significance: The paper points out a relevant issue in using normalization techniques such as batch normalization together with momentum based optimization algorithms in training deep neural networks. While the paper could be considered "another algorithms for training NNs", the papers illustrates nicely the main arguments, and is backed up with more than sufficient experimental results.  Main pros:   In the main pros, AC and reviewers admit the phenomenal job in responding to reviewers  questions and requests   The paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method.   After the reviews, The authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of SGDP and AdamP. Finally, they also added the experiments with higher weight decay, showing that indeed 1e 4 was the best value.  Main cons:   One reviewer requires more explanation why the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum based update in equation (8).
Wide agreement from the reviewers.  Interesting theorems.  Empirical work illustrates the theory. Claim and insight: failure of VAEs is caused by the inherent limitations of ELBO learning with inflexible encoder distribution. Good discussion pointed out related work and insights from the experiments.
The reviewers’ main concern was a lack of experiments, and additional experiments were provided by the authors.  While the rebuttal was not addressed by the reviewers, the AC feels that the rebuttal did address a number of experimental concerns well enough to justify accepting this paper.
The paper presents a method for unsupervised/semi supervised clustering, combining adversarial learning and the Mixture of Gaussians model. The authors follow the methodology of ALI, extending the Q and P models with discrete variables, in such a way that the latent space in the P model comprises a mixture of Gaussians model.   The problem of generative modeling and semi supervised learning are interesting topics for the ICLR community.  The reviewers think that the novelty of the method is unclear. The technique appears to be a mix of various pre existing techniques, combined with a novel choice of model. The experimental results are somewhat promising, and it is encouraging to see that good generative model results are consistent with improved semi supervised classification results. The paper seems to rely heavily on empirical results, but they are difficult to verify without published source code. The datasets chosen for experimental validation are also quite limited, making it it difficult to assess the strengths of the proposed method.
The paper considers the global convergence and stability of SGD for non convex setting. The main contribution of the work seems to be to remove uniform bounded assumption on the noise, and to relax the global Holder assumption typically made. Their discussions in Appendix A provide an example for which the uniform bounded assumption on the noise commonly assumed in the literature fails.  The authors establish that SGD’s iterates will either globally converge to a stationary point or diverge  and hence tehir result exclude limit cycle or oscillation. Under a more restrictive assumption on the joint behavior of the non convexity and noise model they also show that the objective function cannot diverge, even if the iterates diverge.  The reviewers are on the fence with this paper. While they agree that the paper is interesting, they only give it a score of weak accept (subsequent to rebuttal as well). One of the qualms is that while the authors claim the result helps show success of SGD in more natural non convex problems, they don’t provide realistic examples supporting their claim. Further, while the extension to holder smoothness assumption while is indeed interesting, unless practical significance is shown via examples, the result is not that exciting.  From my point of view and reading, while the reviews are not extensive, i do not disagree with reviewers sentiment. Technically the paper is strong but there is a unanimous lack of strong excitement for the paper amongst reviewers. While there is this lack of more enthusiasm, given the number of strong submissions this year, I am tending towards a reject.
Four knowledgeable referees lean towards rejection because of the missing detailed complexity analysis [R1,R2,R3], the choice of rather small datasets which hinders the rigorous evaluation of GNN models [R3,R4], missing state of the art comparisons [R2] and ablations [R4]. The rebuttal addressed some of the concerns raised by the reviewers, in particular, clarifications request by R2, smoothness of the weights questions of R4, and the difference in performance of the baseline methods of R1. However, after discussion, the reviewers are still concerned with the missing ablations, comparisons, and complexity analysis. I agree with their assessment and therefore must reject. However, I agree with the reviewers that this is an interesting approach and encourage the authors to consider the reviewer s suggestions for future iterations of their work. 
The paper provides a new way of weighting data to build weighted estimators of causal effects (which themselves can be used in other contexts, e.g. doubly robust estimators). It s novel in the sense that it optimizes the choice of weighting based on information about the response function space. The approach is simple to implement, and opens up other possibilities for different classes of estimators.  I liked it. I think the paper is nearly there in terms of a well rounded contribution. But I have to say that I did share the concern about the choice of random response functions. It s not only a matter of function space (everybody wants the most flexible one), but also of the random measure that goes in it   so the more flexible the random space, the least understood (to me at least) is the influence of the random measure. Surely that are choices of function space distributions that can do worse than uniform weights for some classes of problems? It s not that it s a implausible starting point (Bayesians do it all the time in terms of prior distribution, on top of a full blown likelihood function that is more often than not just a big nuisance parameter), but I think the paper covers this aspect just too lightly. I think it s of benefit to the authors to release a published version of this paper once they have some more formal guidance or a more complex experimental setup providing a more thorough insight of it. I do think the contribution is really promising, but it feels unfinished, and I d be curious to see where it could go following this direction.
The paper presents an analysis of the spectral impact of non linearities in a neural network, using harmonic distortion analysis as a means to quantify the effect they have in the spectral domain, linking a blue shift phenomenon to architectural choices. This is an interesting analysis, that could be strengthened by a more thorough exploration of how this analysis relates to other properties, such as generalization, as well as through the impact of the blueshift effect through the training process.
This paper extends Bootstrap DQN with multi step TD target. The initial submission had missing details, communication problems, and results lacking rigor. The authors made a clear effort to address the reviewers concerns.  This paper s contribution is supported primarily by the empirical results which need major work. The lack of statistical significance in the key results is a major problem. The new 5 run results (originally only 3 runs) shows no clear evidence of improving over the baseline. Additionally one must either justify the use of such few runs by investigating the distributions and using the proper statistical tools (Colas et al [2]) or simply do more runs. Regardless statistical significance in the precise sense is a requirement.  In addition other adjustments to the paper would strengthen it significantly: (1) The qualitative results like state visitations can be interpreted either in favour of the method or not, this could be improved with discussion or omitted see [1]; (2) the discussion of heterogeneity was informal; (3) discussion of the impact and sensitivity of hyper parameters should be included this includes addressing the concern that the performance of the baseline was as strong as it could be; (4) the current results do clearly separate if the improvement in performance (if it can be shown to be significant) is due to improvements in the rep via auxiliary task effect or the multi step return the reviewer has made a nice suggestion for an experiment here.  In summary, the reviewers did not find the text and examples in the paper convincing as to why the proposed method should be better than bootstrap q, and the results are not significant and need more work.   references that may be helpful: [1] https://openreview.net/forum?id rkl3m1BFDB&utm_campaign RL%20Weekly&utm_medium email&utm_source Revue%20newsletter [2] https://arxiv.org/abs/1806.08295
The authors demonstrate how neural networks can be used to learn vectorial representations of a set of items given only triplet comparisons among those items.  The reviewers had some concerns regarding the scale of the experiments and strength of the conclusions:  empirically, it seemed like there should be more truly large scale experiments considering that this is a selling point; there should have been more analysis and/or discussion of why/how the neural networks help; and the claim that deep networks are approximately solving an NP hard problem seemed unimportant as they are routinely used for this purpose in ML problems.  With a combination of improved experiments and revised discussion/analysis, I believe a revised version of this paper could make a good submission to a future conference.
This paper proposes a refinement, and analysis of, continuous time inference schemes.  This paper got in depth criticism from some very thoughtful and expert reviewers, and the authors seem to have taken it to heart.  I m still worried about the similarity to GRU ODE Bayes, but I feel that the clarifications to the general theory of continuous time belief updates is a worthy contribution, and the method proposed is a practical one.  One reviewer didn t update their score, but the other reviewers put a lot of thought into the discussion and also raised their scores.  I do think the title and name of the method is a bit misleading   I would call it something like "Consistent continuous time filtering", because the jump ODE is really describing beliefs about an SDE.
The proposed approach for evaluating reward functions is theoretically grounded while having several properties appealing to practical RL tasks. This novel approach fills a gap in the literature. All reviewers agree that this paper has a place at ICLR.
All reviewers suggested acceptance of the paper based on that the paper addresses an important problem and presents and validates interesting ideas for approaching it. Therea are some concerns regarding limited experiments   I d like to encourage the authors to make an effort to address these concerns and also a few others raised in the reviews in the final version of their paper. The authors already made several updates to their paper in that regard during the discussion phase so I believe that the paper would be an interesting conttribution to the conference and I am recommending acceptance of the paper.
The paper presents a defense against the gradient sign flip attacks on federated learning. The proposed method is novel, technically sound and well evaluated. The crucial issue of the paper is, however, that this defense is specific to gradient flip attacks. The authors show the robustness of their method against white box attacks adhering to this threat model and claim that "an adaptive white box attacker with access to all internals of TESSERACT, including dynamically determined threshold parameters, cannot bypass its defense". The latter statement does not seem to be well justified, and following the extensive discussion of the paper, the reviewers were still not convinced that the proposed method is secure by its design. The AC therefore feel that the specific arguments of the paper should be revised   or the claim of robustness further substantiated   in order for the paper to be accepted.    Furthermore, as a comment related to ethical consideration, the AC remarks that the paper s acronym, Tesseract, is used by an open source OCR software (https://tesseract ocr.github.io/) as well as in a recent paper: Pendlebury et al., TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time, USENIX Security 2019.  All of the above mentioned reservations essentially add up to a "major revision" recommendation which, given the decision logic of ACLR, translates into the rejection option.
The paper studies a novel problem setting of automatically grading interactive programming exercises. Grading such interactive programs is challenging because they require dynamic user inputs. The paper s main strengths lie in formally introducing this problem, proposing an initial solution using reinforcement learning, and curating a large dataset from code.org. All reviewers generally appreciated the importance of the research problem studied and the potential of the work. Even though the reviewers found the work interesting, there was a clear consensus that the work is still immature and not yet ready for publication. I appreciate the authors  engagement with the reviewers during the discussion phase. Overall, the reviewers have provided very detailed and constructive feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper. 
Strengths  The paper introduces a promising and novel idea, i.e., regularizing RL via an informationally asymmetric default policy  The paper is well written.  It has solid and extensive experimental results.  Weaknesses   There is a lack of benefit on dense reward problems as a limitation, which the authors further acknowledge as a limitation. There also some similarities to HRL approaches.  A lack of theoretical results is also suggested. To be fair, the paper makes a number of connections with various bits of theory, although it perhaps does not directly result in any new theoretical analysis. A concern of one reviewer is the need for extensive compute, and making comparisons to stronger (maxent) baselines. The authors provide a convincing reply on these issues.  Points of Contention  While the scores are non uniform (7,7,5), the most critical review, R1(5), is in fact quite positive on many aspects of the paper, i.e., "this paper would have good impact in coming up with new  learning algorithms which are inspired from cognitive science literature as well as mathematically grounded." The specific critiques of R1 were covered in detail by the authors.  Overall  The paper presents a novel and fairly intuitive idea, with very solid experimental results.   While the methods has theoretical results, the results themselves are more experimental than theoretic. The reviewers are largely enthused about the paper.  The AC recommends acceptance as a poster. 
All of the reviewers believe the paper should not be accepted, and I concur with their recommendation for the reasons they mention.  Four of the reviewers (vEBH, idrP, KoFV, 5k4c) believe the technique proposed in this paper is not particularly novel. Rather, the novelty is that it is being used on a BERT model rather than the computer vision models that are typically the starting point for pruning work. They also argue that the paper was not particularly thorough in its comparison to other pruning techniques (specifically dynamic sparsity techniques), which is essential for pruning work given how crowded and noisy the space is. Finally, they rightfully note that the paper does not look at the real world speedups attainable on conventional hardware (GPUs and TPUs), the latter of which has no support for sparsity and the former of which (NVIDIA Ampere) has limited support for specific kinds of sparsity and especially limited support for sparse training.  The reviewers also raised several more specific methodological issues with evaluation (e.g., using the MLM loss rather than fine tuning as a basis for evaluation), but the above concerns alone were enough to convince me that the paper does not merit acceptance at this time.
The paper considers the problem of knowledge distillation from a few samples. The proposed solution is to align feature representations of the student network with the teacher by adding 1x1 convolutions to each student block, and learning only the parameters of those layers. As noted by Reviewers 1 and 2, the performance of the proposed method is rather poor in absolute terms, and the use case considered (distillation from a few samples) is not motivated well enough. Reviewers also note the method is quite simplistic and incremental.
This is an interesting contribution that sheds some light on a well studied but still poorly understood problem. I think it might be of interest to the community.
The paper considers the problem of black box optimization and proposes a discrete MBO framework using piecewise linear neural networks as surrogate models and mixed integer linear programming. The reviewers generally agree that the paper suggests an interesting approach but they also raised several concerns in their initial reviews. The response from the authors addressed a number of these concerns, for instance regarding scalability and expressivity of the model. However, some of these concerns remained after the discussion period, including doubts about the usefulness for typical applications in discrete black box optimization and some concerns about the balance between exploration and exploration.   Overall the paper falls below the acceptance bar for now but the direction taken by the authors has some potential. I encourage the authors to address the problems discussed in the reviews before resubmitting.
The proposed method is compressing video sequences with an end to end approach, by extending a variational approach from images to videos. The problem setting is interesting and somewhat novel. The main limitation, as exposed by the reviewers, is that evaluation was done on very limited and small domains. It is not at all clear that this method scales well to non toy domains or that it is possible in fact to get good results with an extension of this method beyond small scale content. There were some concerns about unfair comparisons to classical codes that were optimized for longer sequences (and I share those concerns, though they are somewhat alleviated in the rebuttal).  While the paper presents an interesting line of work, the reviewers did present a number of issues that make it hard to recommend it for acceptance. However, as R1 points out, most of the problems are fixable and I would advise the authors to  take the suggested improvements (especially anything related to modeling longer sequences) and once they are incorporated this will be a much stronger submission.
While all reviewers applaud the motivation to bridge the gap between machine learning and bioinformatics communities, they also raise a number of concerns regarding the choice of tasks and of baselines, and about the accuracy in their description. They feel the paper is not ready to be published in its current form, and we hope that their comments will help the authors prepare a revised version for the future.
This paper aims at improving the data efficiency of pretraining in CLIP. This is a practically meaningful research direction. The proposed method is simple, even kind of straightforward and has limited innovations. It combines self supervision within each modality, multi view supervision across modalities, and nearest neighbor supervision from other similar pairs. Such a combination showed strong empirical results: achieved better performance using seven times fewer data. The rebuttals resolved most critical concerns on experiments, such as fair comparisons with the original CLIP work.
The authors address the problem of self supervised monocular depth estimation via training with only monocular videos. They propose to use additional information extracted from semantic segmentation at training time to (i) provide additional “semantic context” supervision and (ii) to improve depth estimation at discontinuities through an edge guided point sampling based approach. Results are presented on the KITTI and Cityscapes datasets.    One of the main concerns is related to the utility of the semantic supervision given the relative cost required to obtain semantic training data in the first place. The authors state that "the pixel wise local depth information can not be well represented by current depth network". However, Guizilini 2020a can generate detailed depth edges and they do NOT require any semantic information during training. The authors also state that "the required labeled semantic dataset only accounts for a very tiny proportion, which indicates a relatively lower cost." This is a bit misleading. The proposed method uses per pixel semantic ground truth from three datasets (Mapillary Vistas, Cityscapes, and KITTI). It takes a lot of effort to provide this ground truth compared to self supervised methods which do not require any ground truth depth. It is encouraging that dataset specific semantic finetuning does not seem to have a large impact (Table 3), but this still requires access to a large enough initial semantic training set. Finally, the quantitative results are not much better than methods that don t require any semantics e.g. Guizilini 2020a, Johnston and Carneiro. Clearly, methods that do not require semantics are much more scalable, especially when adapting to new types of scenes.    Regarding the specific contributions of the paper, the SEEM module is the most novel component of the model. However, the addition of the SEEM module does not improve quantitative performance by much (see Table 2). In addition, the qualitative improvement it provides is also very subtle. This can be seen by comparing the last two rows of Fig 7 i.e. without and with. The authors need to make a stronger case, either quantitatively or qualitatively, as to why this is valuable.    Finally, but only a minor concern, the following relevant reference is missing: Jiao et al. Look Deeper into Depth: Monocular Depth Estimation with Semantic Booster and Attention Driven Loss, ECCV 2018   In conclusion, there were mixed views from the reviewers   with some supportive of the paper (R2&3) others not as enthusiastic (R1&4). The authors should be commended for their detailed responses and changes already made based on reviewer comments and suggestions. Unfortunately, this did not change the mind of the reviewers. It is the opinion of this AC that there is still more work required to fully show the utility of the proposed approach, especially considering the non trivial effort that is required to obtain semantic supervision in novel domains.  
Pros:   interesting algorithmic idea for using successor features to propagate uncertainty for use in epxloration   clarity  Cons:   moderate novelty   initially only simplistic experiments (later complemented with Atari results)   initially missing baseline comparisons   no regret based analysis   questionable soundness because uncertainty is not guaranteed to go down  All the reviewers found the initial submission to be insufficient for acceptance, and the one reviewer who read the rebuttal/revision did not change their mind, despite the addition of some large scale results (Atari).
The paper proposes Virtual MCTS, an early termination rule for MCTS to improve its efficiency.  The basic idea is to introduce a termination rule that prunes the search process when the final policy at the root node is unlikely to change from the current one. The proposed approach is empirically evaluated on 9x9 Go and Atari games.  After reading the authors  feedback, all reviewers participated in the discussion without reaching a consensus. Although all reviewers appreciated the authors  answers to their concerns, only one reviewer voted for acceptance. The other two reviewers,  while acknowledging some merits, still have concerns: the technical contribution is minor, the theoretical findings are quite trivial, it is unclear when the proposed termination strategy is could help. In summary, this paper is borderline and I think it still needs some work to clearly break the bar of a top conference.
This is a difficult borderline decision, with the reviewers evenly split in their final recommendation.  Overall, the authors provided good responses to the reviewer questions: this was much appreciated.  The reviewers requested additional ablations and explanations, which the authors provided.  A prevailing concern is that the experimental evaluation, restricted to a few standard MuJoCo environments, does not really demonstrated a distinctive advantage for the proposed approach.  In fact, one of the new ablations added raises concerns about the significance of the paper s main technical contributions: the \lambda_f 0 row added to Table 3 shows very strong reward results, which apparently obviates a key aspect of the proposed approach.   This work is interesting, and would like to see it published, but the current state of the evaluation does not support the significance of the main contribution.  I think the authors need to expand their empirical evaluation, as suggested, to better highlight the effectiveness of the proposed approach over the \lambda_f 0 baseline.  In the end, I think the authors would be better served by broadening the evaluation, isolating scenarios where the key proposal shows significant (rather than marginal) benefits, and publishing a more compelling version.
This paper presents SimVLM, a simpler generative VLP framework with billion scale web image text data, which has good zero shot potential while remaining competitive on standard VL benchmarks. SimVLM achieves SotA on several tasks and shows promising zero shot capacity in certain tasks. Most of the reviewers liked the work; they had concerns about data scaling, but the authors showed that SimVLM_small with the smaller cc3m dataset does not drop performance too much (although the large data scaling with their large scale weakly aligned data is still important to achieve good zero shot learning). All reviewers also mentioned the strong concern of reproducibility and data accessibility, so we encourage the authors to address this as clearly as possible via releasing models and safely cleaned/anonymized data subsets, etc.
This paper considers the problem of learning symbolic representations from raw data. The reviewers are split on the importance of the paper. The main argument in favor of acceptance is that bridges neural and symbolic approaches in the reinforcement learning problem domain, whereas most previous work that have attempted to bridge this gap have been in inverse graphics or physical dynamics settings. Hence, it makes for a contribution that is relevant to the ICLR community. The main downside is that the paper does not provide particularly surprising insights, and could become much stronger with more complex experimental domains. It seems like the benefits slightly outweigh the weaknesses. Hence, I recommend accept.
The authors propose training free neural architecture search using two theoretically inspired heuristics: the condition number of the Neural Tangent Kernel (to measure "trainability" of the architecture), and the number of linear regions in the input space (to measure "expressivity"). These two heuristics are negatively and positively correlated with test accuracy, respectively, allowing for fast, training free Neural Architecture Search. It is certainly not the first training free NAS proposal, but achieves competitive results with much more expensive NAS methods.  A few reviewers mentioned limited novelty of the method, a claim with which I agree. The contribution of the paper, however, is something different than how it was presented. The core message seems to be that the two proposed heuristics can greatly speed up NAS, and should be a baseline method against which more expensive methods should test.  I feel like this is a borderline paper, but may be of interest to researchers in the field.
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
Pros: + Interesting alternative algorithm for training autoencoders  Cons:   Not a lot of practical value because DANTE does not outperform SGD in terms of time or classification performance using autoencoder features.  This is an interesting and well written paper that doesn t quite meet the threshold for ICLR acceptance. If the authors can find use cases where DANTE has demonstrable advantages over competing training algorithms, I expect the paper would be accepted. 
CausalWorld is a benchmark for robotic manipulation to address transfer and structural learning. The benchmark includes (i) a variety of tasks (picking, pushing, tower, etc) relating to manipulating blocks, (ii) configurable properties for environments (properties of blocks, gravity, etc), (iii) customizable learning settings involving intervention actors, which can change the environment to induce a curriculum.  The reviewers found the paper compelling and with many strengths, including ‘interesting and important ideas’ (R4), ‘simple API with a standardized interface’ for ‘procedural generation of goals’ (R5), ‘strongly motivated and tackles a real and practical problem’ (R3), and ‘benchmark with many good properties’ (R2). By and large, the reviewers agree that the paper presents an important benchmark satisfying several desiderata, which I certainly agree with.   On the other hand, most of the reviewers (3 out of 4) also raised serious concerns, more prominently, about the experimental results and the causal inference component. For instance, R5 commented that “all the SOTA algorithms fail,” and it is hard to quantify how agents would perform well in different tasks. R3 pointed out the lack of “qualitative results exploring the relationship between the identified and proposed causal variables,” emphasizing that ‘the benchmark is well motivated, but not backed up with strong experimental results.‘’ R2 identified the lack of clear causal component in the paper while the paper mentions “opportunity to investigate causality” and “underlying structural causal model (SCM).” All in all, these are valid concerns.  The authors  rebuttal was quite detailed, and appreciated, but left some important questions unanswered.  The first and critical issue is about the causal nature of the simulator. The simulator s name is "causalworld" and its stated goal is to provide "a benchmark for causal structure and transfer learning in a robotic manipulation environment."  Also, the first bullet in the list of contributions is: "We propose CausalWorld, a new benchmark comprising a parametrized family of robotic manipulation environments for advancing out of distribution generalization and causal structure learning in RL." After reading the paper, I was quite surprised to realize there is no *single* example of a causal model, in any shape or form (e.g., SCM, DAG, Physics) or a structural learning benchmark. In other words, there is a serious, somewhat nontrivial gap between the claimed contributions and what was realized in the paper. One way to address this issue would be to make the causality more explicit in the paper, for example, by sharing the underlying structural causal model, how variables form causal relationships, what causal structures are being learned, and how these learned structures compare with the ground truth. I think these would be reasonable expectations of a simulator that aims to disentangle the causal aspect of the learning process.   The second issue is about the experimental results in terms of generalizability. The authors emphasized on different occasions that "The primary goal of this work is to provide the tools to build and evaluate generalizable agents in a more systematic fashion, rather than building generalizable agents for the tasks specified," or "the experiments is to showcase the flexibility regarding curricula and performance evaluation schemes offered with CausalWorld, rather than solving new tasks or proposing new algorithms." These responses are somewhat not satisfactory given that the goal of the paper is providing tools to build generalizable agents, while the authors seem to suggest they are not committed to actually building such agents. Specifically, the experiments did not demonstrate the simulator as a benchmark but only showcased its flexibility (i.e., offering a large number of degrees of freedom). One suggestion would be to evaluate how algorithms (agents) with varying degrees of "generalizability" power perform across tasks with various difficulty levels. As it currently stands, the tasks are too easy or too hard for the standard, uncategorized algorithms, which makes it difficult to learn any lessons from running something in the simulator.   Lastly, I should mention that the work has a great potential to introduce causal concepts and causal reasoning to robotics, there is a natural and compelling educational component here. Still, the complete absence of *any* discussion of causality and the current literature results hurt this connection and the realization of this noble goal. I believe that after reading the paper, the regular causal inference researcher will not be able to understand what assumptions and types of challenges are entailed by this paper and robotics research. On the other hand, the robotics researcher will not be able to understand what a causal model is and the tools currently available in causal reasoning that may be able to help solve the practical challenges of robotics. In other words, this is a huge missed opportunity since there is a complementary nature of what the paper is trying to do in robotics and the results available in causal inference. I believe readers expect and would benefit from having this connection clearly articulated and realized in a more explicit fashion.  If the issues listed above are addressed, I believe the paper can be a game changer in understanding and investigating robotics & causality.  Given the aforementioned potential and reasons, I recommend the paper s acceptance *under the assumption that* the authors will take the constructive feedback provided in this meta review into account and revise the manuscript accordingly. 
This paper presents a CNN architecture equivariant to scaling and translation which is realized by the proposed joint convolution across the space and scaling groups. All reviewers find the theorical side of the paper is sound and interesting. Through the discussion based on authors’ rebuttal, one reviewer decided to update the score to Weak Accept, putting this paper on the borderline. However, some concerns still remain. Some reviewers are still not convinced regarding the novelty of the paper, particularly in terms of the difference from (Chen+,2019). Also, they agree that experiments are still very weak and not convincing enough. Overall, as there was no opinion to champion this paper, I’d like to recommend rejection this time.  I encourage authors to polish the experimentations taking in the reviewers’ suggestions.  
The paper deals with cross domain few shot learning in the case of large source target domain shifts.  The paper received mostly below threshold reviews, with one exception (R3) whose review is addressing more general aspects, but still with some concern, especially in relation to the experimental part (to which authors did not answer). R1 s review is not of much help.  Clarity of the presentation and missing details seem to be recurrent issues all over the reviewers, together with remarks concerning the experimental validation, which would have required a deep revision and improvement, in particular regarding the use of more backbones, better ablation (Hebbian learner contribution, unclear initialization), processing times/computational complexity, significant comparative analysis re robust baselines.   The rebuttal clarifies some of the raised remarks but there are still issues, especially regarding Hebbian learning rule and ensemble learning strategies, and about results too, so not all reviewers were convinced to raise their ratings.  Overall, given the above issues, I consider the paper not yet ready for publication in ICLR 2021. 
This manuscript investigates and characterizes the tradeoff between fairness and accuracy in neural network models. The primary empirical contribution is to investigate this tradeoff for a variety of datasets.  The reviewers and AC agree that the problem studied is timely and interesting. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty of the results. IN particular, it is not clear that the idea of a fairness/performance tradeoff is a new one. In reviews and discussion, the reviewers also noted issues with clarity of the presentation. In the opinion of the AC, the manuscript is not appropriate for publication in its current state. 
The reviewers agree that this paper has some strengths to it, and some commented that the revision improved the manuscript, but the paper remained borderline with no strong champions in its favor. The reviews are encouraging and suggest that the paper is a bit tightly packed for the conference format, and perhaps because it is dense it is hard to the strength and scope of contributions, while other relationships such as to the Bayesian context could be explored more fully. Multiple reviewers find that a longer improved version would "shine" in a better suited journal.   The decision to reject is independent of the fact that the authors seem to have violated the anonymity rules in the revised version.
This paper is borderline, as evidenced by all of the reviewer s scores.  The pros are:   important and relevant topic    IMPORT is a reasonable, technically sound approach   paper is relatively clear  The cons all lie in the experimental evaluation, and whether the experiments sufficiently back the claim that IMPORT can learn sophisticated exploration strategies and validate IMPORT s merits compared to prior algorithms. In particular:   The choice of benchmarks does not sufficiently test the ability to explore in a sophisticated manner   Lack of comparisons to PEARL and MANGA, which can readily be applied to the online setting   The empirical improvements are relatively modest.  Overall, the cons slightly outweigh the pros of the paper. Indeed, no reviewer was willing to champion the paper s acceptance.
This submission proposes a deep network training method to verify desired temporal properties of the resultant model.  Strengths:  The proposed approach is valid and has some interesting components.  Weaknesses:  The novelty is limited.  The experimental validation could be improved.  Opinion on this paper was mixed but the more confident reviewers believed that novelty is insufficient for acceptance.
This paper presents an approach to hierarchical RL which automatically learns intrinsic task agnostic options. The approach involves a two level hierarchy, with policies learned by lower layer Workers and selected by a higher layer Scheduler. The approach is evaluated on four complex tasks and is shown to outperform existing methods. There were initial concerns with this paper around clarity of a number of points. These included the contributions of this work and questions around the experimental results, such as  discussing the learned options themselves. The authors provided extensive responses to these concerns, and updated the paper accordingly, including addition results and analysis. I believe the paper is now much clearer with interesting contributions.
This paper received 4 reviews with mixed initial ratings: 4,5,6,7. The main concerns of R2 and R4, who gave unfavorable scores, included lack of clarity around design choices and inconclusiveness of some of the experiments. In response to that, the authors submitted a new revision with a summary of changes and provided detailed responses to each of the reviews, which seemed to have addressed these concerns. R2 and R4 upgraded their scores and recommended acceptance. As a result, the final recommendation is to accept for presentation at ICLR as a poster.
This paper proposes a new solution for tackling domain adaptation across disjoint label spaces. Two of the reviewers agree that the main technical approach is interesting and novel. The final reviewer asked for clarification of the problem setting which the authors have provided in their rebuttal. We encourage the authors to include this in the final version. However, there is also a consensus that more experimental evaluation would improve the manuscript and complete experimental details are needed for reliable reproduction.
Although the paper presents some interesting ideas, in general the reviewers agree that the paper lacks clear results and is not an easy read. The paper proposes a factorisation of value functions, a topic that has received quite some attention in the literature (e.g. QPLEX), and it seems that their is not sufficient innovation in the proposed method in the paper. There are also a number of claims in the paper (e.g. partial observability etc.) with which some of the reviewers disagree, and should be discussed more carefully in a revised version of the article, that all in all seems to need more work.
The paper introduces a method for removing what they call representation error and apply the method to super resolution and compressive sensing.   The reviewers have provided constructive feedback. The reviewers like aspects of the paper but are also concerned with various shortcomings. The consensus is that the paper is not ready for publication as it stands.  Rejection is therefore recommended with strong encouragement to keep working on the method and submit elsewhere.
This paper addresses the problem of learning disentangled representations and shows that the introduction of a few labels corresponding to the desired factors of variation can be used to increase the separation of the learned representation.   There were mixed scores for this work. Two reviewers recommended weak acceptance while one reviewer recommended rejection. All reviewers and authors agreed that the main conclusion that the labeled factors of variation can be used to improved disentanglement is perhaps expected. However, reviewers 2 and 3 argue that this work presents extensive experimental evidence to support this claim which will be of value to the community. The main concerns of R1 center around a lack of clear analysis and synthesis of the large number of experiments. Though there is a page limit we encourage the authors to revise their manuscript with a specific focus on clarity and take away messages from their results.   After careful consideration of all reviewer comments and author rebuttals the AC recommends acceptance of this work. The potential contribution of the extensive experimental evidence warrants presentation at ICLR. However, again, we encourage the authors to consider ways to mitigate the concerns of R1 in their final manuscript.  
The initial reviews were mixed (2 positive, 2 negative).  The main concerns were about presentation issues: unclear contribution or main point; unclear analysis of figures; missing some motivation of selecting object detectors; etc.).  On the other hand, reviewers appreciated the well formulated paper, analysis and recommendations from the experiments;   The author response addressed the presentation issues and added additional motivations and clarifications. All reviewers in the end recommended accept. 
The paper studies the join Q value decomposition problem in MARL. Some of the results are interesting, e.g., the True Global Max condition and several experiments. However, the majority of the reviews are negative due to the current presentation of the paper. We encourage the authors address all the reviewers  comments and submit a new version to the next conference.
The paper studies an interesting problem with a reasonable solution.  However, reviewers feel that the technical contributions are somewhat incremental.  Furthermore, the empirical study would have been stronger with more proper baselines (simple adaptation to the multitask setting), and on problems beyond the simple grid worlds.  In addition, reviewers also find the presentation should be improved substantially.
This paper presents a continuous CNN model that can handle nonuniform time series data. It learns the interpolation kernel and convolutional architectures in an end to end manner, which is shown to achieve higher performance compared to naïve baselines.  All reviewers scored Weak Reject and there was no strong opinion to support the paper during discussion. Although I felt some of the reviewers’ comments are missing the points, I generally agree that the novelty of the method is rather straightforward and incremental, and that the experimental evaluation is not convincing enough. Particularly, comparison with more recent state of the art point process methods should be included. For example, [1 3] claim better performance than RMTPP. Considering that the contribution of the paper is more on empirical side and CCNN is not only the solution for handing nonuniform time series data, I think this point should be properly addressed and discussed. Based on these reasons, I’d like to recommend rejection.   [1] Xiao et al., Modeling the Intensity Function of Point Process via Recurrent Neural Networkss, AAAI 2017. [2] Li et al., Learning Temporal Point Processes via Reinforcement Learning, NIPS 2018. [3] Turkmen et al, FastPoint: Scalable Deep Point Processes, ECML PKDD 2019. 
Reviewers were almost unanimous in favor of this paper, with scores of 5,8,6,8. I think it s a neat idea and am inclined to accept despite some issues w/ motivation / scalability.  Science proceeds in increments, and it s OK to propose something with scalability issues that someone else later tries to fix, etc.
The paper proposes a method to generate adversaries close to the (training) data manifold using GANs rather than arbitrary adversaries. They show the effectiveness of their method in terms of human evaluation and success in fooling a deep network. The reviewers feel that this paper is for the most part well written and the contribution just about makes the mark.
The paper studies the convergence rate and generalization of deep ReLU networks trained with gradient descent and SGD in the NTK regime. Although the analysis technique is not really novel and heavily relies on past results, the paper is easy to follow and does provide some nice improvements compared to prior work (e.g. it require less overparametrization, and the NTRF function class is allowed to misclassify a fraction of the training data). Some of the results are very incremental, e.g. the generalization bound for GD seems to simply combine existing bounds on the Rademacher complexity from Bartlett et al. 2017 and from Cao et al. 2019. Nevertheless, the paper does have the potential to yield further improvements in the field and I therefore recommend acceptance as a poster.
This paper studies the general problem of out of distribution (OOD) detection, where the goal is to detect outliers (i.e., points not in the distribution of training data) in the sample. The paper introduces a methodology for measuring robustness by using adversarial search/distributions. Experimental evaluation indicates that traditional metrics fail to fully capture OOD detection. The reviewers  evaluations of this work were mixed. Overall, there was consensus about the importance of the problem. Moreover, some of the reviewers argued that the submission contains some interesting new ideas. On the other hand, concerns were raised regarding lacking comparison to prior work, potential overselling of the contributions, and several aspects of the experimental evaluation. At the end, there was not sufficient support for acceptance. In its current form, the work appears to be slightly below the acceptance threshold.
This paper has been independently assessed by three expert reviewers. The results place it at the borderline of acceptance decision: while one of the reviewers gave it a straight accept evaluation, two others assessed it as marginally rejectable, even after discussion with the authors. All of the reviewers agreed that the theoretical results provided should help promote the use of MLE estimators over perhaps more prevalently used in current practice TMO, and that is the main contribution of this work. The reviewers were concerned with the clarity of the presentation and with a confusing notation used. Some of these issues have been addressed in the authors  responses. All things considered, I conclude that this work can be of some interest to the ICLR audience, and as such it can be assessed as marginally acceptable for this conference: "accept if needed". I will recommend it as such for consideration by the Senior Area Chair and the Program Committee.
  This paper studies the difference between cross entropy and contrastive learning losses in the feature representations that they learn, specifically looking at class imbalanced datasets. The authors show that contrastive losses result in a more "balanced" representation, as measured by the balance of accuracy across the classes when a linear classifier is learned mapping from the feature representation to the class labels. They also show that empirically this tends to result in better generalization to downstream tasks. Inspired by this, they devise a simple modification of the prior supervised contrastive loss method and show that it can improve performance on ImageNet LT and even generalization performance when trained on balanced datasets and applied to downstream tasks.     The reviewers identified several weaknesses, including some clarity issues (R1), limitations of how balancedness is measured and lack of theoretical/statistical rigor in terms of the resulting claims (R2), and differences with respect to concurrent work (R4). A lengthy discussion occurred between reviewers and authors, as well as input from a co author of the concurrent work. In the end, the reviewers were not fully satisfied both in terms of the balancedness measure and relationship to the concurrent work.     Overall, despite this and the valid limitations of the work, I recommend accepting this paper as I believe the contributions outweigh the limitations, and that the findings would be interesting to the community. First, the paper provides some interesting analysis of balancedness and differences across these two loss functions, as well as connections to generalization, which even the concurrent work does not provide. The resulting method, while being a simple modification of the supervised contrastive loss work, is effective both for long tailed datasets and generalization to downstream tasks (even when trained in a balanced manner) which is nice. In the end, we should not use [3] to reject this paper since it was accepted right before the ICLR deadline.     However, I **strongly** recommend that the paper address the valid limitations mentioned in the discussions. Specifically:    1) While I agree that [3] is concurrent work, this paper should none the less tone down its claims of being the first in exploring balance for the camera ready version and clearly address differences between this paper and that one (even if mentioned as concurrent work). It is important to give credit when it is due, and while I think [3] is a different perspective it should be mentioned. Further, the claim that their methodology is not correct is highly arguable, so this should not be mentioned; rather the differences in perspectives and what each paper shows should be emphasized. Even without [3], self supervised pre training (initialization) should arguably be included as a baseline given that it is the logical first choice for incorporating self supervised learning.    2) Like R2, I do not believe the balancedness metric shows uniformity of the feature space. This would have to be shown through methods such as t SNE or in some other way. Being linearly separable in a balanced way across classes (which is what you showed) is not sufficient to show that feature space "uniformity". One can draw many feature space distributions that do not have the intuitive meaning of this (which isn t precisely defined by the authors) but still be linearly separable. I recommend authors remove this type of characterization (unless they can define/show it) and instead include a discussion of the limitations of the current methodology for measuring balancedness. Figure 1 should also emphasize that it is notional (not from real data). 
The paper presents an approach to address the open set recognition task based on inter and intra class distances. All reviewers are concerned with novelty and more experimental comparisons. Authors have added some results, but reviewers did not think these were enough to make the paper convincing enough. Overall I agree with reviewers and recommend to reject the paper.
This paper investigates knowledge distillation in the context of non autoregressive machine translations. All reviewers are supportive of acceptance, especially after the thoughtful author responses. A well motivated and simple to implement approach that is giving good empirical results.
This paper initially received mixed ratings but after the rebuttal, all reviewers recommended acceptance. Reviewers appreciate the novel technical ideas and extensive experimental results. 
The authors develop regularization schemes that aim to promote tightness of convex relaxations used to provides certificates of robustness to adversarial examples in neural networks.  While the paper make some interesting contributions, the reviewers had several concerns on the paper: 1) The aim of the authors  work and the distinction with closely related prior work is not clear from the presentation. In particular, the relationship to the ReLU stability regularizer (Xiao et al ICLR 2019) and the FastLin/CROWN IBP work (https://arxiv.org/abs/1906.06316) is not very well presented in the theoretical sections or the experiments.  2) The theoretical results (proposition 1) requires very strong conditions to apply, which are unlikely to be satisfied for real networks. This calls into question the effectiveness of the framework developed by the authors.  While the paper has some interesting ideas, it seems unfit for publication in its present form. 
This paper presents several variants and extensions (including stochastic and proximal) of the error feedback method EF21 and provides convergence rates for each of them and shows that they improve upon previous state of the arts. Despite the much broadened application scenarios and SOTA  in convergence rates/complexity, the main and common concern from the reviewers is the novelty of the paper beyond the original EF21 work. There are also concerns on the empirical evaluations that do not fully support the theoretical promises. I agree with the reviewers and regrettably have to recommend rejection for ICLR.
This paper explores the application of the lottery ticket hypothesis to NLP and RL problems for better initialisations of deep networks and reduced model sizes. This is evaluated in a variety of settings, including continuous control and ATARI games for RL, and LSTMs and Transformers for NLP, showing very positive results.  The main issue raised by the reviewers was the lack of algorithmic novelty in the paper. Despite this, I believe the paper to present an important contribution that could stimulate much additional research. The paper is well written and the results are rigorous and interesting. For these reasons I recommend acceptance.
This work proposes a continuous disentanglement variational autoencoder. The approach is a direct extension of Sha & Lukasiewcz (2021). The proposed method appears effective in learning disentangled factors on synthetic data. However, the approach is a minor change to Sha & Lukasiewcz (2021) that samples a weighted sum over all style values. This limits the novelty of the paper. Additionally, evaluation is only on small synthetic datasets that was created for this paper. The lack of evaluation on standard datasets such as an emotion dataset as motivated in the paper, means the results may be due to data selection rather than a superior method. This raises doubts as to whether the approach would generalize to other datasets. In the rebuttal the authors state they wanted to focus on a synthetic dataset since various metrics are easily method but additional real world/standard dataset results can be added while keeping the synthetic results.
This paper presents a method for producing higher quality uncertainty estimates by mapping the predictions from an arbitrary (e.g. deep learning) model to an exponential family distribution.  This is achieved by using the model to map from the inputs to a low dimensional latent space and then using a normalizing flow to map to the parameters of the distribution.  The authors show empirically that this improves over a variety of baselines on a number of OOD and uncertainty quantification tasks.  This paper received 5 reviews who all agreed that the paper should be accepted (6, 6, 8, 8, 8).  The reviewers in general found the method novel compared to existing literature, compelling and the results strong.  Multiple reviewers asked for experiments with higher dimensional output distributions (e.g. CIFAR 100) and had concerns regarding the "entropy regularization" term (akin to the beta term in a beta VAE, this is a constant applied to the entropy term).  The reviewers seemed satisfied with the author response, however, and the concensus decision is to accept.
This paper suggests that noise regularized estimators of mutual information in deep neural networks should be adaptive, in the sense that the variance of the regularization noise should be proportional to the range of the hidden activity. Two adaptive estimators are proposed: (1) an entropy based adaptive binning (EBAB) estimator that chooses the bin boundaries such that each bin contains the same number of unique observed activation levels, and (2) an adaptive kernel density estimator (aKDE) that adds isotropic Gaussian noise, where the variance of the noise is proportional to the maximum activity value in a given layer. These estimators are then used to show that (1) ReLU networks can compress, but that compression may or may not occur depending on the specific weight initialization; (2) different nonsaturating noninearities exhibit different information plane behaviors over the course of training; and (3) L2 regularization in ReLU networks encourages compression. The paper also finds that only compression in the last (softmax) layer correlates with generalization performance. The reviewers liked the range of experiments and found the observations in the paper interesting, but had reservations about the lack of rigor in the paper (no theoretical analysis of the convergence of the proposed estimator), were worried that post hoc addition of noise distorts the function of the network, and felt that there wasn t much insight provided on the cause of compression in deep neural networks. The AC shares these concerns, and considers them to be more significant than the reviewers do, but doesn t wish to override the reviewers  recommendation that the paper be accepted.
Important problem (analyzing the properties of emergent languages in multi agent reference games), a number of interesting analyses (both with symbolic and pixel inputs), reaching a finding that varying the environment and restrictions on language result in variations in the learned communication protocols (which in hindsight is that not surprising, but that s hindsight). While the pixel experiments are not done with real images, it s an interesting addition the literature nonetheless.  
This paper proposes to follow inspiration from NLP method that use position embeddings and adapt them to spatial analysis  that also makes use of both absolute and contextual information, and presents a representation learning approach called space2vec to capture absolute positions and spatial relationships of places. Experiments show promising results on real data compared to a number of existing approaches. Reviewers recognize the promise of this approach and suggested a few additional experiments such as using this spatial encoding as part of other tasks such as image classification, as well as clarification and further explanations on many important points. Authors performed these experiments and incorporated the results in their revisions, further strengthening the submission. They also provided more analyses and explanations about the granularity of locality and motivation for their approach, which answered the main concerns of reviewers. Overall, the revised paper is solid and we recommend acceptance.
The paper proposes a variational inference method for Bayesian neural networks where the approximate posterior models the correlations between the weights at all layers, using the concept of “global” inducing points.   Some concerns raised by the reviewers regarding how global inducing points allow us to capture uncertainty across the compositional structure and clarity of the manuscript have been addressed by the authors. However, the more general issue of interpretability has been left for future work.   One of the main deficiencies of this paper is that it seems to ignore other scalable approaches that also provide more complex posteriors, for example, those based on stochastic gradient Hamiltonian Monte Carlo (see, e.g., https://arxiv.org/abs/1806.05490, and references therein). Overall, although there was support for this paper, it is unclear if approaches such as those presented here are really necessary. A comparison between the two methodologies maybe not only illustrative but required.  
This paper attempts at modeling coherence of generated text, and proposes two kinds of discriminators that tries to measure whether a piece of text is coherent or not.  However, the paper misses several related critical references, and also lacks extensive evaluation (especially manual evaluation).  There is consensus between the reviewers that this paper needs more work before it is accepted to a conference such as ICLR. 
The paper presents a novel with compelling experiments. Good paper, accept.  
This paper introduces a control based approach to sampling. All of the reviewers found the idea interesting. There were serious concerns by some of the reviewers regarding how the paper positioned itself relative to the literature, how it designed baselines for experiments, and how it compared itself to existing methods. There was vigorous rebuttal phase. The authors submitted a slightly late revision based on a procedural misunderstanding, and I decided to incorporate their late revision.  Based on the revision, the majority of reviewers felt that the paper was at least above the bar for acceptance and some of the more positive reviewers stood strongly by the paper. I believe that this paper is of value to the community, so I will recommend that it is accepted, but I want to be very clear about something: the authors **must** incorporate the late revision as the basis of their camera ready and I **strongly recommend** that they address the concerns of reviewer d7Mk, including but not limited to:    "Our approach avoid long mixing time theoretically and is more efficient": this claim is too strong.   Figure 1 is not particularly informative and the authors should reconsider it.   The presentation of Eq (2), Algorithm 1, and Algorithm 2 should be simplified.    Section F.1 should incorporate the comments of Reviewer d7Mk.   Citing standard references mentioned by Reviewer d7Mk.  The reason I highlight these recommendations is that I believe they will greatly improve the quality, longevity, and impact of this paper. Slightly overselling ideas feels like a good strategy, but it is a bad long term strategy. I believe addressing these points is in the interest of the authors.
This paper proposes to add constraints to the RL problem within a variational method. The hope is to specify a safe vs non safe states. The reviewers were not convinced that this paper makes the cut for ICLR. Moreover, there was no rebuttal from the authors, so it didn t give the reviewer a chance to reconsider their opinion. Based on the current ratings, I recommend to reject this paper. 
This paper studies the behavior of weight parameters for linear networks when trained on separable data with strictly decreasing loss functions. For this setting the paper shows that the gradient descent solution converges to max margin solution and each layer converges to a rank 1 matrix with consequent layers aligned.  All reviewers agree that the paper provides novel results for understanding implicit regularization effects of gradient descent for linear networks. Despite the limitations of this paper such as studying networks with linear activation, studying gradient descent not with practical step sizes, assuming data is linearly separable, reviewers find the results useful and a good addition to existing literature.
This paper looks into growing neural networks, and finds an improved approach to the initialisations of new layers, viz by maximising the gradient norm.  Simple, straightforward, neat, and no good reason to reject.  It will benefit those who are using growing NNs.
The paper studies an narrowly focused but interesting problem   if the Visual Question answering model “FILM” from Perez et al (2018) is able to decide if “most” of the objects have a certain attribute or color. While the work itself is appreciate by the reviewers, concerns remain about the conclusion being limited in scope due to the synthetic nature of the data, and the analysis fairly narrow (a single model with a single very specific task). We encourage the authors to use reviewer feedback to make the manuscript stronger for a future deadline. 
This paper attempts to explain why popular UNMT training objective components (back translation and denoising autoencoding) are effective. The paper provides experimental analysis and draws connections with ELBO and mutual information. Reviewers generally agree that the paper s goal is worthy: trying to form a better theoretical understanding of successful approaches to UNMT.  However, most reviewers raised serious concerns about the current draft and suggested another round of revision and resubmission. Specifically, reviewers were concerned that some of the analogies used to explain UNMT are underdeveloped. Further, reviewers pointed to issues with clarity that made some of the arguments hard to follow. Finally, one reviewer argued that many of the results are expected and agree with common understanding of UNMT in the literature, thus undermining their value to some extent. I tend to agree with reviewers that this paper is not ready for publication in its current form. Thus I recommend rejection. 
All of the reviewers find the approach interesting, but they have reservations regarding the practical impact and empirical evaluation. The paper needs improvement both on the motivation and on the experimental results by including more baseline methods and neural architectures. 
Strengths: * Well written paper *Theoretical analysis demonstrates that dual encoder models have similar capacity as CA models *New distillation algorithm for learning DE students from CA teachers  Weaknesses: * No reviewer seems particularly excited about this work  * Theoretical analysis doesn’t provide actionable insight   it does not directly motivate the suggested distillation methods * Empirical results are lacking   reviewers asked for qualitative examples of improvements from their distillation method
The reviewers have arrived at the consensus that this is a paper with an interesting idea, both novel and well explained, but not quite backed up with sufficient empirical evidence. Like them, I think there is a lot of potential in modular methods for continual learning, and I know these are challenging advances to demonstrate. So I encourage you to persist, iterate and submit a stronger version of this paper in the future!
Most reviewers came to the conclusion, that this work lacks novelty and theoretical depth. Further severe concerns about the validity of some statements and about the experimental setup have been raised. The rebuttal was not perceived as being fully convincing, and nobody wanted to champion this paper.  I share most of these points of criticism. Although there is certainly some potential in this work, I think it is not ready for publication and would (at least) need a major revision.
The paper proposes to use a feature extractor (encoder) $C(x)$, pre trained with label supervision or contrastive learning on a large image dataset, to both regularize the discriminator s last feature layer $D_f(x)$ and encode the data $x$ itself as the conditional input of the generator $G(z|G_{emb}(C(x)))$. The main purpose is to help the training of GANs when there is a limited number of images in the target domain. A clear concern of this approach is that to generate a fake image, one will need to first sample a true image, making the model unattractive if the training dataset size is large (need to store the whole training dataset even after training). To mitigate this issue, the authors propose to fit up to 200k randomly sampled $G_{emb}(C(x))$ with a GMM with 1k components. To validate the practice of requiring a GMM (a shallow generative model) to help a GAN (a deep generative model) to generate, the authors have done a rich set of experiments under state of the art GAN architectures or training methods (SNGAN, BigGAN, StyleGAN2, DiffAugment) to illustrate the efficacy of the proposed data instance prior and its compatibility with the state of the art methods in a variety of settings. In the AC s opinion, the paper is missing references to 1) related work that combines VAE (or some other type of auto encoder) and GAN, which often helps stabilize the GAN training [1,2,3], 2) VAE with a VampPrior [4], and 3) more broadly speaking, empirical Bayes related methods where the prior model is learned from the observed data (see [5] and the references therein). The potential advantages of using a VAE rather than a GMM to help a GAN to generate include: 1) there is no need to store 1k GMM components, which may require a large amount of memory; 2) there is no need to subsample the training set; and 3) the VAE and GAN can be jointly trained. The AC recommend the authors to discuss the connections to these related work in their future submission.  [1] Larsen, Anders Boesen Lindbo, et al. "Autoencoding beyond pixels using a learned similarity metric." International conference on machine learning. PMLR, 2016.  [2] Zhang, Hao, et al. "Variational Hetero Encoder Randomized GANs for Joint Image Text Modeling." International Conference on Learning Representations. 2019.  [3] Tran, Ngoc Trung, Tuan Anh Bui, and Ngai Man Cheung. "Dist gan: An improved gan using distance constraints." Proceedings of the European Conference on Computer Vision (ECCV). 2018.  [4] Tomczak, Jakub, and Max Welling. "VAE with a VampPrior." International Conference on Artificial Intelligence and Statistics. PMLR, 2018.  [5] Pang, Bo, Tian Han, Erik Nijkamp, Song Chun Zhu, and Ying Nian Wu. "Learning Latent Space Energy Based Prior Model." Advances in Neural Information Processing Systems 33 (2020). 
Clearly explained, well motivated and empirically supported algorithm for training deep networks while simultaneously learning their sparse connectivity. The approach is similar to previous work (in particular Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011) but is novel in that it satisfies a hard constraint on the network sparsity, which could be an advantage to match neuromorphic hardware limitations.
First this is the seed for a  very good paper on approximating manifolds and densities using injective flows.  Reviewers have done an admirable effort reviewing the paper giving detailed reviews and suggestions to improve the theory and  corrections  that resulted in an improvement of the paper during the rebuttal/ revision phase.  Unfortunately the paper still needs major rewriting and organization to be accessible by other readers, and should undergo another round of review in its last polished version to further vet the correctness of some of its claims as explained below .  The paper was discussed at length among reviewers and the AC and here are the suggestions to improve the paper.   * Implementing Reviewer 4sjW suggestion w.r.t  to the narrative and adding explanations to improve the readability and accessibility  of the paper.   * Another concerns were raised by reviewer eR1p  in the discussion  regarding the correctness of Theorem 1 and Corollary 1. " The proof of Corollary 1 is so rough that I could not confirm its correctness. For example, the functions $r$ and  $f$ are undefined."  Please revisit the proof of this Corollary. Theorem 1 builds on Lemma 7 point 5.  In point 5 of Lemma 7 :"The embedding  $r$ depends on $\epsilon$ , hence so is the measure $\mu$.Therefor the statement  $W_2(g \mu ,f \mu)< B_{K,W}(f,g) + \epsilon$ for all $\epsilon$, does not imply that  $W_2(g \mu ,f \mu)< B_{K,W}(f,g)$. One solution can be by  building a sequence of measures that would converge to that measure and see if the argument goes through.    We encourage the authors to implement all the feedback  and suggestions of the reviewers and to submit this interesting work to an upcoming venue.
This paper presents a study of the over parametrization of linear representations in the context of recursive value estimation.  The reviewers could not reach a consensus over the quality of the paper, with a fairly wide range of scores even after the rebuttal.  After considering the paper, the rebuttal, and the discussion, I lean towards accepting the paper. Despite the concerns voiced by some of the reviewers, the topic and analysis of the manuscript are novel and interesting, and it is my expectation that this manuscript will prove a valuable source of inspiration for future work.  I invite the authors to carefully consider the feedback received by all the reviewers (and in particular Reviewers xq3y and gT5o and) and to revisit the manuscript accordingly.
This paper proposes a new policy gradient method based on stochastic mirror descent and variance reduction. Both theoretical analysis and experiments are provided to demonstrate the sample efficiency of the proposed algorithm. The main concerns of this paper include: (1) unclear presentation in both the main results and the proof; and (2) missing baselines (e.g., HAPG) in the experiments. This paper has been carefully discussed but even after author response and reviewer discussion, it does not gather sufficient support.  Note: the authors disclosed their identity by adding the author names in the revision during the author response. After discussion with PC chair, the openreview team helped remove that revision during the reviewer discussion to avoid desk reject.  
The authors present an approach to learning from noisy labels. The reviews were mixed and several issues remain unresolved. I do not accept the following as a valid response: "We fully agree that noisily collected labels are common for many problems other than image classification. However, the focus of our paper is image classification, and we thus concentrate on classification problems related to the widely popular CIFAR 10 and ImageNet classification problems." ICLR is a conference on theoretical and applied ML, and the fact that a technique has not been used for image classification before, does not mean you bring something to the table by doing so. The NLP literature is abundant with interesting work on label noise and should obviously be considered related work. That said, there s also missing references directly related to the connection between early stopping/regularization and label bias correction, including:   [0] https://arxiv.org/pdf/1904.11238.pdf [1] https://arxiv.org/pdf/1705.03419.pdf [2] http://proceedings.mlr.press/v80/ma18d/ma18d.pdf  See also this paper submitted to this conference: https://openreview.net/forum?id SJldu6EtDS
This paper proposes to perform sample selection for deep learning   which can be very computationally expensive   using a smaller and simpler proxy network. The paper shows that such proxies are faster to train and do not substantially harm the accuracy of the final network.  The reviewers were all in agreement that the problem is important, and that the paper is comprehensive and well executed. I therefore recommend it should be accepted.
This paper proposes a diagonal approximation to the Hessian in a quasi Newton method for non convex stochastic optimization problems. They combine several good existing ideas and show empirically that the method performs well on several learning tasks, but reviewers found that the comparisons were limited in that as an (approximate) second order method, it would be more fair to compare to other second order methods rather than largely focusing on SGD and some variants of ADAM. Overall, reviewers found the novelty limited and had some concerns about the strength of assumptions, parameter wise updates, and some more minor comments on gaps in the presentation. The author response did not fully convince the borderline/negative reviewers, though the paper includes good ideas that would potentially be well received in a future revision.
The paper propose a Fully Online Meta Learning (FOML) method which extend MAML for continual learning in a fully online learning  without requiring the knowledge of the task boundaries. Experiments show that FOML was able to learn new tasks faster than several existing online learning methods on Rainbow MNIST, and CIFAR100 datasets.   There are a few major concerns from reviewers. One concern is about the lack of clarity on the problem statement: The authors cast the problem as meta learning that must be done in a fully online setting, but it requires to store all the training data in a buffer storing all the training data seen so far, which contradicts to the principle of “online learning”. Another major weakness is the poorly written literature survey, which missed to cite a large body of related work in continual learning and online meta learning (such as Online Continual Learning, task free continual learning, continual learning without task boundaries, etc). These should at least be discussed carefully if not fully compared in the empirical studies. Also experiments are quite weak in both settings, datasets and rather out of date baselines. Finally, there also lacks of theoretical justification or analysis.   Therefore, the paper is not recommended for acceptance in its current form. I hope authors found the review comments informative and can improve their paper by addressing these review comments carefully in future submissions.
The paper introduces a novel and effective approach to policy optimization.  The overall contribution is sufficient to merit acceptance.  Nevertheless, the authors should improve the presentation and experimental evaluation in line with the reviewer criticisms.  The criticisms of AnonReviewer2 in particular should not be neglected.  Regarding the theory, I agree with AnonReviewer3 that the UNOP assumption is too limiting.  The paper would be much stronger if this assumption could be significantly weakened, or better justified.
Overall this paper presents a few improvements over the code2vec model of Alon et al., applying it to seq2seq tasks. The empirical results are very good, and there is fairly extensive experimentation.  This is a relatively crowded space, so there are a few natural baselines that were not compared to, but I don t think that comparison to every single baseline is warranted or necessary, and the authors have done an admirable job. One thing that still is quite puzzling is the strength of the "AST nodes only baseline", which the authors have given a few explanations for (using nodes helps focus on variables, and also there is an effect of combining together things that are close together in the AST tree). Still, this result doesn t seem to mesh with the overall story of the paper all that well, and again opens up some obvious questions such as whether a Transformer model trained on only AST nodes would have done similarly, and if not why not.  This paper is very much on the borderline, so if there is space in the conference I think it would be a reasonable addition, but there could also be an argument made that the paper would be stronger in a re submission where the above questions are answered.
The paper initially received a mixed rating, with two reviewers rate the paper below the bar and two above the bar. The raised concerns include the need for an autoregressive model for upsampling and the effect of batch sizes. These concerns were well addressed in the rebuttal. Both of the reviewers that originally rated the paper below the bar raise the scores. After consulting the paper, the reviews, and the rebuttal, the AC agrees that the paper has its merits and is happy to accept the paper.   
This works presents a new and interesting notion of margin for deep neural networks (that incorporates representation at all layers). It then develops generalization bounds based on the introduced margin. The reviewers pointed some concerns, including some notation issues, complexity in case of residual networks, removal of exponential dependence on depth,  and dependence on a hard to compute quantity   \kapp^{adv}. Some of these concerns were addressed by the authors. At the end, most of the reviewers find the notion of all layer margin introduced in this paper a very novel and promising idea for characterizing generalization in deep networks. Agreeing with reviewers, I recommend accept. However, I request the authors to accommodate remaining comments /concerns raised by R1 in the final version of your paper. In particular, in your response to R1 you mentioned for one case you saw improvement even with dropout, but that is not mentioned in the revision; Please include related details in the draft. 
Looks like a great contribution to ICLR. Continuous adaptation in nonstationary (and competitive) environments is something that an intelligent agent acting in the real world would need to solve and this paper suggests that a meta learning approach may be quite appropriate for this task.
meta score: 8  The paper present a distributed architecture using prioritized experience replay for deep reinforcement learning.  It is well written and the experimentation is extremely strong.  The main issue is the originality   technically, it extends previous work in a limited way;  the main contribution is practical, and this is validated by the experiments.  The experimental support is such that the paper has meaningful conclusions and will surely be of interest to people working in the field.  Thus I would say it is comfortably over the acceptance threshold.  Pros:    good motivation and literature review    strong experimentation    well written and clearly presented    details in the appendix are very helpful Cons:    possibly limited originality in terms of modelling advances 
This paper investigates and evaluates learning high dimensional embeddings of time, which is useful for a variety of applications. This paper received 4 reviews (due to a missing review, we requested several emergency reviews). R1 recommends Weak Accept, calling the method simple but saying it could be of wide interest and utility in practice. R3 recommends Reject, identifying concerns about the significance of the contribution, caused by the simplicity of the approach, the connection to existing work, and missing comparisons to baselines. In a short review, R4 recommends Accept with several positive comments. In a long, thoughtful review, R5 recommends Weak Reject, due to concerns and questions about the theoretical motivation and depth of experiments. The authors have submitted detailed responses that have addressed many of the questions of the reviewers; however, R3 feels the response does not address their concerns, and R5 is closer to accepting but still feels additional improvements in presentation and experimentation are needed.  Given the split decision, the AC also read the paper. The AC agrees with R1 and R4 that this is an interesting problem and the approach here may be useful in practice, but shares concerns with R3 and R5 about the depth of contribution with respect to existing work, and need for additional experimental validation against stronger baselines. 
The work demonstrates that adversarially perturbing inputs can change the output of concept based explainability tools. Reviewers generally agreed that the writing was clear and the experiments were easily understood. Regarding novelty, reviewers noted that there are several existing works which study the adversarial robustness of explainability tools (one even has experiments specifically on concept based explainability tools). As a result, there is not much novelty in the finding that concept based explainability tools are sensitive to adversarial perturbation. Regarding the technical contribution of the algorithm, it is expected that standard optimization approaches (e.g. PGD) would be sufficient to break concept based explainability tools so there is not a clear technical challenge being solved in the work.  The work could be improved by refocusing the robustness analysis to derive new insights regarding the behavior of concept based explainability tools. In doing so, it would be beneficial to deemphasize the claims regarding novel security concerns these methods don t even work reliably in non adversarial settings, as evidenced by poor out of distribution robustness. It is expected that performance will be even worse under adversarial settings.
While this paper was perceived as being fairly well written, the level of novelty and the evaluation were seen as weak by many reviewers. The aggregate opinions across reviewers is just too low to warrant an acceptance rating by the AC. The AC recommends rejection.
This is a strong empirical paper that studies scaling laws for NMT in terms of several new aspects, such as the model quality as a function of the encoder and decoder sizes, and how the composition of data affects scaling, etc. The extensive empricial results offer new insights to the questions and provide valuable guidance for future research on deep NMT. The datasets used in the study are non public, which may make it hard to reproduce the evaluation.
Thanks for your feedback to the reviewers, which helped us a lot to better understand your paper. Through the discussion, the overall evaluation of this paper was significantly improved. However, given the very high competition at ICLR2020, this submission is still below the bar unfortunately. We hope that the discussion with the reviewers will help you improve your paper for potential future publication.
All reviewers found that the paper offers interesting contributions for multi agent RL and favour acceptance of the paper. The strengths of the paper are summarized below:   Good algorithmic contribution   Offers a new set of benchmark tasks for coordination in MARL settings   Exhaustive experiments on complex tasks with a reasonable number of agents   All the issues raised by the reviewers (missing references, missing discussion of limitations...) have been satisfactorliy addressed.  I therefore join the reviewers in the recommendation to accept the paper.
This paper considers helping to decide whether behavior cloning or offline RL is likely to be more effective given a particular offline dataset. The reviewers initially appreciated the importance of insights into this question around how to best leverage an existing dataset. They also had some initial concerns, due in part because the theory is restricted to tabular settings, whereas many challenges typically arise when function approximators are used, the realisticness of the assumptions over the data collection process, and a number of places where further details or clarifications would better situate and strengthen the work. The authors gave very extensive responses to the feedback which made reviewers feel much more confident about the revised paper and resulted in significantly higher scores. Though there remains many interesting areas for future work, this paper makes an interesting contribution that may be of interest to many using batch decision making data.
In general, this seems like a sensible idea, but in my opinion the empirical results do not show a very compelling margin between using *entropy* as an active learning selection criterion vs the proposed methods. The difference is small enough that in practice it is very hard for me to believe that many researchers would choose to use the meta learning via deep RL method (given that they d need to train on multiple datasets and tune REINFORCE which is not going to be obviously easy). For that reason I am inclined to reject the paper.  In a follow up version, I would heed the advice of Reviewer 1 and do more ablation analyses to understand the value of myopic vs non myopic, cross dataset vs. not, bandits vs RL, on the fly vs not (these are all intermingled issues). The relative lack of such analyses in the paper does not help in terms of it passing the bar.
Reviewers liked the concept of the zero day attack and yet raised different concerns about the other parts of the paper. In general, Reviewers wanted to see more thorough experimental evaluations (e.g., against blackbox attack and adaptive attack) and improved clarity of the theoretical analyses. AC encourages authors to incorporate Reviewers  comments when preparing the paper for elsewhere.
This paper presents a new approach to grounding language based RL tasks via an intermediate semantic representation, in an architecture called language goal behavior (LGB).  The architecture permits learning a mapping from internal goals to behavior  (GB) separately from learning a mapping from language to internal goals (LG), and prior to flexibly combining all three (LGB).  The architecture is studied in a specific implementation called DECSTR.  The architecture has multiple desired attributes including support for intrinsic motivation, decoupling skill acquisition from language grounding, and strategy switching.  The experiments demonstrate the utility of different components in the architecture with a variety of ablation results.  The reviews initially found the paper to be poorly organized with required content described only in the appendix (R1, R2, R4), with unclear main contributions (R1, R2, R4), and with results restricted to demonstrations (R3).  Despite these reservations, the reviewers found the content to be potentially relevant though narrow in scope.  The authors substantially revised the paper. They improved its organization, clarified contributions, separated the architecture from the specific examples, and improved the experimental baselines.  After reading the revised paper, the reviewers agreed that the paper s organization and insights were improved, making the new paper s contribution and insight clear.  The experimental baselines were also improved, providing more support for the potential utility of the proposed method.  Three reviewers indicate to accept this paper for its contribution of a novel approach to grounding language and behavior with an intermediate semantic representation. No substantial concerns were raised on the content of the revised paper. The paper is therefore accepted.
This paper received high quality reviews, which highlighted numerous issues with the paper.  A common criticism was that the results in the paper seemed disconnected.  Numerous technical concerns were raised. Reading the responses, it seems that some of these issues are nonissues, but it seems also that the writing was not sufficiently up to the standard required of this type of technical work. I suggest the authors produce a rewrite and resubmit to the next ML conference, taking the criticisms they ve received here very seriously.
This paper introduces a new approach for risk sensitive RL by using an objective that depends on the full distribution and can apply a weight to the resulting trajectory. The reviewers thought that focusing on more general and expressive objectives for RL is well motivated. However, they had a number of concerns of the current paper state, including its clarity in a number of sections and its relation to other work in risk sensitive RL. The authors provided thoughtful responses but some concerns lingered around the prior concerns.
Given two data measures in R^d, this paper proposes to use a NN to augment the representation of each data point found in these measures with additional coordinates. The measures are then compared using the sliced Wasserstein distance on these augmented representations. Because this augmentation is injective by design (the original vectors are part of the new representation) simple metric properties are kept. The authors propose to learn in a robust/adversarial way these augmentations. They propose simple experiments to illustrate that idea.  Although I found the idea interesting, I think it falls short of acceptance at ICLR. I agree with the sentiment of other reviewers 1 and 2 that defining another variant of robust/NN inspired variant of the W distance is interesting, but at this point the readership of the conference expects more than simple experiments on toy data and hard to interpret GAN results. I think there is value in the draft as it stands now, but that more efforts are needed to convince this variant is scalable / useful for other downstream tasks (e.g. W barycenters, or other easier to interpret W problems in lower dimensions).  minor comments   as it stands, equation 2 is wrong if you do not add more conditions on the cost function d(.,.).    " the idea of SWD by projecting distributions onto hypersurfaces rather than hyperplane"  > this is wrong, the projection is done onto lines or curves, not hyperplanes or hypersurfaces. 
although the idea is a straightforward extension of the usual (flat) attention mechanism (which is positive), it does show some improvement in a series of experiments done in this submission. the reviewers however found the experimental results to be rather weak and believe that there may be other problems in which the proposed attention mechanism could be better utilized, despite the authors  effort at improving the result further during the rebuttal period. this may be due to a less than desirable form the initial submission was in, and when the new version with perhaps a new set of more convincing experiments is reviewed elsewhere, it may be received with a more positive attitude from the reviewers.
This paper modifies the loss of supervised contrastive (SupCon) learning by adding a self contrastive loss. Utilizing a multi exit network and contrasting the multiple outputs of this network, the proposed self contrastive (SelfCon) learning removes the requirement of additional data augmentation samples for creating positive pairs. The proposed SelfCon loss is theoretically connected to the lower bound of a label conditional mutual information between the intermediate and last feature. The paper focuses its study on SupCon & SelfCon M, which are multi batch variates that first augment the images, and then contrast the views between both augmented and non augmented samples of the same class, and on SupCon S and SelfCon S, which are single batch variates that only contrast between the samples of the same class and do not require additional data augmentations. A wide variety of experiments have been done on CIFAR 10, CIFAR 100, TinyImageNet, ImageNet 100, and ImageNet, but mostly with relatively small networks.   The ratings for the paper were mixed [3,5,5,8 before rebuttal; 5,5,6,8 after rebuttal]. All four reviewers had provided detailed initial reviews, pointing out a long list of issues. The authors had incorporated these reviews to make a large number of improvements to their initial submission. After the author rebuttal period, while one reviewer raised the score from 5 to 6, two reviewers maintained their negative positions: Reviewer ZiPE is clearly concerned about the risk of accepting a method that may break as soon as a slightly larger model (ResNet50 instead 18) is used, the model is trained a bit longer, or the baselines are tuned, while Reviewer MBzi is unsatisfied with how the paper motivates its empirical construction from the perspective of mutual information maximization.   Given the disagreements between the reviewers, the AC has carefully read the paper to provide an additional review. Some concerning observations of the AC are summarized as follows:  1. Echoing the concern of Reviewer ZiPE, the performance gain of SelfCon S over SupCon diminishes in ImageNet with ResNet 18, as shown in Table 13, making it become even more important for the authors to conduct experiments following more standard settings (e.g., ResNet 50 on ImageNet).  2. The main paper seems to suggest SelfCon S outperforms SelfCon M and SupCon outperforms SupCon S, while Table 13 in the Appendix suggests the opposite.   3. Table 3 that compares SelfCon S with SupCon appears very misleading, as SupCon consumes more memory and computation than SelfCon S simply because it has used data augmentations. If SupCon S is used, it would take less memory and computation than SelfCon S.  4. SelfCon S adds a subnetwork to the backbone to boost its performance, so technically, it has more parameters than the backbone. Comparing it with a baseline that only uses a backbone model does not seem to be that fair. This point has not been discussed in the paper.   5. Last but not least, echoing the concerns of Reviewer ZiPE and MBzi, the paper seems to try to validate the motivating of the added loss with mutual information maximization.  However, establishing the causal relationship between maximizing the mutual information of the intermediate and last layers and the classification performance needs much more than the correlation analysis provided in the paper.   Given the above mentioned concerns, the AC does not consider the paper to be ready for publication at its current stage.
All reviewers are in agreement for a rejection decision. Details below.
The authors propose a new model to learn voice style transfer using an encoder decoder framework with the aim of disentangling content and style representations.  The strengths of the paper are: + the method is well motivated with sound theoretical justification + the authors improve up on the prior work by augmenting the loss with an information theoretic term + empirical evaluations demonstrate performance improvements in speaker verification and speech similarity tasks + demonstrate improvements in the challenging zero shot task  Several reviewers requested improvements in readability + “ideally the central intuitions and actual specific bottom line criteria used would be much clearer.” + more clarify on empirical details including challenges that needed to be addressed 
This paper addresses the problem of few shot learning and then domain transfer. The proposed approach consists of combining a known few shot learning model, prototypical nets, together with image to image translation via CycleGAN for domain adaptation.  Thus the algorithmic novelty is minor and amounts to combining two techniques to address a different problem statement. In addition, as mentioned by Reviewer 2, though meta learning could be a solution to learn with few examples, the solution being used in this work is not meta learning and so should not be in the title to avoid confusion.   As this is a new problem statement the authors apply multiple existing works from few shot learning (and now adaptation) to their setting. The proposed approach does outperform prior work, however this is not surprising as the prior work was not designed for this task. Despite improvements during the rebuttal to address clarity the specific experimental setting is still unclear   especially the setup of meta test data vs unsupervised da data.   This paper is borderline. However, since the main contribution consists of proposing a new problem statement and suggesting a combination of prior techniques as a first solution, the paper needs a more thorough ablation of other possible combination of techniques as well as a clearly defined experimental setup before it is ready for publication.
This paper presents a semantically controllable generative framework by integrating explicit knowledge. In particular, a tree structured generative model is proposed based on knowledge categorization. Reviewers raised concerns about technical details, experiments, and missing references. In the revised paper, the authors provided more justifications and clarifications, such as the definition of adversarial attack. During the discussion, reviewers agreed that the previous concerns have been partially addressed, but there are still concerns on experiments, e.g., more recent work should be considered as baselines.  Overall, I recommend to reject this paper. I encourage the authors to take the review feedback into account and submit a future version to another venue.
The authors propose a multi resolution pyramidal attention mechanism to capture long range dependencies in time series forecasting, achieving linear time and space complexity. The authors conduced an extensive set of experiments and ablation studies demonstrating that  the proposed method consistently outperforms the state of the art and provided evidence for the various components of the architecture. They also provided a proof guarantee the linear complexity of long sequence encoding and adequately addressed the concerns raised by the reviewers. The additional additional benchmarks conducted by the author further demonstrated the strong performance of the method. All reviewers agreed that this work makes a solid contribution to the field.
The paper received Weak Reject scores from all three reviewers. The AC has read the reviews and lengthy discussions and examined the paper. AC feels that there is a consensus that the paper does not quite meet the acceptance threshold and thus cannot be accepted. Hopefully the authors can use the feedback to improve their paper and resubmit to another venue.
The paper shows that for a simple nonlinear (quadratically parametrized linear) model, stochastic gradient descent (SGD) with a certain label noise and learning rate schedule recovers the data generating model. In contrast, gradient descent with or without Gaussian noise fails. While the results are novel and interesting, they hold for a rather specialized model, which may not reveal anything about deep neural networks, which was the original motivation for this work. Given the narrow focus of the work, unfortunately, I cannot recommend that the paper be accepted. 
This paper proposes a new source code modeling benchmark, with the unique twist being that we not only have code source text, but we also have build information, which allows extracting richer information to construct labels from. This enables, for example, a null pointer prediction task with labels coming from an inter procedural static analysis tool. AC and reviewers agree that this is a valuable framing for a benchmark suite. Unfortunately, it’s not clear that the benchmark in its current form delivers on the promise of the framing. Much of the interest and novelty is limited to just the one NullToken task, and reviewers raise a number of concerns including dataset size and whether the task truly measures the inter procedural reasoning that it sets out to measure. AnonReviewer2 raised some good questions here that the authors promised to address in a forthcoming comment, but that didn’t come before the discussion deadline. I’d encourage the authors to use the reviewer suggestions to more strongly establish that these tasks measure what they set out to measure, and also to consider adding other tasks that measure whether our ML models are capable of deeper / longer range reasoning. In total, there is a lot of potential here, but the work needs another iteration before it’s ready for publication.
Overall, this seems like a neat idea and well done work. Main principle is to extract a very sparse net that does a good job at locally "explaining" a given example. The NeuroChains idea does this with a diffentiable sparse objective. I think this work is well positioned and has nice properties: (1) retains a very small percentage of "filters", (2) it appears that all the selected filters are actually needed/useful (3) there are some generalization properties wrt to unseen samples that are close to the sample of interest.  I appreciate that the authors responded with very detailed rebuttals to the concerns of the reviewers. I m still worried, like AnonReviewer4, about the generalization around local regions though the follow up experiments satisfy me for the most part. There is a genuine concern that while this method has the *potential* to produce useful outputs that could be useful for downstream experts to analyze the underlying network, the paper itself doesn t really show this. In other words, while I agree that on the technical side of things, the work passes the bar, it s not clear that the work passes the bar from the impact side of things.  This did make for a genuinely a borderline case in terms of decisions and unfortunately this work landed on the reject side this time around.
This paper provides a method (loss function) for training GAN model for generation of discrete text token generation. The aim of this loss method to control the trade off between quality vs diversity while generating the text data.  The paper is generally well written, but the experimental section is not overly good: Interpretation of the results is missing; error bars are missing. 
The paper touches upon explainable anomaly detection. To that extend, it modified hypersphere classifier towards fully convolutional data description (FCDD). This is, as also pointed out by two of the reviewers a direct application of a fully convolutional network within the hyperspherical classifier. However, the paper  also shows how to then upsample the receptive field using a strided transposed convolution with a fixed Gaussian kernel. Both together with tackling explainable anomaly detection is important. Moreover, the empirical evaluation is quite exhaustive and shows several benefits compared to state of the art. So, yes, incremental, but incremental for a very interesting an important case. 
This paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement.  Reviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification. While many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly. AC believes that the updated version is acceptable.  Hence I recommend acceptance.
The paper studies an elegant formulation of personalized federated learning, which balances between a global model and locally trained models. It then analyzes algorithm variants inspired by local update SGD in this setting. The problem formulation using the explicit trade off between model differences and global objective was received positively, as mentioned by R1 and R2. After a productive discussion including the authors and reviewers, unfortunately consensus remained that the paper remains below the bar in the current form. The contributions are not presented clearly enough in context, the set of competing algorithms (including e.g. EA SGD, ADMM, SVRG/Scaffold for the heterogeneous setting, and others) needs to be clarified in particular for the modified formulation compared to traditional FL, since objectives are different. Some smaller concerns also remained on the applicability to more general non convex settings in practice. We hope the feedback helps to strengthen the paper for a future occasion.
An interesting application of self ensembling/temporal ensembling for visual domain adaptation that achieves state of the art on the visual domain adaptation challenge. Reviewers noted that the approach is quite engineering heavy, but I am not sure it s really much worse than making a pixel to pixel approach work well for domain adaptation.  I hope the authors follow through with their promise to add experiments to the final version (notably the minimal augmentation experiments to show just how much this domain adaptation technique is tailored towards imagenet like things).  As it stands, this paper would be a good contribution to ICLR as it shows an efficient and interesting way to solve a particular visual domain adaptation problem.
The paper addresses the problem of continual learning and solutions based on variational inference. Updates to the paper have improved it and addresses many of the concerns raised by the reviewers during the discussion period.
This paper proposes a new black box adversarial attack based on tiling and evolution strategies. While the experimental results look promising, the main concern of the reviewers is the novelty of the proposed algorithm, and many things need to be improved in terms of clarity and experiments. The paper does not gather sufficient support from the reviewers even after author response. I encourage the authors to improve this paper and resubmit to future conference.
This paper includes an interesting idea of pushing towards good, and away from bad, trajectories, in a natural clean way.  The main problem of the paper is one of clarity.  The paper could be written to be more concise and clear, which would allow, for instance, for sufficient space for the figures (which are currently sometimes rather tiny) as well as not having to fiddle with the margins and spaces quite as much as the current submission seems to do (which would be strictly disallowed at most conferences).  The issue of clarity was also clear during discussion, where sometimes multiple rounds of clarifications were needed to allow the reviewers to correctly interpret parts.  For these reasons, I recommend that the authors resubmit a new, cleaned up, version of the work, with all the changes neatly incorporated.  Then I think this could make for a nice addition to the literature.  I appreciate this will be a disappointment to the authors, but I think ultimately it will make their work more impactful, and longer lasting.
This paper develops a stagewise optimization framework for solving non smooth and non convex problems. The idea is to use  standard convex solvers to iteratively optimize a regularized objective with penalty centered at previous iterates   which is standard in many proximal methods. The paper combines this with the analysis for non smooth functions giving a more general convergence results. Reviewers agree on the usefulness and novelty of the contribution. Initially there were concerns about lack of comparison with current results, but updated version have addressed this issue.  The main weakness is that the results only holds for \mu weekly convex functions and the algorithm depends on the knowledge of \mu. Despite this limitations, reviewers believe that the paper has enough new material and I suggest for publication. I suggest authors to address these issues in the final version. 
Standard algorithms for deep hypergraph learning have not been designed for hypergraphs with edge dependent vertex weights (EDVWs), where the weight of a vertex can depend on the edge of which it isa member. This paper develops a connection between EDVW hypergraphs and undirected simple graphs, thus enabling the use of existing undirected graph neural networks as subroutines. This is done via a unified random walk framework.  (Two typos: ``equivalency" should be ``equivalence", and ``undigraphs" should be ``undirected graphs".)   The theory of equivalence between EDVW hypergraphs and undirected graphs via random walks is a good contribution. The experimentation across different domains is laudable.   However, there are concerns over the lack of key baselines in the experiments. The author rebuttal has presented additional results with some baselines: sensitive hyperparameters (e.g., learning rate) are not tuned for the baselines. The clarity of the paper is mixed. The map from hypergraphs to graphs is not injective, so there could be ambiguity issues (different hypergraphs mapped to the same graph, thus having the same representations).   Also, the contributions of Section 3 (designed for simple undirected graphs alone) do not appear significantly novel.
This paper proposes  a new mechanism, called HIRE, to  improve the down stream performance of a pre trained Transformer on NLP tasks. Different from directly using the last layer of transformer, the proposed model allows the system to dynamically decide which intermediate layers to use based on the input through some sort of gating. The model is evaluated on GLUE, a benchmark for natural language understanding.  My major concerns are the following  1.  the gating mechanism on using intermediate sentence representation is not new,  as pointed by some reviewers, although its implementation on transformers is still interesting. 2.  the empirical part is not convincing enough:  a) GLUE data set is relatively simple, the authors should try something more complex, b）the improvement over baseline is rather modest, which could be achieved with simpler modification.  I d suggest to reject this paper.
This paper proposes environment fields, a representation that models reaching distances within a scene. Dense environment fields are learnt using a neural network, and the effectiveness of this representation is shown on 2D maze environments and 3D indoor environments. This paper received hugely contrasting reviews, with two reviewers being very supportive and one reviewer providing the lowest score of 1. In light of this, I ll start with providing my takeaways on the review and discussion with reviewer z3Y4 (rating of 1) and then proceed to the remaining discussion.  Reviewer z3Y4 has provided the score of 1 and has made strong remarks that include: "what is proposed in this paper is simply not comprehensible", "description of the method itself is simply devoid of all required detail", "The main claims of the paper are incorrect or not at all supported by theory or empirical results." and " what is being proposed in this paper is simply too unclear and vague to be assessed". **Such dismissive remarks, in my opinion, are completely unnecessary and create a toxic discussion and review environment.**  Reviewer z3Y4 has many criticisms of the submission, but the primary ones include: (a) the lack of details throughout the paper (b) the positioning of the paper in the abstract and introduction, and (c) the lack of experiments in continuous environments. Re (a): It is well understood in our research community that providing every last detail in the main submission is nearly impossible due to the restriction on the number of pages. Providing excess details in the main paper also often reduces the readability of the paper. Such details are better addressed in the appendix and crucially, the code. The authors have provided some details in the appendix and have indicated that they will release a code base.  I also agree with the authors that justifying every last detail in the network architecture such as choice of an activation function is not necessary for this submission. The same goes with describing methods in past works in detail vs referring the reader to the appropriate citation. As a result, I believe that the authors have addressed (a) well. Re (b): This has also been addressed by the authors, by pointing out relevant parts of the paper that had the necessary details. Re (c): In this regard, the paper clearly contains a well laid out experiment in 3D indoor scenes, so as far as I am concerned, this has been addressed in the main submission.  Reviewers AhgQ and fAEP have supported this submission but also laid out some concerns that include: (1) Are the gradients suboptimal ? (2) Positioning the paper with regards to past works (3) Motivation behind using the VAE (4) Qualitative analysis and failures The authors have addressed these 4 concerns well using the rebuttal as well as via a revision of the appendix. The reviewers, post discussion have indicated their satisfaction with the revised submission.  I think this paper is interesting and proposes a novel scene representation which can be useful for others in the Embodied AI community. I am in agreement with reviewers AhgQ and fAEP, and in spite of the strong reject score by z3Y4, I recommend accepting this paper.
Two reviewers recommend rejection, whereas two reviewers slightly lean towards acceptance. All reviewers agree that the paper tackles an important problem, and the proposed direction holds promise and is worth exploring. However, the reviewers raised concerns about the novelty of the proposed approach [R3,R4], the applicability of sparsification to GCN based models [R3,R4], baseline experiments [R1,R3,R4] and the gap between the theoretical aspect of the paper and the implementation of the proposed approach [R2]. The authors engaged with the reviewers during the discussion period and succeeded in motivating the speedup gains of their method, and clarifying some of the reviewer s concerns. However, after discussion, the reviewers still think this is a borderline paper, which could be significantly strengthened by validating the applicability of the proposed sparsification to other GNNs [R1,R2,R3], and in particular, by including the suggested FastGAT sparsified GCN experiment [R1,R3,R4]. The paper could also benefit from improving the presentation of both the analyzed approach and the practical one [R2]. I agree with reviewers  assessment and therefore must reject. However, I acknowledge that the paper does raise notable interest and I encourage the authors to consider the reviewers  suggestions in future iterations of their work.
This paper presents a novel black box adversarial attack algorithm, which exploits a sign based rather than magnitude based, gradient estimator for black box optimization. It also adaptively constructs queries to estimate the gradient. The proposed approach outperforms many state of the art black box attack methods in terms of  query complexity. There is a unanimous agreement to accept this paper.
This paper proposes a modification of RNN that does not suffer from vanishing and exploding gradient problems. The proposed model, GATO partitions the RNN hidden state into two channels, and both are updated by the previous state. This model ensures that the state in one of the parts is time independent by using residual connections.   The reviews are mixed for this paper, but the general consensus was that the experiments could be better (baseline comparisons could have been fairer). The reviewers have low confidence in the revised/updated results. Moreover, it remains unclear what the critical components are that make things work. It would be great to read a paper and understand why something works and not that something works.   Overall: Nice idea, but the paper is not quite ready yet.  
This paper studies Population Based Augmentation in the context of knowledge distillation (KD) and proposes a role wise data augmentation schemes for improved KD. While the reviewers believe that there is some merit in the proposed approach, its incremental nature and inherent complexity require a cleaner exposition and a stronger empirical evaluation on additional data sets. I will hence recommend the rejection of this manuscript in the current state. Nevertheless, applying PBA to KD seems to be an interesting direction and we encourage the authors to add the missing experiments and to carefully incorporate the reviewer feedback to improve the manuscript.
This paper presents an interesting idea that is related to imitation learning, safe exploration, and intrinsic motivation. However, in its current state the paper needs improvement in clarity. There are also some concerns about the number of hyperparameters involved. Finally, the experimental results are not completely convincing and should reflect existing baselines in one of the areas described above. 
All reviewers agree that the paper is below the acceptance threshold and the authors did not respond to the reviews. In summary, this is a clear reject
This paper shows that SLGD can be non private (in the sense of differential privacy) even when a single step satisfies DP and also when sampling from the true posterior distribution is DP. I believe that it is useful to understand the behavior of SLGD in the intermediate regime. At the same time the primary question is whether SLGD is DP when the parameters are chosen so as to achieve some meaningful approximation guarantees after some fixed number of steps T and the algorithm achieves them while satisfying DP (but at the same does not satisfy DP for some number of step T  >T). Otherwise the setting is somewhat artificial and I find the result to be less interesting and surprising. So while I think the overall direction of this work is interesting I believe it needs to be strengthened to be sufficiently compelling.
This paper provides a data driven approach that learns to improve the accuracy of numerical solvers. It solves an important problem and provides some promising direction. However, the presented paper is not novel in terms of ML methodology. The presentation can be significantly improved for ML audience (e.g., it would be preferred to explicitly state the problem setting in the beginning of Section 3).
The paper proposes a meta learning algorithm to learn the divergence measure of variational inference as well as the initialization of the variational parameters (which reduces optimization steps of VI). Improved performance by the learned divergence against hand designed ones are empirically shown on: Gaussian mixture approximation, Bayesian neural regression, and p VAE based recommender. Reviewers initally raised some concerns on hyperparameters selection, weakness of experiments, and motivation for the proposed scheme. The authors responded by adding additional experiments (MNIST) as well as some new sections in their appendix about details of their method or the baselines. The reviewers greatly appreciated the response and commonly believed that the revised version is significantly improved over the initial draft  and the improvements of the draft. As a result of that, some reviewers increased their scores. However, some of their concerns did not resolve. In particular, R1 questions the impact of the work and importance of learning divergence measure (referring to GAN or VQ VAE for obtaining realistic samples). Also R1 finds evaluation based on MNIST unsatisfactory, as it is commonly considered as a toy dataset. To motivate the method, it is suggested that the authors think about real applications which can highlight the benefits of their method in practice. Similar concerns are shared by R2 after authors  response. In particular, R2 is not convinced about motivation and the necessity of using meta learning for learning the divergence. I suggest authors improve on issues around motivation and support the impact of their scheme in a more practical setting.
The authors develop a certified defense for label flipping attacks (where an adversary can flip labels of a small number of training set samples) based on the randomized smoothing technique developed for certified defenses to adversarial perturbations of the input. The framework applies to least squares classifiers acting on pretrained features learned by a deep network. The authors show that the resulting framework can obtain significant improvements in certified accuracy against targeted label flipping attacks for each test example.  While the paper makes some interesting contributions, the reviewers had the following shared concerns regarding the paper: 1) Reality of threat model: The threat model assumes that the adversary has access to the model and all of the training data (so as to choose which labels to flip), which is very unlikely in practice.  2) Limitation to least squares on pre trained features: The only practical instantiation of the framework presented in the paper is on least squares classifiers acting on pre trained features learned by a deep network.  In the rebuttal phase, the authors clarified some of the more minor concerns raised by the reviewers, but the above concerns remained.  Overall, I feel that this paper is borderline   If the authors extend the applicability of the framework (for example relaxing the restriction on pre training the deep features) and motivating the threat model more strongly, this could be an interesting paper.
Two reviewers increased their scores after considering the responses from the authors, and all reviewers are somewhat positive. However, the increased scores are still 6 only, and as the area chair, I have concerns about the foundations of this research.  The authors write "there is no off the shelf baseline that can automatically disentangle the data from different domains in the open ended regression setting." This is not true for the standard situation of a mixture of regression lines, as in Section 4.1. Completely standard EM (not necessarily hard EM) will solve this problem, as long as the individual lines (sinusoids etc.) can be represented easily by the EM components. Another standard method that should be another baseline is a mixture of experts neural network.   One thing that EM cannot handle is learning the number of components in a mixture model, as in learning the _k_ in _k_ means. To the extent that "open ended" here refers to a new approach for this problem, with mathematical guarantees, it is interesting. But this point of view needs more explanation.  The paper begins "A hallmark of general intelligence is the ability of handling open ended environments, which roughly means complex, diverse environments with no manual task specification." If there is one aspect of natural environments that is crucial and fundamental, it is the presence of noise. However, starting theoretically with Definition 1 and empirically with Section 4.1, the authors work in a world of deterministic functions. This mismatch undermines the conceptual significance of the paper.  Perhaps because of the universality of noise, the authors do not present a real world dataset or task for which the OSL method is directly natural or applicable. Rather, they impose restricted specifications on datasets such as MNIST and introduce metrics that are novel, hence hardly natural, undermining the empirical significance of the paper.
The paper evaluates several off the shelf algorithms for predicting the return on real estate property investment. The problem is interesting, but there is a consensus that the paper contains little technical novelty, and the empirical study on a fairly small dataset is also not convincing. 
This paper presents "BabyAI", a research platform to support grounded language learning. The platform supports a suite of 19 levels, based on *synthetic* natural language of increasing difficulties. The platform uniquely supports simulated "human in the loop" learning, where a human teacher is simulated as a heuristic expert agent speaking in synthetic language.     Pros: A new platform to support grounded natural language learning with 19 levels of increasing difficulties. The platform also supports a heuristic expert agent to simulate a human teacher, which aims to mimic "human in the loop" learning. The platform seems to be the result of a substantial amount of engineering, thus nontrivial to develop. While not representing the real communication or true natural language, the platform is likely to be useful for DL/RL researchers to perform prototype research on interactive and grounded language learning.   Cons: Everything in the presented platform is based on synthetic natural language. While the use of synthetic language is not entirely satisfactory, such limit is relatively common among the simulation environments available today, and lifting that limitation is not straightforward. The primary contribution of the paper is a new platform (resource). There are no insights or methods.  Verdict: Potential weak accept. The potential impact of this work is that the platform will likely be useful for DL/RL research on interactive and grounded language learning.
This paper considers the problem of searching over the joint space of hardware and neural architectures to trade off accuracy and latency.   Reviewers raised some valid questions about the following aspects: 1. Low technical novelty 2. Prior work on hardware and neural architecture co design, and closely related work are not addressed 3. Lacking details on hardware platform and discussion on physical constraints to determine invalid hardware designs (addressed somewhat, but the response is not satisfactory)  One additional comment: if we care about latency for a particular hardware platform, it is possible to automatically configure adaptive inference techniques to meet the latency constraints.   Overall, my assessment is that the paper requires more work before it is ready for publication.
The paper introduces unsupervised skill discovery using Lipschitz constrained skills. It is well written and demonstrates the advantages in a solid experimental section.
The authors present a new memory augmented neural network that is related to the Kanerva machine of Wu et. al.  The reviewers considered the ideas in the paper novel and interesting, but were concerned about presentation issues and literature review.  The authors have improved both... however  authors: please even under limited space constraints, make more room for related work!  Clarifying your contribution in the context of the literature is critical for reader understanding, and neglecting this almost had your paper rejected out of hand.    I am voting to accept
Strengths: The method extends [21], which proposes an unordered set prediction model for multi class classification. The submission proposes a formulation to learn the distribution over unobservable permutation variables based on deep networks and uses a MAP  estimator for inference. While the failure of NMS to detect overlapping objects is expected, the experiments showing that perm set prediction handles them well is interesting and promising.   Weaknesses:  Reviewer 1: "I find the paper still too scattered, trying to solve diverse problems with a hammer without properly motivating / analyzing key details of this hammer. So I keep my rating." Reviewer 2: "I m glad that the authors are seeing good performance and seem to have an effective method for matching outputs to fixed predictions, however the quality of the paper is too poor for publication."  Points of contention:   Although there was one reviewer who gave a high rating, they were not responsive in the rebuttal phase.  The other two reviewers took into account the author responses, and a contributed comment by an unaffiliated reviewer, and both concluded that the paper still had serious issues.  The main issues were: lack of clear methodology and poor clarity (AnonReviewer2), and poor organization and lack of motivation for modeling choices (AnonReviewer1).
The authors present a method for learning representations of remote sensing images from multiple views. The main ideas is to use the InfoNCE loss to learn from multiple views of the data.   The reviewers had a few concerns about this work which were not adequately addressed by the authors. I have summarised these below and would strongly recommend that the authors address these in subsequent submissions:  1) Experiments on a single dataset and a very specific task: Authors should present a more convincing argument about why the chosen dataset and task are challenging and important to demonstrate the main ideas presented in their work. Further, they should also report results on additional datasets suggested by the reviewers.  2) Comparisons with existing works: The reviewers suggested several existing works for comparison. The authors agreed that these were relevant and important but haven t done this comparison yet. Without such a comparison it is hard to evaluate the main contributions of this work.   Based on the above objections raised by the reviewers, I recommend that the paper should not be accepted.
The paper presents a variance reduction technique to the Straight Through version of the Gumbel Softmax estimator. The technique is relying on the truncated Gumbel of Maddison et al. I share the excitement of the reviewers about this work and I expect this technique to further influence the field. 
The paper presents a variant of network morphism (Wei et al., 2016) for dynamically growing deep neural networks. There are some novel contributions (such as OptGD for finding a morphism given the parent network layer). However, in the current form, the experiments mostly focus on comparisons against fixed network structure (but this doesn t seem like a strong baseline, given Wei et al. s work), so the paper should provide more comparisons against Wei et al. (2016) to highlight the contribution of this work. In addition, the results will be more convincing if the state of the art performance can be demonstrated for large scale problems (such as ImageNet classification). 
Initially, we had some borderline scores for this paper. After the (indeed very convincing!) rebuttal and a the end of the discussion phase, however, all reviewers agreed that this is a very solid piece of work, with significant methodological and practical contributions. I fully share this positive impression of the paper!.
This paper investigates a tighter bound for mutual information and proposes some novel estimators of MI from the importance sampling perspective. The proposed approach provides a unifying framework for mutual information bounds that can deduce many existing approaches. The theoretical and experimental analyses well justify the proposed approach and shows the bounds are much tighter than the existing ones.   Overall, this paper is well written. The relevant literatures are exhaustively reviewed and well compared with the proposed method. The experimental results show remarkable superiority of the proposed method. The proposed framework would shed light on the literatures and open up a new direction of the relevant researches.   For those reasons, I would like to recommend this paper to be accepted by ICLR2022 conference.
The paper investigates the capacity for neural language models to perform fast mapping word acquisition using a proposed multimodal external memory architecture. Much work exists that shows that neural models are capable of following instructions whose meaning persists across episodes (i.e., slow learning), however much less attention has been paid to instruction following in a one shot learning context. Using a simulated 3D navigation/manipulation domain, the paper shows that the proposed multimodal memory network is capable of both slow and one shot word learning when trained via standard RL.  The submission was reviewed by four knowledgable referees, who read the author feedback and engaged in discussion with the authors. The paper is topical one shot language learning for instruction following using neural models is of significant interest of late. The reviewers agree that the proposed multimodal memory architecture is both interesting and technically solid. The reviewers raised concerns about the experimental evaluation and the role of embodiment. The author feedback together with discussion with reviewers were helpful in resolving some of these issues. However, the authors are encouraged to ensure that the paper clearly motivates the importance of embodiment to slow learning and fast mapping, particularly given the large body of work in language acquisition in robotics, a truly embodied domain, which is notably missing from the related work discussion.
This paper proposes a graph information bottleneck (GIB) framework for subgraph recognition, including the proposal of a MI objective as well as a bi level optimization scheme for minimizing said objective. The paper receive mixed reviews, with two reviewers in favor of acceptance and two reviewers in favor of rejection.   One negative reviewer was too short to judge and had low confidence. I think most of the concerns arise from lack of understanding of the work and the authors adequately address this on the rebuttal. The authors are encouraged to make minor modifications for clarity. In particular, classical IB considers random variables x, z, y, and learns latent representation z that is maximally informative about output y and sufficiently informative about input x. Therefore, it is natural to expect that the input to GIB is a random graph.   The other negative reviewer finds the paper lacks novelty and points to multiple references. The positive reviewers also ask about the connection with additional references. Im my opinion, the authors do an excellent job at clarifying the differences with all prior work mentioned by the reviewers, including the closest one, a GIB paper in NerurIPS 2020. In my view, the present submission contains sufficient novelty relative to prior work, specifically as it focuses on a different problem (sub graph) and proposes a different optimization method. That being said, I think it is absolutely essential that the author responses be added to the paper. In other words, the final version must add citations to the relevant work mentioned by the reviewers and clarify the differences.  All other comments from the two remaining reviewers are very positive: the reviewers find the paper contributes with "quite interesting information theoretic objective functions that actually work on multiple graph learning tasks" and "makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation." I share the views of the positive reviewers and recommend acceptance, subject to the authors incorporating their responses to the reviewers  comments.
This paper is borderline for publication for the following reasons: 1) the title is misleading. The majority of the ICLR audience understands by "spatial structure" the structure of the external 3D world, as opposed to the position of the sensors in the internal coordinate system of the agent. Though the authors argue that knowing the positions of the sensors eventually leads to learning the 3D world structure, this appears like a leap in the argument.  2) The equation s \phi(m) described a mapping from robot postures to sensory states. This means the agent should remain within the same scene. The description of this equation in the manuscript as "The mapping  can be seen as describing how “the world” transforms changes in motor states into  changes in sensory states ..." makes this equation appear more general than what it is. s  \psi(s,m) would be better described by such sentence.   
Initially there were some shared concerns about the work being too incremental, lack of technical clarity on the algorithmic side and experiments, and lack of clear mathematical formulations. The authors did a good effort and cleared up many questions and remarks satisfactorily, and several reviewers have increased their scores as a consequence. In its current state I recommend to accept the paper.
The paper makes two fairly incremental contributions regarding training binarized neural networks: (1) the swish based STE, and (2) a regularization that pushes weights to take on values in { 1, +1}. Reviewer1 and reviewer2 both pointed out concerns about the incremental contribution, the thoroughness of the evaluation, the poor clarity and consistency of the writing. Reviewer3 was muted during the discussion. Given the valid concerns from reviewer1/2, this paper is recommended for rejection. 
This paper provides further analysis of convergence in deep linear networks. I recommend acceptance. 
The paper studies whether the best strategy for transfer learning in RL is to transfer value estimates or policy probabilities. The paper also presents a model based value centric (MVC) framework for continuous RL. The reviewers raised concerns regarding (1) the coherence of the story, (2) the novelty and importance of the MVC framework and (3) the significance of the experiments. I encourage the authors to either focus on the algorithmic aspect or the transfer learning aspect and expand on the experimental results to make  them more convincing. I appreciate the changes made to improve the paper, but in its current form the paper is still below the acceptance threshold at ICLR.  PS: in my view one can think of value as (shifted and scaled) log of policy. Hence, it is a bit ambiguous to ask whether to transfer value or policy.
The authors propose a notion of feature robustness, provide a straightforward decomposition of risk in terms of this robustness measure, and then provide some empirical evidence for their perspective. Across the board, the reviewers raised issues with missing related work, which the authors then addressed. I will point out that some things the authors say about PAC Bayes are false. E.g., in the rebuttal the authors say that PAC Bayes is limited to 0 1 error. It is generally trivial to obtain bounds for bounded loss. For unbounded loss functions, there are bounds based on, e.g., sub gaussian assumptions.   Despite improvements in connections with related work, reviewers continued to find the theoretical contributions to be marginal. Even the empirical contributions were found to be marginal.
The paper proposes a parallelization approach for speeding up scheduled sampling, and show significant improvement over the original.  The approach is simple and a clear improvement over vanilla schedule sampling.  However, the reviewers point out that there are more recent methods to compare against or combine with, and that the paper is a bit thin on content and could have addressed this.  The proposed approach may well combine well with newer techniques, but I tend to agree that this should be tested.
This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc. Although the novelty of the paper may be limited, empirical performance seems impressive. The source code is not available. I think this is a borderline paper but maybe good enough for acceptance. 
This paper proposes reducing so called "negative transfer" through adversarial feature learning. The application of DANN for this task is new. However, the problem setting and particular assumptions are not sufficiently justified. As commented by the reviewers and acknowledged by the authors there is miscommunication about the basic premise of negative transfer and the main assumptions about the target distribution and it s label distribution need further justification. The authors are advised to restructure their manuscript so as to clarify the main contribution, assumptions, and motivation for their problem statement.  In addition, the paper in it s current form is lacking sufficient experimental evidence to conclude that the proposed approach is preferable compared to prior work (such as Li 2018 and Zhang 2018) and lacks the proper ablation to conclude that the elimination of negative transfer is the main source of improvements.   We encourage the authors to improve these aspects of the work and resubmit to a future venue. 
This paper proposes to build an  imitative model  to improve the performance for imitation learning. The main idea is to combine the model based RL type of work to the imitation learning approach. The model is trained using a probabilistic method and can help the agent imitate goals that were previously not easy to achieve with previous works.  Reviewers 2 and 3 strongly agree that the paper should be accepted. R3 has increased their score after the rebuttal, and the authors  response helped in this case. Based on reviewers score, I recommend to accept this paper.
This paper is a resource and numerical investigation into the variability of BERT checkpoints. It also provides a bootstrap method for making investigations on the checkpoints.  All reviewers appreciate this contribution that can be expected to be used by the NLP community.
The reviewers unanimously recommended that this paper be accepted, as it contains an important theoretical result that there are problems for which heavy ball momentum cannot outperform SGD. The theory is backed up by solid experimental results, and the writing is clear. While the reviewers were originally concerned that the paper was missing a discussion of some related algorithms (ASVRG and ASDCA) that were handled in discussion. 
All reviewers agree that this paper is very solid work, that presents great progress in no press diplomacy. The method and presented experiments are of very good quality and the work merits to be presented at ICLR.
The paper proposes a neural network architecture that uses a hypernetwork (RNN or feedforward) to generate weights for a network (variational RNN), that models sequential data. An empirical comparison of a large number of configurations on synthetic and real world data show the promise of this method.  The authors have been very responsive during the discussion period, and generated many new results to address some reviewer concerns. Apart from one reviewer, the others did not engage in further discussion in response to the authors updating their paper.  The paper provides a tweak to the hypernetwork idea for modeling sequential data. There are many strong submissions at ICLR this year on RNNs, and the submission in its current state unfortunately does not pass the threshold.
Strengths:  The proposed method is relatively principled.  The paper also demonstrates a new ability: training VAEs with autoregressive decoders that have meaningful latents.  The paper is clear and easy to read.  Weaknesses:  I wasn t entirely convinced by the causal/anticausal formulation, and it s a bit unfortunate that the decoder couldn t have been copied without modification from another paper.  Points of contention: It s not clear how general the proposed approach is, or how important the causal/anti causal idea was, although the authors added an ablation study to check this last question.  Consensus:  All reviewers rated the paper above the bar, and the objections of the two 6 s seem to have been satisfactorily addressed by the rebuttal and paper update.
Reviewers uniformly suggest acceptance. Please look carefully at reviewer comments and address in the camera ready. Great work!
Most of the reviewers and AC found many claims of this submission unsubstantiated. 
The paper addresses questions on the relationship between model free and model based reinforcement learning, in particular focusing on planning using learned generative models. The proposed approach, GATS, uses learned generative models for rollouts in MCTS, and provide theoretical insights that show a favorable bias variance tradeoff. Despite this theoretical advantage, and high quality models, the proposed approach fails to perform well empirically. This surprising negative results motivates the paper and providing insights on it is the main contribution.  Based on the initial submitted version, the reviewers positively emphasized the need to understand and publish important negative results. All reviewers and the AC appreciate the import role that such a contribution can bring to the research community. Reviewers also note the careful discussion of modeling choices for the generative models.   The reviewers also noted several potential weaknesses. Central were the need to better motivate and investigate the hypothesis proposed to explain the negative results. Several avenues towards a better understanding were proposed, and many of these were picked up by the authors in the revision and rebuttal. A novel toy domain "goldfish and gold bucket" was introduced for empirical analysis, and experiments there show that GATS can outperform DQN when a longer planning horizon is used.   The introduced toy domain provides additional insights into the relationship between planning horizon and GATS / MCTS performance. However, it does not address key questions around why the negative result is maintained. The authors hypothesize that the Q value is less accurate in the GATS setting   this is something that can be empirically evaluated, but specific evidence for this hypothesis is not clearly shown. Other forms of analysis that could shed further light on why the specific negative result occurs could be to inspect model errors. For example, if generated frames are sorted by the magnitude of prediction errors   what are the largest mistakes? Could these cause learning performance to deteriorate?  The reviewers also raised several issues around the theoretical analysis, clarity (especially of captions) and structure   these were largely addressed by the revision. The concern that most strongly affected the final evaluation is the limited insight (and evidence) of the factors that influence performance of the proposed approach. Due to this, the consensus is to not accept the paper for publication at ICLR at this stage.
This paper discusses a relatively new concept called "magnitude" for finite metric spaces and investigates its potential applications in machine learning, in particular for computer vision.  Reviewers generally agree that this is an interesting concept and appreciate the algorithm for reducing its computational cost.  However, there are concerns (1) that the concept is not well motivated theoretically for machine learning problems  (2) the experimental results, for edge detection and adversarial robustness, are not convincing. More rigorous empirical work should be carried out.
The paper investigates the tendency of image recognition models to depend on image backgrounds, and propose a suite of datasets to study this phenomenon.  All the reviewers agree that the paper investigates an important problem, is well written and contains several interesting insights that should be of interest to the community. I recommend acceptance. 
This paper tackles the problem of exploration using intrinsic rewards in RL in states that have never been encountered before. The authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement, which estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples. The intrinsic reward resulting from the so called DISDAIN (discriminator disagreement intrinsic reward) exploration bonus is more tailored to the true objective compared to pseudocount based methods.  Reviewers agree that the paper is well motivated and well written, that the proposed DISDAIN exploration method is simple and practical, and that experiments are convincing. Experiments on continuous control tasks such as MuJoCo locomotion environments could have strengthened the paper further.
Based on the previously observed neural collapse phenomenon that the features learned by over parameterized classification networks show an interesting clustering property, this paper provides an explanation for this behavior by studying the transfer learning capability of foundation models for few shot downstream tasks. Both theoretical and empirical justifications are presented to elaborate that neural collapse generalizes to new samples from the training classes, and to new classes as well.  The problem that this paper delves into is important. The paper is well motivated, and well structured with a good flow. Both theoretical and empirical analyses of the paper are solid. Preliminary ratings are mixed, but during rebuttal, multi round responses and in depth discussions were carried out between authors and reviewers, and the final scores are all positive with major concerns well addressed. AC considers the paper itself and all relevant threads, and recommends the paper for acceptance. Authors shall incorporate all response materials into the future version.
This paper clearly surveys a set of methods related to using generative models to produce samples with desired characteristics.  It explores several approaches and extensions to the standard recipe to try to address some weaknesses.  It also demonstrates a wide variety of tasks.  The exposition and figures are well done.
This paper addresses unique windowing schemes for the input of an LSTM model for time series forecasting, in particular an exponential partitioning, where bin sizes increase as moving further from the current time point. Although the basic idea is interesting and motivating and experimental results are strong; as reviewers pointed out, technical significance and novelty are limited because of lack of theoretical or conceptual justification and motivation the proposed approach.  The authors’ claim is primarily based on experiments results. Other critical issues include the lack of comparison with recent advances in specifically designed to attend to longer history length or the discussion of modern approaches. other issues include presentation (e.g., grammatical errors) and the use of acronyms before introducing them.
This work uses a variational autoencoder based approach to combine the benefits of recent methods that learn policies with behavioral diversity with the advantages of successor representations, addressing the generalization and slow inference problems of competing methods such as DIAYN.  After discussion of the author rebuttal, the reviewers all agreed on the significant contribution of the paper and that concerns about clarity were sufficiently addressed.  Thus, I recommend this paper for acceptance.
The paper seeks to increase receptive fields of GNNs by aggregating information beyond local neighborhoods with the idea of addressing oversmoothing and/or overfitting issues with message passing algorithms. The proposed method is simple and primarily makes use of node features and local structure similarities. In this sense the approach is related to Pei et al. Several concerns remained as articulated in the reviews, including: oversmoothing is not discussed/analyzed, performance gains are small, more extensive comparisons are needed.  
The paper got a quite high disagreement in the scores from the reviewers. R2 voted for rejecting the paper as he did not see the connection of the algorithm to the continuation method and also that the continuation method does not address the distributional shift, which is one of the main problems for offlline RL. Yet, these concerns have been properly answered in the rebuttal of the authors and the distributional shift is also addressed by the continuation method by reducing the error in policy evaluation. Further concerns from the reviewers were raised in terms of related work to a similar algorithm (BRAC), which is also addressed in the revision of the paper.   The reviewers also identified the following strong points of the paper:   The algorithm is a simple and very effective adaptation to SAC    The presented results are exhaustive and convincing   The paper provides strong theoretical results for the presented algorithm   The authors did a very good job with their revision, adding more comparisons and ablation studies.  I agree that this paper very interesting and recommend acceptance.
After reading the author s rebuttal, the reviewer still hold that the main contribution is just the simple combination of already known losses. And the paper need to pay more attention on the clarity of the paper.
In this paper, the authors proposed a method to handle the problem of LUMP GNN architecture. This problem is indeed important and the proposed method has some merits. However, the proposed approach is only applicable to node classification.  Moreover, the proposed approach shows the similar theoretical results of Sato et al 2020. In the paper, it can be applicable to any GNN tasks because it only adds random features to each node. Therefore, the novelty of the proposed method is limited.  I encourage authors to revise the paper based on the reviewer s comments and resubmit it to a future venue. 
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.     The approach is novel   The experimental results are convincing.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The authors didn t show results with non Gaussian noise   Some details that could help the understanding of the method are missing.   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
Strong submission that analyses the unsupervised skill discovery setting from the perspective of information geometry, which leads to some interesting conclusions. In particular, it is shown that this does not lead to skills that are optimal for all reward functions, but does provide a good initialization for methods that aim to find optimal policies.   Across the board, the reviewers believe the analysis provided by this work is both important and novel. And while there were some initial concerns raised, such as lack of empirical confirmation of some of the claims and some questions about the analysis, the authors have addressed all of these concerns convincingly.   Hence, I strongly recommend acceptance of this submission.
The extension of convnets to non Euclidean data is a major theme of research in computer vision and signal processing.  This paper is concerned with Graph structured datasets. The main idea seems to be interesting: to improve graph neural nets by first embedding the graph in a Euclidean space reducing it to a point cloud, and then exploiting the induced topological structure implicit in the point cloud.   However, all reviewers found this paper hard to read and improperly motivated due to poor writing quality. The experimental results are somewhat promising but not completely convincing, and the proposed framework lacks a solid theoretical footing. Hence, the AC cannot recommend acceptance at ICLR 2019.  
This paper proposes an anomaly detection approach by augmenting VAE encoder with a network multiple hypothesis network and then using a discriminator in the decoder to select one of the hypothesis. The idea is interesting although the reviewers found the paper to be poorly written and the approach to be a bit confusing and complicated.  Revisions and rebuttal have certainly helped to improve the quality of the work. However, the reviewers believe that the paper require more work before it can be accepted at ICLR. For this reason, I recommend to reject this paper in its current state. 
This paper presents a new method for performing Bayesian optimization for hyperparameter tuning that uses learning curve trajectories to reason about how long to train a model for (thus "grey box" optimization) and whether to continue training a model.  The reviewers seem to find the paper clear, well motivated and the presented methodology sensible.  However, the reviews were quite mixed and leaning towards reject with 3, 6, 5, 3, 6.  A challenge for the authors is that there is already significant related literature on the subject of multi fidelity optimization and even specific formulations for hyperparameter optimization that reason about learning curves.  A common criticism raised by the reviewers is that while there are extensive experiments, they don t seem to be the right choice of experiments to help understand the advantages of this method (e.g. epochs instead of wall clock on the x axis, choice of baselines, demonstration that early results are used to forecast later success, etc.).  Unfortunately, because there is significant related literature, the bar is raised somewhat in terms of empirical evidence (although theoretical evidence of the performance of this method would also help).  It seems clear that some of the reviewers are not convinced by the experiments that were presented.  Thus the recommendation is to reject the paper but encourage the authors to submit to a future venue.  It looks like the authors have gone a long way to address these concerns in their author responses.  Incorporating these new results and the reviewer feedback would go a long way to improving the paper for a future submission.
In this work the authors build on the Dirichlet prior network of Malinin & Gales, replacing the loss function and adding a regularization term which improve training in the setting with a significant number of classes.   Improving uncertainty for deep learning is a challenging but very important problem.  The reviewers of this paper gave two weak rejects (one is of low confidence) and one weak accept.  They found the paper well written, easy to follow and well motivated but somewhat incremental and not entirely empirically justified.  None of the reviewers were willing to strongly champion the paper for acceptance.  Unfortunately as such the paper falls below the bar for acceptance.  It appears that the authors significantly added to the experiments in the discussion phase and hopefully that will make the paper much stronger for a future submission.
This paper proposes an outlier detection method that maps outliers to low probability regions of the latent space. The novelty is in proposing a weighted reconstruction error penalizing the mapping of outliers into high probability regions. The reviewers find the idea promising. They have also raised several questions. It seems the questions are at least partially addressed in the rebuttal, and as a result one of our expert reviewers (R5) has increased their score from WR to WA. But since we did not have a champion for this paper and its overall score is not high enough, I can only recommend a reject at this stage.
The paper proposes an improved method for uncertainty estimation in deep neural networks.  Reviewer 2 and AC note that the paper is a bit isolated in terms of comparing the literature.  However, as all of reviewers and AC found, the paper is well written and the proposed idea is clearly new/interesting.
This paper proposes a self supervised auto encoder latent image animator that animates images via latent space navigation. The task of transferring motion from driving videos to source images is formulated as learning linear transformations in the latent space. Experiments conducted on real world videos demonstrate that the proposed framework can successfully animate still images. The proposed framework is novel, the experimental results are supportive and promising. However, some related works are still missing and might need to be added to the current paper for discussion and comparison.   The rebuttal has addressed all major concerns raised by all 5 reviewers. The revised paper also included some feedback from the reviewers, except those discussions and comparisons with some missing related works pointed out by reviewers. After the rebuttal, all reviewers tend to accept the paper. AC agrees with the reviewers and recommends accepting the paper as a poster. Lastly, AC urges the authors to further improve their paper by incorporating the discussion on other missing related works suggested by the reviewers.
The authors study co ordination in multi agent systems. Specifically they propose a scheme where agents model future trajectories through the environment dynamics and other agents  actions, they then use this to form a plan which forms the agents  intention which is then communicated to the other agents.  The major concerns raised by the reviewers were around novelty, lack of ablations and significance of results as improvements were modest. During the rebuttal, the authors have extended their work with ablations and have conducted a statistical test. While it is true the current results present a small improvement, i think this is an interesting contribution in the field of emergent communication
This paper presents a very creative threat model for neural networks.  The proposed attack requires systems level intervention by the attacker, which prompts the reviewers to question how realistic the attack is, and whether it is well motivated by the authors.  After conversing with the reviewers on this topic, they have not changed their mind about these issues.  As an AC, I think the threat model is both interesting and potentially realistic in some scenarios, however I agree with the reviewers that the motivation for the threat model could be more powerful.  For example the authors could focus more on realistic types of malicious behaviors that a developer could embed into a neural network.  I also think there s lots of opportunities for a range of applications that exploit the type of "two nets in one" behavior that the authors study.  Despite the interesting ideas in this paper, the post rebuttal scores are not strong enough to accept it.  I encourage the authors to address some of these presentation issues, and resubmit this interesting paper to another venue.
This paper investigates the topic of nondeterminism and instability in neural network optimization. The reviewers found the results on different sources of nondeterminism particularly interesting and relevant. The experiments are carried on both language and also vision, which strengthens the findings. Concerns were raised about the use of smaller non standard models, which were somewhat mitigated by the addition of Resnet 18 experiments on CIFAR. The reviewers also noted that the measures used in the experimental protocol were already present in the literature, and that the proposed mitigation strategy is from another work. Furthermore, R2 also found that the optimization instability section should be more developed. The paper should be resubmitted with an improved discussion of related works and more developed section on instability as suggested by the reviewers.
This paper proposes \alphaVIL, a method for weighting the task specific losses in a multi task setting in order to optimize the performance on a particular target task. The idea is to first collect gradient updates for the model based on all the separate tasks, and then re weight those updates in order to optimize the loss on a held out development set for the target task. In practice, this meta optimization is performed with gradient descent. Experiments on multi MNIST and several tasks that are part of GLUE and SuperGLUE show that \alphaVIL is close in performance to a baseline multitask method and discriminative importance weighting.  Strengths:   The idea is intuitively appealing. Directly reweighting tasks as a meta optimization step is straightforward and appears to not be proposed previously in the literature.   The paper is clear in its presentation.  Weaknesses:   The reviewers agree that the main weakness is that the experimental results do not show that \alphaVIL offers any substantial benefits over existing methods. On the multi MNIST task, while \alphaVIL tends to have the highest mean performance, the difference is small (less than a standard deviation). On the GLUE/SuperGLUE tasks, it outperforms other methods on only 1 out of 10 experiments. There are also no confidence intervals/standard deviations provided to assess the significance of the results.
This paper a theoretical interpretation of separation rank as a measure of a recurrent network s ability to capture contextual dependencies in text, and introduces a novel bidirectional NLP variant and tests it on several NLP tasks to verify their analysis.   Reviewer 3 found that the paper does not provide a clear description of the method and that a focus on single message would have worked better. Reviewer 2 made a claim of several shortcomings in the paper relating to lack of clarity, limited details on method, reliance on a  false dichotomy , and failure to report performance. Reviewer 1 found the goals of the work to be interesting but that the paper was not clear, that the proofs were not rigorous enough, and clarity of experiments. The authors responded to the all the comments. The reviewers felt that their comments were still valid and did not adjust their ratings.  Overall, the paper is not yet ready in its current form. We hope that the authors will find valuable feedback for their ongoing research.
This paper proposes an improved (over Andrychowicz et al) meta optimizer that tries to to learn better strategies for training deep machine learning models. The paper was reviewed by three experts, two of whom recommend Weak Reject and one who recommends Reject. The reviewers identify a number of significant concerns, including degree of novelty and contribution, connections to previous work, completeness of experiments, and comparisons to baselines. In light of these reviews and since the authors have unfortunately not provided a response to them, we cannot recommend accepting the paper.
This paper proposes to learn an ensemble of weights given a set of base weights from some point late in normal training. The authors apply this approach to a number of configurations and find modest performance improvements for normal test settings and larger improvements for out of distribution settings. While reviewers had some concerns about the size of the improvement relative to baselines, all reviewers agreed that the proposed method is interesting and will likely impact future work, especially given the new experiments provided by the authors. I recommend that the paper be accepted. 
The paper proposes a contextual reasoning module following the approach proposed by the NIPS 2011 paper for object detection. Although the reviewers find the proposed approach reasonable, the experimental results are weak and noisy. Multiple reviewers believe that the paper will benefit from another review cycle, pointing out that the authors response confirmed that multiple additional (or redoing of) experiments are needed.  
This paper gives a new theoretical tool to connect the gap between the spectral perspective and spatial perspective of graph neural networks. The frame work is considerably broad and can deal with several existing methods. From this view point, the connection between the spatial and spectral perspectives are made explicit while they are noticed in an informal way by existing researches. The frequency response of several methods are analyzed through theories with support by some numerical experiments.  The idea of connecting spatial and spectral perspective would not be entirely new, but the main novelty of this paper is to make it explicit and analyzed the frequency response of well known methods concretely. This is informative to the literature and extends some known results to more general settings. The numerical experiments well justify the plausibility of the theory. For reasons mentioned above, I think this paper is worth publishing in ICLR2021.
The authors define the task of solving a family of differential equations as a task of gradient based meta learning generalizing the gradient based model agnostic meta learning to problems with differentiable solvers. According to the reviews, there were some concerns regarding the practical value of the paper, for example, (1) the proposed technology is restricted to linear systems, and relatively easy problems (2) there is no demonstration of practical application utility (3) It lacks systematic comparison with other methods (4) some technical details are missing. There were quite a lot of discussions on the paper among the reviewers, and the consensus is that the paper is not solid enough for publication at ICLR in its current form (the reviewer who gave the highest score is less confident and does not want to champion the paper).
This paper introduces a form of cubic smoothing for use with ODE RNNs, to remove the jump when new observations occur.  I think this paper s motivation is based on a misunderstanding of what the hidden state of an RNN represents.  Specifically, an RNN hidden state is a belief state, not the estimated state of the system.  I think R2 is right that it s correct for a filter to jump when seeing new data.   It s not a matter of whether the phenomenon being modeled is slow changing or not.  The filtering output is a belief state, which can change instantaneously even if the true state does not.  The important distinction to make is filtering (conditioning only on previous in time data) vs smoothing (conditioning on all data).  The smoothing posterior should generally be smooth if the true state changes slowly.  As R4 notes, all of the tasks are based on interpolation, which is not what the ODE RNN is trying to do, and the proposed method would make the same predictions as a standard ODE RNN.  Finally, as R4 notes, "The authors do not provide any experimentation on real world irregularly sampled time series".
The paper proposes to use reconstruction error of autoencoder as the energy function and normalize the resulting density for detecting anomalous/OOD examples. Reviewers have raised several concerns with the paper, including, lack of insights into why the AE energy is better for OOD detection than other energy function parameterizations, and incremental nature of the proposed method. Authors have not responded to these concerns. The paper is not suitable for publication in its current form. 
The paper proposes a method for learning intristic reward from demonstrations. The inartistic reward is computed as time to reach and generalizes to unseen states.  The reviewers agree that the method is novel useful, and of interest to ICLR community.   Although the authors  significantly improved the manuscript during the rebuttal phase with new results, and addressed many of the reviewers  comments, the overall novelty of the paper is still somewhat limited, making it unsuitable for ICLR in its current form.     The future  version of the paper should address the comments below and go through a detailed pass for clarity.  Additional comments that did not influence the final decision:  The idea of learning temporal distance to the goal is not novel [1], although the application as an intristic reward is. The authors should connect the temporal difference to the reachability theory and solving two point boundary problem for systems with non linear dynamics, as a theoretical foundation of the method [1].  I am curious about the decision to use the time to reach as a reward directly, instead of delta between the states. Some empirical work provides evidence [2,3] that delta yield less side effects in behaviors.   [1] https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber 8772207 [2]https://arxiv.org/abs/1803.10227 [3] https://arxiv.org/abs/2003.06906
This paper aims to improve the training of generative adversarial networks (GANs) by incorporating the principle of contrastive learning into the training of discriminators in GANs. Unlike in an ordinary GAN which seeks to minimize the GAN loss directly, the proposed GAN variant with a contrastive discriminator (ContraD) uses the discriminator network to first learn a contrastive representation from a given set of data augmentations and real/generated examples and then train a discriminator based on the learned contrastive representation. It is noticed that a side effect of such blending is the improvement in contrastive learning as a result of GAN training. The resulting GAN model with a contrastive discriminator is shown to outperform other techniques using data augmentation.  **Strengths:**   * It proposes a new way of training the discriminators of GANs based on the principle of contrastive learning.   * The paper is generally well written to articulate the main points that the authors want to convey.   * The experimental evaluation is well designed and comprehensive.  **Weaknesses:**   * Even though the proposed learning scheme is novel, the building blocks are based on existing techniques in GAN and contrastive learning.   * The claim that GAN helps contrastive learning is not fully substantiated.   * It is claimed in the paper that the proposed contrastive discriminator can lead to much stronger augmentations *without catastrophic forgetting*. However, this “catastrophic forgetting” aspect is not really empirically validated in the experiments.   * The writing has room for improvement.  Despite its weaknesses, this paper explores a novel direction of training GANs that would be of interest to the research community. 
This paper studies the implicit acceleration of gradient flow in over parameterized two layer linear models. The authors show that the amount of acceleration depends on the spectrum of the data without assuming small, balanced, or spectral initialization for the weights, and establish interesting connections between matrix factorization and Riccati differential equations.  While this paper provides some interesting results regarding implicit acceleration in training linear neural networks, the reviewers raised quite a few questions and concerns about some claims made in the paper, as well as an inadequate comparison with previous work. Even after the author s response and reviewer discussion,  the reviewers  doubts are still not completely cleared away. I feel the current form of the paper is slightly below the bar of acceptance, and encourage the authors to carefully address reviewers  comments in the revision.
This paper presents a generalization bound for RNNs based on matrix 1 norm and Fisher Rao norm. As the initial bound relies on non signularity of input covariance, which may not always hold in practice, the authors present additional analysis by noise injection to ensure covariance is positive definite. Through the resulted bound, the paper discusses how weight decay and gradient clipping in the training can help generalization. There were some concerns raised by reviewers, including  rigorous report of the experiment results,  claims on generalization in IMDB experiment,  claims of no explicit dependence on the size of networks, and the relationship of small eigenvalues in input covariance to high frequency features. The authors responded to these and also revised their draft to address most of these concerns (in particular, authors added a new section in the appendix that includes additional experimental results). Reviewers were mainly satisfied with the responses and the revision, and they all recommend accept. 
This paper develops a variational auto transformer model (VAT), a VAE based on the transformer (encoder decoder) architecture designed to provide isotropic representations by adding a token level loss for isotropy. All the reviewers agree that this is a novel architecture with a valid and interesting goal behind it.  Reviewers varied somewhat on their impressions of the paper, but none were strongly positive on accepting it. I think the strongest and most aligned concerns were from reviewers ZoL1 and pcez. They both feel that the experiments do not convincingly demonstrate what is required. It would be good to better establish the success of variational sampling and the usefulness of isotropic representations. I would think that even a page of examples in the appendix, contrasting sampling by various methods, would add a lot of information to what is presented here. It would be even better to have experiments showing the relation between improved isotropy and improved task performance (suggested by j72L). Both reviewers are concerned about the small model and weak results and whether these results would extend to larger models that people actually use. While on the one hand, controlled comparisons are valuable, it is also true that people in NLP routinely like to see results on models of a reasonably competitive size. In practice, for 2019 2021, it seems that people regard having models of BERT base size as the "reasonable" small size that they will accept and for which there is reasonably good performance and lots of available empirical results. Transformers directly trained with very few layers do not perform that well. Reviewer pcez is also concerned about the change of the data set in the MiniBERT comparison, which seems valid, and reviewer 5v5U is concerned about what s fair in terms of parameter counts.  This paper needs further work with larger and more careful experimental comparisons to meet the needed level of experimental rigor to be convincing. The authors were not able to iterate sufficiently quickly to achieve this during the ICLR reviewing period, so it seems best that the paper be rejected for now, and the authors look to subsequently submit a more developed version of this work.
The paper proposes and validates a simple idea of training a neural network for a parametric family of losses, using a popular AdaIN mechanism. Following the rebuttal and the revision, all three reviewers recommend acceptance (though weakly). There is a valid concern about the overlap with an ICLR19 workshop paper with essentially the same idea, however the submission is broader in scope and validates the idea on several applications.
This paper offers an in depth analysis of attention in large scale language models including (AL)BERT and XLNet in the context protein representation learning, and obtains many interesting findings. This is not a typical paper with novel technologies proposed, instead, it studies the existing technologies in a specific (biology) context and explains what the learned representations and attention map really mean.  All the reviewers see the value in this paper and give positive feedback in general. At the same time, they also raised a few concerns, e.g., regarding the claim on “well calibrated" attention head, on some missing details of the algorithm description and the experiments,  on phenomenon vs. causality of the finding, etc. The authors really did a very good job in their rebuttal and paper revision, and most of these concerns were (at least partially) addressed, and a few reviewers raised their scores. With this, we are quite confident that this paper is above the bar of ICLR.  
The paper presents a novel approximate second order optimization method for convex and nonconvex optimization problems. The search direction is obtained by preconditioning the gradient information with a diagonal approximation of the Hessian via Hutchinson s method and exponential averaging. The learning rate is updated using an estimate of the smoothness parameter.  The merit of the paper has to be evaluated from the theoretical and empirical point of view.  From the internal discussion, the reviewers agreed that the new algorithm is a mix a known methods, mainly present in AdaHessian, with a small tweak on the exponential average. Moreover, the theoretical guarantees do not seem to capture the empirical performance of the algorithm nor they provide any hint on how to set the algorithm s hyperparameters. For example, in Theorem 4.6 the optimal setting of $\beta_2$ is 1. That said, the most important theoretical contribution seems to lie in the fact that AdaHessian did not have any formal guarantee. Hence, this paper is the first one to show a formal guarantee this type of algorithms.  From the empirical point of view, the empirical evidence is very limited for the today standards in empirical machine learning papers. The reviewers and me do not actually believe that the proposed algorithm dominates the state of the art optimization algorithms used in machine learning. However, in the internal discussion we agreed that the algorithm has still potential and it should be added to the pool of optimization algorithms people can try.  Overall, considering the paper in a holistic way, there seems to be enough novelty and results to be accepted at this conference.  That said, I would urge the authors to take into account reviewers comments (and I also add some personal ones here). In particular, a frank discussion of current theoretical analysis and empirical evaluation is needed.  Some specific comments:   AdaGrad was proposed by two different groups at COLT 2010, so both papers should be cited. So, please add a citation to:  McMahan and Streeter. Adaptive bound optimization for online convex optimization. COLT 2010.   Remark 4.7, second item: Neither Reddi et al.(2019) nor Duchi et al. (2011) *assume* bounded iterates, that must be proved not assumed. Instead, they explicitly project onto a domain that they assumed to be bounded.   The convergence of the gradient to zero does not imply convergence to a critical point. To prove convergence to a critical point you should prove that the iterates converge, that in general is false even for lower bounded functions. Indeed, consider $f(x) log(1+exp( x))$, the iterates would actually diverge while the gradient still go to zero.
Reviewers agree that this paper contains interesting results and simple, but good ideas. However, a few severe concerns were raised by reviewers. Most prominent one was the experiment set up   authors use a pre trained ResNet101 (which has seen many classes of Imagenet) for testing which makes is unclear how well their proposed method would work for unlabeled pool of dataset that classifiers has never seen.  While authors claim that their the dataset used for testing was disjoint from Imagenet, a reviewer pointed out that dogs dataset, bird datasets both state that they overlap with Imagenet. A few other concerns are raised (need more meaningful metric in Figure 4d, which wasn’t addressed in rebuttal). We look forward to seeing an improved version of the paper in your future submissions. 
The paper explores the idea of using implicit human feedback, gathered via EEG, to assist deep reinforcement learning. This is an interesting and at least somewhat novel idea. However, it is not clear that there is a good argument why it should work, or at least work well. The experiments carried are more exploratory than anything else, and it is not clear that much can be learned from the results. It s a proof of concept more than anything else, of the type that would work well for a workshop paper. More systematic empirical work would be needed for a good conference paper.  The authors did not provide a rebuttal to reviewers, but rather agreed with their comments and that the paper needs more work. In light of this, the paper should be rejected and we wish the authors best of luck with a new version of the paper. 
This paper proposed an additional training objective for unsupervised neural machine translation (UNMT). They first train two UNMT models and use these models to generate pseudo parallel corpora.  These parallel corpora are used to optimize the UNMT training objective. The experiments are conducted on several language pairs and they also compared with several alternative works.   All the reviewers admit that the proposed method is straightforward and effective. The authors claim that the new training objective is used to enhance the "data diversification". This point has been questioned by the reviewers. Some reviewers are convinced by the response and some still have different opinions.  From my point of view, the proposed method can also be considered as a kind of combination of  (pseudo) supervised NMT and unsupervised NMT.   The presentation and description of its key contributions seem unclear. However, we encourage the authors to modify their paper and we believe this proposed method can inspire the MT community for further research. At the moment, the paper is seen as not yet ready for publication at this time.
PAPER: This paper presents a multimodal auto encoder architecture built on the premise that unimodal variations can be best generated when taking advantage of a shared latent space. This is operationalized by defining a hierarchical model with two primary levels: a shared structure space and unimodal variations (which could be multi layer).  DISCUSSION: The reviewers and follow up discussion brought many questions and issues. The authors submitted a significantly revised version of their paper which clarified many issues and added a few extra results. While many of the reviewers’ questions were addressed by the authors, it seems that reviewers ended up not changing significantly their review scores. One fundamental concern is if the basic assumption about the shared structure is effectively the proper way to approach such generative modeling task. The experimental for image generation did not seem to support this hypothesis. SUMMARY: While the revised version was an improvement over the original submission, improving clarity and adding some experimental measures, the experimental results did not seem to always support the main hypothesis. Human evaluation results may help in this direction.
Reviewers are in a consensus and weakly recommended to reject after engaging with the authors, with the reviewers updating their scores on Dec 11 after engagement. The authors answered most of the reviewers  concerns, however from further discussions with the  reviewers there are still some points which lead them to rank the paper lower than others. I thus lean to reject. Please take reviewers  comments into consideration to improve submission should you choose to resubmit.
This paper studies the problem of how to collect demonstrations via crowd sourcing for imitation and offline learning. The paper received mixed reviews initially. The reviewers had difficulty understanding empirical results, asked for some more ablations, and were little unconvinced by the proposed usefulness of the collected data. The authors provided a strong thoughtful rebuttal that addressed many of those concerns. The paper was discussed extensively with one of the reviews who increased their score from 3 to 5. Reviewers generally agree that the paper is good but not all reviewers are on board with acceptance. AC recommends accept but agrees with the reviewers and the authors are urged to look at reviewers  feedback and incorporate their comments in the camera ready.
The paper proposed a parameterized convolution layer using predefined filterbanks. It has the benefit of less parameters to optimize and better interpretability. The original submission failed to inlcude many related work into the discussion which was addressed during the rebutal.  The main concerns for this paper is the limited novelty and insufficient experimental validation and comprisons:  * There have been existing work using sinc parameterized filters, learnable Gammatones etc, which are very similar to the proposed method. Also in the rebutal, the authors acknowledged that "We did not claim that cosine modulation was the novelty in our paper" and it is "just a way of simplifying implementation and dealing with real values instead of complex ones" and "addressing the question of convergence of parametric filter banks to perceptual scale". * Although the authors addressed the missing related work problem by including them into discussions, the expeirmental sections need more work to include comparisons to those methods and also more validations on difference datasets to address the concern on the generalization of the proposed method. 
The authors propose an architecture for learning and predicting graphs with relations between nodes. The approach is a combination of recent research efforts into Graph Attention Networks and Relational Graph Convolutional Networks. The authors are commended for their clear and direct writing and presentation and their honest claims and their empirical setup. However, the paper simply doesn t have much to offer to the community, since the algorithmic contributions are marginal and the results unimpressive. While the authors justify the submission in terms of the difficult implementation and the extensive experiments, this is not enough to support its publication at a top conference. Rather, this could be a technical report.
The paper proposes a new model called differential decision tree which captures the benefits of decision trees and VAEs. They evaluate the method only on the MNIST dataset. The reviewers thus rightly complain that the evaluation is thus insufficient and one also questions its technical novelty.
The paper considers the natural class of algorithms, namely Aggregators with Gaussian noise for distributed SGD with differential privacy (DP) and Byzantine resilience (BR). Previous results shows VN >BR > convergence of SGD. The authors first show that aggregators with Gaussian noise algorithms satisfy DP but violates VN necessarily, so approximate VN is proposed. Theorem 2 shows approximate VN >convergence. Proposition 2 shows the above algorithms satisfies approximate VN with certain parameters. With the combined bound Corollary 1, the authors observe (and then verify by experiments) that larger batch size is beneficial and in particular more beneficial than when DP or BR is enforced alone. In the formulation, an important baseline of robust mean aggregation [Diakonikolas,Kamath,Kane,Li,Moitra,,Stewart 2016] and even more relevant baseline of robust and DP mean aggregation[Liu,kong,Kakade,Oh, 21] are somehow missing. One would assume that directly applying these well known techniques might give the desired DP and robust SGD. The field at the intersection of differential privacy and robustness has evolved quite a bit recently and tremendous technical innovations are happening. Given the relveance of the proposed problem to this line of work, one should make the connections precise and explain the differences.
This paper suggests a Bayesian approach to make inference about latent variables for image inference tasks. While the idea in the paper seems elegant and simple, reviewers pointed out a few concerns, including lack of comparisons, missing references, and requested for more extensive validations. While a few comments might have been misunderstandings (eg lack of quantification   seems to be resolved by author’s comments), other comments are not (eg equation (8) needs further justification even if the final results don’t use it). We encourage authors to carefully review comments and edit the manuscript (perhaps some appendix items should be in the main to reduce confusion) for resubmitting to future conferences. 
This is a borderline paper.  This paper proposed feature kernel distillation (FKD), a new distillation framework, by matching the kernels obtained from the networks of student and the  teacher.  Theoretical justification is provided by  extending the results of Allen Zhu and Li(2020)(ALi20 hereafter). Empirical results show superiority of FKD over vanilla KD on several datasets. There is however concern that the technical novelty is limited and  incremental, an opinion shared by DKJu, and 68WG, compared to ALi20. Reviewer DKJu suggests that the authors could highlight those results which are not straightforward extensions of ALi20. Another important point of concern is that the paper may have some Overstated claims.  The authors clarified that the language of the claims be suitably edited. In this regard Reviewer h8ud have some specific suggestions which should be easy to incorporate.   In view of additional experiments conducted and detailed discussion during rebuttal addressed some of the concerns of the reviewers. If accepted, the final version, should include most of the discussion and additional experiments.
This paper received borderline recommendations (5, 5, 6, 7) but even the two slightly more positive reviewers were lukewarm (R1 and R2). While the reviewers acknowledged the heavy computational requirements to do an apples to apples comparison with existing baselines, they remain underwhelmed with the lack of experiments. I agree with their criticism; even though the proposed idea seems promising, without comprehensive experiments, it is difficult to judge the significance of this work. R1 commented after the discussion period that an earlier version of this paper actually had ImageNet results. R4 made excellent suggestions to improve the paper further. The authors are strongly encouraged to incorporate them into their future submission.  (I am copying R1 s comment below in case it is invisible to the authors after the notification.)  Sorry for the late update   I have read the rebuttal earlier. I would like to keep my acceptance rating but after the rebuttal I am fine either way. The paper first appeared in March on ArXiv, so indeed it is a concurrent work (actually an earlier work compared to BYOL or SwAV). We have actually tried to reproduce the results in the paper a while back but it did not go well (could not reproduce it), but this time the submission also includes the code. While I haven t run it, I trust the results are reproducible (maybe there are some tricks that I am not aware of).  Regarding running experiments on toy examples   I can understand that this research is resource constrained for ImageNet, but the earlier draft actually had some results on ImageNet (60+ top 1 accuracy) (see appendix of https://arxiv.org/pdf/2007.06346v1.pdf), and for some reason this submission removed that. So this is not a positive sign. Overall, my experience for CIFAR vs ImageNet is that it is easier to make things work on CIFAR, while it is much harder to do so on ImageNet. So maybe some trials are indeed done by the authors, but they choose to not report it in the submission for some reason. On the other hand, one can argue that results on toy datasets are good enough contributions for an early develop of something and they are just not ready for larger and more challenging datasets yet.  Therefore, this paper is quite a struggle. I hoped to see a better than this submission as this paper actually had all the time from March to October to improve its quality of experiments (actually even for ImageNet, one can to dozens of cycles on it during this time), but it did not for some reason.
This paper introduces a class of deep neural nets that provably have no bad local valleys. By constructing a new class of network this paper avoids having to rely on unrealistic assumptions and manages to provide a relatively concise proof that the network family has no strict local minima. Furthermore, it is demonstrated that this type of network yields reasonable experimental results on some benchmarks. The reviewers identified issues such as missing measurements of the training loss, which is the actual quantity studied in the theoretical results, as well as some issues with the presentation of the results. After revisions the reviewers are satisfied that their comments have been addressed. This paper continues an interesting line of theoretical research and brings it closer to practice and so it should be of interest to the ICLR community.
This paper proposes a novel benchmark for neural architecture search methods, which consists of 25 different combinations of search spaces and datasets. The main motivation is that existing NAS benchmarks, such as NAS Bench 201, consider very small search space and few datasets, such that conclusions drawn with them do not generalize to unseen settings with different search spaces and datasets. The authors first describe the 25 different combinations of the search space and tasks for the given benchmark, and then conduct an extensive empirical study of existing NAS methods and performance predictors with the proposed benchmark, to show that architectures and hyperparameters found with the popular benchmarks do not generalize to other settings, which is consistent with their assumption.  —  All reviewers were initially positive about the paper, and remained positive throughout the discussion period. The reviewers found the paper well motivated, and the proposed benchmark useful, as they agree with the need of introducing a single, unified framework that can validate a NAS method under diverse settings, since existing benchmarks only consider specific datasets and search spaces. However, the reviewers were also concerned with the weak technical novelty (Reviewer 2xvD), and that the work lacks deeper insights that could guide the community towards better methods (Reviewer Gku7).   I also agree with the authors and the reviewers on the necessity of having a unified benchmark that incorporates all different settings considered in the previous benchmarks, and find the extensive empirical study of existing NAS methods useful.   However, I find the work as rather technically weak as mentioned by R2xvD, since the authors spent too much time describing and showing the limitations of existing benchmark methods, while what is more important for benchmarks, is to justify how the proposed benchmark can evaluate the performance of different methods in a fair manner, while being representative of the practical settings. In short, the authors need to justify their design choices. Yet, the 25 settings proposed in the paper seem to have been arbitrarily chosen, and it is not clear if having a good performance on this benchmark is indeed a fair evaluation, or well reflects how the NAS method will perform in practice. The proposed benchmark also does not really consider a novel search space or setting that have been overlooked in the past either, and does not provide much insights on the problem, as mentioned by Reviewer Gku7.   Thus, although I recommend an acceptance for its practical value acknowledged by the reviewers, the authors need to put a considerable amount of effort in revising the paper, and If this were a journal submission, the paper may need to undergo a major revision. Most importantly, as described, the authors should justify their design choices as well as whether evaluating a model on the benchmark yields “fair” and “representative” results, focusing more on describing the proposed benchmark itself.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The problem is well motivated and related work is thoroughly discussed   The evaluation is compelling and extensive.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    Very dense. Clarity could be improved in some sections.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
This is a solid paper that proposes a new method for approximating softmax attention in transformer architectures that scales linearly with the size of the sequence. Even though linear architectures have been proposed before using a similar idea (Katharopoulos et al 2020), this paper provides a better solution along with theoretical analysis and makes a rigorous empirical comparison against other methods. All reviewers agree that this is a strong paper that should be accepted. I suggest citing the recent paper https://arxiv.org/abs/2011.04006 (Long Range Arena, mentioned in the discussion) which provides further comparisons on long range benchmarks, including the method presented in this paper and Katharopoulos et al 2020, along with a detailed discussion of the differences between the two methods.
This paper introduces a model that learns a slot based representation and its transition model to predict the representation changes over time. While all the reviewers agree that this paper is focusing on an important problem, they expressed multiple concerns regarding the novelty of the approach as well as lacking experiments. It certainly is missing multiple important relevant works, thereby overclaiming at a few places. The authors provided a short general response to compare their approach with some of the previous works and conduct stronger experiments for a future submission. We believe this paper is not at the stage to be published at this point.
The introduced method is novel and interesting. However, as pointed in the reviews the` paper misses several important references. The authors should extend their discussion on related work by methods from both recommender systems and extreme classification. Besides the papers listed by the reviewers, the introduced method seems also to be related to LTLS (https://arxiv.org/abs/1611.01964) and W LTST (http://papers.neurips.cc/paper/7953 efficient loss based decoding on graphs for extreme classification.pdf), as well as to probabilistic classifier chains (https://icml.cc/Conferences/2010/papers/589.pdf) used for multi label classification (recommendation can be reduced to multi label classification under 0/1 loss by coding each item using a binary code of a fixed length). Nevertheless, the introduced method seems to be novel, nicely reusing and fitting together existing ideas.   Unfortunately, the authors did not submit any rebuttal. Therefore, the paper cannot be accepted to ICLR. We encourage the authors to work further and extend the paper by an exhaustive discussion about related work, a wider experimental study, a more detailed description of all the steps of the method.   
The experimental part of the work has been reported by all reviewers as too limited and not convincing enough. At this point this work cannot be endorsed for publication at ICLR.
The paper presents a model combining AC GAN and StyleGAN for semi supervised learning of disentangled generative adversarial networks. It also proposes new datasets of 3d images as benchmarks. The main claim is that the proposed model can achieve strong disentanglement property by using 1 5% of the annotations on the factors of variation. The technical contribution is moderate but the architecture itself is not highly novel. While the proposed method seems to work for controlled/synthetic datasets, overall technical contribution seems incremental and it s unclear whether it can perform well on larger scale, real datasets. The experimental results on CelebA don t look convincing enough. 
The paper proposes to model uncertainty using expected Bayes factors, and empirically show that the proposed measure correlates well with the probability that the classification is correct.  All the reviewers agreed that the idea of using Bayes factors for uncertainty estimation is an interesting approach. However, the reviewers also found the presentation a bit hard to follow. While the rebuttal addressed some of these concerns, there were still some remaining concerns (see R3 s comments).   I think this is a really promising direction of research and I appreciate the authors  efforts to revise the draft during the rebuttal (which led to some reviewers increasing the score). This is a borderline paper right now but I feel that the paper has the potential to turn into a great paper with another round of revision. I encourage the authors to revise the draft and resubmit to a different venue.
The paper presents a simple and interesting idea to improve exploration efficiency, using the notion of action permissibility.  Experiments in two problems (lane keeping, and flappy bird) show that exploration can be improved over baselines like DQN and DDPG.  However, action permissibility appears to be very strong domain knowledge that limits the use in complex problems.  Rephrasing one of reviewers, action permissibility essentially implies that some one step information can be used to rule out suboptimal actions, while a defining challenge in RL is that the agent needs to learn/plan/reason over multiple steps to decide whether an action is suboptimal or not.  Indeed, the two problems in the experiments have such a property that a myopic agent can solve the tasks pretty well.  The paper would be stronger if the AP function can be defined for more common RL benchmarks, with similar benefits demonstrated.
The paper shows that initializing the parameters of a deep linear network from the orthogonal group speeds up learning, whereas sampling the parameters from a Gaussian may be harmful.  The result of this paper can be interesting to the deep learning community. The main concern the reviewers raised is the huge overlap with the paper by Du & Hu (2019). It would have been nice to actually see whether the results for linear networks empirically also hold for nonlinear networks. 
Strong points:    Interesting, fairly systematic and novel analyses of recurrent NMT models, revealing individual neurons responsible for specific type of information (e.g., verb tense or gender)    Interesting experiments showing how these neurons can be used to manipulate translations in specific ways (e.g., specifying the gender for a pronoun when the source sentence does not reveal it)    The paper is well written  Weak points    Nothing serious (e.g., maybe interesting to test across multiple runs how stable these findings are).  There is a consensus among the reviewers that this is a strong paper and should be accepted.  
The topic of learning reward functions from preferences and how to do this efficiently is of high interest to the ML/RL community. All reviewers appreciate the suggested technical approach and the thorough evaluations that demonstrate clear improvements. While the technical novelty of the paper is not entirely compelling, all reviewers recommend acceptance of the paper.
The submission presents an approach to speed up network training time by using lower precision representations and computation to begin with and then dynamically increasing the precision from 8 to 32 bits over the course of training. The results show that the same accuracy can be obtained while achieving a moderate speed up.   The reviewers were agreed that the paper did not offer a signficant advantage or novelty, and that the method was somewhat ad hoc and unclear. Unfortunately, the authors  rebuttal did not clarify all of these points, and the recommendation after discussion is for rejection. 
This papers proposed a solution to the problem of disease density estimation using satellite scene images.  The method combines a classification and regression task.  The reviewers were unanimous in their recommendation that the submission not be accepted to ICLR.  The main concern was a lack of methodological novelty.  The authors responded to reviewer comments, and indicated a list of improvements that still remain to be done indicating that the paper should at least go through another review cycle.
The paper addresses coordination improvement in the MARL setting by learning intristic rewards that motivate the exploration and coordination. The  paper is theoretically founded and the empirical evaluations back up the claims.  During the rebuttal the carried out an impressive amount of work. They provided several additional studies and substantially improved the presentation, addressing all of the reviewers  requests. Although not all the reviewers responded to the authors, the authors  response was taken into the account when recommending the decision.  Minor:   The authors should comment on the learning intristic rewards with evolution (Faust et al, 2019): https://arxiv.org/abs/1905.07628
No reviewer has made a strong case for accepting this paper or championed it so I am recommending rejecting it. The unfavorable reviewers, although they mention real issues, have not highlighted some of the most important barriers to accepting this work.  One major, but not necessarily dispositive, concern is that the paper only presents results on MNIST. However, even if we put aside this concern, there are several issues with the motivation and approach of this paper. If this technique is actually good at improving the model outside the clean image distribution, then the paper should show that and not just L2 worst case perturbations. To quote the intro of the paper: "How can deep learning systems successfully generalise and at the same time be extremely vulnerable to minute changes in the input?" The answer is: they don t generalize and this work does not show us improved generalization. Even a small amount of test error in the data distribution suggests that the closest test error to a given point will often be quite close to the starting point, although this is easier to see with linear models. The best way to fix this work would be to study (average case) error on noisy distributions (as in the concurrent submission https://openreview.net/forum?id S1xoy3CcYX ).
The authors did a good job addressing reviewer concerns and analyzing and  testing their model on interesting datasets with convincing results.
The reviewers unanimously agree that this paper is worth publication at ICLR. Please address the feedback of the reviewers and discuss exactly how the potential speed up rates are computed in the appendix. I speed up rates to be different for different devices.
This paper discusses an empirical scaling law in terms of samples needed for pretraining for effective downstream transfer. The reviewers liked the premise but had major concerns with the evaluation and some clarifications about empirical choices made. The paper initially received reviews tending towards rejection. The authors provided a thoughtful rebuttal that addressed some of the questions. The paper was discussed heavily and all the reviewers updated their reviews in the post rebuttal phase. In conclusion, all reviewers still believed that their concerns regarding empirical evaluation like why evaluate only sim2real transfer, etc. still stand. AC agrees with the reviewers  consensus and encourages the authors to take the feedback into account for future submissions.
The paper proposes new methods for optimization of optimization of KL(student_model||teacher_model).   The topic is relevant. The paper also contains interesting ideas and the proposed methods are interesting; they are elegant and seems to work reasonably well on the tasks tried.  However, the reviewers do not all agree that the paper is well written. The reviewers have pointed out several issues that need to be addresses before the paper can be accepted.     
The reviewers agree in their positive evaluation of the paper. A weakness of the paper pointed out by several reviewers was its presentation, which has hovewer improved. Thus, I m glad to recommend acceptance.
This paper is a variant of the large growing class of Neural ODEs, and adds dependency on a time delay to the baseline, which allows to model a larger class of physical systems, in particular adding the possibility of crossing paths in phase space.  After initial evaluation, the paper was on the fence, with 2 reviewers providing favorable reviews, and 2 reviewers recommending rejection. A particular important issue raised was positioning with respect to prior art, [Dupont 2019], with some substantial overlap between the papers; requests of theoretical discussions of the class of studied systems and its properties.  Most of these remarks have been addressed by the authors, in particular positioning and experimental comparisons.  The AC judged that the paper had been sufficiently improved and recommends acceptance.
This paper does as it’s title suggests, it introduces an algorithm for constraining a CRF’s output space to correspond to a pre specified regular language. The authors build upon a wealth of prior work aiming to enable CRFs to capture particular non local dependencies and output constraints and present a coherent general algorithm to specify such constraints with a regular language. This is a clearly presented and well motivated contribution.  The reviewers predominantly agree that this work is clearly and rigorously presented and that the formalisation of constraints for CRFs through regular languages is a useful contribution for practitioners. One reviewer questioned the utility of constraining the output distribution at training time. In response the authors convincingly argue that unconstrained models will fail to learn the data generating distribution when non local constraints exist in the data and have included a clear synthetic example of this in the paper.  The most significant weakness identified of this paper is the limited experimentation, consisting of one synthetic experiment and an application to semantic role labelling. The key motivation for formalising constraints on CRFs with regular languages is the argument that this allows model builders to use a familiar formalism across disparate tasks rather than producing bespoke solutions for each. As such it would be informative when assessing the contribution of this work to see a number of practical examples of task output spaces formalised as regular languages such that we can form an intuition for how natural this representation is for more than one task, while also shedding light on the ease, or otherwise, of the crucial processing of minimising the representation to maximise efficiency.  While the application to a broader range of tasks would definitely strengthen this paper, in its current form it provides a useful formalism that will be of interest to those working in structured learning and as such is a contribution worthy of publication.
The paper received mixed scores: Weak Reject (R1 and R2) and Accept (R3). AC has closely read the reviews/comments/rebuttal and examined the paper. After the rebuttal, R2 s concerns still remain. AC sides with R2 and feels that the generated interpretations are not convincing, and that the conclusions drawn are not fully supported. Thus the paper just falls below the acceptance threshold, unfortunately. The work has merits however and the authors should revise their paper to incorporate the constructive feedback.
The paper introduces a GNN approach to solve the problem of source detection in an epidemics. While the paper contains some interesting new ideas, the reviewers raised some important concerns about the paper and so the paper should not be accepted in the current form. In particular,    the paper does not motivate the ML approach to the problem   the experiments are limited for an empirical paper   the method used in the paper is not very novel   the proofs presented in the paper are not formal enough 
This paper proposes a GAN model to synthesize raw waveform audio by adapting the popular DC GAN architecture to handle audio signals. Experimental results are reported on several datasets, including speech and instruments.   Unfortunately this paper received two low quality reviews, with little signal. The only substantial review was mildly positive, highlighting the clarity, accessibility and reproducibility of the work, and expressing concerns about the relative lack of novelty. The AC shares this assessment. The paper claims to be the first successful GAN application operating directly on wave forms. Whereas this is certainly an important contribution, it is less clear to the AC whether this contribution belongs to a venue such as ICLR, as opposed to ICASSP or Ismir.  This is a borderline paper, and the decision is ultimately relative to other submissions with similar scores. In this context, given the mainstream popularity of GANs for image modeling, the AC feels this paper can help spark significant further research in adversarial training for audio modeling, and therefore recommends acceptance. I also encourage the authors to address the issues raised by R1.  
This paper proposes constraints to be applied to the weights of a deep neural model during training. These constraints, motivated by an analysis of Rademacher complexity, are compared with other constraints and penalty approaches in transfer learning. The authors were able to build on the reviewers feedback to improve their paper on several points during the discussion phase, leading to a consensus for acceptance among reviewers. They also agreed to conduct experiments targeting stronger experimental results to compare all methods in the situation where they provide state of the art results. This will make a useful contribution to the ICRL audience, and I recommend acceptance. 
Thank you for your submission to ICLR.  The reviewers and I are in agreement that the work presents some interesting connections between closed loop control and stabilization of activations to an observed manifold.  Specifically, the idea of using optimal control dynamic programming techniques to compute optimal adjustments to ensure control on this manifold is an interesting one and may have other implications within deep networks.  Although the reviewers were convinced by the experiments on robustness, I remain a bit skeptical here.  The results show that while the method marginally improves robustness to small epsilon perturbations, the models are still quite non robust against the size perturbations frequently used in assessing adversarial robustness (e.g., to eps 8 perturbations on CIFAR10, where the best approach gets ~11% accuracy against PGD attacks).  It doesn t really matter how well a defense works against sub optimal attacks: if PGD is able to decrease its accuracy this much, clearly the model is not very robust (and it seems upon reading that only 20 step PGD, with no restarts, was used as an attack, which is a fairly weak variant of PGD).  Furthermore, the approach didn t improve much upon PGD based adversarial training when combined with it, either, overall suggesting that the impact on robustness is somewhat minor, and needs to be evaluated quite a bit more.  While I don t believe these concerns are substantive enough to override the beliefs of all reviewers, I think that the authors could do a much better job of evaluating the actual robustness of these models (following the advice of https://arxiv.org/abs/1902.06705).  And if the resulting metrics are not as strong as hoped for, then it would be good to evaluate other possible benefits of the approach (perhaps to random distribution shift? it seems a much more likely situation for there to be real gains?).  Thus, while I believe the paper has some interesting ideas, I think the authors should probably tone down some of the current claims of improving adversarial robustness unless they can provide a much more thorough evaluation.
This paper proposes a new kind of CNN that convolves on deformable regions and cooperates with the Poisson equation to determine the deformable regions. Experiments on texture segmentation look promising.  Pros: 1. The paper is well written and easy to follow. 2. The idea is interesting and the reviewers liked it.  3. The experiments on texture segmentation are promising.  Cons: 1. Actually, convolution on non rectangular region is not new, in contrast to the authors  claim and reviewers  belief, although the authors may argue that the mechanisms of determining the region for convolution are different and the CNNs are used for different tasks. See, e.g.,   Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei: Deformable Convolutional Networks. ICCV 2017: 764 773  and other papers by the same first author. So the AC would discount the novelty of the paper.  2. As most of the reviewers commended, the experiments on texture segmentation were insufficient. Although extra experiments were added (thank the authors  effort on doing this), the reviewers actually still deemed that they were not convincing enough, e.g., should compare with more state of the art methods.  Reviewers #2&#4 both confirmed this issue in confidential comments.  Although Reviewer #1 increased his/her score, the final average score is still below the threshold. So the AC decided to reject the paper.
The paper introduces a theory of mind benchmark.   This paper certainly improved during the discussion period. However, the paper is still incomplete. The authors are working related work (paper was not updated in this regard). The experiments still need significant work. The original submission used only 3 runs (very, very low). Although the authors bumped up the # of runs, the learning curves in the appendix feature very large and overlapping error bars, and the main table of results presented in the paper contains no measures of certainty those a reported in a separate table in the appendix making comparison tedious. The paper has a fairly informal approach to dealing with hyper parameters that should be discussed and improved. The reviewers pointed out (in their reviews and dialog with the authors) several ways the experiments should be extended.   The contribution of the benchmark is evaluated primarily via experiments; much work needs to be done before acceptance.
The reviewers agree that the manuscript is below the acceptance threshold at ICLR.  Many points of criticism were evident in the reviewer comments, including small artificial test domain, no new methods introduced, poor writing in some places, and dubious need for DeepRL in this domain.  The reviews mentioned a number of constructive comments to improve the paper, and we hope this will provide useful guidance for the authors to rewrite and resubmit to a future venue.
This paper gets decent performance gains (~2% on GLUE) by soft regularization to make negatives closer to positives in contrastive learning and hard correction of too close negatives to at least avoid synonyms. These are useful ideas which to some extent build on the simple technique of ELECTRA (controlling the size of the generator MLM in Electra encourages the negatives to in general be "close but not too close", right?). As such, the paper is correct and provides potentially useful gains, but it appears a quite small adjustment of existing techniques, and in addition the use of WordNet is fairly brittle (and its similarity calculations do not consider context at all).   The authors should be commended for the thorough job they did at updating their paper to address particular questions and concerns of reviewers, and useful new information emerged. Relative to the question of whether this method can be applied with other MLMs, the new Appendix A results do show that the answer is Yes, but the gains turn out to be much more modest (~0.5% on GLUE). However, ultimately, while this is all useful information and side experiments, these improvements just can t fix the key problem that all the reviewers felt that this paper does not provide sufficient "Technical Novelty and Significance". As such without bigger new ideas, this improved paper would probably be best as a good workshop paper.  My recommendation is that this paper not be accepted to ICLR 2022 on the basis of its limited technical novelty and significance.
Policy gradient methods typically suffer from high variance in the advantage function estimator. The authors point out independence property between the current action and future states which implies that certain terms from the advantage estimator can be omitted when this property holds. Based on this fact, they construct a novel important sampling based advantage estimator. They evaluate their approach on simple discrete action environments and demonstrate reduced variance and improved performance.  Reviewers were generally concerned about the clarity of the technical exposition and the positioning of this work with respect to other estimators of the advantage function which use control variates. The authors clarified differences between their approach and previous approaches using control variance and clarified many of the technical questions that reviewers asked about.  I am not convinced by the merits of this approach. While, I think the fundamental idea is interesting, the experiments are limited to simple discrete environments and no comparison is made to other control variate based approaches for reducing variance. Furthermore, due to the function approximation which introduces bias, the method should be compared to actor critic methods which directly estimate the advantage function. Finally, one of the advantages of on policy policy gradient methods is its simplicity. This method introduces many additional steps and parameters to be learned. The authors would need to demonstrate large improvements in sample efficiency on more complex tasks to justify this added complexity. At this time, I do not recommend this paper for acceptance.
This paper proposes a model based RL algorithm which, instead of simply fitting a parameterized transition model and uses rollout for planning, learns latent landmarks via distance based clustering and conducts planning on the learned graph. Although some of these ideas themselves have appeared in literatures, the overall approach is very nice, novel and sophisticated. The experimental results appear strong and interesting. Most reviewers feel positive about the contributions of the paper, but there remain concerns that need to be addressed.  The proposed approach is highly nontrivial, and more ablation, generalization and environments need to be studied to fully justify what s going on. The authors agree to expand the paper and add the needed results, which would require substantial work thus reviewers recommend that the paper be submitted again to a future conference and receive another round of review. Showing the generalization is nontrivial, and it would be make the paper stronger if the authors put more thoughts into this issue, although it is not a must.  Minor: Another technical comment is that the approach seems heavily rely the choice of embedding distance. Learning the best embedding with meaningful embedding distance has been considered in other scenarios, see eg https://arxiv.org/abs/1906.00302. It would be interesting to try out and compare difference choices of the embedding distance.  
The authors present a Bayesian model for time series which are represented as discrete events in continuous time and describe methods for doing parameter inference, future event prediction and entropy rate estimation for such processes.  However, the reviewers find that the novelty of the paper is not high enough, and without sufficient acknowledgement and comparison to existing literature.
The paper studies theoretical properties of ridge regression, and in particular how to correct for the bias of the estimator.   The reviewers appreciated the contribution and the fact that you updated the manuscript to make it clearer.  I however advise the authors to think about the best way to maximize impact for the ICLR audience, perhaps by providing relevant examples from the ML literature.
This paper analyses the signal propagation through residual architectures; then suggests a scaling method which, together with weight standardization, allows to train such networks to high accuracy with batch norm; it demonstrates that the method performs better than previous methods (Fixup, SkipInit), and can be used on more advanced architectures.   The reviewers initially had several concerns, but after the author s revision, these concerns were addressed and most reviewers recommended acceptance. One reviewer did not respond, but I think these concerns were addressed. I think it will help to further convince the readers on the usefulness of the method readers if the authors would check the sensitivity to the learning rate with the current method and compare with other methods (SkipInit, Fixup, BN). The reason I m suggesting this is that I think one of the main reasons BN is still in popular use is that it commonly tends to make training more robust to changes in hyper parameters, such as the learning rate (while other methods, like SkipInit and Fixup, require more hyper parameter tuning).  Overall the analysis and the suggested method seem useful, especially at a small batch size and the writing is mostly clear, so I recommend acceptance.   
The topic covered by the paper is timely, and the way the authors have addressed the problem seems correct. The provided empirical evidence seems to be sufficient to support the main claim of the paper. Presentation is well structured and clear. Notwithstanding the above merits, the proposed approach seems to confirm other similar proposals presented in the literature, so the contribution of the paper seems to be limited. Although presentation is good, it is not highlighting enough the differences w.r.t. those proposals and the basic approximation result given by Chebyshev polynomials. Especially a better theoretical characterisation w.r.t. to approximation capabilities by Chebyshev polynomials (with no truncation) would have helped to better gain understanding of the merits of the proposed approach. Finally, some of the experimental results do not seem to have a significant  statistical difference w.r.t to the baselines, so it would have helped to have the result of a statistical test.
This paper describes means of incorporating boundary conditions into graph neural networks used for simulation.  Assorted techniques dynamically adjust computations near a boundary.  Reviewers agreed this work is well written but disagreed regarding whether the scope of the contribution (and application, which focuses mainly on granular flow despite the title of the paper) merits publication at a top conference like ICLR.  The authors rebut the claims of limited scope strongly, but the experiments in the paper somewhat belie the claims of broad scope.  Some comparisons are also missing to Sanchez Gonzalez et al. 2020, which appears to be closely related.  The AC agrees that the scope of the work and contribution here have not met the bar for publication.  Reviewer QRXr has some thoughtful suggestions for ways to improve this work in future submissions, or sharing with an audience that can better appreciate the application oriented contributions might be a reasonable direction to take.
All reviewers agree on acceptance and I agree with them. I recommend a spotlight.
This paper studies when hidden units provide local codes by analyzing the hidden units of trained fully connected classification networks under various architectures and regularizers. The reviewers and the AC believe that the paper in its current form is not ready for acceptance to ICLR 2020. Further work and experiments are needed in order to identify an explanation for the emergence of local codes. This would significantly strengthen the paper.
The authors propose a novel approach for measuring gradient staleness and use this measure to penalize stale gradients in an asynchronous stochastic gradient set up. Following previous work, they provide a convergence proof for their approach. Most importantly, they provide extensive evaluations comparing against previous approaches and show impressive gains over previous work.  After the author response, the primary concerns from reviewers is regarding the gap between the proposed method and single worker SGD/synchronous SGD. I feel that the authors have made compelling arguments that ASGD is an important optimization paradigm to consider, so their improvements in narrowing the gap are of interest to the community. There were some concerns about the novelty of the theory, and my impression is that theorem is straightforward to prove based on assumptions and previous work, however, I view the main contribution of the paper as empirical.  This paper is borderline, but I think the impressive empirical results over existing work on ASGD is a worthwhile contribution and others will find it interesting, so I am recommending acceptance.
The paper considers an interesting application of Bayesian neural nets to the geophysics domain;  however, the paper does not make a novel contribution from the machine learning perspective, and the improvements on top of the previously proposed approach by Ahamed & Daub (2019) seem to be quite modest. Overall, the paper does not seem to be ready for publication at ICLR.  
This paper was unanimously rated above the acceptance threshold by the reviewers.  While all reviewers agree it is worth accepting, they differed in their enthusiasm.  Most reviewers agree that  major limitations of the paper include that the paper provides no insight into why Dale s principle exists and the actual results are not truly state of the art.  Nevertheless there is agreement that the paper presents results worth publicizing to the ICLR audience.  The comparison of the inhibitory network to normalization schemes is interesting. Also, please reference the Neural Abstraction Pyramid work.  
This paper shows gradient flow of ReLU activated implicit networks converges to a global minimum at a linear rate for the square loss when the implicit neural network is over parameterized. While the analyses follow the existing NTK type analyses and there are disagreements among reviewers on the novelty of this paper, the meta reviewer values new theoretical results on new, emerging settings (implicit neural networks), and thus decides to recommend acceptance
The paper considers the problem of training neural networks asynchronously, and the gap in generalization due to different local minima being accessible with different delays. The authors derive a theoretical model for the delayed gradients, which provide prescriptions for setting the learning rate and momentum.  All reviewers agreed that this a nice paper with valuable theoretical and empirical contributions.  
The paper introduces some interesting ideas on how use causal random forests for conditional average treatment effects (CATE), with respect to some baseline treatment level ("0"), when the treatment variable is continuous. Figure 1 summarises the scope of the paper neatly. Scalability issues are also considered.  I think this *is* a paper "nearly there" in terms of a impactful contribution. The main issues are some presentation kinks and extra steps in the theory. I think the very low scores from the reviewers are not quite representative of the overall quality (I would be more generous). However, I m afraid I m also inclined towards a reject. The paper neglects some other developments on ML for CATE with continuous treatment e.g. Bica et al. s "Estimating the Effects of Continuous valued Interventions using Generative Adversarial Networks" (NeurIPS 2020) and the references within. A focus on the theory would help to differentiate it, but I m not that confident that the results are currently mature enough to claim them.  Although I suggest a rejection, let me make clear I strongly encourage the authors to further pursue their ideas. You are doing good work, and the next iteration might nail it. As you found out in the discussion, emphasise the continuous aspect of it. I d also emphasise the fact that you have a clear setup of the problem in terms of the contrast wrt to a baseline treatment effect instead of some generic contrast function. People in ML tend to be oblivious to such a setup, but I m not convinced you are properly exploiting it.
All reviewers tend towards accepting the paper, and I agree.
This paper investigates layer normalization and learning rate warmup in transformers, demonstrating that placing layer norm inside the residual connection (pre LN) leads to better behaved gradients than post LN placement. Doing so allows the learning rate warm up stage to be removed, leading to faster training.  Reviewers were mildly positive about the submission, commenting on the interesting insight provided about transformers, as well as the clear, focused motivation and contribution.  However they also stated that it seem rather incremental of a contribution, as pre LN placement has been introduced before, and found it confusingly written at times.  R2 clearly read it very closely, and had many detailed comments and discussions with authors and other reviewers. They had concerns about the relationship of this work with gradient clipping. The authors deserve credit for quickly investigating this in further experiments. Interestingly, the found that even with gradient clipping, post LN models still needed the learning rate warm up stage, although this issue went away with smaller clipped values or much lower learning rates. Overall, R2 appears to find the paper’s motivation very compelling, but the insights incomplete and not fully satisfactory, while all reviewers find the novelty rather limited.  I think a future submission that forges closer connections between the empirical findings and the theoretical interpretations would be of a great interest to the community, but in its current form is probably unsuitable for publication at ICLR 2020. 
The paper proposes an extension to the reverse curriculum RL approach which uses a discriminator to label states as being on a goal trajectory or off the goal trajectory. The paper is well written, with good empirical results on a number of task domains. However, the method relies on a number of assumptions on the ability of the agent to reset itself and the environment which are unrealistic and limiting, and beg the question as to why use the given method at all if this capability is assumed to exist. Overall, the method lacks significance and quality, and the motivation is not clear enough. 
Unfortunately, the authors did not submit a response during the rebuttal phase.
The paper proposes to add noise to the weights of a policy network during learning in Deep RL settings and finds that this results in better performance on DQN, A3C and other algorithms that use other exploration strategies. Unfortunately, the paper does not do a thorough job of exploring the reasons and doesn t offer a comparison to other methods that have been out on arxiv for several months before the submission, in spite of reviewers and anonymous requests. Otherwise I might have supported recommending the paper for a talk. 
All reviewers noted the significance of the problem tackled by this paper and felt that it is going in the right direction. However, they also all noted that the paper was not finalized and polished well enough to be granted publication: details missing, typos, clarifications needed. The reviewers acknowledged the large amount of work that went into improving the paper during the discussion period. R1 even increased their score to reflect that.  Still the paper still needs some work to be accepted at ICLR. In particular, we encourage the authors to improve on 2 axes. 1. Clarifying motivations and contribution: it is still unclear if the main point of the paper is to propose new methods around FTM & constrained updates, etc. or around proposing a new benchmark for catastrophic forgetting, lifelong learning. 2. Reorganizing experimental section: the experimental section should be organized to support #1. Reviewers made a lot of suggestions, like moving Table 4 from the appendix, that should be further refined  We hope that this will allow to increase the clarity and impact of this research work.
The reviewers agree that the paper studies and interesting problem with an interesting approach. The reviewers raised some concerns regarding the theoretical and empirical results. The authors have made changes to the paper, but given the theoretical nature of the paper and the extent of changes, another review is needed before publication.
The paper learns an embedding on the nodes of the graph, iteratively aligning the vector associated to a node with that of its neighbor nodes (based on the Hebbian rule).   The reviews state that the approach is interesting though very natural/straightforward, and that it might go too far to call it "Hebbian" (Rev#2)   you might want also to see it as a Self Organizing Map for graphs.   A main criticism was about the comparison with the state of the art (all reviewers). The authors did add empirical comparisons with the suggested VGAE and SEAL, and phrase it nicely as "our algorithm outperforms SEAL on one out of four data sets". Looking at the revised paper, this is true: the approach is outperformed by SEAL on 3 out of 4 datasets.  Another criticism regards the insufficient analysis of the results (e.g. through visualization, studying the clusters obtained along different runs, etc).  This aspect is not addressed in the revised version.  An excellent point is the scalability of the approach, which is worth emphasizing.  I thus encourage the authors to rewrite and polish the paper, improving the positioning of the proposed approach w.r.t. the state of the art, and providing a more thorough analysis of the results.  
This paper provides an improved method for deep learning on point clouds.  Reviewers are unanimous that this paper is acceptable, and the AC concurs. 
In this paper, the authors propose a novel approach for learning the structure of a directed acyclic graph from observational data that allows to flexibly model nonlinear relationships between variables using neural networks. While the reviewers initially had concerns with respect to the positioning of the paper and various questions regarding theoretical results and experiments, these concerns have been addressed satisfactorily during the discussion period.  The paper is now acceptable for publication in ICLR 2020. 
The novelty of the paper is limited and it lacks on comparisons with relevant baselines, as pointed out by the reviewers. 
The study of the impact of the noise on the Hessian is interesting and I commend the authors for attacking this difficult problem. After the rebuttal and discussion, the reviewers had two concerns:   The strength of the assumptions of the theorem   Assuming the assumptions are reasonable, the conclusions to draw given the current weak link between Hessian and generalization.  I m confident the authors will be able to address these issues for a later submission.
this submission follows on a line of work on online learning of a recurrent net, which is an important problem both in theory and in practice. it would have been better to see even more realistic experiments, but already with the set of experiments the authors have conducted the merit of the proposed approach shines. 
This paper aims to address the imbalanced class problem in unsupervised domain adaptation. The challenge lies in how to handle the difficulties introduced by imbalanced classes. To this end, this work proposes a new data augmentation strategy by taking the interpolation of two samples from the same class but from different domains as the augmented samples. The experiments demonstrate promising performance on the class imbalanced domain adaptation datasets.  However, there are several concerns raised by the reviewers. 1) The interpolation between a source and target sample of the same class can potentially be unreliable as the pseudo label methods. 2) Some statements are based on intuition but not well supported by either theoretical analysis or experimental evaluations. 3) The proposed method is inferior to baseline methods on some datasets, it would be helpful to have further analysis of the advantages and limitations of the proposed method.   Overall, the paper provides some new and interesting ideas. However, given the above concerns, the novelty and significance of the paper will degenerate. More discussions on the principles behind the proposed method and more experimental studies are needed. Addressing the concerns needs a significant amount of work. Although we think the paper is not ready for ICLR in this round, we believe that the paper would be a strong one if the concerns can be well addressed.
This paper proposes a multiresolution spectral geometric loss called the zoomout loss to help with matrix completion, and show state of the art results on several recommendation benchmarks, although experiments also show that the result improvements are not always dependent upon the geometric loss itself. Reviewers find the idea interesting and the results promising but also have important concerns about the experiments not establishing how the approach truly works. Authors have clarified their explanations in the revisions and provided requested experiments (e.g., on the importance of the initialization size), however important reservations re. why the approach works are still not sufficiently addressed, and would require more iterations to fulfill the potential of this paper. Therefore, we recommend rejection.
This paper examines classifiers and challenges a (somewhat widely held) assumption that adaptive gradient methods underperform simpler methods.  This paper sparked a *large* amount of discussion, more than any other paper in my area. It was also somewhat controversial.  After reading the discussion and paper itself, on one hand I think this makes a valuable contribution to the community. It points out a (near ) inclusion relationship between many adaptive gradient methods and standard SGD style methods, and points out that rather obviously if a particular method is included by a more general method, the more general method will never be worse and often will be better if hyperparameters are set appropriately.  However, there were several concerns raised with the paper. For example, reviewer 1 pointed out that in order for Adam to include Momentum based SGD, it must follow a specialized learning rate schedule that is not used with Adam in practice. This is pointed out in the paper, but I think it could be even more clear. For example, in the intro "For example, ADAM (Kingma and Ba, 2015) and RMSPROP (Tieleman and Hinton, 2012) can approximately simulate MOMENTUM (Polyak, 1964) if the ε term in the denominator of their parameter updates is allowed to grow very large." does not make any mention of the specialized learning rate schedule.  Second, Reviewer 1 was concerned with the fact that the paper does not clearly qualify that the conclusion that more complicated optimization schedules do better depends on extensive hyperparameter search. This fact somewhat weakens one of the main points of the paper.  I feel that this paper is very much on the borderline, but cannot strongly recommend acceptance. I hope that the authors take the above notes, as well as the reviewers  other comments into account seriously and try to reflect them in a revised version of the paper.
This paper proposes a model for disentangling content and dynamics, but unlike the majority of previous work, the dynamics are modeled using ODEs rather than their discrete approximations   RNNs. The reviewers agree that the paper is well written, and the results look good, especially for longer trajectories. Hence, I am happy to recommend this paper for acceptance.
The paper proposes a weakly supervised contrastive learning, using auxiliary cluster information, for representation learning. Their method generates similar representations for the intra cluster samples and dissimilar representations for inter cluster samples via a clustering InfoNCE objective. Their approach is evaluated thoroughly on three image classification task.  The reviewers agree that the paper is well written, presenting interesting theoretical analysis (Reviewer h3zd,  a8kw) and solid experimetal results (Reviewer RhYi, 1ziy). The core idea of the paper is relatively simple and well motivated (Reviewer h3zd). While the focus is using the clustering with auxiliary labels, the method can be applied without auxiliary labels with K means.  There were some concerns from the reviewers:  the overlap with a concurrent work [1]. The authors have provided detailed discussions on conceptual (concurrent work focuses on unsupervised cases where this work focuses on weakly supervised setting) and emprical comparisons. Accordingly, reviewer a8kw and 1ziy had some issues with the novelty of the paper, as it can be interpreted as slight modification from previously explored idea (vanilla InfoNCE loss).   Despite some overlap with existing approaches, the paper presents an interesting and well conducted study of integrating clustering information for learning representation, so I vote for acceptance.   [1] Weakly Supervised Contrastive Learning. ICCV 2021.
Main content:  Blind review #2 summarizes it well:  This paper investigates the security of distributed asynchronous SGD. Authors propose Zeno++, worker server asynchronous implementation of SGD which is robust to Byzantine failures. To ensure that the gradients sent by the workers are correct, Zeno++ server scores each worker gradients using a “reference” gradient computed on a “secret” validation set.  If the score is under a given threshold, then the worker gradient is discarded.   Authors provide convergence guarantee for the Zeno++ optimizer for non convex function. In addition, they provide an empirical evaluation of Zeno++ on the CIFAR10 datasets and compare with various baselines.     Discussion:  Reviews are generally weak on the limited novelty of the approach compared with Zeno, but the rebuttal of the authors on Nov 15 is fair (too long to summarize here).     Recommendation and justification:  I do not feel strongly enough to override the weak reviews (but if there is room in the program I would support a weak accept).
This paper proposes an approach to representing a symbolic knowledge base as a sparse matrix, which enables the use of  differentiable neural modules for inference. This approach scales to large knowledge bases and is demonstrated on several tasks.     Post discussion and rebuttal, all three reviewers are in agreement that this is an interesting and useful paper. There was intiially some concern about clarity and polish, but these have been resolved upon rebuttal and discussion. Therefore I recommend acceptance. 
This is an interesting and carefully presented work which discusses how to implement finite width NTKs more efficiently.  Overall, the reviews were slightly tending positive, though with a variety of concerns, including some concern that the contribution is not sufficiently substantial.  In my own perusal of the paper, personally I feel it could be made more compelling if (a) more speedups could be considered, including ones with various tradeoffs, for instance via randomized linear algebra, (b) explicit consequences on various prediction tasks, rather than plotting wall clock times (e.g., as this paper cites many works which tried to use finite width NTK, and as this paper claims massive speedups, then it will be able to repeat some of those experiments at much larger sizes, which should lead to interesting and valuable larger scale experiments which ideally have some new phenomena, but are even interesting if they simply confirm the smaller scale phenomena).  As a separate concern, I second the comments of one reviewer, that part of this paper s contribution is to a single software package, which is moreover listed in the abstract (and not just part of the standard code release, e.g., as a footnote); this feels a little strange, like an announcement of a code release, and further limits the impact to general machine learning researchers (for instance, I feel completing some of my preceding suggestions could result in, say, researchers who use other software feeling eager to re implement this).  Overall, I urge the authors to continue with their interesting work and aim to resolve these concerns and those of the reviewers.
This paper proposes a new way to stabilise GAN training.  The reviews were very mixed but taken together below acceptance threshold.  Rejection is recommended with strong motivation to work on the paper for next conference. This is potentially an important contribution. 
This paper proposes a method for regularizing the pre training of an embedding function for relation extraction from text that encourages well formed clusters among the relation types. Experiments on FewRel, SemEval 2010 Task 8, and a proposed FuzzyRed dataset show that the proposed prototype method generally outperforms prior state of the art, including MTB (Soares et al., 2019), which was the strongest. The key, novel idea is to model prototype representations for target relations as part of the learning process. A contribution of the work is to show that learning prototype representations are useful in supervised deep learning architectures even beyond few shot learning. This additional learning objective is useful as an inductive bias, and is perhaps of interest even beyond relation extraction research.  Reviewers generally found the proposed method sound and intuitive, and the original set of experiments promising. Some of the reviewers raised concerns about the setup of the experiments, including the relationship between the pre training and target tasks, and the need for several additional baselines. The authors were able to address these concerns, and the reviewers did not raise any follow up concerns.
The paper introduces a new method for encoding dynamics of temporal networks.  The approach, while not ground breaking, is interesting and the results are fairly convincing.  The submission raised a number of concerns from the reviewers. They questioned the complexity of the proposed approach (R3 and R4), the clarity/readability (R2 and R1), and appropriateness of the link sampling strategy (R2), as well as raised several more minor (from my perspective) issues. I believe that the authors adequately addressed most of these concerns in their rebuttal and the revision.  R2 has confirmed that they read the rebuttal and raised their score to strong accept. Unfortunately, the other reviewers have not engaged during the discussion period, and it is unclear if they are satisfied with the clarifications and changes. Nevertheless, after reading the authors  responses and skimming through the manuscript, I believe that most concerns have been addressed, and this is a good paper that deserves to be accepted. That being said, the issue of readability has been raised by the reviewers, and, while I do not think the paper is unreadable, I do agree that there is much room for improvement. I would encourage the authors to polish the manuscript for the camera ready version, as well as try to address the remaining concerns raised by the reviewers.   
The paper proposes a curriculum learning approach to training generative models like GANs. The reviewers had a number of questions and concerns related to specific details in the paper and experimental results. While the authors were able to address some of these concerns, the reviewers believe that further refinement is necessary before the paper is ready for publication.
The paper presents new contrastive based self supervised objective based on Chi squared divergence that helps with mini batch sensitivity, training stability and improved downstream performance. An accept.
The paper defines a new measure of influence and uses it to highlight important features. The definition is novel however, the reviewers have concerns regarding its significance, novelty and a thorough empirical comparison to existing literature is missing.
The paper presents a method for forward prediction in videos. The paper insufficiently motivates the proposed method and presents very limited empirical evaluations (no ablation studies, etc.) to backup its claims. This makes it difficult for the reader to put the work into  the context of the broader research around learning from unsupervised video data; leading reviewers to complete about perceived lack of novelty and clarity.
The authors demonstrate that starting from the 3rd epoch, freezing a large fraction of the weights (based on gradient information), but not entire layers, results in slight drops in performance.  Given existing literature, the reviewers did not find this surprising, even though freezing only some of a layers weights has not been explicitly analyzed before. Although this is an interesting observation, the authors did not explain why this finding is important and it is unclear what the impact of such a finding will be. The authors are encouraged to expand on the implications of their finding and theoretical basis for it. Furthermore, reviewers raised concerns about the extensiveness of the empirical evaluation.  This paper falls below the bar for ICLR, so I recommend rejection.
The paper presents a new deep learning approach for combinatorial optimization problems based on the Transformer architecture. The paper is well written and several experiments are provided. A reviewer asked for more intuition to the proposed approach and authors have responded accordingly. Reviewers are also concerned with scalability and theoretical basis. Overall, all reviewers were positives in their scores, and I recommend accepting the paper.
This paper presents a useful contribution to the growing literature on uncertainty estimation with deep learning. The review process has significantly helped with strengthening this paper, specifically with the concerns about novelty and sufficient comparisons to existing work. I hope you will continue to improve this work for submission to a future venue.
The paper presents a unified system for perception and control that is trained in a step wise fashion, with visual decoders to inspect scene parsing and understanding. Results demonstrate improved performance under certain conditions. But reviewers raise several concerns that must be addressed before the work is accepted.  Reviewer Pros: + simple elegant design, easy to understand + provides some insight behind system function during failure conditions (error in perception vs control) + improves performance under a subset of tested conditions   Reviewer Cons:   Concern about lack of novelty   Evaluation is limited in scope   References incomplete   Missing implementation details, hard to reproduce   Paper still contains many writing errors
This paper introduces the concept of gradient confusion to show how the neural network architecture affects the speed of training. The reviewers  opinion on this paper varies widely, also after the discussion phase.  The main disagreement is on the significance of this work, and whether the concept of gradient confusion adds something meaningful to the existing literature with respect to understanding deep networks. The strong disagreement on this paper suggest that the paper is not quite ready yet for ICLR, but that the authors should make another iteration on the paper to strengthen the case for its significance. 
This paper provides an interesting insight into the fitting of variational autoencoders.  While much of the recent literature focuses on training ever more expressive models, the authors demonstrate that learning a flexible prior can provide an equally strong model.  Unfortunately one review is somewhat terse.  Among the other reviews, one reviewer found the paper very interesting and compelling but did not feel comfortable raising their score to "accept" in the discussion phase citing a lack of compelling empirical results in compared to baselines.  Both reviewers were concerned about novelty in light of Huang et al., in which a RealNVP prior is also learned in a VAE.  AnonReviewer3 also felt that the experiments were not thorough enough to back up the claims in the paper.  Unfortunately, for these reasons the recommendation is to reject.  More compelling empirical results with carefully chosen baselines to back up the claims of the paper and comparison to existing literature (Huang et al) would make this paper much stronger.  
This is an interesting paper trying to answer how contrastive learning can be used in multi label classification. The reviewers however had raised several doubts about motivations, novelty, or the impact of contrastive module on final results. For many of them the authors had delivered satisfying responses, but after a long discussion, we decided that the paper needs revision to improve in these aspects. For example, the authors should make it clear whether the image retrieval application from Section 4.4 is the main motivation of the method. If so, what are the competitive approaches to solve such problem? How to measure the performance of such methods? Answers for the above questions are crucial to find the right motivations for the contrastive module used by the authors.   We hope that the authors will follow the recommendations and resubmit the paper to another top conference.
This paper presents a new link prediction framework in the case of small amount labels using meta learning methods. The reviewers think the problem is important, and the proposed approach is a modification of meta learning to this case. However, the method is not compared to other knowledge graph completion methods such as TransE, RotaE, Neural Tensor Factorization in benchmark dataset such as Fb15k and freebase.  Adding these comparisons can make the paper more convincing. 
The reviewers kept their scores after the author response period, pointing to continued concerns with methodology, needing increased exposition in parts, and not being able to verify theoretical results. As such, my recommendation is to improve the clarity around the methodological and theoretical contributions in a revision.
In this paper, the authors designed a disentanglement mechanism for global and local information of graphs and proposed a graph representation method based on it. I agree with the authors that 1) considering the global and local information of graphs jointly is reasonable and helpful (as shown in the experiments) and 2) disentanglement is different from independence.   However, the concerns of the reviewers are reasonable   Eq. (2) and the paragraph before it indeed show that the authors treat the global and the local information independently. Moreover, the disentanglement of the global information (the whole graph) and the local information (the patch/sub graph) is not well defined. In my opinion, for the MNIST digits, the angle and the thickness (or something else) of strokes can be disentangled (not independent) factors that have influences on different properties of the data. In this work, if my understanding is correct, the global and the local factors just provide different views to analyze the same graphs and the proposed method actually designs a new way to leverage multi view information. It is not sure whether the views are disentangled and whether the improvements are from "disentanglement".   If the authors can provide an example to explain their "disentanglement" simply as the MNIST case does, this work will be more convincing. Otherwise, this work suffers from the risk of overclaiming.
This paper proposes a new approach to graph based active learning, using the query whether the predictions made by the current model are correct or not. Although the theoretical underpinnings of the proposed approach are a bit weak, the problem formulation that is newly proposed in this paper makes sense from a practical point of view, and the paper makes a simple and interesting proposal that would be worth sharing with the community.
This paper proposes to use PCS to replace the conventional decoder for 3D shape reconstruction. It shows competitive performance to the state of the art methods. While reviewer #3 is overall positive about this work, both reviewer #1 and #2 rated weak rejection. Reviewer #1 concerns that important details are missing, and the discussion of results is insufficient. Reviewer #3 has questions on the clarity of the presentation and comparison with SOTA methods. The authors provided response to the questions, but did not change the rating of the reviewers. The ACs agree that this work has merits. However, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.
There is a consensus among the reviewers that the work is interesting and the paper should be accepted.  Nevertheless, several reviewers struggled with understanding the details. While the authors  (largely successfully) addressed these concerns, I believe that the paper is still too dense and hard to follow, I would encourage the authors to invest more time into improving its readability.  One important point which came late in the discussion is the provenance of baseline scores in the result tables (see the review by AnonReviewer3, the current manuscript claims that the numbers are taken from the original papers while in some cases, the numbers cannot be located in these papers). Unfortunately, the authors did not have a chance to respond to this criticism, and fortunately we could trace the key numbers and establish that the results are strong enough to warrant accepting the submission. Still, we would ask the reviewers to fix this issue in the final version.
Overall the paper makes good contributions to the area of robust deep reinforcement learning. The presentation needs to be improved to avoid any confusions. Please take all the reviews into account and revise the paper accordingly.
This paper studies augmentation based methods to improve GNN fairness.  Specifically, based on upper bounds, they propose augmentation tricks to reduce such bounds, empirically validated from benchmark datasets.  Before rebuttal, there was a negative consensus that evaluation results are inconclusive and important state of the arts are not discussed.  Authors have significantly revised to address the concerns of some reviewers (gszb and LHQM), though some concerns still remain that the scheme is rather ad hoc.  Meanwhile, reviewer xMz4 did not find rebuttal sufficient, as some valid comments were not fully discussed. First, datasets where sensitive attributes are the inherent attributes of instances are more suitable for evaluation, which has not been properly addressed by authors. Second, important baselines were mentioned by xMz4, but they were only mentioned briefly in the new section of related work in the revised work. For example, (Agarwal et al 2021) is mentioned as dealing with counterfactual fairness only, but statistic parity studied in this paper is highly related to this concept. Similarly, this work can be viewed as an ad hoc extension for fairness of GCA, without in depth discussions to compare/contrast with these work, and to better highlight novelty/distinction.  Summing up reviewer discussions, we conclude this paper is not ready yet as an ICLR publication.
Dear authors,  The authors all agreed that this was an interesting topic but that the novelty, either theoretical or empirical, was lacking. This, the paper cannot be accepted to ICLR in its current state but I encourage the authors to make the recommended updates and to push their idea further.
This paper provides an approach to improve the differentially private SGD method by leveraging a differentially private version of the lottery mechanism, which reduces the number of parameters in the gradient update (and the dimension of the noise vectors). While this combination appears to be interesting, there is a non trivial technical issue raised by Reviewer 3 on the sensitivity analysis in the paper. (R3 brought up this issue even after the rebuttal.) This issue needs to be resolved or clarified for the paper to be published.
This paper investigates the usage of the extragradient step for solving saddle point problems with non monotone stochastic variational inequalities, motivated by GANs. The authors propose an assumption weaker/diffrerent than the pseudo monotonicity of the variational inequality for their convergence analysis (that they call "coherence"). Interestingly, they are able to show the (asympotic) last iterate convergence for the extragradient algorithm in this case (in contrast to standard results which normally requires averaging of the iterates for the stochastic *and* mototone variational inequality such as the cited work by Gidel et al.). The authors also describe an interesting difference between the gradient method without the extragradient step (mirror descent) vs. with (that they called optimistic mirror descent).  R2 thought the coherence condition was too related to the notion of pseudo monoticity for which one could easily extend previous known convergence results for stochastic variational inequality. The AC thinks that this point was well answered by the authors rebuttal and in their revision: the conditions are sufficiently different, and while there is still much to do to analyze non variational inequalities or having realistic assumptions, this paper makes some non trivial and interesting steps in this direction. The AC thus sides with expert reviewer R1 and recommends acceptance.
The paper proposes a new approach called pessimistic model selection (PMS)  for model selection in offline RL and tests it in 6 different environments. Under certain assumptions this allows theoretical results that the best model is recovered with high probability.  Several points were raised by the reviewers and maintained after the rebuttal:   Theoretical results were considered weak as they only hold asymptotically.   Experimental results limited (potentially different regret scales, no sufficient comparison to other baselines).   Exposition of the paper that needs to be improved.  Given the strong consensus among the reviewer I recommend rejecting this paper.
This paper proposes to use hypernetwork to prevent catastrophic forgetting. Overall, the paper is well written, well motivated, and the idea is novel. Experimentally, the proposed approach achieves SOTA on various (well chosen) standard CL benchmarks (notably P MNIST for CL, Split MNIST) and also does reasonably well on Split CIFAR 10/100 benchmark. The authors are suggested to investigate alternative penalties in the rehearsal objective, and also add comparison with methods like HAT and PackNet.
This paper investigates robustness of the neural networks under bit level network and file corruptions, and proposes corruption agnostic and corruption aware defense approaches. The Bit corruption Augmented Training is introduced, which is about applying the data augmentation at a bit level.  The majority of the reviewers are against the acceptance of the paper. R1 gave the rating of "marginally above acceptance threshold", finding the problem interesting but not the proposed solution. The other reviewers gave rejections and a clear reject rating.  The main concern raised by all of the reviewers is regarding the technical novelty of the proposed approach in the paper. While some reviewers appreciate the importance of the problem and the thoroughness of the experiments more than the others, none of the reviewers find the proposed solution novel and interesting.  The AC agrees with the reviewers that the technical contribution of the paper is not significant despite that it focuses on an interesting problem. We do not recommend this paper to ICLR.
This paper relates deep learning to convex optimization by showing that the forward pass though a dropout layer, linear layer (either convolutional or fully connected), and a nonlinear activation function is equivalent to taking one τ nice proximal gradient descent step on a a convex optimization objective. The paper shows (1) how different activation functions correspond to different proximal operators, (2) that replacing Bernoulli dropout with additive dropout corresponds to replacing the τ nice proximal gradient descent method with a variance reduced proximal method, and (3) how to compute the Lipschitz constant required to set the optimal step size in the proximal step. The practical value of this perspective is illustrated in experiments that replace various layers in ConvNet architectures with proximal solvers, leading to performance improvements on CIFAR 10 and CIFAR 100. The reviewers felt that most of their concerns were adequately addressed in the discussion and revision, and that the paper should be accepted.
The reviewers raised concerns and the authors have not provided a response. All reviewers concur that this paper should be rejected at this time, and I agree.
The paper proposes a meta learning approach to "language guided policy learning" where instructions are provided in the form of natural language instructions, rather than in the form of a reward function or through demonstration. A particularly interesting novel feature of the proposed approach is that it can seamlessly incorporate natural language corrections after an initial attempt to solve the task, opening up the direction towards natural instructions through interactive dialogue. The method is empirically shown to be able to learn to navigate environments and manipulate objects more sample efficiently (on test tasks) than approaches without instructions.   The reviewers noted several potential weaknesses: while the problem setting was considered interesting, the empirical validation was seen to be limited. Reviewers noted that only one (simple) domain was studied, and it was unclear if results would hold up in more complex domains. They also note lack of comparison to baselines based on prior work (e.g., pre training).  The authors provided very detailed replies to the reviewer comments, and added very substantial new experiments, including an entire new domain and newly implemented baselines. Reviewers indicated that they are satisfied with the revisions. The AC reviewed the reviewer suggestions and revisions and notes that the additional experiments significantly improve the contribution of the paper. The resulting consensus is that the paper should be accepted.  The AC would like to note that several figures are very small and unreadable when the paper is printed, e.g., figure 7, and suggests that the authors increase figure size (and font size within figures) to ensure legibility.
The authors proposed to train a large network and a small network simultaneously with a new loss function. The parameters are shared between the two networks, and the loss also incorporates the KL divergence between the outputs of the two models. In this way, the authors claim that one can train a small network with similar accuracy to the large one, while using less memory and having faster inference speed.  The reviewers think the papers is at the borderline. It has some interesting results, however, it also has quite a few problems: 1)	The technical novelty of the paper as compared to the teacher student models is not adequate.  2)	There are many missing references and baselines, since model compression has been a long studied problem. 3)	Experiments on more different NN models are preferred in order to verify the generalization ability of the proposed approach 4)	The compatibility with the pretraining framework is not very clear  The authors provided their rebuttals to the review comments. However, according the discussions among the reviewers, their concerns were not fully addressed yet and most of them would like to stand on their original scores. As a result, we do not think the paper should be accepted in its current form. 
The conditional network embedding approach proposed in the paper seems nice and novel, and consistently outperforms state of art on variety of datasets; scalability demonstration was added during rebuttals, as well as multiple other improvements; although  the reviewers did not respond by changing the scores, this paper with augmentations provided during the rebuttal appears to be a useful contribution  worthy of publishing at ICLR. 
This paper suggests the use of networks for supervised learning which are composed of a bijective network (e.g. a flow) followed by a separable function. This allows easy integration over the input space, which can be used to formulate novel regularizers (examples given are for local consistency, and for out of distribution detection).  The approach is pretty novel, and it s an interesting paper. The reviewers were very divided, however; one reviewer giving it a 1, and another an 8, with the other two reviewers arguing weakly to accept. The "1" took issues with the general formulation, feeling that the necessity to optimize bounds on the true objectives in cases such as softmax regression greatly limits the viability of the work. Personally I disagree with that reviewer s characterizations of the novelty and significance of this work, and I think the OOD detection / classification experimental setting is sufficient to make their point that their approach can be applied in such settings.  In the end I (AC) would agree with the "weak accept" / 6, given the draft of the paper at this time. While I think a few things could be presented more clearly, and I think the empirical evaluation could be more robust (e.g. exploring what goes wrong when the integrated objective functions are included on CIFAR and SVHN), and it would be nice to explore additional applications, I think this is a creative paper which it would be nice to include at ICLR.
This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.
## A Brief Summary This paper proposes two critical modifications to the original RUDDER algorithm: 1. Proposes the Align RUDDER method that assumes that the episodes with high rewards can be used as demonstrations. 2. Uses a profile model from the Multiple sequence alignment approach to align the demonstrations and redistribute the rewards according to how frequently events in the demos are shared across different demonstrators. MSA is being used as a profile model instead of LSTM.  The paper uses successor features to represent state action pairs, which is then used to compute the similarity matrix used for MSA afterward.  The paper shows promising results in the Minecraft environment (ObtainDiamond task,) as well as synthetic grid world environments.   ## Reviewer bJbP *Strengths:*    Empirical evaluation is well done. *Weaknesses:*   The writing requires more work.   Limited experiments: Mostly on toy grid world/navigation environments, it is not clear if the results will generalize to the control problems.  ## Reviewer mK3T *Strengths:*    Simple and effective technique for identifying sub goals.   Large improvements over original RUDDER.   Impressive results on Minecraft. *Weaknesses:*   More through ablations on the importance of different elements of Align RUDDER.   Presentation and writing need more improvements.   Assumption of a single underlying successful strategy is an important limitation.   Figure 1 is problematic and confusing because of the way it explains the RUDDER algorithm.   ## Reviewer nk2L *Strengths:*    Impressive results on Minecraft. *Weaknesses:*   Poor justification and motivation.   RUDDER vs Align RUDDER comparisons are only done on two grid world environments.   More ablations are required to justify the approach.   Writing requires more work, some important concepts require more clarity. Some undefined concepts...   Incorrect claims such as: > Q function of an optimal policy resembles a step function  ## Reviewer YcqX *Strengths:*    Strong motivation.   MSA for demos is novel.   Strong experimental results. *Weaknesses:*    Several grammatical errors.   The method is not explained well in the paper, the writing needs more work to improve the clarity.   Lack of sufficient analysis and ablations on the Align RUDDER approach.  ## Key Takeaways and Thoughts Overall, the result provided in this paper in the Minecraft environment is impressive. The motivation for the Align RUDDER is clear for me. I like the paper; in particular, the application of the MSA for the alignments across the demos is novel. However, as all the reviewers of this paper agreed that the paper is unclear, especially the method description requires more work. The paper needs to present more ablations and analysis to justify which components of Align RUDDER algorithm are essential. I agree with both insights, the authors have made improvements in the paper to improve the exposition of the algorithms, but still, the paper feels a bit rushed. I would recommend the authors reconsider the paper s current structure and improve the writing further, especially the description of the method can be further improved. I would recommend that the authors fix those essential issues with the paper and the other comments reviewers made in a future resubmission.
The reviewers liked the paper in general but the empirical evaluation lacks studies on a wider range of different data sets.
The paper proposed a sharp attention mechanism in the context of image to sequence modeling. It seeks to build a “clear” alignment from the attention in order to improve the performance of the task. I don’t think there is a general consensus in the research community that the “clear” or “hard” attention performs better than the vanilla “soft” attention. Therefore, experiments become the key in justifying such motivation (and the model). However, as all the reviewers point out, the experiments in this paper are not satisfying. The numbers are far from the current mainstream results (Reviewer mESb). The experiments are done on relatively small datasets/tasks (Reviewer 1PRC) and the comparisons aren’t strictly speaking fair (Reviewer 5Hay). I think this alone is enough reason for the rejection of this paper.   Additionally, on the algorithmic side, the novelty of this mechanism is not that high, given the existence of work such as the hard attention (Xu et al, 2015) and variational attention (Deng et al, 2018). It is also an unanswered question how such a mechanism can be introduced into the modern architectures that use attention (e.g. the multi head attention). The authors did not respond to the questions of the reviewers.
Two of the reviewers liked the intent of the paper   to analyze gradient flow in residual networks and understand the tradeoffs between width and depth in such networks.  However, all reviewers flagged a number of problems in the paper, and the authors did not participate in the discussion period.  Pros: + Interesting analysis suggests wider, shallower ResNets should outperform narrower, deeper ResNets, and empirical results support the analysis.  Cons:   Independence assumption on weights is not valid after any weight updates.   The notation is not as clear as it should be.   Empirical results would be more convincing if obtained on several tasks.   The architecture analyzed in the paper is not standard, so it isn t clear how relevant it is for other practitioners.   Analysis and paper should take into account other work in this area, e.g. Veit et al., 2016 and Schoenholz et al., 2017. 
This paper proposes to train and compose neural networks for the purposes of arithmetic operations. All reviewers agree that the motivation for such a work is unclear, and the general presentation in the paper can be significantly improved. As such, I cannot recommend this paper in its current state for publication. 
The paper presents a simple and effective solution to tune the receptive field of CNNs for 1D time series classification. The reviewers think the idea is original and elegant but would appreciate more theoretical insights into the solution.
This paper proposed algorithms (based on natural actor critic methods) to solve two player zero sum Markov games. The authors established theoretical support for the convergence properties and hence sample complexity of the proposed methods. The authors claimed, based on their theoretical results, that the proposed methods are sample efficient.   As the reviewers pointed out, the original submission focused on the dependency on epsilon without explicit dependency on other important parameters like S, A, B, etc. The revised version has made explicit the dependencies on all these problem parameters, which I appreciated. However, the sample complexity presented in the new version scales as either S^3 max{A,B} or S^4 * \max{A,B}^6 on the sizes of state space and action spaces, which are all huge. What is more, the sample complexities also rely on additional parameters like rho, x, y, which could all depend on S,A,B, etc. As a result, the resulting sample complexity bounds do not seem to imply sample efficiency. In addition, Assumptions 1 and 2 are somewhat unnatural to make.
The reviews end up split. The most positive reviewer notes that the paper may be useful broadly to researchers and practitioners. That is the main promise of the work. However as another reviewer points out, the authors fall short of convincingly explaining how the said practitioners and researchers would benefit from the work. And, also as noted in a review, the paper does not quite go deep enough into the discussion of what saliency means for different methods analyzed.  Contrary to the positive reviewers comment, I do not think that a paper must provide "novel work to built upon", but for a paper that doesn t, there is a significant threshold on how convincing it must be regarding the potential impact of the work. I think the paper in its current state, even after the rebuttal/revision, does not meet this threshold.
The paper studies poisoning attacks of small perturbations to training samples. Existing attacks introduce features that allow for easily fitting the data, but do not lead to good generalization. They use this principle to generate attacks based on random class specific patterns. They finally propose defense using a pre trained model whose last layer is fine tuned. Several criticisms of novelty of the work in comparison to prior work were raised during the reviews. At the same time the validity and effectiveness of defenses remains unsubstantiated to a large extent. Although the authors put an effort in addressing these concerns, for the most part some reviewers and myself remain critical of the contributions and novelty of this work.
The paper proposed to learn a sequence metric by sharing a memory cell between two LSTMs that run on pairs of sequences. I found the idea quite interesting, but I (and the reviewers) found the inspiration and the analogy from a dynamical systems perspective a unclear, unconvincing and maybe not even necessary to the core essence of the method that was proposed.   The reviewers appreciated the improvement in clarity over the course of the review, but felt that there was still some more distance to cover. In addition, the results were not of a high enough quality to lend support to the success of the method and the experimental section needs more work making it not ready for ICLR acceptance at this time. 
This paper presents work on a dataset for object concept learning.  The main contributions include causal relations in the dataset and a method (OCRN).  The initial reviews pointed to concerns over the nature of the causal relations, the presentation of the paper, and the OCRN method and its motivations / use of the do operator.  The reviewers engaged in significant discussions after considering the authors  responses and the others  reviews.  After this delibration, the concerns over the dataset, its annotations, and the presentation of the methods were deemed to be better served by a full revision and reconsideration of the paper.  As such, the paper is not recommended for acceptance at this time.
The paper is written in defense of pseudo labeling. The authors `aim at demonstrating that pseudo labeling based methods can perform on par with consistency regularization methods which have been show to achieve strong performance.  The paper is well written and easy to follow. The reviewers are generally positive about the contribution. It has to be underline, however, that pseudo labeling is still a controversial approach with a very limited theoretical understanding. This paper does not provide any further understanding of it, but proposes several "heuristics", intuitively well motivated, but justified only experimentally. Nevertheless, the results are promising and the paper is an important voice in the general discussion around learning with weak labels and semi supervised learning, two crucial problems in many practical applications. Taking this into account I recommend to accept the paper as a poster.   The reviewers have raised several problems that the authors have been exhaustively discussing in their rebuttals. The one remaining issue is the interaction between calibration and the threshold. This problem has to be clarified in the final version of the paper, as indeed calibration usually does not change the order.
This work tries to tackle the problem of anomaly detection across different tasks. To do so, authors employ energy based models (EBMs) and define an outlier score in terms of the EBM energies, having a shared sparse code for different tasks. This pipeline is tested on some image and video anomaly datasets for industrial inspection.  The reviewers highlighted some concerns that need to be addressed before the paper is ready for publication.   First, a revision could benefit from a rewriting the clearly formalizes the learning problem from page 2 and then discusses about the possible modeling options given i) the task at hand and ii) some efficiency requirements.  Second, concerning the modeling choices of the proposed pipeline, the motivation behind the choice of EBMs should be strengthened. For example, it is not clear why the proposed sparse coding could be used for any other latent variable probabilistic model. As observed by one reviewer, the pros of having energies instead of probabilities (or just reconstructions from a deterministic autoencoder) is not discussed sufficiently. Additionally, the heuristics of running Langevin dynamics for only 5 steps should be backed up by stronger empirical evidence as it lacks theory, and it should be discussed how much you should run the Markov chain to obtain sensible negative samples.  Third, conclusions over the experiments on the provided benchmarks seem preliminary. For instance, a new revision could benefit from adding a statistical significance analysis to the reported accuracies. I appreciate that authors added further ablation studies including experiments on contaminated data in the latest revision. I suggest them to extend the experimental suite to more benchmarks including the commonly used for anomaly detection.
The paper proposes a  deep learning architecture for forecasting Origin Destination (OD) flow. The model integrates several existing modules including spatiotemporal graph convolution and periodically shifted attention mechanism.   The reviewers agree that the paper is not written well, and the experiments are also not executed well. Overall, we recommend rejection.
The authors extend prior work showing that networks trained to be certifiably robust using interval bound propagation are universal approximators. They extend prior results applicable to ReLU networks to a much broader class of networks with general activation functions.   The paper makes an interesting contribution to the literature relative to prior literature showing that one need not sacrifice universal approximation guarantees while training networks with IBP to be certifiably robust to l_inf attacks.  Since the paper is primarily theoretical, the main concern raised by the reviewers was around novelty and the theoretical significance of ideas presented relative to prior work. While proof techniques may be novel, the extension of AUA results to alternate activation functions is not surprising and do not substantially contribute to the field s understanding of learning certifiably robust networks particular since most SOTA results for IBP based training have been achieved with ReLU based networks. The authors  rebuttal did not providing convincing arguments for the reviewers to revise their scores. Hence I do not feel that the contributions of the paper justify acceptance at this time.
The paper proposes to use absolute value activations, in a joint supervised + unsupervised training (classification + deep autoencoder with tied encoder/decoder weights). Pros:  + simple model and approach on ideas worth revisiting Cons:   The paper initially approached these old ideas as novel, missing much related prior work   It doesn t convincingly breathe novel insight into them.   Empirical methodology is not up to standards (non standard data split, lack of strong baselines for comparison)   Empirical validation is too limited in scope (MNIST only).
The manuscript introduces a taxonomy for organizing continual learning research settings and a software framework that realizes this taxonomy. Each continual learning setting is represented by as a set of shared assumptions (e.g., are task IDs observed or not) represented in a hierarchy, and the software is introduced with the hopes of unifying continual learning research.  The manuscript identifies a clear issue in the field: settings and methods for continual learning have proliferated so that there is little coherence in benchmarks, making progress difficult to judge. Reviewers generally agreed that the motivation of building software to help unify continual learning research was a positive.  However, reviewers also pointed to many concerns with the manuscript and software package (Sequoia) that comprises its main contribution. In particular, there is concern that the software is at an early stage of development and makes heavy use of existing libraries to function (e.g. Avalanche and Continuum). This makes it unclear what Sequioa offers over using its dependencies directly. As well, there is concern that multiple standard benchmark tasks and common methods are missing from the implementation — particularly for large scale experiments with, e.g. ImageNet 1k. In theory, the library allows extension and these might be implemented by others in the community. However, this would require that the original manuscript+software are strong enough to draw buy in from other researchers.  In sum, the manuscript+software does not yet offer a convincing starting point for researchers looking for a starting point to begin their continual learning research.
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * Solid experimental results against strong baselines on a task of great interest * Method presented is appropriate for the task * Paper is presented relatively clearly, especially after revision  Cons: * The paper is somewhat incremental. The basic idea of aggregating across multiple examples was presented in Kadlec et al. 2016, but the methodology here is different. 
The paper introduces the concept of an Expert Induced MDP (eMDP) to address imitation learning settings where environment dynamics are part known / part unknown. Based on the formulation a model based imitation learning approach is derived and the authors obtain theoretical guarantees. Empirical validation focuses on comparison to behavior cloning. Reviewers raised concerns about the size of the contribution. For example, it is unclear to what degree the assumptions made here would hold in practical settings.
Fitting a neural net is a stochastic process, with many sources of stochasticity, including initialization, batch presentation, data augmentation, non deterministic low level operations and the non associativity of rounding errors in multi threads systems such as GPUs and TPUs. In this paper, the authors aim to alleviate this randomness by incorporating specific regularizers during learning or by using co distillation.  As the reviewers pointed out, the paper is quite clearly written, but the motivation for this work is not clear. The example of system updates does not correspond to the current study that targets the internal variability of the learning process. Reproducibility is an important issue, but in a statistical context, why would it be relevant to assess reproducibility by the individual decisions made by a single estimate? The usual way of assessing learning algorithms is to look at (a summary of) the distribution of performance for a given learning problem characterized by a data distribution, not to look at individual decisions made by a particular estimate. Furthermore, this study ignores the randomness due to the selection of hyper parameters. Why would the partial reproducibility studied here, for a fixed choice of hyper parameters, be of particular interest? As is, either the work is ill defined and incomplete, or it lacks a clear rationale, and I thus recommend rejection.   I would also like to point a reproducibility issue in the proposed experimental study. The exact meaning of the variability measures reported in the tables is not given, but I assume that it is the standard deviation of the different runs (for example, the 5 replicates in Table 1). These figures are not directly related to the variability of each setup, as they ignore the variability due to the random selections during the ablation study (for example, as I understand it, the last result of Table 1 was obtained for a single arbitrary initialization and a single arbitrary batch order).  
The paper proposes a new loss function for the training of spiking neural networks leading to significant improvements in generalization performance across a variety of datasets and network architectures. While conceptually simple, the approach leads to substantial performance gains, and some intuition is provided to explain its success.  The reviewers are split on the issue of significance of the paper, in part due to the simplicity of the proposed loss function. Still, good results speak for themselves, and the effectiveness of the technique has been demonstrated thoroughly.
The paper proposes a form of autoencoder that learns to predict the neighbors of a given input vector rather than the input itself.  The idea is nice but there are some reviewer concerns about insufficient evaluation and the effect of the curse of dimensionality.  The revised paper does address some questions and includes additional helpful experiments with different types of autoencoders.  However, the work is still a bit preliminary.  The area of auto encoder variants, and corresponding experiments on CIFAR 10 and the like, is crowded.  In order to convince the reader that a new approach makes a real contribution, it should have very thorough experiments.  Suggestions:  try to improve the CIFAR 10 numbers (they need not be state of the art but should be more credible), adding more data sets (especially high dimensional ones), and analyzing the effects of factors that are likely to be important (e.g. dimensionality, choice of distance function for neighbor search).
The paper proposes a quantization framework that learns a different bit width per layer.  It is based on a differentiable objective where the Gumbel softmax approach is used with an annealing procedure.  The objective trades off accuracy and model size.  The reviewers generally thought the idea has merit.  Quoting from discussion comments (R4): "The paper cited by AnonReviewer 3 is indeed close to the current submission, but in my opinion the strongest contribution of this paper is the formulation from architecture search perspective." The approach is general, and seems to be reasonably efficient (ResNet 18 took "less than 5 hours")  The main negatives are the comparison to other methods.  In the rebuttal, the authors suggested in multiple places that they would update the submission with additional experiments in response to reviewer comments.  As of the decision deadline, these experiments do not appear to have been added to the document. In the discussion: R4: "This paper seems novel enough to me, but I agree that the prior work should at least be cited and compared to. This is a general weakness in the paper, the comparison to relevant prior works is not sufficient." R3: "Not only novel, but more general han the prior work mentioned, but the discussion / experiments do not seem to capture this."  With a range of scores around the borderline threshold for acceptance at ICLR, this is a difficult case.  On the balance, it appears that shortcomings in the experimental results are not resolved in time for ICLR 2019.  The missing results include ablation studies (promised to R4) and a comparison to DARTS (promised to R3): "We plan to perform the suggested experiments of comparing with exhaustive search and DARTS. The results will be hopefully updated before the revision deadline and the camera ready if the paper is accepted." These results are not present and could not be evaluated during the review/discussion phase.
Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning  The topic is maximally timely and important: Understanding human decision making behaviour based on observational data. Any tangible steps towards this challenging goal are bound to be significant, and those this paper makes.  A Bayesian policy learning method is introduced for this task, and validated on both simulated data and user exeperiments in a real decision making task. The novel contribution is on learning interpretable decision dynamics  The paper is written clearly enough..  The updated paper clarified most major concerns the reviewers had. In particular, they added a user study.  The biggest remaining weaknesses are that    relationship to the AMM model did not become completely clear yet    the real user study has been carried out with only a small set of users. But a large cohort study would be too much work to ask for a paper which has also a strong methodological contribution. 
In this paper, the authors combine ideas from SLAM (using an Extended Kalman Filter and a state with nonlinear transitions and warping) and differentiable memory networks that store a spherical representation of the state (from the ego centric point of view of an RL agent moving in an environment) with depth and visual features stored at each pixel and dynamics transitions corresponding to warping.  The main idea in the paper is very simple and elegant, but I will concur with the reviewers that the writing of the first version of the paper was extremely hard to understand and that the experimental section was too dense. Two subsequent revisions of the paper have dramatically improved the paper.  Given the spread of scores (R1: 6, R2: 7 and R3: 4) and the fact that only R1 and R2 have acknowledged the revisions, I will veer towards acceptance. 
This paper proposes a modification to MCTS in which a sequence of nodes (obtained by following the policy prior) are added to the search tree per simulation, rather than just a single node. This encourages deeper searches that what is typically attained by vanilla MCTS. STS results in slightly improved performance in Sokoban and much larger improvements Google Research Football.  R4 and R1 both liked the simplicity of the idea, with R1 also praising the paper for the thoroughness of its evaluation. I agree that the idea is interesting and worth exploring, and am impressed by the scope of the experiments in the paper as well as the additional ones linked to in the rebuttal. However, R1 and R5 explicitly noted they had many points of confusion, and across the reviews there seemed to be many questions regarding the difference between STS and other variants of MCTS. I also needed to read parts of the paper multiple times to fully understand the approach. If this many experts on planning and MCTS are confused, then I think readers who are less familiar with the area will definitely struggle to understand the main takeaways. While I do think the clarifications and new experiments provided in the rebuttal help, my overall sense is that the paper at this stage is not written clearly enough to be ready for publication at ICLR. I would encourage the authors to try to synthesize their results and organize them more succinctly in future versions of the paper.  One comment about a point of confusion that I had: I noticed the PUCT exploration parameter was set to zero for Sokoban, and one for GRF (with an explanation given that many values were tried, though these values are unspecified). As the exploration parameter is normally considered to be the thing that controls whether MCTS acts more like BFS ($c   \infty$) or DFS ($c   0.0$), I would encourage the authors to more explicitly report which values they tried and to be clearer about the advantage of STS s multi step expansions over low values of the exploration parameter.
The focus of the submission is the measuring of the discrepancy of two probability distributions. Using the notion of H entropy, the authors propose a new divergence measure the H divergence (Def. 2), as a common generalization of Jensen Shannon divergence and maximum mean discrepancy. They suggest an empirical estimator for H divergence and show that it is consistent (Theorem 2). The efficiency of the technique is illustrated in the context of 2 sample testing, sample quality evaluation and measuring climate change.  Overall, the submission addresses an important problem (defining a class of divergence measure). As pointed out by the reviewers, however fundamental questions on the proposed estimator are not addressed: 1)the motivation of using a set of actions (in the definition of H divergence) and how to design them are unclear,  2)the impact of the loss function l (in the definition of H divergence) is not explored: this can easily lead to instabilities due to the lack of closed form solution and by the arising non convex optimization task. The gain compared to existing solutions have to be understood and explored further.
The reviewers agree that the proposed method of joint model policy optimization using a lower bound is novel and interesting and worthwhile pursuing. But all reviewers find a variety of issues in the paper, such that ratings are just above borderline or below. Given all the mixed feedback, it appears that the paper is still a bit premature for publication and could greatly benefit from improvements in a future submission.
This paper develops an active variable selection framework that couples a partial variational autoencoder capable of handling missing data with an information acquisition criteria derived from Bayesian experimental design. The paper is generally well written and the formulation appears to be natural, with a compelling real world healthcare application. The topic is relatively under explored in deep learning  and the paper appears to attempt to set a valuable baseline. However, the AC cannot recommend acceptance based on the fact that reviewer 2 has brought up concerns about the competitiveness of the approach relative to alternative methods reported in the experimental section, and all reviewers have found various parts of the paper to have room for improvement with regards to technical clarity. As such the paper would benefit from a revision and a stronger resubmission.
The reviewers agree that the formulation is novel and interesting, but they raised concerns regarding the motivation and the complexity of the approach. I find the authors  response mostly satisfying, and I ask them to improve the paper by incorporating the comments.  Detailed comments: The maximum entropy objective used in Eq. (13) reminds me of maximum entropy RL objective in previous work including [Ziebart, 2010], [Azar, 12], [Nachum, 2017], and [Haarnoja, 2017].
This paper shows that linear layers can be replaced by butterfly networks. Put simple, the paper follows the idea of sketching to design new architectures that can reduce the number of trainable parameters and also gives the theoretical and empirical analysis to validate this claim. In this regard, the paper would be  appealing.  But the theoretical results given in this paper are incremental.
The authors propose a way to recover latent factors implicitly constructed by a neural net with black box access to the nets output. This can be useful for identifying possible adversarial attacks. The majority of reviewers agrees that this is a solid technical and experimental contribution.
This paper presents a framework for joint differentiable simulation of physics and image formation for inverse problems. It brings together ideas from differentiable physics and differentiable rendering in a compelling framework.
This paper introduces a defense method (gradient broadcast adaptation) against backdoor attacks on pretrained language models. It proposes to utilize prompt tuning to guide the perturbed weights back to a normal state and thus helps avoid the degradation of model s generalization ability.   Strengths:   Experiments are conducted across multiple datasets with different types of backdoor attacks, demonstrating the effectiveness of the proposed approach   The proposed idea is well motivated and intuitive  Weakness:   Improvement on experiment results seems marginal   Some technical details of the attack setup are unclear   Writing of the paper needs improvement
The authors introduce a framework for automatically detecting diverse, self organized patterns in a continuous Game of Life environment, using compositional pattern producing networks (CPPNs) and population based Intrinsically Motivated Goal Exploration Processes (POP IMGEPs) to find the distribution of system parameters that produce diverse, interesting goal patterns.  This work is really well presented, both in the paper and on the associated website, which is interactive and features source code and demos. Reviewers agree that it’s well written and seems technically sound. I also agree with R2 that this is an under explored area and thus would add to the diversity of the program.  In terms of weaknesses, reviewers noted that it’s quite long, with a lengthy appendix, and could be a bit confusing in areas. Authors were responsive to this in the rebuttal and have trimmed it, although it’s still 29 pages. My assessment is well aligned with those of R2 and thus I’m recommending accept. In the rebuttal, the authors mentioned several interesting possible applications for this work; it’d be great if these could be included in the discussion.   Given the impressive presentation and amazing visuals, I think it could make for a fun talk. 
This paper presents work on zero shot learning.  The reviewers appreciated the simplicity of the method and its clear exposition.  However, concerns were raised over novelty, motivation, and empirical validation.  After reading the authors  response, the reviewers remained of the opinion that these concerns have not yet been addressed sufficiently.  Based on these points, the paper is not yet ready for publication.
The authors propose to use counterfactual (a.k.a as contrastive, as they do not account for causal mechanism) explanations  to explain the errors of an already trained predictive model with images as input data. To this end the authors rely on the manipulation of the latent space of  a VAE with disentangled representations. In general the idea is simple (and based on approaches from prior works) and extends work on counterfactual explanations which have been broadly studied in other domains like decision making where the input data have often semantic meaning (in contrast with the pixel of an image).  While the technical contribution is quite limited, I believe that the general approach of the paper is interesting.   However, even after the rebuttal and reading the updated version, it is still unclear what exactly means key concepts for the paper like trivial and actionable, and more importantly how to use the proposed approach in practice, beyond checking/correcting for potential gender bias in  the data (given that you have access to gender information). In particular, it is not clear how you measure "non triviality" for the predictive task when you do not have additional knowledge (like the gender) or when the disentangled latent representation do not correspond to semantical features (which as far as I understand they do not need to).  Similarly, while actionability and diversity have been broadly discussed in the decision making domain, it is again not clear what an actionable feature means here to me. Is it just that you can perturb the latent space?   Furthermore, I believe it is worth exploring the connection to approaches for adversarial examples. As it has already discussed in the literature, in terms of formulation,  counterfactual explanations resemble the problem of adversarial examples, but it seems substantially different semantically. At times when reading the paper, it feels that it is indeed more related to adversarial examples than to counterfactual explanations, as the explainability part seems quite superficial. Thus, I would encourage the authors to better position their paper.   In summary, I believe that the paper requires further work before being ready for publication. In particular, the paper would significantly from: i) a better positioning of paper with respect to the literature; ii) formally introduce  key concepts like actionability, diversity and triviality, explaining what they mean in this context, and how to measure them; and more importantly, iii)  explaining how the proposed approach (which by the way involves training a generative model)  can be used in general to  understand  a model. On a final note, I believe that the paper would benefit from from bringing back to the main body of the paper the experiments that were moved to the appendix during the rebuttal.  
This paper extends recent and very active literature on analyzing learning algorithms in the simplified setting of Gaussian data and model weights, with the main generalization being to allow for non isotropic covariance matrices.  The main technical results seem to be correct and slightly novel, though reviewers feel they are not innovative or unexpected enough to stand on their own.  However, the main contributions of the paper are then to interpret these results to give phenomenological results (regarding double descent, etc.), and reviewers were unanimously happy with these. In the end, all reviewers were positive about the paper.  The largest reviewer criticisms of the paper were technical issues (ot31) and lack of context of recent literature (3RfG). Both of these concerns were mostly addressed by the revision/rebuttal. The reviewers still had specific comments on improvement, but found no major faults.
The paper presents an empirical comparison of translation invariance property in CNN and capsule networks. As the reviewers point out, the paper is not acceptable quality at ICLR due to low novelty and significance. 
Although this paper proposes an intriguing method for using neuron importance based regularization to reduce catastrophic forgetting in continual learning, the method is substantially based upon Jung et al (2020), reducing its novelty. Additionally, the experimental evaluation was unconvincing that the proposed method is an improvement over current methods and precisely how the proposed method differs from Jung et al.  The authors are encouraged to revise the paper to incorporate the reviewers suggestions and many of the points the authors raised in their rebuttals, which the reviewers felt were not adequately addressed in the current version of the paper (as mentioned in private discussions among the reviewers).
The authors identify a limitation of aggregating GNNs, which is that global structure can be mostly lost. They propose a method which combines a graph embedding with the spatial convolution GNN and show that the resulting GNN can better distinguish between similar local structures.   The reviewers were mixed in their scores. The proposed approach is clearly motivated and justified and may be relelvant for some graphnet researchers, but the approach is only applicable in some circumstances   in other cases it may be desirable to ignore global structure. This, plus the high computational complexity of the proposed approach, mean that the significance is weaker. Overall the reviewers felt that the contribution was not significant enough and that the results were not statistically convincing.  Decision is to reject.
This paper follows the observations of Renda et al. (2020) that the learning rate in the fine tuning or retraining phase of neural network pruning is an under considered component of the pruning process. Renda et al. (2020) argue for a technique that uses the learning rate schedule of the original training regime for fine tuning. However, their work does not offer a hypothesis or an explanation for why this works.  This work instead offers more insight into why reusing the original learning rate is productive. Specifically, it shows that using high learning rates is the key component. To demonstrate this, the paper includes a study of using the original step wise learning from the original training regimen, except accelerated for a given number of fine tuning epochs. The paper also demonstrates that Cyclic Learning Rate Restarting (CLR) also provides an effective, if not better, learning rate schedule for fine tuning.  As noted by the reviewers, the core observations and contributions of this work are modest, but are still a valuable addition to the literature in the pruning community.    Having said that, there are some confounding issues with CLR. Specifically, that CLR itself may simply be a more effective learning rate schedule for training neural networks, independent of the particular application to fine tuning (Reviewer 1). The revision includes an additional appendix that dispels some of this concern. However, indeed, the CLR does improve the base network performance for some configurations.  Broadly, the value proposition here is a thorough demonstration of learning rate schedules for fine tuning with an overall take that comparisons between techniques need be more sensitive to this choice as previous work perhaps has not thoroughly considered alternative learning rates.  
This paper extends the Neural Relational Inference framework for probabilistic inference of interaction relations between entities, to a scenario where entities may have private features, which requires modifications of the standard graph encoders and decoders in NRI.  Reviewers appreciated both the model and the overall execution of the paper: the building blocks are clear, the evaluation does its job well. The main doubts are about the applicability of the setting, for which the authors don t provide too many examples. However, the construction is somewhat intuitive, and even in cases where private attributes aren t explicit, it may be valuable to disentangle the shareable attributes this way. We encourage the reviewers to discuss the applicability a bit further.  Typos: (not exhaustive, please doublecheck with a spell checker)    multiple occurrences of Gumble instead of Gumbel    bottom of pg 4, factorzied  > factorized
This paper studies whether neural networks with different architectures, especially different width and depth, learn similar representations. All reviewers agree that the investigations are thorough and the experimental discoveries are convincing and well explained. Good work. I recommend accept.
Dear authors,  The reviewers all appreciated the treatment of the topic and the quality of the writing. It is rare for all reviewers to agree on this, congratulations.  However, all reviewers also felt that the paper could have gone further in its analysis. In particular, they noticed that quite a few points were either mentioned in recent papers or lacked an experimental validation.  Given the reviews, I strongly encourage the authors to expand on their findings and submit the improved work to a future conference.
This paper introduces an important and interesting problem and a potentially interesting approach. Unfortunately, the reviewers agree that the current version isn t appropriate for ICLR in its current form. However, hopefully the feedback will be useful for the authors in revising and resubmitting this paper to another venue.
Main content: paper introduces a new variant of equilibrium propagation algorithm that continually updates the weights making it unnecessary to save steady states. T  Summary of discussion: reviewer 1: likes the idea but points out many issues with the proofs.  reviewer 2: he really likes the novelty of paper, but review is not detailed, particularly discussing pros/cons.  reviewer 3: likes the ideas but has questions on proofs, and also questions why MNIST is used as the evaluation tasks. Recommendation: interesting idea but writing/proofs could be clarified better. Vote reject.  
The paper considers Markov potential games (MPGs),  where the agents share some common resource. They consider MPGs with continuous state action variables, coupled constraints and nonconvex rewards, which is novel. The reviews are all positive and point out the novel contributions in the paper
All reviewers recommended reject. No responses from the authors.
Thanks for the detailed replies to the reviewers, which significantly helped us understand your paper better. However, after all, we decided not to accept your paper due to weak justification and limited experimental validation. Writing should also be improved significantly. We hope that the feedback from the reviewers help you improve your paper for potential future submission. 
Dear authors,  I like to topic of your paper very much. Indeed, your work is trying to show that 2nd order methods can be efficiently implemented in a distributed environment and can achieve improvement in training times.  However, having worked on distributed computing for many years, I personally think that reporting running time in your work is not very informative (without mentioning what hardware is used during computation), and one cannot understand the connection or reproduce your results. Also, it was not clear how the baselines were implemented, and how the hyper parameters were tuned. It is also not clear why you haven t picked better benchmarks to compare your work.   I think that addressing both the concerns from reviewers and the one mentioned above would improve the paper significantly. I would really like to see if accepted in the near future after these issues are fully addressed. 
This meta review is written after considering the reviews, the authors’ responses, the discussion, and the paper itself.  The paper has 2 main contributions: 1) analysis of the sensitivity of a deep network predicting steering angle from images w.r.t. different synthetic image perturbations, 2) A training method, based on adaptively adjusted data augmentation, which improves the robustness of a model to seen and previously unseen perturbations.  The reviewers’ opinions are somewhat mixed, leaning towards negative. The reviewers point out that the task is important and the methodology makes sense, but the experiments are limited: only one dataset, only synthetic perturbations, only the steering angle prediction task (which is not necessarily very practical), not strong enough baselines, no results on established datasets like ImageNet C. The authors addressed some of these issues in the updated version of the paper (more datasets, one more baseline), but most reviewers did not change their evaluation.  Based on all this information and reading the paper itself, I recommend rejection at this point. The paper has interesting ideas, but the experimental evaluation is not sufficient. Moreover, I find the use of the steering prediction task confusing   there does not seem to be anything driving specific in the method, so using standard datasets (like ImageNet C) would be more convincing. For driving datasets, using real world (not synthetic) image perturbations would be advisable. As the paper stands, it looks neither like a proper application paper, nor as a fundamental method/analysis paper, but something inbetween, which is not to its favor. 
This paper proposes a global model agnostic explanation method. The method relies on a neural model that learns to predict which input features are important for the original model’s predictions. Using experimentation, the authors demonstrate that their approach outperforms LIME and Integrated Gradients. They compare the explanation methods in terms of the faithfulness of explanations and computational complexity. While the premise of the work is interesting, reviewers have suggested several areas for improvement: i) writing can use more clarity. writing seems verbose and hard to understand especially in intro and methods section ii) several points have been raised about empirical evaluation including lack of user studies, a more extensive set of baselines and data modalities. Given this, we are unable to recommend an acceptance at this time. We hope the authors find the reviews helpful.
This paper provides interesting results on convergence and stability in general differentiable games. The theory appears to be correct, and the paper reasonably well written. The main concern is in connections to an area of related work that has been omitted, with overly strong statements in the paper that there has been little work for general game dynamics. This is a serious omission, since it calls into question some of the novelty of the results because they have not been adequately placed relative to this work. The authors should incorporate a thorough discussion on relations to this work, and adjust claims about novelty (and potentially even results) based on that literature.
This paper explores addition of a version of divisive normalization to AlexNets and compares performance and other measures of these networks to those with more commmonly used normalization schemes (batch, group, and layer norm).  Various tests are performed to explore the effect of their divisive normalization.  Scores were initially mixed but after clarifications for design and experiment decisions, and experiments run in response to comments by the reviewers the paper improved significantly.  While reviewers still had several suggestions for further improvements, after the authors   revisions reviewers were in favor of acceptance which I support.
The authors propose improved techniques for program synthesis by introducing the idea of property signatures. Property signatures help capture the specifications of the program and the authors show that using such property signatures they can synthesise programs more efficiently.  I think it is an interesting work. Unfortunately, one of the reviewers has strong reservations about the work. However, after reading the reviewer s comments and the author s rebuttal to these comments I am convinced that the initial reservations of R1 have been adequately addressed. Similarly, the authors have done a great job of addressing the concerns of the other reviewers and have significantly updated their paper (including more experiments to address some of the concerns). Unfortunately R1 did not participate in subsequent discussions and it is not clear whether he/she read the rebuttal. Given the efforts put in by the authors to address different concerns of all the reviewers and considering the positive ratings given by the other two reviewers I recommend that this paper be accepted.   Authors, Please include all the modifications done during the rebuttal period in your final version. Also move the comparison with DeepCoder to the main body of the paper.
The paper proposes and evaluates an asynchronous hyperparameter optimization algorithm.  Strengths:  The experiments are certainly thorough, and I appreciated the discussion of the algorithm and side experiments demonstrating its operation in different settings.  Overall the paper is pretty clear.  It s a good thing when a proposed method is a simple variant of an existing method.  Weaknesses:  The first page could have been half the length, and it s not clear why we should care about the stated goal of this work.  Isn t the real goal just to get good test performance in a small amount of time?  The title is also a bit obnoxious and land grabby   it could have been used for almost any of the comparison methods.  The proposed method is a minor change to SHA.  The proposed change is kind of obvious, and the resulting method does have a number of hyper hyperparameters.  Consensus:  Ultimately I agree with the reviewers that is just below the bar of acceptance.  This does seem like a valid contribution to the hyperparameter tuning literature, but more of an engineering contribution than a research contribution.  It s also getting a little bit away from the subject of machine learning, and might be more appropriate for say, SysML.
The article studies benefits of over parametrization and theoretical properties at initialization in ReLU networks. The reviewers raised concerns about the work being very close to previous works and also about the validity of some assumptions and derivations. Nonetheless, some reviewers mentioned that the analysis might be a starting point in understanding other phenomena and made some suggestions. However, the authors did not provide a rebuttal nor a revision. 
The paper addresses interpretability in the video data domain. The authors study and compare the saliency maps for 3D CNNs and convolutional LSTMs networks, analysing what they learn, and how do they differ from one another when capturing temporal information. To search for the most informative part in a video sequence, the authors propose to adapt the meaningful perturbations approach by Fong & Vedaldi (2017) to the video domain using temporal mask perturbations.  While all reviewers and AC acknowledge the importance and potential usefulness of studying and comparing different generative models in continual learning, they raised several important concerns that place this paper below the acceptance bar:  (1) in an empirical study paper, an in depth analysis and insightful evaluations are required to better understand the benefits and shortcomings of the available and proposed models (R5 and R2). Specifically:  (i) providing a baseline comparison to assess the benefits of the proposed approach   please see R5’s suggestions on the baseline methods;  (ii) analyzing how the proposed approach can elucidate meaningful differences between 3D CNNs and LSTMs (R5, R2). The authors discussed in their rebuttal some of these questions, but a more detailed analysis is required to fully understand the benefits of this study.  (2) R5 and R2 raised an important concern that the temporal mask generation developed in this work is grounded on the generation of the spatial masks, which is counterintuitive when analysing the temporal dynamics of the NNs   see R5’s suggestions on how to improve.  Also R5 has raised concerns regarding the qualitative analysis of the Grad CAM visualizations. Happy to report that the authors have addressed these concerns in the rebuttal, namely reporting the results in Table 2 and providing an updated discussion. R1 has raised a concern about the importance of the sub sampling in the CNN framework, which was partially addressed in the rebuttal.  To conclude, the AC suggest that in its current state the manuscript is not ready for a publication and needs a major revision before submitting for another round of reviews. We hope the reviews are useful for improving and revising the paper. 
The submission proposes to learn a causal transformer model over a pretrained VQ GAN representation to generate videos. While the paper is well written and clear, proposing a simple idea, the novelty of this method is not well explained compared to pre existing publications see ([reviews JjT4](https://openreview.net/forum?id K hiHQXEQog&noteId 22EyPvpodh), [3gJ8](https://openreview.net/forum?id K hiHQXEQog&noteId 9ikn_nBC_Sf), [6x6m](https://openreview.net/forum?id K hiHQXEQog&noteId zCX9VP8I5uL), and [pKCo](https://openreview.net/forum?id K hiHQXEQog&noteId uRgHWX4C5yX)) especially since it s lacking [ablation](https://openreview.net/forum?id K hiHQXEQog&noteId uRgHWX4C5yX) or comparative (see [reviews 6x6m](https://openreview.net/forum?id K hiHQXEQog&noteId zCX9VP8I5uL), and[pKCo](https://openreview.net/forum?id K hiHQXEQog&noteId uRgHWX4C5yX)) experiments.  The authors have expressed their consideration of reviewers requests but will not satisfy them in time for this conference. Therefore I am currently recommending this submission for rejection.
This paper describes active vision for object recognition learned in an RL framework. Reviewers think the paper is not of sufficient quality: Insufficient detail, and insufficient evaluation. While the authors have provided a lengthy rebuttal, the shortcomings have not yet been addressed in the paper. 
{418}; {Classifier agnostic saliency map extraction}; {Avg: 4.33}; {}  1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.  The paper is well written and the method is simple, effective, and well justified.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.  1. The introduction, in particular the last row of pg 1, implies that this work is the first to show that a class agnostic saliency estimation method can produce higher quality saliency maps than class dependent ones. However, Fan et al. have already shown this. For this reason, AR1 recommended that the authors reword the introduction to reflect prior work on this aspect but the authors declined to do so. The AC would have liked to see a discussion of how the different points of view of the two works (robustness to corruption vs class agnosticism) both address the same issue (poor segmentation of the salient image regions). 2. The work of Fan et al has a very similar approach and a deeper comparison is needed. While the authors dedicated two paragraphs of discussion to this work, they should have gone further. For example, the work of Fan et al. uses a very simple saliency map extraction network and it s unclear how much this impacts their performance when compared to the proposed method, which uses ResNet50. The AC agrees with the authors that re implementing the method of Fan et al. is asking a lot but a discussion of the potential impact would have sufficed. 3. The authors didn t mention at all the vast body of work on salient object detection (for a somewhat recent review see Borji et al. "Salient object detection: A benchmark." IEEE TIP). The differences to this line of work should have been discussed.  Points 1 and 2 were particularly salient for the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Two major points of contention were:   The discussion of differences between the proposed method and the method of Fan et al.   The fairness of the comparison to Fan et al. AR1 felt that the paper was deficient on both counts (AR2 had similar concerns) and the authors disagreed, arguing that the discussion was complete and the quantitative comparison fair.  The AC was sympathetic to these concerns and found the authors  responses to be dismissive of those concerns. In particular, the AC agrees that the paper, as currently organized, minimizes the degree to which the work is derived from Fan et al.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
The authors propose to take a token level generative approach to the task of vision language navigation (R2R/R4R). The reviewers raise a number of concerns which should be noted in the final version of this work.  The primary concern revolves around generality.  How will this approach generalize to more sophisticated generative and discriminative models?  To what extent is the model relying on the short instruction/action sequences to succeed and would not perform well on longer instructions, longer trajectories, or more abstract language.  Finally, the discussion of the uninformed prior is interesting because while "clean", reviewers note there is no realistic grounded language scenario in which an uninformative prior makes sense.
All three referees have provided detailed comments, both before and after the author response period. While the authors have carefully revised the paper and provided detailed responses, leading to clearly improved clarity and quality, there remain clear concerns on novelty (at least not sufficiently supported with ablation study) and experiments (neither strong enough nor sufficient to support the main hypotheses). The authors are encouraged to further improve their paper for a future submission.
The paper introduces an ensemble of RL agents that share knowledge amongst themselves. Because there are no theoretical results, the experiments have to carry the paper.  The reviewers had rather different views on the significance of these experiments and whether they are sufficient to convincingly validate the learning framework introduced. Overall, because of the high bar for ICLR acceptance, this paper falls just below the threshold.  
The authors address the problem of training an NMT model on a really massive parallel data set of 40 billion Chinese English sentence pairs, an order of magnitude bigger than other cz en experiments. To address noise and training time problems they propose pretraining + a couple of different ways of creating a fine tuning data set. Two of the reviewers assert that the technical contribution is thin, and the results are SOTA but not really as good as you might hope with this amount of data. This combined with the fact that the data set is not released, makes me think that this paper is not a good fit with ICLR and would more appropriate for an application focussed conference. The authors engaged strongly with the reviewers, adding more backtranslation results. The reviewers took their responses into account but did not change their scores. 
The paper addresses the problem of learning disentangled representations in supervised and unsupervised settings.   In general, the problem of representation learning in of course a core problem in ICLR. However, in the set up described by the authors, R2 commented on the the set up for supervised being a bit unnatural in as detailed labels need to be given (somewhat confusingly, the labels are called control variates in the paper).  Several reviewers commented on the novelty of the paper being on the low side, with R2 commenting the contribution being fairly small, and R3 noting similarities to stackgan.  There were also some comments on quality, and clarity. On the topic of technical quality, R2 did note that the authors present extensive results, but R3 mentions that the case for the disentanglement improving is not sufficiently supported. In terms of clarity, there was some initial confusing about e.g. the inference procedure, though the authors addressed these issues in the discussion. 
Reviewers are in a consensus and recommended to reject. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit.
While there has been lots of previous work on training dictionaries for sparse coding, this work tackles the problem of doing son in a purely local way. While previous work suggests that the exact computation of gradient addressed in the paper is not necessarily critical, as noted by reviewers, all reviewers agree that the work still makes important contributions through both its theoretical analyses and presented experiments. Authors are encouraged to work on improving clarity further and delineating their contribution more precisely with respect to previous results.
This paper proposes techniques for improving the scalability of set to hypergraph models. The main issue with the submission is that all reviewers found the clarity of the paper to be problematic including the proofs, the experimental conditions, and many other parts. The authors responded but some reviewers explicitly state that their questions have only partially been answered and some reviewers did not respond to the authors. Unfortunately, given the number of clarity issues raised by the reviewers it makes more sense to re submit this paper after re writing based on all the suggestions from the reviewers.
The paper introduces the use of J S shrinkage estimator in policy optimization, which is new and promising.  The results also show the potential.  That said, reviewers are not fully convinced that in its current stage the paper is ready for publication.  The approach taken here is essentially a combination existing techniques.  While it is useful, more work is probably needed to strengthen the contribution.  A few directions have been suggested by reviewers, including theoretical guarantees and stronger empirical support.
The reviewers had remarkably consistent feedback about this paper. They appreciated the formulation of the federated learning problem with architectures having both shared and private (personalized) components. On the other hand, they felt the experiments were insufficient to prove the effectiveness of the method, and had several suggestions in terms of tasks and datasets. They also felt that it s hard to assess whether the existence of private/personalized components is warranted without visualizing the difference between architectures. Overall, the reviewers had good feedback that could strengthen the paper.
We thank the authors for their submission. The paper feels more like an early draft, with several fundamental factual mistakes (mistake on computational and statistical complexities) as highlighted by the reviewers. There s plenty of material in the reviews to help authors improve their submission, we encourage them to use these recommendations to improve motivation / experiments.
The paper provides an algorithmic framework to accelerate RL through Behavioral Priors, while having some notion of safety incorporated. The reviewers are divided about this paper:  On the positive side, some of the reviewers consider the problem important, and the experimental results reasonable and promising.  On the negative side, reviewers raised issues such as 1) The paper is on a heuristic side. 2) No formal guarantee on the safety is provided. 3) The paper is not as self contained as it should be, as it relies much on Singh et al. (2021). 4) The algorithm requires access to unsafe offline data.  I do not give the same weights to all these concerns. For example, even though (d) is an issue in some applications, it is alright for others. What concern me most are (1) and (2).   A method for safety that is only evaluated empirically and does not have any formal guarantee cannot be used for safety critical tasks. I realize that some other published papers may have the same issue. But given that this is a real concern, and that two out of four reviewers believe that the paper should not be accepted, unfortunately I cannot recommend acceptance of this paper, especially given the competitiveness of this conference.  P.S: I also noticed that in the proof of Proposition 3.1, an expectation term $E[p_\phi(a|s,c)]$ in Eq. (9) is replaced by a $\log p_\phi(a|s,c)$. This requires more justifications.
The paper studies the problem of stochastic optimization where the gradient noise process is non stationary. While this is an important problem in the community, the reviewers find that the assumptions are poorly justified. While the authors provided extensive feedback, the reviewers did not change their initial assessment. This paper can therefore not be accepted in its current form. I think the reviewers provided some very critical and useful feedback and I therefore strongly encourage the authors to take advantage of this feedback to resubmit their paper to another venue. 
The paper proposes a simple modification to conditional GANs, where the discriminator involves an inner product term between the condition vector y and the feature vector of x. This formulation is reasonable and well motivated from popular models (e.g., log linear, Gaussians). Experimentally, the proposed method is evaluated on conditional image generation and super resolution tasks, demonstrating improved qualitative and qualitative performance over the existing state of the art (AC GAN). 
This paper proposes a method for learning non separable Hamiltonian dynamics by including a state of the art symplectic integrator (Tao, 2016) into the model training pipeline. This is a nice improvement on the past work that primarily addressed separable Hamiltonians. The reviewers agree that the paper is well written and that the empirical evaluation is solid. The paper was further improved during the discussion period by incorporating the reviewers  feedback. For this reason I am happy to recommend this paper for acceptance.
This paper got 3 acceptance and 1 marginally below the threshold. After the rebuttal, the rating was raised to above the threshold. All the reviewers are positive about this submission. They agree that the method proposed in the submission is novel, the experiments are comprehensive and convincing. AC agrees and recommend acceptance.  
This paper exposes a method to reduce the training cost of once for all networks. Overall this paper is well written and easy to follow, and the experimental section shows a clear reduction of training time on the examples used. However, the reviewers point out that the experimental section could benefit from adding more design spaces, and have a better explanation of the results. More importantly, three out of four reviewers agree that the novelty of this work is too low for the submission to be accepted, with the fourth one only giving a score of 6 (and also noting the lack of novelty). I therefore recommend reject for this paper.
Learning policies from video demonstrations alone without paired action data is a promising paradigm for scaling up Imitation Learning. As such the paper is well motivated. Two approaches P SIL and P DAC train rewards for RL training, based on learning Sinkhorn distances between trajectory embeddings and an adversarial approach.  The reviews brought up lack of clarity in presentation and experimental results and ablation studies falling short of convincingly demonstrating value of distance functions used and other design tradeoffs. As such the paper does not meet the bar for acceptance at ICLR.
This work presents the Neural Simulated Annealing (NSA) approach as a heuristic for general combinatorial optimization problems. After revising the paper and reading the comments from the reviewers, here are the general comments:     In general, the paper is clear enough. The contributions are stated in a proper way.   The novelty is rather limited, but the key idea of using neural networks in SA, and training it with RL, has merit.   This approach has merit but the novelty is very limited.   The NSA improves the vanilla SA, but the benchmark reveals that NSA is not enough competitive with other state of the art methods.    The benchmark does not reveal enough information about the NSA against the SOTA methods.   The work needs technical improvements and validation is required before accepting the work.
The reviewers tend to agree that the empirical results in this paper are good compared to the baselines. However, the paper in its current form is considered a bit too incremental. Some reviewers also suggested additional theory could help strengthen the paper.
This paper introduces the concept of classifier orthogonalization. This is a generalization of orthogonality of linear classifiers (linear classifiers with orthogonal weights) to the non linear setting. It introduces the notion of a full and principal classifier, where the full classifier is one that minimizes the empirical risk, and the principal classifier is one that uses only partial information. The orthogonalization procedure assumes that the input domain, X can be divided into two sets of latent random variables Z1 and Z2 via a bijective mapping. The random variables Z1 are the principal random variables, and Z2 contains all other information. Z1 and Z2 are assumed to be conditionally independent given the target label. The paper outlines two approaches to construct orthogonal classifiers that operate only on Z2. The approach is highlighted in three applications: controlled style transfer, domain adaptation, and fair classification.  The reviewers all found the proposed method to be principled and compelling. Beyond clarification questions and some discussion on related work, the reviewers raised a few issues that were subsequently addressed: 1) Additional baselines for domain adaptation and fairness. 2) Controlled style transfer being a new task with no established baselines, and 3) The feasibility of training a proper “full classifier” that minimizes the empirical risk, and its necessity in the approach. The authors addressed these concerns and updated the paper, to the satisfaction of the reviewers. All of them unanimously recommend acceptance.
This paper received two weak rejects (3) and one accept (8).  In the discussion phase, the paper received significant discussion between the authors and reviewers and internally between the reviewers (which is tremendously appreciated).  In particular, there was a discussion about the novelty of the contribution and ideas (AnonReviewer3 felt that the ideas presented provided an interesting new thought provoking perspective) and the strength of the empirical results.  None of the reviewers felt really strongly about rejecting and would not argue strongly against acceptance.   However, AnonReviewer3 was not prepared to really champion the paper for acceptance due to a lack of confidence.  Unfortunately, the paper falls just below the bar for acceptance.  Taking the reviewer feedback into account and adding careful new experiments with strong results would make this a much stronger paper for a future submission.
This paper proposes a model based policy optimization approach that uses both a policy and model to plan online at test time. The paper includes significant contributions and strong results in comparison to a number of prior works, and is quite relevant to the ICLR community. There are a couple of related works that are missing [1,2] that combine learned policies and learned models, but generally the discussion of prior work is thorough. Overall, the paper is clearly above the bar for acceptance.  [1] https://arxiv.org/pdf/1703.04070.pdf [2] https://arxiv.org/pdf/1904.05538.pdf
This paper proposes a design of GNNs that are amenable to operating on heterogeneous graphs. The proposed model introduces numerous operations over the adjacency matrix and combines them as a final aggregation result. Experiments are conducted to show that the proposed method outperforms some baseline methods.  The submission suffers from, an incremental novelty, missing important references, and unconvinced experiments.   All reviewers tend to reject this submission before and after the rebuttal.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.  The paper tackles an interesting and relevant problem for ICLR: optical character recognition in document images.   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.     The authors propose to use small networks to localize text in document images, claiming that for document images smaller networks work better than standard SOTA networks for scene text. As pointed out in the reviews, the authors didn t make any comparisons to SOTA object detection networks (trained either on scene text or on document images) so their central claim has not been experimentally verified.   The reviewers were unanimous that the work lacks novelty as object detection pipelines have already been used for OCR so a contribution of considering smaller detection networks is minor.   There were serious issues with formatting and clarity. These three issues all informed the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.   There were no major points of contention and no author feedback.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
This paper uses an autoencoder with neural style transfer to generate images from previously seen classes to avoid catastrophic forgetting in continual learning.  While reviewers had some concerns about the paper (experiments on high resolution images, comparison with FearNet), authors have addressed all the concerns. R1 s concern about the motivation for generation instead of replaying actual images is not necessary since this is not the first work to use generative replay.
The paper considers the problem of accelerated magnetic resonance imaging where the goal is to reconstruct an image from undersampled measurements. The paper proposes a zero shot self supervised learning approach for accelerated deep learning based magnetic resonance imaging. The approach partitions the measurements from a single scan into two disjoint sets, one set is used for self supervised learning, and one set is used to perform validation, specifically to select a regularization parameter. The set that is used for self supervised learning is then again split into two different sets, and a network is trained to predict the frequencies from one set based on the frequencies in the other set. This enables accelerated MRI without any training data.  The paper evaluates on the FastMRI dataset, a standard dataset for deep learning based MRI research, and the paper compares to a trained baseline and an un trained baseline (DIP). The paper finds their self supervised method to perform very well compared to both and shows images that indicate excellent performance. It would have been even better to compare the method on the test set of the FastMRI competition to have a proper benchmark comparison.  Here is how the discussion went:   Reviewer pt6r is supportive of acceptance, but notes a few potential irregularities, such as the method pre trained on brain and tested on knees performing better than the method pre trained on knees and tested on knees, and not providing a comparison of the computational cost. The authors added a table to the appendix revealing that the computational costs are very high, much higher than for DIP even. The reviewer was content with the response and raised the score.      Reviewer mBMk argues that the contribution is too incremental compared to prior work, in particular relative to the results of [Yaman et al., 2020], and also argues that the idea of partitioning the measurements is not new. The authors argue in response that their approach of partitioning the measurements is new, and the reviewer was inclined to raise the score slightly, but still thinks that the novelty on the technical ML side remains limited, and doesn t want to back the submission too much, and did not raise the score at the end in the system.    Reviewer 19v3 has the concern that the all elements used (transfer learning, plug and play, etc) are well known techniques and have been applied before to MRI, and therefore thinks that the paper does not clear the bar for acceptance. The paper points out that while those ideas might be applied for the first time to MRI, they have been used before in other image reconstruction problems, in particular denoising.   I ve read the paper in detail too, and am somewhat on the fence: I think it s very valuable to see that a clever application of self supervised learning works so well for MRI. I agree with the reviewers that the technical novelty is relatively small, but on the other hand this is the first time that I see self supervised learning being applied that successfully to MRI. I don t share the concern about novelty   yes, the paper s approach builds on prior work, but it s not clear from the literature how well such a well tuned self supervised learning approach would work.  What I would have liked to see in addition to the experimental results is a proper evaluation on the FastMRI dataset: An advantage of the FastMRI dataset is that it provides a benchmark and if researchers evaluate on that benchmark (on the testset/validation set) we can compare different methods well. The paper under review doesn t do that, it only evaluates on 30 test slices, and thus it s hard to benchmark the method. Also, the paper would benefit from more ablation studies.  In conclusion, I would be happy to discuss this paper at the conference, and think that other researchers in the intersection of deep learning and inverse problems would be too, and therefore recommend acceptance.
This paper proposes an amortization strategy for MC sampling from a single chain rather than per datapoint chains, and uses this strategy to define a new Bayesian autoencoder based on Langevin dynamics.  The reviewers find the line of thought very promising, and a potentially interesting addition to the latent variable literature, while also raising some concerns. The dimension of the single chain must match the dataset size, which limits the computational benefits coming from amortization, and in fact this restriction seems hard, as empirical results (added in the discussion period) are qualitatively worse in the `d<n` case. This could be emphasized much more strongly in the current version, and seems worth deeper investigation. In the discussion, the authors agreed that in the case when the feature matrix G is the identity matrix then there can be no amortization improvement, but for other choices of (fixed) features, amortization *can* yield improvements; this is quite unclear. In addition, in response to the reviewers  observation, the authors improved in the discussion period the implementation of the EBM baseline, leading to much less clear cut differences on metrics. To improve the work further, the authors should clarify the source of amortization improvement, and discuss more the relationship to Bayesian Neural Networks (perhaps by evaluating against Bayesian / hyper net / hyper GAN generative models.)
Dear authors,  I apologize to the authors for insufficient discussion in the discussion period. Thanks for carefully responding to reviewers. Nevertheless, I have read the paper as well, and the situation is clear to me (even without further discussion). I will not summarize what the paper is about, but will instead mention some of the key issues.  1) The proposed idea is simple, and in fact, it has been known to me for a number of years. I did not think it was worth publishing. This on its own is not a reason for rejection, but I wanted to mention this anyway to convey the idea that I consider this work very incremental.  2) The idea is not supported by any convergence theory. Hence, it remains a heuristic, which the authors admit. In such a case, the paper should be judged by its practical performance, novelty and efficacy of ideas, and the strength of the empirical results, rather than on the theory. However, these parts of the paper remain lacking compared to the standard one would expect from an ICLR paper.  3) Several elements of the ideas behind this work existed in the literature already (e.g., adaptive quantization, time varying quantization, ...). Reviewers have noticed this. 4) The authors compare to fixed / non adaptive quantization strategies which have already been surpassed in subsequent work. Indeed, QSGD was developed 4 years ago. The quantizers of Horvath et al in the natural compression/natural dithering family have exponentially better variance for any given number of levels. This baseline, which does not use any adaptivity, should be better, I believe, to what the author propose. If not, a comparison is needed.  5) FedAvg is not the theoretical nor practical SOTA method for the problem the authors are solving. Faster and more communication efficient methods exist. For example, method based on error feedback (e.g., the works of Stich, Koloskova and others), MARINA method (Gorbunov et al), SCAFFOLD (Karimireddy et al) and so on. All can be combined with quantization.  6) The reviewer who assigned this paper score 8 was least confident. I did not find any comments in the review of this reviewer that would sufficiently justify the high score. The review was brief and not very informative to me as the AC. All other reviewers were inclined to reject the paper.  7) There are issues in the mathematics   although the mathematics is simple and not the key of the paper. This needs to be thoroughly revised. Some answers were given in author response. 8) Why should expected variance be a good measure? Did you try to break this measure? That is, did you try to construct problems for which this measure would work worse than the worst case variance?   Because of the above, and additional reasons mentioned in the reviewers, I have no other option but to reject the paper.  Area Chair
This paper proposes looking at the duality gap to measure performance. However, the metric is just an upperbound on the true metric of interest, and therefore its value can be ambiguous.   The reviewers found the paper to be in an unacceptable form and was clearly hastily prepared. They were also skeptical about the novelty of the result as well as the comprehensiveness of the experiments.  This paper would require extensive revisions before any potential acceptance. Reject   
This manuscript proposes an adversarial method to address non IID heterogeneity on federated learning. Distinct from existing methods, the mitigation is implemented by training and local communicating learned representations, so the metric of success is indistinguishability of representations across devices.   There are four reviewers, all of whom agree that the method addresses an interesting and timely issue. However, reviewers are mixed on the paper score, and many raised concerns about communication overhead, apparent privacy costs, and convergence concerns with the adversarial methods. There is also some limited concern of novelty compared to existing methods. The authors provide a good rebuttal addressing these issues   either based on experimental evidence (adding differential privacy), comparing communication overhead tradeoffs as it varies with model and sample size, and some existing convergence analysis. However, after reviews and discussion, the reviewers are unconvinced that the method is sufficiently robust to the concerns outlined. Authors are encouraged to address the highlighted technical concerns in any future submission of this work.
Thank you for submitting you paper to ICLR. The paper studies an interesting problem and the solution, which fuses student teacher approaches to continual learning and variational auto encoders, is interesting. The revision of the paper has improved readability. However, although the framework is flexible, it is complex and appears rather ad hoc as currently presented. Exploration of the effect of the many hyper parameters or some more supporting theoretical work / justification would help. The experimental comparisons were varied, but adding more baselines e.g. comparing to a parameter regularisation approach like EWC or synaptic intelligence applied to a standard VAE would have been enlightening.  Summary: There is the basis of a good paper here, but a comprehensive experimental evaluation of design choices or supporting theory would be useful for assessing what is a complex approach.
Strengths     Hallucinations are a problem for seq2seq models, esp trained on small datasets  Weankesses    Hallucinations are known to exists, the analyses / observations are not very novel     The considered space of hallucinations source (i.e. added noise) is fairly limited, it is not clear that these are the most natural sources of hallucination and not clear if the methods defined to combat these types would generalize to other types. E.g., I d rather see hallucinations appearing when running NMT on some natural (albeit noisy) corpus, rather than defining the noise model manually.     The proposed approach is not particularly interesting, and may not be general. Alternative techniques (e.g., modeling coverage) have been proposed in the past.      A wider variety of language pairs, amounts of data, etc needed to validate the methods. This is an empirical paper, I would expect higher quality of evaluation.  Two reviewers argued that the baseline system is somewhat weak and the method is not very exciting.    
Important problem (modular & interpretable approaches for VQA and visual reasoning); well written manuscript, sensible approach. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance. 
This paper proposes using Flush+Reload to infer the deep network architecture of another program, when the two programs are running on the same machine (as in cloud computing or similar).   There is some disagreement about this paper; the approach is thoughtful and well executed, but one reviewer had concerns about its applicability and realism. Upon reading the author s rebuttal I believe these to be largely addressed, or at least as realistically as one can in a single paper. Therefore I recommend acceptance.
This paper demonstrates that per image semantic supervision, as opposed to class only supervision, can benefit zero shot learning performance in certain contexts.  Evaluations are conducted using CUB and FLOWERS fine grained zero shot data sets.  In terms of evaluation, the paper received mixed final scores (two reject, one accept).  During the rebuttal period, both reject reviewers considered the author responses, but in the end did not find the counterarguments sufficiently convincing.  For example, one reviewer maintained that in its present form, the paper appeared too shallow without additional experiments and analyses to justify the suitability of the contrastive loss used for obtaining embeddings applied to zero shot learning.  Another continued to believe post rebuttal that reference Reed et al., (2016) undercut the novelty of the proposed approach.  And consistent with these sentiments, even the reviewer who voted for acceptance alluded to the limited novelty of the proposed approach; however, the author response merely states that a future revision will clarify the novelty.  But this then requires another round of reviewing to determine whether the contribution is sufficiently new, especially given that all reviewers raised this criticism in one way or another.  Furthermore, the rebuttal also mentions the inclusion of some additional experiments, but again, we don t know how these will turn out.  Based on these considerations then, the AC did not see sufficient justification for accepting a paper with aggregate scores that are otherwise well below the norm for successful ICLR submissions.
The paper proposes a GAN model with adaptive convolution kernels. The proposed idea is reasonable, but the novelty is somewhat minor and the experimental results are limited. More comprehensive experiments (e.g., other evaluation metrics) will strengthen the future revision of paper. No rebuttal was submitted. 
This paper proposes a document classification algorithm based on partitioned word vector averaging. I agree with even the most positive reviewer. More experiments would be good. This is a very developed old area.
The paper studies end to end training of a multi branch convolutional network. This appears to lead to strong accuracies on the CIFAR and SVHN datasets, but it remains unclear whether or not this results transfers to ImageNet. The proposed approach is hardly novel, and lacks a systematic comparison with "regular" ensembling methods and with related mixture of experts approaches (for instance: S. Gross et al. Hard Mixtures of Experts for Large Scale Weakly Supervised Vision, 2017; Shazeer et al. Outrageously Large Neural Networks: The Sparsely Gated Mixture of Experts Layer, 2017).
This paper has a deep analysis of the over smoothing phenomenon in BERT from the perspective of graph. Over smoothing refers to token uniformity problem in BERT, different input patches mapping to similar latent representation in ViT and the problem of shallower representation better than deeper (overthinking). The authors build a relationship between Transformer blocks and graphs. Namely, self attention matrix can be regarded as a normalized adjacency matrix of a weighted graph. They prove that if the standard deviation in layer normalization is sufficiently large, the outputs of the transformer stack will converge to a low rank subspace, resulting in over smoothing.  In this paper, they also provide theoretical proof why higher layers can lead to over smoothing. Empirically , they investigate the effects of the magnitude of the two standard deviations between two consecutive layers on possible over smoothing in diverse tasks.  In order to overcome over smoothing, they propose a series of hierarchical fusion strategy that adaptively fuses presentation from different layers, including concatenation fusion, max fusion and self gate fusion into post normalization. These strategies reduce similarities between tokens and outperforms BERT baseline on a few datasets (GLUE, SWAG and SQuAD).  Overall I agree with reviewers that this is a good contribution.
This paper provides actor critic method for fully decentralized MARL. The results remove some of the restrictions from existing results and have also obtained a sample bound that matches with the bound in single agent RL. The authors also give detailed responses to the reviewers  concerns. The overall opinions from the reviewers are positive.
The paper presents a graph neural network model inspired by the consciousness prior of Bengio (2017) and implements it by means of two GNN models: the inattentive and the attentive GNN, respectively IGNN and AGNN. The reviewers think    The idea of learning an input dependent subgraph using GNN seems new.    The proposed way to reduce the complexity by restricting the attention horizon sounds interesting. 
The submission presents an approach to visual planning. The work builds on semi parametric topological memory (SPTM) and introduces ideas that facilitate zero shot generalization to new environments. The reviews are split. While the ideas are generally perceived as interesting, there are significant concerns about presentation and experimental evaluation. In particular, the work is evaluated in extremely simple environments and scenarios that do not match the experimental settings of other comparable works in this area. The paper was discussed and all reviewers expressed their views following the authors  responses and revision. In particular, R1 posted a detailed justification of their recommendation to reject the paper. The AC agrees that the paper is not ready for publication in a first tier venue. The AC recommends that the authors seriously consider R1 s recommendations.
The paper proposes to learn layout representations for graphic design using transformers with a masking approaching inspired by BERT.  The proposed model is pretrained on a large collection of ~1M slides (the script for crawling the slides will be open sourced) and evaluated in several downstream tasks.   Review Summary: The submission received slightly negative with scores of 4 (R3) and 5 (R2,R4).   Reviewers found the paper to be well written and clear, and the problem of layout embedding to be interesting.  Reviewers agree that the use of transformers for layout embedding has not been explored in prior work.  However, the paper did not have proper citation and comparisons against prior work for layout embedding, and lacked systematic evaluation.  Reviewers also would like to know more details about the dataset that was used for pre training.    Pros:   Novel use of transformers for layout embedding (not yet explored in prior work)   Use of large dataset of slides  Cons:   Lacked proper citation and comparisons against prior work for layout embedding   Lacked systematic evaluation   Missing details about the dataset  Reviewer Discussion: During the author response period, the authors responded to the reviews indicating that they will improve the draft based on the feedback, but did not submit a revised draft. As there was no revision to the submission, there was limited discussion with the reviewers keeping with their original scores.  All reviewers agrees that the direction is interesting but the current submission should not be accepted.  Recommendation: The AC agrees with the reviewers that the current version is not ready for acceptance at ICLR, and it would be exciting to see the improved version.  We hope the authors will continue to improve their work based on the reviewer feedback and that they will submit an improved version to an appropriate venue.  
This paper proposes a new approximate algorithm for Bayesian logistic regression in the online setting. The primary approximation involved in the algorithm is the use of a diagonal Gaussian approximation. (A probably more minor approximation is approximating the sigmoid with a Gaussian.) The main discussion focused on two issues: Firstly, there was some sentiment that the paper lacked theoretical guarantees. Second, there were concerns about the experimental results. I feel that it is not a serious flaw that the paper lacks a theoretical regret bound. Given the current state of algorithms for this problem practical algorithms remain very much of interest. However, the general sentiment of reviewers was that the experimental results were not as strong (or as numerous) as would be hoped. For an algorithm without a theoretical regret bound, I do agree that stronger empirical evidence would be expected. This was partially addressed in a revision but still I agree with the consensus that more extensive numerical evidence should be expected, and for that reason I am recommending rejection.  Finally, I ll mention some other issues that I view as not counting substantially against the paper. Firstly is the dependence on the prior. Here I am in agreement with the authors that this is an aspect shared by all Bayesian methods. This issue is a (valid) argument about the value of all Bayesian methods, but not one I think we will resolve here. Second, there were suggestions from the reviewers about improvements that could be made to the baseline methods. Here I don t feel that it s fair that we ask the authors to make novel improvements to other algorithms, unless those improvements are very "obvious".
The paper investigates an interesting question and points at a promising research direction in relation to whether adversarial examples are distinguishable from natural examples.   A concern raised in the reviews is that the technical contribution of the paper is weak. A main concern with the paper is that the experiments have been conducted only on one simple data set. The authors proposed to add more experiments and improve other points, but a revision didn t follow.   The reviewers consistently rate the paper as ok, but not good enough.   I would encourage the authors to conduct the improvements proposed by the reviewers and the authors themselves. 
This paper improves the dynamic convolution operation by replacing the dynamic attention over channel groups with channel fusion in a low dimensional space. It includes extensive experiments with reasonable baselines. Dynamic convolutions are a fruitful method for making convnets more efficient, and this paper further improves their efficiency and efficacy with a novel technique. Reviewers all agreed that the paper was clearly written (though some parts were improved after rebuttal).
The paper proposes a variational inference based on singular learning theory (SLT), where the resolution of singularity is learned by normalizing flow so that the latent distribution is factorized.  Pros:   A unique idea to use SLT for variational inference.  Cons (only serious concerns):   Goal is unclear.  The authors say that they propose variational inference based on SLT.  But apparently, they propose it not as an alternative to the state of the art variational inference for neural networks (if so the experiments shown are far from the acceptable level).  The authors must clearly say for what purpose they propose a new method.  I would guess the proposed method is for analyzing singular models to compute their RLCT.  In that case, the authors should compare with existing methods for evaluating RLCT, e.g., MCMC based methods:  K. Nagata and S. Watanabe, "Exchange Monte Carlo Sampling From Bayesian Posterior for Singular Learning Machines," in IEEE Transactions on Neural Networks, vol. 19, no. 7, pp. 1253 1266, July 2008, doi: 10.1109/TNN.2008.2000202.  and discuss pros and cons of the proposed method.  For DNN, you should use the state of the art MCMC sampling methods like  Wenzel, F., Roth, K., Veeling, B., Swiatkowski, J., Tran, L., Mandt, S., Snoek, J., Salimans, T., Jenatton, R. &amp; Nowozin, S.. (2020). How Good is the Bayes Posterior in Deep Neural Networks Really?. <i>Proceedings of the 37th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 119:10248 10259 Available from https://proceedings.mlr.press/v119/wenzel20a.html.  as a baseline.  Approximating the posterior with normalizing flow can be another baseline.    Large n issue.  SLT can be seen as a generalization of the asymptotic learning theory for the regular model, where the model complexity is represented by the parameter dimension d, and "asymptotic" means n >> d.  Watanabe revealed that the model complexity cannot be represented by d in singular models, and therefore the definition of "asymptotic" is not as clear as the regular case.  But it is known that typical neural networks are overparameterized and can achieve zero training error.  I have seen no work arguing that SLT holds in this regime.  If the authors insist that their method is applicable to deep neural networks, they should cite references where it would be proved that SLT holds in the overparameterized regime or prove it by themselves.  There are many more concerns including those pointed out by reviewers, and the paper is not ready for publication.
This work proposes a new architecture for abstract visual reasoning called "Attention Relation Network" (ARNe), based on Transformer style soft attention and relation networks, which the authors show to improve on the "Wild Relation Network" (WReN). The authors test their network on the PGM dataset, and demonstrate a non trivial improvement over previously reported baselines.   The paper is well written and makes an interesting contribution, but the reviewers expressed some criticisms, including technical novelty, unfinished experiments (and lack of experimental details), and somewhat weak experimental results, which suggest that the proposed ARNe model does not work well when training with weaker supervision without meta targets. Even though the authors addressed some concerns in their revised version (namely, they added new experiments in the extrapolation split of PGM and experiments on the new RAVEN dataset), I feel the paper is not yet ready for publication at ICLR.  
All reviewers agree that this is a well written and interesting paper that will be of interest to the ICLR and broader ML community.
This paper proposes an approach of generating mathematical expressions with a recurrent neural network, which is trained with risk seeking policy gradient to maximize the quality of best examples rather than average examples.  The proposed approach also enables easily incorporating domain knowledge or constraints to avoid illegal or redundant expressions.  In extensive experiments, the proposed method is shown to significantly outperform strong baselines, including commercial software.  All of the reviewers find the work interesting and relevant, and there are no major concerns or issues after discussion.  The topic is also of interest to a wide range of audience in the ICLR community.
The paper proposes an interesting idea for efficient exploration of on policy learning in sparse reward RL problems.  The empirical results are promising, which is the main strength of the paper.  On the other hand, reviewers generally feel that the proposed algorithm is rather ad hoc, sometimes with not so transparent algorithmic choices.  As a result, it is really unclear whether the idea works only on the test problems, or applies to a broader set of problems.  The author responses and new results are helpful and appreciated by all reviewers, but do not change the reviewers  concerns.
The paper received borderline negative scores (6,5,5) with R1 and R2 having significant difficulty with the clarity of the paper. Although R3 was marginally positive, they pointed out that the experiments are "extremely weak". The AC look at the paper and agrees with R3 on this point. Therefore the paper cannot be accepted in its current form. The experiments and clarity need work before resubmission to another venue.  
While the submission has promising components, the reviewers were not able to reach a consensus to recommend acceptance. The main concerns is that (1) theorem statements and assumptions are not clearly explained, and (2) the novelty of the approach is not made clear, and (3) there remain concerns on whether the experimental results are due to hyperparameter search or improvements due to the model.  
The paper proposes a new convolutional network architecture, called CrescendoNet. Whilst achieving competitive performance on CIFAR 10 and SVHN, the accuracy of the proposed model on CIFAR 100 is substantially lower than that of state of the art models with fewer parameters; the paper presents no experimental results on ImageNet. The proposed architecture does not provide clear new insights or successful new design principles. This makes it unlikely the current manuscript will have a lot of impact.
In this paper, the authors proposed a geometric graph generator that applies a WGAN model for efficient geometric interpretation. All the reviewers agree that the idea is interesting and the method has the potentials for graph generation tasks. Unfortunately, the experimental part is unsatisfying, which makes the paper on the borderline. More analytic experiments should be designed to verify the properties of the proposed GG GAN, especially its scalability. Although in the rebuttal phase the authors add a simple example to generate large but simple graphs, we would like to see more experiments and comparisons on more real world large graphs (even if the performance may not be good, the results will be constructive for both readers and authors to understand the work). 
*Summary:* Study inductive bias of natural gradient flow.   *Strengths:*    Some reviewers found the invariance to reparametrization insightful, a good way to better understand the interaction of the learning rule and parametrization.    Experiments support the theory.     *Weaknesses:*    Unclear takeaway message.    Comparison with Euclidean case not comprehensive.    Insufficient distinction between reparametrization (invertible) and different parametrization. No experiments on actual dataset/architecture.   Some reviewers found the the cases considered in the paper are already well understood.   *Discussion:*   2Yhk found that although the author responses and other reviews clarified some of their concerns, particularly about reparametrization conditions, the result provided in the paper is not strong enough and could be further clarified. The authors found that this reviewer might have misunderstood the paper. Following the discussion period, the reviewer raised his/her score and lowered his/her confidence. In response to CBDn the authors added demonstration of NGD being worse than EGD on matrix completion. In one of the responses, the authors summarize their contribution as:  replacing EGD with NGD … disturbs the second mechanism [dynamics and GD trajectories] . I find the question really is what kind of quantitative conclusions can be made. gWo5 pointed out important related work that was not discussed in the initial submission. Authors added discussion. VPSX misses applications to less well understood settings. Authors however only offer to keep this in mind for future work. VPSX also asks to emphasize the insights into the inductive bias of NGD. Authors added some discussion, however mostly pertaining previous works and not specific enough in my opinion.   *Conclusion:*   One reviewer found this work marginally above the acceptance threshold and four other reviewers found that it does not reach the bar for acceptance. I find the topic worthwhile and that it deserves a thorough investigation. However, I concur with the reviewers that some concepts require a clearer presentation and that it would be desirable to see more general results and more comprehensive discussions. Several suggestions were made by the reviewers and acknowledged by the authors, but many of these were left for future work. In summary, I think that the article makes a good start but needs more work. Therefore I am recommending a rejection at this time. I encourage the authors to revise and resubmit.
Quantization is an important practical problem to address.  The proposed method which quantizes a different random subset of weights during each forward is simple and interesting. The empirical results on RoBERTa and EfficientNet B3 are good, in particular, for int4 quantization.  During the rebuttal, the authors further included quantization results on ResNet which were suggested by the reviewers. This additional experiment is important for comparing  this proposed approach with the existing methods which do not have quantization results on the models in this paper. 
All reviewers have converged to an unanimous rating of the paper, highlighting, in the paper or during the discussion, many strengths, including a compelling approach clearly relevant to applications and its solid range of experiments.   A clear accept, and I would encourage the authors to push in the final version the experiments and discussions following the threads with reviewers (in particular, Vo8C and ULvk).  Thanks also to authors and reviewers for a thorough discussion which helped to strengthen further the paper s content.  AC.
This paper proposes a scalable learning method for GNN that gradually increases the training data size by randomly adding vertexes generated from a graphon. Theoretical justification to the proposed method is given that bounds the difference between the gradients on the sampled network and on the graphon. A numerical experiment was conducted to support the validity of the proposed method.   Unfortunately, this paper contains several issues as listed below:   1. Novelty: There are already some existing work  to address the issue of scalability of training a graph neural network model. However, the relation to them is not appropriately exposed.   2. Experiments: Although the main purpose of this paper is to resolve the scalability of GNN, the numerical experiments are conducted only on a small scale dataset ($\sim$1k).   3. Practicality: There are several hyperparameters. However, the theory and methodology do not give a practical guideline to determine them (e.g., how many vertexes should be added at each epoch).   4. Correctness: The proof of the theorems would contain some flaws, which should be resolved by the authors. However, there was no response from the authors.  For these reasons, this paper would not be appropriate to appear in ICLR.
The paper addresses two important aspects of deep learning: model transferability and authorization for use. It presents original solutions for both of these problems. All of the reviewers agree that the paper is a valuable contributions. Minor concerns and critical remarks have been addressed by the authors during the discussion.
This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work (see in particular the updates from AnonReviewer1), and I urge the authors to continue to develop refinements and extensions.
The authors present a thorough exploration of large sparse models that are pruned down to a target size and show that these models can perform better than small dense models. Results are shown on a variety of datasets with as conv models and seq2seq. The authors even go so far as to release the code. I think the authors are to be thanked for their experimental contributions. However, in terms of accepting the paper for a premier machine learning conference the method holds little surprise or non obviousness. I think the paper is a good experimental contribution, and would make a good workshop paper instead but it offers little contribution by way of machine learning methods. 
The authors focus on low resource text classifications tasks augmented with "rationales". They propose a new technique that improves performance over existing approaches and that allows human inspection of the learned weights.  Although the reviewers did not find any major faults with the paper, they were in consensus that the paper should be rejected at this time. Generally, the reviewers  reservations were in terms of novelty and extent of technical contribution.  Given the large number of submissions this year, I am recommending rejection for this paper. 
The paper considers planning through the lenses both of a single and multiple objectives. The paper then discusses the pareto frontiers of this optimization. While this is an interesting direction, the reviewers feel a more careful comparison to related work is needed.
The paper suggests an RL based approach to design a data valuation estimator. The reviewers agree that the proposed method is new and promising, but they also raised concerns about the empirical evaluations, including not comparing with other approaches of data valuation and limited ablation study. The authors provided a rebuttal to address these concerns. It improves the evaluation of one of the reviewers, but it is difficult to recommend acceptance given that we did not have a champion for this paper and the overall score is not high enough.
While the reviewers acknowledge the broad experimental work done in this paper, they all find several issues, which in their combination show that the paper is simply not in a good enough shape. This impression has not changed during the rebuttal phase and as a result, this is a clear reject.
This paper studies how to statistically test if a given model violates the constraint of individual fairness. This is an interesting and novel problem, and the paper leverages the technique of gradient flow to identify a "witness" pair for individual fairness violation.  During the rebuttal, the authors have addressed many concerns raised in the reviews. The author should also consider discussing the runtime and improving the exposition to resolve some of the presentation issues raised in the reviews.
This is an interesting paper that develops new techniques for analyzing the loss surface of deep networks, allowing the existence of spurious local minima to be established under fairly general conditions.  The reviewers responded with uniformly positive opinions.
The paper investigates the interesting problem of the local intrinsic dimension (LID) of graphs, and interpreted the GNN learning from Feature LID (FLID), Structure LID (SLID), and Representation LID (RLID). The concepts are novel but the paper needs better insights on how LID can improve graph learning and stronger empirical evidence to support their claims.
The authors propose a new algorithm based on tensor decompositions for the problem of knowledge base completion. They also introduce new regularisers to augment their method. They also propose an new dataset for temporal KB completion.    All the reviews agreed that the paper addresses an important problem and presents interesting results. The authors diligently responded to reviewer queries and addressed most of the concerns raised by the reviewers.   Since all the reviewers are in agreement, I recommend that this paper be accepted. 
This paper argues that incorporating unsupervised/semi supervised learning into the training process can dramatically increase the performance of models. In particular, its incorporation can result in performance gains that dwarf the gains obtained by collecting data actively alone. The experiments effectively demonstrate this phenomenon.   The paper is written with a tone that implicitly assumes that "active learning for deep learning is effective" and therefore it is a surprise and a challenge to the status quo that using unlabelled data in intelligent ways alone gets such a boost. On the contrary, reviewers found that active learning not working very well for deep learning is a well known state of affairs. This is not surprising because the most effective theoretically justifiable active learning algorithms rely on finite capacity assumptions about the model class, which deep learning disobeys.   Thus, the reviewers found the conclusions to lack novelty as the power of semi supervised and unsupervised learning is well known. Reject. 
This paper describes a method called  stochastic  inverse reinforcement learning. It is somewhat unclear how this differs from other probabilistic approaches to IRL. In particular Bayesian approaches have been used in the past to obtain distributions over reward functions. However, SIRL tries to estimate a generative model over such distributions. All the reviewers foudn the paper suffering from lack of clarity, in particular with respect to how the model/algorithm is constructed. There are some possible technical problems with respect to claims about inferring demonstrations by different experts (cf. work on multi task IRL). The experiments also seem to be insufficient.
This paper proposes an approach for unsupervised meta learning for few shot learning that iteratively combines clustering and episodic learning. The approach is interesting, and the topic is of interest to the ICLR community. Further, it is nice to see experiments on a more real world setting with the Market1501 dataset. However, the paper lacks any meaningful comparison to prior works on unsupervised meta learning. While it is accurate that the architecture used and/or assumptions used in this paper are somewhat different from those in prior works, it s important to find a way to compare to at least one of these prior methods in a meaningful way (e.g. by setting up a controlled comparison by running these prior methods in the experimental set up considered in this work). Without such as comparison, it s impossible to judge the significance of this work in the context of prior papers. The paper isn t ready for publication at ICLR.
High quality paper, appreciated by reviewers, likely to be of substantial interest to the community. It s worth an oral to facilitate a group discussion.
This paper presents a new method for approximate Bayesian inference in neural networks.  The reviewers all found the proposed idea interesting but originally had questions about its novelty (with regard to normalizing flows) and questioned the technical rigor of the approach.  The authors did a good job of addressing the technical concerns, causing two of the reviewers to raise their scores.  However, the paper remains just borderline and none of the reviewers are willing to champion the paper as their questions about novelty and empirical evaluation remain.  The reviewers all questioned fundamental technical aspects of the paper (which were clarified in the discussion), indicating that the paper requires more careful exposition of the technical contributions.  Taking the reviewers feedback and discussion into account, running some more compelling experiments and rewriting the paper to make the technical aspects more clear would make this a much stronger submission.  Pros:   Provides an interesting idea for approximate Bayesian inference in deep networks   The paper appears correct   The approach is scalable and tractable  Cons:   The technical writing is not rigorous   The reviewers don t seem convinced by the empirical analysis   Incremental over existing (but recent) work (Luizos and Welling)
The reviewers agree that test time model adaptation is an interesting problem, providing a new perspective to improve model robustness. The proposed method builds on intuitive assumptions that are easy to understand. However, there are mainly two concerns regarding novelty and effectiveness. The paper can improve in these two aspects to meet ICLR standard.
The paper investigates several properties of adversarial examples obtained by hard label attacks. There are some interesting findings in this paper, such as the connection between query efficiency and distance to the image manifold. However, all the reviewers think the paper is below the acceptance threshold due to several weaknesses, including insufficient experiments, clarity, and whether the observations made in the paper can benefit query efficiency or quality of hard label attacks. 
This manuscript proposes a new algorithm for learning from positive and unlabeled data. The motivation for this work includes cases of selection bias, where the positive label is correlated with observation. The resulting procedure is shown to learn a scoring function that preserves the class posterior ordering, and can thus be thresholded to obtain a classifier.  The problem addressed is interesting, and the approach sounds reasonable. The writing seems to be well done, particularly after the rebuttal when the work was better placed in context.  The reviewers and AC note issues with the evaluation of the proposed method. In particular, the authors do not provide a sufficiently convincing empirical evaluation on real data. 
All reviewers recommend acceptance. The authors have addressed several of the reviewers  concerns in their comments, conducted additional experiments, and updated the manuscript accordingly.  A concern was raised regarding the size of the dataset introduced and used by the authors for this work. However, I agree with the authors that it doesn t necessarily make sense to compare this to datasets designed for training video classification and/or generation models; In the compression setting, the quality of individual data points matters much more than their quantity, as the authors argue.  Reviewer 2 was curious about the potential of a pre trained optical flow module. I believe the authors have convincingly argued that end to end learning is likely to be more effective and practical (and indeed, there is plenty of evidence for this in other ML contexts where training data is not scarce). I agree that a direct comparison in the paper would have been interesting, but this would constitute a significant investment of time and effort on the authors  part (as they also point out, training such a module separately could actually be more difficult), and I think it would be unreasonable to make this a condition for acceptance.
While the paper has merits, the experiments are lacking in important respects: I agree with Reviewer 1 that it is a serious problem that the approach is not evaluated on truly low resource languages   since a significant pivot to target language bias is to be expected (as also suggested by Reviewer 2). I also agree with the sentiment that the work is not properly baselined, without considering alternative ways of using the pivot language development data. I also agree with Reviewer 3 that the 1:1 assumption is limiting, given that multi source transfer has been de facto standard since 2011 (see, e.g., work by McDonald, Søgaard, Cohen, etc.). I’m also a little worried about using dev data for unlabelled data, since this is data from the exact same sample as the test data. In practice, dev data will be biased, and artificially removing this bias will lead to overly optimistic results. 
This work proposes a method for extending hindsight experience replay to the setting where the goal is not fixed, but dynamic or moving. It proceeds by amending failed episodes by searching replay memory for a compatible trajectories from which to construct a trajectory that can be productively learned from.  Reviewers were generally positive on the novelty and importance of the contribution. While noting its limitations, it was still felt that the key ideas could be useful and influential. The tasks considered are modifications of OpenAI robotics environments, adapted to the dynamic goal setting, as well as a 2D planar "snake" game. There were concerns about the strength of the baselines employed but reviewers seemed happy with the state of these post revision. There were also concerns regarding clarity of presentation, particularly from AnonReviewer2, but significant progress was made on this front following discussions and revision.  Despite remaining concerns over clarity I am convinced that this is an interesting problem setting worth studying and that the proposed method makes significant progress. The method has limitations with respect to the sorts of environments where we can reasonably expect it to work (where other aspects of the environment are relatively stable both within and across episodes), but there is lots of work in the literature, particularly where robotics is concerned, that focuses on exactly these kinds of environments. This submission is therefore highly relevant to current practice and by reviewers  accounts, generally well executed in its post revision form. I therefore recommend acceptance.
The paper adopts CVAE to generate OOD samples for training an outliner detector. It consists of two phases that train an OOD detector by leveraging the generated OOD data and shows it outperform other methods. According to reviewers’ discussion, there is a concern from the discussion: why CVAE works but other variants or cGAN doesn’t. The paper needs more motivation or evidence or ablations to support the generality of the work.
The paper raised a natural question: why good synthetic images can be not so good at training/fine tuning models for downstream tasks (e.g., classification and segmentation)? This problem is named synthetic to real (domain) generalization (where syn/real images are regarded as from the source/target domain), and it is of practical importance when using GAN like methods given limited real images for training. The authors found that the answer to the question is the diversity of the learned feature embeddings, and argued/advocated that we should encourage such diversity when training on syn images in order to better approximate training on real images. To this end, a novel contrastive synthetic to real generalization framework was proposed and shown effective in the well designed experiments.  Overall, the quality is above the bar. While some reviewers had some concerns about the applicability and the motivations for the algorithm design, the authors have done a particularly good job in the rebuttal. After the rebuttal, we all think the paper should be accepted for publication.  I have some comments on the writing. The introduction claiming so many things has only 4 citations, especially the first two paragraphs have no citation. While I do think what claimed there are correct, the authors should include certain supportive evidences after each claim by themselves. Moreover, while I do think the problem hunting part is well motivated, the problem solving part needs its own motivation/justification. When two or more components are combined in a proposal, why this component is chosen and is there other choice that can achieve the same purpose (this concern has also been raised by reviewers)? I believe the components are not randomly chosen among possible candidates (e.g., "we further enhance the CSG framework with attentional pooling"), but for writing a paper, the authors should explain the motivation for the algorithm design because we cannot know the motivation unless they tell us.
Being able to give confidence intervals or have a robust measure of uncertainty is very important for offline RL methods. In this work, they proposed a dropout based method to have a measure of uncertainty. The authors provide an significant empirical improvements over other baselines. Nevertheless, as it stands right now and as AnonReviewer5 have pointed out, this paper has some important shortcomings. I have noticed that the authors have updated the paper, but still the some of the important points made by AnonReviewer5 are unaddressed as it stands right now. Thus, I am suggesting to reject this paper hoping that the authors will address those issues and resubmit to a different venue.  Firstly, I agree with AnonReviewer5, it is not clear if the dropout and the variance trick used in this paper actually represents epistemic uncertainty that we would like to have for an offline RL algorithm, because the variance do not necessarily need to shrink as you train it with more data, and as opposed to supervised learning setting it is not clear what type of uncertainty the proposed dropout method will induce in the offline RL setting. It would have been nice to have some results showing how calibrated the uncertainty estimates coming from the dropout is... I would recommend the authors not to include any claims regarding the epistemic uncertainty in the camera ready version of the paper.  Also as AnonReviewer5 pointed out, having distributional baselines and/or ensemble methods like REM or bootstrapped DQN would be a more fair comparison. So, it would be nice to see some of those baselines in a future version of this paper.
The paper considers a special case of decision making processes with                                                                non Markovian reward functions, where conditioned on an unobserved task label                                                       the reward function becomes Markovian.                                                                                              A semi supervised loss for learning trajectory embeddings is proposed.                                                              The approach is tested on a multi task grid world environment and ablation                                                          studies are performed.                                                                                                                                                                                                                                                  The reviewers mainly criticize the experiments in the paper. The environments                                                       studied are quite simple, leaving it uncertain if the approach still works in                                                       more complex settings.                                                                                                              Apart from ablation results, no baselines were presented although the setting is                                                    similar to continual learning / multi task learning (with unobserved task label)                                                    where prior work does exist.                                                                                                        Furthermore, the writing was found to be partially lacking in clarity, although                                                     the authors addressed this in the rebuttal.                                                                                                                                                                                                                             The paper is somewhat below acceptance threshold, judging from reviews and my own                                                   reading, mostly due to lack of convincing experiments. Furthermore, the general setting                                             considered in this paper seems quite specific, and therefore of limited impact.
The paper addresses unsupervised domain adaptation under covariate shift and missing source and target features. Three approaches are proposed for tackling respectively covariate shift, missing data and simultaneous covariate shift and missing data. The proposed method relies on the minimization of the maximum mean discrepancy between the source and target representations in the different settings. Experiments are performed on a synthetic dataset and on two other datasets.  All the reviewers highlighted several weaknesses: lack of formal definitions and of formal analyses, lack of connection with existing approaches for handling missing data, weak reproducibility. The authors did not provide responses. Reject.
I found the main algorithmic contributions to be interesting and of potential value to practitioners, as highlighted by Reviewer 3. Like several reviewers, I found the causal framing to be confusing, or at least not really to be framed in a framework like Pearl s: the word "confounding" is thrown a few times in the manuscript, but there is no formal sense by which it is linked to what we commonly understand as confounding. We are still talking about what happens inside a predictive model (a deterministic function), not what happens in the real world (the authors are not alone as targets of my observation: my point applies to a lot of the papers in the references, where the causal interpretation is hardly illuminating for those coming from a causal inference background, for instance). The reply to Reviewer 2, for instance, cites [1], which is about Granger causality and has little to do with Pearl s framework. Despite its name, Granger "causality" is a probabilistic concept (or, at best, an idea for identifying non causality) with a very minimal causal basis besides the use of time ordering. A much more rigorous explanation of confounding in this paper s context needs to be provided.   That been said: as helpfully highlighted by Reviewer 3 (and summarized without any need to resort to a causal framing), there are several positive contributions added here, which might be of interest to the ICLR audience. The causal framing unfortunately gets in the way without adding clarity.  In its present state, the paper is not yet ready for publication. We hope that the reviewer comments prove helpful for preparing a strong future submission. 
The paper presents a general solution method for constrained RL problems using reward free exploration. While the reviewers found this reduction interesting in general, they had concerns about the price of this reduction in general (such as the increased regret or for suboptimal dependence of the bounds on some problem parameters), which is to be paid in exchange for the simplicity and flexibility of the proposed approach. This, coupled with the limited technical novelty used in the derivations, made all reviewers think that this is a borderline paper, and I also agree with this assessment. The paper could benefit a lot from presenting more evidence of the benefits of their approach (either theoretically or empirically). Based on the above, unfortunately, I am not able to recommend acceptance at this point.
The paper presents generative models to produce multi agent trajectories. The approach of  using a simple heuristic labeling function that labels variables that would otherwise be latent in training data is novel and and results in higher quality than the previously proposed baselines. In response to reviewer suggestions, authors included further results with models that share parameters across agents as well as agent specific parameters and further clarifications were made for other main comments (i.e., baselines that train the hierarchical model by maximizing an ELBO on the marginal likelihood?).
This paper extends the Neural Collapse (NC) phenomenon discovered by Papyan, Han and Donoho (2020) on deep learning image classifications with Cross Entropy (CE) loss, to the scenario with Mean Squared Error (MSE), that achieves similar performance to CE and favors deeper analysis. In particular, the paper shows that the least square loss can be decomposed orthogonally into a  central  path as the optimal least square loss, and its perpendicular loss. Moreover, the paper shows by experiments that after the zero training error (Terminal Phase of Training, or TPT) the perpendicular loss is typically much smaller than the optimal least square loss, and the optimal least square loss is further decomposed into the NC1 loss which is the dominance and NC2/3 loss (even smaller than the perpendicular loss). Such a discovery with loss decomposition is very thought provoking to understand the training dynamics of deep neural networks.  Reviewers unanimously accept the paper, so is the final recommendation.
This paper shows a promising new variational objective for Bayesian neural networks. The new objective is obtained by effectively considering a functional prior on the parameters. The paper is well motivated and the mathematics are supported by theoretical justifications.   There has been some discussion regarding the experimental section. On one hand, it contains several real and synthetic data which show the good performance of the proposed method. On the other hand, the reviewers requested deeper comparisons with state of the art (deep) GP models and more general problem settings. The AC decided that the paper can be accepted with the experiments contained in the new revision, although the authors would be strongly encouraged to address the reviewers’ comments in a “non cosmetic manner (as R2 put it).  
This paper describes a procedure for estimating the number of clusters in assortative sparse networks generated from a stochastic blockmodel, where the average degree scales sublogarithmically with the number of nodes. The approach proposed by the authors is based on the spectra of the Bethe Hessian matrix.  The article is well written. The reviewers raised a number of questions regarding the ad hoc procedure for estimating the parameter $\zeta$ needed to estimate $K$, and the limited experiments. While the authors provided some additional experiments in the revised version, including on a real world dataset, the overall article stills appears to be too borderline for ICLR. Adding experiments on other real world datasets would strengthen the paper.   I recommend rejection.
The authors propose an MPC based approach for learning to control systems with continuous state and actions   the dynamics, control policy and a Lyapunov function are parameterized as neural networks and the authors claim to derive stability certificates based on the Lyapunov function.  The reviewers raised several serious technical issues with the paper as well as the lack of clarity in the presentation of the main technique in the initial version of the paper. While the clarity concerns were partially addressed during the rebuttal, the technical concerns (in particular those raised by reviewer 1) remain unaddressed   the stability certificate derived is questionable due to the fact that sampling based approaches to certifying that a function is a valid Lyapunov function are insufficient to derive any stability guarantee. Further, the experimental results are only demonstrated on relatively simple dynamical systems. Hence I recommend rejection.   However, all reviewers agree that the ideas presented in the paper are potentially interesting   I would suggest that the authors consider revising the paper to address the feedback on technical issues and submit to a future venue.  
The authors present an improved method to convert ANNs to spiking neural networks (SNNs). First, a network with quantized activations is constructed, then it is converted. They analyze the conversion errors theoretically. In addition to previously considered errors [Li et al. 2021] they also consider an error they call "unevenness error" and propose a way to compensate for that. They test the method on data sets such as CIFAR 100 and show good improvements over previous methods with respect to classification accuracy and inference time. The reviewers agree that the manuscript presents interesting and valuable work with a significant novel contribution.The manuscript is well written.  Weak points according to the first reviews were:   Lack of ImageNet conversion experiments.   Analysis of energy consumption was missing.   More related work needs to be compared. The revision addressed all these points, This was acknowledged by the reviewers with increased ratings. All reviewers propose acceptance.
The paper is on the borderline. From my reading, the paper presents a reasonable idea with quite good results on novel image generation and one shot learning. On the other hand, the comparison against the prior work (both generation task and one shot classification task) is not convincing. I also feel that there are many work with similar ideas (I listed some below, but these are not exhaustive/comprehensive list), but they are not cited or compared, I am not sure about if the proposed concept is novel in high level. Although some implementation details of this method may provide advantages over other related work, such comparison is not clear to me.  Disentangling factors of variation in deep representations using adversarial training https://arxiv.org/abs/1611.03383 NIPS 2016  Rethinking Style and Content Disentanglement in Variational Autoencoders https://openreview.net/forum?id B1rQtwJDG ICLR 2018 workshop  Disentangling Factors of Variation by Mixing Them http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Disentangling_Factors_of_CVPR_2018_paper.pdf CVPR 2018  Separating Style and Content for Generalized Style Transfer https://arxiv.org/pdf/1711.06454.pdf  Finally, I feel that the writing needs improvement. Although the method is intuitive and has simple idea, the paper seems to lack full details (e.g., principled derivation of the model as a variant of the VAE formulation) and precise definitions of terms (e.g., second term of LF loss).  
This paper proposes a weakly supervised model for numerical reasoning. After discussion with the reviewers it seems that it is already known that training NMNs directly on DROP is not successful and requires taking additional measures. Past work (NERD) has resorted to using data augmentation, and this work encodes it directly to the model. This paper needs to show the advantages of their approach and that it generalizes better to other scenarios. Other minor issues include (a) clarity fo writing (b) focus on a subset of questions (c) no evaluation on other numerical datasets (d) mild inaccuracies w.r.t prior work (GenBERT)
This paper studies an interesting phenomenon related to adversarial training   that adversarial robustness is quite sensitive to semantically lossless shifts in input data distribution.   Strengths   Characterizes a previously unobserved phenomenon in adversarial training, which is quite relevant to ongoing research in the area.   Interesting and novel theoretical analysis that motivates the relationship between adversarial robustness and the shape of input distribution.  Weaknesses   Reviewers pointed out some shortcomings in experiments, and analysis of causes and remedies to adversarial robustness. The authors agree that given the current state of understanding, these are hard questions to pose good answers for. The result and observations by themselves are interesting and useful for the community.  The weakness that the paper does not propose a solution for the observed phenomenon remains, but all reviewers agree that the observation in itself is interesting. Therefore, I recommend that the paper be accepted. 
This paper propose to obtain high pruning ratio by adding constraints to obtain small weights. Reviewers have a consensus on rejection due to not convincing experiments and lack of novelty.
The paper proposes to launch adversarial attacks in the latent space of VAE such that the minimal change in the latent representation leads to the decoder producing an image with class predictions altered.  Given the pros/cons the paper in its current form falls short of acceptance.  Pros: Reviewers agree that the paper is well written and easy to follow  Cons:   The paper lacks novelty and uses standard attacks and defense methodology.   Reviewers find the attack scenario presented is unrealistic and hence may not useful.   Experiments lack rigorous comparisons with baselines and it is not clear if the attack in the latent space will be stronger than the attack in the input space. 
The authors introduce the Time Aware Multiperistence Spatio Supra Graph CN that uses  multiparameter persistence to capture the latent time dependencies in spatio temporal data.   This is a novel and experimentally well supported work. The novelty is achieved by combining research in topological analysis (multipersistence) and neural networks. Technically sound. Clear presentation and extensive experimental section. Reviewers were uniformly positive, agreeing that the approach was interesting and well motivated, and the experiments convincing. Some concerns that were raised were successfully addressed by the authors and revised in the manuscript.   Happy to recommend acceptance. A veru nice paper!
This paper introduces an algorithm for online Bayesian learning of both streaming and non stationary data.  The algorithmic choices are heuristic but motivated by sensible principles.  The reviewers  main concerns were with novelty, but because the paper was well written and addressing an important problem they all agreed it should be accepted.
This paper proposes  a novel and interesting embedding of graphs emulating the Wasserstein distance. The experiments are good and the authors did a detailed answer taking into account the comments of the reviewer. The responses were appreciated and the AC recommends the paper to be accepted.
The paper derives a new parameter initialization for deep spiking neural networks to overcome the vanishing gradient problem.  During the review, concerns were expressed about how well the method would scale to larger neural networks. It was also questioned how this parameter initialization technique compares with a recently proposed batch normalization technique, especially when training larger neural network on more challenging datasets. There were also concerns raised about the readability of the paper.  I commend the authors for improving the readability of their paper in their revision. I also commend them for taking the time to implement the comparisons requested by the reviewers. These new comparisons revealed that batch normalization and its recently proposed variant were superior to the initialization method on its own, and that the initialization proposed in the paper did not significantly improve performance when paired with batch norm [[1](https://openreview.net/forum?id T8BnDXDTcFZ&noteId yIAPcSbUAQ0)]. The authors also acknowledged based on the new results, that their proposed parameter initialization scheme appears to fail to scale to more complex datasets and networks, especially relative to competing methods, which invalidates a key claim that their approach can "accelerate training and get better accuracy compared with existing methods" [[2](https://openreview.net/forum?id T8BnDXDTcFZ&noteId j12fwayWEb)].  The recommendation is to reject the paper in its current form.
As all the reviewers have highlighted, there is some interesting analysis in this paper on understanding which models can be easier to complete. The experiments are quite thorough, and seem reproducible. However, the biggest limitation and the ones that is making it harder for the reviewers to come to a consensus is the fact that the motivation seems mismatched with the provided approach. There is quite a lot of focus on security, and being robust to an adversary. Model splitting is proposed as a reasonable solution. However, the Model Completion hardness measure proposed is insufficiently justified, both in that its not clear what security guarantees it provides nor is it clear why training time was chosen over other metrics (like number of samples, as mentioned by a reviewer). If this measure had been previously proposed, and the focus of this paper was to provide empirical insight, that might be fine, but that does not appear to be the case. This mismatch is evident also in the writing in the paper. After the introduction, the paper largely reads as understanding how retrainable different architectures are under which problem settings, when replacing an entire layer, with little to no mention of security or privacy.   In summary, this paper has some interesting ideas, but an unclear focus. The proposed strategy should be better justified. Or, maybe even better for the larger ICLR audience, the provided analysis could be motivated for other settings, such as understanding convergence rates or trainability in neural networks.
This paper addresses a central problem in inference in implicit models  classical approaches on such problems ( ABC ) rely on computation of summary statistics, and multiple methods for automatically finding summary statistics have been proposed. This paper provides a fresh take on this classical problem, by providing a methods for finding information maximising summary stats. The work is original, likely impactful, and carried out rigorously and carefully. The reviewers flagged some issues with empirical comparisons, as well as discussion or relevant work  those issues mainly seem to have been resolved in the review process. Moreover, given the originality of the approach, and provided that the description of empirical comparisons and relationship with other work are carefully and conservatively worded, I believe this will be worth publishing even if it is not always the  best  method on all problems. 
While the main idea of the paper is nice, the reviewers are not satisfied with the clarity of the material and the execution.
The reviews are a bit mixed, so the AC independently examined the submission as well. While the authors  response helped clarifying some issues, the draft would still need some major revision, especially the motivation and the experiment part. Here are some concrete suggestions:  (a) The problem definition in Eq (1) is already problematic, as the authors term the least squares reconstruction as an "inversion." When exact reconstruction is not possible, one needs to question the choice of the 2 norm here: why 2 norm and what happens if we change the norm? How does this choice of norm enter the proof and the theorem statements? The uniqueness, as pointed out by one of the reviewers, requires further elaborations: It is not clear that for applications like image denoising or impainting it is necessary to have a unique latent vector, if all one cares about is good reconstruction.  (b) The authors need to explain the motivation of the "inversion" problem better, in the context of generative models. In generative models, G pushes a distribution p on R^{n_0} to q on R^n. If both p and q have nontrivial support and n_0 < n, the map G can be highly irregular and the reconstruction problem can be very ill conditioned. Putting assumptions on G (such as incoherence) might heavily constrain what kind of distributions q we can learn. This trade off was hardly discussed. In fact, if we take n_0   n, then there exist normalizing flows that can learn any distribution q and that can be trivially inverted. If one is interested in inversion, why not use a universal normalizing flow?   (c) The authors mentioned a few possible applications (image denoising, compressed sensing, image inpainting) of the inversion problem, but none of them (to my best knowledge) relies on modelling the underlying distribution. There are also classic algorithms for each of these applications. It would be much more convincing if the authors could explain why a deep generative model is advantageous for these applications and compare the proposed algorithms on standard benchmarks of these applications. The AC agrees with the reviewers that the current experiments are a bit toy ish (which is not wrong by itself but does require a bit more elaboration when the motivation is in question).  (d) The significance of Theorem 1 requires further elaboration. How does one verify its conditions? How often does a trained network satisfy these conditions? How do these conditions restrict the expressiveness of the underlying model? Some of these questions were asked in the reviews but regrettably the authors largely dismissed them. Results like Theorem 2 also require further clarification: what does random weight mean for a generative model? The authors seemed to only care about inversion while completely ignored the expressiveness of a generative model. As a trivial example, one could take a linear network, with regularization we can always invert (in the sense of the authors  definition) the signal. Why is this any different from (if not better than) the authors  results?
This paper proposes Direct Sparse Optimization (DSO) NAS to obtain neural architectures on specific problems at a reasonable computational cost. Regularization by sparsity is a neat idea, but similar idea has been discussed by many pruning papers. "model pruning formulation for neural architecture search based on sparse optimization" is claimed to be the main contribution, but it s debatable if such contribution is strong: worse accuracy, more computation, more #parameters than Mnas (less search time, but also worse search quality). The effect of each proposed technique is appropriately evaluated. However, the reviewers are concerned that the proposed method does not outperform the existing state of the art methods in terms of classification accuracy. There s also some concerns about the search space of the proposed method. It is debatable about claim that "the first NAS algorithm to perform direct search on ImageNet" and "the first method to perform direct search without block structure sharing". Given the acceptance rate of ICLR should be <30%, I would say this paper is good but not outstanding. 
The paper formalizes the adversarial attack problem for transductive defenses, where the model is sequentially updated with a batch of (adversarial) test inputs. The paper comes up with a quite generic attack scheme and their instantiation of this scheme shows that RMC and DENT are not robust respectively not more robust than the underlying adversarially robust base model.    Positive    formal treatment of attacks on transductive defenses including discussion about different types of attacker knowledge   the attack model is quite generic and could work for future transductive defenses and thus is a useful baseline attack which could be suggested to be used by future transductive defenses for robustness evaluation. In particular, as the standard AutoAttack is not designed for transductive defenses and thus can overestimate adversarial robustness  Negative   the description is sometimes overly technical and some (important) details had to be clarified   the technical novelty of the attack is limited   the overall accuracy but also robust accuracy depends on the chosen batch. Therefore the authors should report mean and standard deviation over several different random draws of batches    the Transductive Adversarial Training Defense seems to consist of adversarial retraining from scratch after each incoming batch. This is excessively costly and not practical.  Minor:   The batch size is an important parameter which apparently is assumed to be known in this work  The paper is borderline. Two reviewers argue for rejection, two for acceptance. Only one reviewer engaged in the discussion.  In my point of view the positive point of having a reference for correct evaluation of adversarial robustness of transductive defenses weighs more than the raised negative points which can be fixed (at least partially). Thus I think that this paper is a valuable contribution to the field of adversarial robustness.
This paper proposes a deterministic policy gradient method that does not require to inject noise in the action selection. Although the reviewers acknowledge that this paper has merits (novel and interesting idea, well written, technically sound), they have some doubts about the motivations for the proposed approach and about its empirical performance: a deeper analysis is requested. The paper is borderline and needs to be revised before being ready for publication.
This paper proposes to extend learning to learn framework based on zeroth order optimization. Generally, the paper is well presented and easy to follow. The core idea is to incorporate another RNN to adaptively to learn the Gaussian sampling rule.  Although the method does not seem to have a strong theorical support, its effectiveness is evaluated in the well organized experiments including realistic tasks like black box adversarial attack.  All reviewers including two experts in this field admit the novelty of the methods and are positive to the acceptance. I’d like to support their opinions and recommend accepting the paper. As R#1 still finds some details unclear, please try to clarify these points in the final version of the paper.
This paper investigates how the properties of an environment affect the success of reinforcement learning, and in particular finds that random dynamics and non episodic learning makes learning easier, even though these factors make learning more difficult when applied individually. The paper was reviewed by three experts who gave Reject, Weak Reject, and Weak Reject recommendations. The main concerns are about missing connections to related work, overstating some contributions, and experimental details. While the author response addressed many of these issues, reviewers felt another round of peer review is really needed before this paper can be accepted; R2 s post rebuttal comments give some specific, constructive, concrete suggestions for preparing a revision.
This paper proposed an unsupervised domain adaptation method for 3D lidar based object detection. Four reviewers provided detailed reviews: 3 rated “Marginally above acceptance threshold”, and 1 rated “Ok but not good enough   rejection”. The reviewers appreciated simple yet effective idea, the well motivated method, the comprehensiveness of the experiments, and well written paper. However, major concerns are also raised regarding the core technical contributions on the proposed approach. The ACs look at the paper, the review, the rebuttal, and the discussion. Given the concerns on the core technical contributions, the high competitiveness of the ICLR field, and the lack of enthusiastic endorsements from reviewers, the ACs believe this work is not ready to be accepted to ICLR yet and hence a rejection decision is recommended. 
This paper proposes a framework for generating auxiliary tasks as a means to regularize learning. The idea is interesting, and the method is simple. Two of the three reviewers found the paper to be well written. The experiment include a promising result on the CIFAR dataset. The reviewer s brought up several concerns regarding the description of the method, the generality of the method (e.g. the requirement for class hierarchy), the validity and description of the comparisons, and the lack of experiments on domains with much more complex hierarchies. None of these concerns were not addressed in revisions to the paper. Hence, the paper in it s current state does not meet the bar for publication.
This paper proposes to leverage topological structure between domains, expressed as a graph, towards solving the domain adaptation problem.  Reviewer n4Lk thought the ideas were interesting, appreciated the theoretical analysis and indicated that the experiments were “well thought out”. The reviewer asked for more detail on Lemma 4.1 and suggested that a proof be provided for Proposition 4.1. They asked for more justification on why the change of task for the discriminator from classification to generation would improve performance. The authors responded to these comments, clarifying the proof of Lemma 4.1 in the appendix. They clarified that proposition 4.1 can be derived from Corollary 4.3 or Corollary 4.4. On the point of classical vs. enhanced discriminator the authors provided additional experiments.   Reviewer rNQp commented that the method was easy to follow and noted the theoretical and empirical analysis. They expressed some concern that previous work on graph based domain adaptation was inadequately addressed. Like reviewer n4Lk they seemed unconvinced that the proposed graph discriminator was an improvement over past SOTA and questioned its novelty. In terms of claims about novelty and competitiveness relative to previous works I would have liked to see the reviewer make specific references rather than criticize in general terms. The authors’ responded to the reviewer, adding a recent entropy based method (SENTRY) to the experiments and showed that their method outperformed this ICCV 2021 work by a large margin. They responded to the reviewer’s remarks about the original discriminator and variants, pointing out that this was already established in the paper. They used the other reviews to dispute the claim of lack of novelty.   Reviewer uDYW felt that the work was novel and interesting. Like rNQp they thought the paper was clear. They questioned the practical advantage over baselines. The authors responded to the reviewer’s question about using a data graph. They responded to the question about parameter tuning and computational cost. They addressed the question about limited improvements in real world datasets.  I had some difficulty motivating the reviewers to engage in the discussion and acknowledge the authors’ response. The authors also politely attempted to nudge the reviewers to consider their updated results. In my opinion, the author responses have addressed most of the reviewer concerns  and I don’t see any critical issues remaining. Therefore I think that this paper should be accepted as a poster.
This paper considers the idea of meta learning the loss function for domain generalization. It s a simple idea, that seems to work reasonably well. Although, as pointed out by the reviewers, the margin is actually quite modest when compared to the strongest baselines (not ERM).  On a positive note, many reviewers agree that the idea was simple, novel, and interesting. The insight that cross entropy can be improved for domain generalization is interesting. On the other hand, many reviewers pointed out that the, despite some careful empirical work, it s not clear why this idea works. I read the paper myself, and I agree that the paper could use a bit more work before it is ready for publication. Specifically, I agree with Reviewer eZ71, who asked for a clear justification of the proposed idea. The idea seems sensible, but there is some burden on the paper to provide insight, and not simply present an idea.  Here are some specific suggestions that came up during discussion, which could strengthen the paper:   A more comprehensive discussion of the limitations of this approach.   It would be good to understand how critical was the specific choice of parametric loss family. Here are some questions that would be good to address: does the parametric family interact with the type of domain shift in the datasets? Why are Taylor polynomials preferable or beneficial for domain generalization compared to, e.g., a linear combination of standard loss functions?   Is the dataset on which you learn your ITL loss critical? I.e., how critical was the choice of rotated MNIST for learning the ITL loss? Does it generalize to very different and more diverse domain shift tasks, like those in the WILDS benchmark? It would be particularly interesting to see if loss functions meta trained on distinct datasets learn similar parameters.   More broadly, evaluation on larger and more diverse domain shift tasks, like those in the WILDS benchmark, would further strengthen the conclusions in the paper.
The reviewers agree that the proposed architecture is novel. However, there are issues in terms of the motivation. It would be helpful in future drafts to strengthen the argument about why the architecture is expected to be better than others. Most importantly, the gains at this stage are still incremental. A larger improvement from the new architecture would motivate more researchers to focus on this architecture.
All reviewers recommend that the paper be rejected.  The reviewers appreciate the line of research and is worthwhile, but find that the paper lacks in technical novelty and insight.  The AC is in consensus with their reviews due to the concerns raised regarding novelty and insight and recommends rejection.
This paper presents a self supervised learning method for the multi modal setting where each modality has its own feature extraction mapping, and i) the extracted features shall be close for paired data,  ii) in the feature space each view has close to diagonal covariance, while iii) the scale for each feature dimension is constrained away from zero to avoid trivial features. The presentation is clear and the reviewers do not have major confusion on the methodology. There have been some discussions between the authors and reviewers, and most questions on the empirical study have been addressed by the authors with additional experiments. The remaining concern is on the novelty (difference from prior SSL methods especially Barlow Twins) and significance.  I think that while it is relatively straightforward to extend methods like Barlow twins to the multi modal setting, I do see the value of empirically demonstrating the effectiveness of an alternative loss to the currently pervasive contrastive learning paradigm, and hence the paper is worth discussion in my opinion. In the end, the method resembles classical multi modal methods like canonical correlation analysis, in terms of the objective (matching paired data in latent space) and constraints (un correlated feature in each view, and unit scale constraint for each feature dimension); such connections shall be discussed.
All reviewers recommend rejection: concerns were raised in terms of technical correctness, quality of presentation and the quality of experiments. There was no rebuttal. The AC agrees with the reviewers and recommends rejection.
Very solid work, recognized by all reviewers as worthy of acceptance. Additional readers also commented and there is interest in the open source implementation that the authors promise to provide.
Reviewers were somewhat lukewarm about this paper, which seeks to present an analysis of the limitations of sequence models when it comes to understanding compositionality. Somewhat synthetic experiments show that such models generalise poorly on patterns not attested during training, even if the information required to interpret such patterns is present in the training data when combined with knowledge of the compositional structure of the language. This conclusion seems as unsurprising to me as it does to some of the reviewers, so I would be inclined to agree with the moderate enthusiasm two out of three reviewers have for the paper, and suggest that it be redirected to the workshop track.  Other criticisms found in the review have to do with the lack of any discussion on the topic of how to address these limitations, or what message to take home from these empirical observations. It would be good for the authors to consider how to evaluate their claims against "real" data, to avoid the accusation that the conclusion is trivial from the task set up.  Therefore, while well written, it is not clear that the paper is ready for the main conference. It could potentially generate interesting discussion, so I am happy for it to be invited to the workshop track, or failing that, to suggest that further work on this topic be done before the paper is accepted somewhere.
This paper proposes an alternative approach to epsilon greedy exploration by instead generating multi step plans from an RNN, and then stochastically determining whether to continue with the plan or re plan. The reviewers agreed that this idea is novel and interesting, that the paper is well written, and that the evaluations are convincing, showing large improvements over epsilon greedy exploration and more consistently strong performance than other baselines. While the original reviews contained some questions around discussion of related work and the simplicity of the evaluation environments, the reviewers felt these concerns were adequately addressed by the rebuttal. I agree the paper explores a very interesting idea and convincingly demonstrates its potential, and should be of wide interest to the deep RL community especially as it touches on many different subfields of RL: MBRL/planning, exploration, HRL, navigation, etc. I recommend acceptance as a spotlight presentation.
The main concern raised by reviewers is limited novelty, poor presentation, and limited experiments. All the reviewers appreciate the difficulty and importance of the problem. The rebuttal helped clarify novelty, but the other concerns remain.
This paper develops a Python library for geospatial data based on Pytorch, TorchGeo. TorchGeo is a useful tool for applying deep learning methods to geospatial data. The reviewers agrees the contribution of this library. It will help machine learning researchers to use geospatial data and help geospatial researchers to apply machine learnig methods. However, the technical contribution is low, and the novelty is not high enough since the results can be achieved by a combination of existing packages.
The three reviewers all felt the paper was above threshold for acceptance to ICLR. To improve the final version, they suggest some additional discussion and experiments may help the paper.
This paper proposes an novel way of expanding our VAE toolkit by tying it to adversarial robustness. It should be thus of interest to the respective communities.
This paper applied probability matching to A* sampling in order to provide an approximate variant without a bound function. It is a novel idea and a good contribution to the A* sampling family. The authors also provided regret analysis for the adoption of PM.  However, as pointed out by R1 and R3, the authors failed to clarify the approximation introduced by the PM and its implication in the output samples. The empirical comparison should also take into account this difference. Further analysis of the bias in the sample distribution would also help clarify the pros and cons of the proposed method.  R3 also raised the concern that the description of the preliminary section and the main contribution in section 4 was not clear.
This paper is solid. It is correct, the text and author response demonstrate good knowledge of the area, the results are significant and solid, the experiments are strengthened by many independent runs (refreshing to see), the ablation study is well done, and the proposed distributed hyper parameter and NAS alg is simple and practical. The paper is well written and reasonably polished.  The main drawback of the work in the eyes of the reviewers is that the paper is well described as a combination of existing ideas and a significant engineering effort with good but not stellar results. The reviewers found they did not gain any substantial technical insights from the work. As a result no reviewer was willing to champion the paper. However, the discussion, reviews, and author response made it clear that (1) the paper is enjoyable to read and informative, (2) the method is actually useful and performant, and (3) the combination of implementation details and methods is worth documenting. In balance, the paper is just below the bar. The program was extremely competitive this year.   
The reviewers all agree that the idea is interesting, the writing clear and the experiments sufficient.   To improve the paper, the authors should consider better discussing their meta objective and some of the algorithmic choices. 
The paper proposes a plug and play method for solving imaging problems. Plug and play methods use a denoiser to solve linear inverse problems. The paper proposes a plug and play method and uses convex optimization tools from analyzing proximal gradient methods to provide convergence guarantees. The algorithm is applied to a variety of inverse problems showing that the method works well.   After the discussion period, all four reviewers recommend acceptance.    Reviewer QQES provided a detailed review and raised a few concerns including a clear motivation for and description of the denoiser, and unsupported claims, in particular related to a proof in the paper. The authors revised the paper and responded in length to the claims, in particular they detailed steps and assumptions related to the theorem in their response. As a response, the reviewer changed their score to accept.   Reviewer xYLt strongly supports acceptance based on the strong theoretical results and a very good exposition.   Reviewer GZzY likes the overall idea of the paper and raised a few minor concerns and questions, which were addressed by the authors.    Finally, reviewer E8QG also appreciates the method, convergence analysis, and extensive validation. The reviewer also raised a few minor concerns and asked for clarification, and the response of the authors resolved those concerns.   Based on my own reading and based on the reviews, I recommend acceptance. The paper provides a variant of a plug and play method, proves interesting convergence results for the method, and has a strong experimental evaluation of the method. I encourage the authors to take the feedback of the reviewers into account, which they have done for the most part already, and it would also be interesting to see the performance of the method for compressive sensing problems.
The paper proposes a new style transfer task, contextual style transfer, which hypothesises that the document context of the sentence is important, as opposed to previous work which only looked at sentence context. A major contribution of the paper is the creation of two new crowd sourced datasets, Enron Context and Reddit Context focussed on formality and offensiveness. The reviewers are skeptical that it was context that has really improved results on the style transfer tasks. The authors responded to all the reviewers but there was no further discussion. I feel like this paper has not convinced me or the reviewers of the strength of its contribution and, although interesting, I recommend for it to be rejected. 
This paper consider an important problem CounterFactualRegret (CFR) minimization, and proposes a new algorithm to solve this problem.  Reviewers raised many questions and concerns that the authors chose not to answer.  We can only recommend rejection
This paper modifies the conditional diffusion model guided by a classifier, as introduced by Dhariwal & Nichol 2021, by replacing the explicit classifier with an implicit classifier. This implicit classifier is derived under Bayes  rule and combined with the conditional diffusion model. This combination can be realized by mixing the score estimates of a conditional diffusion model and an unconditional diffusion model. A trade off between sample quality and diversity, in terms of the IS and FID scores, can be achieved by adjusting the mixing weight. The paper is clearly written and easy to follow. However, the reviewers do not consider the modification to be that significant in practice, as it still requires label guidance and also increases the computational complexity. From the AC s perspective, the practical significance could be enhanced if the authors can generalize their technique beyond assisting conditional diffusion models.
This paper received a majority voting of rejection. In the internal discussion, one reviewer updated his/her score from 1 to 3 according to the author response. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.  **Interesting Idea**  Every reviewer including me agree that the idea of modelling Bayes label transition is novel and interesting.   **The motivation lacks of supportive evidence**  The second motivation that "the feasible solution space of the Bayes label transition matrix is much smaller than that of the clean label transition matrix" is not well supported. The authors should theoretically or empirically demonstrate this point. The current description on uncertainty is not strong enough. Moreover, if so, the benefits are not illustrated. The feasible solution space, even with a small coverage area is continuous with infinite solutions.   **A new concept**  The authors tried to sell the concept of a new transition matrix, but failed. I believe it might result from the organization and presentation. The authors spent too much pages introducing others  work. At least, a formal definition of the new concept should be given. In the current version, Definition 1 is from Cheng et al., 2020 on distilled examples.  **Title**  Literally from title, I guess DNN is a key component or a selling point of this paper. Actually no. We expect the authors could provide the insights on what benefits are using DNN over other techniques and how to apply DNN to estimate the transition matrix. If this is not a selling point, this word might be removed from the title.  **Algorithm 1**  I am a little surprised that the only algorithm listed in this paper is label noise generation. Instead the proposed algorithm of this paper is expected.  **Experimental Evaluation**  The experimental results look much better than other baselines. It is a little confusing that some best results are bold, some not.   **Presentation**  Although I did not notice obvious grammar errors, some sentences are very long (3 lines). They made difficulties to follow the idea. I have to read these sentences several times. In my eyes, this is the biggest one! Presentation means how to sell the idea to audience (not only reviewers, but also future readers) in an easy understood way. The current version spent much space introducing others  work; on the contrary, the original or key part is not well illustrated.   Although this paper has a novel idea and good experimental support, other issues listed above demonstrate the current version is not ready for a top tier conference. No objection from reviewers was raised to again this recommendation.
This paper is very different from most ICLR submissions, and appears to be addressing interesting themes.  However the paper seems poorly written, and generally unclear.  The motivation, task, method and evaluation are all unclear.  I recommend that the authors add explicit definitions, equations, algorithm boxes, and more examples to make their paper clearer.
The paper addressed the problem of machine bias when training machine learning models. The authors propose an approach based on representation learning with adversarial training. As opposed to the majority of previous works that trying to create a representation from which it is not possible to predict the sensitive feature (bias), the authors propose to minimize the dependency between the learned features and the sensitive feature with adversarial training. While acknowledging that the proposed model is addressing an important problem and is potentially useful, the reviewers and AC note the following potential weaknesses:   (1) limited technical contribution   the proposed approach is similar to a number of works published in machine learning and computer vision before the submission deadline that were overlooked by the authors. Specifically: i) adversarial training for learning fair representations [Edwards and Storkey, Censoring Representations with an Adversary, ICLR 2016], [Beutel, et al 2017, Data decisions and theoretical implications when adversarially learning fair representations], ii) learning fair representation by minimizing the dependency between the latent representation and the sensitive attributes [The variational fair autoencoder, ICLR 2016 by Louizos et al.; Fairness Constraints: Mechanisms for Fair Classification, by Zafar et al, 2015] or by minimizing the mutual information between feature embedding and bias [Learning Not to Learn: Training Deep Neural Networks with Biased Data, CVPR 2019].  (2) Limited empirical evidence   the baseline methods used in the evaluation are not sufficient to assess the benefits of the proposed approach over the existing SOTA methods mentioned above. In fact, none of the baseline methods used in the evaluation tackle machine bias (via adversarial training or minimizing statistical dependence).  (3) It would be beneficial to also report fairness metrics, e.g. equality of opportunity, statistical parity, to assess the effectiveness of bias removal. R1 has raised some concerns regarding empirical evidence   see the point about mixed results. Also R2 has reported concerns regarding controversial results in experiment 4.2 and suggested ways to justify when and why the results of the CNNs baseline are close to the BR Net. Addressing these concerns would strengthen the contributions of the proposed method.      Among these, (3) did not have a decisive impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
The premise of the work is simple enough: investigate if networks that are trained with an adversarial objective end up being more suitable for transfer learning tasks, especially in the context of limited labeled data for the new domain. The work uncovers the fact that shape biased representations are learned this way and this helps for the tasks they considered.  There was rather robust back and forth between the authors and the reviewers. The consensus is that this work has merit, has good quality experiments and investigates something with high potential impact (given the importance of transfer learning in general). I hope that most of the back and forth findings are incorporated in the final version of this work (especially the discussion and comparison with Shafahi et al., as well as all the nuances of the shape bias).
The authors introduce a neural network approach for solving the fixed point equations arising in deep equilibrium models. This consists of a tiny network that provides an initial guess for the fixed point, as well as a small network that computes coefficients inside an algorithm inspired by Anderson iteration.  Overall, there is consensus among the reviewers that the paper is well written and is a strong empirical study.  I recommend acceptance as a poster.  Additional remarks:    The authors argue the DEQs / implicit deep learning models allow a decoupling between representational capacity and inference time efficiency. Yet, in the "Regularizing Implicit Models" paragraph, they write "Implicit models are known to be slow during training and inference. To address this, recent works have developed certain regularization methods that encourage these models to be more stable and thus easier to solve.", which seems like a contradiction to me. So while in theory I agree with this decoupling, in practice, it seems not completely true.    Section 3 should include some discussion on conditions on f_theta for the existence of a fixed point.    Since the initialization and HyperAnderson networks are trained using unrolling, there is some memory overhead compared to vanilla DEQs, that are differentiated purely using implicit differentiation. It would be great to clarify the amount of extra memory needed by these networks. It is necessary to justify that the initialization and HyperAnderson networks are smaller than usual neural networks.
The major contribution of this paper is the use of random Fourier features as temporal (positional) encoding for dynamic graphs. The reviewers all find the proposed method interesting, and believes that this is a paper with reasonable contributions. One comment pointed out that the connection between Time2Vec and harmonic analysis has been discussed in the previous work, and we suggest the authors to include this discussion/comparison in the paper.
The authors propose a generative model based on variational autoencoders that provides means to manipulate the high level attributes of a given input. The attributes can be either pre defined ground truth attributes or unknown attributes automatically discovered from the data.  While the reviewers acknowledged the potential usefulness of the proposed approach, they raised important concerns that were viewed by AC as a critical issue: (1) very limited experimental evaluation (e.g. no baseline or ablation results, no quantitative results); comparisons on other more complex datasets and more in depth analysis would substantially strengthen the evaluation and would allow to assess the scope of the contribution of this work  – see, for example, R3’s suggestion to use other dataset like dSprites or CelebA, where the ground truth attributes are known; (2) lack of presentation clarity – see R2’s latest comment how to improve.  A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs clarification, more empirical studies and polish to achieve the desired goal. 
This manuscript proposes an implicit generative modeling approach for the non linear CCA problem. One contribution is the proposal of Conditional Mutual Information (CMI) as a criterion to capture nonlinear dependency, resulting in an objective that can be solved using implicit distributions. The work seems to be well motivated and of interest to the community.  The reviewers and AC opinions were mixed, and the rebuttal did not completely address the concerns. In particular, a reviewer pointed out an issue with a derivation in the paper, and the issue was not satisfactorily resolved by the authors. Some additional reading suggests that the misunderstanding may be partially due to incomplete notation and other issues with clarity of writing.
The submitted paper proposes a novel model/approach for deep clustering which shows good empirical performance on a set of standard benchmark datasets as compared to state of the art baseline algorithms. While I believe that this paper can be turned into a good ICLR paper, it doesn’t meet the standard of ICLR in its current form.  More specifically: 1) The quality of the writeup is poor, containing many typos but more problematically many unclear/confusing statements which are either vague/unclear, not supported by citations and/or substantiated in other parts of the paper (Examples: „This might result in inferior clustering performance, degenerated generative model, and stability issues during training.“ Or “However, this objective seems to miss the clustering target, since the reconstruction term of is not related to the clustering and actual clustering is only associated with the regularization term optimization.“). An important contribution of the paper could be to substantiate these statements and I argue that achieving better performance alone is not sufficient therefore.  2) From a theoretical perspective, it would be interesting whether there is any justification for statements like the ones references above. Furthermore, the authors’ approach involves several approximations whose implications are neither studied nor explained. The authors responded only partially to questions in that regard by reviewers leaving certain concerns unanswered.   3) From an empirical perspective, an extended study of the proposed approach would help t better understand its benefits over existing approaches. Commonly considered settings like mismatch in the number of specified clusters are not studied at all. The proposed approach also seems to be initialized by VaDe (mentioned in Section 4) and it would be interesting to understand to which extend this is necessary and why (and how does performance change/degrade if this is not done). It also makes statements regarding stability unclear as training VaDe itself can be quite challenging. Furthermore, the overall algorithm for training the proposed model should be presented in a compact form in the paper. The paper should also be self contained in the sense of containing information on the important hyper parameters needed for training the proposed model.  In summary, the proposed approach is potentially interesting but the paper should not be accepted in its current form. 
The paper claims to present actionable visual representations for manipulating 3D articulated objects. Specifically, the approach learns to estimate the spatial affordance map as well as the trajectories and their scores. After checking the rebuttal from the authors, all reviewers agree that the paper adds value to the research area. In the end, it got three borderline accept ratings. The initial criticism included lacking (experimental) comparison to baselines, and the authors successfully corresponded to the request from the reviewer. One reviewer commented that the proposed approach is a combination of Where2Act and curiosity guidance for RL Policy for Interactive Trajectory Exploration, which we believe is a valid point. Still, the paper extends the previous Where2Act and successfully demonstrates its success on difficult tasks.  We recommend accepting the paper.
Thank you for submitting you paper to ICLR. The reviewers and authors have engaged well and the revision has improved the paper. The reviewers are all in agreement that the paper substantially expands the prior work in this area,  e.g. by Balle et al. (2016, 2017), and is therefore suitable for publication. Although I understand that the authors have not optimised their compression method for runtime yet, a comment about this prospect in the main text would be a sensible addition.
This well written and well motivated paper has been independently reviewed by four expert reviewers. They all voted for the acceptance with three straight accepts and one marginal. The feedback provided to authors was constructive and the authors responded comprehensively. I recommend acceptance of this work for ICLR.
The authors explore the use of flow based models for video prediction. The idea is interesting. The paper is well written. It is a good paper worthwhile presenting in ICLR.  For final version, we suggest that the authors can significantly improve the experiments: (1) report results on human motion datasets; (2) include the results by the FVD metric.  
This paper proposes to compress the deep learning model using both activation pruning and weight pruning. The reviewers have a consensus on rejection due to lack of novelty. 
This paper proposes the c score, which is the aggregation of a "consistency profile" that measures per instance generalization.  Naive computation of the c score is expensive and thus requires an approximation.  The paper then uses the c score to analyze several image benchmarks and their learning dynamics.    While the reviewers found the experiments to be well done, their primary concern was over the novelty and ultimate usefulness of the c score.  As R1 and R4 point out, the c score correlates with other known measures such as accuracy and training speed.  The authors claim this is a contribution.  In turn, it is hard to tell if the c score is a true metric of interest or a recapitulation of what is already known.  No reviewer was in favor of acceptance.
This paper presents a model for spatiotemporal point processes using neural ODEs. Some technical innovations are introduced to allow the conditional intensity to change discontinuously in response to new events. Likewise, the spatial intensity is expanded upon that proposed in prior work on neural SDEs. Reviewers were generally positive about the contributions and the empirical assessments, and the authors made substantial improvements during the discussion phase.
Dear authors,  The reviewers appreciated your work and recognized the importance of theoretical work to understand the behaviour of deep nets. That said, the improvement over existing work (especially Montufar, 2017) is minor. This, combined with the limited attraction of such work, means that the paper will not be accepted.  I acknowledge the major modifications done but it is up to the reviewers to decide whether or not they agree to re review a significantly updated version.
The paper is a contribution to the recently emerging literature on learning                                                         based approaches to combinatorial optimization.                                                                                     The authors propose to pre train a policy network to imitate SOTA solvers for                                                       TSPs.                                                                                                                               At test time, this policy is then improved, in an alpha go like manner, with                                                        MCTS, using beam search rollouts to estimate bootstrap values.                                                                                                                                                                                                          The main concerns raised by the reviewers is lack of novelty (the proposed                                                          algorithm is a straight forward application of graph NNs to MCTS) as well a the                                                     experimental results.                                                                                                               Although comparing well to other learning based methods, the algorithm is far                                                       away from the performance of SOTA solvers.                                                                                                                                                                                                                              Although well written, the paper is below acceptance threshold.                                                                     The methodological novelty is low.                                                                                                  The reported results are an order of magnitude away from SOTA solvers, while previous work                                          has already reported the general feasibility of learned solvers to TPSs.                                                            Furthermore, the overall contribution is somewhat unclear as the policy relies                                                      on pre training with solutions form existing solvers. 
There was quite a bit of internal discussion on this paper. To summarize:   The idea is very neat and interesting and likely to work   The paper is likely to inspire future work   There are still serious doubts  about the experimental evaluation that is not entirely up to par with current standards     The reviewers were not convinced 100% by the arguments about the  custom  environments     The reviewers were not convinced 100% that the baselines were given their best shot  While the paper has potential to provide valuable input for the community, it needs a bit more work before being presentable at a highly competitive venue like ICLR. 
The paper extends the previously established connection between adversarial training (AT) and Wasserstein distributional robustness (WDR) to other adversarial defense methods such as PGD AT, TRADES and MART, and connects them to WDR. While this connection itself is not surprising given earlier works connecting AT and WDR, the paper makes contributions in establishing it formally and proposing algorithmic variations (eg, softball projection) that show clear empirical gains on standard benchmarks of MNIST/CIFAR10/CIFAR100 over point wise adversarial defense methods.
The primary contribution of this manuscript is a conceptual and theoretical solution to the sample elicitation problem, where agents are asked to report samples. The procedure is implemented using score functions to evaluate the quality of the samples.  The reviewers and AC agree that the problem studied is timely and interesting, as there is limited work on credible sample elicitation in the literature. However, the reviewers were unconvinced about the motivation of the work, and the clarity of the conceptual results. There is also a lack of empirical evaluation. IN the opinion of the AC, this manuscript, while interesting, can be improved by significant revision for clarity and context, and revisions should ideally include some empirical evaluation.
This paper defines a parameter tying scheme for a general feed forward network with the equivalence properties of relational data. Most reviewers raised a few concerns around the experiments, baselines, datasets used and motivation. A few pointed out that the paper is hard to read   for a person without heavy database theory literature, which includes most of ICLR readers. While this paper may read well for the folks in the domain, authors should consider revising the paper to be more inclusive so that it can be read more widely. The motivation of the problem was also another point that many reviewers have mentioned (perhaps related to the language issues above) that some noted that you may not always want equivariance in relational DB and other noted that the paper did not sufficiently demonstrate the advantage of the proposed methods. Reviewers also univocally commented on experiments   many voiced the lack of baselines (not even any simple one). Authors wrote back to defend that there is no similar method and even simple tensor factorization isn’t applicable. That makes me wonder   is there really no single simple method you can compare with? If nobody had solution for this problem, is this a problem worth solving? Reviewers also encouraged to use larger (beyond Kaggle dataset) real world datasets to strengthen the paper. All the points raised by reviewers suggests that this paper can benefit from another round of nontrivial editing before it’s ready for the show. 
This paper presents an approach to synthesize programmatic policies, utilizing a continuous relaxation of program semantics and a parameterization of the full program derivation tree, to make it possible to learn both the program parameters and program structures jointly using policy gradient without the need to imitate an oracle.  The parameterization of the full program derivation tree that can represent all programs up to a certain depth is interesting and novel.  In its current form this won’t scale to large programs that require large tree depth, but is a promising first step in this direction.  The learned programmatic policies are more structured and interpretable, and also demonstrated competitive performance against other commonly used RL algorithms.  During the reviewing process the authors have actively engaged in the interaction with the reviewers and addressed all the concerns, and all reviewers unanimously recommend acceptance.
This paper explored pre training for deep offline reinforcement learning, developing a method that first pre trained decision transformers on trajectories without rewards, and then fine tuned on limited data with rewards. The reviewers were pleased with the overall research questions and directions, but found that they were substantial shortcomings in the experimental setup and results that make this paper not yet suitable for inclusion. The approach is relatively simple and straightforward, which is actually a good thing, but that means that it must be correspondingly investigated and developed with convincing empirical results. Unfortunately, there are a number of open questions about the experimental set up, and the results are not convincing that the method is effective against alternatives, as detailed in the reviews. There was no author rebuttal.
This paper combines probabilistic models, VAEs, and self organizing maps to learn interpretable representations on time series. The proposed contributions are a novel and interesting combination of existing ideas, in particular, the extension to time series data by modeling the cluster dynamics. The empirical results show improved unsupervised clustering performance, on both synthetic and real datasets, compared to a number of baselines. The resulting 2D embedding also provides an interpretable visualization.  The reviewers and the AC identified a number of potential weaknesses in the presentation in the original submission: (1) there was insufficient background on SOMs, leaving the readers unable to comprehend the contributions, (2) some of the details about the experiments were missing, such as how the baselines were constructed, (3) additional experiments were needed in regards to the hyper parameters, such as number of clusters and the weighting in the loss, and (4) Figure 4d required a description of the results.  The revision and the comments by the authors addressed most of these comments, and the reviewers felt that their concerns had been alleviated.  Thus, the reviewers felt the paper should be accepted.
PAPER: This paper introduces an extension of the HuBERT audio only model for the audio visual setting, allowing for self supervised pre training of multimodal model which also performs well on the unimodal tasks (lip reading and ASR). The paper applies the idea of modality dropout to their multimodal pre training setup and introduce the idea of masking with substitution as a way to improve visual representation learning. A strong aspect of the paper is its experimental section, showing strong improvement for lip reading tasks, bringing a new state of the art performance. The experiments also show improvement for ASR task.  DISCUSSION: All reviewers seemed to appreciate the experimental results, with new state of the art performance on both unimodal task, when performing multimodal pre training. The paper does bring some technical novelty, but primarily because of its application to the audio visual domain. The modality dropout idea was already explored for other audio visual tasks such as speech driven face animation (Abdelaziz et al., ICMI 2020), but the idea of “masking by substitution” seems novel and helps learning better visual representations. The authors were able to address many questions and concerns expressed by reviewers. All reviewers took the time to read these responses and acknowledge them. SUMMARY: This paper brings an interesting extension of the audio only HuBERT model for the audio visual setting. The strength of the paper is in its evaluation, with strong performances, establishing many new state of the art results.  All reviewers supported the acceptance of this paper.
The authors propose three strategies for coreset selection in the context of continual learning. In particular, the authors consider class imbalance and noisy scenarios. The authors run extensive benchmarks and ablation showing that the approach can be effective in practice. All reviewers were positive about this work, but found that the methodological contributions were relatively modest. The clarifications provided by the authors were highly appreciated. I would encourage the authors to revise the paper to incorporate these additional details as there were a number of concepts that reviewers found were not sufficiently documented/explained and lacked clarity. I would also highly encourage the authors to explain their use of "online continual learning" as this reads like a tautology.  Finally, I would like to ask the authors to reflect on their insistance with the reviewers; while we would all want engaging and long discussions about our work, the reality is that reviewing papers and discussing them is time consuming and taxing, especially in the middle of continued pandemic. The authors should be grateful of the time reviewers have spent reading their work and providing feedback, and it is not in the authors  interest to ask for a revision of the scores.
The paper proposes a modification to Adam which is intended to ensure that the direction of weight update lies in the span of the historical gradients and to ensure that the effective learning rate does not decrease as the magnitudes of the weights increase.  The reviewers wanted a clearer justification of the changes made to Adam and a more extensive evaluation, and held to this opinion after reading the authors  rebuttal and revisions.  Pros: + The basic idea of treating the direction and magnitude separately in the optimization is interesting.  Cons:   Insufficient evaluation of the new method.   More justification and analysis needed for the modifications.  For example, are there circumstances under which they will fail?   The modification to Adam and batch normalized softmax idea are orthogonal to one another, making for a less coherent story.   Proposed method does not have better generalization performance than SGD.   Concern that constraining weight vectors to the unit sphere can harm generalization. 
The authors address the important task of improving dialogue summarization using conversation structure and factual knowledge.  Pros: 1) Clearly written and well motivated (as acknowledge by all reviewers) 2) Technically sound (the proposed architecture is clearly in line with the problem that the authors are trying to solve) 3) Significant upgrades to the paper after the reviewer comments (in particular the authors have added detailed ablation studies and results on non dialogue datasets)  Cons: 1) There is a significant difference between the results in the ablation studies in the original version and in the new version. Originally, the differences between KGEDCg and KGEDCg GE and KGEDCg FKG were very minor, but now the margins are as large as 7+ pts. I would request the authors to explain this in the final version  The reviewing team felt that while many Qs were sufficiently addressed by the authors, the large difference in the numbers reported for the ablation study in the initial and final version of the paper raises some new Qs which need to be addressed before the paper can be accepted. 
This paper proposes a method called iterative proportional clipping (IPC) for generating adversarial audio examples that are imperceptible to humans. The efficiency of the method is demonstrated by generating adversarial examples to attack the Wav2letter+ model. Overall, the reviewers found the work interesting, but somewhat incremental and analysis of the method and generated samples incomplete, and I’m thus recommending rejection.
All four reviewers raised concerns on the limited technical novelty and insufficient experiments. They unanimously recommended a rejection. I carefully read the authors  rebuttal but did not find strong reasons to go against the reviewers  recommendations. The reviewers made excellent points to further improve the paper. The authors are encouraged to incorporate those for a future submission.
The proposed method proposes a new architecture that uses mixture of experts to determine what to share between multiple languages for transfer learning. The results are quite good.  There is still a bit of a lack of framing compared to the large amount of previous work in the field, even after initial revisions to cover reviewer comments. I think that probably this requires a significant rewrite of the intro and maybe even title of the paper to scope the contributions, and also make sure in the empirical analysis that the novel contributions are evaluated independently on their own (within the same experimental setting and hyperparameters).  As such, and given the high quality bar of ICLR, I can t recommend this paper be accepted at this time, but I encourage the authors to revise this explanation and re submit a new version elsewhere.
Since there were only two official reviews submitted, I reviewed the paper to form a third viewpoint.  I agree with reviewer 2 on the following points, which support rejection of the paper: 1) Only CIFAR is evaluated without Penn Treebank; 2) The "faster convergence" is not empirically justified by better final accuracy with same amount of search cost; and 3) The advantage of the proposed ACSA over SBMD is not clearly demonstrated in the paper.  The scores of the two official reviews are insufficient for acceptance, and an additional review did not overturn this view.
The paper tried to introduce a new interpretation of dropout and come with improved algorithms. However, the reviewers were not convinced that the presented arguments were correct/novel, and they found the paper difficult to follow. The authors are encouraged to carefully revise their paper to address these concerns. 
The paper proposes a GAN based approach for disentangling identity (or class information) from style. The supervision needed is the identity label for each image. Overall, the reviewers agree that the paper makes a novel contribution along the line of work on disentangling  style  from  content . 
The paper proposes a new architecture named Iterative Memory Network (IMN) to encode long user behavior sequence for recommendations. Reviewers appreciate the clarity of the writing as well as practicality and the O(L) complexity of the proposed architecture, however do raise questions on novelty. Different design choices employed in the paper are not well explained. The rebuttal was not able to convince the reviewers to accept the work at this venue, but reviewers do feel the paper could fly in an application oriented venue.
Motivated by empirical observations that SGD performed on deep networks converge to regions of flatter loss curvature relative to large or full batch GD, the authors perform a theoretical analysis of trajectories of SGD with the presence of heavy tailed noise. The primary observation of the theory is that heavy tailed noise has a higher probability of "kicking" the current parameters to a new region of the input space, which has some probability of lying in a sharper region. However, it s important to note that in this analysis SGD with heavy tailed noise doesn t stay in the sharp regions, but will eventually be kicked back out of it back to other regions. In a sense, this defines a transition graph which predicts that the steady state distribution should spend some fraction of time in different regions of the input space (and different sharpness) while never "converging" anywhere. This is shown most clearly in Figure 1 top center where the heavy tailed SGD randomly jumps between different regions of the input space throughout the entire training trajectory. Experiments are then run on deep networks showing that heavy tailed SGD with gradient clipping converges to regions of flatter curvature.   Reviews of the work were generally positive, the theory is well presented and Figure 1 does a solid job demonstrating the main idea. The primary criticism was raised by reviewer HGyL, arguing that the results should be largely irrelevant to deep learning. Most of the debate between this reviewer and the authors centered around whether or not ReLU networks have minima which extend off to infinity. The AC will not dig into the details of the argument. It seems clear, however, that if there were a deep learning workload with heavy tailed noise that the authors results will have some relevancy, though the exact nature of the resulting transition graph may have a complicated dependence on the loss surface. Unfortunately the authors were unable to find a such a workload in image classification (there is some prior work suggesting the NLP models with rare tokens may be a better fit) and so needed to artificially induce heavy tailed noise to test their theory. This is a bit of a limitation, but given the clear writing and interesting experiments as noted by reviewers the work seems worth accepting. The AC strongly urges the authors though to include a more lengthy discussed of Wu. et. al. as that work seems to agree with experiment of the sharpness of stable regions selected by SGD when run on deep models without heavy tailed noise.
The paper provides a comprehensive study and generalisations of previous results on linear permutation invariant and equivariant operators / layers for the case of hypergraph data on multiple node sets. Reviewers indicate that the paper makes a particularly interesting and important contribution, with applications to graphs and hyper graphs, as demonstrated in experiments.   A concern was raised that the paper could be overstating its scope. A point is that the model might not actually give a complete characterization, since the analysis considers permutation action only. The authors have rephrased the claim. Following comments of the reviewer, the authors have also revised the paper to include a discussion of how the model is capable of approximating message passing networks.   Two referees give the paper a strong support. One referee considers the paper ok, but not good enough. The authors have made convincing efforts to improve issues and address the concerns.       
This paper provides a new differentially private training method. The key idea is sparse gradient updates that is, their variant of differentially private SGD (DP SGD) only updates on a random subset of the parameters in each iteration. The authors argued that their method has a benefit in terms of memory and communication efficiency. The reviews suggested that the paper may require further evidence to motivate and justify the novelty of the proposed method. First, the reviewers are not fully convinced that the proposed method reduced both memory and communication. In particular, would the technique of random freeze require running DP SGD for more iterations? Even though the authors added a new theoretical result (mostly adapted from Chen et al.), the newly added Theorem 2 does not explain the benefits of the freezing technique. Thus, the paper can benefit from more extensive theoretical analyses or justification. The authors should also consider including the additional related work brought up by the reviewers. In summary, the paper is not ready for publication at ICLR.
This paper proposes a generalization metric depending on the Lipschitz of the Hessian.  Pros: Paper has some nice experiments correlating their Hessian based generalization metric with the generalization gap,   Cons: The paper does not compare its results with existing generalization bounds, as there is substantial work in the area now.  It is not clear whether existing generalization bounds do not capture this phenomenon with different batch sizes/learning rates, and the necessity of having and explicit dependence on the Lipschitz of the Hessian.  The bound by itself is also weak because of its dependence on number of parameters  m .   The paper is poorly written and all reviewers complain about its readability.  I suggest authors to address concerns of the reviewers before submitting again. 
The paper presents an interesting theoretical analysis by deriving polynomial sample complexity bounds for the training of GANs that depend on the approximator properties of the discriminator. Even if it is not clear if the theory will help to pick suitable discriminators in practice, it provides new and interesting theoretical insights on the properties of GAN training. 
This paper proposes an automatic tuning procedure for the learning rate of SGD. Reviewers were in agreement over several of the shortcomings of the paper, in particular its heuristic nature. They also took the time to provide several ways of improving the work which I suggest the authors follow should they decide to resubmit it to a later conference.
In this work reviewers use structured attention as a way to induce grammatical structure in NMT models. Reviewers liked th motivation of the work and found experiments mostly well done. However reviewers found the paper a bit difficult to follow, with several commenting that distinctions made between the different sub types of attention were not clear. Mainly the reviewers were not overwhelmed by the results of the work, saying that these gains, while clearly isolated to the use of structure were not significantly large. Additionally there were some concerns about the claimed novelty of the work, particularly compared to Liu and Lapata and other use of syntax in translation, and also which aspects were new or necessary. 
In this paper, the authors aim to develop a new method for credit assignment, where certain types of future information is conditioned on.  The authors are well aware that naive conditioning on future information introduces bias due to Berkson s paradox (explaining away), and introduce a number of corrections (described in section 2.4 and 2.5).  The authors illustrate their approach via a number of simulation studies and constructed problems.  I think it would be nice if the authors found a way of connecting their notion of counterfactual to one used in causal inference (for instance, I think there is a connection via e.g. importance correction terms).  Reviewers were worried about the contribution being incremental given existing work (from 2019), and relative simplicity of the evaluation of the approach, compared to existing similar work. 
One referee recommends acceptance, while three referees recommend rejection. All referees agree that augmenting GAT with structural information is an interesting direction to explore; however, they raised concerns about the empirical validation of the method, the related work covered, as well as the discussion of insights such as the method s limitations. The rebuttal addresses R2 s concerns by better positioning the work w.r.t the literature and by discussing the method s potential limitations. The rebuttal also covers R1 and R3 s comments on scalability and complexity of the proposed approach. However, the rebuttal only partially addresses the evaluation concerns of the reviewers. After discussion, all reviewers agree that further work should be devoted to remedy this. I agree with their assessment and hence must reject. In particular, I would strongly recommend following the referees  suggestions and consider incorporating experiments on additional OGB datasets, including a GAT based comparison to those (and on OGB arxiv), and eventually toning down their claims.
The strength of the paper is that it designs an LP based algorithm for training neural networks with runtime exponential in the number of parameters and linear in the size of the datasets and the algorithm works for worst case datasets. As reviewer 2 and reviewer 3 pointed out, the cons include a) it s not clear why the algorithm provides any theoretical insights on how to design in the future polynomial time algorithm   it seems that the algorithms are inherently exponential time and b) it s not clear whether the algorithm is practically at all relevant. The AC also noted that brute force search algorithm is also exponential in # parameters and linear in size of datasets, and the authors agreed with it. This leaves the main contribution of the paper be that it works for the worst datasets. However, theoretically speaking, it s not clear this should be counted as a feature for algorithm design because we cannot go beyond the intractability without making assumptions on the data and in the AC s opinion, the big open question is how to make additional assumptions on the data (instead of removing them.) In summary, the drawback b) makes this a purely theoretical paper and the theoretical significance of the paper is unclear due to a). Therefore based on a), the AC decided to recommend reject, although the AC suggested the authors to re submit to other top theory or ML theory conferences which may better evaluate the theoretical significance of the paper. 
The paper proves new rates of convergence for stochastic subgradient under an interpolation condition. The analysis is rather simple but it produces better rates than previously known, which all reviewers agree is interesting. As pointed out by the reviewers, this work has the potential to help the community better understand optimization with over parametrized neural networks (where convexity or other related assumptions play a role).  To the authors, please add a citation to Pegasos as requested by the reviewers.  
This paper has been evaluated  by three expert reviewers, two of whom recommended rejection and one acceptance. Two of the three reviews are particularly detailed and thorough. Both point out a few points of conceptual issues that leave the reader confused. These key issues have not been addressed sufficiently in the rebuttal to result in changing the reviewers  assessments. One major concern is lacking novelty of the work as presented, which limits its current utility to the ICLR audience. I recommend a rejection.
 This paper proposes an efficient attention mechanism linear in time and space using random features. The approach has some similarities with the simultaneous ICLR 2021 submission "Rethinking Attention with Performers", with a key difference of a gating mechanism present in this work, motivated by recency bias. This paper is a valuable contributions to the efficient attention research topic. The reviewers appreciate the experiments and the in depth analysis. I recommend acceptance.  A noteworthy concern brought up in the discussion period has to do with whether the attention mechanism dominates the feed forward computations in the neural network, and how much this is architecture specific. The authors provide TPU timings, but I encourage the authors to add a discussion and timings of relative performance of feed forward vs. attention layers that covers GPU and CPU optimizers as well.
Pros:   interesting novel formulation of policy learning in homogeneous swarms   multi stage learning process that trades off diversity and consistency (fig 1)  Cons:   implausible mechanisms like averaging weights of multiple networks   minor novelty   missing ablations of which aspect is crucial    dubious baseline results   no rebuttal  One reviewer out of three would have accepted the paper, the other two have major concerns. Unfortunately the authors did not revise the paper or engage with the reviewers to clear up these points, so as it stand the paper should be rejected.
The authors study bias amplification [Zhao et al, 2017] and propose an improved metric for measuring it. The authors also discussed normative issues in bias amplification (predicting a sensitive feature), and how to measure amplification when we do not have labels, or where labels correspond to uncertain future events. While the reviewers acknowledged the importance to study bias amplification, normative measures and social context, they raised several important concerns:   (1) limited technical contributions (R3 and R4)   see reviewers’ concerns that the metric is the only technical contribution; one possible suggestion is to propose a mitigation strategy based on the proposed metric similarly to [Zhao et al, 2017];   (2) ‘the usage of error bars because of the Rashomon effect seems incomplete and almost trivial’ (R3, R4), ‘lacks a proper grounding’ (R1)   see two suggestions by R3 how to revise; these suggestions have not been discussed in the rebuttal;   (3) empirical evidence lacks a controlled scenario of tuning the bias source to evaluate consistency (suggested by R3) and is limited in the context of algorithmic fairness benchmarks (R4)   this has been partly addressed in the rebuttal;   (4) normative contributions and broader discussions are oversimplified – see R3’s comments and suggestions on how to better position the paper.  Among these, (3) and (4) did not have a major impact on the decision but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed work and were viewed by AC as critical issues.  A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs technical strength, more empirical studies and polish to achieve the desired goal. We hope the detailed reviews are useful for revising the paper. 
With an average review score of 4.67 and a short review for the one positive review it is just not possible to accept the paper.
The paper proposes an embedding for nodes in a directed graph, which takes into account the asymmetry. The proposed method learns an embedding of a node as an exponential distribution (e.g. Gaussian), on a statistical manifold. The authors also provide an approximation for large graphs, and show that the method performs well in empirical comparisons.  The authors were very responsive in the discussion phase, providing new experiments in response to the reviews. This is a nice example where a good paper is improved by several extra suggestions by reviewers. I encourage the authors to provide all the software for reproducing their work in the final version.  Overall, this is a great paper which proposes a new graph embedding approach that is scalable and provides nice empirical results.
The paper demonstrates that one phase of de novo assembly, specifically the layout phase, can be replaced with graph neural network based methods. The paper clarifies in the rebuttal that it focuses on building a method for assembling high quality long reads.   All four reviewers rated the paper as below the acceptance threshold. The reviewers largely agree that the idea of using GNNs to assemble a genome from reads is novel, interesting, and has the potential to be very useful.  The reviewers raise the following concerns: The paper only considers synthetic data, and the synthetic reads used in the simulations are error free. In practice, reads are not error free, and thus simulations on real data or at the very least on reads with errors are needed. The authors acknowledge that, and state that they ll provide such experiments in future work. In summary, the reviewers found the experiments to be insufficient to support the claims, even though it is understood by the reviewers and me that the paper only presents a proof of concept idea. I agree with the reviewers that simulations on erroneous reads, ideally real data, would be needed for acceptance.  I recommend to reject the paper, since the paper provides insufficient experiments to understand the merits of the proposed approach.
The authors provide bounds on the expected generalization error for noisy gradient methods (such as SGLD). They do so using the information theoretic framework initiated by Russo and Zou, where the expected generalization error is controlled by the mutual information between the weights and the training data. The work builds on the approach pioneered by Pensia, Jog, and Loh, who proposed to bound the mutual information for noisy gradient methods in a step wise fashion.  The main innovation of this work is that they do not implicitly condition on the minibatch sequence when bounding the mutual information. Instead, this uncertainty manifests as a mixture of gaussians. Essentially they avoid the looseness implied by an application of Jensen s inequality that they have shown was unnecessary.  I think this is an interesting contribution and worth publishing. It contributes to a rapidly progressing literature on generalization bounds for SGLD that are becoming increasingly tight.  I have one strong request that I will make of the authors, and I ll be quite disappointed if it is not executed faithfully.  1. The stepsize constraint and its violation in the experimental work is currently buried in the appendix. This fact must be brought into the main paper and made transparent to readers, otherwise it will pervert empirical comparisons and mask progress.  2. In fact, I would like the authors to re run their experiments in a way that guarantees that the bounds are applicable. One approach is outline by the authors: the Lipschitz constant can be replaced by a max_i bound on the running squared gradient norms, and then gradient clipping can be used to guarantee that the step size constraint is met.  The authors might compare step sizes, allowing them to use less severe gradient clipping. The point of this exercise is to verify that the learning dynamics don t change when the bound conditions are met. If they change, it may upset the empirical phenomena they are trying to study. If this change does upset the empirical findings, then the authors should present both, and clearly explain that the bound is not strictly speaking known to be valid in one of the cases. It will be a good open problem.     
The submission proposes to leverage a commonsense knowledge graph and an attention GNN based model to aggregate the node features on the graph for the problem of zero shot learning. It received three reviews two of them recommending rejection, and another review was initially borderline however they moved to acceptance after the rebuttal. The meta reviewer finds that the paper is not yet ready for publication and recommends rejection based on the following observations.  Although the model is interesting, as agreed by the reviewers the initial version of the paper fell short on convincingly evaluating the method, e.g. generalized zero shot learning (GZSL) setting as pointed out by R5, ImageNet and small scale dataset results as pointed out by R1. Similarly, the main paper (without the annexes) has been found to fall short on providing enough details of the model as pointed out by R1.   The authors ran additional experiments during the rebuttal phase which showed some promise, however one more review round may be necessary to carefully validate these results. As R1 pointed out, the paper only reports results on two small datasets i.e., AWA2 and aPY, which contain classes similar to ImageNet. It would be interesting to observe the behaviour of the model on more challenging scenarios on other publicly available benchmark datasets of fine grained nature whose distribution are far from ImageNet. This would indicate the generalisation ability of the model.  Furthermore, as pointed out by R1, moving the details on the implementation and the architecture details from appendix and from python scripts to the main paper may be beneficial. However, this would end up significantly extending the paper. Hence, one more review round may be necessary for this paper.
The paper proposed a GNN model based on a weighted line graph (dual of the input graph), where information is simultaneously propagated on both graphs, coupling the two propagations at each step.   Overall, the reviewers were lukewarm about the paper, with some raised criticism including    limited novelty in light of Monti et al. 2018   limited theoretical justification   unconvincing and incomplete experiments, not offering significant improvement compared to other alternatives  While the presented approach is interesting, we believe the paper is below the bar and recommend Rejection.  
The paper studies and compares different notions of robustness. However, reviewers found there are many unjustified claims in the analysis, and the paper does not provide novel findings nor useful approaches.
After the rebuttal stage, all reviewers lean positive (in final scores and/or in comments during the discussion phase). The AC found no reason to disagree. The benefit of the proposed method is demonstrated in many diverse settings, and the authors argue novelty in that no prior work addresses both fg/bg imbalance and relation distillation. 
**Problem significance** This paper proposes an attack mechanism in the latent space of a neural network f(x), which produces out of distribution examples. The AC agrees reviewers on the significance of the OOD detection problem, particularly addressing the vulnerability aspect is relevant and of great interest to the community.   **Technical contribution** The AC shares the concern with several reviewers on the limited technical novelty as well as the problem formulation. While the authors have clarified the difference between adversarial attack vs. OOD attack, the underlying attack mechanism is not new to the community (except for allowing for a larger degree of search space without constrained by the visual imperceptibility). In some sense, the search is made easier than the standard adversarial attack by removing the similarity constraint. Given the unrealisticness of the created OOD examples (largely noisy patches), the AC thinks perhaps a more interesting problem is to look at naturally occurring OOD examples that would lead to the similar latent encoding w.r.t in distribution data, or adversarial robustness w.r.t the OOD detector.  This to me, would steer the community in the right direction.   From a problem formulation perspective, the AC thinks it s useful to differentiate three highly related attacks (that are distinct but can cause confusions):    adversarial attack w.r.t the classifier   OOD attack w.r.t the classifier    adversarial attack w.r.t the OOD detector (see recent works [1][2][3] which considered the robustness aspect of OOD detector)   **Rebuttal feedback** The AC recognizes the effort made by the authors to address the concerns and comments raised by reviewers. The AC agrees with R1/R2/R3 that the additional experiments are valuable, however, the changes to the manuscript are substantial enough to deem another round of review in the future venue. The paper can improve with better organization and presentation, moving the results in the appendix to the main paper.   **Recommendation** The AC recommends rejection.   References  [1] Sehwag et al. Analyzing the robustness of open world machine learning. 2019  [2] Hein et al. Why relu networks yield high confidence predictions far away from the training data and how to mitigate the problem. 2019  [3] Chen et al. Informative Outlier Matters: Robustifying Out of distribution Detection Using Outlier Mining. arXiv:2006.15207    
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
This paper deals with domain generalization with causal modeling. Specifically, it considers a broader class of distribution shifts, arising from the system intervention perspective, and proposes some robust learning principle to achieve domain generalization. The paper is well written and has some interesting ideas. However, as pointed by Reviewers #1 and #4, the exact problem setting should be made more explicit, the theory and algorithm should be more consistent, and some very relevant contributions in the literature should be discussed or compared with. 
This submission addresses the problem of few shot classification. The proposed solution centers around metric based models with a core argument that prior work may lead to learned embeddings which are overfit to the few labeled examples available during learning. Thus, when measuring cross domain performance, the specialization of the original classifier to the initial domain will be apparent through degraded test time (new domain) performance. The authors therefore, study the problem of domain generalization in the few shot learning scenario. The main algorithmic contribution is the introduction of a feature wise transformation layer.   All reviewers suggest to accept this paper. Reviewer 3 says this problem statement is especially novel. Reviewer 1 and 2 had concerns over lack of comparisons with recent state of the art methods. The authors responded with some additional results during the rebuttal phase, which should be included in the final draft.   Overall the AC recommends acceptance, based on the positive comments and the fact that this paper addresses a sufficiently new problem statement. 
There were opinions on both sides of this paper from the reviewers.  Reviewers were excited by the novel application of energy based models (EBMs) to continual learning and the resulting performance gains, but were concerned by the more direct application of EBMs (which has been explored in other work, and here adapted to the continual learning setting, so its contribution is marginal) and with the depth of the evaluation, which they thought could be pushed farther. Overall, the reviewers agreed that this paper could benefit from another round of revisions to strengthen its contribution, incorporating many of the excellent points made by the authors in their responses.
The paper proposes a manifold distance based detection based against adversarial samples, i.e., using the difference between the highest and second highest softmax outputs from a model to detect adversarial examples. All the reviewers gave negative scores. The main concerns lie in 1) poor quality of writing; 2) contributions of the paper are not clearly stated; and 3) limited engagement with well known recommendations from the research community on the evaluation of defenses/detection methods for adversarial examples. No rebuttals are provided. Thus, I cannot recommend accepting the paper to ICLR.
The paper was evaluated by 3 knowledgeable reviewers. All reviewers raised concerns about the motivation of the contribution of the paper. It is unclear why the use of an additional discriminator should reduce the variance of the log density ratio estimate. Also, the derivations were found to be not convincing or intuitive. These concerns have also not been alleviated after a rather extensive discussion of the reviewers with the authors. Moreover, the transfer setting to a new environment was unclear as it does not show how the  reward function transfers to new dynamics, so the transfer experiments rather evaluate how well the algorithm can imitate a policy on a different dynamics, but it does not tell that the extracted reward function is valid. While the experimental results seem promising, the authors are encouraged to improve the motivation of contribution, check which of the "incremental" contributions are very necessary and improve their evaluation on the transfer scenario.
Thank you for submitting you paper to ICLR. The idea is simple, but easy to implement and effective. The paper examines the performance fairly thoroughly across a number of different scenarios showing that the method consistently reduces variance. How this translates into final performance is complex of course, but faster convergence is demonstrated and the revised experiments in table 2 show that it can lead to improvements in accuracy.  
The reviewers were not convinced by the authors  responses to their concerns, and this paper generated little followup discussion. Some primary concerns include the privacy analysis, limited technical contribution and scope (e.g., only being applicable to iid data), and lacking comparison to suggested baselines. The authors are suggested to take the reviewer comments into account for further investigation.
This paper introduces a set of new analysis methods to try to better understand the reasons that multilingual BERT succeeds. The findings substantially bolster the hypothesis behind the original multilingual BERT work: that this kind of model discovers and uses substantial structural and semantic correspondences between languages in a fully unsupervised setting. This is a remarkable result with serious implications for representation learning work more broadly.  All three reviewers saw ways in which the paper could be expanded or improved, and one reviewer argued that the novelty and scope of the paper are below the standard for ICLR. However, I am inclined to side with the two more confident reviewers and argue for acceptance. I don t see any substantive reasons to reject the paper, the methods are novel and appropriate (even in light of the prior work that exists on this question), and the results are surprising and relevant a high profile ongoing discussion in the literature on representation learning for language.
This paper proposes two new sets of conditions under which we can identify temporally causal latent processes. In this sense, this work makes valuable contributions to the theories of identifiability in this topic. The authors also propose LEAP, extending the VAE, to estimate temporally causal latent processes.  The reviewers had many constructive comments, and the authors strived to address them. In the end, the reviewers were satisfied with the final version of the paper.   Given that the theoretical identifiability theorems are major parts of the paper, I encourage the authors to elaborate more on the two sets of assumptions. They should discuss when these assumptions will hold and provide examples in which they will be violated.
This paper provides an overview of evaluating graph generative models (GGMs). It systematically evaluates one of the more popular metrics, maximum mean discrepancy (MMD). It highlights some challenges and pitfalls for practitioners and suggests some ways to mitigate them. The reviewers found the paper practically relevant and several reviewers upgraded their scores through the discussion process. The authors acknowledged there are still some remaining issues regarding (i) considering other metrics & descriptor functions; ii) evaluating node/edge attributes and iii) addressing molecule generation. I am satisfied that these areas are beyond the scope of the current work and that the clarification improvements in the paper are adequate. It stands well enough on its own to accept in its present form.
This paper proposes a method for modeling higher order interactions in Poisson processes. Unfortunately, the reviewers do not feel that the paper, in its current state, meets the bar for ICLR. In particular, reviewers found the descriptions unclear and the justifications lacking. While the responses did aid the reviewers understanding, the paper would benefit from rewriting and more careful thought given to the experimental design.
The authors show that in a reinforcement learning setting, partial models can be causally incorrect, leading to improper evaluation of policies that are different from those used to collect the data for the model.  They then propose a backdoor correction to this problem that allows the model to generalize properly by separating the effects of the stochasticity of the environment and the policy.  The reviewers had substantial concerns about both issues of clarity and the clear, but largely undiscussed, connection to off policy policy evaluation (OPPE).    In response, the authors made a significant number of changes for the sake of clarity, as well as further explained the differences between their approach and the OPPE setting.  First, OPPE is not typically model based.  Second, while an importance sampling solution would be technically possible, by re training the model based on importance weighted experiences, this would need to be done for every evaluation policy considered, whereas the authors  solution uses a fundamentally different approach of causal reasoning so that a causally correct model can be learned once and work for all policies.  After much discussion, the reviewers could not come to a consensus about the validity of these arguments.  Futhermore, there were lingering questions about writing clarity. Thus, in the future, it appears the paper could be significantly improved if the authors cite more of the off policy evaluation literature, in addition to their added textual clairifications of the relation of their work to that body of work.  Overall, my recommendation at this time is to reject this paper.
The paper presents a technique for approximately sampling from autoregressive models using something like a a proposal distribution and a critic. The idea is to chunk the output into blocks and, for each block, predict each element in the block independently from a proposal network, ask a critic network whether the block looks sensible and, if not, resampling the block using the autoregressive model itself.  The idea in the paper is interesting, but the paper would benefit from   a better relation to existing methods   a better experimental section, which details the hyper parameters of the algorithm (and how they were chosen) and which provides error bars on all plots (and tables)
All reviewers believe that this paper is valuable, and the authors have made a significant, careful contribution.  Some suggestions from the area chair:   "in causality" is not a standard technical term and also not non technical idiomatic English, so it should be explained the first time it is used.   The authors should briefly cite and discuss research on so called positive and unlabeled (PU) learning. This seems like the special case where there is exactly one known class and one novel class. The distinction between sampling in causality and labeling in causality appears in the PU literature, though not under this name.   The authors could also mention the obvious but surprising point that if data are generated by two clusters, then a classifier can be learned using exactly one labeled example not even one from each class.   I have read the reference EJ A’Court Smith. Discovery of remains of plants and insects. _Nature_, 1874 and I fail to see its relevance. It is only one paragraph. Work from the 1800s should not be cited merely to suggest a veneer of scholarliness.   The writing uses italics for emphasis much too often.
The paper proposes a novel off policy meta RL algorithm able to achieve efficient exploration in meta training able to perform a fast task identification. Although the reviewers agree that this paper has merits (relevant topic, interesting idea, nice experimental analysis), they have raised several concerns about the clarity, the novelty, and the soundness of the proposed approach, which make the paper not ready for publication. However, the authors are encouraged in improving their approach since the direction is considered promising by the reviewers.
This paper considers the problem of transfer learning among families of MDP, and proposes a variational Bayesian approach to learn a probabilistic model of a new problem drawn from the same distribution as previous tasks, which is then leveraged during action selection.   After discussion, the three respondent reviewers converged to the opinion that the paper is novel and interesting, and well evaluated. (Reviewer 1 never responded to any questions the authors or me, so I have disregarded their review.) I am therefore recommending an accept.
This paper investigates methods for gradient based tuning of optimization hyperparameters.  This is an interesting area, and the paper isn t bad.  The examination of hypervariance seems relatively novel and useful.  I also appreciate the point about Bayesopt sometimes working well simply due to small ranges.  However, I agree with the criticisms of the reviewers.  Overall this paper isn t quite clear, thorough and impactful enough to make it in this round, but I think with more attention to baselines and scope this paper could be acceptable.  Some minor comments:  1) The signed based optimizer, while simple and sensible (which is good), seem kind of ad hoc. 2) The authors don t seem to have properly scoped the problem and method, since greediness is only a major concern for inner optimization hyperparameters specifically.  It s not clear that for regularization parameters that this problem exists or that your method would apply.  A small nit:  Is hypervariance the right thing to look at, since the problem can exist even in deterministic settings?  Perhaps some sort of sensitivity analysis would be more appropriate.  Also you should reference Barack Pearlmutter s thesis which first explores these issues.   I would also mention that the hypervariance is generally tiny for smaller than optimal learning rates, and massive for larger than optimal learning rates, (the chaotic regime).
This work aims to improve style transfer in the unsupervised non parallel case. It does this by proposing a style equalization approach to prevent content leakage and assuming that content information is time dependent whereas style information is time dependent. This is an important problem to solve and lots of prior work in the area exists. The work is well organised with good experimental results. However, there are strong claims in the paper and there is insufficient experimental comparison to similar related work such as Hsu et al. 2019 and Ma et al. 2018 to back that up. If there s no comparison with the current state of the art (e.g. due to a private implementation or dataset) then it s hard to justify calling a new work a new state of the art. Even though an implementation may be private, it can be worth spending time to reproduce a paper or asking the authors for an implementation. Finally task and metric selection could be improved to better highlight the performance of the approach. The reviewers thank the authors for the rebuttal but it was insufficient to change their decision.
This paper studies the properties of Differentiable Architecture Search, and in particular when it fails, and then proposes modifications that improve its performance for several tasks. The reviews were all very supportive with three Accept opinions, and authors have addressed their comments and suggestions. Given the unanimous reviews, this appears to be a clear Accept. 
This paper proposes a new distributional assumption and a new algorithm for learning convolutional neural networks. However, the reviewers reach a consensus that this paper s assumptions are not natural and may not be satisfied in real world domains. The meta reviewer agrees and thus decides to reject the paper.
This paper conducts a study of the adversarial robustness of Bayesian Neural Network models. The reviewers all agree that the paper presents an interesting direction, with sound theoretical backing. However, there are important concerns regarding the significance and clarity of the work. In particular, the paper would greatly benefit from more demonstrated empirical significance, and more polished definitions and theoretical results. 
The paper investigates weighted empirical risk minimization where the weights on an example in the training set is given by a polynomial function evaluated on the loss on the given example. Authors show that the choice of the weighting function induces a data dependent variance penalization in the training objective. Authors present an algorithm for weighted ERM and empirical results to support their claims. While the problem setting is broadly relevant and the approach the authors take in this paper is interesting, several questions remain unanswered. First, the authors argue that variance penalization helps but do not compare with other regularized ERM approaches. Second, it is not clear if the proposed algorithm is indeed gradient descent on the weighted ERM objective as pointed out by one of the reviewers. Finally, the writing can be improved with more emphasis on the novelty and significance of the contributions. I believe the initial comments from the reviewers has already helped improve the quality of the paper. I encourage the authors to further incorporate the feedback and work towards a stronger submission.
The paper shows that replacing fully connected layers by dense layers in the networks used by actors and critiques in RL can improve the results significantly.  The improvements for several RL techniques across several benchmarks are very nice.  That being said, replacing fully connected layers by dense layers is not particularly novel and it is not clear why dense layers instead of resnet layers works better.  The reviewers appreciate the addition of experiments confirming that dense layers work better than resnet layers.  This addresses an important concern of the reviewers.  However, at this point in deep learning, it is well known that fully connected layers do not work well in general and therefore engineers are expected to use resnet, dense or highway style connections to improve performance when increasing the depth.  The fact that published baselines in OpenAI, TensorFlow and PyTorch do not use those improved networks is one thing, but this does not justify the publication of a paper.  The paper suggests that an RL specific architecture will be proposed, but at the end of the day what is being proposed is not specifically for RL, but rather the addition of new connections to the inputs similar to the well known dense architecture to augment fully connected layers in RL.  It is not clear why this works better than resnet connections.  Another alternative that was not considered are highway networks.  To strengthen the contribution of the paper, the authors are encouraged to provide an analysis of the possible approaches and to provide some insights.  
Pros: The proposed regularization for GAN training is interesting and simple to implement.  Cons:   Reviewers agree that the methodology is incremental over the WGAN with gradient penalty and the modification is not well motivated.   Experimental results do not clearly demonstrate the benefits of the proposed algorithm and the paper also lacks comparisons with related works. GIven the pros/cons, the committee feels the paper is not ready for acceptance in its current state.
+ experiments on an interesting task: inferring relations which are not necessarily explicitly mentioned in a sentence but need to be induced relying on other relations + the idea to frame the relation prediction task as an inference task on a graph is interesting     the paper is not very well written, and it is hard to understand what exactly the contribution is. E.g., the authors contrast with previous work saying that previous work was relying on pre defined graphs rather than inducing them. However, here they actually rely on predefined full graphs as well (i.e. full graphs connecting all entities).   (See questions from R1)    the idea of predicting edge embeddings from the sentence is an interesting one. However, I do not see results studying alternative architectures (e.g., fixed transition matrices + gates / attention), or careful ablation studies. It is hard to say if this modification is indeed necessary / beneficial.  (See also R3, agreeing that experiments look preliminary)    Extra baselines? E.g., what about layers of multi head self attention across entities? (as in Transformer). What about the number of parameters for the proposed model? Is there chance that it works better simply because it is a larger model? (See also R3)    evaluation only one dataset (not clear if any other datasets of this kind exist though)  Overall, though I find the direction and certain aspects of the model quite interesting, the paper is not ready for publication.
This paper modifies the AlphaZero algorithm to generate proof tree size heuristics and shows empirical improvements over standard search algorithms. This is an interesting distinction that might lead to algorithms with distinct play styles and a deeper understanding of the games that we apply our agents to.  The two positive reviewers felt that it was a solid contribution, worthy of publication. There were some questions regarding the clarity of the writing that were addressed in the discussion phase. The two reviewers that gave lower scores felt that the paper did not do a sufficient job motivating the work and distinguishing itself from the literature. Ultimately, I agree with the positive reviewers, and it is my opinion that the revised version is acceptable for publication.
This paper addresses a meta learning method which involves bilevel optimization. It is claimed that two limitations (myopia of MG and restricted consideration of geometry of search space) that most of existing methods have can be resolved by the MBG with a properly chosen pseudo metric. The algorithm first bootstraps a target from the meta  learner, then optimizes the meta learner by minimizing the distance to that target under a chosen pseudo metric. The authors also establish conditions that guarantee performance improvements and show that metric can be sued to control meta optimization. All the reviewers agree that the idea is interesting and experiments well support it. Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. While the current theoretical results are limited to a simple case where L 1$, the method is attractive for meta learning community. All reviewers agree to champion this paper. Congratulations on a nice work.
The authors conduct experiments to study orientation selectivity in neural networks.   The reviewers generally agreed that the paper was clearly written and easy to follow. Further, the experimental analysis demonstrates that contrary to what was claimed in some previous work, the learned orientation selectivity can be useful for generalization.   However, the reviewers also raised a number of concerns: 1) that the conclusions are drawn on the basis of a couple of neural network architectures; the authors attempted to add results using a Resnet50 model, but this analysis was ultimately removed when the authors discovered a bug; 2) in the context of the contributions in neuroscience it was not clear that the limited results on the two artificial networks are sufficient to help draw such conclusions, and that 3) since the network is trained to recognize objects, it would seem natural that the model would learn neurons that are sensitive to orientation and that it is not clear how the author’s observations might lead to better trained models.  While the reviewers were not completely unanimous in their scores, the AC agrees with a majority of the reviewers that the work while interesting could be strengthened by additional experiments on other architectures.  
The setting and the problem addressed by this paper has been considered as important and interesting to tackle with reinforcement learning. Yet, the reviewers expressed several concerns about this paper. Especially, the lack of comparison to state of the art methods and to the standard visualization methods was a shared concern. The empirical validation also appeared as not ambitious enough. Finally, the novelty in the field of machine learning was also questioned since the paper is mainly about applying existing algorithms to a known problem. 
This work gives an interesting perspective on combining options with exploration in the non tabular case. The reviewers have raised a number of important areas for improvement (primarily missing ablations to support the claims of the paper, but also specific suggestions about improvements to the text), and feel that sufficient work is required to address these that the paper should be rejected at this time.
The authors tackle the questions of automatic metrics for assessing document similarity and propose the use of Transformed based language models as a critic providing scores to samples. As a note, ideas like these have been also adopted in Computer Vision with the use of the Inception score as a proxy the quality of generated images. The authors ask great questions in the paper and they clearly tackle a very important problem, that of automatic measures for assessing text quality. While their first indications are not negative, this paper lacks the rigor and depth of experiments of a conference paper that would convince the research community to abandon BLEU and ROUGE in lieu of some other metric. It s perhaps a good workshop paper or a short paper at a *CL conference. Specifically, we would need more tasks where BLEU/ROUGE is the standard measure and so how the proposed measure correlates better with humans,  so cases where word overlap is in theory a good proxy of similarity assuming reference sentence (e.g., logical entailment is not such a prototypical task). MT is a first step towards that, but summarization is also a necessary I would say. Other questions of interest relate to the type of LM (does it only need to be Roberta?) and the quality of LM (what if i badly tune my LM?)  On a more personal note: We all know that BLEU is not a good metric (especially for document level judgements) and every now and then there have been proposals to replace BLEU that do correlate better (e.g., http://ccc.inaoep.mx/~villasen/bib/Regression%20for%20machine%20translation%20evaluation.pdf) . However, BLEU is still here due to each simplicity. Please keep pushing this research and I’m looking forward to seeing more experimental evidence.
The paper studies the adversarial robustness of vision transformers. The authors conclude that vision transformers are generally more adversarially robust than the convolutional neural networks. Several interesting empirical conclusions are made for the robustness property of vision transformers. Sufficient empirical experiments are conducted. Overall, the paper is well written, well organized, and interesting. However, there are some concerns about the current version. (1) There are some concurrent works having similar empirical findings, which have been formally published and would weaken the interest of readers in the paper. (2) The reviews suggest that the authors use the insights from the paper to design more robust and effective vision transformers. The four reviewers have unanimous recommendations below the acceptance threshold. We therefore cannot recommend acceptance. However, we believe that by taking the comments, the next version would be a very strong paper.
The reviewers were unanimous that this submission is not ready for publication at ICLR in its present form.  Concerns raised included lack of relevant baselines, and lack of sufficient justification of the novelty and impact of the approach.
This is a strong paper presenting a very clean proof of a result that is similar, though now incomparable to one due to Bartlett et al. These bounds (and Bartlett s) are among the most promising norm based bounds for NNs.  I would simply add that the citation of Dziugaite and Roy (2017) could be improved. There work also connects sharpness (or flatness) with generalization via the PAC Bayes framework, and moreover, there bounds are nonvacuous.  Are the bounds in this paper nonvacuous, say, on MNIST for 60,000 training data, for the network learned by SGD?  If not, how close do they get to 1.0?
This is a borderline case (quite comparable to the other borderline case in my batch). The paper has received careful reviews and based on my weighting of the different arguments I arrive at an average score between 5.75 and 6.. The authors present some worthwhile ideas related to disentanglement that deserves more attention and that could spark more research in this direction. At the same time, the level of novelty and significance of this work remains a bit limited. Taken together the paper is likely not compelling enough to be among the top papers to be selected for publication at ICLR. 
Three knowledgeable referees recommend Accept. Reviewer eyrZ s concerns have been addressed by the authors in the rebuttal, in my opinion.  Therefore I recommend Accept. I ask the authors to 1) rename the title of their paper and their model to the more specific name Multi task Neural Processes (MTNP). I agree with both reviewers F6YH and ACBa that the name "Multi task Processes" does not make justice to the many other models out there that also provide ways to model several stochastic processes simultaneously. Make sure you propagate the name of the new model through the paper. 2) include a discussion in the main paper about the variability of the new results provided in the rebuttal. Only mean NLL and MSE are provided which can be misleading without standard deviations and potential tests for statistical significance.
This paper investigates variational models of speech for synthesis, and in particular ways of making them more controllable for a variety of synthesis tasks (e.g. prosody transfer, style transfer).  They propose to do this via a modified VAE objective that imposes a learnable weight on the KL term, as well as using a hierarchical decomposition of latent variables.  The paper shows promising results and includes a good amount of analysis, and should be very interesting for speech synthesis researchers.  However, there is not much novelty from a machine learning perspective.  Therefore, I think the paper is not a great fit for ICLR and is better suited for a speech conference/journal.
The reviewers are in consensus that the manuscript is not ready for publication in its current form: more comprehensive evaluation, and careful analysis (either theoretical or empirical) of the simple but effective methodology would improve the quality further. The discussion was constructive and helped the authors to reason about their work better.  The AC recommends Reject and encourages the authors to take the constructive feedback into consideration . 
The authors provide a framework for improving robustness (if the model of the dynamics is perturbed) into the RL methods, and provide nice experimental results, especially in the updated version. I am happy to see that the discussion for this paper went in a totally positive and constructive way which lead to a) constructive criticism of the reviewers b) significant changes in the paper c) corresponding better scores by the reviewer. Good work and obvious accept.
The paper proposes to use simple regression models for predicting the accuracy of a neural network based on its initial training curve, architecture, and hyper parameters; this can be used for speeding up architecture search. While this is an interesting direction and the presented experiments look quite encouraging, the paper would benefit from more evaluation, as suggested by reviewers, especially within state of the art architecture search frameworks and/or large datasets.
The reviewers all agreed that the proposed modification was minor. I encourage the authors to pursue in this direction, as they mentioned in their rebuttal, before resubmitting to another conference.
This is a solid paper that proposes a new slicing approach to the fused Gromov Wasserstein distance using projection on directions sampled from  von mises fisher direction, the location parameter of the von mises fisher is choosen to be maximally discriminating between the distribution  $(\max_{\epsilon}\mathbb{E}_{\theta|vMF(\theta|\epsilon,\kappa)}\beta W(\theta\mu,\theta\nu) +(1 \beta)GW(\theta\mu,\theta\nu))$, the new sliced distance is analyzed and extended to mixture of von mises distributions with $k$ locations or directions.  This contribution of the paper is of general interest beyond the application of the paper as mentioned by the reviewers.  Authors applies the new sliced Fused Gromov Wasserstein distance to relational auto encoders and show improvement.  The spherical slicing is original and new and of independent interest and the application is good as it pushes the boundary of relational auto encoders .Reviewers  and AC did not have any concerns with the paper and the rebuttal and revisions addressed all questions raised. Accept  
The paper proposes a policy gradient algorithm related to entropy regularized RL, that instead of the KL uses f divergence to avoid mode collapse.  The reviewers found many technical issues with the presentation of the method, and the evaluation. In particular, the experiments are conducted on particular program synthesis tasks and show small margin improvements, while the algorithm is motivated by general sparse reward RL.  I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit an improved version elsewhere.
The paper proposes an adversarial framework that learns a generative model along with a mask generator to model missing data and by this enables a GAN to learn from incomplete data. The method builds on AmbientGAN but it is a novel and clever adjustment to the specific problem setting of learning from incomplete data, that is of high practical interest.
The authors analyzing the VC dimension of a class of neural networks with hard thresholds at the hidden nodes that include a low rank weight matrix and hard thresholds at hidden units.  The bounds are independent of the number of weights used to represent functions mapping a hidden layer to the output.  They also provided some experiments supporting the practicality of networks like those treated in their theoretical analysis.  There was some question about whether the VC dimension continues to be relevant. Also, while the upper bounds have attractive properties that were highlighted by the authors, they also are not very strong in other respects.  The consensus view overall, though, was that this is a "nice result", a clean illustration of a generalization affect of the type that has been of wide interest lately.
This paper discusses the conditional independence test using GAN.   In the same way as GCIT (Bellot & van der Schaar, 2019), they realize sampling under the null hypothesis by generating sample from P(X|Z) approximately with GAN.  They propose to use a test statistic defined by the maximum of generalized covariance measures (GCM) over random neural networks.  They theoretically discuss the advantage of GCM and show the asymptotic results of the proposed test statistic, which demonstrates improved justification over GCIT.  Experimental results show favorable performances over existing conditional independence tests.   The proposed method gives an advance in the methodology of conditional independence tests for continuous domain, which is an important but difficult problem because of the difficulty of obtaining the null distribution.  In the line of Bellot & van der Shcaar (2019), they solve it using the strong conditional sampling ability of GAN, which is an important research area. The theoretical analysis and experimental results are also making good contributions.    However, there are some weakness in the proposed method and comparison with existing methods. First, as R4 points out, there are many hyperparameters in the proposed method, and their choice is not easy.  While the authors addressed some aspects of this issue in their rebuttal and revision, it is still unclear how to justify the choice of B, the functions h_j, and the neural networks of GAN, which should potentially have significant influence on the test performance.  Second, the comparison with Bellot and van der Schaar (2019) is not very clear.  In the paper, the GCIT has been used with the distance correlation, which is known to be an instance of HSIC (MMD) with a specific choice of positive definite kernel (Sejdinovic et al 2013).  The HSIC can be formulated as the maximum of generalized covariance measures over the unit ball of the RKHS.  Thus, the difference of GCIT with distance correlation and the proposed methods are essentially the difference of the function classes for the maximum.  On the other hand, the experimental results show significant difference in the test performance.  I think more elaborate and careful comparison is needed for these two methods.   Overall, the paper is a good contribution on the topic.  However, the evaluation of the reviewers is not high enough to justify the acceptance in the high competition of ICLR.  I encourage the authors complete their work by reflecting reviewers’ comments and submit this work to another conference or journal.   Reference: Sejdinovic, D., Sriperumbudur, B., Gretton, A., & Fukumizu, K. (2013). Equivalence of distance based and RKHS based statistics in hypothesis testing. Annals of Statistics, 41(5), 2263–2291. 
This paper has been evaluated by three reviewers with 2 borderlines leaning towards the accept, and with 1 accept. The reviewers have noted that the idea of alignment is not particularly novel per se. Nonetheless, they found some merit in the use of a network learning the alignment and they liked experiments.  AC has however some concerns about this work. Firstly, it is not clear why Lifted+SoftDTW and Binomial+SoftDTW completely fail in Table 1, and in Table 5, SoftDTW is worse by 30% than TAP. Is soft DTW set up properly in these experiments (the softmax temperature, the base distance used, the maximum number of steps away from the main path etc.)?  AC is also not convinced about the principled nature of the proposed alignment. Eq. 3 and the residual design above seem more as heuristics than a principled OT transport as Eq. 1 and 2 set out to suggest. With concatenation of distances between sequence features and positional encoding, the proposed alignment seems more similar to attention and transformers than OT.
This paper studies the problem of visual question answering in multi turn dialogues. The proposed method is to identify relevant dialog turns as a path in a semantic graph that connects the dialogue turns. Empirical performance of the proposed method is strong. Reviewers concerns have been compressively addressed. Overall, the paper has novelty, and explores an interesting direction in this line of work.
This paper extends the degree to which ReLU networks can be provably resistant to a broader class of adversarial attacks using a MMR Universal regularization scheme.  In particular, the first provably robust model in terms of lp norm perturbations is developed, where robustness holds with respect to *any* p greater than or equal to one (as opposed to prior work that may only apply to specific lp norm perturbations).  While I support accepting this paper based on the strong reviews and significant technical contribution, one potential drawback is the lack of empirical tests with a broader cohort of representative CNN architectures (as pointed out by R1).  In this regard, the rebuttal promises that additional experiments with larger models will be added in the future to the final version, but obviously such results cannot be used to evaluate performance at this time.
This paper seeks to understand the effect of learning rate decay in neural net training. This is an important question in the field and this paper also proposes to show why previous explanations were not correct. However, the reviewers found that the paper did not explain the experimental setup enough to be reproducible. Furthermore, there are significant problems with the novelty of the work due to its overlap with works such as (Nakiran et al., 2019), (Li et al. 2019) or (Jastrzębski et al. 2017).
The paper proposes and studies a method for the responsible disclosure of a fingerprint along with samples generated by a generative model, which has important applications in identifying "deep fakes". The authors establish both the detectability of their fingerprint without significant loss of fidelity as well as the robustness to perturbations. The reviewers found the problem and contributions to be important and significant, well substantiated by an extensive experimental study.
The paper proposes using the intermediate representation learned in a denoising diffusion model for the label efficient semantic segmentation task. The reviewers are generally positive with the submission. They like the simplicity of the proposed algorithm. They also like the effort of the paper in verifying the intermediate representation learned by a diffusion model is semantically meaningful and can be used for segmentation. Initially, there was some concern about the size of the validation set, which is addressed by the rebuttal. Consolidating the reviews and rebuttals, the meta reviewer agrees with the assessment of the reviewers and would like to recommend acceptance of the paper.
The authors’ feedback has not fully addressed the reviewers’ concerns and the reviewers think that the paper is not ready for the publication. The authors should consider the following issues for the future submission:  1) The concern from Reviewer 1: if a local device receives very little data but its data come from a mixture component with large weight, its gradient will likely be biased (due to the lack of data) but will still dominate others (due to its large mixture weight).  2) Numerical experiments are not consistent with theoretical results. The theory is for convex but experiments are with non convex loss. The response from authors does not resolve this issue.   3) Notation is confusing and changing throughout. We strongly suggest the authors revise carefully this and make it clear.   Although the experimental results are potential, we would like the authors to revise it carefully by addressing the reviewers’ concerns and further improve it by considering theoretical results for non convex in order to submit to the next venues.  
The paper looks at a novel form of physics constrained system identification for a multi link robot, although it could also be applied more generally.  The contributions is in many simple; this is seen in a good light (R1, R3) or more modestly (R2). R3 notes surprise that this hasn t been done before. Results are demonstrated on a simualted 2 dof robot and real Barrett WAM arm, better than a pure neural network modeling approach, PID control, or an analytic model.    Some aspects of the writing needed to be addressed, i.e., PDE vs ODE notations.  The point of biggest concern is related to positioning the work relative to other system identification literature, where there has been an abundance of work in the robotics and control literature. There is no final consensus on this point for R3;  R3 did not receive the email notification of the author s detailed reply, and notes that the author has clarified some respects, but still has concerns, and did not have time to further provide feedback on short notice.    In balance, the AC believes that this kind of constrained learning of models is underexplored, and notes that the reviewers (who have considerable shared expertise in robotics related work) believe that this is a step in the right direction and that it is surprising this type of approach has not been investigated yet.  The authors have further reconciled their work with earlier sys ID work, and can further describe how their work is situated with respect to prior art in sys ID (as they do in their discussion comments).  The AC recommends that: (a) the abstract explicitly mention "system identification" as a relevant context for the work in this paper, given that the ML audience should be (or can be) made aware of this terminology; and (b) push more of the math related to the development of the necessary derivatives to an appendix, given that the particular use of the derivations seems to be more in support of obtaining the performance necessary for online use, rather than something that cannot be accomplished with autodiff. 
This paper addresses the reward free exploration problem with function approximation under linear mixture MDP assumption. The analysis shows that the proposed algorithm is (nearly) minimax optimal.  The proposed approach can work with any planning solver to provide an ($\epsilon + \epsilon_{opt}$) optimal policy for any reward function.  After reading the authors  feedback and discussing their concerns, the reviewers agree that the contributions in this paper are valuable and that this paper deserves publication. I encourage the authors to follow the reviewers  suggestions as they will prepare the camera ready version.
This paper proposes a temporal module for video representation learning, which is a combination of temporal attention and temporal convolution.  The reviewers  opinions diverge. R2 does not find any major flaws of the paper, while R1 expressed concerns in terms of experimental details, ablations, and missing comparison to the state of the arts. R4 expressed a similar concern, while favoring the paper a bit more.  The AC agrees more with the senior reviewers (R1 and R4) that the paper misses its experimental comparison to the state of the arts. In particular, the AC supports the statement from R4 that "Some SOTA performances are ignored selectively" to favor the proposed approach. The missing SOTA includes Slow Fast as pointed out by R4, X3D as pointed out by R1, and more. For instance, X3D is able to obtain 80.4% top 1 accuracy on Kinetics 400, which is superior to 76.9% of this paper, but is being ignored in the paper. X3D that uses a different strategy to abstract temporal information than this paper is also superior in terms of the FLOPS: X3D gets 79.1% while using almost half of the computation the proposed approach is using. The authors responded by "X3D uses network architecture search strategy to discover the optimal setting for 3D CNNs", but this is a misleading statement as X3D does not use any neural architecture search method. The authors argue that their proposed approach might be able to also benefit X3D, but this has not been confirmed and we cannot judge since no quantitative results are provided. In addition, as mentioned, several other standard baselines such as Non Local R101 (with 77.7% accuracy) and ip CSN 152 (with 77.8% accuracy) performing better than the proposed approach are missing.  Overall, we find the experimental section of the submitted paper incomplete.
Thank you for submitting you paper to ICLR.  The consensus from the reviewers is that this is not quite ready for publication. There is also concern about whether ICLR, with its focus on representational learning, is the right venue for this work.  One of the reviewers initially submitted an incorrect review, but this mistake has now been rectified. Apologies that this was not done sooner in order to allow you to address their concerns.
Main content:  Blind review #1 summarizes it well:  This paper presents a semantic parser that operates over passages of text instead of a structured data source.  This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different).  The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations.  This is excellent work, and it should definitely be accepted.  I have a ton of questions about this method, but they are good questions.       Discussion:  The reviews all agree on a generally positive assessment, and focus on details that have been addressed, rather than major problems.     Recommendation and justification:  This paper should be accepted. Even though novelty in terms of fundamental machine learning components is minimal, but the architecture employing neural models to do symbolic work is a good contribution in a crucial direction (especially in the theme of ICLR).
## Description   The paper proposes an improvement to binary neural networks with real valued skip connections between pre activations, by introducing more flexible learnable non linearities on the real valued connections. The parametric non linearity is actually linear at initialization, which makes the training easier at the beginning. Due to learnable parameters it eventually adjusts to a more complex one, able to refine the accuracy. I think this idea is a good finding.  ## Review Process and Decision The reviewers initially gave low ratings to the paper, indicating that the contribution is incremental and not fully clearly presented.  There was no detailed discussion with the authors, since the author s response and the rebuttal revision came in the very end of the discussion period. In the subsequent discussion phase the reviewer board has not indicated any major changes to the initial reviews/ranking. The AC checked the paper and supports rejection.  ## Details  The authors are encouraged to improve the paper carefully addressing points proposed by reviewers.  I think the argumentation of the paper should be improved. Some explanations are intuitive, but operating with fuzzy notions and may in fact be incorrect or irrelevant. The paper should be made more precise, based on verifiable arguments.  I think the following is crucial and not made clear in the paper: The non linearities inserted before the sign function *do not affect the result of sign*. They indeed affect only the residual connections. Furthermore, the structure of residual connections should be fully clarified to reveal that there are complete real valued paths all the way from the input to the network to its output, made of the residual connections with their own learnable parameters (and 1x1 convolutions) and (learnable) non linearities and an intake from binary convolutions on the way. The learnable non linearities can in principle improve performance just because the real valued paths can learn better.  I paste below feedback by reviewers to author s response (I believe they would agree to share it with authors but did not find a suitable way of doing it):  ## Response by R1: I acknowledge that I read and appreciated the authors  answers to my questions. I think the idea of analyzing the role of non linearities is nice and I tend to confirm my score. But I also agree with other reviewers that, as it is, the paper has some unclear parts and would not complain if it is rejected.  ## Response by R3: Thanks for your responses to answer my questions for the paper. I agree with the results of the proposed FBTN for improved Binary Neural Networks (BNNs). However, my concern about the novelty of using group convolution modules in BNNs has not been addressed. I think the paper is not sufficient enough to publish at the conference. So, I do not change my rating of the paper.  ## Response by R4: I maintained my rating when combining other reviews and responses to them, despite of their well response. It is still questionable whether FPReLU, one of the main contributions they claimed, actually improves the performance of BNN remarkably. In particular, this is supported by the fact that the performance of BNN on ResNet 34 which the techniques in this paper were applied does not show much difference from  Real to Bin  model.
The paper studies how neural combinatorial solvers can be susceptible to adversarial examples and what implications does this susceptibility have on the evaluation of neural solvers. Besides proposing some successful adversarial attacks, the authors provide a method for adversarial training and show its effectiveness on improving robustness and generalization. All the reviewers agreed that this paper provides a set of very interesting and novel results.
This paper investigates the scaling laws of neural networks with respect to the number of training samples $D$ and parameters $P$ for some estimators in two regimes: the variance limited regime and the resolution limited regime. The theoretical results are supported by some numerical experiments.  Unfortunately, the paper has several critical issues, in particular, in its novelty and technical correctness.  1. The theoretical analyses lack much of their rigor. The assumptions and problem setups are not precisely introduced. Accordingly, the statement of each theorem is presented in an inaccurate way. Moreover, some theoretical consequences contain technical flaws (e.g., $1/P$ should be replaced by $1/\sqrt{P}$ without an appropriate assumption on the loss function such as strong convexity and smoothness).   2. Many of the presented results are already known in the literatures. It is unfortunate that the authors did no cite relevant existing literatures and did not discuss its novelty compared with the existing work.   For those reasons, this paper lacks its novelty and the quality of the paper is not sufficient to be accepted.   I recommend the authors to thoroughly survey the literature of the statistical learning theory from classic nonparametric regression analyses to recent advances on overparameterization.
The paper introduces a novel way of learning Hamiltonian dynamics with a generative network. The Hamiltonian generative network (HGN) learns the dynamics directly from data by embedding observations in a latent space, which is then transformed into a phase space describing the system s initial (abstract) position and momentum. Using a second network, the Hamiltonian network, the position and momentum are reduced to a scalar, interpreted as the Hamiltonian of the system, which can then be used to do rollouts in the phase space using techniques known from, e.g., Hamiltonian Monte Carlo sampling. An important ingredient of the paper is the fact that no access to the derivatives of the Hamiltonian is needed.   The reviewers agree that this paper is a good contribution, and I recommend acceptance.
This paper introduces an unsupervised algorithm to learn a goal conditioned policy and the reward function by formulating a mutual information maximization problem. The idea is interesting, but the experimental studies seem not rigorous enough. In the final version, I would like to see some more detailed analysis of the results obtained by the baselines (pixel approaches), as well as careful discussion on the relationship with other related work, such as Variational Intrinsic Control.
The paper builds on the prior work by Miryoosefi et al. (2019) that finds a feasible mixed policy under convex constraints through distance minimization over a simplex set. Instead of the primal dual approach used in Miryoosefi et al. (2019), this paper proposes to apply Frank Wolfe type algorithm (particularly, the minimum norm point algorithm) to promote sparsity of the mixed policy, while achieving the same complexity.   Despite the improvement on sparsity, the AC and some reviewers share two main concerns: (1) incremental novelty of the algorithm/theory, which basically follows from existing optimization work, (2) lack of (theoretical and numerical) justification of the significance of sparsity (especially given that the main computation costs come from projection and RL oracle).   Unfortunately, the paper lands just below borderline and cannot be accepted this time.  
This works considers limitations of rehearsal based methods in the context of continual learning (classification and object detection). Rehearsal based methods provide a strong baseline, but a loss in predictive performance arises when the memory is limited in size. The authors propose to leverage compression (JPEG) to increase the number of data (images) stored in the memory. The approach is evaluated in the context of an autonomous driving application.  The additional experiments conducted by the authors were highly appreciated and helped clarify open questions (e.g., class incremental learning set up, DPP objective to determine size of the memory, quantity vs quality of compressed data, etc.). The authors addressed the issues raised by three out of four reviewers, who did not have further comments. The remaining reviewer found that the methodological contributions of this paper, namely of using compression in the context of CL, was pretty straightforward. However, the authors addressed the concerns raised by the reviewer regarding the selection of the compression quality q as far as I am concerned and conducted additional experiments to further demonstrate the usefulness of the approach. I would encourage the authors to include this discussion in the final version of the paper. I would also encourage them to include the additional experiments they conducted with fixed memory size and amount of memory that can be saved.
This paper proposes a method to compute embeddings of states and actions that facilitate computing measures of surprise for intrinsic reward. Though some of the ideas are quite interesting, there are currently issues with the experiments and the motivation.  The experiments have high variance across the 5 runs, with significant overlap of shaded regions representing just one standard deviation from the mean. It is hard to draw any conclusions about improved performance, and statements like the following are much too strong: "For vision based exploration tasks, our results in Figure 5 show that EMI achieves the state of the art performance on Freeway, Frostbite, Venture, and Montezuma’s Revenge in comparison to the baseline exploration methods." Further, the proposed approach has three new hyperparameters (lambdas), without much understanding into how to set them or their effect on the results. Specific values are reported for the different game types, without explanation for how or why these values were chosen.   Similarly strong claims, that are not well substantiated, are given for the proposed approach. This paper seems to suggest that this is a principled approach to using surprise for exploration, contrasted to other ad hoc approaches ("Other approaches utilize more ad hoc measures (Pathak et al., 2017; Tang et al., 2017) that aim to approximate surprise."). Yet, the paper does not define surprise (say by citing work by Itti and Baldi on Bayesian surprise), and then proposes what is largely a intuitive approach to providing a good intrinsic reward related to surprise. For example, "we show that imposing linear topology on the learned embedding representation space (such that the transitions are linear), thereby offloading most of the modeling burden onto the embedding function itself, provides an essential informative measure of surprise when visiting novel states." This might be intuitively true, but I do not see a clear demonstration in Section 4.2 actually showing that this restriction provides a measure of surprise. Additionally, some of the choices in Section 4.2 are about estimating "irreducible error under the linear dynamics model", but irreducible error is about inherent uncertainty (due to stochasticity and partial observability), not due to the choice of modeling class. In general, many intuitive choices in the algorithm need to be better justified, and some claims disparaging other work for being ad hoc should be toned down.   Overall, this paper is as yet a bit preliminary, in terms of clarity and experiments. In a further iteration, with some improvements, it could be a useful contribution for exploration in image based environments. 
This work improves deep generative models by applying Langevin dynamics to sample in the latent space. The authors test their method under different configurations (different loss functions) and various generative models (VAE, flow, besides GAN). Experimental results demonstrate the benefits of the proposed method in different generative tasks.   I tend to accept this solid work. I just have two suggestions: 1) the authors should discuss the connections and the differences between the proposed method and the energy based methods like (Arbel et al., 2020) in depth; 2) it may be more suitable to replace "Wasserstein gradient flow" with "Discriminator gradient flow" in the title.
The idea of integrating causality into an auto encoder is interesting and very timely. While the reviewers find this paper to contain some interesting ideas, the technical contributions and mathematical rigor, scope of the method, and the presentation of results would need to be significantly improved in order for this work to reach the quality bar of ICLR.
The paper looks into generalization performance of NNs in supervised learning setting. The authors propose a regularizer to enhance neuron diversity in each layer(within layer activation diversity) as a regularizer to improve generalization.  The proposed idea is an extension of Cogswell s work with different regularization terms. The appearance of the term related to the layer output diversity in the generalization bound provides theoretical support for the proposed idea.They use Radamacher complexity as a tool to show this and bound the estimation error.  pros.  The paper looks into an interesting problem. Designing a regularizer to improve generalization performance of NNs is of huge importance.  The paper is well presented and clear.  cons.  The main drawback of the paper is lack of proper comparison to other regularizers and showing the uniqueness/superiority of this regularizer and how it improves over existing methods either theoretically or with experiments. Without that the significance of this work is limited.   The authors response to Reviewer 2 s comment was not convincing enough. I encourage the authors to improve this in the next iteration of the paper.    i suggest doing a better job in including the related work as also mentioned by the reviewer.  The experiment section can use more explanation and details on choice of hyper parameters, etc    showing performance improvement for a deep architecture would definitely  improve the paper. In the current version only 2 and 3 layer toy examples are shown.
This paper proposes to overcome the issue of inconsistent availability of longitundinal data via the combination of leveraging principal components analysis and locality preserving projections. All three reviewers express significant reservations regarding the technical writing in the paper. As it stands, this paper is not ready for publication.  
While the reviewers all agree that this paper proposes an interesting application of GANs, they would like to see clearer explanations of the technical details, more convincing evaluations, and better justifications of the assumptions and practical values of the proposed algorithms. 
the proposed approach nicely incorporates various ideas from recent work into a single meta learning (or domain adaptation or incremental learning or ...) framework. although better empirical comparison to existing (however recent they are) approaches would have made it stronger, the reviewers all found this submission to be worth publication, with which i agree.
 In this paper, the authors proposed the expected quadratic utility maximization (EQUM) to implement the risk aware decision making for mean variance RL. The EQUM framework directly optimizes the weighted sum of first and second order moments, while ignores the square of first moments, and thus, largely reduces the difficulty in optimization. The authors tested the proposed policy gradient based algorithm empirically.   However, the connection to classic mean variance is not clearly by simply ignoring the square of first moments in the objective theoretically (R2, R3, R4). Meanwhile, the effect of tunable weights (\psi) is not clear and consistent empirically (R2, R3, R4).    As all reviewers agree this paper is interesting and promising, I encourage the authors to address these issues and consider to submit to next venue.  
This paper considers the information plane analysis of DNNs. Estimating mutual information is required in such analysis which is difficult task for high dimensional problems. This paper proposes a new "matrix–based Renyi’s entropy coupled with ´tensor kernels over convolutional layers" to solve this problem. The methods seems to be related to an existing approach but derived using a different "starting point". Overall, the method is able to show improvements in high dimensional case.  Both R1 and R3 have been critical of the approach. R3 is not convinced that the method would work for high dimensional case and also that no simulation studies were provided. In the revised version the authors added a new experiment to show this. R3 s another comment makes an interesting point regarding "the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities mutual information seems like a leap that s not justified in the paper." I could not find an answer in the rebuttal regarding this.  R1 has also commented that the contribution is incremental in light of existing work. The authors mostly agree with this, but insist that the method is derived differently.  Overall, I think this is a reasonable paper with some minor issues. I think this can use another review cycle where the paper can be improved with additional results and to take care of some of the doubts that reviewers  had this time.   For now, I recommend to reject this paper, but encourage the authors to resubmit at another venue after revision.
Main content: Proposes a deep RL unified framework to manage the trade off between static pruning to decrease storage requirements and network flexibility for dynamic pruning to decrease runtime costs Summary of discussion: reviewer1: Reviewer likes the proposed DRL approach, but writing and algorithmic details are lacking reviewer2: Pruning methods are certainly imortant, but there are details missing wrt the algorithm in the paper.  reviewer3: Presents a novel RL algorithm, showing good results on CIFAR10 and ISLVRC2012. Algorithmic details and parameters are not clearly explained.  Recommendation: All reviewers liked the work but the writing/algorithmic details are lacking. I recommend Reject. 
This submission proposes a simple, efficient, and effective position representation method for the Transformer architecture called ALiBi. ALiBi enables better extrapolation and performance (in terms of efficiency and task performance). The submission also includes careful analysis and extensive experiments, and notably suggests that the gains of ALiBi may be less pronounced in more scaled up settings. All reviewers agreed the paper should be accepted. I think it s reasonably likely that ALiBi will become a common choice in future Transformer models, or at the very least that this work will prompt further work on developing improved position representations for Transformer models. I therefore recommend acceptance.
This paper proposes modifying the training loss for neural net based PDE solvers, by adding an L_infty (max) term to the standard L_2 loss.  The motivation for this loss is sensible in that it matches the definition of a strong solution, but this is only a heuristic motivation, and is missing a theoretical analysis.  This paper s lack of novelty and polish, as well as the lack of clarity in the implementation details, makes this a narrow reject.
The authors propose Variational Inference for Concept Embeddings (VICE), a method to learn representations such that an odd object can be detected given a triplet (i.e. the odd one out task). The authors build on Sparse Positive object Similarity Embedding (SPoSE) which learns sparse, non negative embeddings for images by placing a zero mean Laplace prior. Claimed contributions include replacing it with a spike and slab Gaussian mixture prior, and a principled approach to choosing the subset of the dimensions of the learned embeddings. The empirical results show improvements over the SPoSE baseline.  The reviewers appreciated the empirical improvements over SPoSE and accept that a more informative prior might lead to improved results. However, the **motivation, novelty and significance** of the proposed method doesn’t meet the acceptance criteria for ICLR. After the rebuttal and the discussion phase the reviewers felt that the work necessitates a major revision (notwithstanding the remaining issue with limited novelty), and raised the following as the main improvement points:    Clarifying the motivation and significance.   Stronger empirical validation and generalization beyond the THINGS dataset.   Address the discrepancy with analyzing GMM priors, but using unimodal Gaussians in the implementation.   Comparing the chosen prior to other prior distributions and justifying the design choices.
This paper first discusses some concepts related to disentanglement. The authors propose to decompose disentanglement into two distinct concepts: consistency and restrictiveness. Then, a calculus of disentanglement is introduced to reveal the relationship between restrictiveness and consistency. The proposed concepts are applied to analyze weak supervision methods.   The reviewers ultimately decided this paper is well written and has content which is of general interest to the ICLR community.
Despite the fact that some of the reviewers found the idea interesting, none of them believe that the paper is ready to be published at this stage. For example, better comparison with existing/similar work, and more solid argument on why the idea is better than alternatives are mentioned. All considered, unfortunately I cannot recommend acceptance of this paper in its current form. I encourage the authors to consider these comments and revise their paper accordingly.
i am a big fan of this idea, but i agree with the reviewers that evaluating this idea on bAbI (which was originally created from a small set of rules and primitives) discounts quite a bit of what is being claimed here. one of the future directions mentioned by the authors ("investigating whether the proposed n gram representation is sufficient for natural languages") should have been included even with a negative result, which would ve increased the significance significantly.
This work proposes using spectral element methods to speed up training of ODE Networks for system identification. The authors utilize truncated series of Legendre polynomials to analyze the dynamics and then conduct experiments that shows their proposed scheme achieves an order of magnitude improvement in training speed compared to baseline methods. Reviewers raised some concerns (e.g. empirical comparison against adjoint methods in the multi agent example) or asked for clarifications (e.g. details of time sampling of the data). The authors adequately addressed most of these concerns via rebuttal response as well as revising the initial submission. At the end, all reviewers recommended for accept based on contributions of this work on improving training speed of ODE Networks. R4 hopes that some of the additional concerns that are not yet reflected in the current revision, be addressed in the camera ready version. 
In this paper, the authors generalize the univariate Shapley method to bivariate Shapley method. The authors first build a directly graph based on the asymmetric bivariate Shapley value (adding feature j to all sets contained feature i). Then several graph algorithms are applied to analyze the directly graph to derive (1) univariate feature importance available in univariate approach and (2) relations like mutually redundancy only available in bivariate approaches. Experiments on several datasets with comparison to existing methods demonstrated the superiority of the proposed method.  All reviews are positive.
The paper develops optimization algorithms for fitting structured neural networks. It focuses on the manifold identification property, which guarantees after finitely many iterations, all iterates have the same sparsity structure as at convergence. The proposed method extends dual averaging to include momentum. The paper’s analysis shows that if the proposed method converges, it converges to a stationary point, and identifies the sparsity pattern of the limit in finitely many iterations. Experiments show improvements in sparsification compared to existing two step sparsifiers, without a degradation in accuracy.   The initial review raised concerns about clarity, as well as some of the claimed significance of the results: the paper does not prove that an optimal, or even good sparsity structure is obtained — rather, it proves that the sparsity structure at convergence is obtained after finitely many iterations. The reviewers also raised a number of detailed concerns about the paper’s mathematical exposition.   After considering the authors response, and a revision which significantly clarified both the paper’s notation and its main claims, the reviewers converged to a recommendation to accept. The paper provides a principled approach to sparsification, with supporting theory (albeit about finite identification, rather than optimality). The proposed algorithm appears quite practical and is supported by experiments demonstrating improvements over existing sparsification methods.
The paper proposes a novel ensemble method, CDA^2,  in which base models collaborate to defend against adversarial attacks. To do so the base models have two heads: the label head for predicting the label and the posterior probability density (PPD) head that is trained by minimizing binary cross entropy between it and the true label logit given by the label head. During inference the base model with the highest PPD value is chosen to make the prediction. During training base models learn from the adversarial examples produced by other base models.   The evaluation of the manuscript of different reviewers was very diverse, resulting in final scores ranging between 3 and 8 after the discussion period. While the rebuttal clearly addressed the concerns of one reviewer and several additional experimental results were added for different adversarial attacks, it did not fully addressed the concerns of another reviewer, who rated his confidence higher. He was also not convinced by the update in the revised version of the manuscript, in which crucial changes in the pseudocode describing the proposed algorithm were made, which contradicted some statements in the first version. Therefore, the paper can unfortunately not be accepted in its current version. In a future version of the manuscript, the description of the algorithm and of he role of the PPD head should be improved and experiments on another dataset next to CIFAR 10 could be added.
The paper aims to clean data samples with label noise in the training procedure.   The reviewers and AC note the following potential weaknesses: (1) the assumption of uniform noise, which is not the case in practice, (2) marginal gains under real world datasets and (3) highly empirical and ad hoc approach.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more significant works to publish the work.  
This paper examines a concept (also coined by the paper) of "search discrepancies" where the search algorithm behaves differently with large beam sizes. It then proposes heuristics to help prevent the model from performing worse when the size of the beam is increased.  I think there are some interesting insights in this paper with respect to how search works in modern neural models, but most reviewers (and me) were concerned by the heuristic approach taken to fix these errors. I still think that within a search paper, a clear separation between modeling errors and search errors is useful, and adding heuristics on top has a potential to making things more complicated down the road when, for example, we change our model or we change our training algorithm.  It would be nice if the nice insights in the paper could be turned into a more theoretically clean framework that could be re submitted to a future conference.
This paper proposed Q value weighted regression approach for improving the sample efficiency of DRL. It is related to recent papers on advantage weighted regression methods for RL. The approach is interesting, intuitive, and bears merits. Developing a simple yet sample efficient algorithm using weighted regression would be a critical contribution to the field. The work has the potential to make an impact, if it has all the necessary ingredients of a strong paper.  However, reviewers raised a few issues that have to be addressed before the paper can be accepted. As some reviewers pointed out, there seem to be unaddressed major issues from previous submissions. Novelty appears limited, especially because the proposed approach is very similar to recent works (e.g., AWR). The experiment section lacks comparison to recent similar algorithms, and the available comparisons appear to be not strong enough to justify merits of the proposed algorithm. Theorem 1 requires an unrealistic state determines action assumption for the replay buffer. Although the authors made an effort to justify this assumption, it remains very problematic and rules out most randomized/exploration algorithms.
Authors extend the probabilistic PCA framework to multinomial distributed data. Scalable estimation of principal components in the model is achieved using a multinomial variational autoencoder in combination with an isometric log ratio (ILR) transform. The reviewers did not agree on the degree of novelty of the paper to PC estimation. The presentation of the paper can be improved. The reviewers criticise that large changes have been made to the paper during the rebuttal phase. Overall, the paper is borderline and due to the mentioned large changes I recommend a rejection (and re review at a different venue). 
The reviewers agree that this is an interesting treatise on some relationships between SGD fine tuning and evolutionary algorithms. All reviewers have requested some experimental validation or demonstration of the theory developed in this paper, which is not currently included. Whilst the computational requirements (and time required) may be long, this will significantly assist the many readers of the paper and save them from having to run such an experiment many times themselves.  The reviewers provided a number of suggestions of how this might be done. The reviewers also highlighted a number of specific improvements that can be made to the writing of the paper.
This paper proposes Search with Amortized Value Estimates (SAVE) that combines Q learning and MCTS.  SAVE uses the estimated Q values obtained by MCTS at the root node to update the value network, and uses the learned value function to guide MCTS.  The rebuttal addressed the reviewers’ concerns, and they are now all positive about the paper. I recommend acceptance.
The authors construct a weighted objective that subsumes many of the existing approaches for sequence prediction, such as MLE, RAML, and entropy regularized policy optimization. By dynamically tuning the weights in the objective, they show improved performance across several tasks.  Although there were no major issues with the paper, reviewers generally felt that the technical contribution is fairly incremental and the empirical improvements are limited. Given the large number of high quality submissions this year, I am recommending rejection for this submission.
This paper analyses the interaction between data augmentation strategies  and model ensembles with regards to calibration performance. The authors note how strategies such as mixup and label smoothing, which reduce a single model s over confidence, lead to degradation in calibration performance when such models are combined as an ensemble. They propose a simple solution. The paper merits publication.
The authors propose a novel distributed reinforcement learning algorithm that includes 3 new components: a target network for the policy for stability, a circular buffer, and truncated importance sampling. The authors demonstrate that this improves performance while decreasing wall clock training time.  Initially, reviewers were concerned about the fairness of hyper parameter tuning, the baseline implementation of algorithms, and the limited set of experiments done on the Atari games. After the author response, reviewers were satisfied with all 3 of those issues.  I may have missed it, but I did not see that code was being released with this paper. I think it would greatly increase the impact of the paper at the authors release source code, so I strongly encourage them to do so.  Generally, all the reviewers were in consensus that this is an interesting paper and I recommend acceptance.
This paper generated a lot of discussion (not all of it visible to the authors or the public).   R1 initially requested reasonable comparisons, but after the authors provided a response (and new results), R1 continued to recommend rejecting the paper simply because they personally did not find the manuscript insightful. Despite several requests for clarification, we could not converge on a specific problem with the manuscript. Ungrounded gut feelings are not grounds for rejection.   After an extensive discussion, R2 and R3 both recommend accepting the paper and the AC agrees. Paper makes interesting contributions and will be a welcome addition to the literature. 
The authors propose a reference less metric for evaluating NLG systems by training a discriminator which distinguishes between human generated and machine generated text.   The main concerns raised by the reviewers were (i) lack of clarity in certain portions of the paper (ii) lack of demonstration of the "universal" applicability of the proposed metric (only evaluated for poetry generation) (iii) lack of clear guidelines on how to use the proposed metric in a reproducible manner (iv) lack of details about what exactly does the proposed metric capture and look for in the generated text.  The authors did not respond to the specific queries of the reviewer and agreed that more work is needed on their part.
The reviewers found the paper meaningful but noted that they were not convinced by the experiments as they stand and the presentation was dense for them.
The paper proposes a method to improve explanation by example by identifying important parts of the image when using nearest engihbor explanations by example. Towards this goal, the notion of Critical Classification Regions (CCR) is proposed. The method is tested both computationally and a user study.  The reviewers felt that the paper had interesting ideas, but overall the reviewers agreed that the paper needs more work before being ready for publication: this includes improving the soundness of the empirical evaluations and clarifying the contribution of the paper.
The reviewers point our concerns regarding paper s novelty, theoretical soundness, and empirical strength. The authors provided to clarifications to the reviewers.
This paper describes a method for learning compact RL policies suitable for mobile robotic applications with limited storage.  The proposed pipeline is a scalable combination of efficient neural architecture search (ENAS) and evolution strategies (ES).  Empirical evaluations are conducted on various OpenAI Gym and quadruped locomotion tasks, producing policies with as little as 10s of weight parameters, and significantly increased compression reward trade offs are obtained relative to some existing compact policies.  Although reviewers appreciated certain aspects of this paper, after the rebuttal period there was no strong support for acceptance and several unsettled points were expressed.  For example, multiple reviewers felt that additional baseline comparisons were warranted to better calibrate performance, e.g., random coloring, wider range of generic compression methods, classic architecture search methods, etc.  Moreover, one reviewer remained concerned that the scope of this work was limited to very tiny model sizes whereby, at least in many cases, running the uncompressed model might be adequate.
This work proposes a simple but useful way to train RNN with binary / ternary weights for improving memory and power efficiency. The paper presented a sequence of experiments on various benchmarks and demonstrated significant improvement on memory size  with only minor decrease of accuracy. Authors  rebuttal addressed the reviewers  concern nicely.   
The submission received split reviews: two reviewers recommended weak accepts, and the other two weak rejects.  The AC went through the reviews, responses, and discussions carefully.  The AC agrees that this paper is well written and has demonstrated the possibility of using transformers for particle based physical simulation.  The AC also believes that the authors have addressed the concerns of reviewer dYcg, despite that the reviewer didn t engage in the discussions.  The contributions are however not most exciting, and none of the reviewers would like to champion the submission.  Further, the AC agrees with the knowledgeable and responsible reviewer ZsKn that the presentation and experiments can be better positioned to highlight the key contribution. As reviewer ZsKn has summarized, it s recommended that "the authors took the approach of integrating the different parts of the newly proposed layer into existing architectures (possibly including non simulation settings), and try to understand better that way how the new layer may help in a more apples to apples comparison."  The recommendation is reject, and the authors are encouraged to revise the paper for the next venue.
The paper extends an existing approach to imitation learning, GAIL (Generative Adversarial Imitation Learning, based on an adversarial approach where a policy learner competes with a discriminator) in several ways and demonstrates that the resulting approach can learn in settings with high dimensional observation spaces, even with a very low dimensional discriminator. Empirical results show promising performance on a (simulated) robotics block stacking task, as well as a standard benchmark   Walker2D (DeepMind control suite).  The reviewers and the AC note several potential weaknesses. Most importantly, the contributions of the paper are "muddled" (R2). The authors introduce several modifications to their baseline, GAIL, and show empirical improvements over the baseline. However, the presented experiments do systematically identify which modifications have what impact on the empirical results. For example, R2 mentions this for figure 4, where it appears on first look that the proposed approach is compared to the vanilla GAIL baseline   however, there appear to be differences from vanilla GAIL, e.g., in terms of reward structure (and possibly other modeling choices   how close is the GAIL implementation used to the original method, e.g., in terms of the policy learner and discriminator)? There is also confusion on which setting is addressed in which part of the paper, given that there is both a "RL+IL" and an "imitation only" component.  In their rebuttal, the authors respond to, and clarify some of the questions raised by the reviewers, but the AC and corresponding reviewers consider many issues to remain unclear. Overall, the presentation could be much improved by indicating, for each set of experiments, what research question or hypothesis it is designed to address, and to clearly indicate conclusions on each question once the results have been discussed. In its current state, the paper reads as a list of interesting and potentially highly valuable ideas, together with a list of empirical results. The real value of the paper should come in when these are synthesized into lessons learned, e.g., why specific results are observed and what novel insights they afford the reader. Overall, the paper will benefit from a thorough revision and is not considered ready for publication at ICLR at this stage.  The AC notes that they placed less weight on R3 s assessment, due to their relatively low confidence, because they appear not to be familiar with key related work (GAIL), and did not respond to further requests for comments in the discussion phase.  The AC also notes a potential weakness that was not brought up by the reviewers, and which they therefore did not weigh into their assessment of the paper, but nevertheless want to share to hopefully help improve a future version of the paper. Figure 6(b) should be interpreted with caution given that performance with a greater number of demonstrations (120 vs 60) showed lower performance. The authors note in the caption that one of the "120 demos" runs "failed to take of". This suggests that variance for all these runs may be underestimated with the currently used number of seeds. It is not clear what the shaded region indicates (another drawback) but if I interpret these as standard errors then this plot would suggest lower performance for higher numbers of demonstrations with some confidence   clearly that conclusion is unlikely to be correct.
The reviewers all outlined concerns regarding novelty and the maturity of this work. It would be helpful to clarify the relation to doubly stochastic kernel machines as opposed to random kitchen sinks, and to provide more insight into how this stochasticity helps. Finally, the approach should be tried on more difficult image datasets.
Four reviewers have reviewed this paper. After rebuttal, the reviewers  recommendations were borderline. Rev. 4 remains concerned about relation of second order approaches in CV and second order filters. Indeed, there exists a connection although it is perhaps subtle in its nature and equally concerning is the connection with general Polynomial filters in many GCN papers. As other reviewers point out, MixHop and Jumping Knowledge also allow multi hop designs. More importantly, APPNP and SGC networks allow multiple hops. From that point of view, the proposed approach is rather a recap of existing observations and contributions. Finally, even Rev. 2 has indicated that the paper is perhaps  average  after checking with comments of other reviewers. Therefore, at this point, the paper is slightly below the acceptance threshold. 
Inspired by WaveGAN, this paper proposes a PUGAN to synthesizes high quality audio in a raw waveform. The paper is well motivated. But all the reviewers find that the paper is lack of clarity and details, and there are some problems in the experiments.
This work provides interesting insights on the transferability of adversarial perturbations and proposes ways of making it more effective. While several reviewers have found parts of the paper unsatisfactory, there are interesting results to merit acceptance.
The paper got mixed reviews ranging from 5 to 7. The main concerns of the reviewers were the missing novelty as the paper combines different well known methods for a given problem, so there is no big algorithmic contribution. The presented pipeline for closed loop grasping using imitation learning from a planner, Dagger and subsequent deep RL with TD3 is a straightforward, but sound and intuitive combination of algorithms to address the problem of closed loop grasping.  The presented results and ablation studies also motivate these algorithmic choices. In the rebuttal the authors addressed most concerns regarding the experiments (missing comparisons to open loop grasping and real world experiments), but more real world experiments would be necessary to evaluate the effectiveness of the approach.    This is a borderline paper were  I unfortunately have to recommend rejection due to the missing algorithmic contribution, a major requirement for ICLR. The paper would  however fit very well to a robotics conference and the authors are  encouraged to resubmit the paper the venues such as RSS or CoRL.  
This paper is proposed to address the few shot image classification with the help of two newly designed losses. The first loss function is the Proto Triplet Loss based on the revision of conventional triplet loss. Another loss function is based on an inter  and intra class nearest neighbors score to estimate the quality of embeddings. The proposed method has shown its superiority over the baselines on the miniImagenNet benchmark. The major concern of this paper is the novelty that both proposed techniques are not new. Moreover, the baselines are not the SOTAs, and the evaluations on miniImagenNet only are not comprehensive enough. In addition, the authors have not provided any rebuttal to address the reviewers  concerns.
Overall, the paper provides interesting counter examples for the SGD with constant step size (that relies on a relative noise model that diminishes at the critical points), which provide critical (counter) insights into what we consider as good convergence metrics, such as expected norm of the gradient.   The initial submission took a controversial position between the mathematical statements and the presentation of the statements on the behavior of the SGD method in non convex optimization problems. While the mathematical is sufficient for acceptance at ICLR, the presentation was inferring conclusions that could have been misread by the community.  I am really happy to state that the review as well as the rebuttal processes helped improved the presentation of the results that I am excited to recommend acceptance.
This paper presents an approach for conformal prediction where, in its standard paradigm, a set of prediction candidates is identified as opposed to a single one.  The authors advance the CP framework by presenting a rigorous methods that allows for a smaller set of admissable predictions with a covergae quarantee. Their further contribution is a methodolgy based on cascading that filters out non promising candidates.  After the discussion period, _all_ the reviewers are in favour of accepting the manuscript with the average being marginally above acceptance.  My recommendation is therefore to accept the paper.   Strong points: The advance of a smaller set of admissible predictions in the CP framewrok is quite useful especially in scenaria where the set can grow (expensively) large.  Thorough experimental analysis with good presentation of performance gain and usefulness in real world data.   Weak points: Lack of novelty in the techniques make the work a weaker candidate compared to the rest of submissions. 
This paper presents an empirical study of the applicability of genetic algorithms to deep RL problems. Major concerns of the paper include: 1. paper organization, especially the presentation of the results, is hard to follow; 2. the results are not strong enough to support that claims made in this paper, as GAs are currently not strong enough when compared to the SOTA RL algorithms; 3. Not quite clear why or when GAs are better than RL or ES; Lack of insights. Overall, this paper cannot be accepted yet.  
The paper proposes a sensitivity based pruning method at initialization. For fully connection and and convolutional neural networks, it shows that the model is trainable only when the initialization satisfies Edge of Chaos (EOC). The paper also provided a rescaling method so that the pruned network is initialized on the EOC. For Resnet, the paper shows that the proposed pruning satisfies the EOC condition by default and further provides re parameterization method to tackle exploding gradients. The experiments show the performance of the proposed method on fully connected and convolution neural network, as well as ResNet. There were some concerns about the contribution of the paper compared to that of [1]. I read the two papers carefully and while both papers aim at addressing a similar problem, i.e., pruning at initialization while avoiding layer collapse, the paper provides a different perspective on the problem, and provides enough theoretical contribution and insights to be found helpful and interesting by the community.  
This paper proposes a curriculum based reinforcement learning approach to improve theorem proving towards longer proofs. While the authors are tackling an important problem, and their method appears to work on the environment it was tested in, the reviewers found the experimental section too narrow and not convincing enough. In particular, the authors are encouraged to apply their methods to more complex domains beyond Robinson arithmetic. It would also be helpful to get a more in depth analysis of the role of the curriculum. The discussion period did not lead to improvements in the reviewers’ scores, hence I recommend that this paper is rejected at this time.  
The paper proposes a novel mechanism to reduce the skewness of the activations. The paper evaluates their claims on the CIFAR 10 and Tiny Imagenet dataset. The reviewers found the scale of the experiments to be too limited to support the claims. Thus we recommend the paper be improved by considering larger datasets such as the full Imagenet. The paper should also better motivate the goal of reducing skewness.
This is a borderline paper with some reviewers voted for acceptance and some think it is not still ready. What is clear is more efforts by the authors is needed to make the paper appealing to reviewers with different interests. Changes such as better writing, more in depth literature review, more convincing experiments can definitely improve the quality of the paper. I personally do not think regret analysis is needed for this work, but it was mentioned by a reviewer. I would suggest the authors to use the reviewers  comments, revise their work, and prepare it for future conferences.
The paper proposes a metric to measure the difficulty of training examples. The main thesis is that hard training examples lead to bad test adversarial error. There are theoretical results on simple models establishing such claims. The paper also proposes a method to adaptively weight training examples to improve training which gives improvement for adversarial error.   The reviewers have raised a number of questions and the rebuttal period has been useful. In particularly, I agree with the reviewers that  model agnostic  is misleading in this context and the authors have agreed to remove this in the future. It is felt that more experiments, comparison to adversarial training, etc. is needed and I think the paper will need to go through a proper review process again before acceptance.
While one reviewer did upgrade their Rating from 6 to 7, the most negative reviewer maintains: "Overall, I find this work interesting and current results surprising. However, I find it to be a preliminary work and not yet ready for publication. The paper still lacks a conclusion / a leading hypothesis / an explanation for the shown results. I find this conclusion indispensable even for a small scientific study to be published." after the rebuttal. With scores of 7 5 4 it is just not possible for the AC to recommend acceptance.
Most of the reviewers thought this paper has issues where it could be improved.  There was a range of concerns. Most importantly, several reviewers felt the novelty in the paper was unclear as well as the requirement for more details in the experimental evaluations.
The reviewers unanimously raised concerns over the clarity and technical correctness of the theory and the Imagenet experiments during the first round. The authors submitted a highly revised version during the rebuttal which allayed concerns for multiple reviewers, however all the reviewers raised the concern that the paper has gone through a very significant change, almost becoming "a new paper" in the rebuttal phase and should go through another cycle of resubmission and review to correctly judge the contributions and claims.  The reviewers were aligned in their judgement that the original manuscript with minor changes was not ready for publication. The authors are strongly encouraged to polish the version submitted during rebuttal and resubmit.  
The authors make an empirical study of the "dimension" of a neural net optimization problem, where the "dimension" is defined by the minimal random linear parameter subspace dimension where a (near) solution to the problem is likely to be found.   I agree with reviewers that in light of the authors  revisions, the results are interesting enough to be presented at the conference.
This paper examines ways of encoding structured input such as source code or parsed natural language into representations that are conducive for summarization. Specifically, the innovation is to not use only a sequence model, nor only a tree model, but both. Empirical evaluation is extensive, and it is exhaustively demonstrated that combining both models provides the best results.  The major perceived issue of the paper is the lack of methodological novelty, which the authors acknowledge. In addition, there are other existing graph based architectures that have not been compared to.  However, given that the experimental results are informative and convincing, I think that the paper is a reasonable candidate to be accepted to the conference.
Thank you for your submission. The reviewers agree that this paper provides new contributions to data privacy. In particular, the proposed definition interpolates between the local differential privacy and shuffled differential privacy definitions. As argued in the paper, mechanisms under this framework can prevent certain inferential attacks based on the relationships across the individuals (e.g., which individuals belong to the same household). The paper also provides good evidence that their mechanism guards against a specific type of inferential attacks and provides stronger utility than mechanisms based on uniform shuffling.
The paper makes a good theoretical contribution by formulating the GAN training as primal dual subgradient method for convex optimization and providing convergence proof. The authors then propose a modified objective to standard GAN training, based on this formulation, that helps address the mode collapse issue. One weak point of the paper as pointed out by reviewers is that that the experimental results are underwhelming and the approach may not scale well to high dimensional datasets / high resolution images. Interestingly, the proposed approach is general enough to be applied to other GAN variants that may address this issue in future. I recommend acceptance.
This paper proposes to estimate the predecessor state dynamics for more sample efficient imitation learning. While backward models have been used in the past in reinforcement learning, the application to imitation learning has not been previously studied. The paper is well written and the results are good, showing clear improvements over GAIL in the presented experiments. The primary weakness of the paper is the lack of comparisons to the baselines suggested by reviewer 1 (a jumpy forward model and a single step predecessor model) to fully evaluate the contribution, and to SAIL and AIRL. Despite these weaknesses, the paper slightly exceeds the bar for acceptance at ICLR. The authors are strongly encouraged to include these comparisons in the final version.
This paper proposes a method for finding the action space in reinforcement learning problems, characterizing the search space into dispensable and indispensable actions through a Monte Carlo approximation.  Reviewers are unanimous that the paper is not fit for publication at this stage. While it tackles an interesting problem and seems to be novel, the presentation leaves much to be desired; this area chair also had a hard time figuring out how the different parts of the paper fit together. Additionally, the use of a unique problem makes it hard to judge the contribution of the algorithm.
The presented work has worse accuracy than existing (and not all the baselines are given correctly) and does not provide the running time comparison. All reviewers recommend rejection, and I am with them.
The authors present a Siamese neural net architecture for learning similarities among field data generated by numerical simulations of partial differential equations. The goal would be to find which two field data are more similar to each. One use case mentioned is the debugging of new numerical simulators, by comparing them with existing ones.   The reviewers had mixed opinions on the paper. I agree with a negative comment of all three reviewers that the paper lacks a bit on the originality of the technique and the justification of the new loss proposed, as well as the fact that no strong explicit real world use case was given. I find this problematic especially given that similarities of solutions to PDEs is not a mainstream topic of the conference. Hence a good real world example use of the method would be more convincing.
This work is attempting to develop a new way to train models that are robust to (l_p bounded) adversarial perturbations and to do so in a way that departs from the tools successfully used for this purpose in the past. This is a worthwhile aspiration, however, as pointed out in the comments/reviews, there are significant problems with the methodology of evaluating the proposed approach (and some well founded skepticism that this approach is indeed successful). As such, this paper cannot be accepted in its current form.  
Three experts reviewed the paper. Two reviewers recommended acceptance as they liked that the work identified a legacy design in object detection networks and resolved it by a new module. All reviewers found the empirical results strong. Reviewer MDN5 recommended rejection main for the concern that this newly designed module is a standard exploitation of network architectures. AC sided with the positive reviewers because of the paper s identification of a legacy design in object detection and the strong experimental results. Hence, the decision is to recommend the paper for acceptance. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
All reviewers agree that the proposed method is novel and experiments do a good job in establishing its value for few shot learning. Most the concerns raised by the reviewers on experimental protocols have been addressed in the author response and revised version.
Good contribution. There was a (heated) debate over this paper but the authors stayed calm and patiently addressed all comments and supplied additional evaluations, etc. 
Unfortunately, this paper fell just below the bar for acceptance.  The reviewers all saw significant promise in this work, stating that it is intriguing, "novel and provides an interesting solution to a challenging problem" and that "many interesting use cases are clear".  AnonReviewer2 particularly argued for acceptance, arguing that the proposed approach provides a very flexible method for incorporating constraints in neural network training.  A concern of AnonReviewer2 was that there was no guarantee that this loss would be convex or converge to an optimum while statisfying the constraints.  The other two reviewers unfortunately felt that while the proposed approach was "interesting", "promising" and "intriguing", the quality of the paper, in terms of exposition, was too low to justify acceptance.  Arguably, it seems the writing doesn t do the idea justice in this case and the paper would ultimately be significantly more impactful if it was carefully rewritten.  
Many problems in machine learning rely on multi task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. In this work, authors formalize notions of task level privacy for MTL via joint differential privacy (JDP). They propose an algorithm for mean regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. Then analyze objective and solver, providing certifiable guarantees on both privacy and utility. The main results, namely the convergence rate results, are hard to parse and hard to interpret. For example, as one reviewer pointed out, it is bounded below by a constant which is not properly explained. Further, comparisons to the literature in user level privacy (which is equivalent as the task level privacy) is not provided enough. Significant improvement in the presentation of the main results, along with an interpretable explanation of the contribution, is necessary for this manuscript.
This paper proposes the distributed Skellam mechanism for differentially private federated learning that relies on secure aggregation. Since multi party computation protocols rely on finite precision, the Skellam distribution meets the criteria of closure under addition and discreteness. During the discussion, the reviewers raised a concern that the proposed idea is highly similar to a recently published NeurIPS paper (https://arxiv.org/abs/2110.04995). However, since the timelines of the two papers are close, they can be considered concurrent work. Both results advance the study of private federated learning that leverages secure aggregation techniques. Through a more in depth comparison, the current paper also provides sufficiently different proof techniques than the ones in the concurrent paper. The authors should provide an extensive comparison between their work and the NeurIPS paper in their next revision.   While the paper provides new results, there are several concerns in the reviews. First, even though the proof techniques are different from concurrent work, the reviewers still think that the main technique of Skellam perturbation has limited novelty. Second, the presented experiments also appeared to be quite weak. For example, the accuracy on MNIST is much lower than simple baselines in earlier work. While the authors tried to justify this reduction of accuracy through their decentralized training setting, the argument is not fully convincing. In particular, even though the noise addition is done in a decentralized fashion, the proposed algorithm is still subject to the same (standard) differential privacy constraint (as opposed to local differential privacy). The authors could consider improving the experiments or providing a more principled justification for the reduction of accuracy in their algorithm. Due to these issues, the paper does not clear the bar for acceptance at ICLR.
This paper introduces neural attention distillation; a new scheme for erasing backdoors in a poisoned neural network. The paper performs an empirical evaluation of their proposed method against  6 state of the art backdoor attacks. The authors show that attention distillation succeeds by using only a small fraction of clean training data without any performance degradation. In addition, the authors have provided ablation studies to clarify the contribution of each component in their proposed approach. Reviewers find the simplicity and effectiveness of the approach an important attribute that may lead this work to have a high impact in the field. The paper is well written, and all reviewers rate it on the accept side. I concur with their opinions and comments and I recommend accept.
This paper offers new ideas about the key question of how to extend modern Transformer architectures to solve problems that require more reasoning steps than the model can implement in a single step forward pass. Reviewers were unanimous that the problem is important, and that the paper is a step in a promising direction. However, reviewers were also unanimous that the proposed experiments are too narrow to be the basis for any confident new claims in this area and that, in addition, the experimental design has a confound that makes it difficult to interpret, even after the addition of a new condition during discussion.
The reviewers all generally find the paper both well motivated in addressing an important challenge as well as well written. However, there s quite a bit of hesitation around whether the proposed metric is convincing enough as an approach to measure local calibration.  Reviewer 76PS and 784d s concerns around the choice of feature map and associated hyperparameters remain unaddressed, and I agree with their concern. There is no clear understanding of what constitutes a "good" feature map, which makes the metric quite difficult to use whether as a benchmark of ML methods or for general application.  I recommend the authors use the reviewers  feedback to enhance their preprint should they aim to submit to a later venue.
Important problem (making NN more transparent); reasonable approach for identifying which linguistic concepts different neurons are sensitive to; rigorous experiments. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance.
All reviewers agree that the current approach is very similar to traditional uncertainty based active learning, and that the empirical results are inconclusive, so at this point the paper is not ready for publication.
This paper has been evaluated by four expert reviewers resulting in two rejections one marginal score and one acceptance recommendation. The authors provided rebuttals to the critiques, but they did not sway the reviewers  assessments. The prevailing impression is that the work is interesting but perhaps not yet mature nor organized enough to benefit the ICLR audience in its current form. There is also some vagueness left at the conceptual level, e.g. regarding the actual objectives   some reviewers pointed out confusing entanglement of the concepts of simplicity and interpretability. Nonetheless, the paper presents an interesting work that will benefit from incorporating the constructive feedback received here 
This paper suggests using RNN and policy gradient methods for improving symbolic regression. The reviewers could not reach a consensus, and due to concerns about the clarity of the paper and the extensiveness of the experimental results, the paper does not appear to currently meet the level of publication.   Also, while not mentioned in the reviews, there appears to be some work on symbolic regression aided by deep learning, (see for example, https://twhughes.github.io/pdfs/cs221_final.pdf, which was found by searching "symbolic regression deep learning") I would thus also recommend the authors do a more thorough literature search for future revisions. 
This paper studies model based RL in the setting where the model can be misspecified. In this case, MLE of model parameters is a not necessarily a good idea because the error in the model estimate compounds when the model is used for planning. The authors solve this problem by optimizing a novel objective, which takes the quality of the next state prediction into account.  This paper studies an important problem and this was recognized by all reviewers. Its initial reviews were positive and improved to 8, 8, 6, and 6 after the rebuttal. The rebuttal was comprehensive and exemplary. For instance, one concern of this paper was limited empirical evaluation. The authors added 5 new benchmarks and also included stabilizing improvements in their original baselines. I strongly support acceptance of this paper.
This paper proposed an MCMC sampler that combines HMC and neural network based proposal distribution. It is an improvement over L2HMC and [Titsias & Dellaportas, 2019], with the major innovation being that, the proposed normalizing flow based proposal is engineered such that the density of the proposal $q(x |x)$ is tractable. Experiments are conducted on synthetic distributions, Bayesian logistic regression and deep energy based model training.  While reviewers are overall happy about the novelty of the approach, some clarity issues have been raised in some of the reviewers  initial reviews. Also concerns on the evaluation settings, including the missing evaluation metric such as ESS/second, are also raised by the reviewers. The revision addressed some of the clarity issues, but some experimental evaluation issues still exist (e.g. comparing with L2HMC in terms of ESS/second), and the replaced MALA baseline results make the improvement of the proposed approach less clear.  I personally find the proposed approach as a very interesting concept. However I also agree with the reviewers that more experimental studies need to be done in order to understand the real gain of the approach. 
This paper introduces set transformer for set inputs. The idea is built upon the transformer and introduces the attention mechanism. Major concerns on novelty were raised by the reviewers. 
The idea of using multiple sparse matrices seems to be new, but the novelty of the idea alone isn t enough to convince the AC and reviewers (indeed, the idea might not be new, but has never been discussed in literature because of the drawbacks we discuss here). As the authors and reviewers/AC seem to agree, the actual benefits of sparse matrix multiplies are hard to realize, especially on embedded devices, so the contributions at this point are mainly hypothetical and only about the new idea. Each reviewer brought up issues (even the most positive reviewer) and mostly the reviewers were not persuaded by the rebuttal. In short, there wasn t evidence that this new idea could really contribute to the state of the art.  This is now a fairly crowded topic (e.g., all the  papers brought up by R3 in just that one class of methods), and new papers should beat state of the art and/or introduce new theory   an example would be a paper from last year s ICLR, https://openreview.net/forum?id HJfwJ2A5KX , "Data Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds" (Baykal, Liebenwein, Gilitschenski, Feldman, Ru) which not only gives an efficient technique (not based on sparsity of weights) but also gives types of generalization guarantees.  As R1 said, the results are not state of the art, and we have to believe the authors that "an iterative like extension of our method could reach even better results". The rebuttal says that the paper s goal is to "pioneer a new approach to neural network compression". But if you can get better results with something better than Palm4MSA, then please do so, and demonstrate the evidence!  Right now, the paper assumes we could implement sparse multiplication efficiently on embedded devices, and assumes we could get better results: both these are quite hypothetical. The AC encourages a resubmission of this paper after these results have been addressed.
The paper describes a new model based RL technique for constrained MDPs based on Bayesian world models.  It improves sample efficiency and safety. The reviewers are unanimous in their recommendation for acceptance.  This represents an important advance in RL.  Great work!
The paper presents a generative model of sequences based on the VAE framework, where the generative model is given by CNN with causal and dilated connections.   Novelty of the method is limited; it mainly consists of bringing together the idea of causal and dilated convolutions and the VAE framework. However, knowing how well this performs is valuable the community.  The proposed method appears to have significant benefits, as shown in experiments. The result on MNIST is, however, so strong that it seems incorrect; more digging into this result, or sourcecode, would have been better.
This work proposes a CNN architecture for joint depth and camera motion estimation from videos. The paper presents a differentiable formulation of the problem to allow its end to end learning, and the reviewers unanimously find the proposed approach reasonable and agree that this is a solid paper. Some of the reviewers find the method itself to be too mechanical, but they all agree that this is a well engineered solution.
The paper tackles a very interesting problem in the context of diffusion based generative models and provides empirical improvements. Pre rebuttal, reviewers  main concerns lie in the motivation and clarification of the method, while after rebuttal, all reviewers satisfied the response and gave positive scores. The authors should include the additional results to well address the reviewers  concerns in the final version.
This work introduces a reward shaping scheme for multi agent settings based on the TD error of other agents.   Overall, reviewers were positive about the direction and the presentation but had a variety of concerns and questions and felt more experiments were necessary to validate the claims of flexibility and scalability, with results more comparable to the scale of the contemporary multi agent literature. One note in particular: a feed forward Q network is used in a partially observable environment, which the authors seemed to dismiss in their rebuttal. I agree with the reviewer that this is an important consideration when comparing to baselines which were developed with recurrent networks in mind.  A revised manuscript addressed concerns with the presentation but did not introduce new results or plots, and reviewers were not convinced to alter their evaluation. There is agreement that this is an interesting paper, so I recommend that the authors conduct a more thorough empirical evaluation and submit to another venue.
The paper proposes a method for learning connectivity in neural networks, evaluated on the ResNeXt architecture. The novelty of the method is rather limited, and even though the method has been shown to improve on the ResNeXt baselines on CIFAR 100 and ImageNet classification tasks (which is encouraging), it should have been evaluated on more architectures and datasets to confirm its generality.
This paper develops a deep convolutional network with RNN layers and  a new data augmentation method for EEG motor imagery classification.  Reviewers agreed that the paper was not very clearly written, and that without comparisons to other related methods or at least demonstration  of the importance of each of the components of the model (through for  example ablation analyses), it was hard to understand the generality of the approach.  The authors did not respond to the reviews, so I am recommending not   accepting this paper.
The paper proposes a new neural network architecture for hyperspectral image reconstruction. The paper received borderline/negative reviews. Significant concerns were raised about the novelty and significance of the contribution. Unfortunately, the authors did not upload a rebuttal, preventing the reviewers from changing their opinion about the paper. There is therefore no reason to overturn their recommendation.
This paper proposes a potentially very interesting and original approach to handle label noise.  The numerical experiments suggest that the method works very well. But the paper  itself has been deemed very hard and demanding to read and understand for a general machine learning crowd and even by experts in the fields of optimal transport and  Markov theory.   Note that due to the low confidence in several review an additional emergency review by an expert was asked and it confirmed the global opinion from  other reviewers that the paper is interesting but needs a major rewriting before acceptance in a ML conference. The AC strongly suggest that the authors work on a more pedagogical introduction and explanation of the method before resubmitting.  
This submission is a continuation of a line of theoretical work that seeks to characterize optimization landscapes of neural networks by the presence or absence of spurious local minima.  As the number of critical points grows combinatorially for larger networks, it is very challenging to show such results.  The present submission extends slightly previous work by considering two hidden units and their proof technique goes beyond that of Brutzkus and Globerson, 2017, potentially leading to more interesting results if they can be extended to more complex networks.  The setting of two hidden units is quite limited   far from any practical setting.  If this were the stepping stone to proving optimality of certain optimization strategies for more complex networks, this may be of some interest, but it seems doubtful.  One indication is given in Sec. 7 / Fig. 1 in which it is shown that for even quite small numbers of hidden units, spurious local optima do occur and are reached 40% of the time for random initializations even with only 11 nodes.  
This paper presents a mechanism for capsule networks to defend against adversarial examples, and a new attack, the reconstruction attack. The differing success of this attacks on capsnets and convnets is used to argue that capsnets find features that are more similar to what humans use.  Reviewers generally like the paper, but took instance with the strength of the claim (about the usefulness of the examples) and argued that the paper might not be as novel as it claims.  Still, this seems like a valuable contribution that should be published.
The paper proposes a method to accelerate training of an architectural hybrid of Transformers and CNNs: first train a CNN and then use the learned parameters to initialize a more general Transformed CNN (T CNN) model; subsequently continue training the T CNN.  Reviewers ratings are marginal, with three "marginally above threshold" and one "marginally below threshold".  However, no reviewer makes a compelling argument for acceptance, and all reviewers point to significant weaknesses in the work.  Reviewer ojmG: "novelty of the proposed method is limited" and "do not always reach the performance of end to end Transformers".  Reviewer Q4Pp: "experiments are very limited" and also (after rebuttal): "it would good to provide some experiments on a dataset different to ImageNet".  Reviewer ZjBY: "proposed model is not compared with many of the existing model architectures" and (after rebuttal): "would benefit from additional experimental analysis".  Reviewer zV42: "limited novelty prevents me from giving a higher rating".  In summary, while reviewer ratings span either side of above/below the acceptance threshold, the reviewer comments point to limited novelty and limited experimental impact.  Results appear not particularly surprising or significant: while the method provide some savings in training time, it does not seem to ultimately improve top accuracy on tasks and still lags behind the latest vision transformer architectures.  The author response did not substantially change reviewer opinion.  The AC has also taken a detailed look at the paper and does not believe the contribution to be of sufficient significance to warrant acceptance.
The reviewers agreed that the work addresses an important problem. There was disagreement as to the correctness of the arguments in the paper: one of these reviewers was eventually convinced. The other pointed out another two issue in their final post, but it seems that 1. the first is easily adopted and does not affect the correctness of the experiments and 2. the second was fixed in the second revision. Ideally these would be rechecked by the third reviewer, but ultimately the correctness of the work is the authors  responsibility.  Some related work (by McAllister) was pointed out late in the process. I encourage the authors to take this related work seriously in any revisions. It deserves more than two sentences.
This paper formalizes the concept of buffer zones, and proposes a defense method based on a combination of deep neural networks and simple image transformations. The authors argue that the proposed method based on buffer zones is robust against state of the art black box attacks methods.This paper, however, falls short of (1) unjustified claims (e.g., buffer zones are widened when the models are diverse); (2) incomplete literature survey and related work; (3) similar ideas are well known in the literature, (4) unfair experimental evaluations and many others. Even after author response, it still does not gather support from the reviewers. Thus I recommend reject.
The paper is about a topic that has been extensively studied for more than a decade, hence a very precise discussion of prior work as well as the new insights is absolutely necessary. Unfortunately both are lacking at this stage, thus the paper cannot be accepted.
This paper provides a novel method for calibrating probabilistic regression models without requiring a held out calibration set. The technical advances are interesting, and the experimental results look promising. The authors made a number of improvements based on the reviews, and the authors have done a good job with reproducibility of the experiments. Nevertheless, it ultimately does not meet the bar for acceptance. In revising the article for a future submission, I would encourage the authors to emphasize the observation in Section 4.3 that overall we want models that are both "well calibrated and sharp". This helps motivate the method and will help the reader interpret the results. 
The authors present a novel stable RL algorithm for the batch off policy setting, through the use of a learned prior.  Initially, reviewers had significant concerns about (1) reproducibility, (2) technical details, including the non negativity of the lagrange multiplier, (3) a lack of separation between performance contributions of ABM and MPO, (4) baseline comparisons.  The authors satisfactorily clarified points (1) (3) and the simulated baseline comparisons for (4) seem reasonable in light of how long the real robot experiments took, as reported by the authors.  Futhermore, the reviewers all agree on the contribution of the core ideas.  Thus, I recommend this paper for acceptance.
The paper proposes formulating safety constraints as formal language constrains, as a step toward bridging the gap between ML and software engineering, and enabling safe exploration in RL.  The authors responded and improved the paper significantly during the rebuttal period. Despite that, the reviewers raise the question, and I agree, that the significance of the paper, especially the novelty of the method, do not meet ICLR standard. The future version of the paper should be developed more in terms of the novelty, evaluations, and related works.  
The paper is interested in assessing the difficulty of popular few shot classification benchmarks (Omniglot and miniImageNet). A clustering based meta learning method is proposed (called Centroid Network), on which a metric is built (gap between the performance of Prototypical Networks and Centroid Networks). As noted by several reviewers, the proposed metric (critical for the paper) is however not motivated enough, nor convincing enough   after discussion, the logic in the metric reasoning seems to remain flawed. 
This paper has two main contributions. The first is that it proposes a specific framework for measuring catastrophic forgetting in deep neural networks that incorporates three application oriented constraints: (1) a low memory footprint, which implies that data from prior tasks cannot be retained; (2) causality, meaning that data from future tasks cannot be used in any way, including hyperparameter optimization and model selection; and (3) update complexity for new tasks that is moderate and also independent of the number of previously learned tasks, which precludes replay strategies. The second contribution is an extensive study of catastrophic forgetting, using different sequential learning tasks derived from 9 different datasets and examining 7 different models. The key conclusions from the study are that (1) permutation based tasks are comparatively easy and should not be relied on to measure catastrophic forgetting; (2) with the application oriented contraints in effect, all of the examined models suffer from catastrophic forgetting (a result that is contrary to a number of other recent papers); (3) elastic weight consolidation provides some protection against catastrophic forgetting for simple sequential learning tasks, but fails for more complex tasks; and (4) IMM is effective, but only if causality is violated in the selection of the IMM balancing parameter. The reviewer scores place this paper close to the decision boundary. The most negative reviewer (R2) had concerns about the novelty of the framework and its application oriented constraints. The authors contend that recent papers on catastrophic forgetting fail to apply these quite natural constraints, leading to the deceptive conclusion that catastrophic forgetting may not be as big of a problem as it once was. The AC read a number of the papers mentioned by the authors and agrees with them: these constraints have been, at least at times, ignored in the literature, and they shouldn t be ignored. The other two reviewers appreciated the scope and rigor of the empirical study. On the balance, the AC thinks this is an important contribution and that it should appear at ICLR.
Dear authors,  I took your concerns into account, and I also understand the whole crazy situation around the COVID 19. Many of the reviewers have families (e.g., in US, many kids are now homeschooled, and there are no good daycare solutions as well). I do not plan to list all the good parts of the paper and list weaknesses that are already mentioned and visible to you. Hence, let me focus on my concerns about this paper (and I hope you could find them interesting and they will help you to improve your paper).  + I personally find the use of 2nd order method in DNN a way to improve many inefficiencies of ADAM/SGD, .... and using diagonal scaling is one way to do it.     I personally find some sections not very motivated and explanatory. E.g. Section 3.2 is just telling half of the story and is missing some details to give the reader the full understanding.    The fact that B_t  is not necessary >0, it makes intermediate sense to use some kind of \max\{B_i, \sigma\}  to have the "scaling" to be $\succ 0$. Note that there are also SR1 methods that would guarantee the matrix to be not necessary pd.    Your main motivation was non convex problems, but the only theorem in the main paper was for convex loss only, right? In this case, I guess there is no issue with B_t to have some coordinates <0, right?  Overall, I find the topic interesting and would like to see an updated paper in some of the top ML venues, but right now I cannot recommend it for acceptance!  
The paper studies Positron Emission Tomography (PET) in medical imaging. The paper focuses on the challenges created by gamma ray photon scattering, that results in poor image quality. To tackle this problem and enhance the image quality, the paper suggests using generative adversarial networks. Unfortunately due to poor writing and severe language issues, none of the three reviewers were able to properly assess the paper [see the reviews for multiple examples of this]. In addition, in places, some important implementation details were missing.  The authors chose not to response to reviewers  concerns. In its current form, the submission cannot be well understood by people interested in reading the paper, so it needs to be improved and resubmitted. 
This paper received a majority voting of rejection. In the internal discussion, no reviewer would like to change the score according to the author response. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.  **Motivation**  The motivation of this paper is not strong. In this paper, the authors claimed that the fairness level of deep clustering methods is relatively poorly compared with the traditional fair clustering methods. The traditional fair clustering methods employ the hard constraints to achieve fairness by scarifying the cluster utility. Instead, deep fair clustering methods seek the trade off balance between fairness level and cluster utility; therefore, the deep fair clustering can be regarded to use the soft constraints. There is no necessary to compare two different fairness constraints. Even the proposed method is a trade off balance between fairness level and cluster utility.  **Self augmented Training**  The relationship between self augmented learning and fairness learning is unclear. I guess that the authors added this modular simply to enhance the cluster utility. However, such a loss or an operator can be also applied to other (fair) clustering algorithms. The experimental comparisons in Section 5 is unfair. No ablation study on this is provided.  **Novelty**  One reviewer pointed out there existed some work that plugs integer linear programming into a probabilistic discriminative clustering model proposed in 2017.   **Experiments**  (1) ScFC and DFCV release their codes; no results of these two methods were reported on HAR. (2) No standard deviation. (3) The Initial ILP Results (Ours) and Ours Result in Table 1 on HAR dataset is 0.653 and 0.468, both higher than the Ground Truth (Optimal) 0.458.  **Presentation**   A few statements are not well supported, or require small changes to be made correct.  No objection from reviewers was raised to again this recommendation.
This works improves the MixMatch semi supervised algorithm along the two directions of distribution alignment and augmentation anchoring, which together make the approach more data efficient than prior work. All reviewers agree that the impressive empirical results in the paper are its main strength, but express concern that the method is overly complicated and hacking together many known pieces, as well as doubt as to the extent of the contribution of the augmentation method itself, with requests for better augmentation controls. While some of these concerns have not been addressed by authors in their response, the strength of empirical results seems enough to justify an acceptance recommendation.
The authors propose a method for multi task learning with time series data. The reviewers found the paper interesting, but the majority found the description of the method in the paper confusing and several technical details missing. Moreover, the reviewers were not convinced that the technique used for uncertainty quantification of the features at each stage of the time series is well founded.
A new sampling strategy for experience is proposed and compared with alternative sampling strategies. The main weakness of the paper is the limited applicability of the strategy as it only works well goal oriented tasks, and stochasticity reduces the effectiveness. And within this setting, only good performance is shown on two gridworld like: MiniGrid and Sokoban. In the rebuttal phase, the authors have added additional experiments that suggest applicability of the approach beyond just goal oriented tasks, which have let several reviewers to raise their score. While general applicability of the approach is still somewhat of a concern, the authors have done enough to show the potential of their approach. Hence, I recommend acceptance.
This paper studies Graph Neural Networks for quantum chemistry by incorporating a number of physics informed innovations into the architecture. In particular, it considers directional edge information while preserving equivariance.  Reviewers were in agreement that this is an excellent paper with strong empirical results, great empirical evaluation and clear exposition. Despite some concerns about the limited novelty in terms of GNN methodology ( for instance, directional message passing has appeared in previous GNN papers, see e.g. https://openreview.net/forum?id H1g0Z3A9Fm , in a different context). Ultimately, the AC believes this is a strong, high quality work that will be of broad interest, and thus recommends acceptance. 
This paper investigates the role of locality (ability to encode only information specific to locations of interest) and compositionality (ability to be expressed as a combination of simpler parts) in Zero Shot Learning (ZSL). Main contributions of the paper are (i) compared to previous ZSL frameworks, the proposed approach is that the model is not allowed to be pretrained on another dataset (ii) a thorough evaluation of existing methods.  Following discussions, weaknesses are (i) the proposed method (CMDIM) isn t sufficiently different or interesting compared to existing methods (ii) the paper does not do an in depth discussion of locality and compositionality. The empirical evaluation being extensive, the accept decision is chosen. 
The paper tries to argue the value of making ensembles more reproducible through the use of a correlation loss to try to make components as different as possible. The paper is tough to follow and the high level motivation is unclear. As one of the reviewers points out, don t ensembles provide an estimate of uncertainty and  calibration?  Further, the experiments were quite limited. Studying the proposed approach in a small, controlled setting might also be revealing.
This paper suggests using a conditional prior in conditional diffusion based generative models. Typically, only the score function estimator is provided with the conditioning signal, and the prior is an unconditional standard Gaussian distribution. It is shown that making the prior conditional improves results on speech generation tasks.  Several reviewers initially recommended rejection, but after extensive discussion and interaction with the authors, all reviewers have given this work a "borderline accept" rating.   Criticisms included that the idea is too simple or obvious to warrant an ICLR paper. I am inclined to disagree: simple ideas that work are often the ones that persist and see rapid adoption (dropout regularisation is my favourite example). Like the authors, I believe simplicity is an advantage in this respect, rather than a disadvantage. Of course, simple ideas do require extensive and convincing empirical validation to be worth publishing at ICLR. After the authors  updates, I believe the work meets this bar.  Another issue raised by several reviewers is the limited theoretical justification for the approach. However, combined with the simplicity of the method, I believe the empirical results of the revised version sufficiently justify the approach on their own. Nevertheless, I would recommend that the authors consider further how they could address this issue in the final version of their manuscript, as they have already begun to do during the discussion phase.  Another way to strengthen the paper further would be to demonstrate how the generic approach can be applied in a different domain (e.g. conditional image generation), but I do not consider this addition necessary for the work to warrant publication.  In light of this, I am recommending acceptance.
All reviewers recommended rejection, and I agree. I encourage the authors to follow the reviewers  recommendation and resubmit.
The issue of when model based methods can be used successfully in RL is an interesting one. However, the reviewers had a number of concerns about the significance and framing of this work with respect to the related literature. In addition, the abstract and title suggest a very generic contribution will be made, whereas the actual contribution is to a much more specific subclass. Some relevant papers (and their related efforts) include the following.  The Dependence of Effective Planning Horizon on Model Accuracy.  (AAMAS 15, best paper award) Nan Jiang, Alex Kulesza, Satinder Singh, Richard Lewis.   Self Correcting Models for Model Based Reinforcement Learning. Erik Talvitie. In  Proceedings of the Thirty First AAAI Conference on Artificial Intelligence (AAAI).  2017.  
This is an exciting paper that provide the efficient algorithms for exact sampling from NDPPs along with theoretical results that are very pertinent in and out themselves. The AC agree with the reviewers that the authors satisfactorily addressed the concerns raised in the reviews, and is convinced that the revised version will be greatly appreciated by the community. We very much encourage the authors to pursue this line of work and in particular to overcome the practical restriction to the ONDPP subclass.
The authors present a metalearning based approach to learning intrinsic rewards that improve RL performance across distributions of problems.  This is essentially a more computationally efficient approach to approaches suggested by Singh (2009/10).  The reviewers agreed that the core idea was good, if a bit incremental, but were also concerned about the similarity to the Singh et al. work, the simplicity of the toy domains tested, and comparison to relevant methods.  The reviewers felt that the authors addressed their main concerns and significantly improved the paper; however the similarity to Singh et al. remains, and thus the concerns about incrementalism.   Thus, I recommend this paper for rejection at this time.
Two reviewers as well as the AC are confused by the paper—perhaps because the readability of it should be improved?  It is clear that the page limitation of conferences are problematic, with 7 pages of appendix (not part of the review) the authors may consider another venue to publish.  In its current form, the usefulness for the ICLR community seems limited.
This paper introduces Regioned Episodic Reinforcement Learning (RERL), which partitions the state space by generating a diverse set of goals and then explores the state space by learning policies that reach those goals. This idea is a combination of episodic memory techniques and “goal oriented” reinforcement learning.   After the authors’ responses and the discussion phase, all reviewers converged to recommending the rejection of this paper. The main concerns regarding this paper are:  * Presentation. The proposed approach is not that well justified, some of the claims in the paper are quite imprecise, and there’s relevant related work missing. * Evaluation. The evaluation sometimes feels rushed and is not held in a diverse enough set of tasks, not capturing important properties one would want to capture.  I recommend the authors to pay close attention to presentation, as well as the experiments and analysis in order to make the paper stronger. 
this submission proposes a novel extension of existing recurrent networks that focus on capturing long term dependencies via tracking entities/their statesand tested it on a new task. there s a concern that the proposed approach is heavily engineered toward the proposed task and may not be applicable to other tasks, which i fully agree with. i however find the proposed approach and the authors  justification to be thorough enough, and for now, recommend it to be accepted.
Exploration can happen at various levels of granularity and at different times during an episode,  and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time scale to do so, and what signals would be good triggers to switch). The study is performed on atari games.  Strenghts:   The study is well motivated and the manuscript is overall well written Studies a new problem area, and proposes an initial novel method for this problem extensive study on atari problems  Weaknesses   some clarity issues as pointed out by the reviewers no illustrative task is given to give a more intuitive exposition of the "when to explore" problem comparison to some extra baselines like GoExplore would have been insightful  Rebuttal:   Most clarity issues have been addressed satisfactorily. It has been explained why some requests for extra baselines would be challenging/or not relevant enough. While the authors agree that GoExplore would be an interesting baseline, they seem to have not added it. An illustrative task was not provided.  Summary:   All reviewers agree that this manuscript opens up and tackles a novel direction in exploration, and provides an extensive empirical study on atari games (a standard benchmark for such problem settings). While I agree with the reviewers that point out that this paper could have been made stronger by adding an illustrative task and additional baselines like GoExplore, there is a general consensus that the provided empirical study on this novel problem setting is a good contribution in itself. Because of this I recommend accept.
The paper provides some contribution to the field by exploiting in an unexplored  way background knowledge already covered by relevant literature. Presentation of the proposal is well structured and clear. The paper is also providing interesting theoretical and experimental results. The theoretical results, however, could be better explained.  Overall the proposed work seems to be incremental. Perhaps a deeper investigation into the relationships with diffusion augmentation would add more value to the contribution. 
The reviewers thought that idea of trying to exploit low rank structure in the loss gradients of a feedforward network to improve training was interesting; however they expressed many concerns about the clarity of the presentation, quality of the empirical evaluation, and significance of the result (since the tests were not done on an architecture anywhere near state of the art). Because the authors did not participate in the discussion period, none of these concerns were addressed.  Pros: + Promising idea for new approaches to optimization.  Cons:   Unclear notation for the intended machine learning audience   Algorithm should be illustrated using pseudocode   Limited significance if the method is only usable with purely feedforward networks.   Limited empirical evaluation: positive results only if weights are poorly initialized. 
This paper proposes a method for quantitatively evaluating GANs. Better quantitative metrics for GANs are badly needed, as the field is being held back by excessive focus on generated samples. This paper proposes to estimate the Wasserstein distance to the data distribution. A paper which does this well would be a significant contribution, but unfortunately (as the reviewers point out) the experimental validation in this paper seems insufficient.  To be convincing, a paper would first need to demonstrate the ability to accurately estimate Wasserstein distance   not an easy task, but one which receives little mention in this paper. Then it would need to validate that the method can either quantitatively confirm known results about GANs or uncover previously unknown phenomena. As it stands, I don t think this submission is ready for publication in ICLR, but I d encourage resubmission after more careful experimental validation along the lines suggested by the reviewers.  
This paper introduces a new (un)fairness metric for recommender systems based on mutual information and then develop an algorithm to account for this metric in matrix factorization based collaborative filtering. The reviewers all agree that the proposed metric and algorithm are sound at a technical level, however, they have concerns regarding the motivation of the introduced metric as well as the experimental evaluation. The rebuttal by the authors did not persuade the reviewers to reconsider their original assessment and they still argued that their concerns remained. In the final recommendation, the simplicity of the metric was not seeing as a weakness of the work.
This paper proposes an attention endowed architecture for deep image based RL. While some positive points were raised by the reviewers, most comments were on the negative side. The reviewers noted marginal/incremental advances in terms of empirical results and low novelty and significance. Moreover, the provided baselines seem weak. Because of this, the present submission unfortunately does not meet the publication bar. I recommend the authors take into account the constructive feedback from reviews and discussion and submit an improved version to another venue.
This paper presents a somewhat new approach to training neural nets with ternary or low precision weights.  However the Bayesian motivation doesn t translate into an elegant and self tuning method, and ends up seeming kind of complicated and ad hoc.  The results also seem somewhat toy.  The paper is fairly clearly written, however.
This paper presents a Markov Random Fields (MRF) for generating adversarial examples in a black box setting, where only it has access to loss function evaluations. The method exploits the structure of input data to model the covariance structure of the gradients. Empirically, the resulting method uses fewer queries than the current state of the art to achieve comparable performance. Overall, the paper has valuable contributions. The main issue is on empirical evaluation, which can be strengthened, e.g., by including results with multi step methods and more thorough analysis of the estimated gradients.
This paper is good, with relatively positive support from the reviewers. However, there were also several legitimate issues raised, for example regarding the semantics of a negative answer and associated explanations. Though this paper cannot be accepted at this time, we hope the feedback here can help improve a future version, as all reviewers agree this is a valuable line of work.
This paper proposes a new method for domain generalization. The main idea is to encourage higher inner product between gradients from different domains. Instead of adding an explicit regularizer to encourage this, authors propose an optimization algorithm called Fish which implicitly encourages higher inner product between gradients of different domains. Authors further show their proposed method is competitive on challenging benchmarks such as WILDS and DomainBed.  Reviewers all found the proposed algorithm novel and expressed that the contributions of the paper in terms of improving domain generalization is significant. A major issue that came up during the discussion period was that we realized that the presented results on WILDS benchmark are misleading. In particular, the following statements in the manuscript are false because on "CivilComments" and "Amazon", Fish utilizes a BERT model (Devlin et al., 2018). However, other methods at WILDS benchmark use DistilBERT (Sanh et al., 2019):    Section 4.2: "For hyper parameters including learning rate, batch size, choice of optimizer and model architecture, we follow the exact configuration as reported in the WILDS benchmark. Importantly, we also use the same model selection strategy used in WILDS to ensure a fair comparison."    Appendix C2: "Results: We compare results to the baselines used in the WILDS benchmark over 3 random seed runs in Table 10. All models are trained using BERT (Devlin et al., 2018)."  Authors explained that the mismatch is because at the time they evaluated their model, an earlier version of WILDS benchmark was available but they later updated other methods  results on a newer version of WILDS benchmark. Of course, I do not think that this explanation makes the misleading statements OK. Authors promised to do the following for the camera ready version to make sure it is not misleading:    Using "Worst U/R Pearson r" as the comparison measure for "PovertyMap"   Submitting their method to WILDS benchmark making sure everything matches the baselines and then reporting the results on "Amazon" and "CivilComments" datasets.  Therefore, I recommend acceptance and I hope that authors would stick to their promise and update the manuscript to include these changes.
Though the observation regarding the importance of the low end of the spectrum is interesting in its own right, the paper would be better substantiated by experiments on more datasets and a more thorough characterization of the paper novelty/contrast to state of the art.
This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete. However, the main concern is that the paper is very similar to a recent work by the authors, which is not cited.
The paper proposes a method to improve adversarial robustness by diversifying the ensemble.   Novelty: As pointed out by several reviewers, promoting diversity of ensembles has been done in the literature, but there s still a moderate novelty in proposing the DML layer.   Empirical validations: The original submission lacks many important comparisons (e.g., with [1]). Despite the authors implicitly compared their method with (Pang et al) via Auto attack in the rebuttal, it will be better if the comparisons are conducted in a well controlled way to confirm that the improved robustness comes from DML instead of other hyperparameter settings. Further, it is not clear whether the proposed method is robust to hyperparameters.   Based on these, we recommend rejection but encourage the authors to improve their paper based on the comments. 
This paper adds to the literature of efficient sparse attention for long range transformer architectures: a learned hash function is proposed by building successfully upon contributions from previous work. A similar idea appears in contemporary work, but with clear and complementary differences.  The reviewers are convinced of the importance of attention complexity, bucket imbalance issues, and agree that learning to hash is is a promising solution. The authors have clarified almost all outstanding concerns, in some cases adding valuable new results (e.g. timing experiments.)  I echo the reviewers  concern and stress to the authors to clarify the precise meaning of "plug and play", as it may be misinterpreted (e.g., no fine tuning needed? or just no change to model but still fine tuning is needed.)  Some of the cited papers are accepted at conferences, please update your bib file with the correct information for credit attribution.
The paper presents a new approach for domain generalization whereby the original supervised model is trained with an explicit objective to ignore so called superficial statistics present in the training set but which may not be present in future test sets. The paper proposes using a differentiable variant of gray level co occurrence matrix to capture the textural information and then experiments with two techniques for learning feature invariance. All reviewers agree the approach is novel, unique, and potentially high impact to the community.   The main issues center around reproducibility as well as the intended scope of problems this approach addresses. The authors have offered to include further discussions in the final version to address these points. Doing so will strengthen the paper and aid the community in building upon this work. 
The paper proposes to improve image segmentation from referring expression  by integrating visual and language features using an UNet architecture and experimenting with top down, bottom up, and combined (dual) modulation.    Review Summary: The submission received divergent reviews with scores spanning from 2 (R2) to 5 (R3,R4) to 10 (R1).  The author response failed to address the reviewer concerns with some reviewer (R4) lowering their score tto 4 after the rebuttal.  It also became clear that some relevant work (Mei et al, 2018) was used for the baseline but not cited.  The author response also did not recognize the importance of significance tests.  As there is considerable work in the area of image segmentation from referring expression, and the proposed model is very similar to the LingUNet model of Misra 2018, the originality and significance of the work is fairly low.  The main contributions appears to be experimental comparisons of the three types of modulation (top down, bottom up, dual).  Pros:   Investigation of a important problem of grounding language to visual regions   Experimental study of whether dual modulation improves image segmentation from referring expression  Cons   Relatively minor novelty with limited analyses (R3,R2)   Missing citations (see R3 s comments).  Relevant work (Mei et al, 2018) which was the basis for the top down baseline model, was used but not cited or properly compared against   Relatively weak experimental results (R2,R4).  As R4 noted, while validation results are good, test results are weak compared to existing work, indicating potential overtuning.     No qualitative comparisons against baselines.     Cognitive claims not backed up and limited discussion/analysis (R2)  Recommendation: The AC concurs with R2, R3, and R4 that the work is limited in novelty and not ready for publication at ICLR.   Despite R1 s high score, referring expression for image segmentation is a well studied task, and it is unclear what are the key innovations of the proposed model over LingUNet.  Due to the limited novelty, relatively weak test results, as well as other flaws pointed out by the reviewers, the AC recommends rejection. 
The paper presents LEAPS, a hybrid model based and model free algorithm that uses a Bayesian approach to reason/plan over semantic features, while low level behavior is learned in a model free manner. The approach is designed for human made environments with semantic similarity, such as indoor navigation, and is empirically validated in a virtual indoor navigation task, House3D. Reviewers and AC note the interesting approach to this challenging problem. The presented approach can provide an elegant way to incorporate  domain knowledge into RL approaches.   The reviewers and AC note several potential weaknesses. The reviewers are concerned about the very low success rate, and critiqued the use of success rate as a key metric itself, given that random search with a sufficiently high cut off could solve the task. The authors added additional results in a metric that incorporates path length, and provided clarifying details. However, key concerns remained given the low success rates. The AC notes that e.g., results in the top and middle row of figure 4 show very similar results for LEAPS and the reported baselines. Further, "figure 5" shows no confidence / error bars, and it is not possible to assess whether any differences are statistically significant. Overall, the questions of whether something substantial has been learned, should be addressed with a detailed error analysis of the proposed approach and the baselines, to provide insight into whether and how the approaches solve the task. At the moment, the paper presents a potentially valuable approach, but does not provide convincing evidence and conceptual insights into the approach s effectiveness.
This paper presents a continual learning method based on a novelty detection technique. All reviewers are concerned about various issues, especially, motivation, experiment, and presentation. One of the reviewers was initially positive about this paper but downgraded his/her score due to unresolved problems in the proposed method. Considering all the comments and communications with the authors, AC believes that this paper is not ready for publication yet.
This paper proposes a new image to image translation technique, presenting a theoretical extension of Wasserstein GANs to the bidirectional mapping case.   Although the work presents promise, the extent of miscommunication and errors of the original presentation was too great to confidently conclude about the contribution of this work.   The authors have already included extensive edits and comments in response to the reviews to improve the clarity of method, experiments and statement of contribution. We encourage the authors to further incorporate the suggestions and seek to clarify points of confusion from other reviewers and submit a revised version to a future conference.
This paper studies the link between generalization behavior and "flatness" of the loss landscape in deep networks. Specifically, the authors study two measures of flatness (local entropy and local energy), and show that these two measurements are strongly correlated with one another. Moreover they show via a careful set of numerical experiments that two previously proposed algorithms (entropy SGD and replica SGD) that optimize for local entropy tend to both find flatter minima as well as provide better generalization.  Despite the fact that the paper proposes no new models or algorithms, the experiments are compelling and provide non trivial insights into predicting generalization behavior of deep networks, as well as solid evidence on the benefits of entropy regularization in SGD. The authors also seem to have satisfactorily answered the (numerous) initial concerns raised by the authors. Overall, I recommend an accept.
The reviewers have agreed that the paper is in borderline. Although the reviewers are not really convinced about the authors’ responses, they still acknowledge that the paper is interesting and developed some new techniques for the analysis of distributed optimization.   The following concerns are raised by the reviewers from their discussions:   1) The paper is heavily based on existing work.  2) The theoretical advantages are based on the regime Hessian variance is 0 or small, but it is not clear if and when the Hessian variance is small for more complicated models, which means we will not know if the theory will be helpful in practice. Although the authors provide some experimental results in the rebuttal showing that $(L_+)^2 / (L_{\pm})^2$ can be large at initial iterations, it is still not clear how long will this advantage keep during training and how much the advantage is. 3) Reviewer wjjy increased the score from 5 to 6, considering that the additional result truly suggests that the implicit setting can hold in some case at the beginning of iteration, which makes the submission a complete story for him/her now, to some extent. But if we are more strict on the evaluation, the experiment result also suggests that the implicit assumption will not hold anymore over iterations, because the ratio is approaching 1 quickly, i.e., $ L_\pm$ is about the same order as $ L_+$, so there is a mismatch between theory and practice, which even brings out the risk that the paper will fail from the beginning because Sec 4.1 will not make sense anymore. 4) On the theory side, two main contributions of this paper is relaxation on the compressors used in MARINA and a new assumption to refine the analysis. These two contributions seem rather limited if only used to analyze this specific algorithm   it s unclear what the authors mean in practice or how they correlated to the MARINA. For example, it is still hard for us to compare or understand MARINA with another algorithm as we wouldn t know if the improvement from MARINA is due to a better design, or this additional assumption. The reviewer also finds the authors statement that "their analysis focuses on MARINA because it is SOTA" confusing. Different from NLP and vision community where standard benchmarks are usually used to evaluate new models, He/she is confused by what it means for a newly proposed optimization algorithm to be SOTA. 5) Since the paper proposes a specific algorithm named PermK, it s quite reasonable to question how it relates to some previously proposed sparsification methods with similar design such as (Jianqiao et al., 2017). However, the authors insist their main contribution is in theory, and the small scale experiments comparing with TopK and RandK are sufficient. The reviewer disagrees about this. As communication compression is usually need in larger scales (at least beyond MNIST), and TopK/RandK are not SOTA baselines of sparsification.  The authors are expected to address them for the clarifications in the final version.
Pros: + The proposed large batch, synchronous SGD method is able to generalize at larger batch sizes than previous approaches (e.g., Goyal et al., 2017).  Cons:   Evaluation on more than one task would make the paper more convincing.   The addition of more hyperparameters makes the proposed algorithm less appealing.   Some theoretical justifiction of the layer wise rate scaling would help.   It isn t clear that the comparison to Goyal et al., 2017 is entirely fair, because that paper also had recommendations for the implementation of batch normalization, weight decay, and a momentum correction as the learning rate is scaled up, but this submission does not address any of those.  Although the revised paper addressed many of the reviewers  concerns, they still did not feel it was quite strong enough to be accepted to ICLR. 
The paper is about adapting a voice generation model to new speakers with minimal amount of training data. The key insight in this paper is that the voice can be adapted using a small set of variables   the bias and the variance associated with the layer that normalizes the mel spectrogram associated with the decoder. Additionally, they characterize voice at the utterance level to capture stationary factors like background acoustic conditions and at the phoneme level to capture factors such as prosody, though there are no explicit constraints to force such representation.  The strength of the paper are: + Simplicity of the approach + Empirical evaluation that demonstrates its effectiveness  The weakness of the paper are:   analysis of what the crucial parameters of the model represent   lack of clarity that is obvious from several back and forths between the reviewers and the author.  A few examples include:   “There is also a phoneme level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme level idiosyncrasies of the speaker, although this isn t clear to me.”   “ it is only the speaker embedding that is the input to fine tuning, and this only via the normalization parameters. (Both the normalization parameters and the speaker embedding itself are fine tuned.) I am not sure what the speaker embedding is left to do with all this acoustic level input, but OK.”
This paper received borderline reviews, but all lean toward acceptance.  The reviews highlighted strengths in the paper, citing that they liked the main idea and its mathematical treatment: * R3: "I liked the abstraction proposed by authors and particularly liked the way authors set up the Definition 1 and analysis afterwards" * R3 post discussion: "I recommend accept because authors have a solid theory which would be useful for the self supervised learning community." * R4: "This work presents a very detailed theoretical analysis for self supervised learning objectives. The idea of inverse predictive learning for filtering task irrelevant information is interesting." * R2: "I like the idea of discarding the redundant task irrelevant information to improve the self supervised learning"  However, there was a consensus among reviewers that the experimental validation was weak, both in terms of not showing enough improvement on enough examples and in terms of studying the effect of certain hyperparameters: * R2: "lack of persuasive experiment results to prove the effectiveness of the proposed method. In fig.3, the improvements on two dataset are marginally, which can not convince me. The \lambda (λ_IP) in proposed objective function seems not robust to different datasets, which makes me doubt about the generalization of this method." * R3: "Ratings can be improved further if authors can relate experimental setup more to the theory which I find slightly disconnected" * R3 post discussion: "All reviewers have concerns about lack of solid experimental evidence [...]  I can not improve my score further because of weak experimental evidence." * R1: "The experiments are conducted in a controlled way [...] Traditional uncontrolled experiments [...] are suggested." * R4: "The variation in the performance shown in Figure 3 is very marginal. [...] Figure 5 a shows some results on Omniglot, but the improvement shown there is very marginal. [...]" * R4: "weights required for inverse predictive learning in the loss formulation is not trivial. [...] Is there a simple way to determine this weights without exhaustive search on target dataset?" * R4: "However, it is not clear from the experimental results if this is really effective."  The authors  revisions aim to improve the discussion of the $\lambda_\text{IP}$ parameter.  Given these experimental limitations, my recommendation is for acceptance but with a low confidence score.
Although some criticism remains for experiments, I suggest to accept this paper.
I am inclined to agree with R1 that there is an extensive literature on learning architectures now, and I have seen two others as part of my area chairing. This paper does not offer comparisons to existing methods for architecture learning other than very basic ones and that reduces the strength of the paper significantly. Further the broad exploration over 17 tasks is more overwhelming, than adding to an insight into the methods.
This paper proposes a new sampling method named Random Coordinate LMC (RC LMC), which integrates the idea of randomized coordinate descent and Langenvine dynamic. The authors prove the total complexity of RC LMC for log concave probability distributions, which are better than that of LMC under different settings. The idea of this paper is very neat and the reviewers are in general positive about it. However, as pointed out by one of the reviewers and seconded by the other reviewers, the proof in the original submission is flawed, and the fix needs some substantial work. The new version needs to be carefully checked before publication, which is far beyond the review process of ICLR. Therefore, I encourage the authors to carefully revise the paper and submit it to the next conference. 
The reviewers reached a consensus that the paper is not fit for publication for the moment because a) the paper lacks thorough experiments and b) the criteria provided by the paper are relatively evague (see more details in reviewer 3 s comments.）
The paper shows that a form of Fictitious Self Play converges to the Nash equilibria in Markov games. Understanding the theoretical properties of Fictitious Self Play is important, however the paper in its current form is not ready for publication. The paper needs a more thorough discussion on related works, the assumptions made, and as pointed out by Reviewer3, the convergence argument needs to be expanded and explained in more detail. Further, I encourage authors to add experiments and compare their algorithm with other methods. 
This paper investigates various pathologies that occur when training VAE models. There was quite a bit of discussion (including "private" discussion between the reviewers) about the theory presented. Particular concerns included: For Theorem 1, while the required conditions formalise the setting in which the learned likelihoods are poor, it s unclear whether these particular conditions they are useful in practice or provided deep insight; for Theorem 2, its relevance and importance was not necessarily clear. In general the results in these two theorems are closely related to known challenges (e.g. that using the ELBO to optimise parameters may lead to bias), without necessarily providing as much new insight as one might hope.  I would note that all the reviewers included positive feedback as to the quality of the experiments, showing the impact of these pathologies on downstream tasks. However, as written much of the paper focuses on the theory — too many of the (very interesting!) figures and experimental results are relegated to the appendix.
This paper proposed a new method for improving offline RL. AC thinks that the paper has a potential, but all reviewers suggest rejection as the current write up is quite poor. This causes many misunderstandings of reviewers. The authors clarify some misunderstandings/concerns in the discussion phase, but did not update the draft accordingly. Hence, AC cannot suggest acceptance, given the current form.
This paper studies online learning using SGD with momentum for nonstationary data. For the specific setting of linear regression with Gaussian noise and oscillatory covariate shift, a linear oscillator ODE is derived that describes the dynamics of the learned parameters. This then allows analysis of convergence/divergence of learning for different settings of the learning rate and momentum. The theoretical results are validated empirically, and are shown to generalize to other settings such as those with other optimizers (Adam) or other models (neural nets). The reviewers praise the clear writing and the rigorous and systematic analysis.  3 out of 4 reviewers recommend accepting the paper. The negative reviewer does not find the main contribution interesting and significant enough for acceptance. Although I think this is a reasonable objection, it is not shared by the other 3 reviewers. Since the negative reviewer does not point out any critical flaws in the paper, I think the positive opinions should outweight the negative one in this case. I therefore recommend accepting the paper.
The reviewers feel there are two issues that make this paper fall short of acceptance: first, the lack of a clear emphasis and focus (evidenced by the significant revisions) and second, a lack of comparison to similar, existing methods for multi agent reinforcement learning.
The paper presents an evolutionary optimization framework for training discrete VAEs, which is different to the standard way of training VAEs. One of the main criticism of the paper was the choice of experiments, but the authors addressed this point by adding an inpainting benchmark.  Unfortunately, the reviewers  scores are borderline, and one of the reviewers pointed out the lack of scalability (more precisely, linear scalability with the number of observations) and cannot recommend acceptance based on the limited application impact. Given the large number of ICLR submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue.
The paper provides a simple method for regularising and robustifying GAN training. Always appreciated contribution to GANs. : )
Based on the observation that the eigenvectors with smaller eigenvalues are more non robust (i.e., adversary adds more components along such directions), the authors propose a method called Feature Spectral Regularization (FSR) to penalize the largest eigenvalue, and as a result, the other smaller eigenvalues get increased relatively. In this paper, in addition to FSR, theoretical analysis along with experimental results on different datasets and models were presented. Although the proposed FSR has some merits, the major concerns from the reviewers include (1) impractical use on large scale datasets and (2) lack of significant improvement over SOTA. Compared with other submissions I m handling, I have to reject this manuscript.
The paper investigates how sample efficiency of image based model free RL can be improved  by including an image reconstruction loss as an auxiliary task and applies it to soft actor critic. The method is demonstrated to yield a substantial improvement compared to SAC learned directly from pixels, and comparable performance to other prior works, such as SLAC and PlaNet, but with a simpler learning setup. The reviewers generally appreciate the clarity of presentation and good experimental evaluation. However, all reviewers raise concerns regarding limited novelty, as auxiliary losses for RL have been studied before, and the contribution is mainly in the design choices of the implementation. In this view, and given that the results are on a par with SOTA, the contribution of this paper seems too incremental for publishing in this venue, and I’m recommending rejection. 
Dear authors,  After carefully reading the reviews, the rebuttal, and going through the paper, I regret to inform you that this paper does not meet the requirements for publication at ICLR.  While the variance analysis is definitely of interest, the reality of the algorithm does not match the claims. The theoretical rate is worse than that of SG but this could be an artefact of the analysis. Sadly, the experimental setup lacks in several ways:   It is not yet clear whether escaping the saddle points is really an issue in deep learning as the loss function is still poorly understood.   This analysis is done in the noiseless setting despite your argument being based around the variance of the gradients.   You report the test error on CIFAR 10. While interesting and required for an ML paper, you introduce an optimization algorithm and so the quantity that matters the most is the speed at which you achieve a given training accuracy. Also, your table lists the value of the test accuracy rather than the speed of increase. Thus, you test the generalization ability of your algorithm while making claims about the optimization performance.
The authors study the expressive power of Graph Neural Network architectures for the link prediction problem and provides theoretical justification for the strong performance of SEAL on link prediction benchmarks. However, The reviewers think the paper needs to improve in several aspects before it can be published: 1. More clearly explain the theoretical analysis and contribution. 2. Extensive and in depth discussion of the similarities with and difference to the work of Li et al. to show the novelty of current work. 
This paper proposes a multi scale fusion self attention model for phrase information, which incorporates convolutional models into self attention to explicitly handle word to phrase correlation. This is paired with a sparse masking strategy to balance between word to word attention and word to phrase attention. The model achieves good performance on downstream tasks.  While the proposed method is simple and looks effective, reviewers have expressed concerns with lack of novelty (see the suggested missing references), lack of clarity in the experimental details, and unclear writing. Unfortunately, there was no response from the authors, which makes me recommend rejection. We urge the authors to follow the reviewers  suggestions in a future iteration of their work.
The paper contributes a theoretical understanding of training over parametrized deep neural networks with rectified linear unit (ReLU) activations using gradient descent with respect to square loss in the neural tangent kernel (NTK) regime. Authors consider a non parametric regression framework wherein the labels are generated using a ground truth function, which is assumed to be in the RKHS associated with the NTK, perturbed with noise. Authors show that gradient descent based training without early stopping fails whereas \ell 2 regularized gradient descent achieved minimax optimal convergence rate. The paper is clearly written and the results are solid. Overall, a good paper.
The paper proposes a language modeling architecture based on the RNN cells leveraging Legendre memory units. The proposal is interesting, but as all the reviewers notice, the paper is not ready for the presentation in the top ML conference for several reasons: comparison with weak baselines, shallow or weak analysis of the presented results, insufficient discussion of the related work, etc. Looking forward for all the comments to be addressed by the authors.  In the rebuttal the authors addressed some of the questions but all the reviewers think that the paper is not ready for acceptance and careful rewriting is needed. Recent research on the improved RNN mechanisms suggests that Legendre memory units and related mechanisms might be a gateway to solving several standard issues of training regular RNNs so the topic is definitely of great importance. Thus the authors are highly encouraged to resubmit the paper after making all suggested corrections.
This paper proposes a pair of complementary word  and sentence level pretraining objectives for BERT style models, and shows that they are empirically effective, especially when used with an already pretrained RoBERTa model.  Work of this kind has been extremely impactful in NLP, and so I m somewhat biased toward acceptance: If this isn t published, it seems likely that other groups will go to the trouble to replicate roughly these experiments. However, I think the paper is borderline. Reviewers were impressed by the results, but not convinced that the ablations and analyses were sufficient to motivate the proposed methods, suggesting that some variants of the proposed methods could likely be substantially better. In addition, I agree strongly with R3 that framing this work around  language structure  is disingenuous, and actively misleads readers about the contribution to the paper.
This is an interesting paper aiming to further advance the knowledge of implicit bias in deep networks.  Unfortunately, the reviewers had many concerns about technical details and presentation.  One concern was about section 5, on margins and implicit bias.  Oddly, this section 5 does not cite the extensive literature on margin maximization, implicit bias, and implicit regularization in deep learning (despite a mention of Soudry et al earlier on), whereas the choice of paper title and also this section title would suggest an advance over this work, or at least reference to this work (which goes far beyond that one paper); instead, that section left me a bit confused about the suggested bias and its implications on generalization.  As such, I suggest the authors spend more time on their submission, aiming to further separate their work from prior work, and address the comments of reviewers.
This paper experimentally investigate the inductive bias of deep neural networks tending to produce low rank embeddings of data, which is important to explain why over parameterized DNN can generalize. In particular, the paper empirically finds that deeper networks are more likely to produce lower rank embedding through thorough numerical experiments with different network architectures, hyperparameters and so on. The authors also proposed a linear over parameterization technique to induce low rank bias and empirically justifies its effectiveness.   Overall, this paper is well written, and the numerical experiments are carefully executed. However, the main drawback of the paper is that the low rank inductive bias itself is a well known phenomenon and this paper gives a kind of additional confirmation to it. I acknowledge that there are several differences from existing papers, but overall we see rather limited insight from the results. Indeed, some of existing studies gave theoretical analyses to understand "why it happens", but this paper does not give a sufficiently novel insight to reveal the reason.   To summarize the decision, this paper lacks deeper insight compared with existing work although the authors did a good job to execute through experiments. Therefore, it is a bit below the acceptance threshold.
The paper has some potentially interesting ideas but it feels very preliminary. The experimental section in particular needs a lot more work.
The title of the paper nicely summarizes the main goal of the paper and the abstract does the same for the achieved results. For this reason I abstain from providing another summary.   The initial reviews were somewhat mixed but during the discussion phase, a lot of questions have been resolved so that actually three reviewers updated (upgraded) their score. Remark 14 certainly needs to be updated according to the discussion in the final few days of the rebuttal phase. In addition, one reviewer pointed to a naive application of Mercer s theorem. This should be addressed as well, either by restricting to compact domains and continuous kernels as suggested by the reviewer, or by considering generalizations as done by e.g. the cited Fischer and Steinwart. Finally, the cited survey by Kanagawa et al also contains some information on learning curves and thus it should be cited more prominently, e.g. around Remark 14.  In any case, this paper is above the acceptance threshold.
The paper proposes a competition on generative models on a new dataset to study memorization in generative models and propose a  new metric Memorization Informed Frechet Inception Distance (MiFID).   While this is an important topic, reviewers raised multiple issues and concerns regarding 1)  the metric definition (that it needs to be max and not min, this was acknowledged in the rebuttal but not updated in the paper) , 2)  how this competition is ran in terms of the definition of "cheating",  that the setup is not controlled and only constraining the time of training 3)  the notion of MiFID is depending on the sets of samples considered and the feature extractor used.   Some other reviewers raised concerns that the paper is only concerned by FID and not other metrics , and that it was only verified on GANs.  We hope the authors will address those concerns and submit the paper to an upcoming venue.
(I acknowledge reading authors  recent note on decaNLP.)  This paper proposes a span extraction approach (SpExBERT) to unify question answering, text classification and regression. Paper includes a significant number of experiments (including low resource and multi tasking experiments) on multiple benchmarks. The reviewers are concerned about lack of support on author s claims from the experimental results due to seemingly insignificant improvements and lack of analysis regarding the results. Hence, I suggest rejecting the paper.
This paper claims a practical improvement over one of earlier meta BO methods. Warm starting BO or HPO by making use of data from past experiments or tasks seems to be interesting and useful for some applications. In fact, there are a large amount of work on this topic, but a lot of relevant prior work is ignored in this paper unfortunately. I appreciate the authors for making efforts in responding to reviewers’ comments. However, after the discussion period, most of reviewers had serious concerns in this work, pointing out that the proposed method is rather trivial and the comparison is made only against a simple baseline. It was also suggested to improve the experiments. While the idea is interesting, the paper is not ready for publication at the current stage.
The paper proposes a method to perform manifold regularization for semi supervised learning using GANs. Although the SSL results in the paper are competitive with existing methods, R1 and R3 are concerned about the novelty of the work in the light of recent manifold regularization SSL papers with GANs, a point that the AC agrees with. Given the borderline reviews and limited novelty of the core method, the paper just falls short of the acceptance threshold for ICLR. 
This paper shows how to obtain more homogeneous activation of atoms in a dictionary. As reviewers point out, the paper is well written and indeed shows that the propose scheme results in a more uniform activation. However, the value of this contribution rests on making a case that uniformity is indeed a desirable outcome per se. As two reviewers explain, this crucial point is left unaddressed, which makes the paper too weak for ICLR.
This paper presents a variant of the Noise Conditional Score Network (NCSN) which does score matching using a single Gaussian scale mixture noise model. Unlike the NCSN, it learns a single energy based model, and therefore can be compared directly to other models in terms of compression. I ve read the paper, and the methods, exposition, and experiments all seem solid. Numerically, the score is slightly below the cutoff; reviewers generally think the paper is well executed, but lacking in novelty and quality of results relative to Song & Ermon (2019).  
This paper has been reviewed by four expert reviewers who gave diverging scores. The three negative reviewers have provided significant constructive feedback. The main criticism is the lack of novelty and clarity in the paper. The authors have submitted their rebuttal which did not improve the scores of these reviewers. After the discussion phase, the paper did not obtain any support for acceptance and stayed under the acceptance threshold. Following the reviewers  recommendation, the meta reviewer recommends rejection.
The paper studies the problem of graph learning with attributes, and propose a 2 D graph convolution that models the node relation graph and the attribute graph jointly. The paper proposes and efficient algorithm and models intra class variation. Empirical performance on 20 NG, L Cora, and Wiki show the promise of the approach.  The authors responded to the reviews by updating the paper, but the reviewers unfortunately did not further engage during the discussion period. Therefore it is unclear whether their concerns have been adequately addressed.  Overall, there have been many strong submissions on graph neural networks at ICLR this year, and this submission as is currently stands does not quite make the threshold of acceptance.
This work is proposing an approach for ensuring classification fairness through models that encapsulate deferment criteria. On the positive side, the paper provides ideas which are conceptually interesting and novel. On the other hand, the reviewers find the technical contribution to be limited and, in some cases, challenge the practicality of the method (e.g. requirement for second set of training samples). After extensive post rebuttal discussion, the consensus is that the above issues make the paper fall below the threshold for acceptance – even if the “out of scope” issue is not taken into account.
The paper considers the task of incorporating knowledge expressed as rules into column networks. The reviewers acknowledge the need for such techniques, like the flexibility of the proposed approach, and appreciate the improvements to convergence speed and accuracy afforded by the proposed work.  The reviewers and the AC note the following as the primary concerns of the paper: (1) The primary concerned raised by the reviewers was that the evaluation is focused on whether KCLN can beat one with the knowledge, instead of measuring the efficacy of incorporating the knowledge itself (e.g. by comparing with other forms of incorporating knowledge, or by varying the quality of the rules that were introduced), (2) Even otherwise, the empirical results are not significant, offering slight improvements over the vanilla CLN (reviewer 1), (3) There are concerns that the rule based gates are introduced but gradients are only computed on the final layer, which might lead to instability, and (4) There are a number of issues in the presentation, where the space is used on redundant information and description of datasets, instead of focusing on the proposed model.  The comments by the authors address some of these concerns, in particular, clarifying that the forms of knowledge/rules are not limited, however, they focused on simple rules in the paper. However, the primary concerns in the evaluation still remain: (1) it seems to focus on comparing against Vanilla CLN, instead of focusing on the source of the knowledge, or on the efficacy in incorporating it (see earlier work on examples of how to evaluate these), and (2) the results are not considerably better with the proposed work, making the reviewers doubtful about the significance of the proposed work.  The reviewers agree that the paper is not ready for publication.
This paper presents new insights for training on random subspaces of low dimension, with several theoretical and experimental contributions. This is a paper that would be interesting to many people doing research in deep learning, both from the theoretical and practical side.
The reviewers agree this is an interesting paper with interesting ideas, but is not ready for publication in its current shape. In particular, there is a need for strong empirical results.
The paper examines the problem of unsupervised domain translation. It poses the problem in a rigorous way for the first time and examines the shortcomings of existing CycleGAN based methods. Then the authors propose to consider the problem through the lens of Optimal Transport theory and formulate a practical algorithm.  The reviewers agree that the paper addresses an important problem, brings clarity to existing methods, and proposes an interesting approach / algorithm, and is well written. However, there was a shared concern about whether the new approach just moves the complexity elsewhere (into the design of the cost function). The authors claim to have addressed in the rebuttal by adding an extra experiment, but the reviewers remained unconvinced.  Based on the reviewer discussion, I recommend rejection at this time, but look forward to seeing the revised paper at a future venue.
The four reviewers believed the paper was below threshold for acceptance to ICLR. They raised concerns with the experimental evaluation and thought that the paper could benefit from another edit to help with the clarity.
The paper proposes a novel  local combinatorial search algorithm for the discrete target propagation framework of Friesen & Domingos 2018, and shows a few promising empirical results.  Reviewers found the paper well written and clear, and two of them were enthusiastic about the direction of this research.  But all reviewers agreed that the paper is too preliminary, particularly in its empirical coverage. More extensive experiments are needed to compare with competitive approaches form the literature, for the task of training hard threshold networks. Experiments would need to evaluate the algorithms on larger models and data more representative of the field, to measure how the approach can scale, and to convince of the superiority or advantage of the proposed method.
The paper proposes an application of the CNN to the microscopy problem of constructing 3D volumes from 2D captured images. The four reviewers thought the paper was a straightforward application of existing techniques to a new problem, while they were borderline towards accept the overall sentiment was that the technical novelty was very low from an ML perspective, and the ICLR community would only find the application potentially of interest. (Two reviewers changed from borderline reject, while the other two chose not to change their scores following the authors’ request.)
This work proposes an approach to improve non ML based methods of text generation. It reformulates the problem with the soft Q learning approach from RL instead of standard hard RL formulations from previous text generation work. By doing this, the work allows application of path consistency learning. This is an elegant formulation. However, this reformulation into soft Q learning appears quite straightforward and so the application of path consistency learning does not require much change to be used for text generation. This limits the novelty of the work. The experiments are also relatively small scale and consists of some non standard tasks such as prompt generation (which is typically evaluated indirectly, the response to the prompts rather than the prompt itself). As the reviewers mention, evaluating on more large scale standard tasks such as summarisation or dialog would be more convincing. Finally the work lacks references to recent works in the field, such as LeakGAN.
This paper presents a unified probabilistic approach for deep continual learning by combining generative and discriminative models into one framework that solves the following problems: catastrophic forgetting, and identifying out of distribution and open set examples. The method termed, OCDVAE in the paper achieves closer or better to SOTA results in different evaluation tasks.   The reviewers had several concerns about the presentation of the paper and some errors in the equations, all of which seem to have been fixed in the latest upload made by the authors. Blind review #3 was delayed as the original reviewer refused to review the paper and this review was then obtained by someone else after the new upload of the paper, so this review looks at the new version of the paper. I would recommend the authors to incorporate suggestions provided by reviewer #3 in the final version of the paper including expanding on the related work section.   However, as of now I recommend to reject the paper.
The concerns raised by AnonReviewer3 point out that, despite the effort of the authors to bridge the SM / ML divide, there is still some work to be done. The gulf between thermodynamic limits and finite effects is oft cited in the author response. This seems to be a catch all. This gap needs to be addressed early. The authors might even suggest some open (empirical) questions looking for these phase transitions in finite systems in cases where they think engineering has placed us "not too close".  
The manuscript proposes an experience replay method that supports two time scales of memory, as in complementary learning systems from the cognitive sciences literature. The authors demonstrate that their method on a wide range of benchmarks and after the rebuttal demonstrate it on one additional benchmark. The reviewers raised a number of questions and concerns around additional experiments (benchmarks and ablations), online training, the cost of training, number of hyperparameters, further analysis, clarifications, and citations. The authors address the majority of these concerns during the rebuttal period, and overall the reviewers were in favour of acceptance. Therefore, I recommend acceptance of this work.
The paper presents a provable correct framework, namely Universal Aggregation, for training GANs in federated learning scenarios. It aims to address an important problem. The proposed solution is well grounded with theoretical analysis and promising empirical results.   The paper receives mixed ratings and therefore there were extensive discussions. One the positive end, some reviewers think that the authors  feedback provide clarification to confusing part of the paper; on the negative side, the authors feedback also confirms some of the concerns raised in the reviews:   1.  It was confirmed that there is no guarantee that one can find an (nearly) optimal discriminator, which decreases the impact of the work, as in practice we work with non optimal discriminators and hence some of the results couldn t apply.   2. It was confirmed that no privacy guarantees can be given. This is concerning since the complexity of GANs won t prohibit skilled attackers from inferring some information.   While it is true that some of the guarantees would be hard to achieve even for a traditional GAN, the paper sets up a high expectation at the beginning of the paper,  but fails to satisfy the readers with enough evidence.  In addition, the writing can be significantly improved to ensure precise formulations and consistency; the added experiment results are useful, but stronger empirical results could help alleviate the issues in theoretical results.   In summary, the paper has built solid foundations for a good piece of work, but the current version could benefit from one more round of revision to become a strong publication in the future. 
This is an interesting paper working on the difficult problem of learning from video demonstration. The authors provided convincing experimental solutions for visual representation, domain adaptation, and imitation. It would be a nice ICLR paper.
The paper provides a new learning technique for problems that require learning embeddings. In particular, the authors analyze a technique that takes into account the frequency of items in an embedding layer to modify the learning rate for each embedding. The paper provides a theoretical analysis of this approached and contrasts it to that of SGD. It also provides experiments validating this approach empirically.  The reviewers agree that the paper provides a simple yet effective method, based on realistic assumptions (non uniform frequencies). In addition, the paper seems to be well written and easy to follow. One issue raised in the reviews was about the focus of the paper, and the fact that the experiments are limited to recommendation systems even though the method is claimed to be generic for any model requiring embeddings. During the rebuttal the authors provided experiments for an NLP task that show favorable results to the new technique in another regime. Given the overall positive feedback and this new evidence validating the proposed method, I recommend accepting the paper.
I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is well written and easy to follow [186e,TAdH,Exgo], well motivated [TAdH], interesting [PsKh], novel [186e] and provides gains over baselines [186e,TAdH,PsKh] with interesting ablations [186e,Exgo]. I thus recommend accepting the paper and  I encourage the authors to further improve their paper based on the reviewer feedback.
This paper proposes to improve offline RL by a data augmentation technique that exploits the symmetry of the dynamics using Koopman operator. The idea is interesting but the draft at its current form has several weaknesses as pointed out by the reviewers. The scores are borderlines at this point. I read the paper and find myself agree with reviewer ohJ3 in both the lack of  clarity and the gap in theory and empirical results. The math presentation still  a careful check and improvement. Eq(1) (4) are already fairly confusing (should $Q_i$ and $\pi_i$ be replaced by $Q$ and $\pi$ in Eq(1) (4), and $\hat Q$ by $\hat Q_i$ in Eq (2) (3)?). I would like to suggest the authors to add a self contained algorithm box for the practical algorithm procedure. Do the readers really need to understand the full Koopman theory (section 3.1) before understanding the algorithm? The authors could think about if it is better to present the practical algorithm first with minimum math, and then analyze the property of the algorithm using the math tools (and in this case, make it clear what theoretical guarantees we get exactly). I think making the paper more accessible can help the paper gain more popularity in ML readers.
This paper proposes to (re )examine VAEs with calibrated uncertainties for the likelihood, which is say VAEs in which the variance is learned rather than chosen as a fixed hyperparameter. The authors argue that doing so provides a reasonable means of automatically navigating the tradeoff between minimizing the distortion (the reconstruction loss) and the rate (the KL loss) in the variational objective. In particular, the authors propose to use a diagonal covariance  Σ   σ^2 Ι that is shared across pixels, and note that it is trivial to define  σ(z)   MSE(x, μ(z)) on a per image basis to minimize the reconstruction loss.   This is very much a borderline paper. Reviewers appreciate that the writing is clear, and acknowledge that revisiting the idea of learning calibrated is of interest to the community. At the same time, the reviewers note that the proposed approach has very limited technical novelty, and note problems with the experimental evaluation.   The metareviewer has read the paper, and is critical of the framing of this work. The manuscript in its current form does not do a sufficiently good job of discussing the large and detailed literature that exists on this topic. Learning calibrated decoders is by no means new, which this submission could and should acknowledge much more clearly. The two seminal papers on VAEs both considered learning calibrated decoders. Moreover there is a lack of thoughtful discussion of the reasons why learning a pixel wise σ(z) is not common practice. The authors note that this can lead to problems with training stability, but fail to note that this problem is mathematically ill posed; A well known property of VAEs is that high capacity models will memorize the training data, in the sense that the optimal learned marginal likelihood is equal to the empirical distribution over the training set (i.e. a mixture over delta peaks).   The metareviewer would expect to see a more thoughtful discussion of  the long line of work on navigating the trade off between rate and distortion, as well as the role of model capacity. A good place to start would be a more careful discussion of the autoencoding and autodecoding limits (Alemi et al 2018) and the GECO paper (Rezende et al 2018). More broadly, the metareviewer would expect some discussion of approaches that improve the quality of generation such as [1], and work that considers effect of model capacity on generalization, such as [2].    In terms of experimental evaluation, this paper also somewhat falls short. As R4 notes, some of the results look worryingly bad, which may be due to the fact that the authors train for only 10 epochs (as indicated in  Appendix B). Moreover, what is once again lacking in experiments is a systematic consideration of the role of model capacity. Some comparison to more recent baselines than the β VAE (e.g. GECO) would also be helpful here.   The metareviewer is sympathetic to the basic premise of this paper, which is the claim that learning a σ that is shared across pixels is a pretty good best practice in terms of finding a reasonable balance between rate and distortion. There is certainly room for a paper that communicates this idea. However, such a paper should (a) more explicitly position itself as revisiting this idea rather than introducing this idea, (b) include a more thoughtful discussion of related work, and (c) include a more robust empirical evaluation.   [1] Engel, J., Hoffman, M. & Roberts, A. Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models. arXiv:1711.05772 [cs, stat] (2017).  [2] Shu, R., Bui, H. H., Zhao, S., Kochenderfer, M. J. & Ermon, S. Amortized inference regularization. in Proceedings of the 32nd International Conference on Neural Information Processing Systems 4398–4407 (Curran Associates Inc., 2018).
This paper introduces a framework for automatic differentiation with weighted finite state transducers (WFSTs), which would allow user specified graphs in structured output prediction tasks and easy plug and play of graphs through the composition operation (demonstrated with variants of CTC). The authors demonstrated their framework on the OCR and ASR domains, which are important application scenarios. All reviewers agree the work is useful and can potentially be significant. However, the reviewers think the paper needs more discussions of similar/parallel work and the key differences from them, and clear description of the novelty in terms of either machine learning insights or algorithmic implementations.  We understand that this may be an implementation heavy work, but the level of details provided in the current version does not convince the reviewers that the proposed approach is already efficient and can scale up. This could be shown by fair comparison with existing approaches (e.g., hard coded error back propagation implementation with a fixed graph) in runtime and accuracy. 
This paper advocates for the application of entanglement entropy from quantum physics to understand and improve the inductive bias of neural network architectures for question answering tasks. All reviewers found the current presentation of the method difficult to understand, and as a result it is difficult to determine what exactly the contribution of this work is. One suggestion for improving the manuscript is to minimize the references to quantum entanglement (where currently is it asserted without justification that entanglement entropy is a relevant concept for modeling question answering tasks). Instead, presenting the method as applications of tensor decompositions for parameterizing neural network architectures would make the work more accessible to a machine learning audience, and help clarify the contribution with respect to related works [1].   1. http://papers.nips.cc/paper/8495 a tensorized transformer for language modeling.pdf
The paper introduces a variant to the option critic framework that encourages options to display a certain level of "diversity" and this is induced by introducing a mutual information objective between the options and their transitions. The authors conjecture that such criterion makes options more suitable for exploration.  Overall, reviewers agree that the idea behind the proposed method and the general approach is sound and interesting. Nonetheless, there is general consensus that the current submission suffers from a number of shortcomings that make it unsuitable for acceptance.  Following the detailed comments provided by the reviewers, I strongly encourage the authors to focus on the following dimensions to improve the paper: 1  The current experiments indeed provide a first illustration of how the proposed algorithm works, but they need significant improvement in variety and scope: As pointed out by the reviewers, the current experiments do not cover single reward challenging exploration benchmarks (such as Montezuma). I agree with the authors that the inductive bias implemented in their algorithm is designed with diversity of goals in mind, but if the main point is to improve exploration, it is natural to expect results in that respect. Alternatively, the authors should state more explicitly the type of problems their method is intended to solve from the very beginning of the paper and design experiments accordingly. 2  The initial mutual information objective is simplified across multiple steps and it is unclear how much the approximations impact the original "semantic" of the objective. 3  A more thorough comparison with mutual information based methods such as DIAYN or VIC is needed. Also, I wonder what is the connection with more goal based exploration approaches such as GoExplore or SkewFit.
The paper addresses counterfactual fairness learning using generative approach. While acknowledging the importance and potential usefulness of generative approach, the reviewers and AC raised several important concerns that place this paper below the acceptance bar:   (1) low degree of novelty – see multiple concerns and suggestions by R2, R3, R4;  (2) the model is not justified by a causal mechanism (R4), and it remains unclear under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples (R2);    (3) lack of technical rigor when presenting the model – see R4’s request to relate to the DAG models, see R1 multiple questions regarding the reinforced data sampler;   (4) lack of empirical evidence (R3) and evaluation details, e.g. on cross validation and more recent methods (see R4’s recommendations);    (5) related work is not discussed in sufficient details – see R4’s elaborate comment.  Among these, (4,5) did not have a substantial impact on the decision but would be helpful to address in a subsequent revision. However, (1), (2) and (3) make it very difficult to assess the benefits of the proposed approach and were viewed by AC as critical issues. In the rebuttal it is stated that ‘Our counterfactual examples are generated using a powerful generator rather than a fixed synthesizer in Kusner et al. (2017)’ – more rigorous comparison has to be provided to support such statement. AC would urge the authors to contrast and compare their synthetic counterfactual examples with Kusner et al on the datasets where causal graph has been built. [Razieh Nabi and Ilya Shpitser. Fair inference on outcomes., AAAI2018], Figure 2 postulates causal graphs for the Compas and Adult Income datasets evaluated in this paper.  A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews and encouragements are useful for revising the paper. 
This paper proposes to improve the faithfulness of data to text generation models, through an attention based confidence measure and a variational approach for learning the model.  There is some reviewer disagreement on this paper.  All agree that the problem is important and ideas interesting, while some reviewers feel that the methods are insufficiently justified and/or the results unconvincing.  In addition, there is not much technical novelty here from a machine learning perspective; the contribution is to a specific task.  Overall I think this paper would fit in much better in an NLP conference/journal.
This work proposes to train networks with mixed image sizes to allow for faster inference and also for robustness. The reviewers found the paper was well written and appreciated that the code was available for reproducibility. However, the paper does not sufficiently compare to related methods. The authors should resubmit once the comparisons suggested by the reviewers have been added to the paper.
Dear authors,  as you have noticed this paper was not easy to review. I have hence invited 2 additional reviewers which I strongly respect and are very knowledgeable. After carefully reading the paper myself, I have to agree with one of the reviewers who said "... it [your paper] makes a good contribution to the literature ....". To be honest, we were working in my group on a very similar approach but did not manage to finish it (and I know how hard it is).  To conclude, when preparing to the final version, please try to go over the reviews, I am sure they can make your paper even stronger :)  
The paper addresses the problem of generating captions for ECG signals where they extend the task in the literature from monolingual to multilingual captions. The model proposed in this work is a variant of mask language model where they augment the target by switching the words from one language to another. In addition to predicting the actual words, they also predict the language associated of the words.  Pros + The problem is motivated by real world application and need.  + The presentation is clear and the authors compare the performance of the model with appropriate recent models.  Cons   The model has higher complexity in both training and parameters, yet achieves only comparable performance to simpler models.   Empirical evaluation of the multi lingual output is inadequate since the ​ground truth is derived from Google Translate.   There are also a number of other specific concerns raised by the reviewers (e.g., VuBG, HBxE).  Reviewers raised questions about several shortcomings. In response the authors updated the paper and were very engaged in the discussion with the reviewers. However, at the end of the day, the paper in its current form has serious shortcomings and left the reviewers unconvinced. I suggest the authors take advantage of the feedback from the reviewers and address them fully in their next iteration.
The reviewers found this paper on improving NLG using a graph to sequence architecture interesting and the results impressive. While I would personally have preferred to see further evaluation of this model on another NLG task, I think it would be overstepping in my role as AC to go against the reviewer consensus. The paper is clearly acceptable.
The paper looks at the favorable properties of feature representations of an adversarially robust model, which are interesting but not surprising, especially in the context of much existing literature has talked about this. All reviewers gave negative scores. The main issues are: 1) The paper only provides experimental demonstration of this phenomenon without going into a more detailed explanation of the phenomenon. This is not enough when the observations, in question, are not very novel and have already been explored in various forms in past published literature. 2) limited novelty since the current submission does not introduce a new approach or algorithm or theoretical results. The paper also lacks comparison/discussion of recent works. Thus, I cannot recommend accepting the paper to ICLR.
This paper introduces an ImageNet scale benchmark UIMNET for uncertainty estimation of deep image classifiers and evaluates prior works under the proposed benchmark. Two reviewers suggest reject, and one reviewer does acceptance. In the discussion period, the authors did not provide any response for many concerns of reviewers, e.g., weak baselines, weak novelty, and lack of justification for the current design. Hence, given the current status, AC recommends reject.
Thanks for your submission to ICLR.  This paper presents an extension to prototypical networks based on using hyperspheres to represent the prototypes.  Strong empirical results are presented using this approach.  Overall, this is a very borderline paper and could go either way.  The idea itself it simple, though the results seem to be fairly strong.  I read through the paper myself and tend to think that it could use a bit more work before it s ready.  Some of the issues raised by the reviewers particularly with respect to experiments and literature review are worth nailing down.  Further, I think that the method could be explored in a more principled/theoretical way.  For instance, when reading this idea, the first thing that pops into my mind is that representing the prototype with a hypersphere is very similar to representing a distribution (e.g., a Gaussian) using a mean and covariance (in this case, a spherical covariance).  Indeed, if you take the KL divergence between two spherical Gaussians, you get something very similar to the expression used in the paper.  This is all to say that there may be other more general directions to take this idea, or other interpretations of what is going on.  Please do keep in mind the comments of the reviewers when preparing a future version of the manuscript.
The paper proposes a Bayesian extension to existing knowledge base embedding methods (like DistMult and ComplEx), which is applied for for hyperparameter learning. While using Bayesian inference for for hyperparameter tuning for embedding methods is not generally novel, it has not been used in the context of knowledge graph modelling before. The paper could be strengthened by comparing the method to other strategies of hyperparameter selection to prove the significance of the advantage brought by the method.
This paper attempts at ranking of tasks handled by deep learning methods based on learning curves.  A main premise of the paper is "fitting learning curves to a power law, and then sorting tasks by empirical estimates of exponents".   The idea of the paper is quite interesting.  However, the paper makes some bold claims which are a bit distant from the empirical study it conducts.  It is hard to line up the order in Table 2 with the Chomsky hierarchy.  Also, for various tasks, various different deep models are used (ResNets for image classification, LSTMs for LM, and so on).  I was not convinced that the beta parameter is model agnostic.  Similar concerns are expressed by the reviewers, and they agree that the paper should address the criticism that they express in their feedback.
This paper is extending the meta learning MAML method to the mixture case. Specifically, the global parameters of the method are now modeled as a mixture. The authors also derive the elaborate associated inference for this approach.  The paper is well written although Rev2 raises some presentation issues that can surely improve the quality of the paper, if addressed in depth.   The results do not convince any of the three reviewers. Rev3 asks for a clearer exposition of the results to increase convincingness. Rev2 and Rev1 also make similar comments.    Rev1 also questions the motivation of the approach, although the other two reviewers seem to find the approach well motivated. Although it certainly helps to prove the motivation within a very tailored to the method application, the AC weighted the opinion of all reviewers and did not consider the paper to lack in the motivation aspect.   The reviewers were overall not very impressed with this paper and that does not seem to stem from lack of novelty or technical correctness. Instead, it seems that this work is rather inconclusive (or at least it is presented in an inconclusive manner): Rev1 says that the important questions (like trade offs and other practical issues) are not answered, Rev2 suggests that maybe this paper is trying to address too much, and all three reviewers are not convinced by the experiments and derived insights.   Finally, Rev2 points out some inherent caveats of the method; although they do not seem to be severe enough to undermine the overall quality of the approach, it would be instructive to have them investigated more thoroughly (even if not completely solving them).
The reviewers were generally split on this paper. On the one hand, reviewers generally appreciated the clear presentation, discussion, and explanations, and the experiments. On the other hand, most reviewers commented on the lack of comparative evaluation to other works, including works that are related conceptually. While the authors have a potentially reasonable argument for omitting such comparisons, in the balance I do not believe that the reviewers were actually convinced by this. Particularly when the novelty of the contribution is not crystal clear, such comparisons are important, so I am inclined to not recommend acceptance at this point (though I acknowledge that the paper is clear borderline and could be accepted).
This paper proposes using the Fisher information matrix to characterize local minima of deep network loss landscapes to indicate generalizability of a local minimum. While the reviewers agree that this paper contains interesting ideas and its presentation has been substantially improved during the discussion period, there are still issues that remain unanswered, in particular between the main objective/claims and the presented evidence. The paper will benefit from a revision and resubmission to another venue.
While all reviewers see a lot of value in the paper, it cannot be accepted in its current form: too many issues with clarity. A more focused paper, with clear task and contributions is recommended. The revisions and answers to reviewer questions are greatly appreciated and go a long way towards addressing these concerns for a future submission.
This paper introduces a method for unsupervised abstractive summarization of reviews.  Strengths:  (1) The direction (developing unsupervised multi document summarization systems) is exciting  (2) There are interesting aspects to the model  Weaknesses:  (1)  The authors are clearly undecided how to position this work: either as introducing a generic document summarization framework or as an approach specific to summarization of reviews. If this is the former, the underlying assumptions, e.g., that the summary looks like a single document in a group is problematic. If this is the latter, then comparison to some more specialized methods are lacking (see comments of R1).  (2) Evaluation, though improved since the first submitted version (when human evaluation was added), is still not great (see R1 / R3). The automatic metrics are not very convincing and do not seem to be very consistent with the results of human eval. I believe that instead or along with human eval, the authors should create human written summaries and evaluate against them. It has been done for extractive multi document summarization and can be done here. Without this, it would be impossible to compare to this submission in the future work.    (3) It is not very clear that generating abstractive summaries of the form proposed in the paper is an effective way to summarize documents.  Basically, a good summary should reflect diversity of the opinions rather than reflect an average / most frequent opinion from tin the review collection.  By generating the summary from a review LM, the authors make sure that there is no redundancy (e.g., alternative views) or contradictions. That s not really what one would want from a summary  (See R3 and also non public discussion with R1)  Overall, I d definitely like to see this work published but my take is that it is not ready yet.  R1 and R2 are relatively negative and generally in agreement.  R3 is very positive. I share excitement about the research direction with R3 but I believe that concerns of R1 and R2 are valid and need to be addressed before the paper gets published.       
The reviewers liked the direction of the paper but unanimously agree that, in its current version, it is not strong enough to justify publication at ICLR. There was no rebuttal from the authors to consider. 
This submission develops a novel technique for domain adaptation for the setup where only a trained model (but no data) from the source task is available. The authors propose to fine tune the feature encoder using batch norm statistics of the features extracted. Additionally their criterion also promotes increasing the the mututal information between features and target classification. The developed method is experimentally evaluated on several benchmarks.  Pros:   The problem considered is of practical relevance and general interest in ICLR community   The proposed methodology is well motivated and shows good performance  Cons:   There is no thorough formal analysis of when the method would work and not work; not even on an intuitive level (state conditions under which the proposed method should be expected to work better/worse than other state of the art optimization criteria for the same setup   Alternatively to a sound theoretical analysis, the authors should provide a more extensive set of ablation experiments (this was mentioned by several reviewers)  In the current format, it remains unclear, how the research community would benefit from the study presented.
Please clarify as early as the abstract that you refine the analysis of the algorithm proposed by Shalev Shwartz et al (which is a great contribution given the importance of the problem).
The paper aims to scale transformers to large graphs. In this regard, authors propose to first obtain a "coarse" version of the large graph using existing algorithms. With reduced number of nodes in the coarse graph, we can employ the transformer efficiently to capture the global information. To capture the local information, GNNs are employed. Finally, authors carry out extensive experiments on a range of graph datasets. Also, reviewers do appreciate reporting the confidence intervals. We thank the reviewers and authors for engaging in an active discussion. Unfortunately, the reviewers are in a consensus that novelty of the proposed method is limited: it is combination of existing techniques and similar ideas have been widely used in the literature. Also, the empirical results are not very significant. Thus, unfortunately I cannot recommend an acceptance of the paper in its current form.
**Summary**  This paper proposes a novel offline model based meta RL approach called MerPO. MerPO combines conservative value iteration with proximal RL policy iteration.  The proposed method is novel despite having some similarities to approaches like COMBO. The paper compares against it in the experiments. The paper provides both empirical and theoretical justification for the proposed approach.  **Final Thoughts**  Overall, I think the authors did a pretty good job at addressing the reviewers  concerns. Overall, I think this is an interesting contribution to the ICLR community. The reviewers were all positive about this paper. For the camera ready version of the paper, I would recommend the authors to go over the reviewers  concerns again and make sure that those concerns are addressed in the paper too as they did in the rebuttal. Some captions are pretty short; for example, see the captions of figure 6 and figure 7. I would recommend the authors add more description to the captions in the camera ready version of this paper.
The authors present a system agnostic interpretable method based on the idea of that provides a brief ( compressed) but comprehensive ( informative) explanation. Their system is build upon the idea of VIB. The authors compare against 3 state of the art interpretable machine learning methods and the evaluation is terms of interpretability ( human understandable) and fidelity ( accuracy of approximating black box model). Overall, all reviewers agreed that the topic of model interpretability is an important one and the novel connection between IB and interpretable data summaries is a very natural one.    This manuscript has generated a lot of discussion among the reviewers during the rebuttal and there are a number of concerns that are currently preventing me from recommending this paper for acceptance. The first concern relates to the lack of comparison against attention methods (I agree with the authors that this is a model specific solution whereas they propose a model agnostic one), however attention is currently the elephant in room and the first thing someone thinks of when thinking of interpretability. As such, the authors should have presented such a comparison. The second concern relates to the human evaluation protocol which could be significantly improved  (Why 100 samples from all models but 200 for VIBI? Given the small set of results, are these model differences significant? Similarly, assuming that we have multiple annotations per sample, what is the variance in the annotations?).  This paper is currently borderline and given reviewers  concerns and the limited space in the conference program I cannot recommend acceptance of this paper. 
This paper was a tough call. The key contribution of the paper is a genuinely useful technique for generating chemical compounds satisfying desired properties. However, there are some key issues with paper.  Reviewer *BjiD* found out that baselines are weak. Most importantly, he run thorough experiments with GraphGA, outperforming by a significant margin the baselines. With minor tweaks (e.g. enabling generating larger molecules). GraphGA achieves comparable though slightly weaker results to DST. Importantly, as pointed out by Reviewer BjiD, there is an important flaw in the experiments that some methods have a cap on the number of atoms they can add. For example, on the logP optimization task, it is possible to optimize the score by just adding carbon atoms. I would like to thank very much Reviewer for going beyond and running these experiments.  All reviewers emphasized the novelty as a key contribution. In internal discussion, I raised concern about novelty and framing of the work. One could argue that any autoregressive model (i.e. adding atoms and bonds at each step) forms a DST. One could also argue that training LSTM to produce the distribution of interest, like in [1], is also a DST because the fine tuned LSTM encodes the distribution of many molecules and is differentiable with respect to the distribution it encodes.  Despite these flaws, it is a solid contribution, which is likely to be useful for the community. Thank you for your submission, and it is my pleasure to recommend acceptance.  For the camera ready please: (a) include a well tuned GraphGA (implementing different tradeoffs of diversity and fitness), (b) include LSTM as implemented in Guacamole as baseline, (c) discuss much more clearly novelty of the work. Additionally please ensure that other baselines are not hampered by limit on number of atoms they can add.  References:  [1] Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks, Segler et al, [https://arxiv.org/abs/1701.01329](https://arxiv.org/abs/1701.01329)
The authors develop a spectral analysis on the boolean cube for the neural "conjugate kernel" (CK) and "tangent kernel" (NTK). The analysis sheds light into inductive biases of neural networks, such as whether they are biased to simple functions.    This work contains rigorous analysis and theory which is useful for further discussions. However, the theory and insights do not feel complete. One important drawback is that the analysis is limited by the boolean cube setting; this also means that it is more difficult to link theory to practical scenarios. This has been discussed a lot during the rebuttal and among reviewers. Empirical validation has attempted to deal with these concerns, but it would be useful to have this validation coming from theory, or at least have further relevant theoretical insights. This could happen by further building on the theorem provided in the rebuttal for eigenvalue behavior when d is large.
This paper proposes GAN training of a non autoregressive generator for text. To circumvent the usual problems with non differentiability of text GANs, the authors turn to Gumbel Softmax parameterisation and straight through estimation.   There are a number of aspects to this submission and they are not always clearly positioned. I will concentrate on the two aspects that seem most crucial:  1. The authors position their generator as an implicit generator, but it really isn t. If we take the continuous interpretation of the output distributions: the Gumbel Softmax transformation does correspond to a tractable density, the Concrete density of Maddison et al, with known parameter. If we take the discrete interpretation of the output distribution: Gumbel argmax is just an alternative to sampling from a Categorical distribution with known parameter. In either case, the generator maps the noise source to a collection of conditionally independent distributions each of which has a known parameter and analytical density/mass function. The authors do, however, train the architecture using a GAN type objective *as if* the generator were implicit.  2. In the discussion phase the authors added that GAN training overcomes the independence assumptions made by the generator. Whereas that makes intuitive sense, it suddenly changes the emphasis of the contributions, from proposing an implicit generator (presumably powerful for it being implicit) to proposing a way to circumvent the strong independence assumptions of the generator with a mechanism other than more traditional approximate marginalisation of VAEs. In their rebuttal, the authors commented on the use of non autoregressive VAEs in neural machine translation, and though those observations have indeed been made, they might well be specific to MT. The simplest and more satisfactory response would be to ablate the use of the GAN objective (that is, to train a non autoregressive VAE, also note that, with the same choice of likelihood, posterior collapse is rather unlikely to happen).  Other problems raised by reviewers were addressed in the rebuttal, and I would like to thank the authors for that. For example, ablating the non autoregressive generator and comparing to REINFORCE. I believe these improved the submission.   Still, I cannot recommend this version for publication. I would suggest that the authors consider careful ablations of the components they see as precisely important for the results (that currently seems to be the GAN like objective despite the model not, strictly speaking, requiring it).      
The paper addresses the interesting  many to many assignement problem between a set of images and a set of text. Most reviewers, (and I agree with them) think that the  idea and its application worth being published although the performance improvement is marginal. I request the authors to update the paper based on the discussions.
The authors propose a simple and effective technique for task specific pruning of transformer models that identifies which model components to prune by minimizing validation loss. Weaknesses of the paper include (1) related work reads more like a list and doesn’t compare and contrast the proposed approach with related work, (2) authors don’t compare to other structured pruning methods (that use different objectives) (3) lack of novelty — main difference with existing work is using validation loss to optimize and (4) one reviewer was unconvinced that the results should be possible given the approach. I share these concerns, and, in particular, I think they might be related. Given that the models are pruned using the development set (essentially equivalent to training on the development set), it seems infeasible that this approach could have been developed without looking at the testing data, and I’m concerned that this explains the unprecedentedly high accuracy compared to previous pruning approaches. At the very least, comparing to a baseline that trains on development data would be prudent in order to understand the result.
This paper studies the loss landscape of neural networks by taking into consideration the symmetries arising from the parametrisation. Specifically, given two models $\theta_1$, $\theta_2$, it attempts to connect $\theta_1$ with the equivalence of class of $\theta_2$ generated by weight permutations.  Reviewers found several strengths in this work, from its intuitive and simple idea to the quality of the experimental setup. However, they also found important shortcomings in the current manuscript, chief among them the lack of significance of the results. As a result, this paper unfortunately cannot be accepted in its current form. The chairs encourage the authors to revise their work by taking the reviewer feedback into consideration. 
This paper proposed an auxiliary loss based on mutual information for graph neural network. Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN ‘message passing’ function. GNN with edge features have already been proposed in the literature. Furthermore,  the reviewers think the paper needs to improve further in terms of explain more clearly the motivation and rationale behind the method. 
This paper gives a framework for using learning in combinatorial optimization problems.  In particular, active search is used to learn hueristics. The reviewers thought the paper had nice conceptual contributions for this approach and that the results would be very interesting to the community.
 The paper investigates how the softmax activation hinders the detection of out of distribution examples.  All the reviewers felt that the paper requires more work before it can be accepted. In particular, the reviewers raised several concerns about theoretical justification, comparison to other existing methods, discussion of connection to existing methods and scalability to larger number of classes.  I encourage the authors to revise the draft based on the reviewers’ feedback and resubmit to a different venue.  
The paper presents a PAC Bayesian approach for meta learning that utilizes information of the task distribution in the prior. The presented localized approach allows the authors to derive an algorithm directly from the bound   this is a worthwhile contribution. Nevertheless there are several concerns that were raised by the reviewers and in its current form the work is not ready to appear in ICLR.    
This paper considers ensemble of deep learning models in order to quantify their epistemic uncertainty and use this for exploration in RL. The authors first show that limiting the ensemble to a small number of models, which is typically done for computational reasons, can severely limit the approximation of the posterior, which can translate into poor learning behaviours (e.g. over exploitation). Instead, they propose a general approach based on hypermodels which can achieve the benefits of a large ensemble of models without the computational issues. They perform experiments in the bandit setting supporting their claim. They also provide a theoretical contribution, proving that an arbitrary distribution over functions can be represented by a linear hypermodel.  The decision boundary for this paper is unclear given the confidence of reviewers and their scores. However, the tackled problem is important, and the proposed approach is sound and backed up by experiments. Most of reviewers concerns seemed to be addressed by the rebuttal, with the exception of few missing references which the authors should really consider adding. I would therefore recommend acceptance.
The paper proposes a unique network architecture that can learn divide and conquer strategies to solve algorithmic tasks, mimicking a class of standard algorithms.  The paper is clearly written, and the experiments are diverse.  It also seems to point in the direction of a wider class of algorithm inspired neural net architectures.
The paper studies the mismatch between value estimation in RL from finite vs. infinite trajectories. This is an interesting problem, but the reviewers raised concerns regarding (1) the consistency and coherence of the story (2) the significance of theoretical analysis and (3) significance of the results. I appreciate that the authors made significant changes to the paper to address the comments. However, given the extent of changes, I think another review cycle is needed to check the details of the paper again.
The paper formalizes domain adaptation by taking the causal (generative) direction of dependencies p(image | class, domain).  They evaluate an ELBO surrogate loss by fitting a reverse q function that is new for this setup, and add a term to the loss that induces independence between class and domain. The paper also  proves identifiability conditions. The approach is then evaluated on two semi synthetic and small datasets, showing some improvement.   Reviewers were concerned about presentation and the experimental validation. The authors addresses some of the concerns in their rebuttal, but several reviewers found that the experimental evidence was still lacking, and that the authors should evaluate their approach in more standard and realistic benchmark datasets.  As a result, the paper cannot be accepted in its current form       
The manuscript studies a random matrix approach to recover sparse principal components. This work extends prior work using soft thresholding of the sample covariance matrix to enable sparse PCA. In this light, the main contribution of the paper is a study of generalizing soft thresholding to a broader class of functions and showing that this improves performance. The contributions of this paper are primarily theoretical.  The reviewers and AC note issues with the discussion that can be further improved to better illustrate contributions, and place this work in context. In particular, multiple reviewers assumed that "kernel" referred to the covariance matrix. The authors provide a satisfactory rebuttal addressing these issues.  While not unanimous, overall the reviewers and AC have a positive opinion of this paper and recommend acceptance.
The author propose a method called global momentum compression for sparse communication setting, and provided some theoretical results on the convergence rate. The convergence result is interesting, but the underlying assumptions used in the analysis appear very strong. Moreover, the proposed algorithm has limited novelty as it is only a minor modification. Another main concern is that the proposed algorithm shows little performance improvement in the experiments. Moreover, more related algorithms should be included in the experimental comparison.
This manuscript proposes strategies to improve both the robustness and accuracy of federated learning. Two proposals are online reinforcement learning for adaptive hyperparameter search, and local distribution matching to synchronize the learning trajectories of different local models.   The reviewers and AC agree that the problem studied is timely and interesting, as it addresses known issues with federated learning. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. Taken together, the AC s opinion is that the paper may not be ready for publication.
The authors propose a novel operator splitting method for solving convex relaxations of neural network verification problems, and develop and validate an optimized implementation of the same on large scale networks, focusing on the problem of verifying robustness to norm bounded adversarial perturbations.  The reviewers agree that the paper contains interesting ideas that are worthy of further development and that these ideas may prove useful eventually in pushing the envelope of what is possible in neural network verification. However, in its current form, the paper misses some key experimental evidence to rigorously evaluate the value of the contributions made: 1) Comparison against SOTA incomplete verifiers: The authors do not provide detailed and rigorous comparisons against well known baselines (for example the incomplete verifiers from Fast and complete (Xu et al., 2021), Beta CROWN(Wang et al. 2021))  2) Incorporating tighter relaxations: It would be valuable for the community to understand whether the proposed algorithm is compatible with tighter relaxations like those of (Tjandraatmadja et al., 2020). Even if they are not, it would be interesting to understand the comparison against standard solvers for these tighter relaxations compared against the advanced solver developed by the authors applied to the weaker relaxation. 3) Showing performance in the context of complete verification: While this is not a requirement, it would be great to see how the method performs in the conjunction with a branch and bound search, as this sometimes reveals surprising tradeoffs or weaknesses of incomplete verifiers (as observed in the results of Beta CROWN(Wang et al. 2021)).  I encourage the authors to strengthen the paper adding these experiments and resubmit to a future venue.
The submission addresses the problem of whether or not to update weights for a previous task in continual learning.  The approach is to specify a trust region based on task similarity and update weights only in the direction of the tasks that are similar enough to the current one.  The paper was on the balance well received (3/4 reviewers recommended acceptance, 2 with scores of 8) and complemented for its simple but effective approach, and good discussion of related literature.  The submission attracted a reasonable amount of engagement and discussion between reviewers and authors, which should be taken into account in the final version of the paper.
**Overview**:  The paper tries to answer which mutual information (MI) objective is sufficient  for representation learning (repL) in reinforcement learning (RL). Three common objectives are considered: forward, state, and inverse. The paper shows that only the forward objective is sufficient for learning, i.e., sufficient for learning of optimal policy/value function. The authors also demonstrate this phenomena using empirical experiments.  **Quality, Clarity, Originality and Significance**:  All the reviewers believe this paper is novel in terms of methodology, i.e., evaluate the sufficiency of the repL in terms of down stream tasks. However, there is a lack of clarity in the experiment sections. The authors have provided more details in the rebuttal phase. The reviewers also have concerns that this paper may be too far from typical experimental settings to have a real impact on the field. An unofficial review pointed out there is a mistake in the proof of the paper. The authors later also confirmed the flaw and claimed it is fixed.  **Recommendation**: The paper is indeed interesting and novel. However, the impact to the practice community might not be significant. That being said, the paper should warrant publication eventually. However, the authors changed large amount of text about the proofs before and after rebuttal, which also introduced some additional typos, confusions, and also technique sloppiness or flaws. The reviewers are concerned about this. Overall I believe that the paper is not in a state to be published yet. 
This paper proposes an approach to regularizing classifiers based on invertible networks using concepts from the information bottleneck theory. Because mutual information is invariant under invertible maps, the regularizer only considers the latent representation produced by the last hidden layer in the network and the network parameters that transform that representation into a classification decision. This leads to a combined ℓ1 regularization on the final weights, W, and ℓ2 regularization on W^{T} F(x), where F(x) is the latent representation produced by the last hidden layer. Experiments on CIFAR 100 image classification show that the proposed regularization can improve test performance. The reviewers liked the theoretical analysis, especially proposition 2.1 and its proof, but even after discussion and revision wanted a more careful empirical comparison to established forms of regularization to establish that the proposed approach has practical merit. The authors are encouraged to continue this line of research, building on the fruitful discussions they had with the reviewers.
The paper proposes NAFS (Node Adaptive Feature Smoothing), which constructs node representations by only using smoothing without parameter learning.  The authors first provide a formulation for the smoothing operator. They then define over smoothing distance to assess how much a node is close to the stationary state. Finally, they use the over smoothing distance to calculate a smoothing weight for each node. Experiments are conducted to verify the efficacy.  Strength * The paper tackles the problem of over smoothing, which is a well known issue in GNN. * The solution appears to be effective.  * The paper is generally clearly written.  Weakness * The novelty and significance of the work might not be enough. Aspects of the contributions exist in prior work.     Additional experiments have been conducted during the rebuttal. The reviewers appreciate the efforts.  After rebuttal：  Reviewer SHxg increased the score accordingly.  Reviewer w2Qg says “Given the concerns proposed by the other reviewers, I adjusted my score.”  Reviewer YM4P says “I read the rebuttal and slightly increased my score.”
This paper presents modifications to the adversarial training loss that yield improvements in adversarial robustness.  While some reviewers were concerned by the lack of mathematical elegance in the proposed method, there is consensus that the proposed method clears a tough bar by increasing SOTA robustness on CIFAR 10. 
This paper presents a defense scheme for adversarial attacks, called self supervised online adversarial purification (SOAP), by purifying the adversarial examples at test time. The novelty of this work is in its incorporation of self supervised representation learning into adversarial defense through purification via optimizing an auxiliary self supervised loss. This is done by jointly training the model on a self supervised task while it is learning to perform the target classification task in a multi task learning setting. Compared with existing adversarial defense schemes such as adversarial training and purification techniques, SOAP has a lower computation overhead during the training stage.  **Strengths:**   * It is novel to incorporate self supervised learning for adversarial purification at test time.   * SOAP’s training stage based on multi task learning incurs low computation overhead compared with the original classification task.  **Weaknesses:**   * Although the proposed adversarial defense scheme is computationally cheaper than the other existing methods during the training stage, it does incur some overhead during test time. This may be undesirable for some applications in which efficiency during test time is an important factor to consider.   * The choice of a suitable self supervised auxiliary task is somewhat ad hoc. The performance varies a lot for different auxiliary tasks.   * The experimental evaluation is only based on relatively small and unrealistic datasets even after new experiments on CIFAR 100 have been added by the authors.  It is said in the paper that SOAP can exploit a wider range of self supervised signals for purification and hence conceptually can be applied to any format of data and not just images, given an appropriate self supervised task. However, this claim has not been substantiated in the paper using non image data.  Despite some limitations and that some claims still need to be better substantiated, the paper presents some novel ideas which are expected to arouse interest for follow up work in the adversarial attack and defense research community. 
In this paper, the authors propose to adapt the recent paper by Yu et al. (ICML 2020), namely FedAwS. In that paper, the authors solved a potential failure mode in federated learning, when all the users only have access to one class in their devices. In this paper, the authors extend FedAwS to a setting in which federated learning is used for User Verification (UV), namely FedUV. The authors argue that the previous paper could not be the solution to learning UV because FedAwS share the embedding vectors with the server.   The authors then show a procedure in which they can learn a classifier in which the embedding vectors are not needed to be shared with the classifier. They use error correcting codes to make the mapping sufficiently different and that allows the training to succeed without sharing the embedding. The proposed change is only marginally worse than FedAwS and centralized learning. This is the part of the paper that has attracted positive comments and is praised by all the reviewers.   The authors take as given that by not sharing the embedding vectors and by using randomly generated error correcting codes, the whole procedure is privacy preserving and secure. The 4th reviewer indicates that these guarantees need to be proven and points out several references that hint toward flaws in the argument by the authors. Reviewer 4th does say that not sharing the embeddings might not be enough, but that self evident arguments are not enough.   This paper provides a significant improvement for a federated machine learning algorithm that deserves publication, but the rationale of the paper is flawed from a privacy and security viewpoint. I think if the paper is published as is, especially with the proposed title, it will create a negative reaction by the security and privacy community for not adhering to their standards. We cannot lower those standards.    I suggest to the authors that they can follow two potential paths for publishing this work:   1 Change the scope of their algorithm. For example, I can imagine that by not sharing the embedding the communication load with the server might be significantly reduced or that adding new users with new classes can be easier.   2 Follow the recommendation from Reviewer 4 and show that the proposed method is robust against the different attacks.   Minor comments:   For a paper that is trying to solve the AU problem, I would expect a discussion about why learning is better than a private algorithm. In a way, learning is sharing, and that increases the risk of mischief by malicious users.      The discussion about error correcting codes and the minimum distance is quite old fashion. In high dimensions, the minimum distance is not the whole story. LDPC codes make sense when we stop focusing on minimum distance codes and minimum distance decoding. I would recommend having a look at the Berlekamp’s Bat discussion in David MacKay’s book (Chapter 13).
Thanks for clarifying several issues raised by the reviewers, which helped us understand the paper.  After all, we decided not to accept this paper due to the weakness of its contribution. I hope the updated comments by the reviewers help you strengthen your paper for potential future submission.
This paper investigates improving robustness to adversarial examples by using mode connectivity in the loss function. The paper received three reviews by experts working in related areas. In a strongly positive review, R1 recommends Accept, but gives some specific technical questions. The authors submitted a response to these questions; in post review comments, R1 was satisfied and maintained the highly positive review. R2 recommended Weak Reject and also asked specific technical questions, including some additional details on experiments, statistical significance, etc. The author response also convincingly responded to these concerns. R3 recommended Weak Accept but suggested improving the writing, which authors have done in their revision. Given that R1 and R3 are highly positive and R2 s concerns were addressed in the response and revision, we now recommend (weak) Accept.
This paper proposes a generalization of the translation style embedding approaches for link prediction to Riemannian manifolds. The reviewers feel this is an important contribution to the recent work on embedding graphs into non Euclidean spaces, especially since this work focuses on multi relational links, thus supporting knowledge graph completion. The results on WN11 and FB13 are also promising.  The reviewers and AC note the following potential weaknesses: (1) the primary concern is the low performance on the benchmarks, especially WN18 and FB15k, and not using the appropriate versions (WN18 RR and FB15k 237), (2) use of hyperbolic embedding for an entity shared across all relations, and (3) lack of discussion/visualization of the learned geometry.  During the discussion phase, the authors clarified reviewer 1 s concern regarding the difference in performance between HolE and ComplEx, along with providing a revision that addressed some of the clarity issues raised by reviewer 3. The authors also justified the lower performance due to (1) they are focusing on low dimensionality setting, and (2) not all datasets will fit the space of the proposed model (like FB15k). However, reviewers 2 and 3 still maintain that the results provide insufficient evidence for the need for Riemannian spaces over Euclidean ones, especially for larger, and more realistic, knowledge graphs.  The reviewers and the AC agree that the paper should not be accepted in the current state. 
This paper studies why input gradients can give meaningful feature attributions even though they can be changed arbitrarily without affecting the prediction. The claim in this paper is that "the learned logits in fact represent class conditional probabilities and hence input gradients given meaningful feature attributions". The main concern is that this claim is verified very indirectly, by adding a regularization term that promotes logits learning class conditional probabilities and observing that input gradient quality also improves. Nevertheless, there are interesting insights in the paper and the questions it asks are very timely and important, and overall, it could have a significant impact on further research in this area.
This paper proposes improving human interpretability and manipulability of neural representations by obtaining syntactic roles (here, subject, object, prepositional object, and main verb) without supervision by means of them becoming linked to latent variables in a novel proposed attention driven VAE (ADVAE) model, which provides cross attention between a language transformer and latent variables. The paper argues that syntactic roles are quite central to meaning interpretation and that the ADVAE recovers them better than LSTM or Transformer (with mean pooling) VAEs.  This is a quite interesting direction and paper. There was active discussion with the reviewers, one of whom (9pDc) moved their rating from reject to quite strong support, while the other reviewers either sat on the fence or raised from reject to borderline. Nevertheless, I overall tend to agree that the paper is still lacking in empirical support, a view clearly shared by reviewers WuPD and 7uFL. The SNLI data is very simple descriptive sentences, nearly all in the form of S V O or S V PP. Would this work on more complex data, in other languages, or with more word order variation? There isn t very much investigation, but the new results added during reviewing based on Yelp data seem to offer more concerns than confidence. These are also very short sentences but with more varied structure and some complementation. It seems like D_{dec} is now very low (much lower than for the sequence VAE), the ability to distinguish out grammatical roles seems limited to {subj} vs. {dobj, pobj} in the encoder and none at all in the decoder (Figure 6/7). And then for the examples in Appendix D, the disentanglement abilities barely seem stronger than being able to pick out subjects, though when there are sentences with subordinate clauses, it is perhaps random which subject you get. The resampled realizations in appendix H also seem to show limited disentanglement: resampling the subject usually seems to change the object as well, often markedly. No convincing downstream applications are shown. As such, while I agree that disentanglement is at the heart of representation learning, I can t get on board with reviewer 9pDc feeling that this paper now has convincing results. Reviewer 7uFL also emphasizes that there is no strong reason that the latent variables have to align with syntactic roles. In particular, the motivation in NMT whereby constituents clump and reorder together does not exist here. It may only work for the very simple and regular sentences of SNLI.  Hence, overall, I feel that this method needs more extensive validation on harder, more varied data sets before it becomes a convincing contribution, and so I propose rejecting the paper at this point in time. Nevertheless, I do think the topic is interesting and this approach has the potential to be good.
 pros:   The paper presents an interesting forward chaining model which makes use of meta level expansions and reductions on predicate arguments in a neat way to reduce complexity.  As Reviewer 3 points out, there are a number of other papers from the neuro symbolic community that learn relations (logic tensor networks is one good reference there). However using these meta rules you can mix predicates of different arities in a principled way in the construction of the rules, which is something I haven t seen.   The paper is reasonably well written (see cons for specific issues)   There is quite a broad evaluation across a number of different tasks.  I appreciated that you integrated this into an RL setting for tasks like blocks world.   The results are good on small datasets and generalize well  cons:   (scalability) As both Reviewers 1 and 3 point out, there are scalability issues as a function of the predicate arity in computing the set of permutations for the output predicate computation.   (interpretability) As Reviewer 2 notes, unlike del ILP, it is not obvious how symbolic rules can be extracted.  This is an important point to address up front in the text.    (clarity) The paper is confusing or ambiguous in places:   Initially I read the 1,2,3 sequence at the top of 3 to be a deduction (and was confused) rather than three applications of the meta rules.  Maybe instead of calling that section "primitive logic rules" you can call them "logical meta rules".   Another confusion, also mentioned by reviewer 3 is that you are assuming that free variables (e.g. the "x" in the expression "Clear(x)") are implicitly considered universally quantified in your examples but you don t say this anywhere.  If I have the fact "Clear(x)" as an input fact, then presumably you will interpret this as "for all x Clear(x)" and provide an input tensor to the first layer which will have all 1.0 s along the "Clear" relation dimension, right?   It seems that you are making the assumption that you will never need to apply a predicate to the same object in multiple arguments?  If not, I don t see why you say that the shape of the tensor will be m x (m 1) instead of m^2.  You need to be able to do this to get reflexivity for example: "a <  a".   I think you are implicitly making the closed world assumption (CWA) and should say so.   On pg. 4 you say "The facts are tensors that encode relations among multiple objectives, as described in Sec. 2.2.".  What do you mean by "objectives"?  I would say the facts are tensors that encode relations among multiple objects.   On pg. 5 you say "We finish this subsection, continuing with the blocks world to illustrate the forward propagation in NLM".  I see no mention of blocks world in this paragraph. It just seems like a description of what happens at one block, generically.   In many places you say that this model can compute deduction on first order predicate calculus (FOPC) but it seems to me that you are limited to horn logic (rule logic) in which there is at most one positive literal per clause (i.e. rules of the form: b1 AND b2 AND ... AND bn  > h).  From what I can tell you cannot handle deduction on clauses such as b1 AND b2  > h1 or (h2 and h3).   There is not enough description of the exact setup for each experiment. For example in blocks world, how do you choose predicates for each layer?  How many exactly for each experiment?  You make it seem on p3 that you can handle recursive predicates but this seems to not have been worked out completely in the appendix.  You should make this clear.   In figure 1 you list Move as if its a predicate like On but it s a very different thing. On is  predicate describing a relation in one state.  Move is an action which updates a state by changing the values of predicates.  They should not be presented in the same way.   You use "min" and "max" for "and" and "or" respectively.  Other approaches have found that using the product t norm t norm(x,y)   x * y helps with gradient propagation.  del ILP discusses this in more detail on p 19.  Did you try these variations?   I think it would be helpful to somewhere explicitly describe the actual MLP model you use for deduction including layer sizes and activation functions.   p. 5. typo: "Such a parameter sharing mechanism is crucial to the generalization ability of NLM to problems ov varying sizes." ("ov"  > "of")   p. 6. sec 3.1 typo: "For ∂ILP, the set of pre conditions of the symbols is used direclty as input of the system." ("direclty"  > "directly")  I think this is a valuable contribution and novel in the particulars of the architecture (eg. expand/reduce) and am recommending acceptance.  But I would like to see a real effort made to sharpen the writing and make the exposition crystal clear.  Please in particular pay attention to Reviewer 3 s comments.  
This paper proposes a simple extension to BERT like pre training for source code models, which allows incorporation of data flow information. This is a new way of incorporating code structural information into models, and it appears practical and effective. Reviewers are all in favor of accepting the paper.
The paper proposes a new recurrent architecture based on discretization of ODEs which allow for learning multi scale representations and help with the vanishing gradient problem. The reviewers all agree this architecture is novel and provide substantial theoretical and empirical evidence. A strong accept.
This paper proposes to train latent variable models (VAEs) based on diffusion maps on the data manifold. While this is an interesting idea, there are substantial problems with the current draft regarding clarity, novelty and scalability. In its current form, it is unlikely that the proposed model will have  a substantial impact on the community.
Two reviewers are concerned about this paper while the other one is slightly positive. A reject is recommended.
This paper is a timely application of linear algebra to propose a method for reducing catastrophic interference by training a new task in a subspace of the parameter space using conceptors. The conceptors are deployed in the backprop, making this a valuable alternative to recent continual learning methods such as EWC. The paper is clearly written and the results give a clear validation of the method. The reviewers agree as to the merits of the paper.
The submission performs empirical analysis on f VIM (Ke, 2019), a method for imitation learning by f divergence minimization. The paper especially focues on a state only formulation akin to GAILfO (Torabi et al., 2018b). The main contributions are: 1) The paper identifies numerical proplems with the output activations of f VIM and suggest a scheme to choose them such that the resulting rewards are bounded. 2) A regularizer that was proposed by Mescheder et al. (2018) for GANs is tested in the adversarial imitation learning setting. 3) In order to handle state only demonstrations, the technique of GAILfO is applied to f VIM (then denoted f VIMO) which inputs state nextStates instead of state actions to the discriminator.  The reviewers found the submitted paper hard to follow, which suggests a revision might make more apparent the author s contributions in later submissions of this work. 
The paper had four borderline reviews with none enthusiastic about championing the merits of the paper. While it was felt that the extension of an existing technique to deep learning via amortization is a useful procedure, it is also not very novel and the experiments didn t demonstrate a significant leap in performance.
This paper proposes mixed distributions over convex polytopes, and provides theory for mixed distributions that is relevant to the machine learning community. All of the reviewers were positive, and agree that this is a solid contribution. I agree, and I believe that this paper stands a chance of being a foundational paper for future work in probabilistic ML and structured learning.
This paper presents a new algorithm for distributed multivariate mean estimation. This method performs significantly better than previous approaches when the input vectors have large norm but are relatively close to each other. The approach relies on lattices and randomized rounding. The approach is evaluated experimentally as well. Overall, there is consensus among the reviewers that this work solves a clean problem using non trivial ideas. I recommend accepting the paper.
This paper proposes an efficient method for message passing that can incorporate structural information that is provably stronger than 1 WL. As compared to three strands of provably powerful (more than 1 WL) GNNs, the method has limited additional computational overhead, and can also show encouraging results on the over smoothing problem. Overall speaking, all the reviewers like this paper quite a lot, although the also raised some minor concerns. The paper also attracted some unofficial reviewers who provided quite a few related works. The authors did a good job in interacting with the reviewers and addressing their minor concerns. So, we believe the paper is worth accepting, and could be a significant work in the field of graph neural networks.
This paper proposes SH LDM, which approximates the LDM model with a hierarchy of clusters. The authors should discuss the details about clustering and how this algorithm can benefit from sparsity in a more rigorous language.    The authors should review the rich literature on scaling up distance based methods such as kNN and kernel methods, which this paper belongs to. The title is also misleading; the paper mainly discusses scalable link prediction rather than learning new embeddings.  The reviewers have raised several questions about the experiments. For example, the main results should be a table for comparing the speed rather than the accuracy of the algorithms. Also, the original LDM should be included in the accuracy tables. The settings in the experiments, such as embedding dimensions, are not appropriate for large graphs.
This paper proposes a Graph Neural Network model to estimate latent dynamics in the human brain using functional Magnetic Resonance Imaging (fMRI) and Diffusion Weighted Imaging (DWI). The representation is tested on a classification task. While reviewers acknowledge the importance of this application, various concerns have been raised and partially addressed. The work focuses on graph deep learning and offers limited evidence of its superiority over more traditional ML or non graph based deep learning. Besides the methodological novelty is unclearly argued, which is not ideal for the audience of a conference like ICLR.  For all these reasons, this work cannot be endorsed for publication at ICLR 2022.
The reviewers of this paper agreed that it has done a stellar job of presenting a novel and principled approach to attention as a latent variable, providing a new and sound set of inference techniques to this end. This builds on top of a discussion of the limitations of existing deterministic approaches to attention, and frames the contribution well in relation to other recurrent and stochastic approaches to attention. While there are a few issues with clarity surrounding some aspects of the proposed method, which the authors are encouraged to fine tune in their final version, paying careful attention to the review comments, this paper is more or less ready for publication with a few tweaks. It makes a clear, significant, and well evaluate contribution to the field of attention models in sequence to sequence architectures, and will be of great interest to many attendees at ICLR.
The paper proposes a symmetry informed neural network for modelling many body systems. The network is empirically evaluated in the tasks of predicting Newtonian trajectories and molecular conformations.  All four reviewers are critical of the paper and recommend rejection (one weak, three strong). The reviews have flagged weaknesses and quality issues with several aspects of the submission, including the proposed methodology, the novelty of the contribution, and the clarity of the presentation. Although detailed clarifications were provided by the authors, most of the reviewers  concerns remain, and the consensus among reviewers remains to reject the paper.  Consequently, the current version of the paper does not appear to meet the quality standards for acceptance to ICLR.
The paper is concerned with improving the scalability of GCNs which is an important problem and relevant to the ICLR community. For this purpose, the authors propose a new distributed training method for GCNs which uses a boundary sampling strategy to reduce the number of boundary nodes. The paper is written well and, overall, good to follow. Reviewers highlight the promising improvements in throughput and memory footprint as well as and the possibility to avoid potential loss of information compared to other methods.  However, currently there exist still concerns around the manuscript. Reviewers raised concerns with regard to the novelty of the method (straightforward combination of existing techniques) as well as the experimental evaluation (overhead of METIS, full batch accuracy, edge boundary vs node boundary, etc.) The revised version addresses some concerns (comparison to DropEdge, p 0, etc.) and clearly improves the paper. However, given the aforementioned issues and the absence of strong support from reviewers, I agree to that the current version would require an additional revision to iron out these points. The presented results are indeed promising and I d encourage the authors to revise and resubmit their work considering the reviewers  feedback.
Strength * The paper is relatively clearly written. * The proposed method appears to be sound.  Weakness * The novelty of the work seems to be limited.   * The experiment part needs significant improvements.  The comparison with existing methods may not be fair.  Evaluation of efficiency should be given. There are also detailed investigations that need to be conducted, as indicated by the reviewers. * There are technical issues that need to be addressed.
This submission proposes a method for learning convolutional filters with trainable size, that builds on top of multiplicative filter networks.  Anti aliasing is achieved by parametrization with anisotropic Gabor filters.  The reviewers were unanimous in their opinion that the paper is suitable for acceptance to ICLR.  The authors are encouraged to make use of the extensive reviewer discussion in improving the final version of the paper.
Addressing the problem of catastrophic forgetting in continual learning, this paper extends OML to use experience replay (ER) during training, instead of the original approach which uses ER during test phase only. The paper proposes a policy for samples replacement from the reservoir. Experiments show the superiority of the approach in three standard benchmarks compared with several baselines.   Reviewers were unanimously concerned that the technical contribution of the paper is not sufficient. The authors addressed several issues, including experiments to compare with additional baselines, but the technical novelty remains limited for an ICLR publication.  The paper cannot be accepted at its current form.
This paper gives a way to learn one hidden layer neural networks on when the input comes from Gaussian mixture model. The main algorithm uses [Janzamin et al. 2014] as an initialization and then performs gradient descent. The main contribution of this paper is 1. to give a characterization of sample complexity for estimating the moment tensors when the input distribution comes from a mixture of Gaussian; 2. to give a local convergence result when the samples come from a mixture of Gaussian. The paper claims certain behavior in the input data would make the problem harder and slow down the convergence, although the claim is based on an upperbound and would be stronger if there is some corresponding lowerbound.
This paper tackles the important problem of endowing deep RL agents with added interpretability. Action values are decomposed as the combination of GVFs learned on externally specified features, offering action explanations in terms of discounted future returns in the space of interpretable quantities. Reviewers praised the approach, as well as the level of detail for reproducibility purposes. R3 had concerns about the generality of the method but follow up experiments have allayed these concerns. Given the reviewer response and the central importance of the problem considered to the field, I can wholeheartedly recommend acceptance.
This paper proposes a modification of SGD to do distributionally robust optimization of deep networks.  The main idea is sensible enough, however, the inadequate handling of baselines and relatively toy nature of the experiments means that this paper needs more work to be accepted.
The paper presents attacks against sentiment recognition NLP systems using specially crafted homoglyphs. The novelty of the proposed method is, however, marginal; contributions over some of the related work are unclear. Furthermore, the quality of writing is insufficient. The authors provided no response to reviewers  comments.
The paper proposes a modification to improve adversarial invariance induction for learning representations under invariance constraints. The authors provide both a formal analysis and experimental evaluation of the method. The reviewers generally agree that the experimental evaluation is rigorous and above average, but the paper lacks clarity making it difficult to judge the significance of it. Therefore, I recommend rejection, but encourage the authors to improve the presentation and resubmit.
This paper proposes a new way of comparing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images, i.e. replacing the conventional test set based evaluation methods with a more flexible mechanism. The main proposal is to build a test set adaptively in a manner that captures how classifiers disagree, as measured by the wordnet tree. As noted by R2, this work has the potential to be of interest to a broad audience and can motivate many subsequent works.   While the reviewers acknowledged the importance of this work, they raised several concerns: (1) the proposed approach is immature to be considered for benchmarking yet (R1,R4), (2) selecting k and studying its influence on the performance ( R1, R3, R4), (3) the proposed approach requires data annotation which might not be straightforward   (R3, R4).  The authors provided a detailed rebuttal addressing the reviewer concerns.  There is reviewer disagreement on this paper. The comments from R3 were valuable for the discussion, but at the same time too brief to be adequately addressed by the authors. The comments from emergency reviewer were helpful in making the decision. AC decided to recommend acceptance of the paper seeing its valuable contributions towards re thinking the evaluation of current SOTA models. 
This paper proposes an approach for multi label text classification. The method constitutes appending few "label" tokens to the beginning of the text input instead of the traditional single <CLS> token. The paper shows improvements over a competitive baseline on two datasets.  Reviewers agree that the novelty and contribution of the paper are marginal. The method of appending extra "fake" tokens has been used in other works as a "trick". It is also unclear how adding a few extra tokens allow for the model to represent label dependencies better.  The authors did not respond to the reviews, so there was no further dicussion.
The paper shows empirically that training unstructured sparse networks from random initialization performs poorly as sparse NNs have poor gradient flow at initialization. Besides, the authors argue that sparse NNs have poor gradient flow during training. They show that DST based methods achieving the best generalization have improved gradient flow. Moreover, they find the LTs do not improve gradient flow, rather their success lies in re learning the pruning solution they are derived from. I read the paper and the reviewers discussed the rebuttal. Although all the reviewers found the rebuttal helpful and they all agree that the paper is decently well written and has some clear value, the majority believes that further observations are required for making the paper and its hypothesis convincing. There are also some recent related work on initialization of pruned networks, e.g. by rescaling their weights at initialization. I believe, adding the discussion of such related techniques and making the connection to existing work will greatly strengthen the paper and provides more evidence to support its claims.   
The paper proposes a sampling technique for unnormalized distributions. The main idea is to gradually transform particles by following the gradient flow of the relative entropy in the Wasserstein space of probability distributions. The paper tackles an important problem and provides an interesting new perspective. However, even putting aside the concerns on the theoretical analysis raised by the reviewers, the experimental evaluations does not seem sufficient to demonstrate the benefits of the proposed approach.
This work develops importance weighted autoencoder like training but with sequential Monte Carlo.  The paper is interesting, well written and the methods are very timely (there are two highly related concurrent papers    Naesseth et al. and Maddison et al.).  Initially, the reviewers shared concerns about the technical details of the paper, but the authors appear to addressed those and two reviews have been raised accordingly.   There is one outlier review (two 7s and one 3).  The 3 is the least thorough and has the lowest confidence (2) so that review is being weighted accordingly.  This appears to be a timely and interesting paper that is interesting to the community and warrants publication at ICLR.  Pros:   Well written and clear   An interesting approach   Neat technical innovations   Generative deep models are of great interest to the community (e.g. Variational Autoencoders)  Cons:   Could include a better treatment of recent related literature   Leaves a variety of open questions about specific details (i.e. from the reviews)
This paper adapts a method called "real time recurrent learning" for training recurrent neural networks. The idea is to project the true gradient onto a subspace of desired dimensionality along a candidate direction. There are a variety of possible candidates: random directions, backpropagation through time, meta learning approaches, etc.   The main strength of the paper is that it is a very simple idea that seems to have practical utility.  While often presented in different contexts, it should be clearly noted by the authors that the general idea of using low dimensional directional derivatives for computational efficiency is fairly common in optimization. Reviewers mention sketch and project methods. This has also been looked, for example, in the context of Bayesian optimization, with [random selection](https://bayesopt.github.io/papers/2016/Ahmed.pdf) and [value of information based](https://proceedings.neurips.cc/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120 Paper.pdf) criteria.   Reviewers appreciated aspects of the paper, though had concerns about relations to sketch and project methods, computational costs, and experimental demonstrations and baselines. Through the rebuttal period, reviewers were mostly satisfied that the concerns about computational costs were well addressed. A better job could still be done about describing relation to other work. There was also still some desire for more thorough experimental demonstrations and consistent baselines, as described in the reviews. The paper also could use some additional proof reading as it contains several grammatical errors. On the whole, the paper makes a nice simple practical contribution. Please carefully account for reviewer comments in updated versions.
The reviewers unanimously recommend rejecting this submission, and I concur with that recommendation. This submission is not appropriate for a machine learning conference like ICLR. It does not display a thorough understanding of the literature nor does it make a sufficiently valuable contribution. There is no need to "generalize" MLPs, the community knows quite well that we can use dropout, skip connections, and batch norm with them. Even the original dropout paper applies dropout on fully connected ReLU MLPs.  As another example, the submission attributes skip connections to He et al. 2016, but skip connections (also known as "shortcut connections" were in common use in the late 1980s and throughout the 1990s in the Connectionist community, including for non convolutional simple feedforward neural networks or "MLPs". They were a well known technique throughout neural network history, although the advent of deeper layered neural network architectures perhaps gave them new importance. He et al. certainly popularized them for modern neural network architectures and popularized their residual formulation. The earliest reference I could find easily for skip connections was "Learning to Tell Two Spirals Apart" which was published in 1988 by Kevin J. Lang and Michael. J. Witbrock, but in general such architectural tricks were not viewed as particularly remarkable in the 1990s neural networks literature.
This paper introduces a method to reduce the number of unique weights in a network. The motivation is that this reduces energy consumption, and can lead to speed ups. The reviewers agree that the paper is generally well written. They also agree that the method presented is interesting and useful to reduce the number of unique weights of the network. However, reviewers challenge the authors claim that fewer unique weights necessarily lead to lower energy consumption. Operations involved with this method may even lead to more memory transfers. The experimental section supports well the claim that the number of unique weights is decreased at very little cost in terms of accuracy, but does not provide much data in terms of actual energy consumption. Generally, the reviewers agree that the fundamental idea of this paper is good and should be presented, but since this paper motivation is entirely focused on energy consumption, I suggest that the authors revise the paper and submit it in a future venue. I can see two possible ways to revise the paper: 1) Add actual energy consumption data to the experimental section; or 2) change the motivation of the paper to solely focus on reducing the number of unique weights. It may be useful for some niche applications, or as a starting point for future works.
After the rebuttal, the reviewers agree that this paper would benefit from further revisions to clarify issues regarding the motivation of the DP based security definition,  any relationship it may have to standard definitions of privacy, and the role of dimensionality in the theoretical guarantees.
While the proposed method is novel, the evaluation is not convincing. In particular, the datasets and models used are small. Susceptibility to adversarial examples is tightly related to dimensionality. The study could benefit from more massive datasets (e.g., Imagenet).
The submitted paper considers the very interesting problem of imitation learning from observations under transition model disparity. The reviewers recommended 2x weak accept and 1x weak reject for the paper. Main concerns about the paper regarded clarity of the presentation, complicatedness of the proposed method, and experimental validation. During the discussion phase, the authors addressed some of the comments and provided an update of the paper providing additional details. While some of the reviewers  concerns still stand, I think the addressed problem is very relevant and the proposed method can be (with clarifications and improvements of the presentation) be interesting to parts of the community. Hence I am recommending acceptance of the paper. Nevertheless, I strongly urge the authors to carefully revise their paper, and taking the reviewers  concerns carefully into account when preparing the camera ready version of the paper.
The paper considers the problem of learning interpretable, low dimensional representations from high dimensional multimodal input via weak supervision in a learning from demonstration (LfD) context. To mitigate the disparity between the abstractions that humans reason over and the robot s low level action and observation spaces, the paper argues for learning a low dimensional embedding that captures the underlying concepts. The primary contribution of the paper is the ability to learn disentangled low dimensional representations that are interpretable from weak supervision using conditional latent variable models.  The paper was reviewed by three knowledgeable referees, who read the author response and discussed the paper. The paper considers a challenging problem in learning from demonstration, namely dealing with the disparity that exists between the ways in which humans and robots model and observe the world, a problem that is exacerbated when reasoning over high dimensional multimodal observations. As the reviewers note, the use of variational inference to learn low dimensional interpretable representations from weak supervision is compelling. The primary concerns are that the contributions need to be more clearly scoped and that the experimental evaluation is a bit narrow. The authors make an effort to resolve some of these issues, in part through the inclusion of an additional experiment that considers pouring tasks. However, the extent to which this second task mitigates concerns about the narrow evaluation is not fully clear. The paper would be strengthened by the inclusion of experiments in a less contrived setting (and one for which the concepts are not necessarily disjoint) as well as a clearer discussion of the primary contributions.
The paper presents an interesting model to sample from the Gibbs distribution using diffusion based method. The theory is interesting and it is related well to the current research in the field. All reviewers agree that this is a noteworthy contribution to ICLR. 
This paper presents a simple approach called PDM for composing non linear and complex normalizing flows with score based generative models. Since score based models can be considered as a special form of continuous time normalizing flows, PDM corresponds to a composition of different classes of normalizing flows.   Pros:  * Combining generic normalizing flows with score based models is an interesting direction as they have different characteristics and can be complementary to each other. * Using Ito s lemma to show that the model learns a non linear SDE in data space is valuable.  * The authors show that the variational gap can be reduced using normalizing flows.  Cons:  * The proposed method does not exhibit a clear advantage compared to the diffusion baseline without the normalizing flow component. On the CIFAR10 dataset, the best NLL and FID results are obtained by the diffusion baseline.  * Theorem 2 makes a very unrealistic assumption that a flow network is flexible enough to transform $p_r$ to any arbitrary distribution. If this holds, we wouldn t need the score based generation model anymore. We could simply train the normalizing flow to map the input data distribution to a Normal distribution.  * This submission chooses to discuss differences with the recent LSGM framework. However, in doing so, several inaccurate claims are made. The lack of inference data diffusion in LSGM is mentioned as one of its drawbacks. However, it is not clear what is the value of having such a mechanism and what implications it may have on the expressivity of the model. Note that mapping from data space to latent space in VAEs can be considered as a stochastic inversion rather than an exact inversion. Ito s lemma does not require invertibility and it can be easily applied to the forward and generative diffusion in LSGM. The authors argue that applying it to the forward diffusion in LSGM will result in $\hat{p_{r}}\ne p_{r}$. But, $\hat{p_{r}}$ would be only considered for visualization of the forward diffusion and it is not used for training or any other purposes. LSGM, the proposed PDM, and score based models are all trained with a reweighting of ELBO (see [here](https://arxiv.org/abs/2106.02808)). It is not clear if the drawback mentioned above has an impact on the training or expressivity of the model.  * The presentation in the paper requires improvement. The motivation on why invertibility plays a key role is not clear beyond generating the visualization in Figure 2.   In summary, the paper proposes an interesting idea and explores directions very relevant to the current focus in generative learning. However, given the concerns above, we don t believe that the paper in its current form is ready for presentation at ICLR.
The paper introduces the maximum n times coverage, a new NP hard (and non submodular) optimization problem. It is shown that the problem can naturally arise in ML based vaccine design, and two heuristics are given to solve the problem. The results are used to produce a pan strain COVID vaccine.   The reviewers and I think that this is an interesting paper with a compelling application. There were some concerns about theoretical novelty and biological accuracy but these were addressed during the author response period. Given this, I am delighted to recommend acceptance. Please incorporate the feedback in the reviews in the final version of the paper.
The paper proposes a defense for adversarial attacks based on autoencoders that tries to find the closest point to the natural image in the output span of the decoder and "purify" the adversarial example. There were concerns about the work being too incremental over DefenseGAN and about empirical evaluation of the defense. It is crucial to test the defense methods against best available attacks to establish the effectiveness. Authors should also discuss and consider evaluating their method against the attack proposed in https://arxiv.org/pdf/1712.09196.pdf that claims to greatly reduce the defense accuracy of DefenseGAN. 
The manuscript proposes a black box optimization algorithm based on statistical hypothesis testing and proposes to use it to solve control problems posed in stochastic environments. While "Reinforcement Learning" appears in the title, this appears to have nothing to do with RL other than that it purports to solve problems that are typically used to benchmark RL algorithms, i.e. sequential decision making for reward maximization with stochastic dynamics. Baselines compared against also don t involve the reinforcement learning formalism. A key claim appears to be that the application of other gradient free methods to such problems incorrectly ignores stochasticity of the objective function owing to the stochastic sequential dynamics.  Reviewers found the paper both clear and well written, but questioned the novelty of the approach, its situation in the wider world of gradient free black box optimization methods, and the limited scope of the empirical results. Reviewer pTGV criticized apparent "magic numbers" in the algorithm description, to which the authors responded rather dismissively.  In their general reply statement, the authors seem to misinterpret reviewer Je5f s comment that “critical gradient information is hard to obtain”: my own interpretation is that Je5f is characterizing _black box optimization problems_ in general, and noting that the special structure of MDPs admits gradient estimation via the policy gradient theorem, etc. In response to Je5f directly, the authors state "[TD3 & SAC] are not BBO techniques, thus we did not feel as though it would be fair to compare HDCA to problems solved by these methods". ADBA raises some concerns about links to the existing literature, which appear to have been addressed to their satisfaction, though was unwilling to champion the paper for acceptance in light of the concerns of other reviewers.  The AC concurs with the reviewers  assessments regarding the limited nature of the experiments  The paper makes the claim that the newly proposed method is a compelling choice for the solution of stochastic MDPs, but fails to adequately defend this claim, and fails to benchmark against either policy or value based RL methods. When challenged on this, they assert that such comparisons would be unfair; this may be true in a limited sense, but if an RL algorithm can outperform the novel BBO method on all problems considered, it s unclear why this would be of significant interest to the ICLR community. A recurring theme in the discussion is that BBO methods are more scalable than RL, though batched A2C methods (e.g. Espeholt et al, 2018) or distributed Q learning (e.g. Kapturowski et al, 2019) would at least merit mention, and would appear to at least challenge this claim.  Finally, the AC would like to note that work on Evolution Strategies predates the work of Salimans et al, 2017, including for control problems (e.g. Cardamone et al, 2009) and in particular Wierstra et al (2014) propose many sophisticated techniques for more effectively utilizing function evaluations (including methods based on hypothesis testing) which are a natural point of comparison for this work. Similarly, CMA ES is a widely used and well regarded technique for gradient free optimization, and would be a reasonable baseline. In short, the empirical investigation is inadequate even if restricted to gradient free methods, and the fact that these methods may not explicitly account for stochastic dynamics does not obviate the need for comparison.  The method presented is interesting, and I encourage the authors to continue studying it, with a more diverse set of environments, strong baselines and careful ablations.
This paper provides some empirical investigation of the choice of the prior distribution for the weights in Bayesian neural networks. It shows empirically that, when trained via SGD, weights in feedforward neural networks exhibit heavy tails, while weights in convolutional neural networks are spatially correlated. From this observation they show that the use of such priors leads to some improved performances compared to the iid Gaussian prior in some experimental settings.  Reviewers have conflicting views on this paper, that have not been reconcilied after the author s response and the discussion. On the plus side, the paper is very well written, the experimental part is carefully conducted, and provides some insights on the choice of the prior in Bayesian neural networks, which could lead to further developments.  On the negative side, the claims made in the introduction are not fully supported by the experiments (the claims have been slightly amended in the revised version), and the take home message is not so clear. In particular, Bayesian approaches with the proposed priors still underperform compared to SGD without tempering. The authors could also have considered a broader sets of experiments.  Overall, I think the contributions outweight the limitations of this paper, and I would recommend acceptance.
This paper investigates how to deploy adaptive learning rates in multi agent RL (MARL). In particular, the learning rates are adaptively chosen based on which directions maximally affect the Q function, and take into account the interplay and balance between the actors and the critics. The topic is certainly of great interest when designing fast convergent MARL algorithms. However, the reviewers point out the inadequacy and insufficiency of empirical gains in the reported experiments. Also, larger scale experimental settings are needed in order to provide more convincing evidence about the practical benefits of the proposed scheme.   
The paper proposes a VAE with a mixture of experts decoder for clustering and generation of high dimensional data. Overall, the reviewers found the paper well written and structured , but in post rebuttal discussion questioned the overall importance and interest of the work to the community.  This is genuinely a borderline submission. However, the calibrated average score currently falls below the acceptance threshold, so I’m recommending rejection, but strongly encouraging the authors to continue the work, better motivating the importance of the work, and resubmitting.
This paper tackles the interesting problem of meta learning in problem spaces where training "tasks" are scarce.  Two criticisms that seems to shared across reviewers are that (i) it is debatable how "novel" the space of meta learning with "few" tasks is, especially since there aren t established standard for how many training tasks should be available, and (ii) the paper could use more comparisons with baseline methods and ablations to understand the contributions.  As an AC, I down weight criticism (i) because I don t feel the paper has to be creating a new problem definition; it s acceptable to make advances within an existing space.  However, criticism (ii) seems to remain.  After conferring with reviewers it seems that the rebuttal was not strong enough to significantly alter the reviewer s opinions on this issue, and so the paper does not have enough support to justify acceptance.  The paper certainly addresses interesting issues, and I look forward to seeing a revised/improved version at another venue. 
This work presents an approach to learning good representations for few shot learning when supervision is provided at the super class level and is otherwise missing at the sub class level.  After some discussion with the authors, all reviewers are supportive of this work being accepted. Two reviewers were even supportive of this work being presented at least as a spotlight.  The approach presented is well motivated, experiments demonstrate its value and include a nice application in the medical domain, making the work stand out relatively to most work in few shot classification. Therefore, I m happy to recommend this work be accepted and receive a spotlight presentation.
This paper proposes a neural network approach to approximate distances, based on a representation of norms in terms of convex homogeneous functions. The authors show universal approximation of norm induced metrics and present applications to value function approximation in RL and graph distance problems.   Reviewers were in general agreement that this is a solid paper, well written and with compelling results. The AC shares this positive assessment and therefore recommends acceptance. 
PAPER: This paper describes a method to generate visual gestures by learning an intermediate representation based on gesture sequences. This proposed method builds from previous work on VAE and vector quantized VAE. DISCUSSION: The reviewers wrote some detailed reviews about the paper, bringing some valid concerns and asking some questions to the authors. Unfortunately, no responses were posted from the authors. SUMMARY: After looking at all reviews, there was a general consensus among the reviewers that this paper was not ready for publication. We hope that the reviews will be helpful for the authors in revising their work for future submission.
This paper proposes a new evaluating metric for assessing the quality of model generated images, that aims to correct some of the problems with the popular FID metric. The reviewers acknowledge the importance of this problem, but do not find the empirical evaluation convincing. In particular, they highlight the following issues * Comparing FID and the new metric on examples that are adversarially selected against FID does not provide a fair comparison. * The methods are compared on images of bad quality (FID > 25) that are therefore not informative. * The comparison against existing techniques is incomplete * The reviewers raise concerns about how the comparison is done quantitatively  The reviewers are not sufficiently convinced by the author response regarding these issues. I therefore recommend not accepting the paper.
The paper motivates the need for robustness, citing a paper on adversarial attacks. The type of perturbations are quite different (and of greater concern) than those originally included in the work, namely additive Gaussian or Laplace noise. This was raised by reviewers 1, 2 and 3.  The authors provided a detailed rebuttal in which they addressed many concerns of the reviewers. In particular, they included experiments with adversarial attacks. While these initial results seem interesting, the AC believes that they are too preliminary and it is not possible to evaluate their significance. The work should incorporate stronger baselines (other than plain networks) and consider more challenging forms of attacks (only FGSM attacks were studying). In particular, the paper should discuss where these ideas sit in the context of the literature and compare against conceptually related works that have been published in the area, such as  Xie, Cihang, et al. "Feature denoising for improving adversarial robustness." CVPR. 2019.  Without the adversarial perturbations, the impact of the work reduces significantly, as highlighted by reviewer 1, 2 and 3. In such case, it would be important to include stronger baselines in that setting as well. Please see the suggestions made by reviewer 1.  The authors satisfactorily addressed the problems in the derivation of the updates raised by Reviewers 1 and 4. However, new questions arise that would require careful consideration, as mention by reviewer 4 (which the authors could not answer as they were posted after the discussion period ended). This alone would not imply rejection, but suggests that the paper would be stronger after incorporating further feedback.  While this did not play a role in the decision, the AC suggests to also look at the literature around LISTA, in particular the work: Liu, Jialin, and Xiaohan Chen. "ALISTA: Analytic weights are as good as learned weights in LISTA." ICLR. 2019.  All four reviewers recommend rejecting the paper and did not change their position after reading the author s response. The AC agrees with this assessment. 
The paper proposes a method for predicting stock market crises using a deep learning approach which combines time series stock market data with text from news articles. Their experiments show that the proposed method works better than the same model using only news or only stock price data, and a couple of deep learning baselines. All the reviewers pointed out that this paper is lack of novelty and significant technical contributions. The experiments are performed on a single dataset with incomplete baselines, and hence insufficient to support the claimed advantages of the proposed method. The writing quality is not up to the standards of an ICLR papers, with too many grammatical mistakes, typos, and unjustified arguments/claims.  The clarity of the writing is poor.  The authors did not provide their rebuttal.
Quoting from R3: "This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices."  With two weak acceptance recommendations and a recommendation for rejection, this paper is borderline in terms of its scores.  The approach and idea are interesting.  The main shortcoming of the paper, as highlighted by the reviewers, is that the approach and theoretical analysis are not properly motivated to solve an actual problem faced in real world data.  The approach does not provide a better algorithm for recovering the eigenvectors of the data, nor is it proposed as part of a learning framework to solve a real world problem.  Experiments are shown on synthetic data and MNIST.  As a stand alone theoretical result, it leaves open questions as to the proposed utility.
Interesting and novel approach of modeling context (mainly external documents with information about the conversation content) for the conversational question answering task, demonstrating significant improvements on the newly released conversational QA datasets. The first version of the paper was weaker on motivation and lacked a clearer presentation of the approach as mentioned by the reviewers, but the paper was updated as explained in the responses to the reviewers. The ablation studies are useful in demonstration of the proposed FLOW approach. A question still remains after the reviews (this was not raised by the reviewers): How does the approach perform in comparison to the state of the art for the single question and answer tasks? If each question was asked in isolation, would it still be the best?   
In this paper, the authors provide a Riemannian version of gradient descent/ascent for min max problems on manifolds. Assuming a tractable retraction mapping for the descent/ascent step, the authors provide a complexity analysis for finding a (local) saddle point in the spirit of Lin et al. (2020).  The paper received three negative recommendations and one positive, with all reviewers indicating high to expert confidence. After my own reading of the paper, I concur with the majority view that the paper does not clear the (admittedly high) bar for ICLR. After discussing the paper with the reviewers, the concerns that led to this recommendation are as follows: 1. On the proposed examples: it is not clear what exactly is the motivation for the DNN training example with orthonormality constraints on the weights. In the paper, $x$ is the vector of DNN weights, so it lives in some real space $R^d$. The authors subsequently assume it is constrained to live on some Stiefel manifold, but which one? The Stiefel manifold is the set of all orthonormal $r$ frames on $R^d$, and the formulation in Section 6 doesn t clarify things. Moreover, the papers cited by the authors either concern the initialization of the DNN or a regularization by an orthonormality/orthogonality penalty. This is quite different since DNN training typically involves at least some neurons with very large weights (which is of course disallowed if the weights are constrained to live on some "norm 1" subspace). 2. The presentation is often lacking in mathematical rigor: in a Riemannian setting, it is crucial to distinguish between the Riemannian distance function and the Riemannian norm. However, the two were used interchangeably at several points, and the authors  revision wasn t satisfactory in this regard   even the basepoint for the norm is missing in cases where it is not made clear from the context on which point the norm is considered. 3. The cost of applying retraction based methods in DNNs is also unclear. On a Stiefel manifold, the only known retractions involve SVD, so they have a superlinear computational cost relative to the size of the input matrix. If the dimensionality of the input matrix is that of the parameter space of the DNN, the efficiency of the method seems somewhat limited.  The above concerns regarding DNN training are perhaps less important if we view this as a primarily theoretical paper. In that regard however, the novelty of this paper over that of Lin et al. (and other Riemannian minimization papers) is not clear, so I am forced to recommend rejection at this stage. That being said, I believe that a thoroughly revised version of this paper could ultimately be publishable at one of the top venues of the field, and I would strongly encourage the authors to pursue this.
This paper proposes a method for performing stochastic variational inference for bidirectional LSTMs through introducing an additional latent variable that induces a dependence between the forward and backward directions.  The authors demonstrate that their method achieves very strong empirical performance (log likelihood on test data) on the benchmark TIMIT and BLIZZARD datasets.  The paper is borderline in terms of scores with a 7, 6 and 4.  Unfortunately the highest rating also corresponds to the least thorough review and that review seems to indicate that the reviewer found the technical exposition confusing.  AnonReviewer2 also found the writing confusing and discovered mistakes in the technical aspects of the paper (e.g. in Eq 1).  Unfortunately, the reviewer who seemed to find the paper most easy to understand also gave the lowest score.  A trend among the reviewers and anonymous comments was that the paper didn t do a good enough job of placing itself in the context of related work (Goyal et. al, "Z forcing") in particular.  The authors seem to have addressed this (curiously in an anonymous link and not in an updated manuscript) but the manuscript itself has not been updated.  In general, this paper presents an interesting idea with strong empirical results.   The paper itself is not well composed, however, and can be improved upon significantly.  Taking the reviews into account and including a better treatment of related work in writing and empirically will make this a much stronger paper.  Pros:   Strong empirical performance (log likelihood on test data)   A neat idea   Deep generative models are of great interest to the community  Cons:   Incremental in relation to Goyal et al., 2017   Needs better treatment of related work   The writing is confusing and the technical exposition is not clear enough
This work proposes context aware representation of graph nodes leveraging attention over neighbors (as already done in previous work). Reviewers concerns about lack of novelty, lack of clarity of paper and lack of comparison to state of the art methods have not been addressed at all. We recommend rejection.
I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is presenting an interesting and systematic study of reward hacking [GVMn] that is useful to the research community [bfGN] and targets an important problem [uYeb] in a rigorous way [16uL]. I thus recommend accepting the paper, but I strongly encourage the authors to further improve their paper based on the reviewer feedback, in particular in regards to improving positioning with respect to related work and a better formalization of their work.
This paper presents results of looking at the inside of pre trained language models to capture and extract syntactic constituency. Reviewers initially had neutral to positive comments, and after the author rebuttal which addressed some of the major questions and concerns, their scores were raised to reflect their satisfaction with the response and the revised paper. Reviewer discussions followed in which they again expressed that they became more positive that the paper makes novel and interesting contributions.  I thank the authors for submitting this paper to ICLR and look forward to seeing it at the conference..
The paper proposed a novel deep learning model specifically designed for periodic time series forecasting problems. The approach includes lay by layer expansion, residual learning, and periodic parametrization. The model outperforms state of the art baselines on several time series forecasting benchmarks.  The reviewers appreciate the extensive experimental results, but also suggested improvement on writing and comparison regarding the parameter efficiency of the model.
Four knowledgeable referees reviewed this paper; one reviewer (weakly) supports accept and other three indicate reject. Even with the rebuttal, all negative reviewers have concerns on the limited novelty and marginal performance improvement, and agree that the paper is not well qualified for the high standard of ICLR.
The paper claims to draw a connection between pruning and differential privacy. There seem to be conceptual issues with the paper, highlighted by all reviewers (see particularly Reviewer 3 s review), which the authors had no response to. For example, a function approximating another does not imply any transfer of differential privacy. This is a fundamental issue that the authors would need to address.  Some papers (suggested by the area chair and reviewers) related to differential privacy and pruning that the authors may wish to be aware of include https://arxiv.org/abs/1503.02031 and https://arxiv.org/abs/2008.13578 
Important problem (visually grounded dialog); incremental (but not in a negative sense of the word) extension of prior work to an important new setting (GuessWhich); well executed. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance.  
This paper studies the dependency of SGD convergence on order of examples. The main observation of the paper is: if the averages of consecutive stochastic gradients converge faster to the full gradient, then the SGD with the corresponding sampling strategy will have a faster convergence rate. For different sampling strategies, sampling with replacement has slower convergence in stochastic gradient, where sampling without replacement can converge faster. The paper also proposes two new algorithms that can improve convergence rates in some interesting settings. The reviewers find the analysis clean and the new algorithms are interesting. There is some concern on the dependency on n or d for the faster rate, which should be discussed more clearly in the final version of the paper. Overall this is a solid contribution to the example selection problem.
This paper proposes a method for attacking graph convolutional networks, where a graph rewiring operation was introduced that affects the graph in a less noticeable way compared to adding/deleting edges. Reinforcement learning is applied to learn the attack strategy based on the proposed rewiring operation. The paper should be improved by acknowledging/comparing with previous work in a more proper way. In particular, I view the major innovation is on the rewiring operation and its analysis. The reinforcement learning formulation is similar to Dai et al (2018). This connection should be made more clear in the technical part. One issue that needs to be discussed on is that if you directly consider the triples as actions, the space will be huge. Do you apply some hierarchical treatment as suggested by Dai et al. (2018)?  The review comments should be considered to further improve too.
All the reviewers liked the paper. The proposed method contains novel ideas of learning feature representation to maixmize the mutral informatio nbetween the latent code and its corresponding observation for fine grained class clustering. The model seems to successfully avoid mode collapse while training generators and able to generate various object (foregrounds) with varying backgrounds. The foreground and background control ability is an outstanding feature of the paper. Please incorporate the comments of the reviewers in the final version.  BTW, the real score of this paper should be 7.0 as Reviewer 5wFE commented that he/she would raise the score from 5 to 6 but at the time of this meta review, ths core was not raised. So the final score of the paper should be 8/8/6/6.
The paper develops an original extension/generalization of standard batchnorm (and group norm) by employing a mixture of experts to separate incoming data into several modes and separately normalizing each mode. The paper is well written and technically correct, and the method yields consistent accuracy improvements over basic batchnorm on standard image classification tasks and models. Reviewers and AC noted the following potential weaknesses: a) while large on artificially mixed data, improvements are relatively small on single standard datasets (<1% on CIFAR10 and CIFAR100)  b) the paper could better motivate why multi modality is important e.g. by showing histograms of node activations c) the important interplay between number of modes and batch size should be more thoroughly discussed d) the closely related approach of Kalayeh & Shah 2018 should be presented and contrasted with in more details in the paper. Also comparing to it in experiments would enrich the work. 
Paper was reviewed by four expert reviewers who identified the following pros/cons for the approach:  > Pros:   Paper addresses an important problem [R3]    Formulation is simple, elegant an easily adoptable [R2, R3, R4]   Experimental results are compelling (ECCV challenge winner) [R2, R3, R4]  > Cons:   Experiments  are using "novel" evaluation metric with little justification [R1, R3]   Details of the approach are unclear [R1]   Lack of simpler baselines in evaluation [R4]    No comparison with handlabelling counterparts [R4]  Authors have addressed a number of comments in the rebuttal. With [R2] and [R4], generally, being convinced about accepting the paper.  However, [R1] and [R3] actually became less positive about the paper as a function of the rebuttal. [R3] mentioned reducing the score to a 3 and [R1] to a 6 in the discussion. It is unclear why these changes are not reflected in the publicly facing reviews. However, the fact that two of the four reviewers were disappointed with author responses and became LESS convinced about the paper is problematic in AC s opinion.   AC also agrees that standard performance metrics should have been included. It is obviously reasonable to propose new metrics, but doing so should be accompanied by (1) reporting of performance with the original standard metric(s) and (2) justification for where prior metrics are problematic or faulty. While the proposed metric is reasonable for the specific use case outlined in Appendix C, it doesn t preclude standard evaluation metrics nor points out why they would be inappropriate or faulty.   Overall, AC likes the paper and agrees that it presents a valuable approach that should be published. Unfortunately, the unjustified use of non standard metrics without accompanied evidence, as noted above, is problematic and needs to be addressed in AC s opinion before that can happen. Since this issue was not addressed by authors in the rebuttal, AC sees no recourse but to reject the paper at this time, with a strong encouragement to address the aforementioned issue and to resubmit to CVPR or another top tier upcoming venue.   
This paper applies a form of recurrent autoencoder for a specific type of industrial sensor signal analysis.  The application is very narrow and the data set is proprietary.  The approach is not clearly described, but seems very straightforward and is not placed in context of prior work.  It is therefore not clear how to evaluate the contribution of the method.  The authors have revised the paper to include more details and prior work, but it still needs a lot more work on all of the above dimensions before it can make a significant contribution to the ICLR community.
The paper presents a practical approach to compute Wasserstein distance based image embeddings. The Euclidean distance in the embedded space approximates the true Wasserstein distance, thus reducing the high computation cost associated with the latter.  Pros:   Reviewers agree that the proposed solution is novel, straightforward and well described.   Experiment demonstrate the usefulness of such embeddings for data mining tasks such as fast computation of barycenters & geodesic analysis.  Cons:   Though the empirical analysis is convincing, the paper lacks theoretical analysis of the approximation quality. 
The authors propose a normalization method for cross lingual text representations. The goal is to normalize the monolingual embeddings based on spectral normalization. The study shows that produced text representations keep their meaning and improve performance on downstream tasks.  There is a disagreement among the reviewers. The main concern is whether the main contribution is an empirical study or a novel idea.  I think the authors well addressed the concerns of most reviewers. The idea and empirical study are enough for publication for ICLR 2022.
This paper focuses on the problem of performing imitation learning from trajectory level data that includes optimal as well as suboptimal demonstrations.  The authors wish to avoid the requirement of a separate filtering process that would throw away the bad trajectories.  The authors propose a clever innovation that allows for leveraging the policy that is itself being learned to reweight the samples for a next round of weighted behavioral cloning.  The paper is also somewhat theoretically rigorous and provides insight into the problem.   The reviewers pointed out some initial issues related to clarity and the authors did a good job of addressing reviewer concerns.  Ultimately all reviewers agreed that the core innovation of the paper was interesting and empirically worked reasonably well.    One older line of work that I think is quite relevant, but which is not discussed, is the empirically observed "clean up effect", described by Michie and colleagues in the 90s (e.g. "Learning to fly" Sammut et al 1992).  This clean up effect is intuitive and reportedly achieved for free in settings where the learning objective is mode seeking and the dataset is large, insofar as the mean value of the resulting policy *should* produce actions that corresponds to the average action produced by demonstrators in the same situation.  I think it would be worth discussing how the analysis of this paper relates to this empirical phenomenon. In particular, it would be worth clarifying in what regimes the suboptimality of training from a dataset with noisy examples arises and how likely this is to effect the mean value of the learned policy (for context, it is fairly common in practice to evaluate the student policy in BC settings by only using the mean action value; perhaps this point was present in the paper, and I missed it). From a certain perspective, the innovation of this paper is to accentuate the clean up effect.  As noted by a reviewer, and subsequently incorporated into the paper, the actual algorithm has some similarities to versions of recent "offline RL" algorithms (though of course it does not leverage rewards).  In particular, the motif of performing a weighted regression could perhaps be a bit more thoroughly contextualized by connecting it to other weighting factors (e.g. see Critic Regularized Regression).  That said, I leave this entirely to the discretion of the authors.  The final scores were 8, 7, & 6.  I see this as a strong paper and will endorse it for a spotlight.
Overall, all reviewers generally agree that the idea of using visual similarity to unsupervised alignment of multiple languages is interesting and the proposed method and dataset are well designed, while three of them raised some concerns related to the retrieval nature of the method. In particular,  discussions about its place as a study of machine translation and comparison with other cross lingual retrieval baselines were the main issues. Although authors made great effort to address reviewers  concerns points and did clarify some of them, unfortunately the reviewers were not fully convinced by the response, and one reviewer decided to downgrade the initial score.  After all, three reviewers rate the paper as  below the acceptance threshold . Based on their opinions, I decided to recommend rejection.  I think the entire picture of the work and the logic flow could be much clearer by discussing in a top down manner why this idea should be implemented with a retrieval based approach, rather than superficially adding "using retrieval" to some sentences.  
The reviewers are unanimous in their assessment that the paper lacks originality in its current form to be publishable at ICLR 2018.
Thanks for an interesting discussion. The paper introduces a sound question generation technique for QA. Reviewers are moderately positive, with low confidence. Some issues remain unresolved, though: While the UniLM comparison is currently not apples to apples, for example, nothing prevents the authors from using their method to pretrain UniLM. Currently, QA results are low ish, and it is hard to accept a paper based solely on BLEU scores (questionable metric) for question generation (the task is but a means to an end). Moreover, the authors do not really discuss how their method relates to previous work (see Review 2 and the related work cited there; there s more, e.g., [0]). I also find it a little problematic that the paper completely ignores all work prior to 2017: The NLP community started organizing workshops on question generation in 2010. [1]
The paper proposes an insightful study on the robustness and accuracy of the model. It was hard to simultaneously keep the robustness and accuracy. A few works tried to improve accuracy while maintaining the robustness by investigating more data, early stopping or dropout. From a different perspective, this paper aims to improve robustness while maintaining accuracy.   There are some interesting findings in this paper, which could deepen our understanding of adversarial training. For example, the authors conducted experiments with different sizes of the network in standard training and adversarial training. The capacity of an overparameterized network can be sufficient for standard training, but it may be far from enough to fit adversarial data, because of the smoothing effect. Hence given the limited model capacity, adversarial data all have unequal importance. Though this technique is simple and widely studied in traditional ML, it is an interesting attempt in adversarial ML and the authors provide extensive experimental results to justify its effectiveness.   In the authors  responses, the concerns raised by the reviewers have been well addressed. The new version becomes more complete by including more results on different PGD steps and the insights on designing weight assignment function. Also, the authors gave an interesting discussion on enough model size for the adversarial training, though it is still kind of an open question. I would thus like to recommend the acceptance of this paper.  
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * Empirical results demonstrate decent improvements over other reasonable models * The method is well engineered to the task  Cons: * The paper is difficult to read due to grammar and formatting issues * Experiments are also lacking detail and potentially difficult to reproduce * Some of the experimental results are suspect in that the train/test accuracy are basically the same. Usually we would expect train to be much better in highly parameterized neural models * The content is somewhat specialized to a particular task in NLP, and perhaps of less interest to the ICLR audience as a whole (although I realize that ICLR is attempting to cast a broad net so this alone is not a reason for rejection of the paper)  In addition to the Cons cited by the reviewers above, I would also note that there is some relevant work on morphology in sequence to sequence models, e.g.: * "What do Neural Machine Translation Models Learn about Morphology?" Belinkov et al. ACL 2017.  and that it is common in sequence to sequence models to use sub word units, which allows for better handling of morphological phenomena: * "Neural Machine Translation of Rare Words with Subword Units" Sennrich et al. ACL 2016.  While the paper is not without merit, given that the cons seem to significantly outweigh the pros, I don t think that it is worthy of publication at ICLR at this time, although submission to a future conference (perhaps NLP conference) seems warranted.
This paper addresses an distribution shift and biased Q values that happens when offline agents are finetuned in an online manner. The final revision of the paper is very well written and easy to understand. The proposed method in the paper is interesting, and aiming to address an important issue in RL. The proposed method involves a combination of two well known methods in RL to tackle the distribution shift issue, the paper first suggests to use a balanced replay mechanism a replay for online experiences and another one for the offline. The second improvement is coming from the ensemble distillation.  It seems like in the light of the reviews, the authors have improved manuscript. However, I would like to recommend the paper for rejection. I would like the authors to do further experiments on the individual components of the algorithms, for example what if we run all the experiments only with BR or only using ED how would the performance change. How much improvement is coming from each one of those individual components? As it stands, it is not clear to me right now, and the proposed solution looks a bit complicated and hacky.   The balanced replay mechanism is very similar to the replay approaches that are used for learning from demos methods like R2D3 [1] and DQfD. Also the ensemble distillation approach is very akin to RAND [2] and distillation approaches that are used in lifelong learning algorithms. It is not clear, why it is that important for offline RL. It should potentially improve online RL as well, perhaps some experiments on online RL would be interesting.  Nevertheless, I think the paper is very interesting and attempting to address a very important problem in RL. I would recommend the authors to resubmit the paper to a different venue after doing some small changes on it.  [1] Gulcehre, C., Le Paine, T., Shahriari, B., Denil, M., Hoffman, M., Soyer, H., ... & Barth Maron, G. (2019, September). Making Efficient Use of Demonstrations to Solve Hard Exploration Problems. In International Conference on Learning Representations.  [2] Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.    
This paper is rejected.  I and the reviewers appreciate the changes made by the authors. The paper presents: * An analysis (based on techniques from previous work) of double Q learning which shows that in an analytic model, double Q learning can have multiple sub optimal "approximated" fixed points. * Propose a modification of the update that uses collected trajectories to lower bound the optimal value. * Experiments on several Atari games.  While the theoretical results on double Q learning are interesting, the authors provide little theoretical analysis of their proposed approach. Doing so will significantly strengthen the paper. Additionally, reviewers had concerns about the experiments. R2 questions the parameter setting in the multi step experiments. 
The main idea of the paper is to  use image data to guide radar data acquisition by focusing on the blocks where the object has appeared. Four reviewers have relatively consistent rating: 3 of them rated “Ok but not good enough   rejection”, while 1 rated “clear rejection”. The main concerns include ad hoc choices of algorithm design, lack of algorithm novelty, not adequate experiments in illustrating the performance, etc. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. While the motivation is clear and the work has merits, the ACs agree with the reviewers’ concerns and this paper can not be accepted at its current state. 
This paper presents a new method for detecting out of distribution (OOD) samples.  A reviewer pointed out that the paper discovers an interesting finding and the addressed problem is important. On the other hand, other reviewers pointed out theoretical/empirical justifications are limited.   In particular, I think that experimental supports why the proposed method is superior beyond the existing ones are limited. I encourages the authors to consider more scenarios of OOD detection (e.g., datasets and architectures) and more baselines as the problem of measuring the confidence of neural networks or detecting outliers have rich literature. This would guide more comprehensive understandings on the proposed method.  Hence, I recommend rejection.  
The paper proposes an adjustment to the ECE metric to make it less biased in the small sample case by including the assumption that the confidence output by a classifier is monotonic with the true correctness probability.  The main idea is to successively make finer bins until a non monotonicity is observed.  The paper is interesting, but the magnitude of the contribution would be just enogh for a short paper if such a track existing in ICLR.    Reviewers have raised concerns about the discrepancy between their revised ECE formula and the Algorithm accompanying it, although that has been fixed through the author feedback phase.  Another concern is that for a paper whose core technical contribution is a revised metric for measuring calibration, a more thorogh empirical study over larger datasets is required.
This work examines how to craft adversarial examples that will lead trained seq2seq models to generate undesired outputs (here defined as, assigning higher than average probability to undesired outputs). Making a model safe for deployment is an important unsolved problem and this work is looking at it from an interesting angle, and all reviewers agree that the paper is clear, well presented, and offering useful observations. While the paper does not provide ways to fix the problem of egregious outputs being probable, as pointed out by reviewers, it is still a valuable study of the behavior of trained models and an interesting way to "probe" them, that would likely be of high interest to many people at ICLR.
This paper considers how to create efficient architectures for multi task neural networks. R1 recommends Weak Reject, identifying concerns about the clarity of writing, unsupported claims, and missing or unclear technical details. R2 recommends Weak Accept but calls this a "borderline" case, and has concerns about experiments and comparisons to baselines. R3 also has concerns about experiments and baselines, and feels the approach is somewhat ad hoc. The authors submitted a response that addressed some of these issues, but the authors chose to maintain their decisions. The AC feels the paper has merit but given these slightly negative to borderline reviews, we cannot recommend acceptance at this time. We hope the reviewer comments help the authors to prepare a revision for another venue.
This paper enhances Lagrangian neural networks by adding conservation of the angular and linear momenta. According to the reviewers, the technical contribution of the paper is marginal, it is a incremental change of an existing model, and it seems that there is some over claim on the generalization of the model to unseen systems. The theoretical contributions in the paper are not significant, and the experiments have not demonstrate the practical potential of the proposed model yet. After the reviewers provided their comments, the authors did not submit their rebuttals. Therefore, as a result, we do not think the paper is ready for publication at ICLR.
The problem of discovering ordering in an unordered dataset is quite interesting, and the authors have outlined a few potential applications. However, the reviewer consensus is that this draft is too preliminary for acceptance. The main issues were clarity, lack of quantitative results for the order discovery experiments, and missing references. The authors have not yet addressed these issues with a new draft, and therefore the reviewers have not changed their opinions.
This paper introduces a distillation approach for black box classifiers that trains generalized additive models (GAM), an additive model over feature shapes, thus providing global explanations for the model. Given the importance of interpretability, the reviewers appreciated the focus of this work. The reviewers also found the experiments, both on real and synthetic datasets, extremely thorough and were impressed by the results. Finally, they also mentioned that the paper was clearly well written.  The reviewers and AC note the following potential weaknesses:  (1) The primary concern, raised by all of the reviewers, is the lack of novelty;the proposed approach is a straightforward application of GAMs to model distillation, where black box output is the training data of the GAM, (2) The reviewers are also concern that the proposed approach is limited in scope to tabular datasets, and would not work for more interesting, complex domains like text or images, and (3) The reviewers are concerned that the interpretability of GAMS is assumed, without describing the limitations, for example, if there are correlated features, the shapes would affect each other in uninterpretable ways. Amongst other concerns, the reviewers were concerned about the formatting of the plots and tables in the paper, which made it difficult to read them, and the lack of a user study to verify the interpretability claims.  In response to these criticisms, the authors provided comments and a substantial revision to the papers, heavily restructuring the paper to fit extra experiments (comparison to other global explanation techniques, including a user study) and make the figures and tables readable. While the paper was much improved by these changes, and two of the reviewers increased their scores accordingly, concerns about the limited novelty and scope still remained.  Ultimately, the reviewers did not reach a conclusion, but the concerns of novelty and scope overwhelmed the clear benefits of the approach and the strong results. This paper was very close to getting accepted, and we strongly urge the authors to submit it to other premier ML conferences.
Description of paper content:  The paper provides a framework to develop a family of algorithms that decompose rewards into linear combinations of several reward channels. The value functions per channel are estimated in a new space using an invertible function transformation, f. The framework encompasses several previously published algorithms, including Log Q Learning. Conditions are provided for acceptable choices of f. Convergence to the optimal Q function in the tabular case is proven for a special learning update.  Summary of paper discussion:  All review scores were above the acceptance threshold. Overall, the reviewers found the idea interesting, the theoretical results satisfying, and the writing and presentation clear. Initial concern about the directedness of the experiments in showing the usefulness of this particular theoretical framework to explain performance improvements was allayed when some of the results in the paper (e.g. reward density in Atari Skiing) were re emphasized. Generally, all reviewers felt that this was a nice, thorough contribution with the demerit that the framework lacked “a killer application” experimentally.
Overall, the reviewers were insufficiently enthused by this paper.  There was no rebuttal, and the authors did not engage or answer questions raised.  I concur with the reviewers, and encourage the authors to carefully consider the provided feedback.
This paper discusses a method to update/optimise invertible matrices via low rank updates. The key property of the proposed method is that it keeps track of the matrix inverse and its determinant through the optimisation (with updates that are much cheaper to compute than a direct inversion/determinant computation).  While the method of performing low rank updates for invertible matrices itself has already been extensively studied in the literature as pointed out by reviews, this work focuses (after extensive revision) on the properties of this update method.  Since the updates may leave the manifold of invertible matrices, a numerical stabilisation step was introduce whereby updates that produce ill conditioned matrices are rejected during optimisation.  Rank one updates allow for fast update of matrix inverse and determinants. So this is particularly interesting when applied to normalising flows, as it allows for cheaper computation of the log det Jacobian terms.    The novelty of this approach is rather limited (as also pointed out by R2). The experiments and, in particular, the application to normalising flows are interesting, well executed. It is not clear if there are advantages of the method in other domains where log det Jacobians are not necessary relative to existing literature. 
Three experts reviewed the paper and all recommended acceptance. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance. However, the reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. For example, the discussion about or comparison with related works, clarity of the writing, suggested experiments, etc. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
Authors present an approach to consolidate multiple teachers into a single student model that can be adapted to new tasks. The method involves using a proxy dataset to facilitate distillation to prevent having to replay images from the teacher datasets. A multi task multi head objective is utilized, agnostic to the loss function, in which two are studied. Downstream task performance is used as the performance measure.  Pros:   The problem of how to best leverage multiple teachers for a downstream task is important and interesting.   Presents a method to generate distilled students that can be finetuned to tasks that demonstrates performance gains over baselines (imagenet alone or task specific teacher).    Easy to follow and implement.   Analysis across multiple datasets.  Cons:   Multiple reviewers expressed concerns about current level of novelty / contribution. In some sense, it is natural to expect that combinations of task related and generalist distillation would improve performance.   Main results demonstrate improvements in performance when teacher and tasks are related to one another. But authors do not address how to select task specific teachers for distillation. Related tasks and their matching to the target task are assumed to be known. Authors cited related prior works that attempt to do this matching, but do not apply it to their study for a full solution.   Authors do not study variations of generalist teachers. How does changing the generalist teacher impact performance?   Some reviewers expressed concern presentation is not clear. In particular, the style of figures may not be appropriate to best convey results and analyses of this type of work. Comparing different approaches is difficult looking at thin lines. Tables are perhaps better suited to convey these results.   Multiple reviewers expressed concerns full finetuning results are not convincing (Fig 4), though few shot results look more convincing  Authors and reviewers had interaction, but reviewers maintained their recommendation of weak reject. All reviews unanimous in their decisions. Authors are encouraged to take into consideration all the comments and submit to another venue.
During the discussion among reviewers, we have shared the concern that this work has a significant overlap with [Liu et al. 2018] and [Liu & Motani 2020]. Although the authors tried to address this concern by the author response, I also think that the difference is not enough. In particular, the reviewers pointed out that Figure 1, Table 1, and Figure 3 are exactly the same with those in [Liu, 2020], and Proposition 2 in [Liu & Motani 2020] is Proposition 1 in this paper. Since these overlaps are not acceptable, I will reject the paper.
Summary: This paper proposes an interesting idea where additional auxiliary tasks allow an agent to more quickly learn in a sparse reward task.  Comments:  * The authors may want to look at "Parallel Multi Environment Shaping Algorithm for Complex Multi step Task" https://www.sciencedirect.com/science/article/abs/pii/S092523122030655X as it has a somewhat related idea and is also in RTS games. * It wasn t clear to me how the auxiliary tasks were generated/selected, or how the algo would work if a poor auxiliary task was used. * I wasn t sure why SAC X wasn t empirically compared to in a domain where it and this method could both apply.  Discussion: The reviewers agreed that this paper could be significantly improved in multiple dimensions.  Recommendation: I recommend we reject this paper. However, I encourage the authors to work to improve it as I d really like to understand where and why this method is successful   I would eventually like to incorporate it into my own work.
This paper at first used the name NAS Bench 360 for the benchmark, which confused several reviewers (who expected a tabular benchmark behind this name). The authors renamed the benchmark, which removed this issue, emphasizing that the contribution does not lie in proposed a new tabular NAS benchmark, but a new performance evaluation of NAS on different data sets. One reviewer recommended acceptance, but 3 reviewers stuck with their rejection scores, the reasons being that    there are by now several papers applying NAS outside of computer vision, with a seemingly more comprehensive analysis    more analysis would be useful   it is unclear how general the conclusions are that can be drawn from performance on the included datasets. (Low technical novelty was also mentioned, but I do not believe that this type of paper can be very impactful even if it has no technical novelty.)  Overall, although I agree with the accepting reviewer that this type of work can be very useful to the community, the rejecting reviewers have too many criticisms to accept the paper in its current form. I encourage the authors to address them and to resubmit. One note (which did not affect the decision, but which I d like to notify the authors about) is that a reviewer found that the author identity was revealed in the anonymous codes provided by the authors (https://anonymous.4open.science/r/NAS Bench 360 26D1).
Main content:  Blind review #2 summarizes it well:  This paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set.     Discussion:  All reviewers rejected.     Recommendation and justification:  The paper must be rejected due to its violation of blind submission (the authors reveal themselves in the Acknowledgments).  For information, blind review #2 also summarized well the following justifications for rejection:  I recommend rejection for this paper due to the following reasons:   The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors).    The experiment results aren t strong enough. And the experiments are done on only one dataset.   I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context aware.
The authors present CLIME, a variant of LIME which samples from user defined subspaces specified by Boolean constraints. One motivation is to address the OOD sampling issue in regular LIME. They introduce a metric to quantify the severity of this issue and demonstrate empirically that CLIME helps to address it. In order to stay close to the data distribution, they use constraints based on Hamming distance to data points. They demonstrate that this approach helps to defend against the recent approach of Slack et al. 2020 to fool LIME explanations.  The paper is close to borderline, though concerns remain about experimental validation and the extent of novel contribution, since the original LIME framework is more flexible than described here and allows a custom distance function. Rev 1 believes that the original LIME framework is sufficient to handle Hamming distance constraints though sampling will be less efficient. To their credit, authors engaged in discussion but this should be further elaborated in a revised version.
There was a clear consensus amongst reviewers that the paper should not be accepted. This view was not changed by the rebuttal. Thus the paper is rejected. 
This paper proposes an extension to previous unsupervised feature learning work, with an EM style latent variable model with momentum encoders. The paper is well written and provides a nice read. It has been noted that it is easy to follow and provides good insights. On the experimental side, compared with MoCo, the proposed approach achieve noticeable improvements. One of the reviewers noted the easy reproducibility of the proposed approach.  Some reviewers noted some comparisons were lacking from the original manuscript, but the authors have update the draft to include those. As noted in the reviews, the field of SSL in vision is moving at a very quick pace, making it hard to clearly state what is the SOTA at time t.  Overall, most questions raised by the reviewers were properly addressed during rebuttal   and given the ratings, I suggest acceptance.
This paper studies the trade off between the model size and quantization levels in quantized CNNs by varying different channel width multipliers. The paper is well  motivated and draws interesting observations but can be improved in terms of evaluation. It is a borderline case and rejection is made due to the high competition. 
This paper addresses the problem of how best to sample hard negatives during contrastive learning, a topic of importance for the recently resurgent field of metric learning / contrastive loss based unsupervised representation learning. Backed by theoretical results for a new low variance version of the NCE, the paper proposes an easy to implement "Ring" method for selecting negatives that are at just the right level of difficulty, neither too hard nor too easy.  Happily, this is a paper that has improved significantly through the interactive peer review of a dedicated set of reviewers combined with prompt responses from the authors. Perhaps the result that tipped this paper over the line in my assessment: the new experimental results now show significant gains from applying the "Ring" approach for hard negative sampling to near state of the art implementations of the MoCo v2 approach, which is among the leading unsupervised visual feature learning approaches. 
This work proposes a new embedding for sets of features. A set is represented by the output means of an EM algorithm for fitting the input set with a mixture of Gaussians. The authors draw a new connection to an existing method for set embedding (OTKE). Moreover, their method achieves good experimental results.  There is general consensus among the reviewers that the paper is sound, well written and provides new insights for set representation, with convincing experiments.  The authors have answered to most comments raised by the reviewers and have revised the paper accordingly.  I recommend acceptance as a poster.
This paper studies the problem of multi domain few shot image classification and proposes a Universal Representation Transformer (URT) layer, which leverages universal features by dynamically re weighting and composing the most appropriate domain specific representations in a meta learning way. The paper extends the prior work of SUR [Dvornik et al 2020] by using meta learning and avoiding additional training during test phase. The experimental results show improvements over SUR in both accuracy (not always significant on some datasets though) and inference efficiency. Overall, the paper is well written with sufficient contributions. After the author s rebuttal and revision, reviewers generally agree the paper can be accepted. I recommend to Accept (Poster). 
In this paper, the authors propose a new max sliced Wasserstein distance. Specifically, the proposed method is a multiple sliced variants of the existing max sliced Wasserstein distance. Compared to the subspace Robust Wasserstein distance, the proposed method can be efficiently computed.  Overall, the proposed method is a good extension of the max sliced Wasserstein and can be used in various applications. All authors agree to accept the paper, so, I also vote for acceptance.
The paper proposes to use projective clustering to compress the embedding layers of DNN. This is a novel interesting idea which can  impact the area of Knowledge distillation. There were some concerns about the empirical study which was addressed to some extent  by the authors during the rebuttal.
This paper takes advantage of a well known fact in the OT literature: that relaxing either of the marginals of OT problems results in nearest neighbor assignments (as e.g. in k means) or soft assignments when using an entropic regularizer. The authors take advantage of this simple property (used e.g. in the first iterations of the word mover s distance) to speed up the inner iterations of the GW problem. As a result, theirs is a very simplified divergence that drops an important piece of info (the weights of the second measure) but which is illustrated on a few tasks dealing with graphs. Overall the paper has been appreciated by most reviewers, some criticizing the paper for its incremental nature but being overall pleased with the experimental validation.
This paper introduces a variant of Nesterov momentum which saves computation by only periodically recomputing certain quantities, and which is claimed to be more robust in the stochastic setting. The method seems easy to use, so there s probably no harm in trying it. However, the reviewers and I don t find the benefits persuasive. While there is theoretical analysis, its role is to show that the algorithm maintains the convergence properties while having other benefits. However, the computations saved by amortization seem like a small fraction of the total cost, and I m having trouble seeing how the increased "robustness" is justified. (It s possible I missed something, but clarity of exposition is another area the paper could use some improvement in.) Overall, this submission seems promising, but probably needs to be cleaned up before publication at ICLR. 
This paper studies methods for using weight sparsification to reduce the computational load of network inference.  While there is not absolute consensus on whether this paper should be accepted, one of the main criticisms of this paper is that sparse compute is not always realistic or efficient on a GPU.  While this may be true of the current SOTA in hardware, emerging computing platforms and CPU libraries may handle sparse networks quite well.  For this reason, I am willing to down weight this criticism. Based on the remaining comments, this paper has the merit to be accepted, even if it is a bit forward looking in terms of the hardware platforms it targets.  
This method has a lot of strong points, but the reviewers had concerns about baselines, comparisons, and hand engineered aspects of the method. The authors gave a strong rebuttal and made substantial updates to the paper to address the concerns. I think that this has saved the submission and tipped the balance towards acceptance. 
Reviewers have different views on the paper and after going through the reviews, replies and the papers, we believe that there is room for improvement here.   While the part related to  indefinite symmetric kernels, and general similarity functions seems to be well covered, as well as the part on Transformers, the relation with learning in RKBS and Transformer is far from being clear and Reviewer 4 makes a strong point on this. For instance,   * what is the goal of the section 5 and Definition 1 . Indeed it is not clear here if the point of the authors is to learn the kernel parameters in equation 9 or to learn to predict the output of a transformer. If it is the latter, the connection with the first part is unclear.  * In Equation 11, I can understand that x and y are the sequences t and s but what is z_ij and how it is obtained? So again, the learning problem drops in without justification and it is not explained how it can be solved. The theoretical results involving the representer theorem is nice though.  * The experiment does not seem very related to the learning problem in Equation 11 introduced by the authors.it seems to me that they are just trying different kernels on top of the dot product. 
The effectiveness of active learning techniques for training modern deep learning pipelines in a label efficient manner is certainly a very well motivated topic. The reviewers unanimously found the contributions of this paper to be of interest, particularly nice empirical gains over several natural baselines.
The paper presents an empirical study into sparse connectivity patterns for DenseNets.  Whilst sparse connectivity is potentially interesting, the paper does not make a strong argument for such sparse connectivity patterns: in particular, the results on ImageNet suggest that sparse connectivity performs substantially worse than full connectivity (at the same FLOPS level, Log DenseNet obtains ~2.5% lower accuracy than baseline DenseNet models, and the best Log DenseNet is ~4% worse than the best DenseNet). On CamVid, both network architectures appear to perform on par.  The paper motivates the model architecture by the high memory consumption of DenseNets but, frankly, that is a very weak motivation: DenseNets are actually very memory efficient if implemented correctly (https://arxiv.org/pdf/1707.06990.pdf). The fact that such implementations are not well supported by TensorFlow/PyTorch is a shortcoming of those deep learning frameworks, not in DenseNets. (In fact, the memory management features that deep learning frameworks have implemented to make residual networks memory efficient (for instance, caching GPU memory allocation in PyTorch) are far more complex than the "thousand lines of C++" currently needed to implement a DenseNet correctly.) Such issues will likely be resolved relatively soon by better implementations, and are hardly a good motivation for a different network architecture.
This paper studies the problem of characterizing the optimal early stopping time in overparameterized learning as a function of model dimension and sample size. To do this the paper uses an explicit form of the gradient flow from prior work to present high probability bounds in the over parameterized setting and characterizes various properties of the optimal stopping time. The authors also conduct various experiments to verify the theory. The reviews though the paper was interesting and insightful. They also raised some concerns about the (1)restrictiveness of the distributional assumptions, (2) poor explanation of the theoretical results, and (3) novelty with respect to other work and (4) other technical issues. The discussion and response mitigated these concerns but the reviewers decided to mostly keep their original score. My own reading of the paper is that there are good ideas in this paper and I agree with the authors that some of the technical issues raised by the reviewers is incorrect. However, it is also clear that the paper needs a bit more work to put it into the right context and also the proof need to be more clearly and carefully written before this paper can be accepted. Therefore I recommend rejection but encourage the authors to submit to a future ML venue after a thorough revision.
This paper suggests the use of generative ensembles for detecting out of distribution samples.   The reviewers found the paper easy to read, especially after the changes made during the rebuttal. However, further elaboration in the technical descriptions (and assumptions made) could make the work seem more mature, as R2 and R1 point out.   The general feeling by reading the reviews and discussions is that this is promising work that, nevertheless, needs some more novel elements. A possible avenue for increasing the contribution of the paper is to follow R1’s advice to extract more convincing insights from the results.  
The authors propose using a SVM, trained as a last layer of a neural network, to identify exemplars (support vectors) to save and use to prevent forgetting as the model is trained on further tasks. The method is effective on several supervised benchmarks and is compared to several other methods, including VCL, iCARL, and GEM. The reviewers had various objections to the initial paper that centered around comparisons to other methods and reporting of detailed performance numbers, which the authors resolved convincingly in their revised paper. However, the AC and 2 of the reviewers were unconvinced of the contribution of the approach. Although no one has used this particular strategy, of using support vectors to prevent forgetting, the approach is a simplistic composition of the NN and the SVM which is heuristic, at least in how the authors present it. Most importantly, the approach is limited to supervised classification problems, yet catastrophic forgetting is not commonly considered to be a problem for the supervised classifier setting; rather it is a problem for inherently sequential learning environments such as RL (MNIST and CIFAR are just commonly used in the literature for ease of evaluation).
This paper considers the benefits of deep multi task RL with shared representations, by deriving bounds for multi task approximate value and policy iteration bounds. This shows both theoretically and empirically that shared representations across multiple tasks can outperform single task performance.  There were a number of minor concerns from the reviewers regarding relation to prior work and details of the analysis, but these were clarified in the discussion. This paper adds important theoretical analysis to the literature, and so I recommend it is accepted.
This paper addresses the problem of learning neural graph representations, based on graph filtering techniques in the vertex domain.  Reviewers agreed on the fact that this paper has limited interest in its current form, and has serious grammatical issues. The AC thus recommends rejection at this time. 
The submission proposes a strategy for creating vector representations of graphs, upon which a CNN can be applied.  Although this is a useful problem to solve, there are multiple works in the existing literature for doing so.  Given that the choice between these is essentially empirical, a through comparison is necessary.  This was pointed out in the reviews, and relevant missing comparisons were given.  The authors did not provide a response to these concerns.
The paper received mixed and divergent reviews. As a paper of unusual topic in ICLR, the presentation of this work would need improvement. For example, it is difficult to understand what s the overall objective function, why a specific design choice was made, etc. It s nice to see that the authors somehow did quite a bit of engineering to make their model work for classification and drawing tasks, but (as an ML person) it’s difficult to get a clear rationale on why the method works other than that it’s biologically motivated. In addition, the proposed model (at a functional level) looks quite similar to Mnih et al. s "Recurrent Models of Visual Attention" work (for classification) and Gregor et al s DRAW model (for generation) in that all these models use sequential/recurrent attention/glimpse mechanisms, but no direct comparisons are made. For classification, the method achieves strong performance on MNIST but this may be due to a better architecture choice compared to Mnih s model but not due to the difference of the memory mechanism. For image generation/reconstruction, the proposed method seems to achieve quite good results but they are not as good as those from DRAW method. Overall, the paper is on the borderline, and while this work has some merits and might be of interest to some subset of audience in ICLR, there are many issues to be addressed in terms of presentation and comparisons. Please see reviews for other detailed comments.
The paper addresses the problem of domain generalization for learning spatio temporal dynamics. It proposes a solution where an encoder captures some characteristics of a given environment, and a forecaster autoregressively predicts future dynamics conditioned on the characteristics learned by the encoder. Said otherwise, the forecaster learns the general form of dynamics parameterized by an environment representation extracted by the encoder. The conditioning is implemented via an adaptive instance normalization mechanism. A form of padding is also introduced in order to take into account boundary conditions. The two components encoder and forecaster are trained sequentially. This approach is casted in a meta learning framework. Theoretical results inspired by multi task learning and domain adaptation are also demonstrated. The model is evaluated and compared to different baselines on three problems, and for two different settings: varying initial conditions with a given dynamics, and dynamics with varying parameters.  This is a borderline paper. It targets a timely and important problem of domain generalization for dynamic environments. The proposed solution is original and compares well experimentally to several baselines. It allows for better generalization performance for the two test settings considered. In the current version, the paper however suffers from different weaknesses. First there is the imprecision of the arguments and the description of the experiments. Some of the arguments and claims are vague and sometimes abusive, not backed up by evidence. For example, a central claim is that the encoder learns time invariant quantities characterizing the environment when the learned representations indeed change with a time shift in the input for any environment. The same goes for the argument developed for the padding construction. It is claimed to model boundary conditions, but this is not supported by any theoretical or empirical evidence. As noted by the reviewers, the theoretical analysis is disconnected from the algorithmic and experimental developments and does not bring much additional value to the paper. What is more embarrassing is that some of the claims in this section are overstated and induce incorrect conclusions.  From Theorem 3.1 and proposition 3.3, the authors suggest that multitask learning leads to better generalization than learning independently, while this is not formally guaranteed by the results (this is acknowledged by the authors in a later comment). Besides, the conditions of validity are not discussed while they seem to only cover situations for which the train and the test distributions are the same. The same holds for the second theoretical results (theorem 3.4). It is claimed that this result supports the authors’ idea of training encoder and forecaster sequentially, while it does not. Besides, the bounds in this result cannot be controlled as noted by the reviewers and are not useful in practice.  Overall, the paper addresses an important topic and proposes new solutions. The results are promising and it is indeed an interesting contribution. However, inaccuracies and incorrect or exaggerated claims make it difficult to accept the current version of the article. The article would make a strong and innovative contribution if it were written as a purely experimental article with a detailed description of the experiments and comparisons.
This paper studies the approximation and integration of partial differential equations using convolutional neural networks. By constraining CNN filters to have prescribed vanishing moments, the authors interpret CNN based temporal prediction in terms of  pde discovery . The method is demonstrated on simple convection diffusion simulations.  Reviewers were mixed in assessing the quality, novelty and significance of this work. While they all acknowledged the importance of future research in this area, they raised concerns about clarity of exposition (which has been improved during the rebuttal period), as well as the novelty / motivation. The AC shares these concerns; in particular, he misses a more thorough analysis of stability (under what conditions would one use this method to estimate an actual PDE and obtain some certificate of approximation?) and discussions about pitfalls (in real situations one may not know in advance the family of differential operators involved in the physical process nor the nature of the non linearity; does the method produce a faithful approximation? why?).  Overall, the AC thinks this is an interesting submission that is still in its preliminary stage, and therefore recommends resubmitting to the worshop track at this time.
All three reviewers expressed consistent concerns on this submission in their reviews. In addition, none of them enthusiastically supported this work during discussion. It is clear this submission does not make the bar of ICLR. Thus a reject is recommended.
This paper first investigates the behavior (e.g., catastrophic overfitting) of fast adversarial training (FastAdv) through experiments. It finds that the key to its success is the ability to recover from overfitting to weak attacks. Then, it presents a simple fix (FastAdv+) that incorporates PGD adversarial training when catastrophic overfitting is observed. The resulting method is shown to be able to train for a large number of epochs. It also presents a version (FastAdvW) that use the improved fast adversarial training as a warmup of PDG adversarial training, similar as in previous work. Overall, the analysis is useful and the ideas are valid. The empirical results also show promise. However, the main weakness of such empirical analysis is that it may be sensitive to the settings (e.g., # of epochs, splitting of datasets, …). The authors’ rebuttal also reflected such potential concerns.
The paper proposes an RL based algorithm for training neural networks that is able to match the performance of backprop on CIFAR and MNIST datasets.  The reviewers generally found the algorithm and motivations interesting, but some had issue with the imprecision of the notion of "biologically plausible" used by the authors. One reviewer had issues with missing discussion of related work and also doubts about the meaningfulness of the experiments, since the networks were quite shallow.  For this type of paper, clarity and precision of exposition is crucial in my opinion, and so I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to a future venue.
This paper received 2 borderline accepts, 1 accept, and 1 reject.  In general, there is broad agreement that this is solid experimental work and that the differences found between recognition and viewpoint estimation were interesting.  The main issue brought up by the more negative reviewer is that some of the experiments are subject to interpretation (specifically the extrapolation problem could become more of an interpolation problem as the number of training examples increases). This issue is acknowledged by the other reviewers who nonetheless see some value in the paper being published and potentially paving the way for additional studies. My suggestion for the authors is to prominently discuss this issue in a revision of this paper. Unfortunately, because of space constraints,  I have to recommend this paper be rejected.
This paper presents a non autoregressive NMT model which predicts the positions of the words to be produced as a latent variable in addition to predicting the words. This is a novel idea in the field of several other papers which are trying to do similar things, and obtains good results on benchmark tasks. The major concerns are systematic comparisons with the FlowSeq paper which seems to have been published before the ICLR submission deadline. The reviewers are still not convinced by the empirical performance comparison as well as speed comparisons. With some more work this could be a good contribution. As of now, I am recommending a Rejection.
The author response and revisions to the manuscript motivated two reviewers to increase their scores to weak accept. While these revisions increased the quality of the work, the overall assessment is just shy of the threshold for inclusion.
Strong paper in an interesting new direction. More work should be done in this area.
The paper is about a software library that allows for relatively easy simulation of molecular dynamics. The library is based on JAX and draws heavily from its benefits.  To be honest, this is a difficult paper to evaluate for everyone involved in this discussion. The reason for this is that it is an unconventional paper (software) whose target application centered around molecular dynamics. While the package seems to be useful for this purpose (and some ML related purposes), the paper does not expose which of the benefits come from JAX and which ones the authors added in JAX MD. It looks like that most of the benefits are built in benefits in JAX. Furthermore, I am missing a detailed analysis of computation speed (the authors do mention this in the discussion below and in a sentence in the paper, but this insufficient). Currently, it seems that the package is relatively slow compared to existing alternatives.   Here are some recommendations: 1. It would be good if the authors focused more on ML related problems in the paper, because this would also make sure that the package is not considered a specialized package that overfits to molecular dynamics. 2. Please work out the contribution/delta of JAX MD compared to JAX. 3. Provide a thorough analysis of the computation speed 4. Make a better case, why JAX MD should be the go to method for practitioners.  Overall, I recommend rejection of this paper. A potential re submission venue could be JMLR, which has an explicit software track.
The paper presented an empirical study of pre trained models on the Out of distribution Generalization problem.  Authors evaluated various factors (such as model sizes, datasets, learning rate, etc) and claim some major findings:  1) larger models have better OOD generalization, and combining both larger models and larger datasets is critical; 2) smaller learning rate during fine tuning is critical; 3) strategies improving in distribution accuracy may hurt OOD. Overall, this paper is a well written empirical study with some useful insights, but the new findings from the empirical studies are generally not surprising and the overall contribution is not significant enough for acceptance.
The paper is overally interesting and addresses an important problem, however reviewers ask for more rigorous empirical study and less restrictive settings.
The paper is rejected based on unanimous reviews.
This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy.  The reviewers found the contribution interesting for the ICLR community. R3 initially found the paper lacked clarity, but the authors took the feedback in consideration and made significant improvements in their revision. The reviewers all agreed that the updated paper should be accepted.
All reviewers appreciated the main result in the paper, which gives  global optimality guarantee for constrained policy optimization for both tabular setting and NTK setting. However, there were a number of unclear parts of the paper reported by several reviewers (assumptions, hyperparameter tuning, complexity dependence on the number of neurons, experimental setups). On top of it, the AC also echoes with R1’s concern about the novelty of this work as it basically stacks existing results (TD by Dalal et al., Neural TD by Cai et al. (2019), NPG by Agarwal et al, CSA algorithm by Lan & Zhou).  These concerns made me reticent to recommend acceptance at this point. I strongly encourage the authors to continue their interesting work in considering the reviewer comments and strengthen the numerical experiments.  
The paper seeks to empirically study and highlight how disentanglement of latent representations relates to combinatorial generalization. In particular, the main argument is to show that models fail to perform combinatorial generalization or extrapolation while succeeding in other ways. This is a borderline paper. For empirical studies it is also less agreed upon in general where one should draw the line about sufficient coverage of experiments, i.e., the burden of proof for primarily empirically derived insights. The initial submission clearly did not meet the necessary standard as the analysis was based on a single dataset and studied only two methods (VAE and beta VAE). The revised version of the manuscript now includes additional experiments (an additional dataset and two new methods), still offering largely consistent pattern of observations, raising the paper to its current borderline status. Some questions remain about the new results (esp the decoder). 
Dear Authors,  Thank you very much for submitting this very interesting paper.  This work analyzes the effect of gradient descent training on the compositionality of the learned model. Their main argument is that GD tries to use the redundant information in the data and, as a result, it doesn t generalize well. The paper then tries to show that theoretically and empirically with some simple experiments.  There is a general consensus among all the reviewers that this paper is not suitable for publication at ICLR. The authors do not entirely address most of the concerns raised by the reviewers during the rebuttal.   If the authors improve the clarity of the paper, making some of the propositions and theories more concrete and grounded in experiments as well, I would recommend them to resubmit this paper to a different venue since the premise of the paper is important and interesting.  Some of the reasons:    The paper claims that the gradient descent can not ignore the redundant information without providing sufficient empirical results. Though the part that is not clear to me whether if it is a credit assignment or an optimization problem. I agree with R1 that it is not clear what type of new insights from the proofs.    As R1 mentions, this paper s claim seems too strong and not supported by experiments.    R2 finds part of the paper unclear and thinks that some of the paper s propositions and theories are either trivial or wrong. The rebuttal doesn t seem to be doing a good job in terms of addressing those concerns.    R4 also is confused with the paper thinks that some of the theories are incorrect.   
The authors present a self supervised framework for learning a hierarchical policy in reinforcement learning tasks that combines a high level planner over learned latent goals with a shared low level goal completing control policy.  The reviewers had significant concerns about both problem positioning (w.r.t. existing work) and writing clarity, as well as the fact that all comparative experiments were ablations, rather than comparisons to prior work.  While the reviewers agreed that the authors reasonably resolved issues of clarity, there was not agreement that concerns about positioning w.r.t. prior work and experimental comparisons were sufficiently resolved.  Thus, I recommend to reject this paper at this time.
This paper presents an empirical study focusing on Bayesian inference on NNGP   a Gaussian process where the kernel is defined by taking the width of a Bayesian neural network (BNN) to the infinity limit. The baselines include a finite width BNN with the same architecture, and a proposed GP BNN hybrid (NNGP LL) which is similar to GPDNN and deep kernel learning except that the last layer GP has its kernel defined by the width limit kernel. Experiments are performed on both regression and classification tasks, with a focus on OOD data. Results show that NNGP can obtain competitive results comparing to their BNN counterpart, and results on the proposed  NNGP LL approach provides promising supports on the hybrid design as to combine the best from both GP and deep learning fields.  Although the proposed approach is a natural extension of the recent line of work on GP BNN correspondence, reviewers agreed that the paper presented a good set of empirical studies, and the NNGP LL approach, evaluated in section 5 with SOTA deep learning architectures, provides a promising direction of future for scalable uncertainty estimation. This is the main reason that leads to my decision on acceptance.  Concerns on section 3 s results on under performing CNN & NNGP results on CIFAR 10 has been raised, which hinders the significance of the results there (since they are way too far from expected CNN accuracy). The compromise for model architecture in order to enable NNGP posterior sampling is understandable, although this does raise questions about the robustness of posterior inference for NNGP in large architectures.
The paper proposed a new kind of neurons for 3D spherical data classification. All the reviewers agreed that the new kind of neurons makes a good contribution. However, all the reviewers also agreed that the experiments are too weak: only at the proof of concept level and no comparison with the state of the arts. Only reviewer FkmY advocated accepting the paper because we need new ideas, and all other reviewers leaned towards rejecting the paper. The AC had exactly the same feeling as the reviewers. Particularly, the AC also agreed with reviewer FkmY that we should not look at experimental results only. However, the AC would like to point out that this by no means means that the experiments can be too simple. Note that this paper is to propose a new tool to improve classification performance, rather than a new theory to explain or predict something. So some basic requirements on the experiments are necessary. If the authors could provide comparison with the state of the arts and with reasonably good performance, not necessarily exceeding or even on par with the state of the arts (namely can be inferior but not too inferior so that others can believe adding engineering tricks could fill in the gap), the AC would consider accepting the paper.
Most of the reviewers have concerns that the experimental results don’t show stronger enough improvements over baselines and that the theoretical contribution of the paper is not completely clear. These concerns make the paper a borderline paper for NeurIPS. Some reviewers have pointed out problematic or unsupported claims in the paper. With these in mind, I encourage the authors to revise the paper with more clarity and address the reviewers  comments on the exposition of the paper.
This paper presents an efficient architecture of Transformer to facilitate implementations on mobile settings. The core idea is to decompose the self attention layers to focus on local and global information separately. In the experiments on machine translation, it is shown to outperform baseline Transformer as well as the Evolved Transformer obtained by a costly architecture search.  While all reviewers admitted the practical impact of the results in terms of engineering, the main concerns in the initial paper were the clarification of the mobile settings and scientific contributions. Through the discussion, reviewers are fairly satisfied with the authors’ response and are now all positive to the acceptance. Although we are still curious how it works on other tasks (as the title says “mobile applications”), I think the paper provides enough insights valuable to the community, so I’d like to recommend acceptance.  
This paper proposes a modification to GCNs that generalizes the aggregation step to multiple levels of neighbors, that in theory, the new class of models have better discriminative power. The main criticism raised is that there is lack of sufficient evidence to distinguish this works theoretical contribution from that of Xu et al. Two reviewers also pointed out the concerns around experiment results and suggested to includes more recent state of the art SOTA results. While authors disagree that the contributions of their work is incremental, reviewers concerns are good samples of the general readers of this paper— general readers may also read this paper as incremental. We highly encourage authors to take another cycle of edits to better distinguish their work from others before future submissions.  
The paper provides a neat idea about explaining (linear) predictors based on designing ways of perturbing parameters. It is focused on linear models (which can still lead to non linear classifiers), but it is a relevant case, particularly for explainability.
The proposed approach seems to have elements of novelty, it is well presented  and  reasonably motivated by the authors. In addition, empirical results seem to be promising. However, although rebuttal helped to clarify some of the pending issues, there are concerns on the fact that the raised issue about "resolution dilemmas" does not find in the paper a quantitative treatment. Without that, it is difficult to fully understand how to drive the learning of useful structural landmarks. Thus, notwithstanding the paper seems to contribute in a significant way to the advancement of the GNN field, it still needs additional work to better develop  the proposed concepts in a quantitative theory. 
This submission receives mixed ratings initially. Two reviewers lean negatively while one reviewer is positive. The raised issues include  whether the proposed method can be adapted to other vision transformers, the design choice of pooling strategy, the computational time cost, the similarity to an existing work, and the influence of the proposed method on downstream tasks. In the rebuttal, the authors have addressed several issues such as pooling strategy analysis and time consumption.  There are still some issues not completely solved. The proposed method introduces K mean clustering on tokens between different layers. The K mean clustering is prevalent and the weighted clustering does not make the technical contribution sufficient. Also as a general token pooling operation, the proposed method shall be integrated into various types of vision transformers (e.g., vanilla ViT [a], ConViT [b]), rather than one single DeiT. Besides, the downstream tasks in DeiT are not conducted in the proposed method.   Overall, the AC feels the proposed method, although interesting, requires a major revision that addresses existing issues.  The authors are suggested to further improve the current submission and welcome to submit for the next venue.   [a]. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Dosovitskiy et al. ICLR 2021.  [b]. ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases. Ascoli et al. ICML 2021.
The reviewers still have several concerns about the paper after the author feedback stage: the novelty of the paper is not sufficient; the experimental results are not very encouraging. We encourage the authors fixing these issues in the next revision.
The addresses open set recognition, namely, detecting anomalous samples that belong to classes not observed during training.  It has been shown that existing methods fail on open world images. The current paper shows empirically that performance can be greatly improved if based on low dimensional features.   Reviewers had grave concerns about the novelty of the approach and the logic behind the workflow. They found merit in the paper but chose to retain their scores after reviewing the rebuttal. As a future recommendation, it would be useful to provide more evidence about what component of the method or workflow are novel and what makes them work well. 
This paper proposes techniques to lower the barrier to run large scale simulations under resource (compute) constraints. The key idea is to do batch simulation and policy learning on 1 or more GPUs without sacrificing the fps rate for rendering (~20k fps on 1GPU). The proposed setup and methods are evaluated on the point navgiation tasks on two environments namely Gibson and Matterport3D. One of the key ideas for rendering is to render a big tile of images for separate instantiations of the environment in parallel. This gives big speeds up to rendering and policy optimization.   ${\bf Pros}$: 1. Large number of FPS with smaller compute budget. Large scale Deep  RL research has been difficult to democratize due to the need for big compute budgets. Although this paper is more heavy on the engineering side, I think it can greatly accelerate research and therefore seems like a good fit for the ICLR community to consider.   2. The paper and proposed steps are clearly written and justified  ${\bf Cons}$: 1. This method is limited to environments where the observation space follows a particular structure. This is perhaps the biggest limitation of this approach but the underlying assumptions are reasonable and quite a few realistic environments will fall into this category.      During the rebuttal and discussion period, R2 raised concerns about ablations but was satisfied with author s response. R5 raised concerns about other prior work   CuLE (Dalton et al, NeurIPS 2020). However, this paper is concurrent work and does not tackle 3D simulation rendering as done in this paper. For these reasons I believe the paper does not have any big red flags or pending concerns. 
Although all reviewers agree that this is an interesting analysis of sample efficiency in Deep RL over the past few years, there is also a consensus that it is not enough material for an ICLR paper. I also share this sentiment, which motivates the "Reject" decision. This work could have been made stronger by reproducing previous results (even partially) and sharing the corresponding code, so as to provide a fair and controlled comparison of algorithms, and setting the stage for future progress in this area. In its form, it is better suited for a presentation at a workshop than for the main conference.
The authors proposed a new problem setting called Wildly UDA (WUDA) where the labels in the source domain are noisy. They then proposed the "butterfly" method, combining co teaching with pseudo labeling and evaluated the method on a range of WUDA problem setup. In general, there is a concern that Butterfly as the combination between co teaching and pseudo labeling is weak on the novelty side. In this case the value of the method can be assessed by strong empirical result.  However as pointed out by Reviewer 3, a common setup (SVHN< > MNIST) that appeared in many UDA paper was missing in the original draft. The author added the result for SVHN< > MNIST  as  a response to review 3, however they only considered the UDA setting, not WUDA, hence the value of that experiment was limited. In addition, there are other UDA methods that achieve significantly better performance on SVHN< >MNIST that should be considered among the baselines. For example DIRT T (Shu et al 2018) has a second phase where the decision boundary on the target domain is adjusted, and that could provide some robustness against a decision boundary affected by noise.  Shu et al (2018) A DIRT T Approach to Unsupervised Domain Adaptation. ICLR 2018. https://arxiv.org/abs/1802.08735  I suggest that the authors consider performing the full experiment with WUDA using SVHN< >MNIST, and also consider the use of stronger UDA methods among the baseline. 
The work falls under the setting of learning based sketching/compressive subsampling. It extends the work of Indyk et al 2019 (including sparsity pattern optimization and some theoretical enhancements).  The reviewers agree that while the conceptual novelty including the greedy optimization step is not too much, it is nonetheless interesting and is non trivial. However, given the highly competitive submissions at ICLR, the current scores are not sufficient for acceptance. 
The paper studies how to construct infinitely deep infinite width networks from a theoretical point of view, and uses the results of its theoretical analysis to design a weight initialization scheme for finite width networks. While the idea is interesting and the paper may contain novel theoretical contributions, the experimental results are weak, as pointed out by all three reviewers from several different perspectives. In particular, it seems that the presented theoretical analysis is useful mainly for weight initialization and hence has limited potential impacts. In addition, the authors have responded to neither the AC s question, nor a detailed anonymous comment that challenges the value of Proposition 1 given the previous work by Aronszajn.
Most of the reviewers had concerns on the model being considered and there are additional concerns in the paper s discussion on the experimental results. 
The reviewers agreed that the paper was too long (more than twice the recommended page limit not counting the appendix) and difficult to follow. They also pointed out that its central idea of learning the noise distribution in a VAE was not novel. While the shortened version uploaded by the authors looks like a step in the right direction, it was not sufficient to convince the reviewers.
Inspired by biological agents that have developed mechanisms like attention as an information bottleneck to help function more effectively under various constraints of life, this paper looks at an approach of learning a hard attention scheme by leveraging off the prediction errors of an internal world model. They demonstrate their approach via a simple but easy to understand 2D pixel multi agent game, a gridworld env, and also PhysEnv, to show the effectiveness of the learned hard attention, and go on to discuss interesting aspects such as curiosity attention.  Overall, I thought more highly of the paper than the reviewers, and might have proposed a score of 6 if I were a reviewer, but I also read each review and respect the points given by all four reviewers, and also agree with much of their feedback in the end. I think if this work was submitted to ALife (Journal or Conference), it might have been accepted. Not that those venues are easier, if anything they can often be more selective, but I think what ICLR (and similar venues like ICML) tends to expect is a bit different than what this paper offers.  To improve this work, I recommend following some of the reviewers  advice (especially R4), particularly on experimental design. Reviewers suggest that the current experiments are small and simple, but while true to some extent, I think more importantly missing are clear baseline methods to compare your approach against. What can your approach do that existing popular approaches in RL will totally fail at doing? It can be worthwhile to try your approach on a larger task domain, such as Atari (but perhaps modified) or ProcGen [1] to show the benefits of hard attention compared to existing approaches. For instance, some recent work [2] demonstrated that hard attention can help agents generalize to out of training domain tasks the agent has not seen before during training   something that traditional approaches without attention tend to fail at doing.  In the current state, the work will be a great workshop paper. But I recommend the authors to continue improving the work in the direction that can help the idea gain acceptance by the broader community.  [1] https://openai.com/blog/procgen benchmark/ [2] https://arxiv.org/abs/2003.08165 
This paper concerns a training procedure for neural networks which results in sparse connectivity in the final resulting network, consisting of an "early era" of training in which pruning takes place, followed by fixed connectivity training thereafter, and a study of tradeoffs inherent in various approaches to structured and unstructured pruning, and an investigation of adversarial robustness of pruned networks.  While some reviewers found the general approach interesting, all reviewers were critical of the lack of novelty, clarity and empirical rigour. R2 in particular raised concerns about the motivation, evaluation of computational savings (that FLOPS should be measured directly), and felt that the discussion of adversarial robustness was out of place and "an afterthought".  Reviewers were unconvinced by rebuttals, and no attempts were made at improving the paper (additional experiments were promised, but not delivered). I therefore recommend rejection. 
The paper proposes a MLP based architecture that makes extensive use of the shift operation on the feature maps. The model performs well on several vision tasks and datasets.  The reviews are mixed even after the authors  response. Main pros are that the proposed architecture is elegant and reasonable, and the experimental evaluation is thorough and strong. The main con is that the novelty is somewhat limited to some prior papers.  Overall, I recommend acceptance. The reviewers point out that the architecture is good and the results are strong. Similarities to prior works do not seem serious enough to warrant rejection   even an author of arguably the most related (concurrent) works   S2 MLP and S2 MLPv2   confirms that there is sufficient difference. Moreover, this is one of the first papers to show very strong results on detection and segmentation.
This paper studies inverse reinforcement learning through the prism of regularized Markov decision processes, by generalizing MaxEntIRL from the negative entropy to any strongly convex regularizer (as a side note, strict convexity might be enough for many results). The reviewers appreciated the clarity, the mathematical rigor and the empirical evaluation of this paper. They asked some questions and raised some concerns, that were mostly addressed in the rebuttal and the revision provided by the authors. This is a strong paper, for which the AC recommends acceptance. 
The paper proposed a novel one pass efficient streaming algorithm for estimating the number of triangles and four cycles. The concerns raised by reviewers were nicely addressed in the rebuttal and all the reviewers agree that the paper is above bar for publication.
The paper presents "recall traces", a model based approach designed to improve reinforcement learning in sparse reward settings. The approach learns a generative model of trajectories leading to high reward states, and is subsequently used to augment the real experience collected by the agent. This novel take on combining model based and model free learning is conceptually well motivated and is empirically shown to improve sample efficiency on several benchmark tasks.  The reviewers noted the following potential weaknesses in their initial reviews: the paper could provide a clearer motivation of why the proposed approach is expected to lead to performance improvements, and how it relates to learning (and uses of) a forward model. Details of the method, e.g., model parameterization is unclear, and the effect of hyperparameter choices is not fully evaluated.  The authors provided detailed replies to all reviewer suggestions, and ran extensive new experiments, including experiments to address questions about hyperparameter settings, and an entirely new use of the proposed model in a learning from demonstration setting. The authors also clarified the paper as requested by the reviewers. The reviewers have not responded to the rebuttal, but in the AC s assessment their concerns have been adequately addressed. The reviewers have updated their scores in response to the rebuttal, and the consensus is to accept the paper.  The AC notes that the authors seem unaware of related work by Oh et al. "Self Imitation Learning" which was published at ICML 2018. The paper is based on a similar conceptual motivation but imitates high value traces directly, instead of using a generative model. The authors should include a discussion of how their paper relates to this earlier work in their camera ready version.
This paper present a framework for creating meaning preserving adversarial examples. It then proposes two attacks within this framework: one based on k NN in the word embedding space, and another one based on character swapping.   Overall, the goal of constructing such meaning preserving attacks is very interesting. However, it is unclear how successful the proposed approach really is in the context of this goal.   Additionally, it is not clear how much novelty there is compared to already existing methods that have a very similar aim.
This paper proposes methodology to train binary neural networks.  The reviewers and authors engaged in a constructive discussion. All the reviewers like the contributions of the paper.  Acceptance is therefore recommended.
The paper presents a neural architecture based on neural memory modules to model the spatiotemporal traffic data. The reviewers think this is an important application of deep learning and thus fits the topic of ICLR. The writing and the novelty of the proposed method need improvement.
Significant spread of scores across the reviewers and unfortunately not much discussion despite prompts from the area chair and the authors. The most positive reviewer is the least confident one. Very close to the decision boundary but after careful consideration by the senior PCs just below the acceptance threshold. There is significant literature already on this topic. The "thought delta" created by this paper and the empirical results are also not sufficient for acceptance.
The goal of this paper is to study the dynamics of convergence of neural network training when weight normalization is used. This is an important and interesting area. The authors focus on analyzing such effect based on a recent theoretical trend which studies neural network dynamics based on the so called neural tangent kernel (NTK). The authors show an interesting phenomena of length direction decoupling. The reviewers raise various points some of which have been addressed by the authors in their response. Two main points not yet clearly addressed is (1) what is the novelty of the theoretical framework given existing literature and (2) what are the benefits of weight normalization based on this theory (e.g. generalization etc. ). The authors suggest improved convergence rate and overparameterization dependence (i.e. that with weight normalization the required width is decreased) as a possible advantage. However, as pointed out by reviewer 3 there are existing results which already obtain better results without weight normalization (the authors  response that this is only true in randomized scenarios is actually not accurate). Based on above I do not think the paper is ready for publication. That said I think this is a nice direction and well written paper. I recommend the authors revise and resubmit to a future venue. Some suggestions for improvements in case this is helpful (1) improve literature review and discussion of existing results (2) identify clear benefits to weight normalization. I doubt that improving overparameterization in existing form is one of them unless you provide a lower bound (I suspect one can eventually obtain even linear overparameterization i.e. number of parameters proportional to number of training data even in the NTK regime without weight normalization. The suggestion by the reviewer at looking at generalization might be a good direction to pursue.       
This paper proposes and end to end trainable architecture for data augmentation, by defining a parametric model for data augmentation (using spatial transformers and GANs) and optimizing validation classification error through the notion of influence functions. Experiments are reported on MNIST and CIfar 10.   This is a borderline submission. Reviewers found the theoretical framework and problem setup to be solid and promising, but were also concerned about the experimental setup and the lack of clarity in the manuscript. In particular, one would like to evaluate this model against similar baselines (e.g. Ratner et al) on a large scale classification problem. The AC, after taking these comments into account and making his/her own assessment, recommends rejection at this time, encouraging the authors to address the above comments and resubmit this promising work in the next conference cycle. 
this is an interesting approach to use reinforcement learning to replace CRF for sequence tagging, which would potentially be beneficial when the tag set is gigantic. unfortunately the conducted experiments do not really show this, which makes it difficult to see whether the proposed approach is indeed a viable alternative to CRF for sequence tagging with a large tag set. this sentiment was shared by all the reviewers, and R1 especially pointed out major and minor issues with the submission and was not convinced by the authors  response.
This paper presents a synthetic oversampling method for sequence to sequence classification problems based on autoencoders and generative adversarial networks.   All reviews reject the paper for two main reasons: 1 The novelty of the paper is not enough for ICLR as the idea of utilizing GAN for data sampling is common now. 2 The experimental is not convincing as authors did not compare with other leading oversampling methods.  The rebuttal did not well answer these two questions; thus I choose to reject the paper. 
The paper proposes learning the prior for AAEs by training a code generator that is seeded by the standard Gaussian distribution and whose output is taken as the prior. The code generator is trained by minimizing the GAN loss b/w the distribution coming out of the decoder and the real image distribution. The paper also modifies the AAE by replacing the L2 loss in pixel domain with "learned similarity metric" loss inspired by the earlier work (Larsen et al., 2015).  The contribution of the paper is specific to AAE which makes the scope narrow. Even there, the benefits of learning the prior using the proposed method are not clear. Experiments make two claims: (i) improved image generation over AAE, (ii) improved "disentanglement".  Towards (i), the paper compares images generated by AAE with those generated by their model. However, it is not clear if the improved generation quality is due to the use of decoder loss on the learned similarity metric (Larsen at al, 2015), or due to the use of GAN loss in the image space (ie, just having GAN loss over decoder s output w/o having a code generator), or due to learning the prior which is the main contribution of the paper. This has also been hinted at by AnonReviewer1. Hence, it s not clear if the sharper generated images are really due to the learned prior.  Towards (ii), the paper uses InfoGAN inspired objective to generate class conditional images. It shows the class conditional generated images for AAE and the proposed method. Here AAE is also trained on "learned similarity metric" and augmented with similar InfoGAN type objective so the only difference is in the prior. Authors say the performance of both models is similar on MNIST and SVHN but on CIFAR their model with "learned prior" generates images that match the conditioned upon labels better. However this claim is also subjective/qualitative and even if true, it is not clear if this is due to learned prior or due to the extra GAN discriminator loss in the image space   in other words, how do the results look for AAE + a discriminator in the image space, just like in the proposed model but without a code generator?  The t SNE plots for the learned prior are also shown but they are only shown when InfoGAN loss is added. The same plots are not shown for AAE with added InfoGAN loss so it is difficult to know the benefits of learning the code generator as proposed.  Overall, I feel the scope of the paper is narrow and the benefits of learning the prior using the method proposed in the paper are not clearly established by the reported experiments. I am hesitant to recommend acceptance to the main conference in its current form.  
The paper is focussed on proposing a new evaluation metric for evaluating untrained, randomly initialized neural network architectures towards predicting their accuracy/performance after training. The metric they propose is based on evaluating the gradient sign. The method shown to outperform existing approaches on NAS benchmarks.  The reviewers found the paper s idea simple but effective. The experimental evaluation and efficacy of the proposed method were the main strong points of the paper. The paper was also significantly improved during the discussion period both in terms of presentation and the scope of experiments/comparisons was enlarged.   While the metric is theoretically motivated, I personally found some of the theoretical statements weak in terms of assumptions/clarity. I would request the authors to consider taking this and other suggestions made by reviewers into account   Overall I recommend acceptance based on the strong and thorough experimental results shown by the paper on a problem of clear interest to the community.
This work presents an original analysis of using the weights of a neural network as a medium on which to hide information. Although the paper offers a novel perspective, its motivation and applicability remain unclear. As reviewer 3 points out, the proposed method does not seem very practical for any particular application, and the authors do not give a practical demonstration that shows the usefulness of the approach. It s not clear how the paper should be positioned with respect to previous work, and the proposed method is not directly compared with standard steganography schemes on metrics such as bandwidth, robustness etc, making it difficult to assess the value of the contribution. For these reasons I recommend rejecting the paper in its current form.
This paper proposes a method (via a novel objective) for feature alignment between source and target tasks in an unsupervised domain adaptation scenario.  Pro   the proposed approach is sensible in many realistic scenarios of distribution shift   the submission provides an extensive empirical evaluation establishing state of the art results on several benchmark tasks  Con   there is no thorough discussion of the the underlying assumptions (when should we expect them to hold? for shich types of tasks? which types of shifts? can they generally be reliably tested? from which type of data? unlabeled?)   one reviewer raised concerns over novelty, which should be more clearly addressed before publication   two reviewers raised concerns over use of target data for hyper parameter selection, which seem valid; these should be fixed or clearly explained (and implications of this be discussed) in subsequent versions of this work  I agree with concerns of the reviewers (the last two points), and would therefore not recommend this work for publication in the current state.
The paper presents a theoretical analysis of the expressive power of equivariant models for point clouds with respect to translations, rotations and permutations. The authors provide sufficient conditions for universality, and prove that recently introduced architectures, e.g. Tensor Field Networks(TFN), do fulfil this property.   The submission received positive reviews ; after rebuttal, all reviewers recommend acceptance and highlight the valuable paper modifications made by the authors to clarify the intuitions behind the proofs.   The AC also considers that this paper is a solid contribution for ICLR, which will draw interest for both theoreticians and practitioners in the community. \ Therefore, the AC recommends acceptance. 
This work proposes a model based optimization using an approximated normalized maximum likelihood (NML). It is an interesting idea and has the advantage of scaling to large datasets. The reviewers are generally positive and are satisfied with authors  response.  
All three reviewers agree on accepting the paper and think that the proposed approach will be of interest for those working in vdieo prediction.  The authors are asked to include the extra discussion with R3 as part of the paper and include the proposed changes by R2 to provide more thorough experimentation.  The paper is recommended as a poster presentation.
This paper proposed an ensemble of diverse models as a mechanism to protect models from theft.  The idea is quite novel. There are some concerns regarding the robustness of the hashing function (that I share), however not every paper has to be perfect, especially when it introduces a novel setup.   AC
The paper proposes an extension to the popular Generative Adversarial Imitation Learning framework that considers multi agent settings with "correlated policies", i.e., where agents  actions influence each other. The proposed approach learns opponent models to consider possible opponent actions during learning. Several questions were raised during the review phase, including clarifying questions about key components of the proposed approach and theoretical contributions, as well as concerns about related work. These were addressed by the authors and the reviewers are satisfied that the resulting paper provides a valuable contribution. I encourage the authors to continue to use the reviewers  feedback to improve the clarity of their manuscript in time for the camera ready submission.
This paper proposes an interesting new direction for low cost NAS. However, the paper is not quite ready for acceptance in its current form. The main area of improvement is around the generalizability of the score presented, both empirically and (ideally) theoretically. The two main directions of generalizability that would be worth investigating are 1) different image datasets (see comments around Imagenet 16) 2) different/larger search spaces. Even simple search spaces consisting of a few architectural modification starting from standard architectures (e.g. resnets) would go a long way in convincing the community that the proposed method generalizes past NasBench.
The authors have presented a simple yet elegant model to learn grid like responses to encode spatial position, relying only on relative Euclidean distances to train the model, and achieving a good path integration accuracy. The model is simpler than recent related work and uses a structure of  disentangled blocks  to achieve multi scale grids rather than requiring dropout or injected noise. The paper is clearly written and it is intriguing to get down to the fundamentals of the grid code. On the negative side, the section on planning does not hold up as well and makes unverifiable claims, and one reviewer suggests that this section be replaced altogether by additional analysis of the grid model. Another reviewer points out that the authors have missed an opportunity to give a theoretical perspective on their model. Although there are aspects of the work which could be improved, the AC and all reviewers are in favor of acceptance of this paper.
The paper proposes to take advantage of implicit preferential information in a single state, to design auxiliary reward functions that can be combined with the standard RL reward function.  The motivation is to use the implicit information to infer signals that might not have been included in the reward function.  The paper has some nice ideas and is quite novel.  A new algorithm is developed, and is supported by proof of concept experiments.  Overall, the paper is a nice and novel contribution.  But reviewers point out several limitations.  The biggest one seems to be related to the problem setup: how to combine inferred reward and the given reward, especially when they are in conflict with each other.  A discussion of multi objective RL might be in place.
In this paper, the authors study the decentralized empirical risk minimization problem with Reproducing Kernel Hilbert Space. I found the problem formulation and the solution quite interesting. The authors also answered the main comments of the reviewers. Even though part of the work is incremental, I feel that there is enough merit to accept this paper.
This paper is about an unsupervised method to learn new skills in non stationary environments by maximizing an intrinsic reward function. Experimental evaluations on OpenAI gym environments show that the proposed approach improves the diversity of the learned skills and is able to adapt to continuously changing environments.  This paper is borderline. After reading each other s reviews and the authors  feedback, the reviewers discussed the pros and cons of this work. Even if the reviewers have pointed out that the paper has some limitations, they agree that the paper represents a valuable contribution and have appreciated the improvements implemented by the authors during the rebuttal, thus reaching a consensus towards acceptance.  The authors need to update their paper according to what they have proposed in their response and they have to take into serious considerations all the reviewers  suggestions while they will prepare the camera ready version of their paper.
The paper builds on earlier work by Wang et al (2015) on Visual Concepts (VCs) and explores the use of VCs for few shot learning setting for novel classes.  The work, as pointed out by two reviewers is somewhat incremental in nature, with main novelty being the demonstration of utilities of VCs for few shot learning. This would not have been a big limitation if the paper had a carefully conducted empirical evaluation providing insights on the effect of various configuration settings/hyperparameters on the performance in few shot learning, which two of the reviewers (Anon3, Anon2) state are missing. The paper falls short of the acceptance threshold in its current form.  PS: The authors posted a github link to the code on Jan 12 which may potentially compromise the anonymity of the submission (though it was after all the reviews were already in) https://openreview.net/forum?id BJ_QxP1AZ&noteId BJaIDpBEM 
This paper presents a novel approach to learning in problems which have large action spaces with natural hierarchies. The proposed approach involves learning from a curriculum of increasingly larger action spaces to accelerate learning. The method is demonstrated on both small continuous action domains, as well as a Starcraft domain.  While this is indeed an interesting paper, there were two major concerns expressed by the reviewers. The first concerns the choice of baselines for comparison, and the second involves improving the discussion and intuition for why the hierarchical approach to growing action spaces will not lead to the agent missing viable solutions. The reviewers felt that neither of these were adequately addressed in the rebuttal, and as such it is to be rejected in its current form.
The review team appreciated the new Bayesian perspective offered by the submission, which lends itself well to selection and ranking, though some of them were still not convinced by the motivation (including in the private post rebuttal discussion, R3). The reviewers also identified many points for improvement. The paper was borderline and, given the lack of enthusiastic support from the reviewers, the program committee decided to reject it. We strongly encourage you to address the raised concerns and resubmit to a future venue.
The paper presents an approach to learn the surrogate loss for complex prediction tasks where the task loss is non differentiable and non decomposable. The novelty of the approach is to rely on differentiable sorting, optimizing the spearman correlation between the true loss and the surrogate. This leads to a pipeline that is simpler to integrate to existing works than approaches that try to learn a differentiable approximation to the task loss, and to better experimental results.  The paper is well written and the approach clearly presented. The reviewers liked the simplicity of the approach and the promising experimental results on a variety of challenging tasks (human pose estimation and machine reading).
The paper aims to encourage deep networks to have stable derivatives over larger regions under networks with piecewise linear activation functions.  All reviewers and AC note the significance of the paper. AC also thinks this is also a very timely work and potentially of broader interest of ICLR audience.
The paper proposes a DP method for generative modelling based on optimal transport. The reviewers agree that the novelty is limited in relation to prior work, while the results are not especially compelling either. So, even though this is a valid approach, correctness is not sufficient for acceptance at ICLR.  
 pros:   nicely written paper   clear and precise with a derivation of the loss function  cons:  novelty/impact: I think all the reviewers acknowledge that you are doing something different in the neural brainwashing (NB) problem than is done in the typical catastropic forgetting (CF) setting.  You have one dataset and a set of models with shared weights; the CF setting has one model and trains on different datasets/tasks.  But whereas solving the CF problem would solve a major problem of continual machine learning, the value of solving the NB problem is harder to assess from this paper...  The main application seems to be improving neural architecture search.  At the meta level, the techniques used to derive the main loss are already well known and the result similar to EWC, so they don t add a lot from the analysis perspective.  I think it would be very helpful to revise the paper to show a range of applications that could benefit from solving the NB problem and that the technique you propose applies more broadly.
The paper proposes a method for accented speech generation using GANs. The reviewers have pointed out the problems in the justification of the method (e.g. the need for using policy gradients with a differentiable objective) as well as its evaluation.
This paper deals with a problem of significant practical relevance: memory efficient neural networks. The authors propose some pruning methods for binary networks. However, several weaknesses were identified by the reviewers (novelty, lack of extensive experiments, problems with the presentation of the paper), and several valid points of concern were raised. These points of criticism were not adequately addressed, hence the paper in its current form cannot be recommended for publication.
The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by choosing more appropriate loss functions.  The submission then proposes TransComplEx to further improve results.  This paper received four reviews, with three recommending rejection, and one recommending weak acceptance.  A main concern was in the clarity of motivating the different models.  Another was in the relatively low performance of RotatE compared with [1], which was raised by multiple reviewers.  The authors provided extensive responses to the concerns raised by the reviewers.  However, at least the implementation of RotatE remains of concern, with the response of the authors indicating "Please note that we couldn’t use exactly the same setting of RotatE due to limitations in our infrastructure."  On the balance, a majority of reviewers felt that the paper was not suitable for publication in its current form.
This paper proposes an OOD evaluation framework under three categories: irrelevant input detection, novel class detection, and domain shift detection. As with several reviewers, the AC recognizes the importance and effort to distinguish between different cases of OOD detection, as well as the amount of experimental comparison across several prominent methods in literature (MSP, MC dropout, cosine similarity, ODIN, Mahalanobis).    Despite being well motivated, three knowledgeable reviewers find the paper not ready yet for publication at ICLR. The AC recommends a rejection, given the standing major concerns from the reviewers. The AC is hopeful that the paper can be significantly improved by     sufficiently discussing and highlighting the novel insights of the results.    a more rigorous definition of  "novel" vs. "irrelevant" inputs. There seem to be overlapping definitions between what Hsu et al. considered vs. this paper. In particular,  Hsu et al distinguish i) samples of a novel class but in a known domain, called semantic shift (S), and ii) samples of known class in a novel domain, called non semantic shift (NS), both of which are reconsidered in this paper. Therefore, the novelty of this submission is more precisely to distinguish within the category of semantic shift. The AC agrees that this might deem some more rigorous measurement and definition of "semantic closeness".    The AC also finds the evaluation of domain shift in Section 3.3.2 may be potentially misleading the community, as it falls out of the standard OOD scope. The notion of common corruption is closer to the robustness problem (which is how ML model predictions changes w.r.t some delta changes in the input space). The changes may not be substantial enough to be "out of distribution".  
Four knowledgeable referees have indicated reject. I agree with the most critical reviewer R4 that the model design lacks a clear and transparent motivation and that the experimental setup is not convincing, and so must reject.
The paper presents a new algorithm for augmenting RL training with human examples, and this is applied to learning safe driving policies. This algorithm is properly tested and compared to other relevant algorithms with favorable results. Reviewers agree that this is good work and that it should be published. Reviewers had multiple questions, which were in my opinion answered satisfactorily by the authors. Notably, the authors ran additional tests against other human in the loop RL algorithms with good results. In sum, this seems to be a solid paper, worth accepting.
This paper proposes to augment training data for theorem provers by learning a deep neural generator that generates data to train a prover, resulting in an improvement over the Holophrasm baseline prover. The results were restricted to one particular mathematical formalism   MetaMath, a limitation raised one by reviewer.   All reviewers agree that it s an interesting method for addressing an important problem. However there were some concerns about the strength of the experimental results from R4 and R1. R4 in particular wanted to see results on more datasets, an assessment with which I agree. Although the authors argued vigorously against using other datasets, I am not convinced. For instance, they claim that other datasets do not afford the opportunity to generate new theorems, or the human proofs provided cannot be understood by an automatic prover. In their words,   "The idea of theorem generation can be applied to other systems beyond Metamath, but realizing it on another system is highly nontrivial. It can even involve new research challenges. In particular, due to large differences in logic foundations, grammar, inference rules, and benchmarking environments, the generation process, which is a key component of our approach, would be almost completely different for a new system. And the entire pipeline essentially needs to be re designed and re coded from scratch for a new formal system, which can require an unreasonable amount of engineering."   It sounds like they ve essentially tailored their approach for this one dataset, which limits the generality of their approach, a limitation that was not discussed in the paper.   There is also only one baseline considered, which renders their experimental findings rather weak. For these reasons, I think this work is not quite ready for publication at ICLR 2020, although future versions with stronger baselines and experiments could be quite impactful.     
The authors present a method that utilizes intrinsic rewards to coordinate the exploration of agents in a multi agent reinforcement learning setting.   The reviewers agreed that the proposed approach was relatively novel and an interesting research direction for multiagent RL.  However, the reviewers had substantial concerns about writing clarity, the significance of the contribution of the propose method, and the thoroughness of evaluation (particularly the number of agents used and limited baselines).  While the writing clarity and several technical points (including addition ablations) were addressed in the rebuttal, the reviewers still felt that the core contribution of the work was a bit too marginal.  Thus, I recommend this paper to be rejected at this time.
This paper proposes an adaptive gradient method for optimization in deep learning called AvaGrad.  The authors argue that AvaGrad greatly simplifies hyperparameter search (over e.g. ADAM) and demonstrate competitive performance on benchmark image and text problems.  In thorough reviews, thorough author response and discussion by the reviewers (which are are all appreciated) a few concerns about the work came to light and were debated.  One reviewer was compelled by the author response to raise their recommendation to weak accept.  However, none of the reviewers felt strongly enough to champion the paper for acceptance and even the reviewer assigning the highest score had reservations.  A major issue of debate was the treatment of hyperparameters, i.e. that the authors tuned hyperparameters on a smaller problem and then assumed these would extrapolate to larger problems. In a largely empirical paper this does seem to be a significant concern.  The space of adaptive optimizers for deep learning is a crowded one and thus the empirical (or theoretical) burden of proof of superiority is high.  The authors state regarding a concurrent submission: "when hyperparameters are properly tuned, echoing our results on this matter", however, it seems that the reviewers disagree that the hyperparameters are indeed properly tuned in this paper.  It s due to these remaining reservations that the recommendation is to reject.  
This paper proposes question answering as a general paradigm to decode and understand the representations that agents develop, with application to two recent approaches to predictive modeling. During rebuttal, some critical issues still exist, e.g., as Reviewer#3 pointed out, the submission in its current form lacks experimental analysis of the proposed conditional probes, especially the trade offs on the reliability of the representation analysis when performed with a conditional probe as well as a clear motivation for the need of a language interface. The authors are encouraged to incorporate the refined motivation and add more comprehensive experimental evaluation for a possible resubmission.
The paper presents hierarchical Bayesian methods for modelling the full covariance structure in cases where noise dimensions cannot be assumed independent.  This is an important problem with potential practical importance. The work is solid.  Conceptual novelty in the work is somewhat limited.  The method is applied in the paper on hierarchical linear regression. It is claimed to be applicable to other methods as well, and the claim is plausible, but to be fully convincing, results and comparisons would need to be shown. The new extended discussion does help somewhat.  There was also discussion about whether ICLR is the best match for this work. This is not a strereotypical ICLR paper though is relevant.  Authors are encouraged to continue this line of work. 
This paper proposes techniques for differentially private training of ResNets inspired by SDEs. The idea has some promise but the paper does not give convincing evidence, either theoretical or empirical that it outperforms existing tehniques. Unfortunately, comparisons with existing techniques are presented in a misleading way that does not clearly provide the privacy parameters.  Another issue with the work is that the authors appear to be unaware of (and hence do not compare with) existing work on privacy of a single prediction (referred to as strategy II in this work).  Approaches for this problem are described in these theoretical works (and several follow ups)  * Dwork, Feldman. Privacy preserving Prediction. COLT 2018 * Bassily,Thakkar,Tahkurta. Model Agnostic Private Learning via Stability. NIPS 2018  Practical results are mentioned in PATE papers of Papernot et al. and more recent work  * van der Maaten,  Hannun. The Trade Offs of Private Prediction https://arxiv.org/abs/2007.05089
The paper proposes a new fine tuning method for improving the performance of existing anomaly detectors.  The reviewers and AC note the limitation of novelty beyond existing literature.  This is quite a borader line paper, but AC decided to recommend acceptance as comprehensive experimental results (still based on empirical observation though) are interesting.
Pros:    A new framework for learning sentence representations   Solid experiments and analyses   En Zh / XNLI dataset was added, addressing the comment that no distant languages were considered; also ablation tests  Cons:     The considered components are not novel, and their combination is straightforward    The set of downstream tasks is not very diverse (See R2)    Only high resource languages are considered (interesting to see it applied to real low resource languages)  All reviewers agree that there is no modeling contribution.  Overall, it is a solid paper but I do not believe that the contribution is sufficient. 
This paper compares autoencoder and GAN based methods for 3D point cloud representation and generation, as well as new (and welcome) metrics for quantitatively evaluating generative models.  The experiments form a good but still a bit too incomplete exploration of this topic.  More analysis is needed to calibrate the new metrics.  Qualitative analysis would be very helpful here to complement and calibrate the quantitative ones.  The writing also needs improvement for clarity and verbosity.  The author replies and revisions are very helpful, but there is still some way to go on the issues above. Overall, the committee is intersting and recommends this paper for the workshop track.
The paper introduces a method for using information directed sampling, by taking advantage of recent advances in computing parametric uncertainty and variance estimates for returns. These estimates are used to estimate the information gain, based on a formula from (Kirschner & Krause, 2018) for the bandit setting. This paper takes these ideas and puts them together in a reasonably easy to use and understandable way for the reinforcement learning setting, which is both nontrivial and useful. The work then demonstrates some successes in Atari. Though it is of course laudable that the paper runs on 57 Atari games, it would make the paper even stronger if a simpler setting (some toy domain) was investigated to more systematically understand this approach and some choices in the approach.
The paper contributes to the understanding of straight through estimation for single hidden layer neural networks, revealing advantages for ReLU and clipped ReLU over identity activations.  A thorough and convincing theoretical analysis is provided to support these findings.  After resolving various issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Valid criticisms of the presentation quality were raised during the review and response period, and the authors would be well served by continuing to improve the paper s clarity.
This paper received a majority vote for acceptance from reviewers and me. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *acceptance*. Here are the comments that I summarized, which include my opinion and evidence.  **Research Motivation and Problem**  This paper is well motivated by the agnostic of CPE assumption that the support of the positive data distribution cannot be contained in the support of the negative data distribution. To tackle this problem, the authors built an auxiliary probability distribution such that the support of the positive data distribution is never contained in the support of the negative data distribution.  **Technical Contribution**  The technical part is simple and clear. The regrouping idea is also easy to implement. The theoretical justification is a good complement of the proposed ReCPE algorithm.  **ReCPE does not affect its base if the assumption already holds**  The authors employed the synthetic datasets to verify this point. This is a plus.  **Experimental Results**  The authors demonstrated their ReCPE algorithm can be used as a booster on seven base PU classifiers.  **Presentation**  The presentation has been much improved with the guidance of one reviewer. But I found two extra minor ones. (1) "PU Learning"  > "PU learning" at the beginning of the second paragraph on Page 1. (2) Two typos in "9 real word datasets." on Page 7  > "9 real world dataset.", where the footnote should be placed after the period.  **Layout**  (1) Too many lines in Table 1. It is suggested to remove the horizontal lines among the same dataset, (2) Appendix should go after the main manuscript, rather than a separate file.  No objection from reviewers was raised to again this recommendation.
I thank the authors and reviewers for the lively discussions. Although reviewers agreed the work is interesting, there are some concerns about the significance of the results and experiments. None of the reviewers were strongly supportive of the paper while majority of them suggest that the paper needs a bit more work before being accepted. Also, reviewers suggest that the paper is not easy to follow and its writing should be improved. Given all, I think the paper , at the current stage, is below the accept threshold. I encourage authors to edit the paper according to the suggestions by the reviewers. 
This manuscript proposed biologically inspired modifications to convolutional neural networks including differences of Gaussians convolutional filter, a truncated ReLU, and a modified projected normalization layer. The authors  results indicate that the modifications improve performance as well as improved robustness to adversarial attacks.  The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work on robust model architectures. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and importance of the results. In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the approach and results. In the opinion of the AC,  the manuscript in its current state is borderline and could be improved with more convincing empirical justification.
The reviewers attempted to provide a fair assessment of this work, albeit with varying qualifications.  Nevertheless, the depth and significance of the technical contribution was unanimously questioned, and the experimental evaluation was not considered to be convincing by any of the assessors.  The criticisms are sufficient to ask the authors to further strengthen this work before it can be considered for a top conference.
 A clearly written paper. While the practical relevance that came up in the review remains, the analysis and discussion is important for a deeper understanding of the deeper connections between these two important areas of machine learning.
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
This paper presents a through study of generalization in visual representation learning. It compares in distribution generalization to out of distribution generalization using a comprehensive benchmark. The paper received very positive reviews from all reviewers. Reviewers agreed that the paper has several strengths: It is very well written, the presented benchmark is very useful and the analysis is thorough. One concern that was brought up by the reviewers was that a majority of the presented findings are expected and in a sense, known to the community. The authors have addressed this concern by pointing out that their findings are more fine grained than past works and that their proposed benchmark is a stepping stone towards measuring general robustness. I must note that in spite of this concern, all reviewers have maintained their strong acceptance scores. I agree with the reviewers. This paper makes a strong contribution to this important problem via its benchmark and analysis, which future works can build off of, and hence I recommend acceptance.
The Authors study the emergence of systematic generalization in neural networks. The paper studies a timely topic and presents a set of concrete results. For example, reviewer ZgRW emphasizes that a key strength of the paper is constructing simple datasets where systematicity emerges. I think indeed it is valuable, as systematicity is sometimes poorly defined and understood, so building a theoretical testbed might be very helpful.  However, the reviewers found important issues, which the rebuttal was unable to address. Perhaps the key issue (raised e.g. by reviewer 9QCY) is that results do not clearly generalize to more practically relevant settings. What is somewhat missing is a clear set of guidelines or implications for how to improve systematicity in more practically relevant neural networks.  Based on this and other issues raised by the reviewers, unfortunately, I have to recommend rejecting the paper. Thank you for your submission, and I hope that the review process will help you improve the work.
Four reviewers have evaluated this submission with one score 6 and three scores 8. Overall, reviewers like the work and note that *a rigorous and principled approach is taken by this work*. AC agrees and advocates an accept.
The submission describes a new two stage training scheme for multi modal image to image translation. The new scheme is compared to a single stage end to end baseline, and the advantage of the new scheme is demonstrated empirically. All three reviewers appreciate the proposed contribution and the quality improvement it brings over the baseline. At the same time, the reviewers see the contribution as incremental and not sufficient for an ICLR paper. The author response and paper adjustment have not changed the opinion of the reviewers, so the overall recommendation is to reject.
The paper studied multi objective reinforcement learning (MORL), and provided a Bayesian optimization approach for challenging MORL scenarios in several simulation environments. The reviewers generally find it interesting to account for robustness in a MORL setup, and all appreciate the algorithmic contributions. However, there were shared critical concerns among \ reviewers in the technical clarity and positioning of the work.   The paper has gone through substantial changes during the rebuttal period, which addressed some concerns regarding the experimental details; however, the major revision raised further issues that affects the clarity of the work. The reviewers are hence unconvinced that the paper is ready for publication. In addition to addressing the existing comments on clarifying the experimental details and properly positioning the work against prior art, a reorganization and optimization of the main content would be beneficial for future submission. 
There is no author response for this paper. The paper formulates a definition of easy and hard examples for training a neural network (NN) in terms of their frequency of being classified correctly over several repeats. One repeat corresponds to training the NN from scratch. Top 10% and bottom 10%  of the samples with the highest and the lowest frequency define easy and hard instances for training. The authors also compare easy and hard examples across different architectures of NNs. On the positive side, all the reviewers acknowledge the potential usefulness of quantifying easy and hard examples in training NNs, and R1 was ready to improve his/her initial rating if the authors revisited the paper.   On the other hand, all the reviewers and AC agreed that the paper requires (1) major improvement in presentation clarity   see detailed comments of R1 on how to improve as well as comments/questions from R3 and R2; try to avoid  confusing terminology such as ‘contradicted patterns’. R1 raised important concerns that the proposed notion of easiness is drawn from the experiment in Fig. 1 of Arpit et al (2017) which is not properly attributed. R3 and R2 agreed that in its current state the experimental results are not conclusive and often non informative. To strengthen the paper the reviewers suggested to include more experiments in terms of different datasets, to propose a better metric for defining easy and hard samples (see R3’s suggestions). We hope the reviews are useful for improving the paper. 
as r1 and r2 have pointed out, this work presents an interesting and potentially more generalizable extension of the earlier work on introducing noise as regularization in autoregressive language modelling. although it would have been better with more extensive evaluation that goes beyond unsupervised language modelling and toward conditional language modelling, but i believe this is all fine for this further work to be left as follow up.  r3 s concern is definitely valid, but i believe the existing evaluation set as well as exposition merit presentation and discussion at the conference, which was shared by the other reviewers as well as a programme chair.
This paper extends the recent work on continuous domain sparse attention mechanisms to use kernel parametrizations, and thus allow more flexible multi modal shapes. Continuous attention extends the standard attention mechanisms to continuous valued key/value/query functions, involving integrals over probability measures instead of sums over softmax weighted sums.   Kernel methods fit very well in the framework and provide great expressivity. Reviewers agree it is an interesting and well motivated idea. The contribution of incorporating kernel families in continuous attention seems substantially novel in comparison to the previous work on the topic.  The main concern, however, is that the paper focuses too much on the theory and not enough on the modeling benefits  enabled by flexible kernels. I would stress that this isn t a question of *improving performance* purely (although quantitative results would help!) but perhaps more of qualitative results, demonstrating e.g. multimodality, selectivity, interpretability.  I very much look forward to a revised version, which I expect would be a strong paper.
This paper presents a new method for solving the problem of inverting image classifier models. The authors introduce three new augmentation based techniques to do this. The techniques are validated using Vision Transformer and MLP models and compared against previous methods. The reviewers appreciate the problem that the paper aims to solve. However, the reviewers are not satisfied with the presentation and evaluation of the proposed approach. The main contribution of the paper is not presented clearly enough, according to the reviewers, and it remains unclear to them what aspect of model inversion the authors most want to improve on, and whether their proposed technique indeed achieves such an improvement. In their response, the authors do provide Inception scores that show that their inversion method improves the perceptual quality of generated images compared to previous approaches. The reviewers acknowledge the author response, but indicate that it does not fully resolve their concerns. I recommend that the authors update their paper to more clearly present their main contributions and conclusions, and to provide a more thorough comparison against previous methods, before submitting to another conference.
The main contribution is a Bayesian neural net algorithm which saves computation at test time using a vector quantization approximation. The reviewers are on the fence about the paper. I find the exposition somewhat hard to follow. In terms of evaluation, they demonstrate similar performance to various BNN architectures which require Monte Carlo sampling. But there have been lots of BNN algorithms that don t require sampling (e.g. PBP, Bayesian dark knowledge, MacKay s delta approximation), so it seems important to compare to these. I think there may be promising ideas here, but the paper needs a bit more work before it is to be published at a venue such as ICLR. 
This paper on automatic option discovery connects recent research on successor representations with eigenoptions. This is a solidly presented, conceptual paper with results in tabular and atari environments. 
The reviewers are unanimous in their opinion that this paper offers a novel approach to secure edge learning.  I concur.  Reviewers mention clarity, but I find the latest paper clear enough.
This submission proposes a new gating mechanism to improve gradient information propagation during back propagation when training recurrent neural networks.  Strengths:  The problem is interesting and important.  The proposed method is novel.  Weaknesses:  The justification and motivation of the UGI mechanism was not clear and/or convincing.  The experimental validation is sometimes hard to interpret and the proposed improvements of the gating mechanism are not well reflected in the quantitative results.  The submission was hard to read and some images were initially illegible.  The authors improved several of the weaknesses but not to the desired level.  AC agrees with the majority recommendation to reject.
The authors are strongly encouraged to elaborate further about the novelty of their method, as well as to give detailed (either theoretical or experimental) justifications for the design choices they make within the paper. Finally, the paper could benefit from additional experiments, as outlined in the reviews.
The reviewers reached a consensus that the paper was not ready to be accepted in its current form. The main concerns were in regard to clarity, relatively limited novelty, and a relatively unsatisfying experimental evaluation. Although some of the clarity concerns were addressed during the response period, the other issues still remained, and the reviewers generally agreed that the paper should be rejected.
Two reviewers are negative on this paper while the other reviewer is positive. Overall, the paper does not make the bar of ICLR. A reject is recommended.
This paper considers the situation where a set of reinforcement learning tasks are related by means of a Boolean algebra.  The tasks considered are restricted to stochastic shortest path problems. The paper shows that learning goal oriented value functions for subtasks enables the agent to solve new tasks (specified with boolean operations on the goal sets) in a zero shot fashion.  Furthermore, the Boolean operations on tasks are transformed to simple arithmetic operations on the optimal action value functions, enabling the zero short transfer to a new task to be computationally efficient. This approach to zero shot transfer is tested in the four room domain without function approximation and a small video game with function approximation.  The reviewers found several strengths and weaknesses in the paper.  The paper was clearly written.  The experiments support the claim that the method supports zero shot composition of goal specified tasks.  The weaknesses lie in the restrictive assumptions.  These assumptions require deterministic transition dynamics, reward functions that only differ on the terminal absorbing states, and having only two different terminal reward values possible across all tasks.  These assumptions greatly restrict the applicability of the proposed method.  The author response and reviewer comments indicated that some aspects these restrictions can be softened in practice, but the form of composition described in this paper is restrictive.  The task restrictions also seem to limit the method s utility on general reinforcement learning problems.  The paper falls short of being ready for publication at ICLR.  Further justification of the restrictive assumptions is required to convince the readers that the forms of composition considered in this paper are adequately general. 
This paper proposes to study Auto induced Distribution Shift (ADS), the phenomenon that models can create a feedback loop: the predictions of a model influence user behaviors when it is deployed, which, in turn, affects the accuracy measure of the model. The paper empirically shows that a meta learning algorithm called PBT causes a distribution shift instead of maximizing accuracy. While the premise of this paper is interesting, the proposed frameworks are very similar to the idea of strategic behavior in machine learning, and of "Performative Prediction" (Juan C. Perdomo, Tijana Zrnic, Celestine Mendler Dünner, Moritz Hardt). However, this line of work is neither cited nor discussed in this paper. In addition, the paper is hard to read in certain parts. We encourage the authors to compare their work with performative prediction. We hope the authors find the reviews helpful.
This paper introduced a probabilistic extension to a pipeline for 3D scene geometry reconstruction from large scale point clouds.  All reviewers recognized the significance of the proposed approach and praised the simplicity of deriving a probabilistic version of Generative Cellular Automata that performs well in a number of reconstruction benchmarks. Authors were responsive during rebuttal and managed to clarify the concerns raised about the limited scope of the experiments and certain parameters involved, and also raise one reviewer s scores.
This paper reveals a novel interpretation of the well established CD for energy based model training as an adversarial game through conditional NCE. The paper could be potential impactful for the community of EBMs.  There are several points should be addressed in final version:  1, Based on such an interpretation, the number of steps becomes a tunable parameters, rather than in vanilla understaning in CD family (the larger, the better in terms of approximation, by with more computation cost).  2, It is okay to stop the gradient when solving an adversarial game as the paper discussed. However, propagating the gradient through the component is also another choice, which leads to the algorithm proposed in [1].  It will be interesting to discuss these in the paper.  [1] Sohl Dickstein, Jascha, Peter Battaglino, and Michael R. DeWeese. "Minimum probability flow learning." arXiv preprint arXiv:0906.4779 (2009).  
One reviewer is positive, while the others recommend rejection. The authors did not submit a rebuttal, thus the reviewers kept their original assessment.
This paper proposes an idea of using a pre trained language model on a potentially smaller set of text, and interpolating it with a k nearest neighbor model over a large datastore. The authors provide extensive evaluation and insightful results. Two reviewers vote for accepting the paper, and one reviewer is negative. After considering the points made by reviewers, the AC decided that the paper carries value for the community and should be accepted.
Pros and cons of the paper can be summarized as follows:  Pros: * The underlying idea may be interesting * Results are reasonably strong on the test set used  Cons: * Testing on the single dataset indicates that the model may be of limited applicability * As noted by reviewer 2, core parts of the paper are extremely difficult to understand, and the author response did little to assuage these concerns * There is little mathematical notation, which compounds the problems of clarity  After reading the method section of the paper, I agree with reviewer 2: there are serious clarity issues here. As a result, I do cannot recommend that this paper be accepted to ICLR in its current form. I would suggest the authors define their method precisely in mathematical notation in a future submission.
The paper explores the use of RL (actor critic) for planning the expansion of a metro subway network in a City. The reviewers felt that novelty was limited and there was not enough motivation on what is special about this application, and what lessons can be learned from this exercise.  
The reviewers (all experts in this area) appreciated the novelty of the idea, though they felt that the experimental results (samples and Inception scores) did not provide convincing evidence value of this method over already established techniques. The authors responded to the concerns but were not able to address the issue of evaluation due to time constraints. The idea is likely sound but evaluation does not meet the bar, it may make a good contribution as a workshop paper.
The paper presents a new regularizer based on singular value decomposition in embedding space to avoid model collapse. The reviewes liked the simplicity of the idea, but there were some remaining concerns regarding the experiments. Moreover, two reviewers mentionned some concerns with respect to the clarity of the paper. While some concerns have been addressed by the rebuttal, in particular regarding the clarity of the paper, the concerns regarding the experiments remained, and the reviewers agreed that the paper needs a revision before publication.   The main directions of improvement are to make the comparison with previous published results clearer, in particular comparing different methods with better hyperparameter tuning, and test on larger datasets. 
This paper revisits the information bottleneck principle, but in terms of the compression inherent in the weights of a neural network, rather than the representation. This gives the resulting IB principle a PAC Bayes flavor. The key contribution is a generalization bound based on optimizing the objective dictated by this principle, which is then tractably approximated and experimentally verified. Reviews raise concerns about assumptions made to achieve the tractable version and a public discussion debates whether this is truly a PAC Bayes bound. The authors address these adequately. Another concern is whether improvements in experiments can be ascribed to the new objective. Authors add new experiments in support of this. Additional concerns about the clarity of certain aspects of the paper were or were promised to be addressed by the authors. Overall, the perspective of this paper, its technical contributions, and experimental evaluations appear to be worthwhile to share with the community, as they advance the applicability of the information bottleneck principle.
The paper proposes an approach to jointly learning a data clustering and latent representation.  The main selling point is that the number of clusters need not be pre specified.  However, there are other hyperparameters and it is not clear why trading # clusters for other hyperparameters is a win.  The empirical results are not strong enough to overcome these concerns.
This paper studies the problem of unsupervised domain translation. Here translation does not refer to language translation. Instead, it refers to the idea of transferring high level semantic features. Specifically, the authors look at digit style transfer (between MNIST/postal address numbers and SVHN/street view house numbers) and Sketches to Reals. The visuals look very convincing and the empirical results are strong, too. There is one weaker review but the authors address the concerns in their response and the reviewer did unfortunately not respond despite promting.
After the rebuttal period the ratings on this paper increased and it now has a strong assessment across reviewers. The AC recommends acceptance.
This paper focuses on leveraging static and dynamic sparsity in efficient robust training. The proposed methods can significantly mitigate the robust generalization gap while retaining competitive performance (standard/robust accuracy) with substantially reduced computation budgets. The philosophy behind sounds quite interesting to me, namely, sparsity allevating overfitting and improving training efficiency simultaneously. This philosophy leads to two novel algorithms design, i.e., static Robust Bird training, and dynamic Flying Bird training.  The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, most of us have agreed to accept this paper for publication! Please include the additional experimental results in the next version.
This paper proposes an extension to learning a representation: it motivates, proposes and evaluates a new regularizer term that promotes smoothness via enforcing the representation to be geometry preserving (isometry, conformal mapping of degree k). Comparisons with a standard VAE and FMVAE (Chen et al. 2020) are shown and experiments are provided on CelebA with several different attributes as target classification tasks.   The paper has received extensive reviews and the authors have successfully answered most of the concerns raised, mostly regarding comparisons to other techniques that try to introduce a regularization based on the properties of the Jacobian of the decoder network.  The appendix has been extended as a result of the rebuttal and the paper could be accepted.  Notes:  I find the formulation based on the notion of the isometric decoder somewhat surprising as the encoder is a key object of interest that controls the nature of the representation. The authors should clarify the assumption 3 in 3.3 better by the consideration of potentially $dim(z) << dim(x)$, how the isometry of the decoder effects the encoder,   Additionally, for the latent space flattening an ablation using SVD (merely a linear mapping for $i(\cdot)$) could be considered.  Reviewer ZGHS has noted that they raise their grade to 6 in their comment, but this is still not currently reflected.
This is an interesting paper on an important topic.  The reviewers identified a variety of issues both before and after the feedback period; I urge the authors to consider their comments as they continue to refine and extend their work.
This paper provides a fresh application of tools from causality theory to investigate modularity and disentanglement in learned deep generative models. It also goes one step further towards making these models more transparent by studying their internal components. While there is still margin for improving the experiments, I believe this paper is a timely contribution to the ICLR/ML community. This paper has high variance in the reviewer scores. But I believe the authors did a good job with the revision and rebuttal. I recommend acceptance.
The reviewers were split about this paper: on one hand they appreciated the clarity and the experimental improvments in the paper, on the other they were concerned about the novelty of the work. After going through it and the discussion I have decided to vote to accept this paper for the following reasons: (a) the potential impact of the work, (b) the simplicity of the idea, and (c) promise of release of open source code. I think these things make the paper a strong contribution to ICLR. The only thing I would like to see added, apart from the suggestions detailed by the reviewers, is a small discussion on the carbon footprint of training such largescale graph networks. The authors motivated the work by saying it could have a beneficial impact for modelling energy which is needed to combat climate change. However, we know from recent results that such large scale models also have a non trivial emission footprint. So I d like to see the authors specifically calculate the carbon footprints of the models they trained. There are tools to help with this such as: https://mlco2.github.io/impact/  With this addition I think this paper will not only make a large impact on graph network training but also start a discussion of how to responsibly decide training, taking environmental impact into account.
This paper introduces a novel SE(3) equivariant graph matching network, along with a keypoint discovery and alignment approach, for the problem of protein protein docking, with a novel loss based on optimal transport. The overall consensus is that this is an impactful solution to an important problem, whereby competitive results are achieved without the need for templates, refinement, and are achieved with substantially faster run times.
SAT is NP complete (Karp, 1972) due its intractable exhaustive search. As such, heuristics are commonly used to reduce the search space. While usually these heuristics rely on some in domain expert knowledge, the authors propose a generic method that uses RL to learn a branching heuristic. The policy is parametrized by a GNN, and at each step selects a variable to expand and the process repeats until either a satisfying assignment has been found or the problem has been proved unsatisfiable. The main result of this is that the proposed heuristic results in fewer steps than VSIDS,  a commonly used heuristic.   All reviewers agreed that this is an interesting and well presented submission. However, both R1 and R2 (rightly according to my judgment) point that at the moment the paper seems to be conducting an evaluation that is not entirely fair. Specifically, VSIDS has been implemented within a framework optimized for running time rather than number of iterations, whereas the proposed heuristic is doing the opposite. Moreover, the proposed heuristic is not stressed test against larger datasets. So, the authors take a heuristic/framework that has been optimized to operate specifically well on large datasets (where running time is what ultimately makes the difference) scale it down to a smaller dataset and evaluate it on a metric that the proposed algorithm is optimized for. At the same time, they do not consider evaluation in larger datasets and defer all concerns about scalability to the one of industrial use vs answering ML questions related to whether or not it is possible to  “stretch existing RL techniques to learn a branching heuristic”. This is a valid point and not all techniques need to be super scalable from iteration day 0, but this being ML, we need to make sure that our evaluation criteria are fair and that we are comparing apples to apples in testing hypotheses. As such, I do not feel comfortable suggesting acceptance of this submission, but I do sincerely hope the authors will take the reviewers  feedback and improve the evaluation protocols of their manuscript, resulting in a stronger future submission.
First, I d like to apologize once again for failing to secure a third reviewer for this paper. To compensate, I checked the paper more thoroughly than standard.  The area of online adaptation of the learning rate is of great importance and I appreciate the authors  effort in that direction. The authors carefully abundantly cite the research on gradient based hyperparameter optimization but I would have appreciated to also see past works on stochastic line search (for instance  "A stochastic line search method with convergence rate") or statistical methods ("Using Statistics to Automate Stochastic Optimization").  The issue with these methods is that, despite usually very positive claims in the paper, they are not that competitive against a carefully tuned fixed schedule and end up not being used in practice. Hence, it is critical to develop a convincing experimental section to assuage doubts. Unfortunately, the experimental section of this work is a bit lacking, as pointed by both reviewers. I would like to comment on two points specifically:   First, no plot uses wall clock time as the x axis. Since the authors state that it can be up to 4 times as slow per iteration, the gains compared to a carefully tuned schedule are unclear.   Second, the use of a single (albeit two variants) dataset also leads to skepticism. Datasets have vastly different optimization properties and, by not using a wide range of them, one can miss the true sensitivity of the proposed algorithm.  While I do not think that the paper is ready for publication, I feel like there is a clear path to an improved version that could be submitted to a later conference.
This paper analyzes problems of existing threshold meta learners and attentional meta learners for few shot learning in polythetic classifications. The threshold meta learners such as prototypical networks require exponential number of embedding dimensionality, and the attentional meta learners are susceptible to misclassification. The authors proposed a simple yet effective method to address these problems, and demonstrated its effectiveness in their experiments. This paper discusses meta learning from a very unique perspective as commented by a reviewer, and clearly explained problems of widely used meta learning methods. However, this paper focus on prototypical networks and matching networks even though there have been proposed many meta learning methods. Some existing methods seem not to have the problems of prototypical networks and/or matching networks. In addition, the practical benefits of the proposed approach are not well demonstrated. Although the additional experiments in the author response addressed some concerns of the reviewers, they are not enough to demonstrate the effectiveness of the proposed method.
This paper proposes an efficient implementation of piecewise linear functions.  While this paper tackles a problem of large apparent interest, as noted by the reviewers the paper (1) is pretty far from the domain of the average ICLR paper, and (2) not written with the high standards of clarity that would make it accessible to the average ICLR reader. I am not impugning on the merits of the paper itself, but would suggest that the authors both take the reviewer s advice with regards to the clarity issues (among other) and consider submitting to the Systems for ML workshop, a systems conference, a compilers conference, or some other venue with a larger percentage of qualified readers (and reviewers).
The paper investigates the effect of soft labels in knowledge distillation from the perspective of sample wise bias variance tradeoff. They observe that during training the bias variance tradeoff varies sample wisely. and under the same distillation temperature setting, we  distillation performance is negatively associated with the number of regularization samples. But removing them altogether hurts the performance (the authors show empirical evidence of this). Based on some observations about regularization samples, the authors propose the weighted soft labels to handle the tradeoff. Experiments on standard datasets show that the proposed method can improve the standard knowledge distillation.  pros.  the paper is written clearly.  through the review period the authors added additional experiments suggested by the reviewers and enhances experimental results. The experiment results are convincing and the authors have now added explanations on hyperparameter choices.  the mathematical setting is now clear after incorporating reviewer s comments.  the missing related work as suggested by reviewers is added  cons.  comparison with results of Zitong Yang et al 2020[1] is missing.  I thank the authors for incorporating the changes requested by reviewers. Please add comparison with result of [1] in the final version.  [1] Rethinking Bias Variance Trade off for Generalization of Neural Networks Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, Yi Ma 
This paper presents good empirical results on an important and interesting task (translation between several language pairs with a single model). There was solid communication between the authors and the reviewers leading to an improved updated version and consensus among the reviewers about the merits of the paper.
This paper brings recent innovations in reinforcement learning to bear on a tremendously important application, treating sepsis.  The reviewers were all compelled by the application domain but thought that the technical innovation in the work was low.  While ICLR welcomes application papers, in this instance the reviewers felt that the technical contribution was not justified well enough.  Two of the reviewers asked for a more clear discussion of the underlying assumptions of the approach (i.e. offline policy evaluation and not missing at random).  Unfortunately, lack of significant revisions to the manuscript over the discussion period seem to have precluded changes to the reviewer scores.  Overall, this could be a strong submission to a conference that is more closely tied to the application domain.  Pros:   Very compelling application that is well motivated   Impressive (possibly impactful) results   Thorough empirical comparison  Cons:   Lack of technical innovation   Questions about the underlying assumptions and choice of methodology
This paper proposes a diversity loss and a topological prior to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. These loss terms significantly improve the efficiency in finding trojaned triggers. The experiments results show that the proposed method performs substantially better than the baselines on the Trojaned MNIST/CIFAR10 and TrojAI datasets, respectively. This paper shows detailed ablation study with great empirical performance. Some reviewers have doubts about the experimental comparisons and some of the assumptions made in the algorithm. Overall, the thorough experimental investigation fo the proposed method makes this paper worthy of publication and widely being shared.
The authors propose a technique for weight pruning that leaves block diagonal weights, instead of unstructured sparse weights, leading to faster inference.  However, the experiments demonstrating the quality of the pruned models are insufficient.   The authors also discuss connections to random matrix theory; but these connections are not worked out in detail.
The paper introduces an approach to self train a source domain classifier on unlabeled data from the target domain, considering the few shot learning setting when there is significant discrepancy between the source and target domains. While the reviewers pointed out a few weaknesses, such as somewhat limited methodological novelty  and lack of comparisons with other methods, they all recommend acceptance as final decision. The paper is beautifully written. The proposed method is very simple, but yields excellent results in a very practical problem, which should be of wide interest to the ICLR community. The experimental evaluation is rigorous and the ablation studies are convincing. The AC agrees with the decision made by the reviewers and recommends acceptance.
The paper introduces a modification of batch normalization technique. In contrast to the original batch normalization that normalizes minibatch examples using their mean and standard deviation, this modification uses weighted average  of mean and standard deviation from the current and all previous minibatches. The authors then provide some theoretical justification for the superiority of their variant of BatchNorm.  Unfortunately, the empirical demonstration of the improved performance seems not sufficient and thus fairly unconvincing. 
The paper introduces a new step size rule for the extragradient/mirror prox algorithm, building upon and improving the results of Bach & Levy for the deterministic convex concave setups. The proposed adaptation of EG/Mirror prox   dubbed AdaProx in the submitted paper   has the rate interpolation property, which means that it provides order optimal rates for both smooth and nonsmooth problems, without any knowledge of the problem class or the problem parameters for the input instance. The paper also demonstrates that the same algorithm can handle certain barrier based problems, using regularizers based on the Finsler metric.  The consensus of the reviews was that the theory presented in the paper is solid and interesting. The main concerns shared by a subset of the reviews were regarding the practical usefulness of the proposed method. In particular, the method exhibits large constants in the convergence bounds and cannot handle stochastic setups. Further, the empirical evidence provided in the paper was deemed insufficient to demonstrate the algorithm s competitiveness on learning problems. If possible, the authors are advised to provide more convincing empirical results in a revised version, or, alternatively, to tone down the claims regarding the practical performance of the method.
This paper received 2 borderline accepts, 1 accept, and 1 reject.  This paper was discussed on the forum and no consensus was reached. The two reviewers who rated the paper as borderline accept emphasized that the biological claims are overblown, that the intellectual contributions (the initialization scheme and partial training) are incremental from a statistical learning perspective, and that the potential applications for the future (like alternate learning rules) are too speculative. I agree with both of these reviewers (and the negative reviewer) that the biological rationale is problematic and the approach is not credible as a model of biology. It is not evaluated as a computer vision model either. And I completely agree with the point raised by several reviewers that there is simply no data about how many synaptic updates to target. Hence, statements regarding % of total synaptic updates and % of brain matches seem empty without a precise target. For all these reasons, I recommend this paper be rejected.
This paper proposes a new training technique to produce a learned model robust against adversarial attacks   without explicitly training on example attacked images. The core idea being that such a training scheme has the potential to reduce the cost in terms of training time for obtaining robustness, while also potentially increasing the clean performance. The method does so by proposing a version of label smoothing and doing two forms of data augmentations (gaussian noise and mixup).   The reviewers were mixed on this work. Two recommended weak reject while one recommended weak accept. All agreed that this work addressed an important problem and that the proposed solution was interesting. The authors and reviewers actively engaged in a discussion, in some cases with multiple back and forths. The main concern of the reviewers is the inconclusive experimental evidence. Though the authors did demonstrate strong performance on PGD attacks, the reviewers had concerns about some attack settings like epsilon and how that may unfairly disadvantage the baselines. In addition, the results on CW presented a different story than the results with PGD.   Therefore, we do not recommend this work for acceptance in its current form. The work offers strong preliminary evidence of a potential solution to provide robustness without direct adversarial training, but more analysis and explanation of when each component of their proposed solution should increase robustness is needed.  
This paper described a system for deriving PDDL (Planning Domain Description Language) operator descriptions from unlabeled visual image pairs.  The goal is to construct STRIPS like descriptions with preconditions, add lists and delete lists for operators that can explain the state transitions seem in the image pairs.  This is combined with a neural form of inductive logic programming (ILP) which historically performs a similar task from logical descriptions rather than images.  While this topic is appropriate for ICLR, the work is fairly incremental and the experiments are limited to the 8 puzzle which, according to a reviewer, is the easiest of the tasks.  In spite of boarder line scores, no rebuttal was provided.  So my recommendation is to not accept the paper.
The paper explores an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) that is better than random initialization and the approach is tested on various MNIST and TIMIT data sets with positive results.  Reviewer 3 raised concerns about the breadth of experiments and novelty. Reviewer 2 recognized that the model performs well on its MNIST baselines and had concerns about applicability to larger settings. Reviewer 1 acknowledges a very well written paper, but again raises concerns about the thoroughness of the experiments. The authors responded to all three reviewers, responding that the tasks were chosen to match existing work and that the approach is complementary to LSTMs to solve different tasks. Overall the reviewers did not re adjust their ratings.  There remains questions on scalability and generality, which makes the paper not yet ready for acceptance. We hope that the reviews support the authors further research.
The main idea proposed by the work is interesting. The reviewers had several concerns about applicability and the extent of the empirical work. The authors responded to all the comments, added more experiments, and as reviewer 2 noted, the method is interesting because of its ability to handle local noise. Despite the author s helpful responses, the ratings were not increased, and it is still hard to assess the exact extent of how the proposed approach improves over state of the art.   Because some concerns remained, and due to a large number of stronger papers, this paper was not accepted at this time.
This paper presents an approach for training GCNs by learning to select subgraphs to train on to improve efficiency when transferring the model to larger graphs. In the proposed method, a meta model and a light weight GCN are trained iteratively in turns. Results are presented on medium to large graph datasets such as Reddit, Flickr, Yelp, PPI, and OGB Product.  The reviewers agree that the method and the results are interesting, and that the topic addressed in the paper is important, but that the paper generally needs more work in terms of presentation, motivation, experimental evaluation, and theoretical analysis to meet the bar for acceptance, which the authors also acknowledge during their rebuttal. 
Main summary: Novel rule for scaling learning rate, known as gain ration, for which the effective batch size is increased.  Discussion:  reviewer 2: main concern is reviewer can t tell if it s better of worse than linear learning rate scaling from their experiment section. reviewer 3: novlty/contribution is a bit too low for ICLR. reviewer 1: algorthmic clarity lacking. Recommendation: all 3 reviewers recommend reject, I agree.
All reviewers agreed that, despite the lack of novelty, the proposed method is sound and correctly linked to existing work. As the topic of automatically learning the stepsize is of great practical interest, I am glad to have this paper presented as a poster at ICLR.
This work studies the task of sound source localization from multi channel audio. An approach to design a wavelet like filter bank for audio feature extraction is proposed.  After discussion, all reviewers have given this work borderline ratings. Concerns were raised about the quality of the writing, missing related work, experimental methodology, and especially with regards to confounding factors in the experimental results, which make it difficult to assess the individual merit of the different components in the proposed system (i.e. features vs. transformer model). The authors have addressed this to some extent with additional experiments in the updated version of the manuscript, but this revealed that the gains obtained by the proposed feature extraction method in isolation are actually quite modest. This is in contrast with how the paper is written, with much more emphasis on this particular contribution than seems to be warranted by the empirical results.  Additionally, I believe the manuscript would benefit from a more careful and thorough revision to improve clarity and accessibility, beyond what is possible within a single review cycle. Therefore I am recommending rejection.
The main contribution of the paper is a technique for training GANs which consists in progressively increasing the resolution of generated images by gradually enabling layers in the generator and the discriminator. The method is novel, and outperforms the state of the art in adversarial image generation both quantitatively and qualitatively. The evaluation is carried out on several datasets; it also contains an ablation study showing the effect of contributions (I recommend that the authors follow the suggestions of AnonReviewer2 and further improve it). Finally, the source code is released which should facilitate the reproducibility of the results and further progress in the field.  AnonReviewer1 has noted that the authors have revealed their names through GitHub, thus violating the double blind submission requirement of ICLR; if not for this issue, the reviewer’s rating would have been 8. While these concerns should be taken very seriously, I believe that in this particular case the paper should still be accepted for the following reasons: 1) the double blind rule is new for ICLR this year, and posting the paper on arxiv is allowed; 2) the author list has been revealed through the supplementary material (Github page) rather than the paper itself; 3) all reviewers agree on the high impact of the paper, so having it presented and discussed at the conference would be very useful for the community.
The paper focuses on adversarial domain adaptation, and proposes an approach inspired from the DANN. The contribution lies in additional terms in the loss, aimed to i) align the source and target prototypes  in each class (using pseudo labels for target examples); ii) minimize the variance of the latent representations for each class in the target domain.   Reviews point out that the expected benefits of target prototypes might be ruined if the pseudo labels are too noisy; they note that the specific problem needs be more clearly formalized and they regret the lack of clarity of the text. The sensitivity w.r.t. the hyper parameter values needs be assessed more thoroughly.   One also notes that SAFN is one of the baseline methods; but its best variant (with entropic regularization) is not considered, while the performance thereof is on par or greater than that of PACFA for ImageCLEF Da; idem for AdapSeg (consider its multi level variant) or AdvEnt with MinEnt.   For these reasons, the paper seems premature for publication at ICLR 2020. 
The paper is clearly written and well motivated, but there are remaining concerns on contributions and comparisons.  The paper received mixed initial reviews. After extensive discussions, while the authors successfully clarified several important issues (such as computation efficiency w.r.t splitting) pointed out by Reviewer 4 (an expert in the field), they were not able to convince him/her about the significance of the proposed network compression method.   Reviewer 4 has the following remaining concerns:  1) This is a typical paper showing only FLOPs reduction but with an intent of real time acceleration. However, wall clock speedup is different from FLOPs reduction. It may not be beneficial to change the current computing flow optimized in modern software/hardware. This is one of major reasons why the reported wall clock time even slows down. The problem may be alleviated with optimization efforts on software or hardware, then it is unclear how good/worse will it be when compared with fine grain pruning solutions (Han et al. 2015b, Han et al. 2016 & Han et al. 2017), which achieved a higher FLOP reduction and a great wall clock speedup with hardware optimized (using ASIC and FPGA);  2) If it is OK to target on FLOPs reduction (without comparison with fine grain pruning solutions),    2.1) In LSTM experiments, the major producer of FLOPs   the output layer, is excluded and this exclusion was hidden in the first version. Although the author(s) claimed that an output layer could be compressed, it is not shown in the paper. Compressing output layer will reduce model capacity, making other layers more difficult being compressed.    2.2) In CNN experiments, the improvements of CIFAR 10 is within a random range and not statistically significant. In table 2, "Regular low rank MobileNet" improves the original MobileNet, showing that the original MobileNet (an arXiv paper) is not well designed. "Adaptive Low rank MobileNet" improves accuracy upon "Regular low rank MobileNet", but using 0.3M more parameters. The trade off is unclear.  In addition to these remaining concerns of Reviewer 4, the AC feels that the paper essentially modifies the original network structure in a very specific way: adding a particular nonlinear layer between two adjacent layers. Thus it seems a little bit unfair to mainly use low rank factorization (which can be considered as a compression technique that barely changes the network architecture) for comparison. Adding comparisons with fine grain pruning solutions (Han et al. 2015b, Han et al. 2016 & Han et al. 2017) and a large number of more recent related references inspired by the low rank baseline (M. Jaderberg et al 2014) , as listed by Reviewer 4, will make the proposed method much more convincing. 
While the idea of revisiting regression via classification is interesting, the reviewers all agree that the paper lacks a proper motivating story for why this perspective is important. Furthermore, the baselines are weak, and there is additional relevant work that should be considered and discussed.
The proposed Bi BloSAN is a two levels  block SAN, which has both parallelization efficiency and memory efficiency. The study is thoroughly conducted and well presented.  
This was a contentious paper, with quite a large variance in the ratings, and ultimately a lack of consensus. After reading the paper myself, I found it to be a valuable synthesis of common usage of saliency maps and a critique of their improper interpretation. Further, the demonstration of more rigorous methods of evaluating agents based on salience maps using case studies is quite illustrative and compelling. I think we as a field can agree that we’d like to gain better understanding our deep RL models. This is not possible if we don’t have a good understanding of the analysis tools we’re using.  R2 rightly pointed out a need for quantitative justification for their results, in the form of statistical tests, which the authors were able to provide, leading the reviewer to revise their score to the highest value of 8. I thank them for instigating the discussion.  R1 continues to feel that the lack of a methodological contribution (in the form of improving learning within an agent) is a weakness. However, I don’t believe that all papers at deep learning conferences have to have the goal of empirically “learning better” on some benchmark task or dataset, and that there’s room at ICLR for more analysis papers. Indeed, it’d be nice to see more papers like this.   For this reason, I’m inclined to recommend accept for this paper. However this paper does have weaknesses, in that the framework proposed could be made more rigorous and formal. Currently it seems rather adhoc and on a task by task basis (ie we need to have access to game states or define them ourselves for the task). It’s also disappointing that it doesn’t work for recurrent agents, which limits its applicability for analyzing current SOTA deep RL agents. I wonder if authors can comment on possible extensions that would allow for this. 
Dear authors,  The topic of variance reduction in optimization is timely and the reviewers appreciated your attempt at circumventing the issues faced with the current popular methods.  They however had a concern about the significance of the results, which I echo:   First, there have been previous attempts at variance reduction which share some similarity with yours, for instance "No more pesky learning rate", "Topmoumoute online natural gradient algorithm" or even Adam (which does variance reduction without mentioning it).   The fact that previous similar methods exist is a non issue should yours perform better. However, the absence of stepsize tuning in the experimental evaluation is a big issue as the performance of an iterative algorithm is highly sensitive to it.  Finally, the link between flatness of the minimum and generalization is dubious, as mentioned for instance by Dinh et al. (2017).  As a consequence, I cannot accept this work for publication to ICLR but I encourage you to address the points of the reviewers should you wish to resubmit it to a future conference. 
The paper studies the nearest neighbor search problem for objects embedded in hyperbolic space. It develops a graph based approach to NNS in hyperbolic space, showing (interestingly) that the time complexity of graph based NNS can be lower in hyperbolic space than in Euclidean space under some assumptions. This nice theoretical analysis feels like an intuitive result, as hyperbolic space is in some sense more graph like/tree like then Euclidean space, so it is not too surprising for graph based methods to perform better here. What s really cool about this theoretical result is to see something that is _easier_ in hyperbolic space—normally things are harder in hyperbolic space due to its great volumes and significant curvature. The discussion among reviewers was mostly positive, with the majority of reviewers leaning towards acceptance and multiple reviewers expressing that the author response and revision addressed their concerns and raising their scores accordingly. There is some question as to whether the revision is too large and should go through peer review again, but on balance I think it is close enough to the original submission that it can be reasonably accepted here. Therefore, following the majority opinion of the reviewers, I will recommend acceptance.
This paper considers the problem of (biological) sequence design and optimization. The authors made an interesting yet important case that in certain sequence design tasks, a simple evolutionary greedy algorithm could be competitive with the increasingly complex contemporary black box optimization models.  Most reviewers appreciate the design of the open source simulation environment in benchmarking AdaLead (and other competing algorithms) in a number of biological sequence design tasks (e.g. TF binding, RNA, and protein). However, there is a common concern that the experimental results are not fine grained enough to explain the outperforming results of the proposed algorithm. There are also unresolved comments on missing important BO baselines in the empirical study. As a purely empirical work, these appear to be important concerns. While these results appear to be useful for the domain of biological sequence design, the reviewers are unconvinced that the proposed algorithm is significantly novel, or the results are sufficiently compelling to merit an acceptance to this venue. 
The paper proposes a new way to understand why neural networks generalize well. They introduce the concept of ensemble robustness and try to explain DNN generalization based on this concept. The reviewers feel the paper is a bit premature for publication in a top conference although this new way of explaining generalization is quite interesting.
This paper proposes an approach for active learning in CNNs. The method computes the expected reduction in the predictive variance across a representative set of points and selects the next data point to be queried from the same set.   Pros:   The method is rather simple and seems practical.    The paper is generally well written.  Cons:   The novelty of the paper is limited, as it essentially applies a known approach to CNNs.   The performance gains presented in experiments seem rather mild, and may not justify using this method.
This paper addresses a method for generating meta tasks via latent space interpolation using a generative model, trained on the unlabeled dataset, to solve the unsupervised meta learning. The method seems to be sound, but it lacks MiniImage Net experiments, which is the main concern raised by most of reviewers. During the author responses, new empirical results on MiniImage Net were added. During the discussion period with reviewers, I communicated with the reviewer with most negative comments. He/she was not fully satisfied, claiming that the protocol used for obtaining new MinImage Net results was slightly unfair. It is suspected that the authors used a generative model trained on the ImageNet training set (of which miniImageNet is a subset) for LASIUM and  did not present results from only using miniImageNet meta training data because it was not competitive with prior work. It should be clarified in the final paper. However, we arrived at the consensus that the paper is worth being presented.   
This paper proposes Feature Contractive Learning (FCL), a training framework that takes a more nuanced view of robustness, refining it to the sensitivity of the feature.  There are some differing opinions among the reviewers, with some applauding the simplicity of this new take on robustness while others are unsure of its underlying definitions and relationship to adversarial robustness.  The authors claimed to have clarified some of these points in their rebuttal / revision, but unfortunately, there was not much follow up discussion by the reviewers.  Ultimately, there are still enough lingering issues that rejection is warranted.
The paper proposed an optimization based defense against model stealing attacks.  A criticism of the paper is that the method is computationally expensive, and was not demonstrated on more complex problems like ImageNet.  While this criticism is valid, other reviewers seem less concerned by this because the SOTA in this area is currently focused on smaller problems.  After considering the rebuttal, there is enough reviewer support for this paper to be accepted.
This paper presents a new perspective for understanding reinforcement learning policies based on meta states, as an effort to improve the explainability of RL control policies. After reviewing the revised paper and reading the comments from the reviewers, here are my comments:     The paper is well written and very concise.   The strategy is novel and deserves merit.   The utility of the explanation is not well described.   The main concerns of the proposal are the utility of the explanation (that is not well described) and its usage in large discrete state spaces or continuous state spaces domains.    From the above, it is difficult to see the contribution and applicability of the paper in a clear manner.
The paper proposes an efficient method to train generative models on multimodal data using a contrastive approach. Usually training such models requires significant training data to be able to learn patterns. The authors propose a variational autoencoder approach that enables multimodal learning of models using a data efficient approach, and shows the effectiveness of the approach on challenging datasets.  The authors have mostly addressed the feedback of the reviewers and done some of the necessary changes to the paper (e.g., adding more results and missing related work). They should make sure to address any lingering concerns about the paper, mentioned by the reviewers in their post rebuttal feedback.
This paper gives a coding theory interpretation of VAEs and uses it to motivate an additional knob for tuning and evaluating VAEs: namely, the tradeoff between the rate and the distortion. This is a useful set of dimensions to investigate, and past work on variational models has often found it advantageous to penalize the latent variable and observation coding terms differently, for broadly similar motivations. This paper includes some careful experiments analyzing this tradeoff for various VAE formulations, and provides some interesting visualizations. However, as the reviewers point out, it s difficult to point to a single clear contribution here, as the coding theory view of variational inference is well established, and the VAE case has been discussed in various other works. Therefore, I recommend rejection. 
The paper reviews and draws connections between several parameter efficient fine tuning methods.  All reviewers found the paper addresses an important research problem, and the theoretical justification and empirical analyses are convincing.
As the public post indicates, significant deliberation went into this decision. However, the core criticism remains: the primary contribution of this paper, Theorem 1, is somewhat incremental. It is acknowledged that MI is an important problem and understanding its intricacies is worthwhile, but the present paper s contributions in this space remains narrow. A more thorough exploration of the points brought up in the latest discussion and author response might help strengthen the paper. ​In particular, more careful discussion and systematic discussion and exploration of relationships between various MI attack efficacy measures (accuracy vs positive accuracy) and privacy notions (pure vs approx DP   it wasn t totally clear how broad the result in Section 5.2.1 is without a precise theorem statement) would strengthen the paper. Additionally, while it indeed seems that the positive accuracy bound given also suffices to protect against the type of attack mentioned (where 1% of the datapoints are highly vulnerable), it is unclear if this is necessary. This feeds into the previous point: it would be valuable to get a more systematic understanding of the various MI efficacy measures and how they interact with DP. Finally, it is now appreciated that the Sablayrolles et al (2019) result worked under an unusual model restriction, though deficiencies of their result does not necessarily make this result stronger (as an aside, I believe their restriction is so that they can get a tight understanding of behavior in other settings, and DP protections were somewhat of an afterthought). The authors are encouraged to further build on this work, potentially in the directions suggested, to get a more thorough understanding of the relationships between DP and MI attacks
This paper applies Dirichlet distribution to the latent variables of a VAE in order to address the component collapsing issues for categorical probabilities. The method is clearly presented, and extensive experiments are carried out to prove the advantage against VAEs with other prior distributions.  The main concern of the paper is the limited novelty. The main methodology contribution of this paper is to combine the decomposition a Dirichlet distribution as Gamma distributions, and approximating Gamma component with inverse Gamma CDF, but both components are common practices.   R3 also points out that the paper is distracted by two different messages the authors try to convey. The presentation and experiments are not designed to provide a cohesive message. The concern is not solved in the authors  feedback.  Based on the current reviews, this paper does not meet the standard for ICLR publication. Despite the limited novelty in the proposed model, if the paper could be revised to show that a simple modification is good for solve one problem with general applications, it would make a good publication in a future venue.
This is an unusual, but interesting submission. Can we use a simple "quantum computer" (in fact, physical system) to solve classification problems in ML? A single photon passes through the screen. Its state is described by the complex vector. A quantum computer makes a unitary linear transformation on this state in such a way that it maximizes the overlap with a corresponding class. Such a model can be parametrized by conventional means, and trained and later possibly realized by an quantum system  Pros:   1.  The area of QC is very important, and such papers shed a new light on the subject.  2. Inspiration to the ICLR community to work on in this area. 3. Technically correct.   Cons:  1. The accuracies are far from SOTA and use very toy datasets. It is not clear, how to get to the accuracies needed in practice. 2. The actual computational speed of inference is not clear. 3. Discussion of more complicated models and their possibility is necessary. 4. Quite a few misprints are in the text which need to be fixed in the final version.    
All the reviewers think that the work is significant and new. Therefore, they support the paper to be published at ICLR 2022. Given the strong results and the “accept” consensus from the reviewers, I accept the paper as “spotlight”. The authors should implement all the reviewers’ suggestions into the final version.
Although two reviewers have given score 6, the other reviews have clearly indicated that the paper is not good enough for ICLR. The paper does not give any new insight into the considered problem, many existing papers are ignored, and the only interesting part about evaluation of label importance is rather very shallow borrowing ideas from other fields without any deep discussion or analysis.
This paper presents a method for relational inference in multi agent/multi object trajectory prediction tasks. Different from the neural relational inference (NRI) model [1], the presented method is able to model time varying relations. Experimental results on physics simulations and sports games (basketball) show benefits over variants of the NRI model.  The reviewers agree that the presented method is mostly solid, that the experiments are insightful, and that this is generally a well written paper. The authors, however, have apparently overlooked recent related work [2] (dNRI) that proposes a very similar model. In the light of dNRI, it is difficult to argue for the novelty of the presented approach, and the paper needs to undergo a revision in order to more clearly differentiate it from the dNRI model, and to resolve the other concerns raised by the reviewers.  [1] Kipf et al., Neural Relational Inference for Interacting Systems (ICML 2018) [2] Graber et al., Dynamic Neural Relational Inference (CVPR 2020)
The paper is concerned with modeling multi relational data with joint hierarchical structure. For this purpose, the authors extend box embeddings to multi relational settings, supporting the modeling of cross hierarchy edges and generalizing from a subset of the transitive reduction. The reviewers highlight that the paper is, overall, well written and organized, relevant to the ICLR community, and that the proposed method offers promising experimental results. Furthermore, the author s rebuttal clarified some concerns of the initial reviews (e.g., relation to GumbelBox, comparison to additional baselines etc.) and improved the manuscript.  However, after rebuttal there exist still concerns regarding the current version. Reviewers raised concerns regarding novelty, clarity, and the empirical evaluation (importantly modeling more than 2 relations; it would also be good to understand more clearly why some of the newly added multi relational hyperbolic baselines perform worse than uni relational Poincare embeddings). While the paper and the proposed method clearly have promise, I agree with reviewers that the manuscript would require an additional revision to clarify these points. Given the positive aspects of the paper, I d strongly encourage the authors to revise and resubmit their work given this feedback.
Authors do not respond to significant criticism   e.g. lack of a critical reference Reviewers unanimously reject. 
This is a paper that is actively discussed.  The general sentiment is that this paper aims to address an important set of questions. While the technique could be improved with more novelty, the empirical study is extensive. The concerns are about how to interpret the results, or rather whether the empirical evidence fully supports the the claim/hypothesis.  After discussion and rebuttal, the reviewers improved their scores (and one reviewer remained at "weaker marginally above threshold").  The AC read the paper and the discussion. One value the AC sees that the discussion threads between the authors and the reviewers provide a significant amount of scientific value   the questions to be answered are hard and might indeed require further refinements in framing and conceptualization, better techniques,  strong power in experimental designs to rule exclusively various hypotheses.  Thus, the AC recommends acceptance.
This paper presents a method to combine graph convolutional neural networks (GCNs) with generative adversarial networks (GANs) for graph based semi supervised learning.  **Strengths:**   * It is a reasonable attempt to combine GCN with GAN for semi supervised node classification.   * The proposed method is general in that it can work with different graph neural networks.  **Weaknesses:**   * The novelty of this work is limited.   * The proposed method, GraphCGAN, has no significant performance improvement over state of the art methods.   * The writing has much room to improve in terms of both clarity and the linguistic quality.  Since both the novelty and the significance of this paper in its current form are limited, it is premature for publication. There is consensus among all the reviewers that this paper is not up to the acceptance standard of ICLR. 
This paper attempts to connect the expressivity of neural networks with a measure of topological complexity. The authors present some empirical results on simplified datasets. All reviewers agreed that this is an intriguing line of research, but that the current manuscript is still presenting preliminary results, and that further work is needed before it can be published. 
This paper studies different properties of the top eigenspace of the Hessian of a deep neural network and their overlap. It raised quite a lot of discussion, which finally went in not very constructive way. The reviewers generally agree that the paper has potential, but the actual contribution is limited.  Pros:    The idea that top eigenspaces between different models have high overlap is interesting   The explanation that these structures can be explained by Kronecker product approximation of the Hessian.  Cons:    The connection to PAC Bayes is unclear and seems artificial.   Many of the related work is missing   The models and datasets are too simple, and general conclusions can not be made on such kind of models. Much more testing is needed to verify the claims, including state of the art architectures and datasets.
This work develops a method for learning camouflage patterns that could be painted onto a 3d object in order to reliably fool an image based object detector.  Experiments are conducted in a simulated environment.  All reviewers agree that the problem and approach are interesting.  Reviewers 1 and 3 are highly positive, while Reviewer 2 believes that real world experiments are necessary to substantiate the claims of the paper.  While such experiments would certainly enhance the impact of the work, I agree with Reviewers 1 and 3 that the current approach is sufficiently interesting and well developed on its own.
The paper propose a scheme to enable optimistic initialization in the deep RL setting, and shows that it s helpful.  The reviewers agreed that the paper is well motivated and executed, but had some minor reservations (e.g. about the proposal scaling in practice). In an example of a successful rebuttal two of the reviewers raised their scores after the authors clarified the paper and added an experiment on Montezuma s revenge.  The paper proposes a useful, simple and practical idea on the bridge between tabular and deep RL, and I gladly recommend acceptance.
This paper proposes a continual learning method that uses anchor points for experience replay. Anchor points are learned with gradient based optimization to maximize forgetting on the current task. Experiments MNIST, CIFAR, and miniImageNet show the benefit of the proposed approach.  As noted by other reviewers, there are some grammatical issues with the paper.   It is missing some important details in the experiments. It is unclear to me whether the five random seeds how the datasets (tasks) are ordered in the experiments. Do the five random seeds correspond to five different dataset orderings? I think it would also be very interesting to see the anchor points that are chosen in practice. This issue is brought up by R4, and the authors responded that anchor points do not correspond to classes. Since the main idea of this paper is based on anchor points, it would be nice to analyze further to get a better understanding what they represent.  Finally, the authors only evaluate their method on image classification. While I believe the technique can be applied in other domains (e.g., reinforcement learning, natural language processing) with some modifications, without providing concrete empirical evidence in the paper, the authors need to clearly state that their proposed method is only evaluated on image classification and not sell it as a general method (yet).  The authors also miss citations to some prior work on memory based parameter adaptation and its variants.  Regardless all of the above issues, this is still a borderline paper. However, due to space constraint, I recommend to reject this paper for ICLR.
This paper presents a way of adapting an HMC based posterior inference algorithm. It s based on two approximations: replacing the entropy of the final state with the entropy of the initial state, and differentiating through the MH acceptance step. Experiments show it is able to sample from some toy distributions and achieves slightly higher log likelihood on binarized MNIST than competing approaches.  The paper is well written, and the experiments seem pretty reasonable.  I don t find the motivations for the aforementioned approximations very convincing. It s claimed that encouraging entropy of P_0 has a similar effect to encouraging entropy of P_T, but it seems easy to come up with situations where the algorithm could "cheat" by finding a high entropy P_0 which leads straight downhill to an atypically high density region. Similarly, there was some reviewer discussion about whether it s OK to differentiate through the indicator function; while we differentiate through nondifferentiable functions all the time, it makes no sense to differentiate through a discontinuous function. (This is a big part of why adaptive HMC is hard.)  This paper has some promising ideas, but overall the reviewers and I don t think this is quite ready. 
This paper studies a problem of graph translation, which aims at learning a graph translator to translate an input graph to a target graph using adversarial training framework. The reviewers think the problem is interesting. However, the paper needs to improve further in term of novelty and writing. 
This paper is proposed to address neural network pruning at initialization with the help of meta gradients considering the high order relations between loss and optimization of trainable sub network. The paper is well organized and written with the clear logic. The discussions of related works, as well as their limitations, are comprehensive. To verify the proposed method, the authors have tested it on various benchmarks with different settings. Overall, the meta learning idea for model pruning is relatively new, which may bring more inspirations to the community.
This paper offers a refinement of the information theoretic characterization of the generalization of models obtained via SGD. This is assessed on some basic neural architectures and inspires the use of new regularizers. Overall, even though the perspective of this paper is not novel, the presented results appear to be clearer and tighter than prior instances of the same ideas. This was appreciated by most reviewers. The few clarity and organization concerns that were raised by the reviewers were adequately addressed by the authors. Overall, the paper deserves to be shared with the community.
The paper proposes a hierarchical policy architecture with two substituent policies, "go" and "stop", and a controller mechanism for switching between them on every step (either a rule or a learned network), taking inspiration from neuroscience concepts of inhibition. Both are trained via Soft Actor Critic on the subset of states assigned to them and comparisons are made against a baseline. The use case targeted is the repurposing of pre trained agents to new or updated environments.  Reviewers regarded the method as sound, technically correct, and involving illustrative experiments (although perhaps picking problems too carefully adapted to the solution being presented), and were positive on the general direction of taking inspiration from neuroscience. Reviewer y7rr found the details unclear, recommended more focus on a concrete realization of the general method, and questioned the differences with more traditional hierarchical RL; while many specific inquiries were addressed the reviewer s broad concerns about contextualization remained. Reviewer dDqD had similar concerns around confusing presentation and positioning within the broader literature on "multi task RL, non stationary environments, online/continual learning, etc.", and the discussion unfolded similarly   many specific concerns addressed but fundamental issues remaining. 6ibM, like y7rr, raised the question of why one should stop at 2 policies rather than N policies, noted the under discussed relationship to options, and questioned the starting point of SAC, and while this was clarified to be about value functions rather than policies, the reviewer still thought this was an ill justified choice that rendered the system "brittle", and remained unhappy with baseline choices not extending beyond SAC based agents. tm8g had similar concerns about clarity and in particular that reward engineering seemed central; the authors clarified that this was not the case.  There is wide consensus among qualified reviewers that the presentation (and in particular situating the method with respect to prior work) is inadequate for publication, and I am inclined to agree. As y7rr put it, "evaluating its importance and correctness is hard" without adequate context on the relationships to in particular existing work on hierarchical, multi task and continual learning. While the direction appears interesting, unfortunately the hard work of contextualizing one s contribution is an utterly essential part of the scientific enterprise; without it we risk retreading well explored terrain while merely wearing slightly different boots. I encourage the authors to further clarify their presentation incorporating the valuable feedback from the reviewers on this aspect.
This work addresses new insights in the imitation learning setting, and shows how a popular type of approach can be extended in a principled way to the off policy learning setting. Several requests for clarification were addressed in the rebuttal phase, in particular regarding the empirical evaluation in off policy settings. The authors improved the empirical validation and overall clarity of the paper. The resulting manuscript provides valuable new insights, in particular in its principled connections, and extension to previous work.
The paper takes the perspective of "reinforcement learning as inference", extends it to the multi agent setting and derives a multi agent RL algorithm that extends Soft Actor Critic. Several reviewer questions were addressed in the rebuttal phase, including key design choices. A common concern was the limited empirical comparison, including comparisons to existing approaches. 
This work proposes to use the MAML meta learning approach in order to tackle the typical problem of insufficient demonstrations in IRL.  All reviewers found this work to contain a novel and well motivated idea and the manuscript to be well written. The combination of MAML and MaxEnt IRL is straightforward, as R2 points out, however the AC does not consider this to be a flaw given that the main novelty here is the high level idea rather than the technical details.  However, all reviewers agree that for this paper to meet the ICLR standards, there has to be an increase in rigorousness through (a) a more close examination of assumptions, sensitivity of parameters and connections to imitation learning (b) expanding the experimental section.
The paper argues that a successful backdoor attack on classifiers is connected with further fundamental security issues. In particular they demonstrate and not only an original backdoor trigger but also other triggers can be inserted by anyone with access to the classifiers. Furthermore, the alternative triggers may appear very different from the original triggers, which confirms the claim in the paper s title that such classifiers are "fundamentally broken".  The paper offers an interesting insight into the features of poisoned classifiers. However, such insight is diminished by the fact that the proposed attack requires a substantial manual interaction. The user must manually analyze the adversarial examples generated for robustified classifiers in order to determine the key parameters of alternative triggers. While manual intervention as such does not undermine the main observation of the paper, this makes an automatic exploitation of this idea hardly feasible and hence decreases the significance of the paper s main result. 
This paper proposes a model selection technique for classification problems, called mutated validation (MV), based on randomizing the labels of the training set. The idea is interesting but not well served by the presentation of the objectives and the experimental results provided in the paper. The discourse is confusing because it is not clear what is the purpose of model selection here. Usually, model selection aims at finding a model with lowest generalization error. This is a well defined goal, and the performance regarding this goal can be measured by the expected test error that is usually estimated from a test set. In this paper, the goal of model selection is not precisely defined, but it cannot be as usual, since some models that minimize test error are said to overfit. Most experimental results show that models selected by MV differ from the ones selected by cross validation (CV), without providing an objective measure of the relative merits of MV and CV (see details below). Although the authors do not make this clear in the paper, they argue in the discussion that the merit of their approach is to select a "simple" model that avoids overfitting. Their message should be clearly stated in the paper, and it should be supported by experiments displaying simplicity *and* test error, or by any experimental result showing the objective benefits of the proposed MV, possibly combined with CV as some of the reviewers suggested. I therefore recommend rejection, but with encouragement to pursue this work, by defining precisely the formal objectives pursued in this work with model selection, and by measuring the benefits of MV on these formalized objectives.  Details: The arguments in Figure 2 and Table 2 are not substantiated and are clearly not applicable to a model selection mechanism that would aim to select the model with the lowest expected error (or complexity). The "obviously ill fitting rectangle shaped decision boundaries" are perfect with respect to expected test error and simply described in Occam s razor terms.  Similarly, most results are subjective:    Figure 3 shows that MV and CV are different (on the left), and that CV is a better estimate of the test error (on the right).    Figure 4 shows some differences between CV and MV, with no clear way to judge which would be better.    Table 4: variance says nothing about relevance; choosing an arbitrary value before seeing the data gives a zero variance.    Figure 6, 7: again, subjective result    Figure 8: no comparison    Figure 10: label swapping or random label replacement is the same in binary classification, there is just a difference in the parameterization (swapping 20% of the labels is the same as randomizing 40% of the labels).    Figure 12: I am not sure what is drawn here, but it seems to be related to the training error only.     Regarding objective test results:    Figure 5 is an experimental result that shows objective differences between CV and MV, and appear to be marginally in favor of MV for selecting the model with lowest test error (in 3 out of 8 graphs). However, there is no joint optimization on the 2 hyper parameters: there is not a single data set for which the best value of the test error is identical in the two graphs (wrt dropout, wrt learning rate). In other words, the graphs for dropout rate and/or learning rate are provided for a suboptimal choice of the other hyperparameter.   Figure 9: CV is more closely related to the test error than MV.   Figure 11: MV chooses models that do not achieve the highest test accuracy.  As a side note, I think that the proposition could be also positioned with respect to papers that presented similar ideas, where model selection is based on stability, either with unlabeled examples [1] or by modifying the training set [2].   [1] Dale Schuurmans, Finnegan Southey: Metric Based Methods for Adaptive Model Selection and Regularization. Mach. Learn. 48(1 3): 51 84 (2002)  [2] Olivier Bousquet, André Elisseeff: Stability and Generalization. J. Mach. Learn. Res. 2: 499 526 (2002)
This work presents a reconstruction GAN with an additional classification task in the objective loss function. Evaluations are carried out on medical and non medical datasets.   Reviewers raise multiple concerns around the following:    Novelty (all reviewers)   Inadequate comparison baselines (all reviewers)   Inadequate citations. (R2 & R3)  Authors have not offered a rebuttal. Recommendation is reject. Work may be more suitable as an application paper for a medical conference or journal. 
This paper proposes and evaluates a formulation of graph convolutional networks for multi relation graphs. The paper was reviewed by three experts working in this area and received three Weak Accept decisions. The reviewers identified some concerns, including novelty with respect to existing work and specific details of the experimental setup and results that were not clear. The authors have addressed most of these concerns in their response, including adding a table that explicitly explains the contribution with respect to existing work and clarifying the missing details. Given the unanimous Weak Accept decision, the ACs also recommend Accept as a poster.
All reviewers agreed this was a very strong submission: it was clearly written, was theoretically and experimentally interesting, and had excellent motivation. A clear accept. Authors: you ve already indicated that you ve updated the submission to respond to reviewer changes, if you could double check their comments for any recommendation you may have missed on accident that would be great! The paper will make a great contribution to the conference!!
This paper got mixed reviews. One for acceptance, one for reject and two borderline. After the rebuttal, AR2 raises the review to borderline.  AR1 gives the highest recommendation but did not provide detailed supporting evidence. Other reviews provide comment on the strength and also share the concerns. Most of the concerns concentrate on the motivation (whether the proposed method is violating the objective of knowledge distillation) and the brought additional computation overhead. Also the scope of this paper was defined wider than the actual one. The authors only did experiments for BERT but did not consider and compare with existing KD method. Overall, AC read the paper and also has the similar concerns, the novelty is limited. the brought increase in inference time is violating the KD objective and the scope of this paper was not defined clearly. The authors should improve the submission in these aspects. At its current status, AC does not recommend acceptance. 
This work describes a series of strategies for optimizing the training speed of word embeddings (as in word2vec and fasttext).  All reviewers appreciate the convincing empirical results, which are without a doubt impressive.  Reviewers also mostly agree that speeding up embedding training is important, and there is no doubt that this type of paper is appropriate for ICLR (as clearly highlighted in the CfP.)  However, the specific optimization strategies deployed and described here are deemed not to bring novel insight, useful in itself to the community, beyond the software contribution described. The paper seems to mostly serve as documentation of the implementation, limiting its value and impact to further research. The pedagogic value is also limited, as the paper tackles multiple different, eclectic optimizations, a narrative strategy that does not leave room to describe a single one more generally, helping the community find other places to apply it. All in all this leads to a borderline negative assessment, and I cannot recommend acceptance.
All reviewers are positive or very positive about this work. The authors successfully addressed all questions. I believe this paper should be accepted.
AnonReviewers 2 and AnonReviewer 3 rated the paper highly, with AR3 even upgrading their score.  AnonReviewer1 was less generous:  " Overall, it is a good empirical study, raising a healthy set of questions. In this regard, the paper is worth accepting. However, I am still uncomfortable with the lack of answers and given that the revision does not include the additional discussion and experiments promised in the rebuttal, I will stay with my evaluation."  The authors have promised to produce the discussion and new experiments. Given the nature of both (1: the discussion is already outline in the response and 2: the experiments are straightforward to run), I m inclined to accept the paper because it represents a solid body of empirical work.
Main content:new training regime for multi resolution slimmable networks.   Discussion: reviewer 4: believes the main contribution of mutual learning from width and resolution is a bit weak reviewer 1: incremental work, details/baselines missing in experimental section reviewer 2: (least detailed): well written with good results Recommendation: I agree with reviewer 1, 4 that the experimental section could be improved. Leaning to reject.   
Reviewers recognize that the method proposed is somewhat novel but have strong reservations on the experimental evaluation. Discussion of some relevant papers is also missing (eg, Li et al, 2017 : ALICE). The authors have not responded to the many concerns expressed by the reviewers. 
This paper describes a method to incorporate multiple candidate templates to aid in response generation for an end to end dialog system. Reviewers thought the basic idea is novel and interesting. However, they also agree that the paper is far from complete, results are missing, further experiments are needed as justification, and the presentation of the paper is not very clear. Given the these feedback from the reviews, I suggest rejecting the paper.
This paper addresses an interesting problem and all reviewers agree.  Most reviewers found the paper difficult to understand and it was hard to see the novel contributions.    The paper will need a significant revision before publication.
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
This paper provides an RL environment defined over Coq, allowing for RL agents and other such systems to to be trained to propose tactics during the running of an ITP. I really like this general line of work, and the reviewers broadly speaking did as well. The one holdout is reviewer 3, who raises important concerns about the need for further evaluation. I understand and appreciate their points, and I think the authors should be careful to incorporate their feedback not only in final revisions to the paper, but in deciding what follow on work to focus on. Nonetheless, and with all due respect to reviewer 3, who provided a review of acceptable quality, I am unsure the substance of their review merits a score as low as they have given. Considering the support the other reviews offer for the paper, I recommend acceptance for what the majority of reviewers believes is a good first step towards one day proving substantial new theorems using ITP ML hybrids.
This paper proposes convergence results for zeroth order optimization.  One of the main complaints was that ZO has limited use in ML. I appreciate the authors  response that there are cases where gradients are not easily available, especially for black box attacks.  However, I find the limited applicability an issue for ICLR and I encourage the authors to find a conference that is more suited to that work.
This paper studies PCA under a generative model setup. The authors analyze the projected power method in a range of natural settings. Moreover, experimental evaluation and comparison to other methods is performed on MNIST. The paper studies an important problem. Despite some initial concerns, the reviewers overall agreed that this is an interesting contribution. I recommend acceptance.
There is insufficient support to recommend accepting this paper.  The authors provided detailed responses to the reviewer comments, but the reviewers did not raise their evaluation of the significance and novelty of the contributions as a result.  The feedback provided should help the authors improve their paper.
Reviewers largely agree that the paper proposes a novel and interesting idea for unsupervised learning through meta learning and the empirical evaluation does a convincing job in demonstrating its effectiveness. There were some concerns on clarity/readability of the paper which seem to have been addressed by the authors. I recommend acceptance. 
The paper argues for a GAN evaluation metric that needs sufficiently large number of generated samples to evaluate. Authors propose a metric based on existing set of divergences computed with neural net representations. R2 and R3 appreciate the motivation behind the proposed method and the discussion in the paper to that end. The proposed NND based metric has some limitations as pointed out by R2/R3 and also acknowledged by the authors   being biased towards GANs learned with the same NND metric; challenge in choosing the capacity of the metric neural network; being computationally expensive, etc. However, these points are discussed well in the paper, and R2 and R3 are in favor of accepting the paper (with R3 bumping their score up after the author response).  R1 s main concern is the lack of rigorous theoretical analysis of the proposed metric, which the AC agrees with, but is willing to overlook, given that it is nontrivial and most existing evaluation metrics in the literature also lack this.  Overall, this is a borderline paper but falling on the accept side according to the AC. 
This paper proposes to automatically determine when the SGD step size should be decreased, by running two "threads" of SGD for a bunch of iterations, divide those into windows, and then look at the average inner product of the gradients in the two threads in each window. If the inner product tends to be high, that indicates that there is still "signal" in the gradient and it should not be decreased. If it is low, that indicates that the gradient is mostly "noise". In the latter case, the learning rate is decreased by a factor of gamma and the length of the next phase is increased by gamma.  Theorem 3.1 essentially assumes smoothness, a bounded fourth moment for the stochastic gradient, and that the stochastic gradient error is not too far from isotropic. Then it shows that if the step size is set small enough, the standard deviation of the diagnostic (Q_i) can be upper bounded in terms of the expected value of Q_i. It follows that the probability of Q_i being negative cannot be too large (bounded in terms of the step size eta and the length of the windows l).  Theorem 3.2 adds the assumption of strong convexity and weakens the assumption on the gradient to a bounded second moment. Then it upper bounds the expected value of the diagnostic in terms of its standard deviation.  Proposition 3.4 gives a proof of convergence. As far as I can tell the proof is essentially that the learning rate decay can t be much worse than what would happen if the diagnostic *always* set to decrease. In particular: (1) It s impossible for the learning rate to decay too quickly, since the length of each phase is increased by gamma whenever the learning rate is decreased by gamma. (This is a "non adaptive result.) (2) The learning rate will eventually decay with probability 1.  Various concerns were brought up by the reviewers. Perhaps the most strongly voiced concern was that the proposed method is a heuristic rather than a method with a rigorous guarantee. For my part I am in agreement with the authors and other reviewers that heuristic methods for decreasing the learning rate are worthy of study given the large practical importance of this problem.  I concur with the concern raised by some reviewers that the theoretical component of the paper may not have little explanatory value for the results that are given. The assumption of strong convexity is not a major concern to me. (Though not true it can still give intuition.) More concerning is that theory essentially takes a fixed step size scheme (repeatedly decrease the step size by gamma and increasing the length of a phase by gamma) and then shows that the diagnostic can’t be too much worse. This isn’t in keeping with the motivation of being adaptive.  The reviewers were also concerned about the explanation of better results due to less overfitting. This may be true, but the theory makes no mention of overfitting.  There was a consensus that the experimental results were promising, though some minor issues were raised.  While the direction explored in the paper has value, there are enough open questions about the relationship of the theory to the experimental results to warrant another round of review.  Small thoughts, not significant to acceptance:  The current heuristic runs two separate threads and looks at the inner product of those gradients. An alternative to this would be to run a single thread along with a "ghost" thread that computes a different gradient at each iteration. It would be great to comment on the difference and why one might be superior to the other. A more radical alternative would be to run a single thread, but then compute the diagnostic on each half of the minibatch. A more radical alternative still would be to analytically do that splitting many times and average the results. This seems like it might simultaneously reduce the variance of the diagnostic and also reduce the computational cost.  2. The current heuristic runs two threads. Is there a tradeoff if you run more?  3. The statement of theorems could be more user friendly. To understand Thm 3.1, I needed to search o find the definitions of: eta, l, i, w, Q_i. With a small amount of effort this could be re written to remind the reader that w is the number of windows, l is the length, eta is the stepsize, etc. It is particularly unfortunate that sd() is never formally defined (only by reading the appendix did I discover that this was the standard deviation.)  4. The fact that the length of threads is always increased by a factor of gamma whenever the step size is reduced by gamma seems contrary to the spirit of the proposed diagnostic. After all, this "bakes in" a kind of "fastest possible" decay schedule. If the diagnostic were fully reliable, shouldn t this not be necessary? The decision to add this doe not get nearly enough discussion in the paper in my view.  5. I think it might be clearer to re state theorem 3.1 including the Chebyshev result after it.
This paper proposes to overcome some fundamental limitations of normalizing flows by introducing auxiliary continuous latent variables. While the problem this paper is trying to address is mathematically legitimate, there is no strong evidence that this is a relevant problem in practice. Moreover, the proposed solution is not entirely novel, converting the flow in a latent variable model. Overall, I believe this paper will be of minor relevance to the ICLR community.
Dear Authors,  The response you have provided, based on the main concerns of reviewers, have answered most of the questions raised.  As far as I understand from the added experiments you have provided, the proposed methodology shows resilience in being at least as good as state of the art approaches, while at the same time it is a mathematically interesting approach.   Your response has covered concerns like comparison to other communication techniques (the comparison list is not complete, but yet your effort is appreciated), adding discussion on the rank parameter, add comments on expressiveness and the connection with low rank parameterization, etc.   These efforts cannot be overlooked, and for that reason I suggest acceptance (poster).  Best  AC
This paper presents a deep RL algorithm to handle tasks where rewards can differ greatly in magnitude.  The proposed solution decomposes the reward into a set of exponentially sized bins with a thermometer encoding, and computes a weighted sum of the value functions learned for each bin.  The approach addresses the common tactic of reward clipping and value rescaling in deep RL algorithms.  The experiments demonstrate the potential utility of this approach on artificially constructed Atari games, and the experiments also show the approach remains competitive on six standard Atari games.    The reviewers found both strengths and weaknesses in the paper.  The overall approach was viewed as a clear and sensible (R1, R2, R4) approach to handling widely varying reward scales in a domain.  It may be a useful contribution in a manner similar to other methods that make deep RL algorithms more robust to scaling issues encountered in practice (R4).  The main concerns were whether this was solving a real problem or not (R2, R3), and the lack of a theoretical development for the multiple heuristics (R2,R3).    The author response then simplified the algorithm, which also served to clarify which aspects of the algorithm were relevant to the performance improvements. The response removed some of the heuristics (mixing in Monte Carlo returns) and changed other choices to be more principled ($1/\sigma^2$). The author response also described how the proposed algorithm addressed different scaling concerns from those handled by earlier methods. The author response also provided clarifications to many minor questions raised by the reviewers.  In the ensuing discussion, the reviewers were happy with the revised paper.  Though some minor theoretical reservations remained, the reviewers agreed this paper was a useful contribution.  The reviewers indicate to accept the paper as a useful contribution in deep RL to address certain reward scaling issues.  The paper is therefore accepted. 
The paper uses free algebras for sequential data representation, and two of the reviewers and the AC find this highly innovative. There were numerous small issues brought up by reviewers (and reviewers disagreed some on the presentation), in particular R3 asking about the experiments, some of which were addressed in the rebuttal.  Overall, because the idea was unusual, it s a bit hard to place and judge this paper. In the end, in the opinion of this AC, the ideas are very creative and there is enough of a chance that this paper could become a very highly cited work, hence we recommend its acceptance.
Overall, this paper has been on the very borderline. All reviewers agree that the motivation and the idea of the paper are reasonable (although somewhat incremental) and make an interesting extension of mixup type data augmentation. However, one expert reviewer raised some concerns which are unfortunately not fully resolved despite the intensive interaction.   The first one is the issue of diversity claimed in the paper. The authors explanation is mostly qualitative, and I, as an AC, also felt a logical jumping here, although I do understand that the proposed method somehow results in better generalization ability w.r.t. the number of generated samples as shown in Fig.3 in the appendix. This point is important because if the better generalization is coming from the label modification rather than the diversity in image space, then the novelty of the paper is limited. Another concern is that there are some inconsistencies in the scores of previous methods implemented by the authors and in the original papers. We understand that the exact score is not always easy to reproduce, but at least it is desirable to follow the original setting (such as the number of epochs) for each method as far as possible, because the accuracy should be the most important criterion as the goal of data augmentation is better generalization. Then, authors may separately discuss computational efficiency.   Based on the discussion with reviewers as above, I conclude that the paper should be further polished and completed before publication. Thus I recommend rejection for this time.
This paper studies the training of over parameterized two layer neural networks by considering high order Taylor approximation, and randomizing the network to remove the first order term in the network’s Taylor expansion. This enables the neural network training go beyond the recently so called neural tangent kernel (NTK) regime. The authors also established the optimization landscape, generalization error and expressive power results under the proposed analysis framework. They showed that when learning polynomials, the proposed randomized networks with quadratic Taylor approximation outperform standard NTK by a factor of the input dimension. This is a very nice work, and provides a new perspective on NTK and beyond. All reviewers are in support of accepting this paper. 
There are many discussions among the reviewers for this paper and eventually none of the reviewers (including the one who gave most positive score) would like to support the publication of this paper.   Some concerns from the reviewers are as follows: 1. Missing the discussion on storage cost.  2. The improvement is limited. $G_0$ must be small and independent of $n$, hence it is not clear if it is possible to give a fair comparison between the current complexity and previous best complexity.  3. Missing the discussion on the case when $n \leq \mathcal{O}(\varepsilon^{ 4})$ of the state the art results.  4. For the complexity results in terms of $\varepsilon$, it requires $\varepsilon$ to be arbitrarily small. The authors should also discuss this point for comparing with their result.  5. Some other statements in the papers are overclaimed.   Please take the comments and suggestions from the reviewers carefully to revise the paper for the future venues since they raised valid points.
This paper proposes a graph embedding method for the whole graph under both unsupervised and semi supervised setting. It can extract a fixed length graph level representation with good generalization capability. All reviewers provided unanimous rating of weak accept. The reviewers praise the paper is well written and is value to different fields dealing with graph learning. There are some discussions on the novelty of the approach, which was better clarified after the response from the authors. Overall this paper presents a new effort in the active topic of graph representation learning with potential large impact to multiple fields. Therefore, the ACs recommend it to be an oral paper.
This paper provides a theoretical background for the expressive power of graph convolutional networks. The results are obviously useful, and the discussion went in the positive way. All reviewers recommend accepting, and I am with them.
The proposed conditional variance regularizer looks interesting and the results show some promise. However, as the reviewers pointed out, the connection between the information theoretic argument provided and the final form of the regularizer is too tenuous in its current form. Since this argument is central to the paper, the authors are urged to either provide a more rigorous derivation or motivate the regularizer more directly and place more emphasis on its empirical evaluation.
This paper addresses the problem of learning with outliers, which many reviewers agree is an important direction. However, reviewers point to issues with the experiments (missing baselines, ablations, etc.) and are concerned that the assumptions in the theoretical analysis are too strong. These were potentially addressed in a revised version of the paper, but the revisions are so major that I do not think it is appropriate to consider them in the review process (and it is hard to assess to what extent they address the issues without asking reviewers to do a thorough re appraisal, which goes beyond the scope of their duties). I encourage the authors to take reviewer comments into account and prepare a more polished version of the manuscript for future submission.
The paper study under which condition a classifier can respect the condition of equalized odds. The reviewers find the paper interesting but they also raise some important concerns about it.  First, multiple reviewers pointed out that the results are not particularly novel or surprising and, even after discussing the rebuttal, they consider the result a bit incremental.  Second, the motivation of the paper are also questioned by multiple reviewers that suggested to study the tradeoff between trade off between EO fairness and accuracy.  Overall, the paper contains some interesting ideas but it is below the high acceptance bar of ICLR.
The paper characterizes the latent space of adversarial examples and introduces the concept of local intrinsic dimenstionality (LID). LID  can be used to detect adversaries as well build better attacks as it characterizes the space in which DNNs might be vulnerable. The experiments strongly support their claim.
The paper proposes a new deep architecture based on polar transformation for improving rotational invariance. The proposed method is interesting and the experimental results strong classification performance on small/medium scale datasets (e.g., rotated MNIST and its variants with added translations and clutters, ModelNet40, etc.). It will be more impressive and impactful if the proposed method can bring performance improvement on large scale, real datasets with potentially cluttered scenes (e.g., Imagenet, Pascal VOC, MS COCO, etc.).
Summary: this is a difficult paper to meta review, since it contains some insightful ideas and interesting experiments, while it also unfortunately contains omissions, confusions, and places where clarity is lacking (see below). One consistent theme is that the paper is too dismissive of prior work; the exposition is not as clear as it should be about what aspects of FORBES are present in previous papers, it uses too broad a brush to describe prior methods (resulting in too general statements about what these methods can t do), and it skips important chunks of the extensive literature on POMDP belief representation and tracking. As a result, the paper doesn’t do a good job concisely and accurately stating its contribution; there is still reasonable concern about how significant this contribution is. On the other hand, the experimental results for FORBES are interesting; the new method seems to represent a better combination of techniques than at least many existing works, at least to the resolution of the experiments’ statistical power. So the end question is whether interesting experimental results and a new combination of techniques are enough to outweigh the problems outlined above. In the end we believe that the correct outcome is rejection; but we have every expectation that a future version of the paper will resolve the difficulties outlined here and will appear in a future conference.  A brief note about the discussion: the original scores for this paper were lower. While some reviewers raised their score later in the discussion, a thorough reading of the discussion and the revised paper indicates that a substantial fraction of the issues leading to the lower scores still remain.  More details:  There is a lot of prior work on tracking belief states, which should be cited more thoroughly. The paper s intro makes it sound like diagonal Gaussians were the only previous alternative. At least, the intro should cite older work on MCMC methods like particle filters (e.g., Thrun’s book Probabilistic Robotics, or Arnaud Doucet’s work), and prior deep net papers that attempt non Gaussian representations, even if these don’t perform as well as hoped (see below for examples). It is also important to compare to RKHS representations of beliefs, such as Nishiyama, Boularias, Gretton, Fukumizu 2012; these handle multimodality, and can behave similarly to deep nets if they use the neural tangent kernel. Accurately comparing to prior work is one of the most important functions of a paper, so it doesn’t make sense to be unfairly critical of prior work or to skip it.  The paper is also unclear about the effects of Gaussian distributions at different places in a variational approximation. Because of this lack of clarity, the criticisms it levels at previous variational methods seem to be true only of some of them.   In particular, the introduction should distinguish between two uses of Gaussian approximations: first for the belief itself, and second for the distribution of observations given a belief. Some prior works make only one of these approximations. For example, a non Gaussian distribution used as a belief state can predict multi modal future behaviors, even if we approximate observations under a given belief as Gaussian.  The introduction should also distinguish between two common places that a Gaussian could enter into a variational approximation: at the input or at the output of a network. A Gaussian latent at the input of a variational network (even if it has diagonal covariance) can result in a highly non Gaussian output distribution, while Gaussian noise added at the end will (if it is the only noise) lead to a Gaussian output. Again, some of the statements in the intro apply only to the latter use of a Gaussian, while some prior work focuses on the former use.  There is an important conceptual confusion in the paper about what it means to have a multimodal belief state: the paper presents the true belief as an inherent property of an environment, while in fact it is a property of an environment *model*. So, there can be two different equally accurate models of the same environment which differ in the belief representation; a simple example would be to use either a continuous state whose components are joint angles, or a discrete state obtained by finely discretizing this continuous one. In the first case the belief would be a distribution over the continuous space, while in the second it would be a categorical distribution (a point in a simplex). A consequence of such a difference is that beliefs can be multimodal in one representation and not another.  The importance of this confusion is that, since we are asking our network to learn a belief state, the learning process could potentially favor representations that lead to unimodal beliefs — so it’s not clear theoretically that forcing a unimodal belief representation is necessarily a disadvantage. The paper presents the situation as if the disadvantage is forced by theory, while instead the argument should be based on experiments: e.g., one could try to show that unimodal representations, even if given a higher latent dimension to work with, aren’t empirically able to capture the same information.  Some interesting prior deep net POMDP papers that might need better discussion: * Han, Doya, and Tani ICLR 2020 (which isn’t cited here) puts the Gaussian latent variable as an input to the network for predicting beliefs (eq 2), resulting in a possibly highly non Gaussian output representing the belief. * Tschiatschek et al, 2018 (also not cited) uses a Gaussian *mixture* as the variational distribution to approximate beliefs, again allowing multimodality. * Igl et al. 2018 (which is cited only late in the current paper, and basically dismissed) uses a deep version of particle filters to allow non Gaussian distributions for both beliefs and observations. * More work that is potentially relevant but not adequately compared (even if briefly cited): Gregor et al. (2019), DreamerV2, Ha & Schmidhuber’s World Models. Each of these makes at least some choices to try to handle at least some kinds of multimodality, so a clear explanation of differences that avoids the confusions mentioned above would be very helpful. * In general, the results of the search “variational encoder POMDP” seem to include a number of papers not cited in the current paper; another useful search is “normalizing flow POMDP"  Finally, in the experiments section, the paper needs to correctly report the reliability of its conclusions. In some places (e.g., Fig. 5) there’s no mention of reliability or repeatability of conclusions; the paper just says that its evidence “support[s] the claim that FORBES can better capture the complex belief states”. In other places (e.g., Fig. 6, 7), the paper displays uncertainty representations based on only a few replications of an experiment (e.g., 3 seeds for Fig. 6, or 5 seeds when a reviewer requested extra experiments). The corresponding uncertainty estimates almost certainly are strongly biased too low (too certain); e.g., three runs would have less than a 50% chance of even seeing failure modes that happen with probability as high as 20% (0.8^3   0.512 > 0.5), meaning that the estimated standard deviation could be almost arbitrarily badly biased downward. To be clear, experiments with few replicates can still be highly useful and informative, and it’s true that some experiments are too expensive to run many times; but in such cases the paper should add appropriate caveats to its conclusions. For example, instead of reporting the sample standard deviation based on a normal model, the paper could report a confidence interval based on a more robust model or test, such as a Wilcoxon test. (To illustrate the difference, confidence intervals at typical significant levels like p 0.05 would be vacuous (infinitely wide) under Wilcoxon with 3 seeds, but much weaker p values would still yield non vacuous intervals.)  A few smaller questions:  The authors added a nice ablation study to compare to Dreamer; this is great to see. It would be good to discuss the connection to earlier methods such as PlaNet and Dreamer at places where the current method is similar or different (e.g., different from Dreamer in the belief state representation in sec 2.2, but similar in the RL framework in section 3.2). These comparisons would aid in the reader’s understanding of what is new in FORBES.  An unusual feature of FORBES is that the variational approximation to the belief at time t+1 is not a function of the belief at time t. Instead the belief inference network q_{\psi,\theta} takes as input the entire past trajectory, uses convolution and recurrence to reduce the variable length input to fixed dimension, and passes this fixed dimension representation through a normalizing flow mapping. It would be interesting to discuss the reason for this design decision. In particular, it seems like it would inhibit tracking — i.e., it could be hard to propagate information from one belief distribution to the immediate next one, particularly if there are a few unlikely observations scattered through a trajectory.  A minor point for clarity: in Fig 1 it s unclear what distributions the white and gray triangles refer to. They don t seem to correspond to a natural belief state: instead maybe they incorporate three simultaneous observations from the same starting belief? Correctly intersecting beliefs is an important issue though, so at a high level the point that the figure is trying to make fits well.  Another point for clarity: “there always exists a diffeomorphism that can turn one well behaved distribution into another”: this is true for some definition of ”well behaved”, but it’s misleading to say it this way. E.g., it is not true if the distributions in question can have atoms, or differ in dimension or topology; these exceptions are unfortunately important cases that do come up in practice.
The authors propose a "jumpy RNN" to adaptively change the step size of an RNN to match the time scales of the system dynamics. Reviewers found merit in the simple and intuitive idea, but were less enthusiastic about the experimental results and the comparison to existing work. (Adaptive step size methods have been a subject of recent work in RNNs, not to mention in numerical methods for ODE solvers.) Overall, I think the additions the authors made in the discussion phase did strengthen the paper, but further work is necessary before publication. 
The reviewers and AC note that the strength of the paper includes a) an interesting compression algorithm of neural networks with provable guarantees (under some assumptions), b) solid experimental comparison with the existing *matrix sparsification* algorithms. The AC s main concern of the experimental part of the paper is that it doesn t outperform or match the performance of the "vanilla" neural network compression algorithms such as Han et al 15. The AC decided to suggest acceptance for the paper but also strongly encourage the paper to clarify the algorithms in comparison don t include state of the art compression algorithms. 
This paper presents two extensions of Relation Networks (RNs) to represent a sentence as a set of relations between words: (1) dependency based constraints to control the influence of different relations within a sentence and (2) recurrent extension of RNs to propagate information through the tree structure of relations.  Pros: The notion of relation networks for sentence representation is potentially interesting.  Cons: The significance of the proposed methods compared to existing variants of TreeRNNs is not clear (R1). R1 requested empirical comparisons against TreeRNNs (since the proposed methods are also of tree shape), but the authors argued back that such experiments are necessary beyond BiLSTM baselines.  Verdict: Reject. The proposed methods build on relatively incremental ideas and the empirical results are rather inconclusive.
The paper attempts at providing a general benchmark for evaluating/analysis of long range transformer models, consisting of a 6 evaluation tasks. The main goal of the paper is to remove conflating factors such as pretraining from model performance and keeping the benchmark accessible. All reviewers agreed that these are important positive aspects of the paper and the presented analysis/results are useful.    While reviewers generally feel positive about the work, there are some critical concerns on how useful this benchmark is in practice, how generalizable are the results, and whether the benchmark is good at what is intended for. For example, the vanilla Transformer model performs very well on all the proposed tasks, making me question on what we can actually learn about long range dependencies through this benchmark. In addition, most tasks are synthetic and all models fail on 1 of the 6 proposed tasks.   Therefore, I think LRA should be viewed more as a tool for analysis, or as authors nicely put in their response, it should be viewed as a means to "encourage hypothesis driven research instead of hillclimbing or SOTA chasing.".  During discussion period with reviewers, while acknowledging the above mentioned issues, this strength was highlighted as a valuable contribution. Therefore, given the general positive sentiment about the work, I d recommend accept.
The paper proposes a data augmentation approach called SpanDrop to help to distill supervision signals from a long sequence prediction problem. The reviewers generally agree on two major drawbacks of the paper. First, the novelty of this approach. Second, the experiment results are not very convincing.  After reading the responses from the authors, I don’t think the authors convinced me of the novelty of the work, especially when comparing it to the word dropout. No matter if you treat the data or the model as a black box, it’s effectively doing the same thing. Apart from that, the model can only be used in the setting of “underspecification” long sequence tasks, which diminishes its value in real applications.  On the experiment side, there are three issues. One, many tasks considered are not long sequence tasks. Second, the improvement is marginal in many cases. Three, more related methods should get considered as baselines. Besides these three points raised by the reviewers, I also want to raise the point that it is not (and should not) be acceptable to report ALL your language experiment results on dev sets. I understand it is more time consuming to get the test results on tasks where the test has to be done online, e.g. SQuAD. However, it is not a good practice in reaching a conclusion merely on dev sets in general.  Based on the reviewers  comments and the reasons listed above, I recommend rejection of this paper.
The authors present a new theoretical framework that establishes that any network can be approximated by pruning a polynomially larger random binary networks, and also an algorithm for pruning binary nets. The results are important in the general context of the "strong" lottery ticket hypothesis, and are of both theoretical and practical interest. Although some of the ideas and technical contributions can be seen as a combination of prior tools and algorithms, the experimental findings are very novel.  Some further concerns of clarity and novelty were addressed by the authors.
This paper endeavors to combine genetic evolutionary algorithms with subsampling techniques. As noted by reviewers, this is an interesting topic and the paper is intriguing, but more work is required to make it convincing (fairer baselines, more detailed / clearer presentation, ablation studies to justify the claims made int he paper). Authors are encouraged to strengthen the paper by following reviewers  suggestions.
The main goal of this paper is to develop a new adaptive strategy to remove the need of hyperparameter fine tuning, which hinders the performance of DGS (Zhang et al., 2020) method. This paper applies a line search of the step size parameter of DGS  to reduce tuning.  A heuristic update rule of the smooth parameter in DGS (Zhang et al., 2020)) is also used.   Pros + The topic of learning hyperparameters on the fly is well motivated and an important direction to improve many existing methods. + A wide range of tasks are considered in the experiments are interesting, with a wide range of tasks considered.    Cons   Reviewers have found the contribution to be incremental and marginal. Specifically, the reviewers have expressed concerns on the impact of the work since it verifies improvement upon DGS only. The paper could be stronger by showing evidence of improving other algorithms.     In the work of [Zhang et. al., 2020], there are two parameters that require careful selection   the learning rate and smoothing radius. However, the proposed approach still relies on some hyperparameters. Although the authors claim that the method is not sensitive to these hyperparameters, it could be better justified.    The initial version lacks evaluation of the adaptive mechanism, although the authors added the comparison in the revised version.    The paper could be further improved by comparing against other hyperparameter optimization methods.   We acknowledge the detailed response and the modifications of the manuscript. We believe the paper will make a more profound contribution and impact after addressing some of the major concerns raised by the reviewers.  
The paper proposes an approach that generates pseudo labels along with confidence to help semi supervised learning. Then, selected pseudo labels are used to update the model. Moreover, the authors include a variation of mixup for data augmentation to train a more calibrated model. Experimental results justify the validity of the proposed approach.  Several reviewers believe that the paper is somewhat well written. The main concern is on the novelty of the work. In particular, many works have discussed selected treatment of unlabeled data, data augmentation for semi supervised learning, and label confidence estimation. Those works deserve more discussions/comparisons. The paper can also be improved with deeper experimental studies that better justify the main assumption and merits of the proposed approach.  
This is a very interesting paper on unsupervised skill learning based on the predictability of skill effects, with the incorporation of these ideas into model based RL.  This is a clear accept, based on the clarity of the ideas presented and the writing, as well as the thorough and convincing experiments.
Post author rebuttal the score of this paper increased. Discussions with reviewers were substantive and the AC recommends acceptance.
This paper proposes learning to make stylistic code edits (semantics remains similar) based on information from a few exemplars instead of one. The proposed method first parses the code into abstract syntax trees and then use the multi extent similarity ensemble. This was compared to a Graph2Edit baseline on C# fixer and pyfixer, which are datasets generated by rule based transcompilers. The proposed method got around 10% accuracy improvement due to a combination of the method and using more than one examplar.   The reviewers find that any improvement due to more examplars to be expected and suggested that 1) one carefully chosen examplar is enough, and 2) that the need for multiple examplars means more practical difficulties in providing them in an application 3) the targets are all generated by rule based methods and the benefits may not extend to a realistic case where the edits are not so clear and the reviewers wondered about the application value and the potential need for human evaluations.  The authors argued that it is unexpectedly difficult to expand the base method to multiple examplars and users should be able to provide examplars in an application. The authors further provided additional results that addressed some of the reviewer s concerns but the reviewers did not change their evaluation.  Rejection is recommended based on reviewer consensus.
The experimental work was seen as one of the main weaknesses.
Although the reviewers like the general idea of the paper, there are concerns regarding the clarity of the statements, especially in stating the main assumptions, referring to related work, and how well the experiments support the results of the paper. Although the authors  long response addressed some of the issues/comments raised by the reviewers, not all of them are convinced that the paper carries enough novelty and is ready for publication. I would suggest that the authors take the reviewers  comments into account, revise the paper, and make it ready to be submitted to an upcoming conference. 
The reviewers unanimously think the paper has lack of novelty, its contributions are quite limited, and is not ready for publication.
This paper studies the problem of learning from data that have been corrupted by label noise. The authors define a natural data dependent noise condition, that allows the noise rate to be large close to the decision boundary, and provide a simple iterative method that eventually converges to the Bayes optimal classifier. The method is evaluated on both synthetic a real datasets. There was a consensus among the reviewers that this is an interesting contribution and I propose acceptance.
This submission presents intriguingly good results on k shot learning and I agree with the authors that the results are better than the presented previous work, and that the method is simple, so I took a deeper look into the paper despite the overall negative reviews. However, I think in its current form, the paper is not suitable for publication:    The previous work, that the authors compare to, were not really using comparable architectures: in fact, likely much worse base models with fewer parameters etc. I think any future version of this work would need to control for architecture capacity, otherwise how can we be sure where the gains come from? To me, this is a major unknown in terms of the credit assignment for the great results.   The authors should be comparing with MAML (and follow up work) by Finn et al. (2017)   I don t really understand why the authors claim to have no need for validation sets. That s a very strong claim: are ALL the hyper parameters (model architectures etc) just chosen in another, principled way? This issue would definitely need to be addressed in a follow up work.
The authors analyze knowledge graph embedding models for multi relational link predictions. Three reviewers like the work and recommend acceptance. The paper further received several positive comments from the public. This is solid work and should be accepted.
This paper presents and evaluates a technique for unsupervised object part discovery in 3d   i.e. grouping points of a point cloud into coherent parts for an object that has not been seen before. The paper received 3 reviews from experts working in this area. R1 recommended Weak Accept, and identified some specific technical questions for the authors to address in the response (which the authors provided and R1 seemed satisfied). R2 recommends Weak Reject, and indicates an overall positive view of the paper but felt the experimental results were somewhat weak and posed several specific questions to the reviewers. The authors  response convincingly addressed these questions. R3 recommends Accept, but suggests some additional qualitative examples and ablation studies. The author response again addresses these. Overall, the reviews indicate that this is a good paper with some specific questions and concerns that can be addressed; the AC thus recommends a (Weak) Accept based on the reviews and author responses.
This submission addressed offline imitation learning problem with non optimal demonstrations. The AC went through the draft, reviews, and replies. The AC agrees with all reviewers that the mathematical analysis, empirical evaluation, and general quality of writing haven t reached the bar of ICLR papers.
This paper proposes a method for bilingual lexicon induction. The proposed method is efficient, it optimizes a reconstruction and transfer loss. Extensive experiments are reported, and the methods provides improvements over prior work. Overall, the paper brings together prior ideas in a useful way.
The reviewers agree that the authors have made an interesting contribution studying the effect of data augmentation, but they also agree that the claims made by the paper require a broader empirical study beyond the limited number of tasks surveyed in the current revision.  I urge the authors to follow this advice and see what they find.
This paper continues the investigation on fairness and privacy in the context of federated learning. We appreciate the detailed response from the authors. During the rebuttal period, the authors have largely updated the set of experiments, since there was an identified bug in the previous implementation. Another drawback that the AC identified is that there is a lack of formulation and formal guarantees in the paper. In particular, is the proposed algorithm trying to satisfy example level or client level data privacy? The resulting noise scale can be quite different. Unlike prior work (e.g. Jagielski et al), the proposed algorithm does not seem to provide any fairness guarantee. Thus, it is not clear why the proposed approach is justified (even under some assumptions). In a similar vein, perhaps the authors could consider a more in depth discussion that compares their approach with prior work and articulate what advantages does their new method offers. Overall, the paper is not ready for publication at ICLR.
The reviewers and authors participated in modest discussion, with the authors providing direct responses to reviewer comments. However, this did not appreciably change the overall ratings of the paper (one reviewer raised their rating, while another grew more concerned), and in aggregate the reviewers do not recommend that the paper meets the bar for acceptance.
This paper presents an approach that relies on DNNs and bags of features that are fed into them, towards object recognition.  The strength of the papers lie in the strong performance of these simple and interpretable models compared to more complex architectures.  The authors stress on the interpretability of the results that is indeed a strength of this paper.  There is plenty of discussion between the first reviewer and the authors regarding the novelty of the work as the former point out to several related papers;  however, the authors provide relatively convincing rebuttal of the concerns.  Overall, after the long discussion, there is enough consensus for this paper to be accepted to the conference.
This paper introduces a new objective for text generation with neural nets.  The main insight is that the standard likelihood objective assigns excessive probability to sequences containing repeated and frequent words.  The paper proposes an objective that penalizes these patterns.  This technique yields better text generation than alternative methods according to human evaluations.  The reviewers found the paper to be written clearly. They found the problem to be relevant and found the proposed solution method to be both novel and simple.  The experiments were carefully designed and the results were convincing.  The reviewers raised several concerns on particular details of the method.  These concerns were largely addressed by the authors in their response.  Overall, the reviewers did not find the weaknesses of the paper to be serious flaws.  This paper should be published. The paper provides a clearly presented solution for a relevant problem, along with careful experiments.   
This paper proposes a new sparsity inducing activation function, and demonstrates its benefits on continual learning and reinforcement learning tasks.  After the discussion period, all reviewers agree that this is a solid paper, and so do I. I am thus recommending it for acceptance as a poster. Hopefully, such visibility (combined with the open source release of the code) will encourage other researchers to try this new technique, and we will see more evidence confirming its usefulness in more varied settings and versus stronger baselines (that remain somewhat limited in the current work: this is the main weakness of the paper).
This submission analyses the VAE objective from the perspective of non linearly scaled isometric embeddings, with the aim of improving our information theoretic understanding of the variational objective.   Reviewers are in consensus that this submission in its current form is very difficult to read, even after revisions by the authors. The metareviewer, who is highly familiar with information theoretic and even information geometric interpretations of VAEs, similarly struggled to understand this paper. Many concepts (e.g. KLT transforms) are not introduced in a self contained manner, nor are related works like RaDOGAGA. Moreover the exposition introduces lots of notation (often somewhat implicitly) and requires more  high level plain English statements that signal the structure of the overall narrative to the reader. As a result, it is hard to understand the paper, even at the level of the contributions that are claimed by the authors. It appears that Section 3.4 should be read as culminating in a "correction" of the rate distortion view of VAEs proposed by Alemi et al. Unfortunately the metareviewer is not able to understand from the writing what fault the authors find with the proposed interpretation, and how their view informs a better perspective.  It is difficult to provide the authors with concrete addressable suggestions at this stage of revision of their manuscript. The metareviewer s advice would be to attempt to focus on defining a narrative structure that clearly explains what insights this perspective of VAEs contributes, what misconception it corrects, and how it corrects it –– and then focus on streamlining notation in a manner that makes it possible to follow along with the exposition more easily. 
The submission hypothesizes that in typical GAN training the discriminator is too strong, too fast, and thus suggests a modification by which they gradually increases the task difficulty of the discriminator. This is done by introducing (effectively) a new random variable   which has an effect on the label   and which prevents the discriminator from solving its task too quickly.   There was a healthy amount of back and forth between the authors and the reviewers which allowed for a number of important clarifications to be made (esp. with regards to proofs, comparison with baselines, etc). My judgment of this paper is that it provides a neat way to overcome a particular difficulty of training GANs, but that there is a lot of confusion about the similarities (of lack thereof) with various potentially simpler alternatives such as input dropout, adding noise to the input etc. I was sometimes confused by the author response as well (they at once suggest that the proposed method reduces overfitting of the discriminator but also state that "We believe our method does not even try to “regularize” the discriminator"). Because of all this, the significance of this work is unclear and thus I do not recommend acceptance.
The paper introduces a technique to improve density ratio estimation. This is an important problem and very relevant to the ICLR conference. The main idea is to consider density ratios with respect to intermediate distributions to “scale” the densities and make the ratios easier to estimate by training a suitable discriminative model (classifier). Reviewers found the idea interesting but there was a consensus the paper is not ready for publication.
This paper recieves extensive discussions among SAC, two ACs and PCs. The decision was not made lightly. We hope that you will find the comments below  from two ACs useful for future publication.    This paper is concerned with the feature attribution framework, which distributes the prediction made by a Graph Neural Network (GNN) to its input features, such as edges or nodes, and identifies an influential subgraph as the explanation. The currently prevailing feature removal strategy, which feeds only the considered subgraph into the target predictor and then measures the importance of the subgraph, will encounter the so called Out Of Distribution (OOD) problem the new subgraphs may not appear in the training dataset.  This paper proposes to use the causal inference framework to deal with this OOD problem. The proposal seems interesting: it considered the considered subgraph as the cause of the prediction and treats domain shift as a (hidden) common cause for both of them. It proposes to estimate a surrogate graph G_s^* to satisfy the front door criterion and then estimate the causal effect of the subgraph on the prediction.  While the proposed method seems novel and interesting and the reported empirical results seem encouraging, I have some basic concerns about the proposal. 1. The authors didn t justify why the feature attribution evaluation problem can/should be considered as a causal effect identification problem. In feature attribution evaluation, one is essentially interested in evaluating the prediction given the subgraph. The distribution shift variable, D, contains information that is helpful for this purpose.  Why should one go with the causal effect identification formulation, in which D is made independent from the subgraph and then integrated out? I think this justification is essential.  2. It is not justified why the constructed variable, G_s^*, satisfies the front door criterion. In Section 3.1, the authors claimed that "G_s^* should follow the data distribution and respect the inherent knowledge of graph properties, thus no link exists between D and G_s^*."  I failed to see why this implies that there is no link from D to G_s^* (or that D and G_s^* are conditionally independent given G_s) note that D is part of the data distribution. Without this justification, I am not sure whether the application of front door adjustment is sensible.  Overall, the paper contains interesting ideas and the results look encouraging. It would be highly appreciated if the authors managed to make this work convincing, by properly addressing the issues above.  I hope to see those components in an updated paper.    This work considers the task of debiasing GNN explainer subgraph importance scores by generating surrogate subgraphs to correct distribution shift problems. Through a causal model, the authors argue that a model prediction based on the explanatory subgraphs suffers from a distribution shift (e.g., induced subgraph degree distributions are different than those of the full graph). Hence, the associations between the important induced subgraphs and the model prediction may be spurious. In particular, the casting of induced subgraph explanations as a front door hidden variable should be definitively valuable to the community (but maybe a little too strong of a condition, seems unnecessary for the proposed goal).  Overall the reviewers believe the goal and observations are novel and valuable. Most of the reviewers  concerns were addressed in the rebuttal. As a pure data driven domain adaptation work, I think the work is good. However, even after discussing with the authors, I am still concerned with the soundness of the causal theory behind the work.  A Conditional Variational Graph Auto Encoder (CVGAE) are tasked to produce subgraphs that act as a front door adjustment variable. The argument is that this front door adjustment solves the challenges of the distribution shift. Front door adjustment is generally performed over observed variables. If performed with a model, the model is likely mechanistic. CVGAE are extremely flexible models for a front door adjustment. And while the generated subgraphs are constrained to reproduce a data driven distribution of induced subgraphs (and that those themselves have constraints), that in itself is not enough to guarantee these generated graphs give a proper front door adjustment. The work also describes "generate counterfactual edges" without a clear causal model for how these edges are generated or why they are counterfactual. The causal theory needs a lot of work.  The work is very promising and may become a cornerstone contribution to graph explanations. However, as it stands now, the causal theory needs to be more formal (the work offers no proofs of the various claims). I am excited to see the causal theory fully developed in the future.
This paper proposed a personalized federated learning algorithm which takes into account the similarity of gradient of different users to update the model. Although the ideas presented are intuitive, the algorithms have fundamental limitations, for example, they may cause large overhead of memory, communication and computation, and are unsuitable for privacy preserving machine learning. In addition, there are no rigorous analysis and the experiments are not convincing. This is a clear rejection.
This is a well done job which combines a few ideas to reach means to identify problematic cases and indicate this when classifying. It has raised doubts about the applicability, though I can see that a abstention rule can have multiple uses. While the work seems to be well done, it has not largely excited the committee members. It initially missed to be placed well wrt existing work to highlight the novelty, and the demonstration that the approach can be generally useful is not complete. Dealing with abstention rules always brings another facet to classification and comparisons are not trivial in many situations. Not surprisingly, this has landed as a borderline case, which I place on the inside (as I like the topic and I think it is interesting work) but it could become an outsider depending on the overall view of the selected papers for the conference and other constraints.
The authors propose an algorithm for enhancing noisy speech by also accounting for the phase information. This is done by adapting UNets to handle features defined in the complex space, and by adapting the loss function to improve an appropriate evaluation metric.  Strengths   Modifies existing techniques well to better suit the domain for which the algorithm is being proposed. Modifications like extending UNet to complex Unet to deal with phase, redefining the mask and loss are all interesting improvements.   Extensive results and analysis.  Weaknesses   The work is centered around speech enhancement, and hence has limited focus.   Even though the paper is limited to speech enhancement, the reviewers agreed that the contributions made by the paper are significant and can help improve related applications like ASR. The paper is well written with interesting results and analysis. Therefore, it is recommended that the paper be accepted. 
This paper addresses the identification of physical systems defined on graphs. The authors introduce the Adversarial Twin Neural Network (ATN), which consists in augmenting a simple linear model (PNN) with a virtual neural network (VNN). Some regularization terms are used to enforce maximum prediction from the PNN, and to enforce diverse outputs between PNN and VNN.   The paper initially received tree rejection recommendations. The main limitations pointed out by reviewers relate to the limited contributions, the limiting assumption of using a linear mode for PNN, the lack of positioning with respect to related works, and clarifications on experiments. The authors  rebuttal answered to some reviewers concerned: Rdem1 increased its grade from 3 to 5, and Rdem1 from 5 to 6   although not willing to champion the paper. R8dT9, which provided a very detailed review and feedback after rebuttal still voted for rejection, especially because he was not convinced by the positioning with recent related works and the answers on experiments.    The AC s own readings confirmed the issues essentially raised by R8dT9 and other reviewers. Especially, the AC considers that:    The contributions for driving a proper cooperation between the PNN and VNN models are weak, since it reduces to using simple skip connection and adversarial training.    The importance of these aspects have not been analysed in depth in the revised version of the paper, neither theoretically nor experimentally: for example, the difference with respect to [Yin+ 2021] for a proper augmentation, the discussion to alternative methods for representing diversity as done in [Rame & Cord 2021], or the positioning with respect to  Wasserstein distance based objectives.     There remains ambiguities in the cross validation process, which have not been addressed in the rebuttal.  Therefore, the AC recommends rejection.
the reviewers seem to agree that this submission could be much more strengthened if more investigation is done in two directions: (1) the effect of different, available resources (e.g., in the comment, the authors mentioned WikiData didn t improve, and this raises a question of what kind of properties of external resources are necessary to help) and (2) alternatives to incorporating external knowledge (e.g., as pointed out by one of the reviewers, this is certainly not the only way to do so, and external knowledge has been used by other approaches for RTE earlier. how does this specific way fare against those or other alternatives?) addressing these two points more carefully and thoroughly would make this paper much more appreciated.
This paper demonstrates the hypothesis that a very small word piece vocabulary (giving a "quasi character level" model) outperforms current methods of neural MT in truly low resource scenarios, and provides some auxiliary studies around word piece frequency and domain transfer. It considers LSTM, CNN, and Transformer NMT models. This is useful information for people working in low resource scenarios to know.  The paper got 3 reviews by people with very strong machine translation expertise. There was a general consensus that the paper was insufficiently aware of prior work on this topic and the paper had problems in experiment construction which raised issues about the comprehensiveness of the result. That is, while this paper adopts a more extremely small vocabulary, Sennrich and Zhang (2017) already showed that a much smaller subword vocabulary can give much stronger results for low resource MT (while Araabi and Monz questioned whether this was as true for Transformer NMT. Meanwhile Cherry et al. (2018) and Kreutzer and Sokolov (2018) argued already the benefits of (almost) character level NMT. On the experimental side, both not having results on genuinely low resource scenarios and the commented of Reviewer FBrF that the problem with larger subword vocals here may be mainly due to the small corpus size used for constructing the subword vocabulary are both quite important. Moreover, as mainly an MT experimental study, this paper seems better suited to a more specialized audience of MT researchers at an ACL, WMT, AMTA, etc. venue.  I recommend rejecting this paper as not sufficiently novel, with experiments that need further work, and lacking strong interest to a broader representation learning audience.
In this paper the authors proposes to use partial optimal transport to align point cloud in the presence of noise and partially observed data. To this end they express the partial Wasserstein Kantorovich Rubinstein duality and use it to adapt the classical WGAN loss to partial OT. The optimal alignment between point clouds is then done by minimizing their proposed loss where the dual potential are modeled as deep neural network hence approximating the partial Wasserstein while being solved using mini batches. Experiments show the interest of the method on a few well understood examples with ground truth from the Stanford repository.   The paper had originally borderline scores with some reviewers concerned about the theoretical contribution. The authors did a very good reply that that greatly appreciated by the reviewers, the one with the lowest score deciding to increase it. The new numerical experiments on the 3D human dataset were also appreciated. The consensus during the discussion was that the paper is worth accepting but that the authors should take into account the comments form the reviewers and better explain their contribution on the KR duality.  For these reason the AC recommends to accept the paper but urges the authors to take into account the comments from the reviewers. In particular the authors should better highlight their theoretical contribution and explain the link and differences with unbalanced minibtach OT.
This paper extends the Transformer, implementing higher dimensional attention generalizing the dot product attention. The AC agrees that Reviewer3 s comment that generalizing attention from 2nd  to 3rd order relations is an important upgrade, that the mathematical context is insightful, and that this could lead to the further potential development. The readability of the paper still remains as an issue, and it needs to be address in the final version of the paper.
The paper explores and discusses the effects of incorporating prior domain knowledge for modeling fluid dynamics with neural networks, with a focus on smoothed particle hydrodynamics. Reviewers agree that the contributions are modest, and that they are not well presented with respect to issues of efficiency, scalability, robustness, etc.  More work need to be done to make it useful to the community.  Many of directions to improve the paper were in reviewer comments.
This paper proposes an approach for probing an environment to quickly identify the dynamics. The problem is relevant to the ICLR community. The paper is well written, and provides a detailed empirical evaluation. The main weakness of the paper is the somewhat small originality over prior methods on online system identification. Despite this, the reviewer s agreed that the paper exceeds the bar for publication at ICLR. Hence, I recommend accept.  Beyond the related work mentioned by the reviewers, the approach is similar to work in meta learning. Meta RL and multi task learning has typically been considered in settings where the reward is changing (e.g. see [1],[2],[3],[4], where [4] also uses an embedding based approach). However, there is some more recent work on meta RL across varying dynamics, e.g. see [5],[6]. The authors are encouraged to make a conceptual connection between this approach and the line of work in model based meta RL (particularly [5] and [6]) in the final version of the paper.  [1] Duan et al. https://arxiv.org/abs/1611.02779 [2] Wang et al. CogSci  17 https://arxiv.org/abs/1611.05763 [3] Finn et al. ICML  17 https://arxiv.org/abs/1703.03400 [4] Hausman et al. ICLR  17: https://openreview.net/forum?id rk07ZXZRb [5] Sæmundsson et al. https://arxiv.org/abs/1803.07551 [6] Nagabandi et al. https://arxiv.org/abs/1803.11347 
The authors present a method that optimizes a differentiable neural computer with evolutionary search, and which can transfer abstract strategies to novel problems.  The reviewers all agreed that the approach is interesting, though were concerned about the magnitude of the contribution / novelty compared to existing work, clarity of contributions, impact of pretraining, and simplicity of examples.  While the reviewers felt that the authors resolved the many of their concerns in the rebuttal, there was remaining concern about the significance of the contribution.  Thus, I recommend this paper for rejection at this time.
The paper establishes an interesting relationship between poisoning and online learning. Instead of framing the poisoning problem as a bi level optimization problem as what is done conventionally, the paper proposes reducing the poisoning attack design to an online learning problem in which the adversary decides on a poisoning data point in a sequential manner.  The data point that leads to the largest difference of loss between the current model and the target model is selected as the next point to inject into the training data.  Pros + If the loss function is convex, the proposed algorithm is guaranteed to converge to the target, and the paper provides a lower bound on the number of samples needed to reach the target model from the current model.   + Experiments on SVMs show the advantageous performance of the proposed attack algorithm.    Cons   The reason why the proposed online learning method could outperform the KKT based attack is not well justified theoretically. Experimentally, the paper would be stronger if evaluations are done on more diverse settings and data.      Reviewers have expressed concerns on how practical the proposed algorithm is, since the guarantees are established on convex loss functions. The paper would be stronger if  the authors can further compare the attack with deep learning poisoning algorithms on larger datasets.   I truly believe that the paper’s exploration of convex models is an important step towards understanding the poisoning attacks and is a step towards understanding attacks for non convex models. However, I do think that the paper could make a more profound contribution and impact with stronger experimental evaluations.   Therefore, I would classify this paper as borderline, toward weak reject compared with other papers.  
This paper studies the problem of how data should be balanced among a set of tasks within meta learning. This problem is interesting, and largely hasn t been studied before. However, the reviewers raised several shortcomings of the current version of the paper, including the significance of the problem setting, the limited experimental study (i.e. the only experiment with real data is CIFAR FS), the depth of the related work section, and the clarity/impreciseness of the writing. Further, the paper has not been revised to address any of these shortcomings. As such, the paper is not ready for publication at ICLR.
This paper received 4 reviews with mixed initial ratings: 7, 5, 4, 5. The main concerns of R1, R2 and R4, who gave unfavorable scores, included: lack of methodological novelty (analysis only paper), absence of experiments on real data (3 synthetic only benchmarks), missing baselines and an overall inconclusive discussion. At the same time R5 notes that the offered fair comparison between SOTA methods was indeed "much needed", and the paper can "serve an important role" in guiding future developments in the community. In response to that, the authors submitted a new revision and provided detailed answers to each of the reviews separately. R1, R2 and R4 did not participate in the discussion, and R5 stayed with the positive rating. AC agrees with R5 that the provided analysis is insightful, and the effort put into organizing the research community around a single set of benchmarks and metrics is indeed valuable. However, given a simplistic nature of the proposed datasets and lack of other methodological contributions, the submission is not meeting the acceptance bar for ICLR. After discussion with PCs, the final recommendation is to reject.
This paper presents a new graph neural network layer that is sensitive to topological structure in the graph. Reviewers all believe the work is technically sound, and the experiments (particularly after author revisions) show clear benefits in cases where topological structure is important. The main questions are about whether the experimental evaluation is sufficient. While there are always more experiments that could be run, I tend to agree with the authors that the chosen experiments support the key claims in the paper, so it seems ok. The other question about the experiments is if they sufficiently convince the reader that topological structure is useful in practice. This seems more mixed. The paper would certainly be improved if there was a motivating application where there was a clear win. For example, molecular structures are used as motivation in the intro, but the best performing method on proteins doesn’t use the topological layer. All in all, though, there does appear to be clear improvements on carefully constructed cases, and there appear to be some benefits in real world datasets.
The authors present a system for end to end multi lingual and multi speaker speech recognition. The presented method is based on multiple prior works that propose end to end models for multi lingual ASR and multi speaker ASR; the work combines these techniques and shows that a single system can do both with minimal changes.   The main critique from the reviewers is that the paper lacks novelty. It builds heavily on existing work, and  does not make any enough contributions to be accepted at ICLR. Furthermore, training and evaluations are all on simulated test sets that are not very realistic. So it is unclear how well the techniques would generalize to real use cases. For these reasons, the recommendation is to reject the paper.
The paper proposes to use the representation learned via CPC to do reward shaping via clustering the embedding and providing a reward based on the distance from the goal.  The reviewers point out some conceptual issues with the paper, the key one being that the method is contingent on a random policy being able to reach the goal, which is not true for difficult environments that the paper claims to be motivated by. One reviewer noted limited experiment runs and lack of comparisons with other reward shaping methods.  I recommend rejection, but hope the authors find the feedback helpful and submit a future version elsewhere.
This paper proposes a new method to combine non autoregressive (NAT) and autoregressive (AT) NMT. Compared with the original iterative refinement for non autoregressive NMT, their method first generates a translation candidate using AT and then fill in the gap using NAT.  All of the reviewers think the idea is interesting and this research topic is not well studied. However, the empirical part did not convince all the reviewers. The revised version and response is good; however, it still does not solve some major concerns of reviewers. 
The paper proposes a model for large scale image retrieval. Unlike previous work that rely on local features, the proposed method aggregates local features into the so called Super features to improve their discriminability and expressiveness. To do so, the method proposes an iterative attention module (Local Feature Integracion Transformer, LIT), that outputs an ordered set of such features. By exploiting the fact that features are ordered, the paper proposes a contrastive loss on Super features that match across images. The paper presents a thorough empirical evaluation on several publicly available datasets including relevant baselines.  Overall the paper is well written and the empirical results are strong (including detailed ablations that motivate the design of the method). All reviewers and the AC appreciate the idea of applying the contrastive training at local feature level while only requiring image level labels.  Reviewer hp4Y points out that the proposed LIT is not particularly novel, but previous work are properly cited. Also this is not a major issue given that the motivation is very clear, it is well executed and the empirical results are strong.   Reviewer uoYN had initial concerns regarding inconsistencies in the mathematical formulation of the method, which were resolved in a detail (and constructive) discussion with the authors.  All reviewers recommend accepting the paper, three of which consider the contribution to be strong. The AC agrees with this assessment and recommends accepting the paper.
Main content:  Blind review #1 summarizes it well:  This paper introduces a variant of the Information Bottleneck (IB) framework, which consists in permuting the conditional probabilities of y given x and y given \hat{x} in a Kullback Liebler divergence involved in the IB optimization criterion.  Interestingly, this change only results in changing an arithmetic mean into a geometric mean in the algorithmic resolution.  Good properties of the exponential families (existence of non trivial minimal sufficient statistics) are preserved, and an analysis of the new critical points/information plane induced is carried out.     Discussion:  The reviews generally agree on the elegant mathematical result, but are critical of the fact that the paper lacks any empirical component whatsoever.     Recommendation and justification:  The paper would be good for ICLR if it had any decent empirical component at all; it is a shame that none was presented as this does not seem very difficult.
The paper describes a method which, given a music waveform, generates another recording of the same music which should sound as if it was performed by different instruments. The model is an auto encoder with a WaveNet like domain specific decoder and a shared encoder, trained with an adversarial "domain confusion loss". Even though the method is constructed mostly from existing components, the reviewers found the results interesting and convincing, and recommended the paper for acceptance.
This work studies a number of feature representations for few shot classification, including representations learned from MAML, supervised classification, and some self supervised tasks. The main conclusion of the study is that learning from more complex tasks result in better representations for few shot classification. As a practical solution, then, the authors suggest using representations learned from multiple tasks for few shot classification.  The paper studies an important problem in machine learning, and reviewers all appreciate that. However, they raised concerns about the draft in its current state. Authors replied to these comments, and while reviewers acknowledged and appreciated the responses and the revision of the draft, unfortunately that did not convince the reviewers. Several major concerns remained unresolved at the end. Specifically, EEwV believes that the paper is a case study, which though useful, does not bring deep new insights or findings. 63j7 believes that even after revisions made to the paper, there are additional experiments required to understand and examine the claims. Tk8a finds the submission unready for publication due to weak experimental analysis and suggests running additional experiments to examine the hypotheses made by the authors (e.g, relating to spurious features, the need of input harmonization, the benefit of voting) and better tie in the findings of this work to related work. Tk8a provides a list of concrete suggestion along these lines. Similar to EEwV, 28ox also thinks that the paper lack novelty or does not really bring new insights on the way to train the backbone.  Based on these comments, and the ratings, I encourage authors to address these issues and resubmit.
The paper eventually got 5 "marginally above the threshold" after rebuttal. Such scores testify to that the paper is a borderline one. By reading the post rebuttal comments, it is evident that most of the reviewers still deemed that the novelty is incremental. One of the reviewer (vUb9) raised the score simply to "encourage the authors to think more important problems", rather than acknowledging the merits of the paper. The AC also read through the paper and had the following opinions: 1. The paper is actually about DNN compression, based on the "new finding" that the weights across layers are low rank. However, the authors would not write the paper in the way of DNN compression, but put more emphasis on the "new finding", which has no theoretical support at all (only some heuristic reasoning). The AC would deem that the "new finding" is only an assumption. 2. Actually the "new finding" is not new at all. For example,  [*] Zhong et al., ADA Tucker: Compressing Deep Neural Networks via Adaptive Dimension Adjustment Tucker Decomposition, Neural Networks, 2019,   used a shared core tensor (which could be regarded as the common dictionary) across all layers for higher compression rates. More recent references that use tensors and consider shared information across layers for compression can be easily found as well.  So the AC thanked the authors for preparing the rebuttals carefully, but regretfully the paper is not good enough for ICLR.
This paper tackles the challenge of incentivising selfish agents towards a collaborative goal. In doing so, the authors propose several new modules.   The reviewers commented on experiments being extremely thorough. One reviewer commented on a lack of ablation study of the 3 contributions, which was promptly provided by the authors. The proposed method is also supported by theoretical derivations. The contributions appear to be quite novel, significantly improving performance of the studied SMGs.  One reviewer mentioned the clarity being compromised by too much material being in the appendix, which has been addressed by the authors moving some main pieces of content to the main text.   Two reviewer commented on the relevance being lower because of the problem not being widely studied in RL. I would disagree with the reviewers on this aspect, it is great to have new problem brought to light and have fresh and novel results, rather than having yet another paper work on Atari. I also think that the authors in their rebuttal made the practical relevance of their problem setting sufficiently clear with several practical examples. 
Overall the reviewers appear to like the ideas in this paper, though this is some disagreement about novelty (I agree with the reviewer who believes that the top level search can very easily be interpreted as an MDP, making this very similar to SMDPs). The reviewers generally felt that the experimental results need to more closely compare with some existing techniques, even if they re not exactly for the same setting.
This paper presents a upper bound on the curvature of a deep network. After the discussion, the author has addressed some concerns of reviwers, but the results are not very strong, there is some limitation on the applications. There is no strong support for this paper. Due to the high standard of ICLR, the acceptance of the paper need strong results in terms of theory or experiments.
This paper considers an important problem, Multi Agent Reinforcement Learning, and looks at a subclass of problem, the Markov Potential Game.  Even though this class is not the more generic one (as pointed out by a reviewer), one must start somewhere before (and maybe the results cannot easily or at all be extended to a larger class), so I will not personally take this as a strong negative concern.  The other reviewers are rather positive about the result and the techniques, and I concur with them.   I will therefore recommend acceptance.
This work provides two contributions: 1) Brain Score, that quantifies how a given network s responses compare to responses from natural systems; 2) CORnet S, an architecture trained to optimize Brain Score, that performs well on Imagenet. As noted by all reviewers, this work is interesting and shows a promising approach to quantifying how brain like an architecture is, with the limitations inherent to the fact that there is a lot about natural visual processing that we don t fully understand. However, the work here starts from the premise that being more similar to current metrics of brain processes is by itself a good thing   without a better understanding of what features of brain processing are responsible for good performance and which are mere by products, this premise is not one that would appeal to most of ICLR audience. In fact, the best performing architectures on imagenet are not the best scoring for Brain Score. Overall, this work is quite intriguing and well presented, but as pointed out by some reviewers, requires a "leap of faith" in matching signatures of brain processes that most of the ICLR audience is unlikely to be willing to take.
The paper proposes an approach to define an "interpretable representation", in particular for the case of patient condition monitoring. Reviewers point to several concerns, including even the definition of explainability and limited significance. The authors tried to address the concerns but reviewers think the paper is not ready for acceptance. I concur with them in rejecting it.
The theory and results presented in this paper provide a new method to avoid collapse in contrastive learning.  All but one reviewer recommend acceptance.  The lone negative reviewer is concerned with the limited experiments, but the other reviewers, and the AC, find the experimentation convincing enough to warrant acceptance.
The paper proposes a new method for unsupervised text style transfer by assuming there exist some pseudo parallal sentences pairs in the data. The method thus first mines and constructs a synthetic parallel corpus with certain similarity metrics, and then trains the model via imitation learning. Reviewers have found the method is sound and the empiricial results are decent. The assumption on pseudo parallal pairs would limited the application of the methods in other settings where the source/target text distributions are very different. The authors have added discussion on this limitation during rebuttal.
This paper presents  a new approach to model uncertainty in DNNs, based on deterministic weights and simple stochastic non linearities, where the stochasticity is encoded via a GP prior with a triangular kernel inspired by ReLu. The empirical results are promising. The comments were properly addressed. Overall, a good paper.
This paper tackles a problem at the intersection of AutoML and trustworthiness that has not been studied much before, and provides a first solution, leaving much space for a lot of interesting future research. All reviewers agree that this is a strong paper and clearly recommend acceptance. I recommend acceptance as an oral since the paper opens the door for a lot of interesting follow ups.
This paper is attempting to improve the OOD generalization performance of neural networks on relational reasoning tasks. This is an important failure point of general neural network architectures and important research topic. The results of the paper shows impressive improvements on a set of subject.  * The paper is improved during the rebuttal, however, I do agree with the R5 and the paper is still lacking a lot in terms o clarity. The writing of this paper still requires some work.   * As R2 also has written, the proposed idea is not so concrete to apply as practical solutions, and the presentation of the paper still requires some more work.  * R3 pointed out some inaccuracies and it seems like authors have added some ablations in the direction that R3 has suggested.  I am suggesting to reject this paper given that the majority of the reviewers are also leaning towards rejection as well. I would recommend the authors to improve the clarity of the paper, do more ablations for their models and resubmit to a different conference.
The paper focuses on hybrid pipelines that contain black boxes and neural networks, making it difficult to train the neural components due to non differentiability. As a solution, this paper proposes to replace black box functions with neural modules that approximate them during training, so that end to end training can be used, but at test time use the original black box modules. The authors propose a number of variations: offline, online, and hybrid of the two, to train the intermediate auxiliary networks. The proposed model is shown to be effective on a number of synthetic datasets.  The reviewers and AC note the following potential weaknesses: (1) the reviewers found some of the experiment details to be scattered, (2) It was unclear what happens if there is a mismatch between the auxiliary network and the black box function it is approximating, especially if the function is one, like sorting, that is difficult for neural models to approximate, and (3) the text lacked description of real world tasks for which such a hybrid pipeline would be useful.  The authors provide comments and a revision to address these concerns. They added a section that described the experiment setup to aid reproducibility, and incorporated more details in the results and related work, as suggested by the reviewers. Although these changes go a long way, some of the concerns, especially regarding the mismatch between neural and black box function, still remain.  Overall, the reviewers agreed that the issues had been addressed to a sufficient degree, and the paper should be accepted.
This paper proposes to use Anderson Acceleration on min max problems, provides some theoretical convergence rates and presents numerical results on toy bilinear problems and GANs.  After the discussion, the reviewers agreed that this paper makes a nice contribution to ICLR. Some concerns were originally expressed in terms of incrementality of the theoretical results with respect to previous work (KCYs, gBHU), but the authors have well clarified their contributions in the discussion, and have updated their manuscript accordingly. There were also initial concerns about the related work coverage, but this was also properly addressed in the rebuttal, with additional experimental comparisons as well as extended related work section, as well as an additional convergence result for convex nonconcave problems.
Reviewers agree that the paper is well done and addresses an interesting problem, but uses fairly standard ML techniques. The authors have responded to rebuttals with careful revisions, and improved results. 
This paper proposes fine grained layer attention to evaluate the contribution of individual encoder layers. This departs from the standard transformer architecture where the decoder uses only the final encoder layer. This paper investigates how encoder layer fusion works, where the decoder layers have access to information for various encoder layers. The main finding of the paper is that the encoder embedding layer is particularly important. They propose SurfaceFusion, which only connects the encoder embedding layer to the softmax layer of decoders, leading to accuracy gains.  There was some disagreement among reviewers about this paper. Overall, I found this a simple but effective contribution with interesting findings that can help future research in seq2seq models. Some of the weaknesses (discussing other relevant works, discussing other variants of FGLA, adding new experimental results) have been addressed in the updated version of the paper. One of the reviewers suggested running additional experiments on GLUE style tasks (with a masked language model) to be really sure if the technique is convincing, and particularly try it with larger models (T5 was suggested). While adding those experiments would be a plus, I disagree that this is crucial   this paper is focusing on seq2seq tasks and is already considering several tasks: summarization, MT, and grammar correction. The results found by this paper are interesting and can foster future research extending this beyond these 3 tasks. Even if larger models can make the improvements smaller, there are many inconveniences in just increasing scale (memory consumption, energy consumption, etc.) It is my opinion that the community should value research that tries to understand the weaknesses of smaller models, rather than relying on large scale models to solve all problems.
Nice ideas with practical advantages.  
This paper proposes applying AlphaGo Zero style ideas for solving combinatorial optimization problems over graphs with two changes:  Replacing CNNs with graph neural networks Normalization of rewards  The reviewers raised valid points about this paper. 1. Lack of technical novelty 2. Experimental results are not strong enough to draw meaningful conclusions 3. Since techniques are mostly off the shelf, extracting general knowledge, insights and conclusions from empirical evaluation is important, but unfortunately was missing.  I agree with the review comments. Overall, my assessment is that the paper requires more work  before it is ready for publication.
This paper presents a locally connected spiking neural network model trained to do classification of MNIST using spike timing dependent plasticity (STDP) and reward modulated STDP. The authors show that this model can learn to classify MNIST images (though not at a very high accuracy) and that it can engage in classical conditioning. The reviews were initially all in the reject range. The common theme in the reviews was concerns about the weak and limited nature of the results. After a good amount of author response and reviewer replies to the authors, one reviewer increased their score to a borderline accept, but the other reviewers did not change their scores, producing scores of 3,3, and 6. Given these scores, and the reviewers  remaining concerns, a reject decision was reached.
The paper describes an interesting approach to predicting continuous closed surface segmentations from discretized image data using a wavelet output representation. This is an interesting idea with a lot of potential. Unfortunately, the paper currently suffers from major weaknesses which we encourage the authors to address.  1. While the idea of generating a continuous output representation of a segmentation is technically interesting, what are some applications where this is actually useful? 2. The ground truth annotations in the datasets evaluated are implicitly quite variable. The annotations are not made to sub pixel precision and there is likely to be large multi pixel variability across different annotations of the same image. This makes a poor problem to demonstrate the need and potential of a sub pixel accurate segmentation algorithm.  I encourage the authors to find applications and datasets where reliable sub pixel ground truth annotations exist, and to demonstrate that their approach to generating sub pixel segmentations is superior to appropriate baselines which also predict sub pixel segmentations.
# Summary The paper was initially well received by reviewers, remarking the new gradient estimator, a new dropbits technique and an interesting observations of better performance when the bitwidth is learned. The experimental results also look promising: showing improved training performance and test performance (including on ImageNet with ResNet 18), properties to reduce quantization error of learned weights, possibility to learn number of bits via learning stochastic bit dropping masks.  A deeper verification of the specific methods proposed however showed principal issues:   The methods proposed in the paper are not sufficiently justified by verifiable formal arguments. The proposed intuitive explanations are entangled and actually lead to wrong conclusions. In particular a main claim of the paper that the proposed estimator reduces bias and variance of Gumbel Softmax estimator was shown wrong and was removed in the revision. The remaining claim that the estimator reduces quantization error is also wrong (see below). With these issues, the gradient part of the paper is largely incorrect, which is in a strong discrepancy with good experimental results.    Other parts of the paper, comprising the remaining technical contributions are not properly positioned with respect to the SOTA and thus are not necessary novel / improving.  The main technical issues were discussed with all reviewers and were either supported or not objected. Therefore, I am confident that the submission has critical problems and must be rejected. I recommend the authors to thoroughly investigate all the raised issues (by all reviewers) before resubmitting to other venues.   # Details  ## Gradient  The overclaim of reducing bias and variance / resolving bias/variance tradeoff has been removed in the revision, but the new gradient estimator remains a central innovation proposed. It is however not justified and cannot indeed be regarded as a good estimator:  * The justification argues about the bias of the Gumbel Softmax sampling distribution, but the proposed estimator does not use a sampling distribution in the forward pass, and thus by design cannot address this problem.  * The backward pass to use gradient in i_max only (Eq. 3) is not based on any justification at all.   * The remaining claimed good property: "to reduce the quantization error" is, according to the definition in sect. 3.4, not a property of a gradient estimator, but of the stochastic relaxation alone. There is an experimental evidence Fig.2 that the estimator _leads_ to lowering the quantization error. This is however in a contradiction with a direct verification of the proposed estimator that was conducted: The verification inspects gradient in a single variable $x$ and a linear loss function of the quantized variable $\hat x$.  It shows that the gradient is zero at grid points and discontinuously reverses the direction at half grid points. Because of such zigzagging, *it does not correspond to minimizing the loss function*, i.e. not a reasonable estimator. The grid points, where the gradient vanishes, may correspond to either local minima or to local maxima of the estimator. Which of the two cases occurs depends exclusively on the sign of the incoming gradient from the loss function. For $L(\hat x)   \hat x$ we observe that the negative gradient points towards nearest grid point, but for $L(\hat x)    \hat x$ it points away from the nearest grid point, i.e. a step would *increase the quantization error*. The implementation of this verification is attached anonymously: https://colab.research.google.com/drive/1PibzRMXQ NVZMUdfgTIK0Q5FxUKyxfqI?usp sharing  * Alternative existing estimators are not sufficiently discussed: e.g. common deterministic STE, as used in quantization papers: to just treat the quantization operation as identity on the backward pass. Estimator used by Shayer et al. (2018),  Ajanthan et al. 2019 “Mirror descent view for neural network quantization”, Unbiased estimators (e.g. Yin et al. 2019 “ARSM: Augment REINFORCE Swap Merge Estimator for Gradient Backpropagation Through Categorical Variables”). While unbiased estimators may still have too high variance and or be too computationally demanding for deep networks, they can be used for verification purposes.   * The claim that it is not possible to apply unbiased estimators, in particular score function estimator, because of dependency on x is incorrect. See e.g. Schulman et al. 2015 “Gradient Estimation Using Stochastic Computation Graphs”. Many works on advanced unbiased estimators also demonstrate experiments with 2 or more layers of hidden discrete stochastic variables. From this and technical discussion with authors, it is seen that the experimental study is Sec 3.4 is very limited and erroneous.   * The rule by which the probability mass of the dropped bits is uniformly spread over the remaining bits is not justified and appears methodologically incorrect. In Fig.4 it is not clear what bits were dropped and why the mass at $ 2\alpha$ has decreased.  ## Gradient and Other Techniques relative to SOTA  * The bias problem of GS estimator, detailed in Fig.1. is not novel to me, it is in fact known that the mean under the concrete distribution (of linear or non linear objective) differs from the mean under the categorical distribution, see e.g.  Lorberbom et al. (2018) Direct Optimization through argmax for Discrete Variational Auto Encoder (Fig.1)  Andriyash et al. (2018) Improved Gradient Based Optimization Over Discrete Distributions  Thus analysis of individual samples in Fig.1 appears unnecessary detailed. The issue that the relaxed distribution of Gumbel Softmax may cause a large estimation error for gradients downstream is already discussed by Louizos et al. (2018) and other works, e.g.   Choi 2017, "Unsupervised Learning of Task Specific Tree Structures with Tree LSTMs" Sec 3.2  and Andriyash (2018). This later problem was previously addressed in many cases by the ST Gumbel Softmax heuristic. This heuristic indeed performs better in CIFAR 10 experiments in the submission / Louizos (2018), which is likely to be a better tuned and more controlled experiment than ImageNet. * More methods should be discussed that reduce the quantization error during learning. E.g.  Cong et al. (2018): “Extremely low bit neural network: Squeeze the last bit out with ADMM”,   who include terms explicitly minimizing the  quantization error. In fact most works quantizing network weights primarily focus on reducing the quantization error, e.g.  Nagel et al. (2019)" Data Free Quantization Through Weight Equalization and Bias Correction  * The prior works on learning bit width should be more extensively discussed / compared to, especially if this part becomes central to the submission. E.g.  Baalen et al. (2020) “Bayesian Bits: Unifying Quantization and Pruning” (or references therein if this is considered contemporaneous).    Courbariaux & David (2015): Training deep neural networks with low precision multiplications  * The new hypothesis for quantization is in fact similar to the effect observed elsewhere that quantizing neural networks progressively leads to better results.  E.g.   Zhou et al. (2017) Incremental Network Quantization  Towards Lossless CNNs with Low Precision Weights.  It is questionable whether the link to the lottery ticket hypothesis is justified, since the latter shows quite the opposite, as reviewers have pointed.
The authors theoretically analyze learning of two layer neural networks by gradient descent with respect to a data distribution that exposes how useful features are learned during training.  Overall, the reviewers felt that the analysis yielded useful insight, and was original.  During the discussion period, a reviewer recommended that the authors look at papers providing lower bounds on statistical query learning of two layer networks, and consider comparing the lower bound technique of this paper with that earlier work.
The paper proposes a novel model free solution to POMDPs, which proposes a unified graphical model for hidden state inference and max entropy RL. The method is principled and provides good empirical results on a set of experiments that relatively comprehensive. I would have liked to see more POMDP tasks instead of Atari, but the results are good. Overall this is good work.
This work tackles video generation using implicit representations, and demonstrates that using these representations enables improvements to long term coherence of the generated videos.  Reviewers praised the writing, the thorough experimental evaluation, and the strong quantitative results. Some concerns were raised about a lack of discussion of relevant related work, novelty/significance, model architecture, and a lack of qualitative examples, many of which the authors have tried to address during the discussion phase. Several reviewers raised their ratings as a result.  Personally I certainly believe that exploring implicit representations for video is important, and I know of no published prior work in this direction, which amplifies the potential significance of this work. Even if results are qualitatively worse than previous work in some ways, this exploration is still valuable and worth publishing.  While the paper ultimately received one reject rating, another reviewer chose to champion this work and award it the highest possible rating. Combined with the other positive reviews, this provides plenty of convincing evidence for me to recommend acceptance. That said, given the rating spread, I would like to encourage the authors to consider the reviewers  comments further as they prepare the final version of the manuscript. Especially providing more qualitative results would be a welcome addition.
This paper proposes a model agnostic FL method called FedKT that performs only one communication round and reduces the communication complexity of federated learning. The reviewers have the following concerns about the paper: * Limited novelty because the proposed method is directly based on PATE * Insufficient experiments  The authors did a great job of responding to the reviewers  comments and also added some new experimental results in the updated version. But the reviewers still recommend significant revision of the paper and resubmission to a future venue. I hope the authors will find their constructive and detailed comments below helpful!
This paper introduces a method that aims to solve the problem of  posterior collapse  in variational autoencoders (VAEs). The problem of posterior collapse is well documented in the VAE literature, and various solutions have been proposed. Existing proposed solutions, however, aim to solve the problem by either changing the objective function (e.g. beta VAE) or by changing the prior and/or approximate posterior models. The proposed method, in contrast, aims to solve the problem by bringing the VAE optimization procedure closer to the EM optimization procedure. Every iteration in optimization consists of SGD updates to the inference model (E step), performed until the approximate posterior converges. This is followed by a single SGD update of the generative model. The multi update E step makes sure that the M step optimizes something closer to the marginal log likelihood, compared to what we would normaly do in VAEs (joint optimization of both inference model and generative model).  The experiments are relatively small scale, but convincing.  The reviewers agree that the method is clearly described, and that the proposed technique is well supported by the experiments. We think that this work will probably be of high interest to the ICLR community.
The paper proposed a general framework to construct unsupervised models for representation learning of discrete structures. The reviewers feel that the approach is taken directly from graph kernels, and the novelty is not high enough. 
The reviewers have provided extensive comments, we encourage the authors to take them into account seriously in further iterations of this work.
The paper proposes a method to learn the stride of downsampling in deep networks using a  gradient based learning approach. The main idea is to work in the frequency domain and to learn the cropping mask in that domain. The authors also introduce a regularization for applications seeking computationally and memory efficiency. The authors investigate the interest of the approach on a number of datasets with audio and image data.   The reviewers praised the paper, appreciating the elegance of the approach and the effort made to thoroughly evaluate it. The reviewers also appreciated the clarity of the exposition and the care in the reporting of the results. The reviewers also expressed some concerns about several choices of the design (stride sharing) and the lack of detail in some experiments (computational /memory efficiency). Finally, the reviewers wished the paper had more theoretical grounding.  The authors submitted detailed responses to the reviewers  comments. After reading the responses, updating the reviews, and discussion, the reviewers found that the responses were ‘reinforcing [their] initial assessments  and their several concerns were satisfactorily addressed. Moreover several of the reviewer’s suggestions clearly already led to an improved manuscript with very thorough experimental evaluation and simpler approaches for stride sharing.   The paper proposed an elegant, learning based, approach to one of the most important design choices in deep network architecture design: the strides in the convolutions. The authors provided a careful and thorough experimental evaluation, and moreover improved it during the review process following the reviewer’s feedback.   Accept, definitely. 
Loosely, while IRM aims to find a feature mapping Phi s.t. response Y given Phi(X) is independent of the environment variables E, they suggest that when E is strongly correlated with Y, then it is possible for Phi obtained via IRM to involve environment variables. They motivate this by suggesting that if there exists a feature mapping Phi(X)   E, it would satisfy the IRM aim, but that this is undesirable.  They suggest instead requiring Phi(X)|Y being invariant to the environment.  The reviewers bring up a couple of concerns. The first is that it is not clear outside some simple examples when Y given Phi(X) being independent of E does not suffice. The second is that the authors also do not empirically validate their fix outside a single simple dataset. Moreover, what are the pitfalls of having Phi(X) given Y being independent of E?  Overall, this is an interesting kernel of an idea; it just needs to be fleshed out a bit more. 
This paper received divergent reviews (7, 3, 9). The main contributions of the paper   that multi agent competition serves as a natural curriculum, opponent sampling strategies, and the characterization of emergent complex strategies   are certainly of broad interest (although the first is essentially the same observation as AlphaZero, but the different environment makes this of broader interest).  In the discussion between R2 and the authors, I am sympathetic to (a subset of) both viewpoints.  To be fair to the authors, discovery (in this case, characterization of emergent behavior) can be often difficult to quantify. R2 s initial review was unnecessary harsh and combative. The points presented by R2 as evidence of poor evaluation have clear answers by the authors. It would have been better to provide suggestions for what the authors could try, rather than raise philosophical objections that the authors cannot experimentally rebut.  On the other hand, I am disappointed that the authors were asked a reasonable, specific, quantifiable request by R2   "By the end of Section 5.2, you allude to transfer learning phenomena. It would be nice to study these transfer effects in your results with a quantitative methodology.”   and they chose to respond with informal and qualitative assessments. It doesn t matter if the results are obvious visually, why not provide quantitative evaluation when it is specifically asked?  Overall, we recommend this paper for acceptance, and ask the authors to incorporate feedback from R2. 
The reviewers and AC liked the basic idea of how this paper improves on ALISTA, and the initial scores were high. Because the contributions rely quite a lot on empirical demonstrations, the reviewers asked for more experiments, changes to experiments, and timing results. The revision and rebuttal addressed most of these requests.  The multipath channel estimation problem was interesting though outside the scope of the AC and reviewer s expertise, so it is hard to evaluate how helpful the method is in that particular setting.
The article studies the set of functions expressed by a  network with bounded parameters in the limit of large width, relating the required norm to the norm of a transform of the target function, and extending previous work that addressed the univariate case. The article contains a number of observations and consequences. The reviewers were quite positive about this article. 
The paper provides an interesting take on GAN training based on Coulomb dynamics. The proposed formulation is theoretically well motivated and authors provide guarantees for convergence. Reviewers agree that the theoretical analysis is interesting but are not completely impressed by the results. The method addresses mode collapse issue but still lacks in sample quality. Nevertheless, reviewers agree that this is a good step towards the understanding of GAN training. 
Overall, the reviewers agree that there is definite value in the empirical evaluation you have provided. However, as you have acknowledged in your responses to the reviewers, the presentation could be significantly improved. A final point that was not touched upon by the reviewers where possible (e.g. certainly not ImageNet, but for some of the smaller datasets in Table 1) it would be helpful to have a comparison to fully Bayesian methods (you have linear regression and GPs, but I don t see the implementation details; my suggestion is to implement these within an MCMC framework, specifying reasonable priors over the (hyper)parameters).
The paper considers sample generation in high dimensional bayesian inference and proposes a multi scale procedure that performs coarse to fine multi stage training and enables interpretability of intermediate activations at coarse scales.  The method is simple, elegant and addresses a very important bottleneck of high dimensional bayesian inference. The clarity of the paper has been greatly improved based on the reviewers suggestions.   However some concerns remain regarding the evaluation that are needed to clearly demonstrate the value of the approach. In particular, it would be important to assess the impact of the number of levels, and how quickly the dimensions grow from one level to the next. The number of forward simulations does not provide a sufficient picture of the computational cost of the approaches. It would be also important to provide wall clock time.  Figure 2a should also provide the best of 3  independent experiments,  or better, more experiments should be run, and curves with shaded areas should be provided so one could visualize variability w.r.t runs. 
This paper proposes a novel approach for network pruning in both training and inference. This paper received a consensus of acceptance. Compared with previous work that focus and model compression on training, this paper saves memory and accelerates both training and inference. It is activation, rather than weight that dominates the training memory. Reviewer1 posed a valid concern about the efficient implementation on GPUs, and authors agreed that practical speedup on GPU is difficult. It ll be great if the authors can give practical insights on how to achieve real speedup in the final draft. 
This paper studies imitation learning from a causal inference perspective. The authors propose a method to remove the effects of confounders on expert action a using instrumental variable regression, which presumably leads to better estimation of P(a|s), and hence better imitation.  The reviews were negative overall at the start. After the discussions, one reviewer stated that he would change his recommendation to accept, although his score is not changed on the review form. However, another reviewer is still not convinced that the causal formalism introduced in the paper improves over the existing RL literature.
Meta score: 4  The paper presents a manually constructed cloze style fill in the missing word dataset, with baseline language modelling experiments that aim to show that  this dataset is difficult for machines relative to human performance.  The dataset is interesting but the fact that the experiments are confined to baseline language models Pros:    interesting dataset    clear and well written    attempt to move the field forward in an important area Cons:    limited experimentation    language modelling approaches not appropriate baseline  
This paper proposes a generalized way to generate sequences from undirected sequence models.  Overall, I believe a framework like this could definitely be a valuable contribution, but as Reviewer 1 and Reviewer 3 noted, the paper is a bit lacking both in theoretical analysis and strong empirical results. I don t think that this is a bad paper at all, but it feels like the paper needs a little bit of an extra push to tighten up the argumentation and/or results before warranting publication at a premier venue such as ICLR. I d suggest the authors continue to improve the paper and aim to re submit at revised version at a future conference. 
This paper proposes a new method for generating synthetic environments and reward networks for reinforcement learning tasks. This happens as a nested process: policies are learned in an inner loop, and environments are evolved in an outer loop. The environment representation is quite simple: the parameters of an MDP. Similarly, the reward networks are simply neural networks. Results show that the the learned environments and reward networks are reasonably good at decreasing policy training time by RL. The proposed method appears to be simple and quite general, and it would be interesting to see how it scales up to more complex environment representations.  The discussion around the paper centered on understanding various details of the method, and on the quality of the results. The reviewers generally agree that the paper is easy to read, and vary in their assessment of the significance of the results. It was pointed out that the generated environments are not necessarily similar to the base tasks, but it was nowhere claimed in the paper that they were. (In fact, it could be argued that the dissimilarity makes the method more interesting, given the good results of policy training.)  I m happy to recommend the paper for poster acceptance. If the results would have been more impressive, it could have been accepted for a more prominent presentation form; however, I believe that the method can yield better results in the future with more sophisticated environment representations.
All the reviewers find the problem studied in the interesting. However all of them raise concerns about the assumptions made in the paper for the analysis. Reviewers find the assumptions very limiting and far from the practical training of GNNs. Improving the analysis by relaxing the assumptions further can significantly help the paper. 
The paper addresses the question of skill discovery in reinforcement learning: can we (without supervision) discover behaviors so that later (when supervision is available via a reward signal) we can learn faster? The paper proposes a new contrastive loss that an agent can optimize for this purpose, based on a decomposition of mutual information between skills and transitions. The reviewers praised the extensive experimental evaluation and good empirical results, as well as the analysis of failure modes of related algorithms.  Unfortunately, there appeared to be errors in the derivation and implementation. (These include typos in derivations that made them difficult to follow, as well as uploaded code that didn t match the experimental results.) While the authors claim to have fixed all of them, the reviewers were not all completely convinced by the end of the discussion period. In any case, these errors caused confusion during review; so, whether the errors are fixed or not, it seems clear that there hasn t been time for a full evaluation of the corrected derivations and code. For this reason, it seems wise to ask that this paper be reviewed again from scratch before being published.
This paper proposes a multiscale variant of Graph Convolutional Networks (GCN) , obtained by combining separate GCN modules using powers of normalized adjacency as generators. The model is tested on several node classification semi supervised tasks obtaining excellent numerical performance.  Reviewers acknowledged the good empirical performance of the model, but all raised the issue of limited novelty, relative to the growing body of literature on graph neural networks. In particular, they missed an analysis that compares random walks powers to other multiscale approaches and justifies its performance in the context of semi supervised learning. Overall, the AC believes this is a good paper, but it can be significantly stronger with an extra iteration that addresses these limitations. 
This paper introduces a new dataset for evaluating disentanglement and its impact on out of distribution generalization based on the trifinger robotics platform. Using this dataset, the authors rigorously investigate the performance of beta VAEs in this setting under a number of conditions, finding that weak supervision is necessary to induce disentangled representations, and that, perhaps surprisingly, disentanglement does not help for sim2real settings despite the similarity between the simulator and the real data. Reviewers were divided on the work, but had a number of concerns related to the claims of novel architecture, comparisons to baselines, and issues with the clarity of the paper, some of which were addressed in the authors  response. I agree with some of these concerns, particularly with respect to the claims of novel architectures since the modifications could simply be viewed as tweaking hyperparameters and are not rigorously compared to baselines. However, I think the novelty of the dataset and the rigorous evaluation of OOD generalization settings is likely to be valuable enough to the community to merit acceptance. I d encourage the authors, however, to tone down some of the claims regarding the architecture (or provide sufficient baseline comparisons), and instead focus on the dataset and the OOD results. I recommend acceptance. 
This paper proposes a broad framework for unifying various pruning approaches and performs detailed analyses to make recommendations about the settings in which various approaches may be most useful. Reviewers were generally excited by the framework and analyses, but had some concerns regarding scale and the paper s focus on structured pruning. The authors included new experiments however, which mostly addressed reviewer concerns. Overall, I think is a strong paper which will likely be provide needed grounding for pruning frameworks and recommend acceptance. 
The authors argue in favor of task aware continued pretraining and demonstrate through experiments that using objectives based on the end task during continued pretraining help in improving downstream performance.   The reviewers generally appreciated the motivation, the formal treatment of the topic and the thoroughness in the experiments. There were some concerns about (i) positioning of the paper (pretraining as opposed to continued pre training) (ii) thorough comparison with other MTL frameworks (iii) evaluating on more datasets (iv) cost of continued pretraining for each task v) the benefit of META TARTAN over MT TARTAN only in specific  settings and (vi) lack of surprise/novelty in the results.   IMO, the authors have adequately addressed ALL the above concerns raised by the reviewers. Further, despite the above concerns, all reviewers agree that the problem is well motivated and of interest to the community and most aspects of this work are thorough. The findings will be useful and may spawn other work in this area.
The authors address the problem of CTR prediction by using a Transformer based encoder to capture interactions between features. They suggest simple modifications to the basic Multiple Head Self Attention (MSHA) mechanism and show that they get the best performance on two publicly available datasets.   While the reviewers agreed that this work is of practical importance, they had a few objections which I have summarised below:  1) Lack of novelty: The reviewers felt that the adoption of MSHA for the CTR task was straightforward. The suggested modifications in the form of Bilinear similarity and max pooling were viewed as incremental contributions.  2) Lack of comparison with existing work: The reviewers suggested some additional baselines (Deep and Cross) which need to be added (the authors have responded that they will do so later). 3) Need to strengthen experiments: The reviewers appreciated the ablation studies done by the authors but requested for more studies to convincingly demonstrate the effect of some components. One reviewer also pointed that the authors should control form model complexity to ensure an apples to apples comparison (I agree that many papers in the past have not done this but going froward I have a hunch that many reviewers will start asking for this) .   IMO, the above comments are important and the authors should try to address them in subsequent submissions.  Based on the reviewer comments and lack of any response from the authors, I recommend that the paper in it current form cannot be accepted. 
This paper presents a novel model for generating images and natural language descriptions simultaneously. The aim is to distangle representations learned for image generation by connecting them to the paired text. The reviews praise the problem setup and the mathematical formulation. However they point out significant issues with the clarity of the presentation in particular the diagrams, citations, and optimization procedure in general. They also point out issues with the experimental setup in terms of datasets used and lack of natural images for the tasks in question.  Reviews are impressively thorough and should be of use for a future submission. 
Pros:   The authors propose a new algorithm to train GAN based on Cramer distance arguing that this eases optimization compared to Wasserstein GAN.    Reviewers agree that the paper reads well and provides a good overview of the properties of divergence measures used for GAN training.  Cons:   It is not clear how much the central arguments about scale sensitivity, sum invariance, and unbiased sample gradients of the distances hold true in practice and generalize.   The reviewers do not agree the benefits of the new algorithm is clear from the experiments shown. Given the pros/cons ,the committee feels the paper falls short of acceptance in its current form.
 The paper proposes a neural network architecture to address the problem of estimating a sparse precision matrix from data, which can be used for inferring conditional independence if the random variables are gaussian. The authors propose an Alternating Minimisation procedure for solving the l1 regularized maximum likelihood which can be unrolled and parameterized. This method is shown to converge faster at inference time than other methods and it is also far more effective in terms of training time compared to an existing data driven method.  Reviewers had good initial impressions of this paper, pointing out the significance of the idea and the soundness of the setup. After a productive rebuttal phase the authors significantly improved the readibility and successfully clarified the remaining concerns of the reviewers. This AC thus recommends acceptance. 
The submission presents an approach to accelerating convolutional networks. The framework is related to depthwise separable convolutions. The reviews are split. R3 expresses concerns about the experimental evaluation and results. The AC agrees with these concerns. The AC also notes that the submission is 10 pages long. Taking all factors into account, the AC recommends against accepting the paper.
This paper implements Group convolutions on inputs defined over hexagonal lattices instead of square lattices, using the roto translation group. The internal symmetries of the hexagonal grid allow for a larger discrete rotation group than when using square pixels, leading to improved performance on CIFAR and aerial datasets.  The paper is well written and the reviewers were positive about its results. That said, the AC wonders what is the main contribution of this work relative to existing related works (such as Group Equivarant CNNS, Cohen & Welling 16, or steerable CNNs, Cohen & Welling 17). While it is true that extending GCNNs to hexagonal lattices is a non trivial implementation task, the contribution lacks significance in the mathematical/learning fronts, which are perhaps the ones ICLR audience will care more about. Besides, the numerical results, while improved versus their square lattice counterparts, are not a major improvement over the state of the art.  In summary, the AC believes this is a borderline paper. The unanimous favorable reviews tilt the decision towards acceptance. 
In this paper, the authors studied algorithmic stability of batch reinforcement learning algorithms, as well as its connection to certain generalization bounds (motivated by the prior work Hardt et al developed for SGD on nonconvex optimization problems). While understanding the stability and generalization of batch RL is certainly an interesting and important direction, the paper in its current form is not yet ready to be published. As the reviewers pointed out, both the analyses and the claims need to be polished (in fact, important details and definitions are missing); and the theoretical contributions are only made in a limited setting.
The paper proposes a new, simple method for sparsifying deep neural networks.                                                       It use as temporary, pruned model to improve pruning masks via SGD, and eventually                                                  applying the SGD steps to the dense model.                                                                                          The paper is well written and shows SOTA results compared to prior work.                                                                                                                                                                                                The authors unanimously recommend to accept this work, based on simplicity of                                                       the proposed method and experimental results.                                                                                                                                                                                                                           I recommend to accept this paper, it seems to make a simple, yet effective                                                          contribution to compressing large scale models.                                                                                                                  
In the discussion, all reviewers acknowledge the novelty of this paper, such as learning from a wide range of AL heuristics, and the ability to transfer the to tasks with arbitrary number of classes. They also think that the additional experiments provided by the authors improve the paper s empirical validity.   However, a major issue raised by the reviewers is that the novelty (especially when compared with Liu et al) may not be enough for ICLR this time. One research direction (implicitly suggested by Reviewer 2) was to learn active learning strategies that go beyond selecting top k scoring examples to explicitly account for batch diversity. We encourage the authors to address this in the next version.
The reviewers are in agreement, that the paper is a big hard to follow and incorrect in places, including some claims not supported by experiments. 
This paper addresses the real world problem of semi supervised learning where the distribution from which the labeled examples are drawn is different from the distribution from which the unlabeled examples are drawn.  The task is motivated by structure activity prediction for drug design (quantitative structure activity prediction, or QSAR).  Examples represent molecules, and we wish to predict a real valued measure of binding affinity.  Exactly the general problem of data skew arose with exactly this task for example in one of the KDD Cup 2001 tasks.  While the authors here mention that labeled data may be focused more on active molecules (those with a high continuous valued response), in the KDD Cup 200`1 data the reverse was true, and the unlabeled test data were skewed to higher activity level.  I say all this to agree with the authors about the real world nature of the problem they address.  Also, some reviewers felt more empirical evaluation was needed, so that may be an additional data set for the authors to consider using.  Reviewer concerns including that the approach was simplistic, the empirical results were insufficient, and the claims were oversold.  The author replies and revisions, and the discussion, moved the reviews to be more favorable but still not strong enough to justify acceptance yet.  Nevertheless, the consensus is that the paper addresses an important problem and the revisions are headed in the right direction to make a strong future paper, and that the authors should be encouraged to continue this work.
The paper proposes a regularization term on the generator s gradient that increases sensitivity of the generator to the input noise variable in conditional and unconditional Generative Adversarial networks, and results in multimodal predictions. All reviewers agree that this is a simple and useful addition to current GANs. Experiments that demonstrate the trade off between diversity and generation quality would be important to include, as well as the experiment on using the proposed method on unconditional GANs, which was conducted during the discussion period. 
This paper presents some innovations to transformers allowing some significant reductions in parameter count. While some reviewers were concerned that the proposed innovations seem incremental and may not stand the test of time, all reviewers recommended acceptance after engaging in a rich and interactive author discussion. Given the clear importance of making transformers more efficient I think this paper will be of interest to the community and is worthy of acceptance at ICLR. 
The paper is in general well written and easy to follow, and the considered approach of controlling beta is sensible. However, all reviewers identify shortcomings in the empirical analysis of the proposed method (missing comparison with stronger baselines, convergence issues of the considered baselines, considered datasets, etc.). Furthermore, compared to the ControlVAE the contribution of the paper seems limited; and the empirical evaluation is insufficient to claim superior results in general. The authors did not address most of the concerns raised by the reviewers in their rebuttal. The authors can improve their paper substantially by performing the experimental results proposed by the reviewers and clarifying differences to the ControlVAE—but in its current form the paper does not meet the standard of ICLR. 
This paper studies the challenging problem of object centric generation of visual scenes. While the paper has some novel ideas that make it interesting, its (quantitative and qualitative) comparison with existing methods is currently premature to allow drawing conclusions with sufficient evidence.  Instead of claiming that existing models cannot do well for the more realistic datasets mentioned by reviewer dAqW, it would be more convincing to conduct a comprehensive experimental study by comparing the proposed method with existing methods on a range of datasets, from simple ones to more realistic ones. The synthetic Fishbowl dataset introduced in this paper can be one of them.  Moreover, the clarity of the paper could be improved to make it appeal better to the readers.  All three reviewers engaged actively in discussions (both including and not including the authors). Although one reviewer recommends 6 (weak accept), the reviewer also shares some of the concerns of the other reviewers. As it stands, the paper is not ready for acceptance. If the comments and suggestions are incorporated to revise the paper, it will have potential to be a good paper for future submission.
The authors develop an approach to improve upon methods for training certifiably robust models. They propose an input dependent margin based weighting and an automatically generated curriculum schedule and demonstrate improvements on training certifiably robust models on MNIST and CIFAR 10.  Reviewers agree that the paper makes interesting and novel contributions. However, the lack of novelty in the approach combined with the limited empirical gains make it difficult to justify acceptance. In particular, reviewers raise valid concerns on the quality of experiments comparing to prior work (in particular Crown IBP (Zhang et al 2020) and COLT (Balunovic & Vechev 2020)) (in particular hyperparameter tuning, inability to recreate baseline results and unjustified claims that the prior art cannot run on GPU hardware). Further, even the gains demonstrated are marginal.   Hence, I recommend rejection, but encourage the authors to revise the paper based on the feedback received.
The paper investigates the effect of convolutional information bottlenecks to generalization. The paper concludes that the width and height of the bottleneck can greatly influence generalization, whereas the number of channels has smaller effect. The paper also shows evidence against a common belief that CAEs with sufficiently large bottleneck will learn an identity map.   During the rebuttal period, there was a long discussion mainly about the sufficiency of the experimental setup and the trustworthiness of the claims made in the paper. A paper that empirically investigates an exiting method or belief should include extensive experiments of high quality in to enable general conclusions. I’m thus recommending rejection, but encourage the authors to improve the experiments and resubmitting.
The paper focuses on individual fair ranking and proposes an approach for that based on optimal transport. The reviewers are in general positive about the paper, however, there are a a couple of concerns that I believe should be addressed before publication.  First, I find the treatment of the term "counterfactual" misleading in the paper.  Counterfactual fairness has been proposed in the literature as a causal notion of individual fairness. However, as far as I can see in the paper, there is not such a causal treatment of counterfactuals in the paper. Thus, I suggest the authors to reconsider their treatment of counterfactuals in the paper, as it may trigger confusion. Second, I also agree with R1 that  it is unfair as SenSTIR is the only algorithm to use the same kind of "counterfactual" data than the one used for the evaluation.   
The reviewers identified missing comparisons to existing baselines (Deep RL and other tree based RL methods) as well as simplistic experiments as the main limitations of the paper. While the authors could address some of the issues raised by the reviewers, the missing comparisons and too simple experiments remain. I therefore agree with (most of) the reviewers that the paper can not be published at its current state.
The paper proposes a new perspective on the generalization performance of interpolating classifiers based on the entire joint distribution of their inputs and outputs. It conjectures that, when conditioned on certain subgroups, the output distribution matches the distribution of true labels. The conjecture is investigated empirically on a number of datasets and models, and proved to hold for a simple nearest neighbor model.  This paper generated varying responses from the reviewers and a detailed discussion. One main concern focused on whether the feature calibration conjecture is actually surprising, given standard expectations about generalization from learning theory. Indeed, from the discussion and the paper itself, it seems the authors conceived of classical generalization as a statement about whether train performance $\approx$ test performance, whereas one reviewer remarked that "what it really talks about is concentration of measure." I agree with the importance of this distinction in general, though it is perhaps less relevant in the current setting of modern interpolating classifiers, for which so little about generalization is understood in the first place. In particular, the empirical observations of varied forms of good generalization behavior for overparameterized models are likely to be interesting to the community, regardless of whether this behavior might be expected in the large sample limit.  As such, this is a very borderline paper, with many good arguments both for and against acceptance. After a detailed discussion among the chairs, it was decided that the current version is just shy of the acceptance threshold, but I would strongly encourage the authors to address the main reviewer concerns and resubmit a revised manuscript to a future venue.
All reviewers have agreed that the topic of evaluating compositional skills of agents is an important one and cast it as compositional learning as meta reinforcement learning is an interesting approach. At the same time, reviewers have raised concerns with respect to the benchmark itself, the exposition and clarify of the ideas as well as the experimental evidence used to support some of the claims. The authors have not provided an author response but have acknowledged the reviewers feedback.   As this paper stands I cannot recommend acceptance for the current manuscript.
The paper proposes to incorporate an autoencoder to transformer based summarization models in order to compress the model while preserving the quality of summarization. The strengths of the paper, as identified by reviewers, are in extensive experiments presented in the paper and in a relatively clear write up. However, the reviewers identify several weaknesses, including missing state of the art summarization baselines and missing relevant compression/knowledge distillation baselines. Although the author response have addressed some of reviewers  concerns, all the reviewers agree that the draft is not yet ready for publication.
Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. The most significant concern raised is that there does not seem to be an adequate research contribution. Moreover, unsubstantiated claims of novelty do not adequately discuss or compare to past work.
The reviewers were generally positive about this paper with a few caveats:  PROS: 1. Important and challenging topic to analyze and any progress on unsupervised learning is interesting. 2. the paper is clear, although more formalization would help sometimes 3. The paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know. 4. A large set of experiments  CONS: 1. Some concerns about whether the claims are sufficiently justified in the experiments 2. The paper is very long and quite dense 
Unfortunately, it falls short of ICLR standards   from evaluation, novelty and clarity perspectives. The method is also not discussed in all details. 
This paper proposes a novel ensemble method that enforces specification of the base models to improve accuracy. Base models are specialized on sub regions of the latent space. To calculate the ensemble prediction for a given example base models are weighted based on how close the example embeddings are to a learned “anchor” embedding. To derive the correlation between samples and anchors,  transformer like attention mechanism is used.   Reviewers pointed out limitations in the experimental analysis. In turn authors added experiments on tiny image net with additional architectures and a comparison to several additional baselines on CIFAR 10/100 supporting their findings, which improved the paper. Nevertheless, the paper remained on the borderline after the discussion period and reviewers continued to have doubts about the the significance and novelty of the proposed method, therefore the paper can not be accepted in its current form.
Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera ready submission.
The paper studies the use of pretrained language models (LM) for training the policy in embodied environments. Specifically, a pretrained GPT 2 LM is used to initialize the policy. Environment observations, goals, and actions are encoded appropriately (e.g., converted into text strings) to apply the LM based policy. The experiments study the generalization effect of initializing with pretrained LMs. Reviewers have found the paper made limited contributions. In particular, prior works on text adventure games have explored the use of pretrained LMs for playing games and studied the generalization effect, such as [1]. It s also suggested that the paper should revise the claims made in the experiments, given the limited experimental scope and results.   [1] Keep CALM and Explore: Language Models for Action Generation in Text based Games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan. EMNLP 2020.
The paper proposes a defense against black box adversarial example attacks based on adding small Gaussian noise to the inputs. Its evaluation is carried out empirically using CIFAR 10 and ImageNet datasets.  Despite a somewhat complete experimental evaluation (on two datasets) the lack of theoretical justification strongly affects the significance of the proposed method. It can be clearly seen from the experimental results that the proposed level of noise is a trade off between clean accuracy and attack effectiveness. However, this tradeoff neither implies a substantial degree of security (the attack success rate is roughly halved but this does imply robustness against attacks since the initial success rate is rather high) nor is  the impact to clean accuracy negligible. Furthermore, the robustness of the proposed method against an attack which is aware of such defense (similar to the Kerckhoff s principle in cryptography) is not evaluated. The authors mentioned several directions for addressing this issue in their response but implementation of such improvements is impossible within the level of revisions acceptable in a post review process.  A major revision of the paper taking into account the feedback provided by the current reviews would certainly improve its acceptance chances.  
The paper proposes a variant of Kanerva Machine Wu et al. (2018) by introducing a spatial transformer to index the memory storage and Temporal Shift Module Lin et al., (2019). The KM++ model learns to encode an exchangeable sequence locally via the spatial transformer. The proposed method is evaluated on conditional image generation tasks. The empirical results demonstrated the nearby keys in the memory encoded related and similar images. Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in a way that satisfied the reviewers. The basic ideas in the paper are interesting to both the machine learning and the wider cognitive science communities. However, additional experiments should be included in Table 1 to complete the "DKM w/TSM (our impl)" row on Fashion MNIST, CIFAR 10, and DMLab in the final revision for completeness. 
 A "quantum deformed" generalization of a probabilistic binary neural network is introduced, which can be either run on a quantum computer or simulated with a classical computer. Reviewers agreed that the paper is well written, introduces some new ideas merging quantum computing with a variational Bayesian framework, and the reported numbers on MNIST and Fashion MNIST outperform prior QNN approachers. However, reviewers questioned how useful the proposed ideas are, noting that: the reported gains could be attributed to increased parameterization (this was not carefully ablated with baseline approaches). Additionally, while the quantum supremacy experiments seem technically correct, there was no clear motivation for empirically demonstrating quantum supremacy when no theoretical guarantees are provided. Taken together, there was no clear path to practical improvements of real systems from the proposed ideas.
While reviewers appreciated the simple approach of this work, the biggest concern reviewers had was with the security guarantee of the method. R4 argued that in a certain case recovering an original image x_1 amounted to guessing 2 coefficients. In the discussion phase the authors argued that security amounts to the adversary guessing 4 floating point numbers, not 2, which requires 100s of millions of years to decode an image correctly. However, R4 is correct that only 2 floating point numbers are necessary. This is because, as described by R4 when one sees outputs x_1 * a_{2,2} and x_2 * a_{2,1}, they can reconstruct x_1 as:  x_1   (x_1 * a_{2,2}   x_2 * a_{2,1}) / (a_{1,1} * a_{2,2}   a_{1,2} * a_{2,1})  Now define:  b_1 :  a_{2,2} / (a_{1,1} * a_{2,2}   a_{1,2} * a_{2,1}) b_2 :  a_{2,1} / ((a_{1,1} * a_{2,2}   a_{1,2} * a_{2,1})  Thus the above equation can be written as:  x_1   x_1 * b_1   x_2*b_2  So an adversary needs to guess 2 floating point numbers. Further, R4 points out that an adversary can obtain x_1 up to a scale factor by simply guessing the relative ratio of the the 2 unknown floating point numbers, i.e., if our guess is c:  x_1/c   x_1 * (b_1/c)   x_2 * (b_2/c)  This is a single floating point number, and not all floating point numbers need to be checked. For many images, information can be leaked even if the true scale of the image is not known.  For this reason I would urge the authors to strengthen the security guarantee of their approach. One way to do this would be to adapt the method so to make the resulting guarantee be a more standard one (e.g., differential privacy, standard cryptographic hardness guarantees). This would eliminate the main reviewer concerns and greatly strengthen the paper.
The reviewers appreciate the importance of the problem, and one reviewer particularly appreciated the gains in performance. However, two reviewers raised concerns about limited novelty and missing comparisons to prior work. While the rebuttal helped address these concerns, the novelty is still limited. The authors are encouraged to revise the presentation to clarify the novelty.
This paper is somewhat unorthodox in what it sets out to do: use neuroscience methods to understand a trained deep network controlling an embodied agent. This is exciting, but the actual training of the virtual rodent and the performance it exhibits is also impressive in its own right. All reviewers liked the papers. The question that recurred among all reviewers was what was actually learned in this analysis. The authors responded to this convincingly by listing a number of interesting findings.   I think this paper represents an interesting new direction that many will be interested in.
This paper proposes a multi frame super resolution method including recursive fusion for co registration and registration loss to solve the problem where the super resolution results and the high resolution labels are not pixel wise aligned. While reviewer #1 is positive about this paper, reviewer #2 and #3 rated weak reject and reject respectively. Both reviewer #2 and #3 have extensive experience in the topic of image super resolution. The major concerns raised by the reviewers include the lack of many references, the comparison of recursive fusion with related work, limited test databases, using a single translational motion for the SR images, and limited novelty on the network modules.  The authors provided detailed response to the concerns, however they did not change the overall rating of the reviewers. While the ACs agree that this work has merits, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.
While two reviewers  rated this paper as an accept, reviewer 3 strongly believes there are unresolved issues with the work as summarized in their post rebuttal review. This work seems very promising and while the AC will recommend rejection at this time, the authors are strongly encouraged to resubmit this work.
AR1 is concerned with the presentation of the paper and the complexity as well as missing discussion on recent  embedding methods. AR2 is concerned about comparison to recent methods and the small size of datasets.  AR3 is also concerned about limited comparisons and evaluations. Lastly, AR4 again points out the poor complexity due to the spectral decomposition. While authors argue that the sparsity can be exploited to speed up computations, AR4 still asks for results of the exact model with/without any approximation, effect of clipping spectrum, time complexity versus GCN, and more empirical results covering all these aspects. On balance, all reviewers seem to voice similar concerns which need to be resolved. However, this requires more than just a minor revision of the manuscript. Thus, at this time, the proposed paper cannot be accepted.  
The article considers Gauss Newton as a scalable second order alternative to train neural networks, and gives theoretical convergence rates and some experiments. The second order convergence results rely on the NTK and very wide networks. The reviewers pointed out that the method is of course not new, and suggested that comparison not only with SGD but also with methods such as Adam, natural gradients, KFAC, would be important, as well as additional experiments with other types of losses for classification problems and multidimensional outputs. The revision added preliminary experiments comparing with Adam and KFAC. Overall, I think that the article makes an interesting and relevant case that Gauss Newton can be a competitive alternative for parameter optimization in neural networks. However, the experimental section could still be improved significantly. Therefore, I am recommending that the paper is not accepted at this time but revised to include more extensive experiments.  
The submission proposes to improve generalization in RL environments, by addressing the scenario where the observations change even though the underlying environment dynamics do not change. The authors address this by learning an adaptation function which maps back to the original representation. The approach is empirically evaluated on the Mountain Car domain.   The reviewers were unanimously unimpressed with the experiments, the baselines, and the results. While they agree that the problem is well motivated, they requested additional evidence that the method works as described and that a simpler approach such as fine tuning would not be sufficient.   The recommendation is to reject the paper at this time.
I tend to agree with the most positive reviewer who characterizes the work with the following statements:  "Kronecker factorization was introduced for Convolutional networks (citation is in the paper). Soft unitary constraints also have been introduced in earlier work (citations are also in the paper). Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g. the nice relationship between Kronecker factors and unitary) is a relevant contribution."  The most negative reviewer feels that the experimental work could have evaluated the different components explored here more clearly. For this reason the AC recommends an invitation to the workshop track.
This paper proposes a new approximate sampling approach called Quasi Rejection Sampling (QRS) to exploit global proposal distributions without requiring to know a bound on the associated importance ratio, and providing a trade off between the approximation quality of the sampler and its efficiency. QRS is demonstrated on EBM based text generation tasks. The reviews acknowledge the simplicity of the approach which when combined with advances in learning proposal distributions opens up many potential applications. At the same time, the reviews indicate that more work could be done to make the empirical demonstrations more compelling, with a more thorough coverage of comparisons with MCMC and other alternatives. The authors are encouraged to revise their submission and clarify significance and novelty.
This paper proposes a meta learning algorithm that performs gradient based adaptation (similar to MAML) on a lower dimensional embedding. The paper is generally well written, and the reviewers generally agree that it has nice conceptual properties. The method also draws similarities to LEO. The main weakness of the paper is with regard to the strength of the experimental results. In a future version of the paper, we encourage the authors to improve the paper by introducing more complex domains or adding experiments that explicitly take advantage of the accessibility of the task embedding. Without such experiments that are more convincing, I do not think the paper meets the bar for acceptance at ICLR.
The authors propose a VAE based architecture for generating multivariate time series. The base version of TimeVAE models a distribution over a fixed length sequences of observations using a latent vector of fixed dimensionality and a convolutional encoder and decoder. The Interpretable TimeVAE model incorporates additional features from traditional time series models such as explicit modelling of trends and seasonality. TimeVAE is compared to several baselines such as TimeGAN on four small times series dataset and seems to perform competitively according to two custom evaluation metrics and a visualization.   The reviewers thought that the paper was interesting but not ready for publication due to the following:  The paper s contributions and their significance are not clear  Interpretable VAE was not used in the experiments and its interpretability has not been verified  Coverage of related work is insufficient
The two most experienced reviewers recommended the paper be rejected.  The submission lacks technical depth, which calls the significance of the contribution into question.  This work would be greatly strengthened by a theoretical justification of the proposed approach.  The reviewers also criticized the quality of the exposition, noting that key parts of the presentation was unclear.  The experimental evaluation was not considered to be sufficiently convincing.  The review comments should be able to help the authors strengthen this work.
I thank the authors for their submission and active participation in the discussion. The reviewers unanimously agree that this submission has significant issues, including comparison to baselines/ablations [BnLV,yX9d,PtA1], clarity [BnLV], justification of the method [nX4W]. Thus, I am recommending rejection of this paper.
The paper proposes a self supervised method to predict the gist features of image frames during navigation of an agent supervised by depth and egomotion. The features are retargeted to train navigation policies and outperform previous methods or other pretraining schemes. The idea is related to self supervised by feature prediction but is employed in a zone level as opposed to isolated image level. Though reasonable, in the context of the recent abundance of self supervised prediction papers in various level of spatial visual granularity, the paper may not be of sufficient novelty to present a sizable contribution for ICLR acceptance. 
The reviewers appreciate the steps taken to combine continual learning with few shot learning, this is an interesting intersection with many potential applications. However, the reviewers generally outlined a number of concerns with the benchmark and paper in its current form. They largely feel that this benchmark doesn’t differentiate itself well enough from other incremental learning benchmarks, nor does it experiment with a wide enough variety of settings (additional episode configurations, more FSL/CL approaches). As such, it is difficult to determine at this point what major insights can be gained from this benchmark. I understand that there is a tradeoff: computation is limited, and there is merit in keeping things simple. However, the general consensus is that more work needs to be done in order to fully realize the potential of this benchmark. 
This paper seeks to analyse the important question around why hierarchical reinforcement learning can be beneficial. The findings show that improved exploration is at the core of this improved performance. Based on these findings, the paper also proposes some simple exploration techniques which are shown to be competitive with hierarchical RL approaches.  This is a really interesting paper that could serve to address an oft speculated about result of the relation between HRL and exploration. While the findings of the paper are intuitive, it was agreed by all reviewers that the claims are too general for the evidence presented. The paper should be extended with a wider range of experiments covering more domains and algorithms, and would also benefit from some theoretical results.  As it stands this paper should not be accepted.
This paper is rejected.  The authors focus on offline RL for the sequential recommender system problem and propose an approach that: * builds multiple models based on splits of the offline data using domain knowledge * splits the policy into a context extraction system and context conditioned policy (similar to Rakelley et al.)  While R1 and R4 appreciate the changes, they both feel that the paper is not ready for publication at this time. R1 s main concerns is the generalizability of the proposed solution because it relies heavily on manually defined rules and domain expert knowledge. R4 was concerned with the definition and precision of robustness. How is robustness quantified? Finally, many of the baselines were not built for partially observed environments, so it is unsurprising that they perform poorly. Baselines with recurrent policies would strengthen the paper. 
The paper presents interesting new results for pruning random convolutional networks to approximate a target function. It follows a recent line of work in the topic of pruning by learning. The results are novel, and the techniques interesting. There are some technical issues that are easy to fix within the camera ready timeline (see comments of reviewers below). I would also suggest refining the title of the paper: the lottery ticket hypothesis has an algorithmic component too, which clearly is not covered by existence results.
This paper proposes a new method for learning a model for spatio temporal data described by an (unknown) spatio temporal PDE. The model learns a continuous time PDE using the adjunct method and uses graph networks to perform message passing between different discrete time steps on a grid obtained with Delaunay triangulation.  The method initially 3 favorable and 1 unfavorable ratings, but convincing responses to some of the raised issues led to unanimous recommendations for acceptance (not all reviewer feedback after the rebuttal has been made public, but feedback has been made to the privately AC on these issues by different reviewers).   The reviewers appreciated novelty of the method and numerous ablations.  Initially perceived weaknesses were some key experiments on generalization over different grid discretizations; the simplicity of some experiments, and links to different prior art   many of these points have been dealt with by authors in their response.  The AC concurs and proposes acceptance.
The paper presents a strategy for randomizing the underlying physical hyper parameters of RL environments to improve policy s robustness. The paper has a simple and effective idea, however, the machine learning content is minimal. I agree with the reviewers that in order for the paper to pass the bar at ICLR, either the proposed ideas need to be extended theoretically or it should be backed with much more convincing results. Please take the reviewers  feedback into account and improve the paper.
This paper augments transformer encoder decoder networks architecture with k nearest neighbors to fetch knowledge or information related to the previous conversation, and demonstrates improvements through manual and automated evaluation. Reviewers note the fact that the approach is simple and clean and results in significant improvements, however, the approach is incremental over the previous work (including https://arxiv.org/pdf/1708.07863.pdf). Furthermore, although the authors improved the article in the light of reviewer suggestions (i.e., rushed analysis, not so clear descriptions) and some reviewers increased their scores, none of them actually marked the paper as an accept or a strong accept.
This paper considers adversarial attacks in continuous action model based deep reinforcement learning. An optimisation based approach is presented, and evaluated on Mujoco tasks.  There were two main concerns from the reviewers. The first was that the approach requires strong assumptions, but in the rebuttal some relaxations were demonstrated (e.g., not attacking every step). Additionally, there were issues raised with the choice of baselines, but in the discussion the reviewers did not agree on any other reasonable baselines to use.  This is a novel and interesting contribution nonetheless, which could open the field to much additional discussion, and so should be accepted.
This paper conducts a study on provable defenses to spatially transformed adversarial examples. In general, the paper pursues an interesting direction, but reviewers had many concerns regarding the clarity of the presentation and the depth of the experimental results, which the authors did not address in a rebuttal. 
This paper was reviewed by four experts in the field. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references.
The paper proposes a doubly robust off policy evaluation method that uses both stationary density ratio as well as a learned value function in order to reduce bias. The reviewers unanimously recommend acceptance of this paper.
The scores were not favorable: 5,5,2. R2 felt the motivation of the paper was inadequate. R3 raised numerous technical points, some of which were addressed in the rebuttal, but not all. R3 continues to have issue with some of the results. The AC agrees with R3 s concerns and feels that the paper cannot be accepted in its current form. 
Pros: + Builds in important ways on the work of Sagun et al., 2016.  Cons:   The reviewers were very concerned that the assumption in the paper that the second term of Equation (6) is negligible was insufficiently supported, and this concern remained after the discussion and the revision.   The paper needs to be more precise in its language about the Hessian, particularly in distinguishing between ill conditioning and degeneracy.   The reviewers did not find the experiment very convincing because it relied on initializing the small batch optimization from the end point of the large batch optimization.  Again, this concern remained following the discussion and revision.  The area chair agrees with the authors  comments in their OpenReview post of 08 Jan. 2018 "A remark on relative evaluation," and has discounted the reviewers  comments about the relative novelty of the work.  It is important not to penalize authors for submitting their papers to conferences with an open review process, especially when that process is still being refined.  However, even discounting the remarks about novelty, there are key issues in the paper that need to be addressed to strengthen it (the 3 "cons" above), so this paper does not quite meet the threshold for ICLR Conference acceptance.  However, because it raises really interesting questions and is likely to provoke useful discussions in the community, it might be a good workshop track paper. 
The paper proposes an approach to performing label smoothing, with the amount of smoothing being sample dependent and guided by the model s prediction (similar to self distillation). While the reviewers find the studied problem relevant and important, they find the contributions (in their current state) to be borderline, mainly on the basis of lack of novelty and missing discussion with some related papers. While authors  response was able to partially resolve these concerns, at the end none of the reviewers was a strong advocate for accepting the paper and all scores remained at the borderline (although on the positive side). In concordance with the reviewers, I believe this submission can be made much stronger by digging a bit deeper into the problem, and also making broader connections with the existing literature.  As a concrete example/suggestion (among many other possibilities for strengthening this work), the authors may want to go a bit deeper into the theoretical analysis. Currently, their analysis shows the approach is able to reduce model s confidence, which is what happens in label smoothing and self distillation. However, self distillation is more than confidence reduction, and the information contained in the "dark knowledge" can provide a much stronger regularization than a sole confidence reduction argument. There are already some papers in the literature on the regularization/generalization effects of self distillation, which the authors might want to use as a stepping stone.
The AC and reviewers agree this is an important line of research. However, only one reviewer was initially positive, as the other reviewers raised some issues, and the rebuttal only partially addressed some of them (e.g., the reviewer is now OK with Lemma 1 being correct), but there are typos in the proofs, and there were other issues like the uniform bound that R3 brought up which was retracted in the revision, and such. These issues give us a bit of lack of confidence in the rigor of all the results.  In addition to the lack of carefulness in places, this paper (more than usual for an accepted paper) seemed to miss references in the literature. In addition to all the ones pointed out in the reviews (especially R3 s, which I don t think was fully adequately discussed in the rebuttal), other tight lower bounds on uniform stability have been developed recently (see Thm 4.2 in Bassily et al. https://arxiv.org/pdf/2006.06914.pdf).  From the optimization point of view, it is undesirable to introduce new conditions unless really necessary, and often these new conditions are previously known under a different name; if they really are new, they should be compared to old conditions. In particular, then new "Hessian contractive condition" should be compared to standard non convex conditions like strong growth, error bound, Polyak Lojasiewicz, etc.  Finally, this is based off the Hardt/Recht/Singer 2016 paper, but there is a more recent Hard/Recht work that argues that algorithmic stability is not the right tool, because it cannot explain the fact that training error drops roughly the same with real data or with data with completely random labels   so any generalization theory has to be data dependent. See: "Understanding deep learning requires rethinking generalization" (https://arxiv.org/abs/1611.03530) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals.  So this issue should be addressed as well.  Overall, this could be a promising paper and the AC recommends the reviewers make a substantial revision addressing these concerns.
This paper proposes to generate 3D molecules using a step by step approach. The reviewers raised major concerns on the experiments, novelty, writing and technical details. The authors also were not aware of many of the important references, part (but not all) of which have been included during discussions. It is clear that this work is not ready to be accepted by ICLR.
The paper proposes an architecture search method based on graph hypernetworks (GHN). The core idea is that given a candidate architecture, GHN predicts its weights (similar to SMASH), which allows for fast evaluation w/o training the architecture from scratch. Unlike SMASH, GHN can operate on an arbitrary directed acyclic graph. Architecture search using GHN is fast and achieves competitive performance. Overall, this is a relevant contribution backed up by solid experiments, and should be accepted.
This paper proposes cpl mixVAE, a method for fitting discrete continuous latent variable models based on mixture representations and a novel consensus clustering constraint. After extensive discussion, no one was willing to argue in favor of acceptance, and a majority of the reviewers felt another round of revision is needed. Ultimately, I concur that while the ideas are novel and potentially interesting, more effort is needed to convincingly demonstrate the efficacy of the method. Valid concerns were also raised regarding the claimed "unsupervised" nature of the proposed method, a claim which at the very least requires some additional context. At this point, these outstanding issues require an additional round of revision.
This paper has been independently reviewed by four expert reviewers. Two of them recommended straight acceptance, one of them assesses this work as marginally acceptable after increasing their score as a result of the author s rebuttal, and the last reviewer considers this paper marginally below the acceptance threshold. While the reviewers agree on the importance of the  targeted problem and relative novelty of the presented work, the main points of criticism involve empirical evaluations   its methodology, experimental design, missing relevant and important comparisons. Since the authors have addressed most of those concerns in their rebuttal, I am leaning towards recommending acceptance of this work for ICLR.
The reviewers are unanimous in their opinion that this paper offers a novel approach to learning naïve physics.  I concur.
The reviewers seem to reach a consensus that the contribution of the paper is somewhat incremental give the prior work of Goel et al and that a main drawback of the paper is that it s not clear the similar technique can be applied to multiple **convolutional filters**. The authors mentioned in the response that some of the techniques can be heuristically applied to multiple layers, but the AC is skeptical about it because, with multiple layers and multiple convolutional filters, one has to deal with the permutation invariance caused by the multiple convolutional filters. (It s unclear to the AC how one could have a meaningful setting with multiple layers but a single convolution filters.) 
The authors propose a novel method to estimate the Lipschitz constant of a neural network, and use this estimate to derive architectures that will have improved adversarial robustness. While the paper contains interesting ideas, the reviewers felt it was not ready for publication due to the following factors:  1) The novelty and significance of the bound derived by the authors is unclear. In particular, the bound used is coarse and likely to be loose, and hence is not likely to be useful in general.  2) The bound on adversarial risk seems of limited significance, since in practice, this can be estimated accurately based on the adversarial risk measured on the test set.  3) The paper is poorly organized with several typos and is hard to read in its present form.  The reviewers were in consensus and the authors did not respond during the rebuttal phase.  Therefore, I recommend rejection. However, all the reviewers found interesting ideas in the paper. Hence, I encourage the authors to consider the reviewers  feedback and submit a revised version to a future venue.
The paper analyses GRUs using dynamic systems theory.  The paper is well written and the theory seems to be solid.  But there is agreement amongst the reviewers that the application of the method might not scale well beyond rather simple 1  or 2 D GRUs (i.e., with one or two GRUs).  This limitation, which is an increasingly serious problem in machine learning papers, should be solved before the paper should be published.  A very recent extension of the simulations to 16 GRUs improves this, but a rigorous analysis of higher dimensional systems is pending and poses a considerable block for acceptance.
The paper derives results for nonnegative matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star convex towards randomized planted solutions.  Overall, the paper is relatively well written and fairly clear.  The reviewers agree that the theoretical contribution of the paper could be improved (tighten bounds) and that the experiments can be improved as well. In the context of other papers submitted to ICLR I therefore recommend to reject the paper.  
This paper proposes routing strategies for multilingual NMT. The motivation is to train a single mixture model that can serve the training and prediction of multiple models. Several strategies are proposed: token level, sentence level and task level. This is a simple and straightforward approach (which is fine). The main concerns from the reviewers regard novelty and missing comparisons. In their updated draft, the authors added comparisons to bilingual models and they added a discussion wrt related work. However, the author’s response did not address enough some of other reviewers’ concerns regarding comparison with other approaches, and the lack of novelty persists (mixture models for multi task learning have been previously proposed in the literature), which makes me lean towards rejection. I suggest the authors address these aspects in future iterations of their work. 
AR1 finds the paper overly lengthy and ill focused on contributions of this work. Moreover, AR1 would like to see more results for G ZSL. AR2 finds the  paper is lacking in clarity, e.g. Eq. 9, and complete definition of the end to end decision pipeline is missing. AR2 points that the manuscript relies on GZSL and comparisons to it but other more recent methods could be also cited:   Generalized Zero Shot Learning via Synthesized Examples by Verma et al.   Zero Shot Kernel Learning by Zhang et al.   Model Selection for Generalized Zero shot Learning by Zhang et al.   Generalized Zero Shot Learning with Deep Calibration Network by Liu et al.   Multi modal Cycle consistent Generalized Zero Shot Learning by Felix et al.   Open Set Learning with Counterfactual Images   Feature Generating Networks for Zero Shot Learning Though, the authors are welcome to find even more relevant papers in google scholar.  Overall, AC finds the paper interesting and finds the idea has some merits. Nonetheless, two reviewers maintained their scores below borderline due to numerous worries highlighted above. The authors are encouraged to work on presentation of this method and comparisons to more recent papers where possible. AC encourages the authors to re submit their improved manuscript as, at this time, it feels this paper is not ready and cannot be accepted to ICLR. 
This paper proposes a neural architecture for tasks involving user interfaces. Tasks involve detecting objects on screen, writing captions about UI components, attribute recognition, etc. The reviewers for this submission found the proposed model to be reasonable and effective. They also found the paper to be well written and easy to understand. However, they did have one major concern, before and after rebuttal: While the model and design choices were reasonable, they questioned if the insights gained from this paper were of interest and relevance to the broader vision community. They also had other concerns/suggestions including adding inference costs and adding more detail, which were addressed in the rebuttal. Another concern was the fact that multi task did not provide large gains over single task. I agree with the authors in this regard. I think the goal here was to produce a multi task model that attained at least parity with respect to single task, because a single model would provide large benefits when running on a device, and hence I think this concern was well addressed.  My takeaway from the paper, reviews and discussion, however, continues to focus on the major concern of the reviewers. I think this paper would have benefited from answering at least one (if not more) of the following questions to the reader: (1) Why should the broader community work on this task ? (2) If this task is of limited interest, are there instead, aspects of this task that serve as a useful testbed for multimodal research ? (3) If the task and testbed are not directly applicable, are there new techniques developed in this paper that are broadly applicable to other problems or domains ?  Unfortunately, I think that this paper does not presently address either of these questions strongly to the reader. The paper proposes a method for their task, but readers who aren t directly interested in that end task may find this submission less interesting, in terms of insights for their own work. Given the above, I encourage the authors to address this concern and resubmit. I recommend rejection.
This paper describes an incorporation of attention into model agnostic meta learning. The reviewers found that the paper was rather confusing in its presentation of both the method and the tasks. While the results seemed interesting, it was difficult to frame them due to lack of clarity as to what the task is, and the relation between attention and MAML. It sounds like this paper needs a bit more work, and thus is not suitable for publication at this time.  It is disappointing that the reviews were so short, but as the authors did not challenge them, unfortunately the AC must decide on the basis of the first set of comments by reviewers.
This paper presents PiCO, a novel approach for partial label learning, which achieves very strong performance close to that of fully supervised learning and outperforms PPL baselines. The experiments are extensive with very impressive results and the analysis are thorough.
The paper considers the problem of 2D point goal navigation in novel environments given access to an abstract occupancy grid map of the environment, together with knowledge of the agent s state and the goal location typical of point goal navigation. The paper proposes learning a navigation policy in a model based fashion, whereby the architecture predicts the parameters of the transition function and then uses this learned transition function to plan the agent s actions. The authors also describe a model free approach that extends a version of DQN to reason over the 2D maps.  The paper was reviewed by four knowledgeable referees, who read the author response. The general problem of learning to navigate a priori unknown environments to reach a desired goal is an interesting problem that has received significant attention of late in the learning community. In its current form, however, the paper does not adequately convey why this is a difficult problem that can not be solved using existing planning techniques or why it benefits from learning, particularly given access to an abstract map. These concerns apply more generally to point goal navigation, namely the assumption that the pose of the agent and goal are fully known throughout (or the agent relative pose of the goal) and that there is no uncertainty in the agent s motion. The practicality of these assumptions is unclear, and they are inconsistent with decades of research in robotics and robot learning, which addresses the more realistic setting in which there is uncertainty in pose and motion. The author response helps to clarify some of these questions, but it is still not fully clear why existing methods are insufficient for this task, whether they use traditional planning methods or are learned. Revisiting the discussion of why this is a hard problem would strengthen the paper, as would a more thorough evaluation that compares against other baselines.
This paper had some quality and clarity issues and the lack of motivation for the approach was pointed out by multiple reviewers.  Just too far away from the acceptance threshold.
Thank you for submitting you paper to ICLR. The reviewers are all in agreement that the paper is suitable for publication, each revising their score upwards in response to the revision that has made the paper stronger.  The authors may want to consider adding a discussion about whether the simple standard Gaussian prior, which is invariant under transformation by an orthogonal matrix, is a sensible one if the objective is to find disentangled representations. Alternatives, such as sparse priors, might be more sensible if a model based solution to this problem is sought.
The paper summarizes existing work on binary neural network optimization and performs an empirical study across a few datasets and neural network architectures. I agree with the reviewers that this is a valuable study and it can establish a benchmark to help practitioners develop better binary neural network optimization techniques.  PS: How about "An empirical study of binary neural network optimization" as the title? 
This manuscript investigates the posterior collapse in variational autoencoders and seeks to provide some explanations from the phenomenon. The primary contribution is to propose some previously understudied explanations for the posterior collapse that results from the optimization landscape of the log likelihood portion of the ELBO.  The reviewers and AC agree that the problem studied is timely and interesting, and closely related to a variety of recent work investigating the landscape properties of variational autoencoders and other generative models. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the technical difficulty and importance of the results. In reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the results. There were also concerns about novelty. In the opinion of the AC, the manuscript in its current state is borderline, and should ideally be improved in terms of clarity of the discussion, and some more investigation of the insights that result from the analysis.
Lean in favor  Strengths:  The paper tackles the difficult problem of automatic robot design. The approach uses graph neural networks to parameterize the control policies, which allows for weight sharing / transfer to new policies even as the topology changes.  Understanding how to efficiently explore through non differentiable changes to the body is an important problem (AC). The authors will release the code and environments, which will be useful in an area where there are  currently no good baselines (AC).   Weaknesses: There are concerns (particularly R2, R1) over the lack of a strong baseline, and with the results  being demonstrated on a limited number of environments (R1)  (fish, 2D walker). In response, the authors clarified the nomenclature and description of a number of the baselines, and added others. AC: there is no submitted video (searches for "video" on the PDF text produces no hits); this is seen by the AC as being a real limitation from the perspective of evaluation.  AC agrees with some of the reviewer remarks that some of the original stated claims are too strong.   AC: the simplified fluid model of Mujoco (http://mujoco.org/book/computation.html#gePassive) is unable to model the fluid state, in particular the induced fluid vortices that are responsible for a good portion of fish locomotion, i.e., "Passive and active flow control by swimming fishes and mammals" and other papers. Acknowledging this kind of limitation will make the paper stronger, not weaker; the ML community can learn from much existing work at the interface of biology and fluid mechancis.  There remain points of contention, i.e., the sufficiency of the baselines. However, the reviewers R2 and R3 have not responded to the detailed replies from the authors, including additional baselines (totaling 5 at present)  and pointing out that baselines such as CMA ES (R2) in a continuous space and therefore do not translate in any obvious way to the given problem at hand.    On balance, with the additional baselines and related clarifications, the AC feels that this paper makes a  useful and valid contribution to the field, and will help establish a benchmark in an important area. The authors are strongly encouraged to further state caveats and limitations, and to emphasize why some candidate baseline methods are not readily applicable. 
 PROS: 1. well written and clear 2. added extra comparison to dagger which shows success 3. SOTA results on open ai benchmark problem and comparison to relevant related work (Shi 2017) 4. practical applications 5. created new dataset to test harder aspects of the problem  CONS: 1. the algorithmic novelty is somewhat limited 2. some indication of scalability to real world tasks is provided but it is limited
*Summary:* Low rank bias in nonlinear architectures.   *Strengths:*     Significant theoretical contribution.    Well written; detailed sketch of proofs.   *Weaknesses:*    More intuitions desired.    Restrictive assumptions.   *Discussion:*   Authors made efforts to improve the discussion in response to 6P7z. Authors agree with eeoo about Assumption 2 being relatively restrictive but point out that main results do not need it. They discuss Assumption 1 and revised it formulation. Reviewer eeoo was satisfied with this. Following the discussion udhX raised their score (after authors acknowledged an early problems and improved them) and found the paper well written with novel and significant results.   *Conclusion:*   Three reviewers consider this a good paper that should be accepted. A fourth reviewer rated it marginally above the acceptance threshold but following the discussion period explicitly recommended acceptance. I find the topic interesting, timely, relevant. In view of unanimously favorable feedback from four reviewers I am recommending accept.
This paper presents an extension of the neural ODE approach to include discrete changes in the continuous time dynamics. All reviewers agree the contribution made by this paper is worth publishing. Most of the reviewers  concerns have been answered in the rebuttal and I therefore recommend accepting this paper. 
All of the reviewers agree that the paper clearly presents promising ideas in developing a novel actor critic algorithm. The experiments do not show a significant gain against the baselines, but they support the presented ideas. I appreciated the ablation study on dual AC.  Detailed comments: My understanding is that the x axis in Figures 1 & 2 shows the number of iterations each of which contains batch_size*1000 environment steps. It is more standard to show those plots in terms of the number of environment steps. Further, the optimal batch_size for different algorithms may be different, so using the same batch_size for all of the algorithms is not fair.
This work proposes an algorithm for generating training data to train automatic theorem proving models. In particular, it allows users to pose specific generalization challenges to theorem proving models and evaluate their performance. In doing so, it provides a degree of control over the task space that is greater than when working with  organic  corpora of real theorems and proofs.   The authors demonstrated the utility of their generated data by training well known models such as transformers and GNNs, and were able to derive insights such as the value of MCTS style planning for finding proofs in particular settings.   After the rebuttal period, all authors agreed that the work was well executed and that the algorithm creates datasets that will be of value to the (learning based) theorem proving community. As such, they all recommended acceptance to a greater or lesser degree. I am convinced by their arguments, because I think there is real value in using controlled synthetic data alongside real data when making scientific progress on hard problems like theorem proving. I am particularly convinced by the observation that the data generated by this method has already led to improved performance on a real corpus of proofs, as the authors state in their rebuttal. If they have not done so already, I encourage the authors to report this fact in the camera ready version of their paper citing the relevant work. 
The paper proposes to improve visual relation prediction by using depth maps.  Since existing RGB images do not contain depth informations, the authors use a monocular depth estimation method to predict depth maps.  The authors show that using depths maps, they are able to improve prediction of relations between ground truth object bounding boxes and labels.    The paper got relatively low scores (with 3 initial weak rejects).  After the revision and suggested improvements, one of the reviewers updated their score so the paper now has 2 weak rejects and 1 weak accept.   The paper had the following weaknesses: 1. The paper has limited technical novelty as it combines off the shelf components.  The components also used different backbones (ResNet at some places, VGGNet at others) that were directly from prior work.  Was there any attempt to have an unified architecture? As the main novelty of the work is not in the model aspect, the paper needs to have stronger experiments and analysis. 2. More analysis on the quality of the depth estimation is needed.  Ideally, the work should provide some insight into whether some of the errors is due to having bad depth estimation?  The depth estimation method used is from 2016, there are newer depth estimation methods now.  Would having better depth estimation give improved results?  Experiments that illustrates that method works well with predicted bounding boxes instead of ground truth bounding boxes will also strengthen the paper.   3. There was the question of whether the related Yang et al. 2018 workshop paper should be included as basis for comparison.  In the AC s opinion, Yang et al. 2018 is not concurrent work and should be treated as prior work.  However, it is not clear whether it is feasible to compare against that work.  The authors should attempt to do so and if infeasible, clearly articulate why that is the case. 4. As pointed out by R3, once there is a depth map available, it is also possible to compare against 3D methods (such as those that operate on point clouds)  Overall the paper had a nice insight by proposing the simple but effective idea of using depth information to help with visual relation prediction.  Still the work is somewhat borderline in quality.  In the AC s opinion, the main contribution and insight of the paper is of limited interest to the ICLR community, and it would be more appreciated in a computer vision conference.  The authors are encouraged to improve the paper with stronger experiments and analysis, incorporate various suggestions from the reviewers, and resubmit to a vision conference. 
This paper proposes an implicit model of graphs, trained adversarially using the Gumbel softmax trick.  The main idea of feeding random walks to the discriminator is interesting and novel.  However, 1) The task of generating  sibling graphs , for some sort of bootstrap analysis, isn t well motivated. 2) The method is complicated and presumably hard to tune, with two separate early stopping thresholds that need to be tuned 3) There is not even a mention of a large existing literature on generative models of graphs using variational autoencoders.
The paper proposes an information theoretic quantity to measure the performance of transferred representations with an operational appeal, easier computation, and empirical validation.   The relation of the proposed measure to test accuracy is not considered. The operational meaning holds exactly only in the special case of linear fine tuning layers. The paper seems to import heavily from previous works.   Reviewers found it difficult to understand whether the proposed method makes sense, that the computation of relevant quantities might be difficult in general, and that the comparison with mutual information was not clear. The revision addresses these points, adding experiments and explanations. Yet, none of the reviewers gives the paper a rating beyond marginally above acceptance threshold.   All reviewers found the paper interesting and relevant, but none of them found the paper particularly strong. This is a borderline case of a sound and promising paper, which nonetheless seems to be missing a clear selling point.   I would suggest that developing the program laid out in the conclusions could make the contributions more convincing, in particular the development of more scalable algorithms and the application of the proposed measure to the design of hierarchies for transfer learning. 
While several reviewers acknowledge that the paper contains potentially useful ideas related to multi modal self training applied to genomic data, they also point out a number of weaknesses and room for improvement that the discussion with authors did not fully address. This includes in particular the need to better explain the details of what is done in the paper; the choice of experiments which is not relevant (eg, predicting promoter regions) or complete (eg, showing results on only one transcription factor); the lack of comparison with existing methods, etc... We therefore consider that the paper is not ready for publication in its current form, but hope that the reviews will help the authors work on a revision addressing the issues.
This paper extends the prior work on disentanglement and attention guided translation to instance based unsupervised content transfer. The method is somewhat complicated, with five different networks and a multi component loss function, however the importance of each component appears to be well justified in the ablation study. Overall the reviewers agree that the experimental section is solid and supports the proposed method well. It demonstrates good performance across a number of transfer tasks, including transfer to out of domain images, and that the method outperforms the baselines. For these reasons, I recommend the acceptance of this paper.
This paper proposes to represent a deep neural network as a graph and analyze its learning dynamics as a time series of weighted graphs corresponding to the neural network. As the graph representations, the authors propose to use a rolled representation in addition to a unrolled representation. Then, they proposed to utilize the graph features of the representations for predicting its predictive accuracy.   This paper presents an interesting idea which could be used for predicting the test accuracy from the first few epochs training. However, there are also several weaknesses as pointed out by the reviewers. First, the justification of using the graph structure to predict the accuracy is weak (indeed, the graph structure can be used for prediction, but its necessity is not well supported), and there is no theory to support the proposed method. Second, the problem setting is a bit wired. The training data is generated by using the same architecture and data set. Although the authors gave additional experiments on the architecture generalization, it is still difficult to see how convincing the method is for more general settings. Third, baseline methods are not shown in their experiments.   In addition to that, the thresholds for the classification tasks seem to be too small (like 40% in CIFAR10) which would make the problem too easy. Therefore, the practicality of the method is rather unclear.   This paper is quite on the borderline, but for the reasons listed above, it is a bit below the acceptance threshold.
This paper studies RL with perturbed rewards, where a technical challenge is to revert the perturbation process so that the right policy is learned.  Some experiments are used to support the algorithm, which involves learning the reward perturbation process (the confusion matrix) using existing techniques from the supervised learning (and crowdsourcing) literature.  Reviewers found the problem setting new and worth investigating, but had concerns over the scope/significance of this work, mostly about how the confusion matrix is learned.  If this matrix is known, correcting reward perturbation is easy, and standard RL can be applied to the corrected rewards.  Specifically, the work seems to be limited in two substantial ways, both related to how the confusion matrix is learned.   * The reward function needs to be deterministic.   * Majority voting requires the number of states to be finite. The significance of this work is therefore mostly limited to finite state problems with deterministic reward, which is quite restricted.  As the authors pointed out, the paper uses discretization to turn a continuous state space into a finite one, which is how the experiment was done.  But discretization is likely not robust or efficient in many high dimensional problems.  It should be noted that the setting studied here, together with a thorough treatment of an (even restricted) case, could make an interesting paper that inspires future work.  However, the exact problem setting is not completely clear in the paper, and the limitations of the technical contributions is also somewhat unclear.  The authors are strongly advised to revise the paper accordingly to make their contributions clearer.  Minor questions:     In lemma 2, what if C is not invertible.     The sampling oracle assumed in def 1 is not very practical, as opposed to what the paper claims.     There are more recent work at NIPS and STOC on attacking RL (including bandits) algorithms by manipulating the reward signals.  The authors may want to cite and discuss.
This paper suggests augmenting adversarial training with a Lipschitz regularization of the loss, and suggests that this improves the adversarial robustness of deep neural networks. The idea of using such regularization seems novel. However, several reviewers were seriously concerned with the quality of the writing. In particular, the paper contains claims that not only are not needed but also are incorrect. Also, the Reviewer 2 in particular was also concerned with the presentation of prior work on Lipschitz regularization.   Such poor quality of the presentation makes it impossible to properly evaluate the actual paper contribution. 
The paper proposes to introduce ideas from singular theory to deep learning. All reviewers agree that the work is not yet ready for publication. The key issue seems to boil down to the fact that the paper does not propose nor verify any clearly motivated scientific hypothesis. Relatedly, the work includes many too broad or unscientific claims such as "To understand why classical measures of capacity fail to say anything meaningful about DNNs". Such statements should be given more precisely and with a proper citation.    Based on this I have to recommend rejecting the paper. At the same time, I would like to thank the Authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work.
The paper is a well written review of regularization approaches in deep learning. It does not offer novel approaches or novel insight with empirically demonstrated usefulness  > ICLR is not the appropriate venue for it.
The paper presented an adaptive stochastic gradient descent method with layer wise normalization and decoupled weight decay and justified it on a variety of tasks. The main concern for this paper is the novelty is not sufficient. The method is a combination of LARS and AdamW with slight modifications. Although the paper has good empirically evaluations, theoretical convergence proof would make the paper more convincing. 
This paper gives a new PAC Bayesian generalization error bound for graph neural networks (GCN and MPGNN). The bound improves the previously known Rademacher complexity based bound given by Garg et al. (2020). In particular, its dependency on the maximum node degree and the maximum hidden dimension is improved.  This paper gives an interesting improvement on the generalization analysis of GNNs. The writing is clear, where its connection to existing work and its technical contribution are well discussed.  The biggest concern is its technical novelty. Indeed, the proof follows the out line of Neyshabur et al. (2017). Given that the technical novelty would be a bit limited, however, the analysis should properly deal with the complicated structure specific to GNNs which makes the analysis more difficult than usual CNN/MLP and requires subtle and careful manipulations.  In addition to that, the improvement of the generalization bound is valuable for the literature (while the improvement seems a bit minor for graphs with small maximum degree).   For these reasons, I recommend acceptance for this paper.
Although the reviewers found the paper well written that analyzes a relatively popular algorithm (TD(0) version of A3C), there are concerns regarding the novelty of the convergence results given those for A2C, the comparison of the results with those for A2C, and the sufficiency of the experiments. Although the authors addressed some of these issues/comments during the rebuttals, it seems none of the reviewers is excited about the paper and there still exist concerns regarding the novelty of the results and how they are compared with those in the literature. I would suggest that the authors take the reviewers  comments into account, have a more comprehensive discussion about the relation of their results with those in the literature (two time scale algorithms), and prepare their work for future conferences. 
This paper is concerned with warm starting Bayesian optimization (i.e. starting with a better surrogate model) through transfer learning among related problems.   While the key motivation for warm starting BO is certainly important (although not novel), there are important shortcomings in the way the method is developed and demonstrated. Firstly, the reviewers questioned design decisions, such as why combine NNs and GPs in this particular way or why the posterior variance of the hybrid model is not calculated. Moreover, there are issues with the experimental methodology that do not allow extraction of confident conclusions (e.g. repeating the experiments for different initial points is highly desirable). Finally, there are presentation issues. The authors replied only to some of these concerns, but ultimately the shortcomings seem to persist and hint towards a paper that needs more work. 
This paper shows experiments in favor of learning and using heteroscedastic noise models for differentiable Bayes filter. Reviewers agree that this is interesting and also very useful for the community. However, they have also found plenty of issues with the presentation, execution and evaluations shown in the paper. Post rebuttal, one of the reviewer increased their score, but the other has reduced the score. Overall, the reviewers are in agreement that more work is required before this work can be accepted.  Some of existing work on variational inference has not been included which, I agree, is problematic. Simple methods have been compared but then why these methods were chosen and not the other ones, is not completely clear. The paper definitely can improve on this aspect, clearly discussing relationships to many existing methods and then picking important methods to clearly bring some useful insights about learning heteroscedastic noise. Such insights are currently missing in the paper.  Reviewers have given many useful feedback in their review, and I believe this can be helpful for the authors to improve their work. In its current form, the paper is not ready to be accepted and I recommend rejection. I encourage the authors to resubmit this work. 
This is a well written paper that aims to address an important problem. However, all the reviewers agreed that the experimental section is currently too weak for publication. They also made several good suggestions about improving the paper and the authors are encouraged to incorporate them before resubmitting.
All the reviewers highlight that the paper addresses the important issue of extending deep latent variable models to handle missing non at random data, which are known to be very difficult. The authors suggest modeling the mechanism of missing values and perform inference  using amortized importance weighted variational inference and demonstrate the capacities of their approach on many experiments. The paper highlight the trade off between the complexity of the data model and that of the missing data mechanism. The authors appropriately answer reviewers comments, add new experiments varying the percentage of missing values, and give more details on the methodological part.  I also think that this is a valuable contribution to the community, that the literature is well covered (the historical statistical litterature and the ML one), and that it provides new insights and methods to tackle this difficult problem.  
This paper evaluates several methods for physical prediction on the PHYRE benchmark, finding that while object based methods (e.g. IN, Transformer) perform better in terms of predictive accuracy, pixel based methods (e.g. STN, Deconv) perform better in terms of downstream task performance. The justification is that it is easier for the agent to evaluate good actions using an image based representation rather than an object based representation.  Pros:   Important attempt to catalogue the current state of the field of physical reasoning   Improved baselines on PHYRE  Cons:   As pointed out by R5, there is a failure to evaluate any hybrid pixel relational methods, such as OP3, R NEM, C SWM, etc. Given that the paper s main contribution is its assessment of the current state of the field (in the authors  own words: "providing a realistic picture of the current state of the field"), this seems like a major oversight to me.   As pointed out by several reviewers, the analysis itself is somewhat limited. I don t see it as a problem that the paper does not propose any new methods, but in that case it needs to present a more thorough picture of why certain methods work better in some cases. For example, I share R1 s concern that the Dec model performs worse than the identity function. Can you provide more detailed analysis demonstrating why the latent space is more useful? Can you demonstrate in what cases the object based classifiers struggle, and why? Incorporating more careful hypotheses and ablations I think would help a lot in turning this into a much stronger paper.  I don t think it s a problem that the paper relies solely on 2D, fully observed environments (many other papers on physical reasoning do this, so I think it s a reasonable choice), and I don t think it s a problem that the paper does not propose a new method. But I do find myself agreeing with the reviewers that the evaluations done within this context are insufficient. In the rebuttal, the authors emphasize the various conclusions stemming from the results (regarding the effect of model error, the extent of generalization, what "accuracy" means), but these conclusions are not that surprising (model error is a well known problem in MBRL, deep models are notorious for their failure to achieve strong generalization, and the limitations of pixel accuracy have spawned whole research areas, such as contrastive and adversarial approaches). Again, I don t think the lack of surprising conclusions is itself an issue. But, the fact that the paper does not really make an attempt to explain any nuances or details regarding the conclusions makes it hard to draw a clear contribution from the paper; in that sense, I don t feel the paper really provides "clear guidance" as is argued in the rebuttal.  I do think this paper is very close to being acceptable, and could make a great submission to a future conference if the authors can spend a bit more time on (1) the baselines (i.e., incorporating hybrid models, and ensuring all methods pass basic gut checks) and (2) supporting their conclusions with more detailed analyses.
There is insufficient support to recommend accepting this paper.  Generally the reviewers found the technical contribution to be insufficient, and were not sufficiently convinced by the experimental evaluation.  The feedback provided should help the authors improve their paper.
This submission presents bounds on the training dynamics (including gradient evolution) for deep linear (and in some cases nonlinear) networks as a function of the width of the layers or number of convolutional layers. The work also presents experimental results that provide evidence that the bounds are tight.  Strengths: The work provides interesting insights into these training dynamics, particularly for the wide but not infinite setting, which is less studied. The work also adapts cluster graphs and Feynman diagrams to derive these bounds, which could be useful tools for researchers in this field.  Weaknesses: The validity and applicability of some of the results for nonlinear networks was not entirely clear at first but has been clarified in the revision.  The reviewer consensus was to accept this submission. 
The paper investigates quantization for speeding up RL. While the reviewers agree that the idea is a good one (it should definitely help), they also have a number of concerns about the paper and presentation. In particular, the reviewers feel that the authors should have provided more insight into the challenges of quantization in RL and the tradeoffs involved. After having read the rebuttals, the reviewers believe that the authors are on the right track, but that the paper is still not ready for publication. If the authors take the reviewer comments and concerns seriously and update their paper accordingly, the reviewers believe that this could eventually result in a strong paper.
Strengths: This paper provides a useful review of some of the recent work on gradient estimators for discrete variables, and proposes both a computationally more efficient variant of one, and a new estimator based on piecewise linear functions.  Weaknesses:  Many new ideas are scattered throughout the paper.  The notation is a bit dense.  Comparisons to RELAX, which had better results than REBAR, are missing.  Finally, it seems that REBAR was trained with a fixed temperature, instead of optimizing it during training, which is one of the main benefits of the method.  Points of contention: Only R1 mentioned the omission of REBAR and RELAX.  A discussion and a few comparisons to REBAR were added to the paper, but only in a few experiments.  Consensus:  This paper is borderline.  I agree with R1: quality 6, clarity 8, originality 6, significance 4.  All reviewers agreed that this was a decent paper but I think that R2 and R3 were relatively unfamiliar with the existing literature.  Update for clarification:    This section has been added to clarify the reasons for rejection.  The abstract of the paper states:  "We show that the commonly used Gumbel Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks."  The fact that Gumbel Softmax is biased is well known.  Reducing its bias was the motivation for developing the _exactly_ unbiased REBAR method, which already has similar asymptotic complexity.  A major side benefit of using an exactly unbiased estimator is that the estimator s hyperparameters can be automatically tuned to reduce variance, as in REBAR and RELAX.  This paper focuses on methods for reducing bias and variance, but hardly discusses related methods that already achieved its stated aims. This a major weakness of the paper.  The experiments only compared with REBAR, and did not even tune the temperature to reduce variance (removing one of its major advantages).  This reject decision is not made mainly on lack of experiments or state of the art results.  It s because the idea of reducing the bias of continuous relaxation based gradient estimators has already been fruitfully explored, and zero bias CR estimators have been developed, but this work mostly ignores them.  However, thorough experiments are always going to be necessary for a paper proposing biased estimators, because there are already many such estimators, and little theory to say which ones will work well in which situations.  Suggestions to improve the paper:  Run experiments on all methods that directly measure bias and variance.  Incorporate discussion of REBAR throughout, not just in an appendix.  Run comparisons against REBAR and RELAX without crippling their ability to reduce variance.   Do more to characterize when different estimators will be expected to be effective.
The paper presents an approach for spatio temporal representation learning using Transformers. It introduces a particular architecture design, which shows an impressive computational efficiency. The reviewers agree that the experimental results are strong, and unanimously recommend the paper for acceptance. The reviewers find their concerns regarding the details of the approach/setting address after the authors  response.  We recommend accepting the paper.
 The paper proposes to improve noise robustness of the network learned features, by augmenting deep networks with Spike Time Dependent Plasticity (STDP). The new network show improved noise robustness with better classification accuracy on Cifar10 and ImageNet subset when input data have noise. While this paper is well written, a number of concerns are raised by the reviewers. They include that the proposed method would not be favored from computer vision perspective, it is not convincing why spiking nets are more robust to random noises, and the method fails to address works in adversarial perturbations and adversarial training. Also, Reviewer #2 pointed out the low level of methodological novelty. The authors provided response to the questions, but did not change the rating of the reviewers. Given the various concerns raised, the ACs recommend reject.
The paper studies optimization landscapes arising the fitting of sparse linear networks to data. It argues that for scalar outputs, every local minimum is global, while for d >  3 dimensional outputs, there can be spurious local minimizers. The paper also argues that similar results hold for deep networks. Counterexamples on the existence of non global local minimizers are constructed analytically and corroborated by probing the optimization landscape experimentally.   Pros and cons:  [+] Network sparsification is an important practical problem, and there are relatively few theoretical guidelines on when and how sparsification can be achieved. In particular, results that help to explain why trained networks are often sparsifiable and/or provide theoretical guarantees for sparsification algorithms would be significant.   [ ] Reviewers raised concerns about the significance of the paper’s results. In particular, they found it difficult to connect the paper’s analysis of landscapes of linear networks to the question of when and why practically occurring networks can be pruned. They also had difficulty isolating new ideas in the mathematical analysis beyond previous works on the landscape of linear networks. Finally, several reviewers expressed concerns about the extent to which the paper’s observations on linear networks generalize to nonlinear networks.  [ ] Reviewers raised concerns about the clarity of the paper. The mathematical exposition is unclear in places: conditions are not clearly stated, terminology is occasionally vague. Moreover the paper’s handling of optimality conditions for constrained optimization problems is unclear (e.g., the proof of Theorem 6 uses the unconstrained optimality condition $\ell^T X   0$ in the inductive step, even though the inductive hypothesis pertains to a constrained problem).   [ ] The technical results make assumptions that are occasionally quite strong. For example, Theorem 7 requires orthogonality of the data matrices, when restricted to indices where the weights are nonzero. As reviewers note, this assumption seems highly restrictive.    Overall, the paper addresses a topic that is important to the ICLR community: developing theoretical analyses of network sparsification. However, the significance of its results is not clear, and the technical exposition would need significant improvement to meet the bar for publication. 
This paper proposes a novel way to learn hierarchical disentangled latent representations by building on the previously published Variational Ladder AutoEncoder (VLAE) work. The proposed extension involves learning disentangled representations in a progressive manner, from the most abstract to the more detailed. While at first the reviewers expressed some concerns about the paper, in terms of its main focus (whether it was the disentanglement or the hierarchical aspect of the learnt representation), connections to past work, and experimental results, these concerns were fully alleviated during the discussion period. All of the reviewers now agree that this is a valuable contribution to the field and should be accepted to ICLR. Hence, I am happy to recommend this paper for acceptance as an oral.
This submission tackles the problem of model explainability from the perspective of masking based saliency methods.  Several metrics are proposed for evaluating saliency methods including  a new « soundness »  concept. Experiments using a consistency score to simultaneously evaluate completeness and soundness are provided.    Most of the reviewers were not convinced by the approach and have raised several issues.   After rebuttal and despite the interest in the introduction of the concept of « soundness »  to better explain model decision, the current proposition needs to be improved. In particular, the interest of this soundness concept does not bring out, many details of the method are not clear enough and the effectiveness of the proposed measure is still questionable. It would be interesting that the authors consider the R’s comments as the ones for additional  experiments to demonstrate the relevancy of their contribution.
This work proposes an approach to unify pre training based and meta learning based few shot learning, inspired by dropout.  None of the reviewers support the acceptance of this work, despite the authors  detailed rebuttals, with the majority of reviewers confirming their preference for rejection following the author response.   I unfortunately could not find a good reason to dissent from the reviewers majority opinion, and therefore also recommend rejection at this time.
This is a very interesting paper that also seems a little underdeveloped. As noted by the reviewers, it would have been nice to see the idea applied to domains requiring function approximation to confirm that it can scale   the late addition of Freeway results is nice, but Freeway is also by far the simplest exploration problem in the Atari suite. There also seems to be a confusion between methods such as UCB, which explore/exploit, and purely exploitative methods. The case gamma_E > 0 is also less than obvious. Given the theoretical leanings of the paper, I would strongly encourage the authors to focus on deriving an RMax style bound for their approach. 
As the reviewers say, the subject matter of this paper is important, and of interest to the ICLR audience (I discount tzHo s suggestion that the paper is more suited to other venues).  However, there are three primary reasons this paper should not be published as is: 1. A theoretical paper *must* be precise, accurate, and clear.    The reviewers universally consider the notation ambiguous, and the theorem unproved because of this ambiguity.  2. The leap from solving a series of tasks optimally to having solved the composition optimally is indeed poorly argued in the paper, and is not resolved by the discussion.  3. I would also strongly recommend showing a less trivial example.  It does not need to be "real world", but it should address numerically the specific doubt of BuW: the relationship of OTE of the composed model to OTE of the subtask models.  In summary: TaFL may be true, but this paper does not show it to be true; or conversely, TaFL may be false, in which case publishing this paper would be a grave error.  The authors should use the reviewers reports to clarify and strengthen the argument.  This does include showing numerical results, because inspection of the code generating such results can often aid reviewers and readers in judging the truth of the theoretical claims, and in finding subtle missteps in the derivations.
The article studies the stability of ResNets in relation to initialisation and depth. The reviewers found that this is an interesting article with important theoretical and experimental results. However, they also pointed out that the results, while good, are based on adaptations of previous work and hence might not be particularly impactful. The reviewers found that the revision made important improvements, but not quite meeting the bar for acceptance, pointing out that the presentation and details in the proofs could still be improved. 
The paper provides an interesting set of theoretical ideas to improve the estimation of normalizing flows on datasets that fail to be fully dimensional. Although the method is appealing, I believe the paper falls a bit short of acceptance at the conference. Too many practical issues are left out, as discussed by reviewers, and the method seems promising but not fully connected to the rest of the literature on estimating low dimensional distributions living in high dimensional spaces. We encourage the authors to use the feedback contained in this round of reviews to improve their work.
The paper extends MAML so that a learned behavior can be quickly (sample efficiently) adapted to a new agend (allied or opponent). The approach is tested on two simple tasks in 2D gridworld environments: chasing and path blocking.  The experiments are very limited, they do not suffice to support the claims about the method. The authors did not enter a rebuttal and all the reviewers agree that the paper is not good enough for ICLR.
Reviewers were all on the positive side for this paper. Multiple reviewers liked the new and interesting task that this paper presents, found that the proposed method works well, is sufficiently compared to alternative approaches, and could serve as a solid baseline for future work in this area. The main limitation that reviewers noted was that results were shown only on the synthetic ABC dataset using dense point clouds with very little noise. The authors wrote *very* thorough responses to all reviewer questions. One reviewer noted that these responses answered all of their questions.  It is worth noting that a paper that solves a similar problem was recently published at NeurIPS ("PIE NET: Parametric Inference of Point Cloud Edges"). This paper was not published as of the ICLR submission deadline, so it was judged as  contemporary work  which the authors have no obligation to compare against. Nevertheless, they did attempt to make a comparison in their responses. I would ask that the authors include some discussion of this comparison in their final version of the paper.
All three reviews for this paper were negative, and the authors did not provide rebuttals or comments on the reviews.  The main drawback of this work identified by the reviewers is that the empirical study is not sufficient (e.g., limited comparisons and ablation studies as well as low dimensional examples).
The paper addresses a problem encountered in many real world applications, i.e. the treatment of tabular data, composed of heterogeneous feature types, where samples are not i.i.d. In this case, learning is more effective if the typically successful approach for i.i.d. data (boosted decision trees + committee techniques) is combined with GNN to take into account the dependencies between samples. The main contribution of the paper with respect to previous work in the field is the introduction of a principled approach to pursue such integration. One important component of the proposed approach is played by the definition of a specific bi level loss (efficient bilevel boosted smoothing) that allows for convergence guarantees under mild assumptions. Both theoretical and experimental contributions are sound and convincing, justifying the claimed merits of the proposed approach. Another strong point is the fact that the proposed approach is general and amenable to support a broad family of propagation rules. One weakness with the original submission was presentation, mainly because some key information was confined into the supplementary material. The revised version addressed this problem and added some more empirical results that confirmed the superiority of the proposed approach. Finally, the fact that learning over tabular graph data is very important in Industry, the proposed approach may be of interest for a wide audience.
The submission proposes a model to handle uncertainties in an irregularly sampling time series setting (HetVAE), built on the VAE framework and the previous work on mTAN (multi time attention networks), and introduces components to encode sparsity information and heteroscedastic output uncertainty. The paper is clear, well motivated, and contains extensive ablation studies showing the effect of eaach added components. I recommend this submission for acceptance.
The paper proposes an efficient attention variant inspired by quadtrees, for use in vision transformers. When applied to several vision tasks, the approach leads to better results and/or less compute.   The reviews are all positive about the paper, after taking into account the authors  feedback (one reviewer forgot to update their official rating, apparently). They point out that the idea is reasonable and the empirical evaluation is thorough and convincing, with good gains on several tasks and datasets.  Overall, I recommend acceptance.
The paper makes two contributions: (1) Multi task benchmarks where the pareto solution is known analytically; and (2) a verification method for testing if solutions are on the pareto front. The authors make the point that MTL methods are applied to large scale problems, but fail to find the pareto front in problems where it is known.   Reviewers appreciated the discussion and insights by the approach, and the idea that correctness of scalable methods should be evaluated with problems that have analytic solutions, but they also had grave concerns. The primary concern is that without an efficient search, a verification method that builds on filters randomly generated solutions cannot scale to high dimensional problems. There were also disagreement about the role of LS and comparison with previous literature.   As a result, the contribution of the submission is not sufficient for acceptance to ICLR
All reviewers recommend accepting this paper, and this AC agrees. 
The paper proposes a simple method for uniform sampling from generative manifold using change of variables formula. The method works by first sampling a much larger number of samples (N) from uniform distribution in the latent space and then does sampling by replacement (using probability proportional to change in volume) to generate a smaller number of final samples (k << N) that are seen as approximately sampled from a uniform distribution from the generative manifold.   Reviewers had some questions/concerns about the confusing language in the abstract and introduction around the use of the term "uniform" which the authors have addressed satisfactorily. Authors have also provided results on quality (FID metric) of the generated samples as asked by the reviewers.   While the proposed method is rather simple, has high computational cost, and novelty is marginal (as noted by two of the reviewers), reviewers agree it is above the acceptance bar.
This paper proposes a new network architecture that implements higher order multivariate polynomials (MVP). They show that MVP generalizes well to different types of conditional variables, and can be applied to a broad range of tasks.   However, unifying discrete and continuous conditions and network without activation function are both well studied in literatures. The inappropriate discussions on the prior works and the advantages of the proposed method over prior approaches are not clearly justified.  Although outperforming SOTA is not necessary, the compared methods need to be well chosen which can provide convincing evidence on why MVP is needed under common settings.
The paper presents an approach to neural analysis of programs whose main feature is that it operates on assembly code, therefore can account for issues that depend on things like compiler settings. The other important claim of the paper is that by combining information from the control flow and data flow graphs extracted from the assembly code, they are able to produce an embedding that can support a variety of tasks.   The meta reviewer agrees with the reviewers that this paper is not suitable for acceptance at ICLR. The novelty in the approach is quite limited, and the gap between the extremely bold claims of the introduction of the paper and what is actually proven in the experiments is quite significant. The evaluation is not strong enough to be considered state of the art.
Reviewers put this paper in the lower half and question the theoretical motivation and the experimental design. On the other hand, this seems like an alternative general framework for solving large scale multi task learning problems. In the future, I would encourage the authors to evaluate on multi task benchmarks such as SuperGLUE, decaNLP and C4. Note: It seems there s more similarities with Ruder et al. (2019) [0] than the paper suggests.   [0] https://arxiv.org/abs/1705.08142
This paper was reviewed by four experts in the field. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references.
This paper proposes a novel model based Bayesian meta learning approach that combines a novel conditional dropout posterior a new variational prior for the data efficient learning and adaptation of deep neural networks. It is applied to tasks such as 1D stochastic regression, image inpainting, and classification.  Overall, this paper received positive reviews: Reviewers thought that the "the paper makes a nice connection between meta learning and variational dropout, resulting in an overall elegant approach" and that the "reasoning of the novel prior is clear and understandable" while the "experiments are comprehensive".  Given the agreement of the reviewers and the novel use of dropout for adaptation of a model to different tasks, I recommend accepting this paper.
The paper discusses propagating input uncertainty through non linear layers by a simple local linearization approach. This is a straightforward idea and the authors explain how this is an optimal approximation of the propagated distribution for Total Variation and reLU non linearity (for a single layer). This is an interesting (if quite limited) theoretical result. What is not clearly stated is that this result only holds for a single layer. It does not mean that the local approach is the best way to approximate (in the total variation sense) a distribution passed through multiple reLU layers.  By repeating the procedure, the authors are able to define closed form objective functions for noise robust training of deep networks.  The reviewers found this an interesting paper and there was a good effort by the authors to improve the results. However, technical innovation is modest and reviewer doubts still remain. For that reason, clarity of presentation is critically important. The overall numerical score isn t convincing and with one reviewer remaining very unconvinced.  I agree with the reviewers that the technical contribution is quite limited and I would argue is not particularly well explained. For example, a simple alternative would be to use a "global" linearization in which one can consider the network function $f$ as a whole, and then simply linearize this (rather than linearizing each layer). Indeed, the way that the paper is written, this would be a natural interpretation since $f$ is defined in the introduction as the network function, but is used later differently (eg section 3.2) as the transfer function. The approach is to recursively compute a new mean and covariance for each layer, propagating these through the network (similar to moment matching approaches). It would have helped if the authors had made pseudocode for their approach. It would be natural and interesting to compare to the simple global linearization approach (which is computationally faster).  The presentation of results and experiments could be improved. For example, in figure 3, it is not clear (nor is it explained in the text) what the definition of "robust accuracy" is.    Given the modest technical innovation, I also feel that the clarity of presentation isn t yet at the level that would merit acceptance. The paper would also benefit from some deeper insight into why the approach might perform better than other approaches (such as local moment matching) at the network level (rather than a single layer).
This paper made a solid contribution studying the convergence rate of a simple distributed gradient clipping algorithm. The proposed algorithm simply clips the gradients on each local machine and then do simple distributed update of the parameters.   The result, if correct, is quite strong and significant: The proposed algorithm is simple, and shows some benefit comparing to previously proposed algorithms   The strongest part of the paper is that it comes with a convergence rate bound (which is typically hard to prove for gradient clipping methods).   However, during the rebuttal period it was discovered that a number of places in the proofs are not well supported, the paper has to go through major revision in order to meet the publication standard.
This was a somewhat unusual submission in that the authors tried to motivate their paper by pointing to a separate anonymous manuscript.  However, the authors didn t seem to want to confirm they would merge the manuscripts when asked about this. It was thought that in fairness the submitted manuscript should be judged on its own. After discussion, it was agreed that the submitted paper on its own, did not generate enough enthusiasm to merit acceptance.
This paper aims to address transfer learning by importance weighted ERM that estimates a density ratio from the given sample and some auxiliary information on the population. Several learning bounds were proven to promote the use of importance weighted ERM.  Reviewers and AC feel that the novelty of this paper is modest given the rich relevant literature and the practical use of this paper may be limited. The discussion with related theoretical work such as generalization bound of PU learning can be expanded significantly. The presentation can be largely improved, especially in the experiment part. The rebuttal is somewhat subjective and unconvincing to address the concerns.  Hence I recommend rejection.
This paper proposes a methodology for learning a representation given multiple demonstrations, by optimizing the representation as well as the learned policy parameters. The paper includes some theoretical results showing that this is a sensible thing to do, and an empirical evaluation.  Post discussion, the reviewers (and me!) agreed that this is an interesting approach that has a lot of promise. But there was still concern about he empirical evaluation and the writing. Hence I am recommending rejection.
This paper presents a self training idea for GCN models to help improve the node classification. The reviewers agreed that the technical contribution of the proposed approach is limited and the performance improvement seems marginal. 
This paper explores the robustness of one class classifiers to geometric transformations at test time. The authors observe that some existing methods fail to detect novel images from the same class when they have undergone specific transformations at test time i.e. in plane rotations. In contrast, it is suggested that humans have no difficulty in ignoring the impact of these types of transformations. To address this issue, the authors propose to take the maximum prediction over the set of rotated versions of a given test image.   The current consensus from reviewers, and this meta reviewer agrees with this view, is that the paper, while not without some merit, is too narrow in focus to be of general interest in its current form. The main contribution is limited to one family of transformations, and it is not immediately clear how to generalize this to others when the entire transformation space is not easily enumerated. There are also legitimate concerns regarding if the specific issue outlined is likely to be a problem in practice (see R1 s comments). The authors allude to some interesting negative results related to data augmentation in their response to R4 (R2 also had questions about this). The authors should consider adding these results to a future revision of the paper as it will strengthen the central message.  In conclusion given the limited support, this AC also agrees that the paper is not yet ready for publication at ICLR.  
This paper tackles an important problem of generating a synthetic data for stock market agents behavior. In particular, GAN is trained to distinguish stock market time series from synthetic ones. Then market agent parameters can be scored by running time series generated using these parameters through a GAN. This way one can optimize the parameters to find realistic ones.  The reviewers appreciate authors response, however, they found that their concerns has not been fully addressed:  1. Clarity. The reviewers believe that the presentation need to be significantly improved and clarified before the paper is ready for acceptance (see reviewers comments for details). 2. The metrics used to evaluate the performance are not sufficient. While "stylized facts" statistics can be a good sanity check it does not mean that the agents that have been produced are useful in any way. Presumably, the goal of calibration such agents is to use them in simulation to generate synthetic trading for the purpose of building a predictive model. If so then it seems that the best way to judge the success of the calibration by the success of downstream task.  Given this, the paper can not be accepted for the presentation in its current form.
This paper proposes a promising solution to a very interesting and challenging problem, and the authors have improved the paper during the rebuttal by adding an important missing baseline. However, all reviewers still agree that the paper currently lacks sufficient analysis that would be required to understand properly the implications of past history on the regret. More specifically, the fact that assumption A2 does not apply to the given problem raises questions that should be addressed before publication. Theoretical analysis was provided for previous similar work (e.g. NeuralUCB). Providing this for the proposed method would significantly improve the impact of this work.
The authors propose a hierarchical attention layer which combines intermediate layers of multi level attention. While this is a simple idea, and the authors show some improvements over the baselines, the authors raised a number of concerns about the validity of the chosen baselines, and the lack of more detailed evaluations on additional tasks and analysis of the results. Given the incremental nature of the work, and the significant concerns raised by the reviewers, the AC is recommending that this paper be rejected.
All reviewers agree that this paper is not ready for publication.
pros: * novel explanation: skip connections < > singualrities * thorough analysis * significant topic in understanding deep nets  cons: * more rigorous theoretical analysis would be better  overall, the committee feels this paper would be interesting to have at ICLR. 
This submission proposes a new loss function for facial attribute GAN editing and transfer via text inputs.  A latent mapping mechanism based on StyleCLIP is used to disentangle the semantic attributes of human face.  The resulting semantic directional decomposition network (SDD Net) transfers attributes from reference image to a target guided with text descriptions. Experiments show on CelebA HQ dataset some qualitative results and ablations  for the « smile » attribute.  The main contribution is essentially a loss term that measures latent similarity in CLIP latent space. Most of the reviewers are not convinced by the approach and have raised several issues. One can question the relevancy of the way that text features are used (as a semantic direction). The role of the reference image in attribute transfer is is also questionable in the proposed framework. Additionally, evaluation is not sufficient, in particular to investigate whether the proposed method works on a wide range of attributes. The paper only conducts experiments on CelebA HQ dataset. It would be interesting to have experiments on other  datasets. The comparison to StyleCLIP is also insufficient, and there are no quantitative experiments to support the authors  claims.  We encourage the authors to take into account all these remarks and Rs  comments in order to get an improved proposition for a future conference.
The paper is proposing a multi task learning approach extending existing weighting approaches. An important and novel contribution of the paper is separating the magnitude and direction information in gradient based information. The joint gradient direction is searched by using angle bisectors of task gradients and magnitude is searched by simply finding a scaling which results in uniform loss scales. This approach solves issues like small gradient norm bias of MGDA, etc. The proposed method works well and authors show that it is conceptually relevant to most of the existing algorithms. These conceptual unification is a strong contribution of the paper. The paper is reviewed by three reviewers and received both accept and reject scores. Specifically,    R#2: Championed the paper and argued for its acceptance   R#3: Argues that the novelty is limited and SOTA claim is problematic.   R#4: Argues that the gap between the empirical performance of the proposed method and existing algorithms is small.   Arguments on the empirical performance and the SOTA are irrelevant to the decision since ICLR does not require algorithms to be SOTA or performed significantly better. Hence, the remaining issues are: claim of the SOTA being true or not, and lack of novelty. I read the paper in detail and decided to accept it with the following comments about the reviews:   The paper is clearly novel. Direction and magnitude are first time treated separately. Moreover, resulting unification of the existing approaches and theoretical derivations of the important connections of existing methods are also significant.   The SOTA claim of the paper is technically correct but little misleading. I would recommend authors to simply rephrase it "proposed method outperforms existing methods loss weighting methods under the same experimental settings". The reason for this is the fact that; in principle, "art" includes every possible solution for that problem. Hence, claiming SOTA in a fair and limited evaluation is rather misleading.  In addition to the reviewer comments, here are additional issues which should be addressed by the camera ready deadline:   I think the discussion about MGDA is a bit problematic since removing $\alpha \geq 0$ assumption simply removes the Pareto stationarity guarantee of the method. The resulting direction can increase some loss function and this disagrees with the main point of the Pareto optimality. Hence, I would recommend authors to clarify this while making the connection. Frank Wolfe algorithm is also not really inefficient and unstable since the problem is quadratic with linear constraints and the stability as well as extremely quick convergence can trivially be proved.   In addition to the previous point, the proposed method can actually increase some loss functions as there is no consistency constraint enforced. This is an interesting observation and empirical results suggest that increasing loss of some objectives might actually be valuable. I think this observation deserves some discussion even in the introduction.  
All four reviewers unanimously recommended for an acceptance (four 7s). They generally appreciated that the proposed idea is novel and experiments are convincing. I think the paper tackles an important problem of evaluating GANs, and the idea of using self supervised representations, as opposed to the conventional ImageNet based representations, would lead to interesting discussions and follow ups. 
This paper is about training a discrete policy that maps an image representation through a differentiable time dependent path planning module. The method is based on [1] and the reviewers are concerned about lack of novelty with respect to this work, and also with [2], however the latter only appeared a few weeks before the ICLR deadline, so I am not factoring it in my recommendation. Unfortunately, in light of [1], 3/4 reviewers do not recommend acceptance, and I agree with them.  [1] Vlastelica, Paulus, Musil, Martius, Rolinek. Differentiation of Blackbox Combinatorial Solvers (2020).  [2] Yonetani, Taniai, Barekatain, Nishimura, Kanezaki. Path Planning using Neural A* Search (2020).
This paper has potential impact in the theorem proving community, and demonstrated the possibility of using LMs for theorem proving in Lean, and is good enough to use "in the real world" through an interactive theorem proving tool.  The reviewers wish their data/models were public to address some concerns raised by the reviewers, but we think the community can benefit from this work.
Dear authors,  The reviewers pointed out a number of concerns about this work. It is thus not ready for publication. Should you decide to resubmit it to another venue, please address these concerns.
Realizing the fact that cross entropy loss and focal loss are widely used for training deep learning models but mathematical understanding and exploration for such losses are lacking, the authors propose a simple framework named PolyLoss to express the loss function as a linear combination of polynomial functions.   In this framework, the aforementioned cross entropy loss and focal loss are the special cases of PolyLoss by easily adjusting the importance of different polynomial bases depending on the targeting tasks and datasets. The final version of PolyLoss, Poly 1 formulation, is simple with one line of code and an extra hyperparameter but outperforms the cross entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.  This paper is well motivated by a novel perspective of polynomial expansion. The proposed method is novel, simple to implement, and effective in practice. The authors have a deep and thorough discussion with reviewers, and most concerns were well addressed. After rebuttal and discussion, reviewers increased their scores, and all agreed with acceptance. AC checked the paper and all relevant information, and found sufficient ground for acceptance.
This paper proposes a method for improving robustness to black box adversarial attacks by replacing the cross entropy layer with an output vector encoding scheme. The paper is well written, and the approach appears to be novel. However, Reviewer 4 raises very relevant concerns regarding the experimental evaluation of the method, including (a) lack of robustness without AT in the whitebox case (which is very relevant as we still lack good understanding of blackbox vs whitebox robustness) (b) comparison with Kannan et al and (c) lack of some common strong attacks. Reviewer 1 echoes many of these concerns.
The paper introduces new methods and building blocks to improve hyperbolic neural networks, including a tighter parameterization of fully connected layers, convolution, and concatenate/split operations to define a version of hyperbolic multi head attention. The paper is well written and relevant to the ICLR community. The proposed methods offer solid improvements over previous approaches in various aspects of constructing hyperbolic neural networks and also extends their applicability. As such, the paper provides valuable contributions to advance research in learning non Euclidean representations and HNNs. All reviewers and the AC support acceptance for the paper s contributions. Please consider revising your paper to take feedback from reviewers after reubttal into account.
This paper studies efficient algorithms for distributional reinforcement learning. The motivation stems from the need of risk neutrality, since other existing approaches might have one sided risk tendencies. The algorithms proposed in this paper are based on sampling from a distributional perturbation rather than using optimism in the face of uncertainty. Both theoretical guarantees and empirical results have been provided to validate the effectiveness of the proposed algorithms. While this is certainly an important and interesting direction, I agree with the reviewers that it is unclear from the theory in this paper why distributional perturbation is helpful.
Although paper has been improved with new quantitative results and additional clarity, the reviewers agree though that larger scale experiments would better highlight the utility of the method. There are some concerns with computational cost, despite the fact that the two networks are trained asynchronously. A baseline against a single, asynchronously trained network (multiple GPUs) would help strengthen this point. Some reviewers expressed concerns with novelty.
All reviewers have acknowledged that the proposed regularization is novel and also results in some empirical improvements on the reported language modeling and image classification tasks. However there are serious concerns on writing and rigor (reviewers Anon1 and Anon3) of the paper. The authors have not uploaded any revision of the paper to address these concerns.
This is a paper introducing a hierarchical RL method which incorporates the learning of a latent space, which enables the sharing of learned skills.  The reviewers unanimously rate this as a good paper. They suggest that it can be further improved by demonstrating the effectiveness through more experiments, especially since this is a rather generic framework. To some extent, the authors have addressed this concern in the rebuttal. 
The paper introduces a new pooling approach "Laplacian pooling" for graph neural networks and applies this to molecular graphs. While the paper has been substantially improved from its original form, there are still various concerns regarding performance and interpretability that remain unanswered. In its current form the paper is not ready for acceptance to ICLR 2020.
This paper combines DQN and Randomized value functions for exploration.   All the reviewers agreed the paper is not yet ready for publication. The experiments lack appropriate baselines and thus it is unclear how this new approach improves exploration in Deep RL. The reviewers also found some of the algorithmic design decisions unintuitive and unexplained. The authors main response was the objective was to improve and compare against vanilla DQN. This could be a valid goal, but it requires clear motivation (perhaps the focus is on simply algorithms that are commonly used in applications or something). Even then comparisons with other methods would be of interest to quantify how much the base algorithm is improved, and to justify empirically all the design decisions that went into building such an improvement (performance vs complexity of implementation etc).  The reviewers gave nice suggestions for improvements.  This is a good area of study: keep going!
This paper proposes a model architecture and training procedure for multiple nested label sets of varying granularities and shows improvements in efficiency over simple baselines in the number of fine grained training labels needed to reach a given level of performance.  Reviewers did not raise any serious concerns about the method that was presented, but they were also not convinced that it represented a sufficiently novel or impactful contribution to an open problem. Without any reviewer advocating for the paper, even after discussion, I have no choice but to recommend rejection.  I m open to the possibility that there is substantial technical value here, but I think this work would be well served by more extensive comparisons and a potentially revamped motivation to try to make the case for it that value more directly.
this submission presents the positive impact of using orthogonal random features instead of unstructured random features for predictive state recurrent neural nets. there s been some sentiment by the reviewers that the contribution is rather limited, but after further discussion with another AC and PC s, we have concluded that it may be limited but a solid follow up on the previous work on predictive state RNN. 
Reviewers mostly recommended to reject after engaging with the authors, with one reviewer slightly suggesting to accept, but with confidence 1. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit.
The paper proposes a method for architecture search using network morphisms, which allows for faster search without retraining candidate models. The results on CIFAR are worse than the state of the art, but reasonably competitive, and achieved using limited computation resources. It would have been interesting to see how the method would perform on large datasets (ImageNet) and/or other tasks and search spaces. I would encourage the authors to extend the paper with further experimental evaluation.
This paper presents a method for unrolling the iterative expectation maximizing steps in the EM algorithm for a Gaussian mixture model into layers in a neural network. Then, the proposed method is applied in the latent space of an autoencoder to allow deep clustering using autoencoder features. The reviewers raised concerns regarding the novelty, lack of systematic ablation experiments, and the efficacy of mimicking iterative optimization steps. Moreover, the training objective does not *exactly* correspond to the variational lower bound that EM often follows.   The additional experimental results in the revised version address some of the concerns regarding the empirical studies. However, the relationship between the proposed method and incremental EM algorithms and top down generative models is not properly discussed. Moreover, backpropagating through unrolled iterations does not seem elegant compared to principled approaches such as variational/amortized inference that are commonly used. For example, increasing the number of unrolled steps is observed to hurt the performance due to "vanishing gradients" whereas in the EM algorithm increasing the quality of optimization in the M steps should provide a better fit to training data.   Given the current presentation, I believe that the paper is not ready to be presented at ICLR. However, I encourage the authors to take the review feedback into consideration to improve the paper.
The paper proposes to build word representation based on a histogram over context word vectors, allowing them to measure distances between words in terms of optimal transport between these histograms. An empirical analysis shows that the proposed approach is competitive with others on semantic textual similarity and hypernym detection tasks. While the idea is definitely interesting, the paper would be streghten by a more extensive empirical analysis. 
All the reviewers agree that the paper is studying an important problem and makes a good first step towards understanding learning in GANs. But the reviewers are concerned that the setup is too simplistic and not relevant in practical settings. I recommend the authors to carefully go through reviews and to present it at the workshop track. This will hopefully foster further discussions and lead to results in more practically relevant settings.
All the reviewers agree that the paper has an interesting idea on regularizing the spectral norm of the weight matrices in GANs, and a generalization bound has been shown. The empirical result shows that indeed regularization improves the performance of the GANs. Based on these the AC suggested acceptance. 
This paper proposes an effective method to train neural networks with quantized reduced precision. It s fairly straight forward idea and achieved good results and solid empirical work. reviewers have a consensus on acceptance. 
This paper studies the Lottery Ticket hypothesis in reinforcement learning for identifying good sparse representations for low dimensional tasks. The paper received initial reviews tended towards acceptance. However, the reviewers had some clarification questions and concerns. The authors provided a thoughtful rebuttal. The paper was discussed and most reviewers updated their reviews in the post rebuttal phase. Reviewers generally agree that the paper should be accepted but still have good feedback. AC agrees with the reviewers and suggests acceptance. However, the authors are urged to look at reviewers  feedback and incorporate their comments in the camera ready.
The proposed method is an extension of Kim & Bengio (2016) s energy based GAN. The novel contributions are to approximate the entropy regularizer using a mutual information estimator, and to try to clean up the model samples using some Langevin steps. Experiments include mode dropping experiments on toy data, samples from the model on CelebA, and measures of inception score and FID.  The paper is well written, and the proposal seems sensible. But as various reviewers point out, the work is a fairly incremental extension of Kim and Bengio (2016). Most of the new elements, such as Langevin sampling and the gradient penalty, have also been well explored in the deep generative modeling literature. It s not clear there is a particular contribution here that really stands out.  The experimental evidence for improvement is also fairly limited. Generated samples, inception scores, and FID are pretty weak measures for generative models, though I m willing to go with them since they seem to be standard in the field. But even by these measures, there doesn t seem to be much improvement. I wouldn t expect SOTA results because of computational limitations, but the generated samples and quantitative evaluations seem worse than the WGAN GP, even though the proposed method includes the gradient penalty and hence should be able to at least match WGAN GP. The MCMC sampling doesn t appear to have helped, as far as I can tell.  Overall, the proposal seems promising, but I don t think this paper is ready for publication at ICLR. 
The paper studies the effect of importance weighting schemes on the implicit bias of gradient descent in deep learning models. It provides several theoretical results which give important insights on the effect of the importance weighting scheme on the limit of the convergence, as well as convergence rates. Results are presented for linear separators and deep learning models. A covariate shift setting is also studied. The theoretical results are supported with empirical demonstrations, and also lead to useful insights regarding which weighting schemes are expected to be more helpful. They also explain some previously observed empirical phenomena.    Pros:   New theoretical results which provide important insights on an important topic   Empirical demonstrations which support the theoretical results   Cons:   No significant issues.  
The paper has received 4 positive reviews all supporting the acceptance of the paper. The authors have provided a strong rebuttal and have addressed the reviewers  concerns. Please make sure to include all reviewer feedbacks in the camera ready version. 
This is an interesting paper that aims to redefine generalization based on the difference between the training error and the inference error (measured on the empirical sample set), rather than the test error. The authors propose to improve generalization in image classification by augmenting the input with encodings of the image using a source code, and learn this encoding using the compression distance, an approximation of the Kolmogorov complexity. They show that training in this fashion leads to performance that is more robust to corruption and adversarial perturbations that exist in the empirical sample set.   Reviewers agree on the importance of this topic and the novelty of the approach, but there continue to exist sharp disagreement in the ratings. Most have concerns about the formalism and clarity in the presentation. Especially given that the paper is 10 pages, it should be evaluated against a more rigorous standard, which doesn t appear to be met. I encourage the authors to consider a rewrite with a goal towards clarity for a more general ML audience and resubmit for a future conference. 
The submission considers a method involving adversarial training to speed up the fine tuning of large pre trained transformer language models. Reviewers consider it to be a borderline paper.  Many suggestions are made by the reviewers which will help improve the presentation and substance and make it more useful for the community.
 This paper tackles the task of translating informal LaTeX math into a formal representation annotated with abstract concepts (sTeX / SMGloM).  The authors build a synthetic training data generation mechanism, and construct an evaluation dataset by hand. The problem is tackled as machine translation, and vanilla systems fail, while GPT 2 pretrained on LaTeX documents performs well. The reviewers recognize the importance of this work, in an area where data is not plentiful and benchmarking is difficult.  The authors do a good job in presenting a difficult topic rather clearly, but I would encourage the authors to continue improving the presentation, possibly with clearer examples or figures.  The particular "copying bias" useful in this task, pointed out by a reviewer, is indeed interesting and I encourage the authors to consider that discussion and the thoughtful reviews deeply. Overall, this is a significant contribution to the field and I recommend acceptance.
The paper proposes a method for the interesting task of dialog summarisation which is slowly getting attention from the research community. In particular, they propose a method which first generates a summary draft and then a final draft.  Pros: 1) The paper is well written 2) Addresses an interesting problem 3) SOTA results  Cons: 1) Lack of novelty 2) No quantitative analysis of the summary draft though it is as an important part of the proposed solution 3) Human evaluations are not adequate (the authors have said they will expand on this but clear details are not provided) 4) The BART model seems to have some advantage as it is pre trained on XSUM data whereas some of the other models are not (the authors haven t clarified this sufficiently in the rebuttal)  Overall, the reviewers were not completely happy with the work and there was not clear champion. 
The manuscript proposes deterministic approximations for Bayesian neural networks as an alternative to the standard Monte Carlo approach. The results suggest that the deterministic approximation can be more accurate than previous methods. Some explicit contributions include efficient moment estimates and empirical Bayes procedures.   The reviewers and ACs note weakness in the breadth and complexity of models evaluated, particularly with regards to ablation studies. This issue seems to have been addressed to the reviewer s satisfaction by the rebuttal. The updated manuscript also improves references to related prior work.  Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. We recommend acceptance.
The authors propose a technique for quantizing neural networks, which consist of repeated quantization/de quantization operations during training, and the second step learns scale factors. The method is simple, clearly presented, and requires no change in the training procedure. However, the authors noted that the work is somewhat incremental, and is similar to previously proposed approaches. As noted by the reviewers, the AC agrees that the work would be significantly strengthened by additional analysis of complexity in terms of computational time and memory relative to the other techniques.  
The paper proposes an edge independent graph generative model that can capture heterophily. The authors propose a 3 stage process to obtain the node representations. The idea of factorization in the form of BB^T CC^T is an interesting approach to model heterophily.  The paper can be improved in terms of writing to better motivate the need for a 3 stage algorithm and how these individual steps are related to the existing techniques in the literature. The authors should elaborate on the implications of the theorems and the concerns raised by the reviewers in the body of the paper.  The algorithm faces scalability challenges, which are not studied well in the experiments. The reviewers also have raised concerns about degeneracy in network reconstruction experiments. Overall, the paper needs further improvements for publication.
This paper explores the use of recurrent neural networks to model neural activity time series data. The hope is that computationally demanding biophysical models of neural circuits could be replaced by RNNs when the goal is simply to capture the right input output functions. The authors show that they can fit RNNs to the behaviour of a complex, biophysical model of the C elegans nervous system, and they explore the space of hyperparameter and network choices that lead to the best fits.  The reviews for this paper were borderline, with scores of 3, 6, and 8. On the positive side, the reviewers agreed that the paper is very effective in demonstrating that the input output behaviour of the biophysical model of C elegans can be replicated by RNNs. But, on the negative side there were concerns about the limited nature of the empirical results, lack of details about the simulation, too much emphasis in describing well known RNN architectures, and lack of systematic strategy for applying this technique in other systems. The rebuttals did not change the borderline scores.  Thus, this is an instance where the AC must be a bit more involved in the decision. After reading the paper and reviews, the AC felt that this work was not sufficiently general in its application. Ultimately, using artificial neural networks to fit neural data is common practice nowadays, so really, this paper serves as a proof of concept for replacing a complex biophysical model with a simpler RNN. But, given that RNNs are quite good at modelling sequence data, it s not terribly surprising that this works. Moreover, though the authors do a very careful search over network design decisions, they don t provide a systematic strategy for others to employ if they so wished. Also, the authors do not provide much insight into what the RNNs learn that might help us to better understand the modelled neural circuits. And most importantly, this only demonstrates the effectiveness for systems where we have biophysical models with well established accuracy, which is not the case for most neural circuits. Given these considerations, a reject decision was reached.
This paper analysis the convergence properties of a family of  Adam Type  optimization algorithms, such as Adam, Amsgrad and AdaGrad, in the non convex setting. The paper provides of the first comprehensive analyses of such algorithms in the non convex setting. In addition, the results can help practitioners with monitoring convergence in experiments. Since Adam is a widely used method, the results have a potentially large impact.  The reviewers agree that the paper is well written, provides interesting new insights, and that is results are of sufficient interest to the ICLR community to be worthy of publication.
# Quality: The experimental evaluation is thorough and well designed.   # Clarity: After rebuttal, the paper is well written and clear in its contributions.  # Originality: The proposed approach builds over existing literature, as clearly indicated in the manuscript and acknowledged by the authors in the rebuttal, by mixing existing contributions in a novel fashion.  # Significance of this work:  This work deal with an important and timely topic. The experimental results are convincing and demonstrate strong performance for the proposed approach.  # Overall: This manuscript provides an incremental but solid contribution to the topic of model based reinforcement learning.  # Personal comments:   I disagree with Reviewer2, in that I feel that after the rebuttal the manuscript clearly and thoroughly states prior works and the novelty of the proposed approach. This is not a survey paper and should be hold to realistic standards.
The submission is acknowledged as having potential value in terms of proposing a new approach for exploration based on ensembles and value functions. However, there are lingering concerns about the discussion of what this paper brings to the table vis a vis prior work, together with a lack of clear demonstration of the explicit gains from the exploration mechanism and more experimental studies. The author(s) would do well to revise as per the feedback given and resubmit a version with a more compelling argument. 
This paper introduces Back2Future, a deep learning approach for refining predictions when  backfill dynamics are present.    All reviewers agree on that the authors successfully motivate their work and  introduce a topic of great interest, i.e. that of dealing with the effect of revising previously recorded data and its effect  timeseries predictions. The reviewers also underline the strong and thorough experimental section. Among the reviews is also underlined the potential impact of the work for the research domain.   Many thanks to the authors for replying to the minor concerns raised.  I concur with the reviews and find this submission very interesting, convincing and thus  recommend for accept.   Thank you for submitting the paper to ICLR.
The key concern from the reviewers that was not addressed is that none of the experimental results illustrate convergence vs. time instead of convergence vs. number of iterations.  While the authors point out that their method is O(ND) instead of O(KND), the reviewers really wanted to see graphs demonstrating this, given that the implicit SGD method requires an iterative solver. The revised paper is otherwise much improved from the original submission, but falls a bit short of ICLR acceptance because of the lack of a measurement of convergence vs. time.  Pros: + Promising unbiased algorithms for optimizing the log likelihood of a model using a softmax without having to repeatedly compute the normalizing factor.  Cons:   The key concern from the reviewers that was not addressed is that none of the experimental results illustrate convergence vs. time instead of convergence vs. number of iterations. 
This paper received 4 reviews with mixed initial ratings: 6,7,7,5. The main concerns of R1, who gave an unfavorable score, included lack of clarity (the manuscript is hard to follow) and limited empirical evaluation (the method is tested on a single synthetic dataset, CLEVRER). The latter point is echoed in other reviews as well. In response to that, the authors submitted a new revision and provided detailed responses to each of the reviews separately, which seemed to have addressed these concerns. R1 upgraded the rating and recommended acceptance. As a result, the final recommendation is to accept this submission for presentation at ICLR as a poster.
The paper presents an efficient approach to computer saliency measures by exploiting saliency map order equivalence (SMOE), and visualization of individual layer contribution by a layer ordered visualization of information.   The authors did a good job at addressing most issues raised in the reviews. In the end, two major concerns remained not fully addressed: one is the motivation of efficiency, and the other is how much better SMOE is compared with existing statistics. I think these two issue also determines how significance the work is.   After discussion, we agree that while the revised draft pans out to be a much more improved one, the work itself is nothing groundbreaking. Given many other excellent papers on related topics, the paper cannot make the cut for ICLR. 
Paper studies if DNNs are modular and proposes statistical methods to quantify modularity. cluster the neurons of the network using spectral clustering applied to a graph that is weighted by similarity between the neurons.  While the reviewers find the question of modularity relevant, they raise the issue that the results are inconclusive regarding the main stated contribution of the paper (i.e., if modularity is appropriately measured). After discussion, some concerns are answered. However, the main problem of inconclusive results stands. Therefore, this borderline paper is rejected.
The manuscript proposes a causal interpretation of the self supervised representation learning problem. The data is modeled as being generated from two independent latent factors: style and content, where content captures all information necessary for downstream tasks, and style captures everything that is affected by data augmentations (e.g. rotation, grayscaling, translation, cropping). The main contribution is a specific regularizer for self supervised contrastive learning, motivated by the assumptions about the data generation.   Reviewers agreed that the manuscript is oversold on the causal jargon, as was noted, the manuscript does not perform any causal inference. Nevertheless, they think that there is an interesting interpretation of self supervised learning and the results are noteworthy. 
PROS: 1. good results; the authors made it work 2. paper is largely well written  CONS: 1. some found the writing to be unclear and sloppy in places 2. the algorithm is complicated   a chain of sub algorithms  A few small points:   I initially found Algorithm 1 to be confusing because it wasn t clear whether it was intended to be invoked for each task (making the training depend on all the datasets).  I finally convinced myself that this was not the intention and that the inner loop of the algorithm is what is actually executed incrementally.  
This paper discusses new methods to perform adversarial attacks on salience maps.  In its current form, this paper in its current form has unfortunately has not convinced several of the reviewers/commenters of the motivation behind proposing such a method. I tend to share the same opinion. I would encourage the authors to re think the motivation of the work, and if there are indeed solid use cases to express them explicitly in the next version of the paper.
The authors propose to linearly combine the utility functions of (batch) active learning algorithms. The linear combination coefficients are "learned" with Monte Carlo estimators to adapt the coefficients to different kinds of tasks automatically.  The reviewers find the presentation within the papers generally clear. The simplicity of the approach, which is highlighted in the authors  rebuttal, should be appreciated. The authors also addressed the issue of robustness with respect to the batch size. But the paper left quite a few unanswered issues even after the authors rebuttal. The novelty with respect to several earlier papers require clarification and concrete comparisons, such as the ones in reinforcement learning and bandit learning as pointed out by reviewers. The lack of comparisons to those existing works, both illustratively and empirically, is a key weakness of the current paper. A more careful study of RL setting (such as reward shaping) is also important to understand the value of the work. Finally, the gap between the ensemble approach and the single approach also deserves more investigation to justify the significance of the contribution. 
This article proposes a novel uncertainty quantification method, formulating the problem as a Bayesian inference problem. Instead of training multiple ensemble models through MAP optimisation, as in ensemble methods, the proposed approach tries to learn a mapping function between the prior distribution and the posterior distribution of model parameters. This avoids the complex training of ensemble models and achieves better efficiency.   The approach is novel, and the problem of importance. The paper however suffers from a number of weaknesses: * Some theoretical results would need to be made mathematically more rigorous * The presentation is unclear and confusing in some places * Empirical results are not reproducible due to the lack of details Although the authors clarified some of the points raised by reviewers in their response, the paper in its current form is not ready for publication, and I recommend rejection.
In this paper, the authors propose a non parametric approach for learning a two layer neural net. I agree with the authors and reviewers that this is a timely problem. However, the solution in this paper come short of achieving this goal. In particular, the assumtions are very strong and cannot be generalized (e.g., non negativity). The authors also need to better spell out the sample complexity.
An interesting paper on combining NerFs with StyleGAN to get high quality and high resolution 3d aware generative models. The results are very good visually and also allow interactive speeds.  The technique is natural and concurrent papers are proposing variations  The reviewers identified a few limitations including that the nerf does not have a viewing direction and also seems limited to aligned objects with a common structure, like faces. Still the results are very interesting and suitable for publication.
This paper introduces a tree structured wavelet deep neural network to effectively extract more discriminative and expressive feature representations in time series signals.  Based on a frequency spectrum energy analysis,  the approach decomposes input signals into multiple subbands and builds a tree structure with data driven wavelet transforms the bases of which are learned using invertible neural networks. In the end, the scattering subband features are fused using a self attention like mechanism.  The effectiveness of the proposed approach is verified extensively on a variety of datasets from different domains including follow up experiments in the rebuttal.  Overall,  the work is technically novel and provides an interesting way of extracting adaptive finer grained features to deal with time series signals.  The authors  rebuttal is solid which has cleared most of the concerns raised by the reviewers with additional supportive experimental evidence.   I would recommend accept.
All three reviewers gave scores of Weak Reject. Only a brief rebuttal was offered, which did not change the scores. Thus the paper connect be accepted. 
This paper proposes a knowledge graph completion approach that represents relations as rotations in a complex space; an idea that the reviewers found quite interesting and novel. The authors provide analysis to show how this model can capture symmetry/assymmetry, inversions, and composition. The authors also introduce a separate contribution of self adversarial negative sampling, which, combined with complex rotational embeddings, obtains state of the art results on the benchmarks for this task.  The reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper.  The reviewers appreciated the author s comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self adversarial sampling, and comparisons to TorusE, (2) improved presentation.  With the revision, the reviewers agreed that this is a worthy paper to include in the conference. 
The paper is interesting, and its focus is timely and important, given the continuing rapid rise of transformers (and their dependence of tokenization of images). All three reviewers recommend acceptance, to varying degree. The paper will be a valuable contribution to the program at ICLR.
The authors propose to define  Expressiveness  in deep RL by the rank of a matrix comprising a number of feature vectors from propagating observations through the learnt representation, and show a correlation between higher rank and higher performance. They try 3 regularizers to increase rank and show that they improve the final score on Atari games compared to A3C or DQN. The AC and reviewers agree that the paper is interesting and novel and could have general significance for the RL field. Also, the authors were very responsive to the reviewers and added more details, plus several experiments and analyses to support their claims. However, the reviewers were concerned about a number of aspects and have recommended that the authors clean up their presentation and analysis a bit more. In particular, the fact that the regularization coefficient is tuned for each Atari game makes it very hard to compare to DQN/A3C which are very careful to keep the same hyperparameters across every game.
This is a clear reject. None of the reviewers supports publication of this work. The concerns of the reviewers are largely valid.
This paper considers a new model of input data specific for image classification problems. In particular, the high level idea is that each image contains certain patterns, and which patterns it contain decides its label. In this framework, under some stronger assumptions (e.g., patterns are orthogonal, one positive pattern and one negative pattern, PSI assumption, etc.) the authors showed that SGD on a 3 layer overparametrized convolutional network will be able to have a small sample complexity, while the VC dimension would be at least exponential in d. The paper also provided some empirical evidence on a modified MNIST dataset. While the idea seems to be an interesting first step, the reviewers find that the current version of theory still relies on fairly strong assumptions.
This work is incremental compared to previous work, solving very specific challenges, and would probably appeal to only a very limited fraction of ICLR s audience. 
This paper is an extension of Monotone Operator Equilibrium Networks (MON). It first tries to address a key issue in MON: whether the activation function $\sigma$ can be represented by a proximal operator of some function $f$. Then it derives the constraints on the weight $W$. Connections to neural ODEs and convex optimization are also built.   Pros: 1. Very nice theory, reads exciting, and provides great insights. 2. Experiments seem to validate part of the theoretical analysis.  Cons: Besides the issue of weak experiments raised by the reviewers (and the authors admitted "quite rudimentary empirical results"), the AC regretted that the proofs have some key flaws.  1. The proof of Proposition 1, which is the cornerstone of the paper, is wrong. Although the AC believed that Proposition 1 is likely to be true, the current proof of "only if" is unfortunately incorrect. Please notice the claim at the first line of the "only if" proof, which writes "$\sigma$ is a non decreasing and piece wise differentiable function". Although it is known in real analysis that monotone functions are almost everywhere differentiable, this does not necessarily mean that monotone functions can be piece wise differentiable functions as the non differentiable points can be dense, though of zero measure.  2. Proposition 7, which claims that LBEN parameterization (8) contains all feedforward networks, is wrong. From the proof on page 19, the AC doubted whether the lower diagonal blocks of $ 2H_D^{ 1}  H_L$ can be $\{W_i\}$. This is because by the definition of $H$ given at 5 lines below (40), $H$ must be positive definite. If the lower diagonal blocks of $ 2H_D^{ 1} H_L$ can be $\{W_i\}$, then $H_i \Lambda_i W_i$.  It is not apparent whether for any choice of $\{W_i\}$ there exist $\{\Lambda_i\} \in D^+$, such that $H$ is positive definite. Moreover, the lower diagonal block of $V^TV$, i.e., $\Gamma_{j+1}V_j\Phi_j$ (corrected from the typo in the minor issue 2 below), must equal to $\Lambda_j W_j$. However, it is obvious that for some $W_j$ (e.g., it has more columns than rows), there cannot exist $\Gamma_{j+1}$, $V_j$, $\Phi_j$ and $\Lambda_j$, such that $\Gamma_{j+1}V_j\Phi_j \Lambda_j W_j$, due to the structures of these matrices. Moreover, due to the existence of $(...)/(2\gamma)$ in the definition of $H$, the diagonal blocks of $H$ cannot be diagonal by the choice of $V$. 3. Proposition 7 again. The choice of parameters in (44) does not match the feedforward network in (15). Please observe that by (44) $z_1 \sigma(U_0 x + b_0)$, rather than $z_1 U_0 x + b_0$ in (15).  Minor issues: 1. The proof on page 19 is for claiming that (9), rather than (8), contains all feedforward networks. 2. In the choice of $V$, $\Psi_i V_i$ should be $V_i\Phi_i$.  Although the reviewers and the AC liked the paper, due to the above flaws and limited acceptance rate, the AC deemed that the flaws should be fixed prior to acceptance.
This paper studies the problem of embedding graphs into continuous spaces.  The authors focus on determining the correct dimension and curvature to minimize distortion or a threshold loss of the embedding. The authors  consider a variety of existing notions of curvature for graphs, introduce a notion of global curvature for the entire graph, and how to efficiently compute it.  Reviewers were positive about the problem under study, but agreed that the current manuscript somewhat lacks a clear contribution. They also pointed out that the goal of using a global notion of curvature should be better motivated. For these reasons, the AC recommends rejection at this time. 
The paper combines bi level optimization and reverse mode differentiation for flow estimation on networks. Most reviewers think the idea of incorporating physical constraints is interesting and novel, and the experiments convincing.
The paper received borderline negative reviews with scores of 5,5,6. A consistent issue was the weakness of the experiments: (i) lack of comparison to appropriate baselines, (ii) differences between published/reported numbers for DeepLab ResNet (R3) and (iii) related work, e.g. Wojna paper, as raised by R1. The AC did not find the author s responses to these issues convincing. For (ii) the gap between 73 and 79 is large and the author s explanation for the difference doesn t seem plausible. For (iii), the response promised comparisons/discussion but there were not added to the draft.  Given this, the paper cannot be accepted in it current form. The experiments should be improved before the paper is resubmitted. 
This paper proposes a novel way (Pani) that constructs image patch level graphs and then linearly interpolates the patch level features. The authors show how this can be used in Virtual Adversarial Training (PaniVAT) and Mixup/MixMatch (Pani Mixup). The method is shown to improve classification compared to standard VAT and related techniques on CIFAR 10 (low data setting), as well as outperform Mixup on CIFAR 10/CIFAR 100/TinyImageNet (standard setting, multiple different architectures) with and without data augmentation.  Reviewer 4 liked that the method was simple, but was not convinced of its effectiveness because of the baselines that were chosen. Specifically they thought that FixMatch was a stronger baseline than MixMatch. The authors said that Pani is complementary to FixMatch and similar improvement could be expected when applying Pani to FixMatch instead of MixMatch.  Reviewer 2 appreciated that the work was “important and interesting” and noted that the experiments showed that Pani improved existing algorithms. They were concerned with lack of motivation and lack of theoretical guarantees. The authors clarified motivation in their response to the reviewer but, understandably, were unable to provide any theoretical analysis.  Reviewer 1 expressed disappointment with the writing and understandability of the paper. I read the paper myself and I agree. They posed several clarifying questions to the authors, to which the authors responded. I note that the reviewer could not find the appendix, but it was attached separately as supplementary material.  Reviewer 3 wrote a very short review and stated that they are not familiar with the topic of the paper. With three other full reviews, I have discounted R3’s review because of their extremely low confidence. They also asked a couple of clarifying questions, to which the authors responded. I found the authors’ response satisfying.  Overall, two reviewers are not extremely excited about the paper and one reviewer thinks the work is interesting but has concerns about clarity. I think that overall it is a neat idea, but the paper could use more polishing and clarification. Compared to other borderline papers in my stack, it is not over the bar. It could get there with further work. I hope the authors continue to improve the paper and re submit it in the near future.
The reviewers raised a number of concerns which are addressed by the authors. The paper provides an interesting/novel perspective for federated learning (as a posterior inference problem rather than an optimization problem) which can potentially allow for faster and more accurate solutions. 
The paper proposes ATR CSPD, which learns a low dimensional representation of seasonal pattern, for detecting changes with clustering based approaches.   While ATR CSPD is simple and intuitive, it lacks novel contribution in methodology. It is unclear how it is different from existing approaches. The evaluation and the writing could be improved significantly.   In short, the paper is not ready for publication. We hope the reviews can help improve the paper for a strong submission in the future. 
The authors introduce an approach to train sparse RNNs with a fixed parameter count. During training, they allow RNN layers to have a non uniform redistribution across cell weights for a better regularization.They also introduce a variant of the averaged stochastic gradient optimizer, which improves the performance of all sparse training methods for RNNs. They achieve state of the art sparse training results on Penn Treebank and Wikitext 2.  The method achieves very good performance on sparse RNNs for challenging tasks. The paper is well written and provides solid analysis with new insights into sparse network models. Most reviewers believe it is a very solid paper.  However, the technical novelty of the paper is limited. It can be seen as some tweaks and improvements of existing techniques, which seem to work very well. Since the number of papers that can be accepted is very limited, and since technical novelty is an essential criterion for published papers at ICLR, I propose rejection.  
The paper considers the problem of estimating the electronic structure s ground state energy of a given atomic system by means of supervised machine learning, as a fast alternative to conventional explicit methods (DFT). For this purpose, it modifies the neural message passing architecture to account for further physical properties, and it extends the empirical validation to also include unstable molecules.   Reviewers acknowledged the valuable experimental setup of this work and the significance of the results in the application domain, but were generally skeptical about the novelty of the machine learning model under study. Ultimately, and given that the main focus of this conference is on Machine Learning methodology, this AC believes this work could be more suitable in a more specialized venue in computational/quantum chemistry. 
This paper studies a learning scenario in which there exist 2 classes of examples: "predictable" and "noise". Learning theory is provided for this setting and a novel algorithm is devised that identifies predictable examples and makes predictions at the same time. A more practical algorithm is devised as well. Results are supported by experiments.  Reviewers have raised a number of concerns (ranging from how realistic this settings is to missing references). Overall they found this work interesting and relevant to ML community and appreciate the effort that authors have put in in their thoughtful response. However, after a thorough deliberation conference program committee decided that the paper is not sufficiently strong in its current form to be accepted.
The submission receives mixed ratings initially. Three reviewers are on the borderline and one reviewer EG97 leans negatively. The raised issues mainly reside on the technical contribution, technical correctness, and experimental validation. In the rebuttal, the authors have tried to address the raised issues and discussed them in depth with reviewers. However, the discussion does not change the reviewer s mind. After checking all the reviews, rebuttals, and discussions. The AC stands for the reviewer side that the technical contribution is a major issue that ought to be solved. The proposed TPN comes from the summarization of the existing FPN based structure and there are not sufficient insights to make significant improvements. Besides, there are still unsolved issues regarding the technical presentation and experimental validations. The authors are suggested to improve the current manuscript based on these reviews and welcome to submit for the next venue.
This was a difficult decision to converge to. R2 strongly champions this work, R1 is strongly critical, and R3 did not participate in the discussions (or take a stand). On the one hand, the AC can sympathize with R1 s concerns   insights developed on synthetic datasets may fail to generalize and fundamentally, the burden is not on a reviewer to be able to provide to authors a realistic dataset for the paper to experiment on. Having said that, a carefully constructed synthetic dataset is often *exactly* what the community needs as the first step to studying a difficult problem. Moreover, it is better for a proceeding to include works that generate vigorous discussions than the routine bland incremental works that typically dominate. Welcome to ICLR19. 
The major concerns about this paper are that (1) There are too many hyper parameters, such as those needed for ADMM. I d point out that there are adaptive variants of ADMM and heuristics methods for choosing optimization hyper parameters, although it would be nice if the authors addressed these issues in the paper.  (2) Some reviewers are concerned that, compared to other related attacks, it’s unclear why flipping fewer bits is an important objective   an attacker might only care about poisoning performance and clean data performance.  The authors respond that flipping fewer bits makes the attack more effective when bits are manipulated by a physical method such as manipulating memory.  Despite these criticisms, reviewers agree that the paper is a well thought out approach that improves the state of the art by some metrics. 
This paper presents methods to scale learning of embedding models estimated using neural networks. The main idea is to work with Gram matrices whose sizes depend on the length of the embedding. Building upon existing works like SAG algorithm, the paper proposes two new stochastic methods for learning using stochastic estimates of Gram matrices.   Reviewers find the paper interesting and useful, although have given many suggestions to improve the presentation and experiments. For this reason, I recommend to accept this paper.  A small note: SAG algorithm was originally proposed in 2013. The paper only cites the 2017 version. Please include the 2013 version as well. 
This paper offers a new approach to cross modal embodied learning that aims to overcome limited vocabulary and other issues.  Reviews are mixed.  I concur with the two reviewers who say the work is interesting but the contribution is not sufficiently clear for acceptance at this time.
This work presents a routing algorithm for capsule networks, and demonstrates empirical evaluation on CIFAR 10 and CIFAR 100. The results outperform existing capsule networks and are at par with CNNs. Reviewers appreciated the novelty, introducing a new simpler routing mechanism, and achieving good performance on real world datasets. In particular, removing the squash function and experimenting with concurrent routing was highlighted as significant progress. There were some concerns (e.g. claiming novelty for inverted dot product attention) and clarification questions (e.g. same learning rate schedule for all models). The authors provided a response and revised the submission , which addresses most of these concerns. At the end, majority of reviewers recommended accept. Alongside with them, I acknowledge the novelty of using layer norm and parallel execution, and recommend accept. 
The paper presents a conformal prediction approach to supervised classification, with the goal of reducing the overconfidence of standard soft max learning techniques. The proposal is based on previously published methods, which are extended for use with deep learning predictors. Empirical evaluation suggests the proposal results in competitive performance. This work seems to be timely, and the topic is of interest to the community.  The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the strength of the empirical evidence supporting the claims. Additional experiments would significantly strengthen this submission.
Knowledgeable R3 found the paper very good (8). He/she found the authors  responses very informative and that edits made the paper much stronger. R2 expressed reservations about rank collapse being the cause of degradation of performance, but also indicated his/her willingness to increase the score if the authors can convincingly respond to his/her concerns. This concerned was shared by other reviewers, and there was an extensive discussion during the discussion period. R3 and R1 found the authors  responses very convincing. Fairly confident R1 found the paper good, appreciated the discussion, and recommends the paper to be accepted. R4 found the paper marginally above the acceptance threshold, however expressing a lower confidence in his/her assessment. In summary, the article contains extensive experiments, theory, and a well motivated idea, elucidating an intriguing phenomenon and useful for designing better bootstrapping based deep RL methods. Although the reviewers expressed some reservations in their initial reviews, there was a lively discussion with quite positive final feedback. Weighing the ratings by confidence and participation in the discussion, I am recommending the paper for acceptance. I would like to encourage the authors to make efforts in making the presentation as clear as possible, having in mind the discussion and comments from the reviewers. 
there is a disagreement among the reviewers, and i am siding with the two reviewers (r1 and r3) and agree with r3 that it is rather unconventional to pick learning to learn to experiment with modelling variable length sequences (it s not like there s no other task that has this characteristics, e.g., language modelling, translation, ...) 
The paper proposed a new VAE based generative model for generating molecular conformations from graphs. Reading the paper, the reviews and the rebuttal, it looks like this project is a work in progress and not yet ready for publication. Some reviewers indicate that the paper lacks significant technical novelty.   This is a worthy application. I encourage the authors to keep working and resubmit to another venue when they feel the work is ready and they have addressed all points raised by the reviewers.
This paper presents a representation method for time series data in the sequential VAE, where the global feature z and local features  s are better disentangled. The intuition behind learning z is to maximize the mutual information between z and input x, while minimizing the mutual information between z and s. The second mutual information is estimated with a discriminator in the DRT framework. Overall, the methodology can be seen as reasonable applications of the disentanglement principle to sequential data. The authors have shown that z and s learned in this way is better disentangled as compared to beta VAE. In the end, the reviewers feel that while there is good intuitions/technical ingredients, the derivations in Section 3 are not very smooth, and several approximations/choices are not very carefully justified (e.g., choice of alpha, choice of DRT vs. other MI estimators), and perhaps stronger baselines than beta VAE can be used.  The reviewers rate this paper to be borderline.
This paper proposes an “iterative” regularized dual averaging method to sparsify CNN weights during learning. The main contribution seems to be in an iterative procedure where the weights are pruned out greedily by observing the sparsity of the averaged gradients. The reviewers agree that the idea seems straightforward and novelty is limited. For this reason, I recommend to reject this paper. 
This paper proposes to learn symmetries of a physical system jointly with its Hamiltonian from data by learning a canonical transformation that render some of the coordinates constant. The Hamiltonian dynamics and "canonical" transformation are softly enforced via loss terms. A few experiments are performed demonstrating that the idea works and can learn a few approximate invariants, as well as some improvements over baselines agnostic to the symmetries. The idea is interesting, but the experiments are limited in scope. It is not clear how to extend this idea to more complex systems where we do not know the number of conserved quantities in advance. It is also not clear how good are the learned invariants, as the results showing errors in conserved quantities (Fig 3) suggest that it is not very precise beyond a few time steps. 
The submission introduces a model that does learning of multisensory representations (by predicting one from the other), with an autoencoder structure. Generally, the reviewers liked the overall idea of the work, but found the clarity lacking, the evaluation insufficient (and not particularly state of the art), the requirement for paired training data quite limiting and the choices (VAE sometimes, autoencoder other times) somewhat ad hoc.
All the reivewers find the similarity between this paper and the references in terms of the algorithm and the proof. The theoretical results may not better than the existing results.
After going over the reviews and the rebuttal, and skimming the paper, I feel like unfortunately this paper is not ready to be accepted.  My reasoning is as follows. I feel the comparison with A2C and PPO is not and should not be the main target of the work. Of course they are good to have as reference points, and they should be in the paper. But the work is not trying to claim that the distilled symbolic policy is more data efficient (or outperforms these methods). If that would be the point, that one has questions similar to reviewer cXsw about these baselines maybe underperforming (compared to other published work). Maybe this is due to a change in setup as argued by the rebuttal, nevertheless this makes comparison and understanding the results difficult. The other argument is that  A2C / PPO are not the most efficient DRL methods for atari.  Lastly, is the question of distilled symbolic policy having access to an expert, making this not an apples to apples comparison.  But as I said, and I think this is the point of the authors as well, this is not the point of the paper. But then I find the results not being sufficiently contextualized either by comparing to other methods in this space, or various ablation studies to motivate the choices taken by the authors. Similar points were raised by other reviewers (wezQ, cXsw). Some of these ablations have been brought forward in the rebuttal, but I think they should be a more central part of the work and implies considerable edits to the paper.  I think the stance that the object identification is decoupled from the symbolic policy is also a bit dangerous. I.e. a learned object identifier (particularly in a visual more complex setting) will have different failure modes, which will affect the policy. I think having a paragraph discussing the issues raised by reviewer AL2N would actually strengthen the paper, and being open about open questions/weaknesses. Alternatively additional ablation or experiments in either other kind of environments (e.g. 3D or environments with occlusion) or just assuming some form of failure at segmentation the visual stream into objects to show robustness would be of interest.   Overall I urge the authors to resubmit their work after properly integrating some of the feedback. In particular focusing on ablation studies or having baselines that are more similar in spirit or at least being more explicit of how it compares with existing work and what aspect of that existing work is trying to fix. For e.g. part of the approach is that it relies on distillation rather than dealing with the RL objective (as other methods might try to do). Now if you take those methods, but you phrase them in a distillation process how would they do? I don’t know if all of this needs to be done, but it just feel as a work to be less grounded and sufficiently far to other existing methods to trivially understand the relationship, while directly only comparing to non symbolic methods in a way that is not in some sense in the advantage of the non symbolic methods.   Additionally,  being more explicit about the potential weaknesses of the method, maybe empirically showing what happens with imperfect segmentation. The work is interesting, and I agree that this is a young field and the goal is *not* to produce state of the art results or outperform DRL methods. And is *not* to solve all the problems with symbolic methods at once, but to improve our understanding in this space. But I think the framing is not the right one in the current manuscript.
The authors propose a random perturbation on top of a soft top k operator that builds upon entropic regularized optimal transport (when applied to a 1D problem). The motivation of the paper is built around an approximation bound (proposed in the Xie et al  20 paper) that compares the true OT matrix from the regularized OT matrix in the case where some of the 1D entries from which one wishes to extract top k values are very close (eg. x_{t} ~ x_{t+1}). The authors argue that this bound, with inverse dependencies in the closest element in the list, diverges.  The authors state that this possible divergence is an issue, because values to be sorted/top ked can be very close in practice. To solve this issue, the authors introduce instead a Gumbel noise mechanism that no longer makes the bound diverge, through a fairly long theoretical analysis. The approach now requires the recomputation for several noisy inputs of the same regularized OT estimator. The authors propose then to use these soft top k approaches to solve a combinatorial problem using gradient descent, namely a capacity constrained problem and clustering, including some tricks on controlling both entropy regularization and Gumbel noise magnitude.  The paper has generated a long discussion among the AC and reviewers. While the paper has a few strong points that were appreciated (interest of empirical validation which seems to suggest some improvements over commercial solvers on considered setups), there remain a few issues.   The theoretical side of the paper is bit blurry. The idea of introducing Gumbel noise on top of an already soft operator is not completely clear, since these perturbations are there to add differentiability to something (reg OT) that was introduced itself to be differentiable. The theoretical motivation is unclear: the noise is introduced because the _upper bound_ diverges (and not the gap between the "true" OT and entropic OT, since it is always bounded). The perturbation mechanism is only motivated to improve the limitations of an upper bound, not of the original algorithm itself. What s more, it s not entirely clear why that gap should be decreased (between true and regularized OT) since it has to exist to obtain some differentiability. While the study of the gap itself was added during the discussion phase in Fig. 1"A toy example to explain Lemma 2", one would expect better foundations for this idea.  With a somewhat unclear theoretical motivation, the experiments should be very convincing. Reviewers have noted some issues related to comparing CPU/GPU times. While I am sympathetic to the problems encountered by the authors when running such comparisons, these issues should be properly reflected in their initial claims, and not appear in the rebuttal only. I also think experiments are still lacking in diversity. For instance, the k means problem is studied in 2D (begging the natural question of whether such an improvement would remain in higher dimensions). I could not find a clear statement on the number of repeats carried out to obtain error bars. Since I don t envision either of the max covering problem nor k means to become the "killer app" of this paper, I would encourage the authors to consider problems that are less synthetic.
The paper introduces a new variance reduced policy gradient method, for directional and clipped action spaces, with provable guarantees that the gradient is lower variance. The paper is clearly written and the theory an important contribution. The experiments provide some preliminary insights that the algorithm could be beneficial in practice. 
The paper presents a modified sampling method for improving the quality of interpolated samples in deep generative models.  There is not a great amount of technical contributions in the paper, however it is written in a very clear way, makes interesting observations and analyses and shows promising results. Therefore, it should be of interest to the ICLR community.
The reviewers point out that this is a well known result and is not novel.
The submission proposes a method for learning a graph structure and node embeddings through an iterative process. Smoothness and sparsity are both optimized in this approach. The iterative method has a stopping mechanism based on distance from a ground truth.   The concerns of the reviewers were about scalability and novelty. Since other methods have used the same costs for optimization, as well as other aspects of this approach, there is little contribution other than the iterative process. The improvement over LDS, the most similar approach, is relatively minor.   Although the paper is promising, more work is required to establish the contributions of the method. Recommendation is for rejection. 
The initial round of reviews showed a consensus among the reviewers that the presentation of the paper was poor, the novelty was unclear, claims were not properly justified, and the experimental evaluation and discussion were quite insufficient. The authors provided a rebuttal and an updated version of the paper. Although the updated paper demonstrated that the proposed approach indeed provides some benefits, it appears that the authors were not successful to address the numerous but constructive reviewers  comments.  The paper is not ready for publication in ICLR 2021 and can benefit from major revisions and careful proofreading. 
This work describes a two stage method for learning with noisy labels. The crux of the reviews, discussions with the authors and post rebuttal discussions between reviews (and myself) was related to the novelty of this work. The main concern is that while this body of work presents a relatively solid method (from an empirical point of view), the underlying components are not altogether that novel, and have been used in the context of learning with noisy labels before. Fundamentally, the proposed S3 method did not feel *convincingly* better, given its relative lack of novel technical insights. I appreciate that this is a frustrating reasoning to get   after all, much of what we do in empirical ML is combinations of existing things. Ultimately, there was consensus amongst the reviewers that the work did not have sufficient insights or such outstanding empirical results so as to overcome this relative lack of technical novelty.   All the reviewers have engaged meaningfully in discussions, provided constructive feedback and I hope that this will make subsequent iterations of this work better in many dimensions.
All Reviewers and myself believe that ICLR may not be the right venue for this paper. Hence, my recommendation is to REJECT it. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Important domain, but out of scope of ICLR.   Collection of sensor and environmental data, which may be potentially hard to collect.  Cons:   Not the appropriate venue.   Lack of machine learning novelty.   Potential lack of generalization of the proposed approach.   Experimental part is hard to follow.   Not very informative figures.
After carefully reading the reviews and the rebuttal, and after going over the paper itself, I m not sure the paper it ready for ICLR. I do believe there is a lot of useful content in the current manuscript, and I urge the authors to keep working on the manuscript and resubmit it in due time.   My concerns are as follows:  (a) there is a lot of discussion about *relational information retrieval*   however there is lack of any formalization of what this term means. I don t mind relational reasoning to be used as motivation, but when it is used to consider what are valid baselines and what are not, I feel compelled to understand what exactly it means. Why is *self attention* retrieval not *relational*? Beside the task being seemingly relational in spirit, how do we test whether the retrieved mechanism carries any relational information whatsoever? I think the community had a learning lesson here in CLEVER dataset, which arguably does not require as much relational reasoning as it seemed. So I agree with Rev5, that there is a decent probability that the task we are using do not require relational information retrieval. While I understand that some of these systems are Transformer inspired, I feel transformer should be a baseline.   (b) I also feel the paper should take one of two paths.           Either embrace larger scale tasks and baseline outside of the relational reasoning literature (like transformer) and particularly settings where potentially self attention will struggle due to the quadratic term or where they tend to be hard to train due to the difficulty of doing credit assignment through the attention mechanism           Provide more careful ablation studies and formalize the claims a bit more. Regarding e.g. the discussion of a single larger memory vs multiple memory blocks. One of the main difference comes from the attention over which memory block to use in the proposed approach, which due to softmax has a unimodal behavior. So is the reason why it works better this potential hiding of part of the memory representation (so a better way of reading a subset of the memory entry). This could potentially be done differently (e.g. multiplicative interaction in the same style, for e.g. that they were used in WaveNet). This is just a random thought on this particular aspect. I have similar questions about the self supervised loss.    I find the paper focusing on improving performance (unfortunately on toy domains) rather than ablation studies and an understanding and careful understanding of how things works. I realize there is some such analysis in the appendix. But I feel more of it should be in the main text. The paper is either proposing something that scales and works well at scale (and then understanding why is less important as it has direct application) or explores a very specific phenomena and then is fine to stay on toy tasks but there should be a bit of clarity in the claims, and an investigation whether the hypothesis (or intuition) put forward initially is the reason why the model works.  
This work proposes an interesting approach for learning the relational constraints of a dataset and then generating according to those constraints. Learning the constraints via a constrained optimization problem is an interesting contribution. The application of constrained generation is also interesting and can be applied to several domains though only music and poetry is examined in this work. However, the music evaluation is unconvincing and the paper lacks clarity in the description of the approach such as building the GCN. Music evaluation could be improved with human evaluation since loss metrics don t paint a full picture. Finally, a more diverse set of experiments and datasets (rather than just one poetry collection and one folk song corpus) and more analysis on the learnt constraints could give a more complete story for this approach s effectiveness on sequence data.
The submission addresses the problem of multiset prediction, which combines predicting which labels are present, and counting the number of each object.  Experiments are shown on a somewhat artificial MNIST setting, and a more realistic problem of the COCO dataset.  There were several concerns raised by the reviewers, both in terms of the clarity of presentation (Reviewer 1), and that the proposed solution is somewhat heuristic (Reviewer 3).  On the balance, two of three reviewers did not recommend acceptance.
While one reviewer remained concerned about the possibility of convergence to bad equilibria and felt that the proposed method appears to be four minor changes from prior work (PAIRED), the authors demonstrate empirically that the proposed changes make a significant difference in their evaluation. Other reviewers were positive about this work and all others rated this work as an accept. Post rebuttal the most positive reviewer increased their score to an 8 and felt did a good job answering their concerns. They wanted to see an analysis of systems with larger numbers of agents, but felt that the current manuscript was more than sufficient to warrant acceptance, and fell into the category of a good paper with the additional ablations provided during the rebuttal.  The AC recommends accepting this paper.
The paper combines discriminative and generative positive unlabeled learning into a single framework. The reviewers argued the novelty and contributions are not enough for ICLR and unfortunately we cannot accept it for publication.
This paper proposes a stochastic trust region method for local minimum finding based on variance reduction, which achieves better oracle complexities than some of the previous work. This is a borderline paper and has been carefully discussed. The main concern of the reviewers is that this paper falls short of proper experiment evaluation to support their theoretical analysis. In detail, the authors proved a globally sublinear convergence rate to a local minimum, yet the experiments demonstrate a linear or even quadratic convergence starting from the initialization. There is a big gap between the theoretical analysis and experiments, which is probably due to the experimental design. In addition, the authors did not submit a revision during the author response (while it is optional), so it is unclear whether a major revision is required to address all the reviewers’ comments. In fact, one reviewer thinks that a major revision is needed. I agree with the reviewers’ evaluation and encourage the authors to improve this paper before future submission.
This paper proposes an approach for learning to generate 3D views, using a surfel based representation, trained entirely from 2D images.  After the discussion phase, reviewers rate the paper close to the acceptance threshold.  AnonReviewer3, who initially stated "My second concern is the results are all on synthetic data, and most shapes are very simple", remains concerned after the rebuttal, stating "all results are on synthetic, simple scenes. In particular, these synthetic scenes don t have lighting, material, and texture variations, making them considerably easier than any types of real images."  The AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper.  Particularly relevant is the criticism that "While the paper is called ‘pix2scene’, it’s really about ‘pix2object’ or ‘pix2shape’." 
This paper presents new results on adversarial training, using the framework of robust optimization. Its minimax nature allows for principled methods of both training and attacking neural networks.  The reviewers were generally positive about its contributions, despite some concerns about  overclaiming . The AC recommends acceptance, and encourages the authors to also relate this work with the concurrent ICLR submission (https://openreview.net/forum?id Hk6kPgZA ) which addresses the problem using a similar approach. 
This paper shows that L2 self attention is Lipschitz and presents a new method for computing the Lipschitz constant. All reviewers are positive about the technical part of the paper. However, the major concern comes from the significance of the computed Lipschitz constant. The paper only presents some numerical results using simple toy examples, which is insufficient to justify the  importance of the proposed method. The paper would be a much stronger paper if better numerical results could be presented.
The paper studies the effect of various hyperparameters of neural networks including architecture, width, depth, initialization, optimizer, etc. on the generalization and memorization. The paper carries out a rather through empirical study of these phenomena. The authors also rain a model to mimic identity function which allows rich visualization and easy evaluation.  The reviewers were mostly positive but expressed concern about the general picture. One reviewer also has concerns about "generality of the observed phenomenon in this paper". The authors had a thorough response which addressed many of these concerns. My view of the paper is positive. I think the authors do a great job of carrying out careful experiments. As a result I think this is a good addition to ICLR and recommend acceptance.
The reviewers in general like the paper but has serous reservations regarding relation to other work (novelty) and clarity of presentation. Given non linear state space models is a crowded field it is perhaps better that these points are dealt with first and then submitted elsewhere.
This paper was on the borderline. I am sympathetic to the authors  point about computational resources. It is helpful to demonstrate performance gains that offer "jump start" performance benefits, as the authors argue. However, the empirical results even on this part are still somewhat mixed  for example, the proposed approach struggles on Private Eye (doing far worse than DQN) in Table 2. In addition, while it is beneficial to remove the need for training a density model, it would be good to show a place where a density model fails (perhaps because it is so hard to find a good one) compared to their proposed approach. 
Strengths   The paper presents a method of training two level hierarchies that is based on relatively intuitive ideas and that performs well. The challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the separation of internal state from external state is a clean principle that can potentially be broadly employed.  The method does well in outperforming the alternative baselines.  Weaknesses  There is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses  a policy ensemble;  phase info is used in DeepLoco/DeepMimic; methods such as "Virtual Windup Toys for Animation"  exploited periodicity (25y ago);  More comparisons with prior work such as Florensa et al. would help.  The separation of internal and external state is an assumption that may not hold in many cases. The results are locomotion focussed. There are only two timescales.  Decision  The reviewers are largely in agreement to accept the paper.  There are fairly simple but useful lessons to be found in the paper for those working on HRL problems, particularly those for movement and locomotion.  The AC sees the novely with respect to different pieces of related work is the weakest point of the paper.   The reviews contain good suggestions for revisions and improvements;  the latest version may take care of these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution to ICLR 2019. 
All reviews are somewhat below the acceptance threshold. The main concerns are in terms of lack of novelty, and that some of the paper s main claims are unsupported. Many of the criticisms are quite focused on specific details, but these seem significant enough to have been deal breakers for this submission.
This paper propose a method to train DNNs using 8 bit floating point numbers, by using an enhanced loss scaling method and stochastic rounding method. However, the proposed method lacks novel and both the paper presentation and experiments need to be improved throughout. 
The authors put a lot of effort in replying to questions and improving the paper (to a point that the reviewers felt overwhelmed).  Pros:   An interesting way of dealing with model bias in MPC   They successfully managed to address the most important concerns of the reviewers, with lots of additional experiments and insights   R3 s concerns have also been successfully addressed by the authors, the review & score were unfortunately not updated  Cons:   The only remaining point is that the simulations seem to be everything but physically realistic (update at end of R1 s review), which is probably a problem of the benchmarks and not the authors faults.
The authors propose a new continual learning setting with a few distinguishing features: 1) the task boundaries are blurry (in other words, past task samples can reappear); 2) training is online; and 3) evaluation using online accuracy (instead of average accuracy). The authors also propose a useful method for this scenario and benchmark it using four different datasets.  The first round of review pointed to two main limitations of the manuscript.  + The authors only provided small scale experiments. The reviewers argued that for the setup and method to have an impact having good results using larger scale data would go a long way.  + Whether “task free” and “class incremental” were compatible.   For the former, the authors were very reactive and provided results using a standard "ImageNet for CL" dataset.   For the latter, I must thank the authors and also the reviewers for discussing this thoroughly. In the end, my understanding is that there was a reconciliation that both were in fact compatible, but the reviewer suggested that this be discussed very clearly by the authors. I second this suggestion. The CL field given its many slightly different settings might be partly to blame here (reviewer Vfw2 made a similar comment, and I also thank them for playing a role in resolving the issue).   A few additional thoughts:  + I believe that more general setups in CL are worthwhile even in the absence of any immediate applications. This is especially true since some of the standard CL assumptions do not seem to be well motivated. However, I find that claiming that something is more realistic requires grounding (e.g. a set of examples from the "real world" or a specific domain/setting). I know the authors backed some of their claims with references, but different real world problems will come with different limitations and I would be hesitant to use phrases such as "most real world" settings without thorough justification. + While different from the core of your work, I believe the framework proposed in this other recent paper has similar goals (although the setup allows pre training and is not online). Might be worth knowing about it in case you do not:  Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning, NeurIPS 2020 https://papers.nips.cc/paper/2020/file/c0a271bc0ecb776a094786474322cb82 Paper.pdf  All in all, this is a good contribution that proposes an interesting and rich setting along with a good baseline method for it. I strongly encourage the authors to follow through on their promise to provide the community with code, dataset splits, Kaggle leaderboard, etc., as a way to maximize the impact of their work.
Main summary:  Design an effective and economical model which spots keywords about pests and disease from community radio data in Luganda and English.  Discussions: all reviewers vote on rejecting the paper, due to lack of generalizability, training and evaluation discussion need work Recommendation: Reject
This paper studies how the architecture and training procedure of binarized neural networks can be changed in order to make it easier for SAT solvers to verify certain properties of them.  All of the reviewers were positive about the paper, and their questions were addressed to their satisfaction, so all reviewers are in favor of accepting the paper. I therefore recommend acceptance.
The reviewers agree this paper is not yet ready for publication.
This paper introduces an architecture based on structured causal model for long tailed IE tasks. It incorporates the dependency tree structure of the sentence using a GCN for learning the representations. The key idea is to use counterfactual reasoning to help with the inference in attempt to reduce the impact of spurious relations. There are some concerns about the presentation of this paper. While the high level idea is reasonably clear and well motivated, the paper is quite messy with the notations and technical details.  How to use the causal effect estimation for the final prediction is not explicitly explained except for in Figure 1.  For the experiments on ED and NER, it is unclear if they assume the trigger or span is given. The method seems to need the span information to make the prediction. If span is given, this is a different set up that is much simpler compared to traditional ED and NER where the span or trigger needs to be detected as well. There are also some question regarding the difference between this work and the prior work on using causal reasoning for improving prediction (the TDE work). One difference is the additional term in equation 8(updated version), which appears to be useful empirically, but the motivation is rather hand wavy and needs more clarification. Overall, there are some useful ideas, but the overall novelty does not particularly stands out, and the presentation of the paper made somewhat straight forward ideas more convoluted than necessary.  
The submission applies architecture search to find effective architectures for video classification. The work is not terribly innovative, but the results are good. All reviewers recommend accepting the paper.
This paper presents a capsule network to handle 3d point clouds which is equivariant to SO(3) rotations. It also provides the theoretical analysis to connect the dynamic routing approach to the Generalized Weiszfeld Iterations. The equivariant property of the method is demonstrated on classification and orientation estimation tasks of 3D shapes. While the technical contribution of the method is sound, the main concern raised by the reviewers was the lack of details in the presentation of methodology and results. Although the authors have made substantial efforts to update the paper, some reviewers were still not convinced and thus the scores remained the same. The paper was on the very borderline, but because of the limited capacity, I regret that I have to recommend rejection.  Invariances and equivariances are indeed important topics in representation learning, for which the capsule network is known as one of the promising approaches but still not well investigated compared to other standard architectures. I encourage authors to resubmit the paper taking in the reviewers  comments.
The reviewers raised a number of major concerns including a poor readability of the presented materials, incremental novelty of the presented and, most importantly, insufficient and unconvincing ablation and experimental evaluation studies presented. The authors’ rebuttal failed to address all reviewers’ questions and failed to alleviate reviewers’ concerns. The authors explain that due to the lack of time they could not complete all experimental studies. A major revision of the paper is needed before the paper can be accepted for publication. Hence, I cannot suggest this paper for presentation at ICLR.
This submission received four high quality reviews and there are a lot of meaningful discussions during the author response period. After the discussions, all four reviewers agreed that the submission can be strengthened in a number of ways, including more solid experimental results and a justification for the correctness of the ELBO.  The AC agrees. The authors are encouraged to revise the paper based on the reviews for the next venue.
Strengths: * Strong results across two benchmarks * Ablation study demonstrates importance of components * Provides improvements especially in low resource settings * Well written paper  Weaknesses: * Novelty of the method may be limited as previous works have explored structured outputs as intermediate plans * Not clear method will extend to other domains as decent AMR parses are required to train the imagination module, which might work well on the datasets used (e.g., RocStories), but wouldn t work in settings with more complex language
The paper extends capsule networks with a pairwise learning objective and evaluates on small face verification datasets. The authors do a great job describing prior work, but lack clarity when articulating their contribution and proposed method. In addition, some important implementation details, such as hyperparameter selection, are missing causing further confusion as to the final approach. Overall, according to the experiments shown, the approach offers modest improvements over prior work.  The approach offers an interesting and promising direction. We encourage the authors to revise the manuscript to clarify their approach and contribution and to improve their evaluation by including the relevant metrics and implementation details. 
The authors propose a learning approach based on mutual information maximization. By considering a view x, and two subviews, x’ and x’’, the authors provide a bound on MI by combining two InfoNCE like bounds on I(x’’; y) and I(x’; y | x’’). The authors show that optimising this (approximate) bound leads to improvements in several tasks covering NLP and vision.  This paper is aiming to address a significant problem for the ICLR community and provide a novel solution. The manuscript is well written and the main idea is clear. The reviewers appreciated the fact that the experimental setup covers both vision and NLP. On the negative side, the reviewers raised several major issues, both with the presented theory and the experimental setup. From the theoretical point of view, the approach hinges on a good  approximation for p(y|x ), which could arguably be as hard as the original problem. The author s response is definitely a step in the right direction, but the changes to the original manuscript are quite substantial and there is no time for a thorough validation of the updated claims. I will hence recommend rejection and strongly suggest that the authors incorporate the reviewers’ feedback and submit the manuscript to a future venue.
This paper formalizes the setting where an autonomous RL agent operates with zero or very few resets, and provides a novel benchmark for this setting with diverse environments ranging from simple manipulation to complex manipulation/locomotion. The paper then uses this benchmark to analyze current methods and provide insight into those crucial factors that affect performance in this setting. The insights into current methods especially are appreciated. As one reviewer stated, "This paper isolates one problematic assumption in the way of [progress in RL], the environment reset problem, and provides the groundwork for [such] progress, i.e. baselines, clear metrics, etc. I believe the community is much better off with this paper published, since prior works don t seem to have used compatible methodologies."
The paper proposed an active search algorithm for efficiently identifying rare concepts among heavily imbalanced datasets. Reviewers find the paper very well motivated and addressing an important real world challenge in active learning. All reviewers appreciate the extensive demonstration of the effectiveness of the proposed algorithm on real world tasks, in particular in industrial settings where the scale of problems goes far beyond common academic datasets.  In the meantime, there are shared concerns among several reviewers in the technical depth of the proposed algorithm. Although the authors provided intuitive explanations of the nearest neighbor based approach, the results reported are restricted mostly to several final performance metrics on the search performance. As a purely empirical work, the paper would benefit from more fine grained experimental analyses and ablation studies (e.g., by breaking down to analyses at intermediate levels). 
This paper proposes a novel Federated Learning (FL) framework that leverages the Neural Tangent Kernel (NTK), to replace the gradient descent algorithm for optimization. Specifically, the workers upload the labels and the Jacobian matrices to the server, and the server uses the tools from the NTK to obtain a trained neural network. However since this could lead to increased communication cost and compromise of data privacy, the authors propose data sampling and random projection techniques to alleviate the problem. The authors provide a theoretical analysis that the proposed scheme has a faster convergence than FedAvg under specific assumptions, and experimentally validate that it significantly outperforms previous FL algorithms, achieving similar test accuracy to ideal centralized cases.  Pros   The idea of using NTK for model optimization without gradient descent and use of it in the FL setting is both interesting and novel.   The paper properly discusses and tackles the new challenges posed by the introduction of the new method.   The paper is well organized and clearly written, with sufficient discussion of related works and backgrounds.   Cons    The proposed method puts heavy computational burdens on the server side.   The method violates the privacy preserving feature of FL by its nature, and while the proposed compression shuffling alleviates the concern, more discussion is necessary.   Missing comparison against popular baselines such as FedProx and SCAFFOLD.    The faster convergence of the proposed method in comparison to FedAvg depends on the learning rate and is not always true.   There is a gap between the theory and practice, which makes the practicality of the algorithm still questionable.   Although the reviewers found the idea as novel, the proposed techniques for alleviating communication cost and privacy concerns convincing, and considered both the theoretical analysis and experimental validation thorough, all reviewers leaned toward rejection due to critical concerns unanswered. During the discussion period, the authors alleviate many of the minor concerns from the reviewers, but there were still remaining concerns on the gap between the theory and practice on its convergence behavior, and insufficient discussion of the privacy preserving feature of the proposed method, as well as shifting of computation burdens to the server. Thus, the reviewers reached a consensus that the paper is not yet ready for publication.   Despite the low average score, the novelty of the idea and the quality of the paper is much higher than those of the accepted papers in my batch, and I strongly believe that this will become a high impact paper, if remaining concerns from the reviewers are properly resolved.
This is a strong submission, and I recommend acceptance. The idea is an elegant one: sparsify a network at initialization using a distribution that achieves approximate orthogonality of the Jacobian for each layer. This is well motivated by dynamical isometry theory, and should imply good performance of the pruned network to the extent that the training dynamics are explainable in terms of a linearization around the initial weights. The paper is very well written, and all design decisions are clearly motivated. The experiments are careful, and cleanly demonstrate the effectiveness of the technique. The one shortcoming is that the experiments don t use state of the art modern architectures, even thought that ought to have been easy to try. The architectures differ in ways that could impact the results, so it s not clear to what extent the same principles describe SOTA neural nets. Still, this is overall a very strong submission, and will be of interest to a lot of researchers at the conference.  
All four reviewers recommend rejecting the paper. However there is agreement that this is an interesting line of research, and the AC agrees. Reviewers provided extensive and well educated feedback. The authors did not respond to the raised concerns.
Meta Review for Invariance Through Inference  The motivation of this work is to address the problem of learning a model that generalizes well on a test distribution that samples outside of the training data distribution. Reviewer X3P2 wrote a good summary of the paper:  In this paper, the transfer of a reinforcement model (RL) setting from an idealized (training) environment to a more realistic environment with distractors in the observations is considered. Instead of augmenting the training environment with more data so as to make the system more resilient to variations and distractors, the system is adapted at test time to be invariant to the specific distractors found in the environment. Experiments in simulation show the benefits of the proposed approach. Crucially, the agent is not able to access any reward data at test time.  Reviewers, including myself, recognize the novelty of the approach, in particular appreciate the authors  motivation to provide a more principled way to model environment invariance that can possibly scale well in comparison to data driven approaches. However, the initial round of feedback is generally negative, in particular, most reviewers raise concerns regarding the lack of clarity in presentation, and also have issues with the narrow range of experimental evaluation. Clearly, this is promising work, but possibly had to be rushed for submission.  To the authors  credit, they devoted substantial efforts to completely revamp their paper, addressing many of the issues head on. The resulting updated manuscript is almost a complete rewrite of the paper. All reviewers acknowledge (and praise) the effort from the authors  to improve the paper, and 3 out of 4 reviewers had improved (or maintained) their scores from rejection to a 6. But as the paper is a complete revamp, reviewers did not have the time to assess the entire rewrite of the paper (it s like the need to review a paper from scratch), so the confidence is reduced.  While X3P2 did not change their score, they did lead a discussion amongst myself and other reviewers, and they spent the time to take a detailed look at the completely revised draft. Here are the comments from that discussion, for full transparency:     *The paper has been completely revamped, to the point in which the presented technique is actually different (the dynamics loss now includes a new forward term). The changes are overall welcome since it significantly improves the clarity of the presentation. Experiments still show promise.*  *I still have problems with the theoretical aspect of it, though. I think that it is unclear why the proposed system is working and fails to provide the minimal system that works.*  *Equation (4) is dimensionally incorrect. It s summing squared error over actions with squared error over latents. Both of these are arbitrary units that can lead to the forward or backward losses dominating. It s also unclear why both of these losses are necessary and not just the inverse one.*  *Equation (7) is similarly dimensionally incorrect. Again, units are arbitrary and for all we know the joint loss could be ignoring the dynamics loss or the adversarial loss.*  *The use of a GAN and the corresponding loss is unjustified. As the authors acknowledge, if the dynamic loss is very small, then the system should already work. They argue that finding that parameterization without the adversarial loss is challenging. There s a difference between using the right loss and finding the right way to optimize it, but here it seems that the right loss is being modified for optimization purposes. Is the adversarial loss something we really want to minimize or just something that helps find the best dynamics loss? What if we remove the adversarial loss after being close to convergence? What if we use multiple restarts or other techniques to help with the optimization of only the dynamics loss? My take from the theory is that the adversarial loss term shouldn t be needed and that the challenging optimization problem should be addressed (rather than modifying the loss).*  *The new ablation experiments are also confusing: If the dynamics loss is the actual driver, and the adversarial loss only helps with finding a good solution, how come that we get almost equally good results when we remove the dynamic loss? Matching the latent distribution shouldn t be enough to have aligned latents. Maybe there s something about the architecture of F that matches the ground truth, so that matching the distribution aligns the latents. This hints towards the adversarial loss actually playing an important role beyond helping with the optimization problem. This is not supported at all by the theory, since matching distributions should result in arbitrary latents and potentially performance of a random system. In fact, given the problems with units, the joint loss might be dominated by the adversarial loss, which would explain this result.*  *It seems like there s something here, but I think more work is necessary to really understand which pieces are necessary in this system and whether there s some sort of adaptation between the experimental setup and F that would explain why distribution matching results in latent alignment, which is not expected (Zhu et al. 2017). Also, the units problem makes the ablation results even harder to interpret: Maybe the dynamics loss is playing a small role in the joint loss, and that s why removing it completely has a small impact.*     After much assessment, while I do find this work to be interesting and potentially highly impactful (since they introduce an alternative approach to data centric one OOD), the final manuscript s assessment is still borderline (the reviewers all mentioned that while they recognize the improvement, they list issues from preventing their full endorsement), and X3P2 still found several issues with the revision (which I do believe can be addressed in due time). While I m fully confident that with additional work, this paper could have the potential to be an impactful one, I am currently on the side of not recommending it for acceptance for ICLR 2022.  Note to the PC s, that this is a borderline decision. If the PC s want to flip the decision to an accept, and think the post rebuttal issues are small enough, I ll be fine with that. But in any case, I look forward to seeing a further improved version (of the revamped manuscript) published in a journal or presented at a future conference. Good luck!
This paper gives sample complexity lower bounds for differentially private empirical risk minimization (ERM). While the reviewers agreed that the results are non trivial, the general consensus was that the proofs are tweaks of previously developed techniques and that the main result is actually new in a rather narrow setting (specifically, for unconstrained ERM and sub constant error parameter). Another concern was that one of the proofs (the one on pure differential privacy) was incorrect in the submission; a different proof was provided subsequently (which also closely follows prior work). Finally, the reviewers pointed out several issues with the clarity of the presentation and comparison to prior work. Given the above, this work is below the acceptance threshold.
The authors present a physics aware models for inpainting fluid data. In particular, the authors extend the vanilla U net architecture and add losses that explicitly  bias the network towards physically meaningful solutions.   While the reviewers found the work to be interesting, they raised a few questions/objections which are summarised below:  1) Novelty: The reviewers largely found the idea to be novel. I agree that this is indeed novel and a step in the right direction. 2) Experiments: The main objection was to the experimental methodology. In particular, since most of the experiments were on simulated data the reviewers expected simulations where the test conditions were a bit more different than the training conditions. It is not very clear whether the training and test conditions were different and it would have been useful if the authors had clarified this in the rebuttal. The reviewers have also suggested a more thorough ablation study. 3) Organisation: The authors could have used the space more effectively by providing additional details and ablation studies.  Unfortunately, the authors did not engage with the reviewers and respond to their queries. I understand that this could have been because of the poor ratings which would have made the authors believe that a discussion wouldn t help. The reviewers have asked very relevant Qs and made some interesting suggestions about the experimental setup. I strongly recommend the authors to consider these during subsequent submissions.   Based on the reviewer comments and lack of response from the authors, I recommend that the paper cannot be accepted. 
The paper proposes to model uncertainty by combining quantile regression and Chebyshev polynomial approximation. The paper addresses the important problem of uncertainty quantification for black box models. However, some major concerns remain after the discussion among the reviewers. In particular, there has been some concerns around the clarity of the presentation. The proposal lacks a clear use case, e.g. where satisfying constrained black box uncertainty problem is a must have. 
The paper shows an automatic piano fingering algorithm. The idea is good. But the reviewers find that the novelty is limited and it is an incremental work. All the reivewers agree to reject.
This paper proposes to a simple method for  tuning parameters of HMC by maximizing the log density under the final sample of the MCMC, and apply it for training VAE. The reviews and discussion raises some critical concerns and questions, which unfortunately, which unfortunately, is not adequately addressed. 
The paper suggests a simple variant for BERT training that improves classification for smaller training samples.  So it has a very specific applicability unlike other published variants which generally improve a broad range of tasks.  The variant adds a self supervision classification task based on clustering.  Experiments are done but it only shows improvement for small training sizes.  AnonReviewer4 suggested a BOW experiment/baseline which was done by the authors in an updated version.  This confirmed the authors line.  AnonReviewer3 asked for computational details, which were added.  AnonReviewer1 lists a number of limitations which the authors need to address and rephrase the statements in their paper.  So it is publishable work, but somewhat marginal due to its specialised nature and thus rejected.
The paper changes the metric in self paced reinforcement learning to be a Wasserstein distance and shows that this outperforms other metrics in simple toy like experiments.  Even after discussions with the authors, two major concerns were identified with this submission: First, the proposed modification of the metric appears to be rather incremental with regards to the original paper. Second, the proposed method is only evaluated on relatively simple environments. The approach should be evaluated on more difficult tasks.  Given that there was no strong champion for acceptance among reviewers of this paper and the above mentioned limitations, I recommend rejecting this paper.
The paper worked on an important problem (robustness concerning spurious correlations) and proposed a useful method (achieving SOTA worst group performance without the true group labels). However, the motivation part is weak so that it is unclear why to go from the theoretical/empirical observations to the proposed method (more specifically the contrastive learning part). The novelty is also not very strong as argued by reviewers (the real novelty was not highlighted by the authors and thus cannot easily be appreciated by readers). It is indeed a borderline case but seems to be below the bar of acceptance and the two reviewers staying on the positive sides would not like to fight for it. Since there is still room for improvement, we hope the paper would benefit from a cycle of revisions for a re submission and the improved version would be accepted in the near future.  By the way, what GgTx suggested is not really an out of scope study, as far as I understood. The authors certainly think that the paper/method has been clearly motivated. This reviewer was asking for a strong motivation, namely, what is missing or what is wrong in existing methods or the SOTA method so that we need/have to apply the proposed method? Without clarifying this point, the paper/method is partially but not fully motivated and the method may look like another alternative though it should be a better one. Instead of showing the better performance, the reviewer would like to see the conceptual advantage of the proposal by understanding what is missing/wrong in the current SOTA method. Therefore, I think this is a great question for the authors to maximize the impact of their work in the end.
This paper addresses a clear open problem in representation learning for language: the learning of language agnostic representations for zero shot cross lingual transfer. All three reviewers agree that it makes some progress on that problem, and my understanding is that a straightforward presentation of these would likely have been accepted to this conference. However, there were serious issues with the framing and presentation of the paper.  One reviewer expressed serious concerns about clarity and detail, and two others expressed serious concerns about the paper s framing. I m more worried about the framing issue: The paper opens with a sweeping discussion about the nature of language and universal grammar and, in the original version, also claims (in vague terms) to have made substantial progress on understanding the nature of language. The most problematic claims have since been removed, but the sweeping introduction remains, and it serves as the only introduction to the paper, leaving little discussion of the substantial points that the paper is trying to make.  I reluctantly have to recommend rejection. These problems should be fixable with a substantial re write of the paper, but the reviewers were not satisfied with the progress made in that direction so far.
The paper addresses the problem of generating images by combining visual components. These components are learned during pretraining, forming a dictionary of visual concpets which plays the role of text in DALLE. The technique is based on DALLE and slot attention approach to generate VQ codes in a way that is consistent.  Reviewers had various concerns, including (1) that using synthetic images makes it easier to combine visual components  (2) that the novelty and relation to literature was not clear enough (3) missing ablations.  The authors provided a detailed rebuttal which addressed reviewer concerns in a convincing way.   One remaining issue of the paper is the writing. The paper fails to clearly explain the workflow (what are input and output during pretraining, training and inference), and how compositionality is controlled (what can be used for conditioning). As a consequence, it requires substantial effort to understand the idea of the paper, and what real problems can be solved with the proposed approach .   The paper can be accepted to ICLR, but it is expected that the writing would be improved. The abstract and introduction should make concrete statements about what the approach does, what problems it solves and how it can be used for the various tasks as disucussed in the experiments
This paper aims to look at the relationship between disentanglement and multi task learning.  The authors claim to show that disentanglement emerges naturally from MTL.  The main discussion was whether the claim that disentanglement emerges naturally from MTL has been adequately demonstrated.  The main   issue is that MTL results in more extraction of information and that is hard to disentangle from the disentanglement metrics used.  Reviewers agreed the work was interesting but not as complete as would be desirable.  I also feel it is not ready for ICLR presentation, but   with further work could be a nice future contribution.
despite the (significant) improvement in language modelling, it has always been a thorny issue whether better language models (at this level) lead to better performance in the downstream task or whether such a technique could be used to build a better conditional language model which often focuses on the aspect of generation. in this context, the reviewers found it difficult to see the merit of the proposed approach, as the technique itself may be considered a rather trivial application of earlier approaches such as truncated backprop. it would be good to apply this technique to e.g. document level generation and see if the proposed approach can strike an amazing balance between computational efficiency and generation performance.
This paper proposes a new method for combining previous state representation learning methods and compares to end to end learning without without separately learning a state representation. The topic is important, and the authors have made an extensive effort to address the reviewer s concerns, particularly regarding clarity, related work, and accuracy of the drawn conclusions. The reviewers found that the main weakness of the paper was the experiments not being sufficiently convincing that the proposed approach is better than the alternatives. Hence, it does not currently meet the bar for publication.
The paper studies a convolutional LSTM (ConvLSTM) based model (DRC: Deep Repeated ConvLSTM) trained through reinforcement, and shows that it performs better than other model free approaches, in particular in term of generalization. The ability to generalize is attributed to being able to plan. This last part is not completely convincing.  The paper is clearly written, the experiments are in 4 limited domains: Sokoban, Boxworld, MiniPacman, Gridworld. While diverse, tasks are still all similarly navigation in top down (2D) grid worlds. It is unclear what are the limits of the reach of this study. The experimental evidence presented here could also be interpreted as: local best response recognition of shapes (Conv) and memory of such patterns and associated actions (LSTM) are sufficient for all those environments.  Overall, this is an interesting direction, but it falls slightly short of being acceptable for publication at ICLR.
The paper studies the impact of rounding errors on deep neural networks. The                                                        authors apply Monte Carlos arithmetics to standard DNN operations.                                                                  Their results indeed show catastrophic cancellation in DNNs and that the resulting loss of                                          significance in the number representation correlates with decrease in validation                                                    performance, indicating that DNN performances are sensitive to rounding errors.                                                                                                                                                                                         Although recognizing that the paper addresses an important problem (quantized /                                                     finite precision neural networks), the reviewers point out the contribution of                                                      the paper is somewhat incremental.                                                                                                  During the rebuttal, the authors made an effort to improve the manuscript based                                                     on reviewer suggestions, however review scores were not increased.                                                                                                                                                                                                      The paper is slightly below acceptance threshold, based on reviews and my own                                                       reading, as the method is mostly restricted to diagnostics and cannot yet be used                                                   to help training low precision neural networks.
Proposed network compression method offers limited technical novelty over existing approaches, and empirical evaluations do not clearly demonstrate an advantage over current state of the art. Paper presentation quality also needs to be improved. 
This work claims two primary contributions: first a new saliency method "expected gradients" is proposed, and second the authors propose the idea of attribution priors to improve model performance by integrating domain knowledge during training. Reviewers agreed that the expected gradients method is interesting and novel, and experiments such as Table 1 are a good starting point to demonstrate the effectiveness of the new method. However, the claimed "novel framework, attribution priors" has large overlap with prior work [1]. One suggestion for improving the paper is to revise the introduction and experiments to support the claim "expected gradients improve model explainability and yield effective attribution priors" rather than claiming to introduce attribution priors as a new framework. One possibility for strengthening this claim is to revisit experiments in [1] and related follow up work to demonstrate that expected gradients yield improvements over existing saliency methods. Additionally, current experiments in Table 1 only consider integrated gradients as a baseline saliency method, there are many others worth considering, see for example the suite of methods explored in [2].   Finally, I would add that the current section on distribution shift provides an overly narrow perspective on model robustness by only considering robustness to additive Gaussian noise. It is known that it is easy to improve robustness to Gaussian noise by biasing the model towards low frequency statistics in the data, however this typically results in degraded robustness to other kinds of noise types. See for example [3], where it was observed that adversarial training degrades model robustness to low frequency noise and the fog corruption. If the authors wish to pursue using attribution priors for improving robustness to distribution shift, it is important that they evaluate on a more varied suite of corruptions/noise types [4]. Additionally, one should compare against strong baselines in this area [5].  1. https://arxiv.org/abs/1703.03717 2. https://arxiv.org/abs/1810.03292 3. https://arxiv.org/abs/1906.08988 4. https://arxiv.org/abs/1807.01697 5. https://arxiv.org/abs/1811.12231 
The reviewers have found that while the task of visual domain adaptation is meaningful to explore and improve, the proposed method is not sufficiently well motivated, explained or empirically tested. 
The paper attempts to provide a theoretical explanation for benefit of language model pretraining on downstream classification task. In this regard, the authors provide a mathematical framework which seems to indicate that the distribution of the next word, conditional on the context, can provide a strong discriminative signal for the downstream task. The reviewers found the formulation insightful, interesting, and novel. Also reviewers enjoyed reading the well written paper and appreciated its cautious in its tone. As correctly pointed out by reviewers, the proposed framework might not directly align with techniques used in practice. Applicability of the framework to other pre training approaches is limited.  Also, there are some unresolved concerns about $O(\sqrt{\epsilon})$ assumption still. Nevertheless, reviewers reached a consensus that the framework would be beneficial for the community and attract follow up works. Thus, I recommend an acceptance to ICLR. Following reviewer suggestion, it is strongly recommended that extensions section be expanded in the revised version using the extra page.
This paper describes a graph convolutional network (GCN) approach to capture relational information in natural language as well as knowledge sources for goal oriented dialogue systems. Relational information is captured by dependency parses, and when there is code switching in the input language, word co occurrence information is used instead. Experiments on the modified DSTC2 dataset show significant improvements over baselines. The original version of the paper lacked comparison to some SOTA baselines as also raised by the reviewers, these are included in the revised version. Although the results show improvements over other approaches, it is arguable BLEU and ROUGE scores are not good enough for this task. Inclusion of human evaluation in the results would be very useful. 
This paper extends the single source H divergence theory for domain adaptation to the case of multiple domains. Thus, drawing on the known connection between H divergence and learning the domain classifier for adversarial adaptation, the authors propose a multi domain adversarial learning algorithm. The approach builds upon the gradient reversal version of adversarial adaptation proposed by Ganin et al 2016.   Overall, multi domain learning and limiting the worst case performance on any single domain is an interesting problem which has been relatively underexplored. Though this work does not have the highest performance on all datasets across competing methods, as noted by reviewers, it proposes a useful theoretical result which future research may build on. I would encourage the reviewers to compare against and discuss the missing prior work cited by Rev 3. 
This paper develops a generative density model based on continuous time flows on a potential field.   Strengths:  The paper contains interesting ideas and connections to physics, in particular the enforcement of symmetry in a computationally cheap way.  Weaknesses:  The main quantitative results of this paper are undercut by the numerical error introduced by the approximate, fixed step integrator used.  In the paper, the authors did not check the degree of numerical error (or to what extend their reported likelihoods do not normalize) as a function of the step size.  This was partially addressed in a comment below.  There does seem to be some novelty but the lack of concrete experiments is a letdown. One could e.g. verify that the samples have similar properties (e.g. moments) to the ground truth, which are known for the Ising model. Regarding clarity, the symmetry constraints are never clearly specified.  This paper contains many ideas that would have been novel, but were scooped by [1] which was put on arXiv 3 months before the ICLR submission date.  The authors have added appropriate references to this paper, but this still undercuts the originality of the contribution.  The explanation of how and which symmetries are enforced is a little bit buried and unclear.  Points of contention:  Two of the reviewers didn t seem to be aware that the main mathematical results of the model are special cases of results from [1].  Consensus:  All reviewers agreed that there were interesting ideas in the paper, and that it was close to the bar.  [1] Chen, Tian Qi, et al. "Neural Ordinary Differential Equations."
Agreement by the reviewers: although the idea is good, the paper is very hard to read and not accurately enough formulated to merit publication.    This can be repaired, and the authors should try again after a thorough revision and rewrite.
The paper combines flow based and energy based models to generate molecular conformations given a molecular graph. For this, a continuous flow model is used to map the graph based molecular representation into a distribution over conformations.  An energy based model (EBM) is used to further help the model capture long range atomic interactions. The proposed method is compared with strong baselines: CVGAE, GraphDG, and RDKit.  The authors addressed most of the reviewers  concerns in the rebuttal.  All the reviewers agree on acceptance.
The reviewers are in general impressed by the results and like the idea but they also express some uncertainty about how the proposed actually is set up. The authors have made a good attempt to address the reviewers  concerns. 
This paper a distillation framework where a light weight student model is trained to handle easy (frequent) instances, while the large teacher model is still used to handle the more difficult (rare) inputs. The models are trained to perform well in this two stage inference setting. Experiments are conducted on computer vision and NLP tasks. While the idea is potentially interesting, the experimental results are fairly weak and not very convincing.
This paper proposes a noise aware knowledge graph embedding (NoiGAN) by combining KG completion and noise detection through the GANs framework. The reviewers find that the idea is interesting, but the comparison to SOTA is largely missing. The paper can be improved by addressing the reviewer comments. 
This paper proposes to incorporate graph topology into pooling operations on graphs, to better define the notion of locality  necessary for pooling.  While the paper tackles an important problems, and seems to be also well written, the reviewers agree that there are several issues regarding the contribution and empirical results that need to be addressed before this paper is ready for publication.
This paper deals with multi agent hierarchical reinforcement learning. A discrete set of pre specified low level skills are modulated by a conditioning vector and trained in a fashion reminiscent of Diversity Is All You Need, and then combined via a meta policy which coordinates multiple agents in pursuit of a goal. The idea is that fine control over primitive skills is beneficial for achieving coordinated high level behaviour.  The paper improved considerably in its completeness and in the addition of baselines, notably DIAYN without discrete, mutually exclusive skills. Reviewers agreed that the problem is interesting and the method, despite involving a degree of hand crafting, showed promise for informing future directions.   On the basis that this work addresses an interesting problem setting with a compelling set of experiments, I recommend acceptance.
This is an interesting paper that shows how improved off policy estimation (and optimization) can be improved by explicitly estimating the data logging policy.  It is remarkable that the estimation variance can be reduced over using the original logging policy for IPW, although this result depends on the (somewhat impractical) assumption that the parametric form for the true logging policy is known.  The reviewers unanimously recommended the paper be accepted.  However, there remain criticisms of the theoretical analysis that the authors should take into account in preparing a final version (namely, motivating the assumptions needed to obtain the results, and providing stronger intuitions behind the reduced variance).
Overall the paper present the idea of caching and using stale information to update instead of sub sampling for speeding up graph convolution neural network. Reviewers liked the idea but also there were concerns about experimental comparisons.  In the rebuttal the authors did provide more evidence of comparison with other caching based and other relevant baselines. Overall the importance of scaling up GCNN and empirical results helped the paper cross the high bar.
This paper attempts to rationalize data augmentation techniques for compositional generalization by evoking the principle of meaningful learning which posits that learning new concepts builds on previously learned concepts (which learners already understand). So for compositional generalization, this means that a model exposed to some new concepts in the test set, should link them to known concepts which have been already attested in the training set. The links between concepts are presumed to be semantic, e.g., hyponyms, hypernyms, or lexical variants. Ideally, a model should perform semantic linking on its own, however the authors do not propose a linking mechanism. Rather they investigate data augmentation as a way of exposing a model to semantic links and then explore whether different operationalizations of semantic linking enable the model to generalize better. Inductive learning is a bottom up approach, where links are created from general to specific concepts, whereas deductive learning is a top down approach where links are created from specific to general concepts. Experimental results indicate that inductive learning works better.   The reviewers had the following issues with the submission (a) the technical contribution is not very strong (the idea of data augmentation is not new, although the authors  meaningful learning perspective is) (b) semantic linking seems to be able to handle only cases pertaining to lexical generalization (even though the authors include examples with structural generalization in their splits, there is no reason why semantic linking could handle these cases); (c) it would be more interesting/useful  to learn the linking than assume it is given. The authors did their best to respond to the criticism, but ultimately addressing the criticism is future work. I would also recommend to take a look at this dataset which might be useful for machine learning experiments: https://arxiv.org/abs/2105.14802
The paper provides a new geometric functional analysis perspective for the generalization bounds for neural networks. As the AC, I actually quite liked the twist the authors are providing for this particular work. Unfortunately, the current presentation is too crude to provide an elementary picture for the developments and I strongly encourage the authors to revise the paper for the next deadline based on the remarks from the reviewers.
Given the reviewer s exchange with the authors, and my own examination of the paper, I don t think that it can be accepted in the present form.  First, since this paper aims at solving an optimization problem (for which existing methods exist, with theoretical guarantees) via a NN, it is important to compare appropriately to those methods, which is not done here.  Further, there are possible issues when applying these only to 2D data, and it is possible that it would not extend appropriately to other types of geometries, and costs in OT problems.
While the reviewers found parts of the paper interesting, the main concern about this paper was lack of novelty and marginal improvements obtained by the proposed methods.
The aim of this paper is to propose a novel "GW" like discrepancy function between probability measures living in different spaces (here restricted to be Euclidean, with a squared euclidean distance as the base metric). While interesting (notably the idea of learning distinct maps mapping a random direction in a latent space onto two spaces) there are a few issues with presentation, incremental nature of work and importantly a few shortcomings in the empirical evaluation as detailed by reviewers. Hopefully these can be used to improve the draft for a future version.
This paper centers on an unbiased variant of the Mutual Information Neural Estimation (procedure), using the so called "eta trick" applied to the Donsker Varadhan lower bound on the KL divergence. The paper s contribution is mainly theoretical though experiments are presented on synthetic Gaussian distributed data as well as CIFAR10 and STL10 classification experiments (from learned representations).  R1 s criticism of the theoretical contributions centers on fundamental limitations on finite sample estimation of the MI, contending that the bounds simply aren t meaningful in high dimensional settings, and that the empirical work centers on synthetic data and self generated baselines rather than comparisons to reported numbers in the literature; they were unswayed by the author response, which contended that these criticisms were based on pessimistic worst case analysis and that "mild assumptions on the mutual information and function class" could render better finite sample bounds. Some of R3 s concerns were addressed by the author rebuttal and associated updates, but remained critical of the presentation, in particular regarding the dual function, and downgraded their score.  Because R2 disclosed that they were outside of their area of strong expertise, a 4th reviewer was sought (by this stage, the paper was the revised version). Concerns about clarity persisted, with R4 remarking that a section was "a collection of different remarks without much coherence, some of which are imprecisely stated". R4 felt variance and sample complexity should be dealt with experimentally, though this was not directly addressed in the author response. R4 also remarked that the plots were difficult to read and questioned the utility of supervised representation learning benchmarks at assessing the quality of MI estimation, given recent evidence in the literature.  The theoretical contributions of this submission are slightly outside the bounds of my own expertise, but consensus among three expert reviewers appears to be that the clarity of exposition leaves much to be desired, and I concur with their assessment that the empirical investigation is insufficiently rigorous and does not draw clear comparisons to existing work in this area. I therefore recommend rejection.
This paper presents FinBERT, a BERT based model that is further trained on a financial corpus and evaluated on Financial PhraseBank and Financial QA. The authors show that FinBERT slightly outperforms baseline methods on both tasks.  The reviewers agree that the novelty is limited and this seems to be an application of BERT to financial dataset. There are many cases when it is okay to not present something entirely novel in terms of model as long as a paper still provides new insights on other things. Unfortunately, the new experiments in this paper are also not convincing. The improvements are very minor on small evaluation datasets, which makes the main contributions of the paper not enough for a venue such as ICLR.  The authors did not respond to any of the reviewers  concerns. I recommend rejecting this paper.
All reviewers agreed that the novelty of the method was not at the level expected for publication, and also raised a number of technical concerns regarding the approach. There was no response from the authors on these issues, hence the reviewer consensus is that the paper is not ready for publication at this time.
This paper extends the idea of influence functions (aka the implicit function theorem) to multi stage training pipelines, and also adds an L2 penalty to approximate the effect of training for a limited number of iterations.  I think this paper is borderline.  I also think that R3 had the best take and questions on this paper.  Pros:    The main idea makes sense, and could be used to understand real training pipelines better.    The experiments, while mostly small scale, answer most of the immediate questions about this model.  Cons:    The paper still isn t all that polished.  E.g. on page 4: "Algorithm 1 shows how to compute the influence score in (11). The pseudocode for computing the influence function in (11) is shown in Algorithm 1"    I wish the image dataset experiments had been done with larger images and models.  Ultimately, the straightforwardness of the extension and the relative niche applications mean that although the main idea is sound, the quality and the overall impact of this paper don t quite meet the bar.
The authors provide a new analysis of learning of two layer linear networks with gradient flow, leading to some novel optimization and generalization guarantees incorporating a notion of the imbalance in the weights.  While there was some diversity of opinion, the prevailing view was that the results were not sufficiently significant for publication in ICLR.
Many existing approaches in multi task learning rely on intuitions about how to transfer information. This paper, instead, tries to answer what does "information transfer" even mean in this context. Such ideas have already been presented in the past, but the approach taken here is novel, rigorous and well explained.  The reviewers agreed that this is a good paper, although they wished to see the analysis conducted using more practical models.   For the camera ready version it would help to make the paper look less dense. 
This paper tackles a bandit problem that incorporates three challenges motivated by common issues encountered in online recommender systems: delayed reward, incentivized exploration, and self reinforcing user preference. The authors propose an approach called UCB Filtering with Delayed Feedback (UCB FDF) for this problem and provide a theoretical analysis showing that UCB FDF achieves the optimal regret bounds. Their analysis also implies that logarithmic regret and incentive cost growth rates are achievable under this setting. These theoretical results are supported by empirical experiments, e.g. using Amazon review data. The main concern with this paper is that the considered challenges have all been tackled already in different bandit settings, so the novelty here is that they are being tackled altogether. It would be more convincing if experiments included baselines from these existing settings to motivate the need for a new strategy rather than simply relying on methods that have been proposed previously to address each of these problems independently; the experiments currently contain only a baseline for bandits with self reinforcing user preference, which has been added during the rebuttal phase.
The paper presents a framework for incorporating physics knowledge (through, potentially incomplete, differential equations) into the deep kernel learning approach of Wilson et al. The reviewers found the paper addresses an important problem and presents good results.  However, one of the main issues raised by R1 is that, although the proposed method can be applied to broader settings such as that of incomplete differential equations, there are still regimes where the comparison is not only possible but perhaps insightful. An example baseline is the work of Lorenzi and Filippone, “Constraining the Dynamics of Deep Probabilistic Models” (ICML, 2018). Another critical issue, raised by R4, is the insufficient clarity in the presentation. Many of the concerns raised by this reviewer were clarified in the discussion and I thank the authors for their engagement. However, the AC believes some of the points raised by R4 in this regard were left unaddressed in the paper and the manuscript does indeed require at least one more iteration.  The format violation concerns raised during the reviewing process did not affect the decision on this paper, as the PCs confirmed that they did not meet the bar for desk rejection and recommended to assess the paper on its technical merits.
This paper proposed a method that creates neural networks that can run under different resource constraints. The reviewers have consensus on accept. The pro is that the paper is novel and provides a practical approach to adjust model for different computation resource, and achieved performance improvement on object detection. One concern from reviewer2 and another public reviewer is the inconsistent performance impact on classification/detection (performance improvement on detection, but performance degradation on classification). Besides, the numbers reported in Table 1 should be confirmed: MobileNet v1 on Google Pixel 1 should have less than 120ms latency [1], not 296 ms.    [1] Table 4 of https://arxiv.org/pdf/1801.04381.pdf
This paper proposes to employ affinity cycle consistency(ACC) for extracting active (or shared) factors of variation across groups. Experiments shows how ACC works in various scenarios.  Pros:   The problem is important and relevant.   The paper is well written.   The proposed method is simple and effective.  Cons:   The experimental section is weak:  It lacks an ablation to validate the contribution of ACC and discussion on   why the method works and the scalability of the proposed method to more complex cases.   The novelty is limited because the proposed ACC is similar to previous work temporal cycle consistency(TCC).   The paper missed some implementation details and could be difficult to reproduce without code  provided.  Reviewers raised the concerns listed in Cons. The authors conducted additional experiments and added more discussions on the experimental results in the revised paper. The authors also explained that ACC is more general than TCC. However, the reviewers were not convinced by the rebuttal and kept their original ratings.  Due to the two main weaknesses   limited novelty and weak experimental analysis, I recommend reject.
In this paper, the authors study how to incorporate experimental data with interventions into existing pipelines for DAG learning. Mixing observational and experimental data is a well studied problem, and it is well known how to incorporate interventions into e.g. the likelihood function, along with theoretical guarantees and identifiability. Ultimately there was a general consensus amongst the reviewers that without additional theoretical results to advance the state of the art, the contribution of this work is limited.
This is a well written paper with good experimentation.  The paper builds on the work of FedDF and does ablation studies to demonstrate its improvements.  The key original idea is the use of a common pool of unlabeled data which is used in transmitting partial results between local and global servers.  The results seem pretty good.  From a practical viewpoint, the unlabelled common data will, in most cases, need to be generated/artificial data since it will need to be public (to the other servers at least).  This option should be tested to demonstrate feasability.  AnonReviewer2 was concerned about whether it was fair to provide additional unlabelled data.  The authors tested this out and showed it was OK.  Regardless, the different servers could easily generate artificial data for this purpose.  AnonReviewer1 had a number of issues which the authors largely addressed. The other two reviewers appreciated the paper.  All reviewers gave constructive suggestions. 
This paper finally received divergent and borderline reviews with two positive (6) and two negative (3) rates. After the thorough reviews by ACs ourselves, we would like to decide to reject this work at this time, even though this submission has a lot of potentials including intensive analyses on instance segmentation frameworks and architectures.  We first would like to appreciate comprehensive author’s responses and additional empirical results. They should be extremely helpful to make this submission stronger. Here are some of our suggested points for improvement: (i) The novelty, significance, and practical implications of this work (compared to previous analysis work) may need to be better presented in a more persuasive way. (ii) Nuance of stylization transformation can be better explained compared to other types of perturbations or transformations. (iii) Empirical fairness can be better justified. (iv) Since the paper is written in a highly condensed way, some of reduction may improve the readability. (v) Finally, given that this paper focuses on empirical study about instance segmentation, it may be more appreciated in a computer vision venue.
This paper presents a framework for navigation that leverages learning spatial affordance maps (ie what parts of a scene are navigable) via a self supervision approach in order to deal with environments with dynamics and hazards. They evaluate on procedurally generated VizDoom levels and find improvements over frontier and RL baseline agents.  Reviewers all agreed on the quality of the paper and strength of the results. Authors were highly responsive to constructive criticism and the engagement/discussion appears to have improved the paper overall. After seeing the rebuttal and revisions, I believe this paper will be a useful contribution to the field and I’m happy to recommend accept. 
The authors extend the result of Ongie et al. (2019)  and derive sparseneural network approximation bounds that refine previous results. The reuslts are quite ineteresting and relevant to ICLR. All the reviewers were positive about this paper.
To address the problem of unauthorized use of data, methods are proposed to make data unlearnable for deep learning models by adding a type of error minimizing noise. Based on th fact that the conferred unlearnability is found fragile to adversarial training, the authors design new methods to generate robust unlearnable examples that are protected from adversarial training. In addition, considering the vulnerability of error minimizing noise in adversarial training, robust error minimizing noise is then introduced to reduce the adversarial training loss. The authors have tried to respond to reviewers  comments along with adding more experiments. Overall, this manuscript finally gets three positive reviews and one negative review, where the possible vulnerability or robustness of error minimizing noise against (simple) image processing operations was not verified. In comparison with other manuscripts I m handling that got consistent positive comments, this manuscript is still recommended to be accepted (poster) with a further study of robustness under simple image transformations in the final version.
This paper studies a generalization of the randomized SVD algorithm with non standard Gaussian vectors, which is then used to incorporate any covariance matrix and to Hilbert Schmidt operators.  It uses a new kernel related to products of weighted Jacobi polynomials; and extensive numerical experiments further strengthen the case for this generalization.  Reviewers had initial concerns that were addressed, and the method should be of broad interest.
The paper introduces a new loss, Maximum Categorical Cross Entropy, which combines the usual cross entropy loss with a maximum entropy regularisation term on the convolutional kernels, and is evaluated on image classification. The authors have trained a face classification algorithm on two datasets: UTKFace (https://susanqq.github.io/UTKFace/) and NIST colorFERET (https://www.nist.gov/itl/products and services/color feret database). The labels consisted in, respectively: White, Black, Indian, Asian, Others (over 18k images) and Asian, Asian Middle Easter, Black of African American, White, Hispanic (over 11k images) (see section 4.1 of the paper).  From the meta reviewer s perspective: As stated in the title, abstract and in paragraph 3 of section 1, the motivation of the paper is to reduce model overfitting and racial bias towards one category. However, there is no further discussion about any "ethical, societal and practical concerns when dealing with facial datasets, especially for the task of race or gender classification". It seems to me that a paper that implements a "race classification" algorithm should at least devote a substantially long part of the discussion on the validity of such a task and of such a labelling process, as well as question the motivations and potential misuses. Who labeled these faces and based on what visual characteristics? Were the subjects of the photographs consenting and did they self declare their ethnicities? Are the authors simply reproducing discredited phrenology assumptions about ethnicities and about "race", which is increasingly defined as a mere social construct? Given that there is nothing specific to face classification in the loss function, I wonder why did the authors decide to focus on ethnicity features? What exactly could a visual ethnicity classifier be used for? Given the sheer amount of questions raised by the paper, we have submitted it for review by the Ethics Board.  Summary of the reviews: Reviewers gave scores scores 3, 4, 5, 5 (without rebuttal from the authors), raising concerns about the novelty and contribution of the method (as it is simply combining maximum entropy with cross entropy), clarity of the explanation of the method, missing related work and baselines and evaluation metrics.  Based on the low scores, unfavourable reviews and an ongoing Ethics Board investigation, I recommend for this paper to be rejected.    While this paper is likely to be rejected (, I believe that these concerns should be raised and potentially reviewed by the Ethics Board (unless this is an obvious rejection). Thank you in advance for your time.
There are some interesting ideas raised on continuous time models with latent variables in machine learning. However, the reviewers argue, and I agree, that the connection to causal models as typically required in applications about the effects of interventions is not addressed with as much care as it might have been needed.
The paper builds up on the gated fusion network architectures, and adapt those approaches to reach improved results.  In that it is incrementally worthwhile.  All the same, all reviewers agree that the work is not yet up to par.  In particular, the paper is only incremental, and the novelty of it is not clear.  It does not relate well to existing work in this field, and the results are not rigorously evaluated; thus its merit is unclear experimentally.  
Summary: This paper provides an interesting and unique challenge problem on human AI collaboration, with sample baselines. I think this is an extremely important topic and the community should embrace such challenge problems.  Discussion: Reviewers agreed this paper should be accepted, particularly after seeing that ICLR has accepted such challenge papers in the past.  Recommendation: I d really like to see this get a spotlight as it would be great to highlight this innovative challenge to the community. 
The authors propose a new method for learning hierarchically disentangled representations. One reviewer is positive, one is between weak accept and borderline and two reviewers recommend rejection, and keep their assessment after rebuttal and a discussion. The main criticism is the lack of disentanglement metrics and comparisons. After reading the paper and the discussion, the AC tends to agree with the negative reviewers. Authors are encouraged to strengthen their work and resubmit to a future venue.
Reviewers uniformly suggest acceptance. Please take their comments into account in the camera ready. Congratulations!
This work explores an auto regressive density estimator based on transformer networks. The model is trained via MLE with an additional MMD regularization term.  Various experiments are performed on small benchmarks and show good results on density estimation. It is great to see that such a simple model is indeed very effective for density estimation on various small benchmarks (such as 2D density estimation and MNIST).   The ablation experiments are informative and justify some of the model choices (such as the use of RNN to encode "positions"). Experiments are nicely chosen and paint a broad picture of the behaviour of the studied model.  The paper and author responses, however, excessively exaggerate the extent to which these results are relevant to the bigger picture in comparison to existing literature (e.g. flows and existing auto regressive models).  As it has been extensively discussed with the reviewers, the proposed model is a straightforward application of a transformer network to auto regressive modelling, this is specially so in light of existing work on auto regressive models with transformers [e.g 1, 6, 8], self attention [e.g 2]. BERT [7] itself can be used for auto regressive modelling almost out of the box (with the appropriate choice of masks during training).  At various points in the paper and author responses, it refers to flow models as "complicated/expensive" counterparts. These arguments are unfounded: auto regressive models are particular cases of flows [3], and there are no obstructions to using transformer networks inside flows (in fact they have been already used, to achieve permutation equivariance and long range correlations [e.g. 4]).  The paper leaves comparisons to spline flows out, arguing they are "hard to implement". This is quite conspicuous, as not only spline flows are straightforward to implement, they produce results entirely on par with the presented model (as an example, look at Fig 2 from [9] in comparison to Fig 1 from this paper). Finally, the paper also misses an important discussion about the computational complexity of the proposed method. Auto regressive models are considerably slower to sample from in relation to other types of directed models. Even more so with transformer networks as conditioners. For instance, flows [3, 5] allow for substantially faster sampling of large dimensional data relative to auto regressive models (by exploiting parallel sampling).   Extra comments:  The paper says "... Self attention also enables permutation equivariance and naturally enables TraDE to be agnostic to the ordering of the features ... " This is true only for a *single* conditional $p(x_i | \text{Transformer}(x_{0 \ldots (i 1)}))$, not for the *joint* density. It is actually not straightforward to build auto regressive models that are permutation invariant or that incorporate other forms of domain knowledge in general. As an example, see [4] for how transformers and spline flows can be used to produce exact permutation invariant densities.   [1] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D. and Sutskever, I., 2020, November. Generative pretraining from pixels. In International Conference on Machine Learning (pp. 1691 1703). PMLR.  [2] Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer, N., Ku, A. and Tran, D., 2018. Image transformer. arXiv preprint arXiv:1802.05751.  [3] Papamakarios, G., Nalisnick, E., Rezende, D.J., Mohamed, S. and Lakshminarayanan, B., 2019. Normalizing flows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762.  [4] Wirnsberger, P., Ballard, A.J., Papamakarios, G., Abercrombie, S., Racanière, S., Pritzel, A., Rezende, D.J. and Blundell, C., 2020. Targeted free energy estimation via learned mappings. arXiv preprint arXiv:2002.04913.  [5] Huang, C.W., Krueger, D., Lacoste, A. and Courville, A., 2018. Neural autoregressive flows. arXiv preprint arXiv:1804.00779.  [6] Sun, C., Myers, A., Vondrick, C., Murphy, K. and Schmid, C., 2019. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE International Conference on Computer Vision (pp. 7464 7473).  [7] BERT: Pre training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming Wei Chang, Kenton Lee, Kristina Toutanova; ACL 2019.  [8] Child, R., Gray, S., Radford, A. and Sutskever, I., 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.  [9] Durkan, C., Bekasov, A., Murray, I. and Papamakarios, G., 2019. Neural spline flows. In Advances in Neural Information Processing Systems (pp. 7511 7522).
The paper received mixed ratings. The proposed idea is quite reasonable but also sounds somewhat incremental. While the idea of separating foreground/background is reasonable, it also limits the applicability of the proposed method (i.e., the method is only demonstrated on aligned face images). In addition, combining AdaIn with foreground mask is a reasonable idea but doesn’t sound groundbreakingly novel. The comparison against StarGAN looks quite anecdotal  and the proposed method seems to cause only hairstyle changes (but transfer with other attributes are not obvious). In addition, please refer to detailed reviewers’ comments for other concerns. Overall, it sounds like a good engineering paper that might be better fit to computer vision venue, but experimental validation seems somewhat preliminary and it’s unclear how much novel insight and general technical contributions that this work provides. 
The reviewers all argued for acceptance citing the novelty and potential of the work as strengths.  They all found the experiments a little underwhelming and asked for more exciting empirical evaluation.  The authors have addressed this somewhat by including multi modal experiments in the discussion period.  The paper would be more impactful if the authors could demonstrate significant improvements on really challenging problems where MCMC is currently prohibitively expensive, such as improving over HMC for highly parameterized deep neural networks.  Overall, however, this is a very nice paper and warrants acceptance to the conference.
The paper proposes a method for learning convolutional networks with dynamic input conditioned filters. There are several prior work along this idea, but there is no comparison agaist them. Overall, experimental results are not convincing enough.
The paper proposes a novel curriculum learning method for RL based on the concept of boosting. The proposed method builds on the curriculum value based RL framework and uses boosting to reuse action values from previous tasks when solving the current task. The method is analyzed theoretically in terms of approximation accuracy and convergence. Moreover, extensive experiments demonstrate the effectiveness of the method. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers  questions and increased their overall assessment of the paper. At the end of the discussion phase, there was a clear consensus that the paper should be accepted. The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper.
The paper studies multi label classification problem. Particularly, they introduce multi label box model, which uses probabilistic semantics of box embeddings, representing labels as boxes instead of vectors. Their model is evaluated extensively on 12 datasets, and reviewers agreed the paper was well written and well motivated. While it is pretty straightforward application of box embeddings to multi label problem, it is well motivated and the paper adds to the existing literature on box embeddings.   Reviewer Eo7g had a concern with experimental setting, including missing a baseline Abboud et al. (2020). Even after the baseline was added, the reviewer was not convinced about the model’s performance, as the baseline was not extensively tuned. The authors responded with the heavy computational costs of associated with tuning the margin. Reviewer X5cP also pointed the problem with HMC dataset, for which the experimental results should be updated. Given the issues, the paper could benefit from another round of revisions.
The submission proposes a method to improve over a standard binary network pruning strategy by the inclusion of a structured matrix product to encourage network weight sparsification that can have better memory and computational properties.  The idea is well motivated, but there were reviewer concerns about the quality of writing and in particular the quality of the experiments.  The reviewers were unanimous that the paper is not suitable for acceptance at ICLR, and no rebuttal was provided.
The paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model based RL methods.  The experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with clearer implications of representation learning.   There are concerns on writing style and comprehension.    The work is on the one hand very specialized, on the other hand just an incremental modification of existing methods.    The presentation is very dense and quite hard to grasp, even with the Appendix.   The formalism, while important, can be very loose in terms of bounds. While that does open questions in RL theory, it would be useful for authors to be more candid about this fact in the paper.  I would recommend including the response to R1 in the paper.  Other relevant and concurrent papers to potentially take note of:   Fine Tuning Offline Reinforcement Learning with Model Based Policy Optimization (https://openreview.net/forum?id wiSgdeJ29ee)    Robust Offline Reinforcement Learning from Low Quality Data (https://openreview.net/forum?id uOjm_xqKEoX)  Given the overall positive reviews, I would recommend acceptance. However, the method would benefit from additional pass on re writing to make the manuscript more accessible, which in turn to increase impact of this work. 
The paper lies on the borderline. An accept is suggested based on majority reviews and authors  response.
This submission tackles an important problem and presents interesting ideas. I am confident that the research will lead to good publications. However, in the particular situation here, AnonReviewer2 had serious concerns that are shared by me. The authors made a great effort to clarify the situation, but the current situation still leaves me uncertain about the presentation and correctness of everything. Because some issues were major, it is not easy to re evaluate and take new conclusions in the short time of this process. I hope the authors do not take this too negatively, but given all the comments and discussions, it is best that another round of improvements and reviews be conducted.
A method for efficient exact computation of the generalized Gauss Newton matrix is given. Using this method the authors provide several empirical observations of first and second order statistics of neural networks during training. Additionally the authors use to tool to propose a new damping technique that some reviewers found particularly interesting. Reviewers noted that the low rank decomposition the authors provide is not new, and has been used in prior work, although the trick may not be widely known within the deep learning community. As such novelty is not a strength of the work, and reviewers suggested the authors could strengthen the work with a convincing demonstration that the method can be made to work at scale, as well as providing more detailed run time and memory comparisons with other approaches to calculating the GGN matrix. Although the authors agreed with reviewer suggestions, the paper was not updated during the rebuttal period. As such I recommend the authors resubmit with the proposed revisions.
Although by now there are several approaches for comparing probability distribution, the paper innovates by making their measure take into account the decision space and loss functions directly. The paper also frames its contribution within the literature at large. Reviewers were unanimous that the result is of major interest to the ICLR audience.
The authors empirically analyse the properties of datasets which lead to poor calibration. In particular, they show that high class imbalance, high degree of label noise, and small dataset size are all likely to lead to poor overall calibration or poor per class calibration. While there are some interesting insights in this work, the reviewers argued that the contribution is not substantial enough for ICLR. To improve the manuscript the authors should consider accuracy and calibration jointly and extend the results pertaining to label noise which were appreciated by the reviewers. For the former, the same conclusions hold for accuracy, instead of calibration, which raises the question of their relationship   is there a tradeoff? For the latter, the reviewers pointed to a concrete extension with structured label noise. Finally, the theoretical analysis is a step in the right direction, but the assumption on the width of the network required to fit the training set is too restrictive in practice. Therefore, I will recommend rejection.
The contribution of this paper basically consists of using MLPs in the attention mechanism of end 2 end memory networks. Though it leads to some improvements on bAbI (which may not be so surprising   MLP attention has been shown preferable in certain scenarious), it does not seem to be a sufficient contribution. The motivation is also confusing   the work is not really that related to relation networks, which were specifically designed to deal with situations where *relations* between objects matter. The proposed architecture does not model relations.  +  improvement on bAbI over the baselines    limited novelty (MLP attention is fairly standard)    the presentation of the idea is confusing (if the claim is about relations  > other datasets need to be considered)  There is a consensus between reviewers. 
The proposed method, Differentiable Symbolic Execution (DSE), addresses the safety of learned navigation and control programs. The approach samples code paths using a softened probabilistic version of symbolic execution,  constructing gradients of a "safety loss" along these paths, and then backpropagating these gradients through program operations using RL.   Pros    The paper is well written and sound    The issue of safety is underexplored    The method improves over a strong baseline on benchmarks  Cons    The benchmarks are relatively small scale and artificial
Inspired by the observtion that the poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end to end supervised training paradigm, the authors propose a novel defense method based on contrastive learning and decouple end to end training to defend against backdoor attacks. The issues, including training time, difference from certain previous studies, ablation study, and so on, raised by the reviewers have been properly addressed and the reviewers are satisfied with the responses from the authors. According to the consistent positive opinions from the reviewrs, this manuscript is recommended to accept.
This paper proposes the framework CAGE (causal probing of deep generative models) for estimating counterfactuals and unit level causal effects in deep generative models. CAGE employs geometrical manipulations within the latent space of a generative model to estimate the counterfactual quantities. The estimator is written in potential outcome language and assumes unconfoundedness, positivity, stable unit treatment value assumption (SUTVA), and linear separability in semantic attributes of the latent space. Furthermore, the framework considers only the case of binary treatments.    One major concern raised by reviewers TgM5 and xP5d is that the method is based on a trained generative model, which may not be the true data generating model. In this case, the paper appears to address statistical dependencies instead of the actual causal relationships in the real world. The authors claim to empirically show that their framework can probe unit level (individual) causal effects. However, the reviewers are concerned that no theoretical support for the correctness of the method is provided. In other words, the problem is assumed away once a probabilistic model is assumed to be equal to the true generative model, which is almost never the case in practice and is well known in the field. We want to encourage the authors to provide a more detailed theoretical justification, perhaps with proofs and/or references, that the proposed method can infer causal and counterfactual relationships given the underlying assumptions.   After all, reviewers were interested but somewhat skeptical about the method s ability to learn causal and counterfactual relationships. Unfortunately, the paper is not ready for publication yet. Still, we would like to encourage the authors to take the reviews seriously and try to improve the manuscript accordingly.
The authors propose techniques to deal with binarization of 3D point clouds and propose EMA and layer wise scale recovery that improve results across the board for PointNet style models. An accept.
The reviewers agree that the paper, in its current form, is not strong enough to allow for publication.  There are specific weaknesses that need to be tackled: a better correlation study; a clearer relationship to existing literature (and improvement on the novelty); clearer, more precise use of descriptions.  The authors are encouraged to continue with their work and submit a more mature manuscript.
All three reviewers recommend rejection, based on multiple (mostly shared) concerns. While the authors address the concerns in their rebuttal, the unanimously negative scores remain. I don t see basis to accept the paper.
The authors present a method for self supervised learning of representations of 2D projections of 3D objects. By performing known 3D transformations of an object of interest, a encoder/decoder network is trained to estimate the applied transformation from a series of 2D projections. The proposed method is used as a regularizer and experiments are performed on supervised 3D object classification and retrieval.     After seeing each others’ reviews, one of the main concerns from the reviewers was the relationship between the proposed method and Zhang et al., CVPR 2019 (i.e. AET). The two methods are conceptually very similar, and the consensus from the reviewers is that the authors did not acknowledge the overlap sufficiently and also did not provide a convincing argument as to why they think the approaches are different.   In their rebuttal the authors provided some additional results on real data which is a valuable and welcome addition. However, there were still other concerns that the reviewers had e.g. R2 wanted to know why the model could not be applied directly to 3D shapes instead of 2D projections.   Given the above concerns (specifically the relationship to AET), there is currently not enough support for accepting the paper in its current form. The authors have received detailed feedback and are encouraged to take it onboard when revising the paper in future.   
This paper studies deep convolutional architectures to perform compressive sensing of natural images, demonstrating improved empirical performance with an efficient pipeline.  Reviewers reached a consensus that this is an interesting contribution that advances data driven methods for compressed sensing, despite some doubts about the experimental setup and the scope of the theoretical insights. We thus recommend acceptance as poster. 
 The paper proposes an augmented adversarial reconstruction loss for training a stochastic encoder decoder architecture. It corresponds to a discriminator loss distinguishing between a pair of a sample from the data distribution and its augmentation and pair containing the sample and its reconstruction. The introduction of the augmentation function is an interesting idea, intensively tested in a set of experiments, but, as two of the reviewers pointed out, the paper could be improved by deeper investigation of the augmentation function and the way of choosing it, which would increase significance of the contribution. 
The paper considers the setting of constrained MDPs and proposes using backward value functions to keep track of the constraints.  All reviewers agreed that the idea of backward value functions is interesting, but there were a few technical concerns raised, and the reviewers remained unconvinced after the rebuttal. In particular, there were doubts whether the method actually makes sense for the considered problem (the backward VF averaging constraints over all trajectories, instead of only considering the current one), and a concern about insufficient baseline comparisons.  I recommend rejection at this time, but encourage the authors to take the feedback into account, make the paper more crisp, and resubmit to a future venue.
The paper presents multi agent RL framework that uses the divergence between the learned policies and a target policy as a penalty that pushes the agent to learn cooperative strategies. The proposed method is built on top of an existing one (DAPO, Wang et al., 2019). Empirical experiments clearly show the advantage of the proposed method.  The reviews for this paper are mixed and borderline. The reviewers appreciate the experiments reported in the paper and that indicate the advantage of the proposed method. But two reviewers do not think that the proposed analysis is sufficiently novel compared to an existing one (DAPO). The responses provided by the authors were appreciated, but did not dissipate these concerns.
This paper proposes a method to leverage the Lead (i.e., first sentence of an article) in training a model for abstractive news summarization.   Reviewers  initial recommendations were weak reject to weak accept, pointing out the limitations of the paper including 1) little novelty in modeling, 2) weak evaluation, and 3) lack of deep analysis. After the author rebuttal and revised paper, one of the reviewers increased the score and were leaning toward weak accept.   However, reviewers noted that there was significant overlap with another submission, and we discussed that it would be best to accept one of the two, incorporating the contributions of both papers. Hence, I recommend that this paper not be accepted, and perhaps some of the non overlapping contents of this paper can be included in the other, accepted paper.  Thank you for submitting this paper. I enjoyed reading it.
The idea behind this paper is to develop a training algorithm that chooses among a fixed set of weights for each true weight in a neural network.  The results are reasonable   though difficult to quantify as either good or surprising   performance from the algorithm. A perhaps interesting point is that additional fine tuning from these found networks can, in some cases, best the accuracy of the original network.  The pros of this paper are that it is a neat original idea. With the exception of the limited scale of the benchmarks (i.e., the selected architectures), the paper is largely well executed.  The primary shortcoming of the paper, as discussed by the reviewers, is the lack of clarity in its implications. Specifically, it is difficult to position the result as contributing to a practical aim or leading to additional future work.    Based on the reviews and discussion, my recommendation is Reject. In particular, this paper would be significantly improved by bringing in a strong motivational context and, therefore, additional comparisons.  For example, the context for the work of Ramanujan et al. (2019) is that, perhaps, it is possible to find subnetworks of large initialized networks that will permit more efficient training. In Appendix A, this paper proposes that the technique here could be cast as pruning within a much larger network. Following results from Zhu and Gupta [1] and also Ramanujan et al. (2019), finding a sparse network within a larger network can produce a more accurate network than training a network of equivalent size to the sparse. Therefore, these results could, potentially, be cast and as a more efficient way to perform the techniques of Ramanujan et al. (2019).   Alternatively, the results that demonstrate that fine tuning the identified networks improves performance over the standard network could be more robustly evaluated and perhaps cast as either an alternative training technique or leveraged as a technique like warm starting [2].  This is a very interesting and promising direction. It appears that the paper just needs a bit more distillation.  [1] To prune, or not to prune: Exploring the efficacy of pruning for model compression. Michael Zhu and Suyog Gupta. In International Conference on Learning Representations Workshop Track, 2018.  [2] On Warm Starting Neural Network Training. Jordan T. Ash, Ryan P. Adams. NeurIPS 2020   
This paper proposed a novel phase oriented algorithm to efficiently construct imperceptible audio adversarial attacks. It leverages the spectrogram consistency of STFT to adversarially transfer phase perturbations to the adjacent frames and dissipate the energy that is crucial for ASR systems. Empirical evaluations show that the attack effectiveness of the proposed attack is high.  As agreed by the reviewers the method is very interesting, but the experimental justification is limited, lacking strong SOTA baseline ASR systems, different ASR model architectures, the adversarial transferability analysis etc. The author did add DeepSpeech 2 results to the initial version s Low rank Transformer only results, which is still not convincing enough. The author also commented that they will "add more comprehensive experiments with different systems and …architectures in our next version of the paper" which is not in the current paper yet. Hence, resubmission with more experimental evaluations is recommended.   The decision is mainly due to the weak experimental justification.
This paper proposes to learn continuous of k mer embeddings for RNA seq analysis. Major concerns of the paper include: 1. novelty seems limited; 2. questions about the scalability of the approach; 3. evaluation experiments were not suitable for supporting the aim. Overall, this paper cannot be accepted yet. 
The reviewers all agreed that although there is a sensible idea here, the method and presentation need a lot of work, especially their treatment of related methods.
This paper proposes benchmark tasks for offline reinforcement learning.  The paper has major strength and weakness, and it has resulted in very active discussion among reviewers, authors, and other participants.  The major strength includes the following:   The proposed benchmark is already heavily used in the community   Offline reinforcement learning is very important to solve reinforcement learning tasks in the real world   The paper covers a range of tasks and provides through evaluation of existing methods to be used as baselines  The major weakness is that it is not sufficiently convincing that the methods that perform well in the proposed benchmark tasks will perform well in the offline reinforcement learning tasks in the real world.    This is partly due to the nature of the benchmark tasks of offline reinforcement learning, which require simulators to evaluate the policies learned with offline reinforcement learning.  This means that one cannot simply collect datasets from real world tasks and provide them as benchmark datasets.    Although one cannot do much about simulators, benchmark tasks for offline reinforcement learning still have many design choices.  In particular, how should the datasets in the benchmark be collected (i.e., behavior policies)?  While the datasets in the proposed benchmark are collected with various behavior policies including humans, it is not necessarily convincing that the resulting benchmark tasks are good for the purpose of evaluating offline reinforcement learning to be used in the real world.  In addition to the suggestions given by the reviewers, a possible direction to improve the paper is to focus on the choice of behavior policies used to generate the datasets in the proposed benchmark.  One might then be able to provide some convincing arguments as to why performing well in the benchmark might imply good performance in the real world by relating it to the choice of behavior policies.
This paper aims to study the effect of data augmentation of generalization performance. The authors put forth a measure of rugosity or "roughness" based on the tangent Hessian of the function reminiscent of a classic result by Donoho et. al. The authors show that this measure changes in tandem with how much data augmentation helps. The reviewers and I concur that the rugosity measure is interesting. However, as the reviewer mention the main draw back of this paper is that this measure of rugosity when made explicit does not improve generalization. This is the main draw back of the paper. I agree with the authors that this measure is interesting in itself. However, I think in its current form the paper is not ready for prime time and recommend rejection. That said, I believe this paper has a lot of potential and recommend the authors to rewrite and carry out more careful experiments for a future submission.
This paper studies the effect of clipping on mitigating label noise. The authors demonstrate that standard gradient clipping does not suffice for achieving robustness to label noise. The authors suggest a noise robust alternative. In the discussion the reviewers raised some interesting questions and technical detailed but mostly agreed that the paper is well written with nice contributions. I concur with the reviewers that this is a nicely written paper with good contributions. I recommend acceptance but recommend the authors continue to improve their paper based on the reviewers  suggestions.
This is an interesting paper and addresses an important problem of neural networks with memory constrains. New experiments have been added that add to the paper, but the full impact of the paper is not yet realised, needing further exploration of models of current practice, wider set of experiments and analysis, and additional clarifying discussion.
This paper studies numerous ways in which the statistics of network weights evolve during network training.  Reviewers are not entirely sure what conclusions to make from these studies, and training dynamics can be strongly impacted by arbitrary choices made in the training process.  Despite these issues, the reviewers think the observed results are interesting enough to clear the bar for publication.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.  The proposed method performed well on 3 visual content transfer problems.   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The paper is hard to follow at times   The problem being addressed is technically interesting but not well motivated. That is, the question "why is this of interest to the ICLR community" was not well answered.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  There were no major points of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
There appears to be to be a fundamental error in the paper, w.r.t. the application of the proposed approach to finite fields. As a result, the paper cannot be accepted in its current form.
Reviewers appreciated the model and the ideas presented and found them very interesting.  The main reason for rejection is the extent of the empirical work.  Unfortunately, and I think what is a bad sign for the ICLR community, the authors could not do adequate empirical work due to their computational resources.  Not belonging to an organisation with extensive computational resources myself, I am in strong symparthy with the authors, though I do not see any way this can be satisfactorily accounted for in reviewing.  Several reviewers commented on the datasets, the extent of evaluations, and the comparisons made with prior work.  For instance, the small CIFAR10 images are not ideal to demonstrate the technique and comparative results with the other data sets are limited.  The reviewers had a number of concerns on the theoretical work and these were well discussed by the authors.  In summary, this is promising research but needs more empirical work.   
The reviewers agree that the contributions may not be relevant to the ML research community or perhaps are a poor fit for the venue, but otherwise find the work potentially useful and addressing a timely topic. Because the paper focuses on a simulation environment for existing epidemiological models, reviewers comment that the technical and methodological novelty is limited.
This paper proposes a method for training an neural network to operate stack based mechanism in order to act as a CFG parser in order to, eventually, improve program synthesis and program induction systems. The reviewers agreed that the paper was compelling and well supported empirically, although one reviewer suggested that analysis of empirical results could stand some improvement. The reviewers were not able to achieve a clear consensus on the paper, but given that the most negative reviewer has also declared themselves the least confident in their assessment, I am happy to recommend acceptance on the basis of the median rather than mean score.
A novel  approach for quantized deep neural nets is proposed,  which is more principled than commonly used  straight through gradient method. A theoretical analysis of the algorithm s converegence  is presented, and empirical results show advantages of the proposed approach. 
This is an interesting paper on improving score based conditional sampling and its use in solving inverse problems. The current method of sampling from NCSNv2 is somewhat inefficient and the authors propose a different SDE that seems to work better for conditional generation.   The paper is applied to Computational imaging and MRI and shows very good results and reasonable comparisons with the recent state of the art. One limitation is that the measurement process is artificial and ignores specifics of MRI (real measurements and multi coils would strengthen the paper). In any case since this is a fundamental methods paper with a solid technical innovation on score based sampling, I recommend acceptance.
The paper proposes an approach to selectively update the weights of neural networks in federated learning. This is an interesting and important problem. As several reviewers pointed out, this is highly related to pruning although with a different objective.  It is an interesting paper but is a marginal case in the end due to the weakness on presentation and evaluation.   
The paper focuses on the problem of finding dense representations of graph structured objects in an unsupervised manner. The authors propose a novel framework for solving this problem and show that it improves over competitive baselines. The reviewers generally liked the paper, although were concerned with the strength of the experimental results. During the discussion phase, the authors bolstered the experimental results. The reviewers are satisfied with the resulting paper and agree that it should be accepted.
This paper studies the relationship between test error as a function of training set size and various design choices of neural network training. Overall all of the reviewers are excited about the prospect of relating error curves to neural network design choices, but different reviewers complain about the rigor of empirical evaluation and the accuracy of conclusions given limited data points. I agree with reviewers on both points, i.e., the paper studies different design choices, but does not do a thorough job studying those design choices. Moreover, it is not clear what aspects of the study are directly related to error curves vs. a standard correlation study done in prior work, e.g. in "Do better ImageNet models transfer better?" for usefulness of ImageNet pre training. So, overall, I believe not only the empirical evaluation needs improvement, but also the story needs refinement. I am looking forward to seeing this paper published in other ML venues.
While the paper shows some encouraging results for scaling up SVMs using coreset methods, it has fallen short of making a fully convincing case, particularly given the amount of intense interest in this topic back in the heydey of kernel methods. When it comes to scalability, it has become the norm now to benchmark results on far larger datasets using parallelism, specialized hardware in conjunction with algorithmic speedups (e.g., using random feature methods, low rank approximations such as Nystrom and other approaches). As such the paper is unlikely to generate much interest in the ICLR community in its current form.
All the reviewers noted that the dual formulation, as presented, only applies to the logistic family of classifiers. The kernelization is of course something that *can* be done, as argued by the authors, but is not in fact approached in the submission, only in the rebuttal. The toy ish nature of the problems tackled in the submission limits the value of the presentation.  If the authors incorporate their domain adaptation results (SVHN >MNIST and others) using the kernelized approach and do the stability analysis for those cases, and obtain reasonable results on domain adaptation benchmarks (70% on SVHN >MNIST is for instance on the low side compared to the pixel transfer based GAN approaches out there!) then I think it d be a great paper.  As such, I can only recommend it as an invitation to the workshop track, as the dual formulation is interesting and potentially useful.
This paper presents a method for improving the learning of neural controlled differential equation (CDE) models. Neural CDE models provide a number of advantages over neural ODE models in terms of their ability to incorporate continuous time observations. The primary strength of this paper is that it proposes a mathematically rigorous approach to enable neural CDE models to be learned more efficiently from long time series by converting the CDE to an ODE via the log ODE method. The results are promising in that the method is able to simultaneously improve accuracy, reduce running time and reduce memory required during learning.   The paper has two main weaknesses. First, the authors claim that due to the problems they are solving (time series with up to 17,000 steps), there are no viable baselines outside of the family of methods that they are proposing. As was noted in the reviews, it would be advisable to consider even very basic baselines for these experiments in addition to current benchmark results. For example, the EigenWorms data set was used in the time series classification benchmark described in Bagnall et al. and there are benchmark results available that appear to outperform those shown in Table 2 (see mean test accuracy results reported here: http://www.timeseriesclassification.com/results/AllAccuracies.zip). The authors are also encouraged to consider even coarse RNN approximations such as partitioning the time series into tractable blocks for learning. It is not clear that the data sets actually have long range dependencies despite being long.   The second weakness is that the representation that underlies the log ODE method (the log signature transform) has been used in previous work in conjunction with discrete time RNNs. It can be viewed as a preprocessing method in a sense, as was noted by a reviewer. However, it is much more fundamentally integrated with methods for solving CDE s than its prior application to RNNs indicates.   Overall,  support for the paper did not rise to the bar required for acceptance, but we encourage the authors to revise and re submit the work to a future venue.  
The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
This paper empirically investigates the gradient dynamic of two layer network nets with ReLU activations on synthetic datasets under $L^2$ loss. The empirical results show that for a specific type of initialization and less overparametrized neural nets, the gradient dynamics experience two phases: a phase that follows the random features model where all the neurons are *quenched* and another phase where there are a few *activated* neurons. As pointed out by Reviewer 1, this paper lacks mathematical support and did not distinguish between *random features model* and *neural tangent model*. Reviewer 3 and Reviewer 4 also complained that the paper is purely experimental. Therefore, this paper may benefit from proposing an at least heuristic or high level conjecture/interpretation/argument that tries to explain the empirical results.  
The reviewers raised a number of concerns, but  the authors provided no rebuttal to the reviewers  comments.  One reviewer felt the experimental fitting was not thorough enough. Suppose one used layers of oriented bandpass filters, separated by non linearities, would that perform well on the task convnets are trained on?  The AC doesn t agree with the arguments of R3.  I hope the comments of the reviewers, particularly the many specific comments of reviewers R1 and R2, will be helpful to you as you revise the manuscript.  The AC feels a more thorough experimental evaluation, and following up on many of the suggestions of the reviewers will lead to a strong paper.  As it stands, however, with 3 recommendations for rejection (1 weak), and only 1 weak recommendation for acceptance, we need to reject. 
This work presents a strong baseline model for several NLP ish tasks such as document classification, sentence classification, representation learning based NLI, and text matching. In terms of originality, reviewers found that "there is not much contribution in terms of technical novelty" but that "one might also conclude that we need more challenging dataset". There was significant discussion about whether it "sheds new lights on limitations of existing methods" or whether the results were "marginally surprising". In terms of quality, reviewers found it to be an "insightful analysis" and noted that these "SWEMs should be considered a strong baseline in future work".  There was significant discussion with the AC about the signficance of the work. In the opinion of the AC reviewers did were too quick to accept the authors novelty claims, and did not push them enough to include other baselines in their tables that were not overly deep model. In particular the AC felt that important numbers were left out of the experiment tables, for document classification that muddied the results. The response of the authors was:  "Moreover, fasttext and our SWEM variants all belong to the category of simpler methods (with parameter free compositional functions). Since our motivation is to explore the necessity of employing complicated compositional functions for various NLP tasks, we do not think it is necessary for us to make any comparisons between fasttext and SWEM."  In addition when a reviewer pointed out the lack of inclusion of FOFE embeddings, the authors noted something similar  "Besides, we totally agree that developing sentence embeddings that are both simple and efficient is a very promising research direction (FOFE is a great work along this line)."  The reviewer correctly pointed out related work that shows a model very similar to what the author s propose. In general this seems like evidence that the techniques are known, not that they are significant and novel. 
The authors propose a neural network model to preserve the sub class similarity. The key of the model is to add a prototype layer to a multi scale deep nearest neighbor network. The prototype layer stores the representative prototypes of some fine grained sub classes. The use of the prototype layer preserves intepretability and computational efficiency. Experimental results demonstrate that the proposed approach reaches state of the art prototype learning performance.  The reviewers generally find the paper clear and with sufficient contributions. The empirical validation is sufficiently thorough to back the claims in the paper. The main concern prior to the rebuttal among some of the reviewers was about the novelty of the paper (e.g. with respect to DkNN), but the authors convinced most of the reviewers in the rebuttal about the key differences. The authors are encouraged to highlight the novelty aspect more clearly in the revision. Another suggestion was to add an ablation study to justify the importance of the r1/r2 parameters, and the authors have done a successful job addressing the suggestion. Several other comments, such as explanations of the hyperparameters, have been taken into account in the revision. The reviewers thus reach the consensus to recommend acceptance.
This paper proposes a controllable text generation model conditioned on desired structures, converting a text into structure information such as part of speech (POS) and participial construction (PC). It proposes a “Structure Aware Transformer” (SAT) to generate text and claims better  PPL and BLEU compared with GPT 2. Reviewers pointed out that limited novelty of this paper   SAT is essentially a transformer run on multiple sequences of structure information, with sums of structure embeddings as input embeddings    the proposed method essentially infuses structure information as features, rather than “controlling” text generation. Some references are also missing, most prominently:   1. Zhang X, Yang Y, Yuan S, et al. Syntax infused variational autoencoder for text generation[J]. arXiv preprint arXiv:1906.02181, 2019. 2. Casas N, Fonollosa J A R, Costa jussà M R. Syntax driven Iterative Expansion Language Models for Controllable Text Generation[J]. arXiv preprint arXiv:2004.02211, 2020. 3. Wu S, Zhou M, Zhang D. Improved Neural Machine Translation with Source Syntax[C]//IJCAI. 2017: 4179 4185.  Unfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection.
Authors propose a new way of early stopping for neural architecture search. In contrast to making keep or kill decisions based on extrapolating the learning curves then making decisions between alternatives, this work learns a model on pairwise comparisons between learning curves directly. Reviewers were concerned with over claiming of novelty since the original version of this paper overlooked significant hyperparameter tuning works. In a revision, additional experiments were performed using some of the suggested methods but reviewers remained skeptical that the empirical experiments provided enough justification that this work was ready for prime time.   
The paper focuses on the problem of high quality video generation. It approaches the problem by extending VQ VAE to videos, where a GPT is used to model the low dimensional representation of the VAE. As agreed upon by the authors and the reviewers, the proposed method is simple and produces interesting results.   Based on all the reviews and the subsequent discussions, it seems that the reviewers  comments were mostly not addressed and they maintain their stance with regards to the paper s technical novelty, empirical justification of the paper s claims (specifically the claim on computational efficiency), and the rigorous comparison with prior art. The authors themselves make it clear that technical novelty was not the main driving force in this paper. However, in this case, it would be expected that the major claims of the paper be very clearly justified (especially with experiments and analysis) and comparison with other methods be more thorough. It seems that these latter two points remain in the latest revision of this paper. Since the paper shows promise, the authors are recommended to take the reviewers  comments and suggestions into consideration to produce a stronger and more thorough submission in the future.
The submission receives mixed ratings initially. Reviewer WK7k stays positive, edUG and qbmC are borderline, and MXU1 is negative. They raise several issues including limited technical contribution, insufficient sota experimental comparison, and detailed theoretical proof. The authors have responded to these issues in the rebuttal. The final ratings of these reviewers do not alter.   After checking the revised manuscript, all the reviews and responses, the AC feels the review from MXU1 lacks sufficient details and the claim shall be better supported with evidence. On the other hand, the issue of the technical contribution raised by edUG still exists. The modification upon original CL is marginal. Also, the experimental validation, although tested in several configurations, only includes ResNet 50 as the encoder backbone. The sufficient validation upon different scales of CNNs are common strategies used in sota SSL methods (e.g., MoCo, BYOL, SimCLR). Without a thorough evaluation, the effectiveness of CACR on different scales of CNN backbones is in doubt. Meanwhile, the experimental comparison upon sota methods (e.g., BYOL, SwAV+multi crop) does not clearly show the performance advantages of CACR. The small scale dataset utilized for ablation study (i.e., CIFAR) is not as convincing as ImageNet.   Overall, the AC feels that the marginal technical contribution is acceptable, but it shall be equipped with thorough experimental validation and thus serves as a fundamental baseline (i.e., potentially benefits the SSL community). Based on the current form, the authors are suggested to further improve the current submission and are welcome to submit for the next venue.
This paper proposes a methodology to create cheap NAS surrogate benchmarks for arbitrary search spaces. Certainly, the work is interesting and useful, with comprehensive studies to validate such approach. It should be credited as belonging to the first efforts of introducing and comprehensively studying the concept of surrogate NAS benchmarks. In AC s opinion, it is a solid paper that will (or has already) inspire many follow up works. The paper is well written.   This paper received highly mixed ratings. Although the authors might not see, all reviewers actually participated in the private discussions. Reviewer 1eb8 indicated hesitation in her/his support. Reviewer yTPb stated that if not considering the arXiv complicacy, she/he "would certainly raise score by one level".  AC also reached out to Reviewer yTPb about her/his mentioned possibility of updating scores, and got confirmed that her/his original opinions wasn t changing after rebuttals. Besides, AC agrees the arXiv/NeurIPS complicacy shouldn t brought into the current discussion, and ignored that factor during decision making.   The main sticking (and considered as valid) critique is on the relatively outdated and incomplete selection of baselines. As a benchmark paper, it should capture and diversify the recent methods. For example, the authors might consider adding: https://botorch.org/docs/papers (latest methods in Bayesian Optimization) https://github.com/facebookresearch/LaMCTS (latest methods in Monte Carlo Tree Search) https://facebookresearch.github.io/nevergrad/ (latest methods in Evolutionary algorithms)  Given the above concerns, AC considers this paper to sit on the borderline, and perhaps with pros outweighing the cons. Hence, a weak accept decision is recommended at this moment.
The paper proposes an approach to constructing steerable equivariant CNNs over arbitrary subgroups of E(3), by generalizing the Wigner Eckart theorem for steerable kernels in Lang & Weiler (2020). The intuitive idea is to use a steerable basis for a large group like O(3) to build a basis for a subgroup of interest like SO(3). Reviewers were generally happy with the author response, finding the paper makes a good contribution to steerable network design, with theoretically interesting ideas. However, there were still questions after the rebuttal about the practical utility of the approach, such as the relevance of subgroups of O(3). Reviewers also felt that much of the material was not written in an accessible way, such that it could only be appreciated by experts working on group equivariant CNNs.   In a final version, the authors should try to make a much clearer case for practical relevance, introduce the key concepts assuming less prior knowledge, and present the material in a way that makes the high level story more clear, as detailed by reviewers.
this paper adds onto the line of research in investigating the mechanism by which a recurrent network solves a supervised sequence classification problem, following the recent studies such as Maheswaranathan et al., 2019 and Maheswaranathan & Sussillo (2020). in doing so, this paper hypothesizes and confirms that the internal hidden state of a recurrent net, be it GRU or LSTM, evolves over a planar (approximate) attractor as it reads the input, amounting to integrating the evidence as it processes the input sequence, and demonstrates the existences of these attractors and integration dynamics on three types of problems (classification, ordered classification and multi label classification.)   there were some potentially misleading or confusing statements throughout the manuscript in the initial version, which were pointed out by the reviewers. the authors however did a commendable job of addressing these concerns by the reviewers to the point that most of them have revised their scores up.   based on the reviewers  assessments, authors  response and their exchange, i strongly believe this work will enrich our understanding of recurrent nets further.
This submission generated significant discussion between the reviewers; three of them ended up on the "accept" side, but one remained firmly in the "reject" camp.  The main strength of the paper is that it tackles a very hard problem: learning an unsupervised generative model (and accompanying inference model) of scene graph structures given only image data. As one reviewer mentioned, it is remarkable that the authors were able to get their system to work at all, given the seeming intractability of this problem. The work builds upon a clear line of prior work in this area, and the type of data on which it is evaluated ("toy" synthetic datasets a la CLEVR) is consistent with prior art.  Multiple reviewers brought up the "toy" nature of the dataset as a drawback to the paper, but most agreed that this is not reason to reject the paper. Rather, the paper demonstrates a convincing proof of concept that this kind of model can be built, and improvements in the elements out of which the model is composed (generative and inference networks) should improve its applicability to real world data.  Another question mark raised by multiple reviewers: could a simpler, handcrafted inference procedure work just as well or better? The authors included a new experiment against a hand coded heuristic in their rebuttal, and their method outperforms it. One reviewer noted that more careful tuning might make a heuristic perform as well as the proposed method, but it is still clear that it is not trivial to get a hand coded solution to perform well (even for this "toy" data). Another reviewer pointed out that this is one of the main attractions of variational inference methods: the ability to specific knowledge as simple generative priors rather than complex bottom up inference procedures.  One reviewer, R1, remains negative about the paper. His (it is a he; I know this reviewer) main concern is that the scene graphs used are shallow and have a simple structure, and thus (a) it s not clear what value they add, (b) a simple postprocess could reconstruct them, assuming the individual object parts could be detected, and (c) it s not clear whether the method would generalize to deeper/more complex hierarchies. He believes this calls into question the validity of the entire method.  I am sympathetic to this argument, but I think setting the bar this high may prevent progress in this field. For point (a), the authors included an image manipulation application in their rebuttal again, a proof of concept, not a directly useful tool. For point (b), the authors did compare against a hand coded inference baseline and achieved better results, so while this may be possible, it is probably not as easy as the reviewer suggests. (c) remains an open question, to me. But even if this method as presented cannot generalize to more complex scene graphs, it likely paves the way for future work that can.
RCRL is return based contrastive learning for reinforcement learning, where the label is whether two samples belong to the same return bin. The reviewers found this to be a well executed paper with good theoretical and experimental results.
The subject of model evaluation will always be a contentious one, and the reviewers were not yet fully convinced by the discussion. The points you bring up at the end of your rresponse already point to directions for improvement as well as a greater degree of precision and control.
 pros:   the paper is well written and presents a nice framing of the composition problem   good comparison to prior work   very important research direction  cons:   from an architectural standpoint the paper is somewhat incremental over Routing Networks [Rosenbaum et al]   as Reviewers 2 and 3 point out, the experiments are a bit weak, relying on heuristics such as a window over 3 symbols in the multi lingual arithmetic case, and a pre determined set of operations (scaling, translation, rotation, identity) in the MNIST case.  As the authors state, there are three core ideas in this paper (my paraphrase):  (1) training on a set of compositional problems (with the right architecture/training procedure) can encourage the model to learn modules which can be composed to solve new problems, enabling better generalization.  (2) treating the problem of selecting functions for composition as a sequential decision making problem in an MDP (3) jointly learning the parameters of the functions and the (meta level) composition policy.  As discussed during the review period, these three ideas are already present in the Routing Networks (RN) architecture of Rosenbaum et al.  However CRL offers insights and improvements over RN algorithmically in a several ways:  (1) CRL uses a curriculum learning strategy.  This seems to be key in achieving good results and makes a lot of sense for naturally compositional problems. (2) The focus in RN was on using the architecture to solve multi task problems in object recognition. The solutions learned in image domains while "compositional" are less clearly interpretable.  In this paper (CRL) the focus is more squarely on interpretable compositional tasks like arithmetic and explores extrapolation. (3) The RN architecture does support recursion (and there are some experiments in this mode) but it was not the main focus.  In this paper (CRL) recursion is given a clear, prominent role.  I appreciate that the authors  engagement in the discussion period. My feeling is that  the paper offers nice improvements, a useful framing of the problem, a clear recursive formulation, and a more central focus on naturally compositional problems.  I am recommending the paper for acceptance but suggest that the authors remove or revise their contributions (3) and (4) on pg. 2 in light of the discussion on routing nets.  Routing Networks, Adaptive Selection of Non Linear Functions for Multi task Learning, ICLR 2018
The authors seem to miss important related literature for their comparison. They also tuned hyperparameters and tested on the same validation set. They should split between train/validation/test.  Reviews are just too low across the board to accept.
All three reviewers argue for rejection on the basis that this paper does not make a sufficiently novel and substantial contribution to warrant publication. The AC follows their recommendation.
This paper proposes a method that combines Bad GAN and Good GAN, in which Good GAN learns to generate the anomalies while Bad GAN reguralizes the anomaly pseudo anomalies at the boundary of inlier distribution. In addition, a new orthogonal loss is proposed to  regularize the generation of anomaly samples to be distributed evenly at the periphery of the training data. The proposed method is new and shows some improvement over existing methods.  However, there are some detailed technical concerns raised by reviewers. Some of the concerns still remain unresolved after the discussion. 1) The proposed method lacks a principled way to select hyperparameters. 2) The experimental setting is a bit simple to verify the effectiveness of the proposed method in challenging real world applications. Especially, there is no theoretical guarantee of the proposed method, empirical evaluation is the only way to show the effectiveness of the proposed method. 3) The overall performance improvement is not very significant compared to existing methods. For example, the performance is very close to a method F AnoGAN published in 2019. Addressing the concerns needs a significant amount of work. Thus, I do not recommend acceptance of this paper.
This paper introduces a "scratchpad" extension to seq2seq models whereby the encoder outputs, typically "read only" during decoding, are editable by the decoder. In practice, this bears quite a lot of similarity—if not in the general concept, then in the the implementation—to a variety of models proposed in the NLP community (see reviews for details). As the technical novelty of the paper is quite limited, and there are issues with the clarity both in the technical contribution and in presenting what exactly is the main contribution of the paper, I must concur with the reviewers and recommend rejection.
This paper applies multi armed bandits to tuning deep learning code optimization. All reviewers agreed that this is an exploratory paper that opens up a new research area. My main criticism is algorithmic. In particular, the paper applies a 20 year old algorithm to a problem with a small number of arms. It is definitely not as impressive as  https://papers.nips.cc/paper/2018/file/f33ba15effa5c10e873bf3842afb46a6 Paper.pdf  who studied a different (but related) problem. The tuning problem in this paper also seems non stochastic and contextual, while the authors apply a stochastic non contextual bandit algorithm.  I shared these concerns with the reviewers, who insisted that the application is important enough to justify the acceptance of the paper. I respect their opinion and therefore suggest an acceptance. I encourage the authors to take my comments into account when revising the paper.
 This paper introduces conditional graph neural fields, an approach that combines label compatibility scoring of conditional random fields with deep neural representations of nodes provided by graph convolutional networks. The intuition behind the proposed work is promising, and the results are strong.  The reviewers and the AC note the following as the primary concerns of the paper: (1) The novelty of this work is limited, since a number of approaches have recently combined CRFs and neural networks, and it is unclear whether the application of those ideas to GCNs is sufficiently interesting, (2) the losses, especially EBM, and the use of greedy/beam search inference was found to be quite simple, especially given these have been studied extensively in the literature, and (3) analysis and adequate discussion of the results is missing (only a single table of numbers is provided). Amongst other concerns, the reviewers identified issues with writing quality, lack of clear motivation for CRFs, and the selection of the benchmarks.  Given the feedback, the authors responded with comments, and a revision that removes the use of EBM loss from the paper, which the reviewers appreciated. However, most of the concerns remain unaddressed. Reviewer 2 maintains that CRFs+NNs still need to be motivated better, since hidden representations already take the neighborhood into account, as demonstrated by the fact that CRF+NNs are not state of art in other applications. Reviewer 2 also points out the lack of a detailed analysis of the results. Reviewer 2 focuses on the simplicity of the loss and inference algorithms, which is also echoed by reviewer 2 and reviewer 1. Finally, reviewer 1 also notes that the datasets are quite simple, and not ideal evaluation for label consistency given most of them are single label (and thus need only few transition probabilities).  Based on this discussion, the reviewers and the AC agree that the paper is not ready for acceptance.
The authors propose an agent that can act in an RL environment to verify hypotheses about it, using hypotheses formulated as triplets of pre condition, action sequence, and post condition variables. Training then proceeds in multiple stages, including a pretraining phase using a reward function that encourages the agent to learn the hypothesis triplets.   Strengths: Reviewers generally agreed it’s an important problem and interesting approach  Weaknesses: There were some points of convergence among reviewer comments: lack of connection to existing literature (ie to causal reasoning and POMDPs), and concerns about the robustness of the results (which were only reporting the max seeds).  Two reviewers also found the use of natural language to unnecessarily complicate their setup. Overall, clarity seemed to be an issue. Other comments concerned lack of comparisons, analyses, and suggestions for alternate methods of rewarding the agent (to improve understandability).    The authors deserve credit for their responsiveness to reviewer comments and for the considerable amount of additional work done in the rebuttal period. However, these efforts ultimately didn’t satisfy the reviewers enough to change their scores. Although I find that the additional experiments and revisions have significantly strengthened the paper, I don t believe it s currently ready for publication at ICLR. I urge the authors to focus on clearly presenting and integrating these new results in a future submission, which I look forward to. 
The paper introduces improving passage retrieval for multi hop QA datasets by recursively retrieving passages, adding previously retrieved passages to the input (in addition to a query). This simple method shows gains on multiple QA benchmark datasets, and the evaluation presented in the paper on multiple competitive benchmark datasets (HotpotQA, FEVER) is very thorough (R1, R3, R4).   While the application is pretty narrow, the performance gain (considering both efficiency and accuracy) is fairly significant, and the paper presents a simple model with less assumption (e.g., inter document hyperlinks), that could be useful for future research.   [1] also seems like a relevant line of work.   [1] Generation Augmented Retrieval for Open domain Question Answering https://arxiv.org/pdf/2009.08553.pdf  
The authors propose a hardware agnostic metric called effective signal norm (ESN) to measure the computational cost of convolutional neural networks. They then demonstrate that models with fewer parameters achieve far better accuracy after quantization. The main novelty is on the metric ESN. However, ESN is based on ideal hardware, and thus not suitable for existing hardware. Assumptions made in the paper are hard to be proved. Experimental results are not convincing, and related pruning methods are not compared. Finally, the paper is not written clearly, and the structure and some arguments are confusing.
This paper proposes a training approach that orthogonalizes gradients to enable better learning across multiple tasks. The idea is simple and intuitive.  Given that there is past work following the same kind of ideas, it would be need to further:  (a) expand the experimental evaluation section with comparisons to prior work and, ideally, demonstrate stronger results.  (b) study in more depth the assumptions behind gradient orthogonality for transfer. This would increase impact on top of past literature by explaining, besides intuitions, why gradient orthogonality helps for transfer in the first place. 
Summary: The paper introduces and studies the expected improvement (EI) technique as a way to balance exploration and exploitation for the contextual bandit problem. The authors propose two EI based algorithms for linear bandits and for neural bandits for a general class of reward functions. The paper presents regret bounds for both methods and shows the experimental results to support their theoretical claims.  Discussion: The reviewers have identified technical issues in the regret bound of LinEL which has been now corrected. Similarly, reviewers have had difficulty assessing the correctness of the paper due to typos and unclear exposition, and raise concerns regarding the amount of corrections that were necessary to reach the current stage. There is no consensus between the reviewers, and some would feel more comfortable if the paper could go through another round of review after major revision. Reviewer UDHj points that after corrections, "the regret has an additional $O(\sqrt{\ln T})$ dependency compared with the regret of LinUCB and Thompson Sampling." and this should be discussed in the updated version. Reviewer co1L suggests to compare to the bounds in "The End of Optimism? An Asymptotic Analysis of Finite Armed Linear Bandits". The authors responded that " To our knowledge, the analysis of the optimal asymptotic regret for contextual bandits as we consider (where the context may be controlled by an adversary) is still an open problem.". In fact, this is the topic of several recent works including: * "Asymptotically Optimal Information Directed Sampling" COLT 2021 * "An asymptotically optimal primal dual incremental algorithm for contextual linear bandits", NeurIPS 2021  The connections of the present work with these two references are strong and should be discussed in more depth. I believe it is a more important discussion than the comparison with the regret bound of LinTS which is yet another problem.    The reviewers have appreciated the originality of the ideas and for that reason we encourage the authors to revise their draft and submit to a future venue.  Recommendation: Reject.
This is an interesting and well written paper introducing two unbiased gradient estimators for optimizing expectations of black box functions. LAX can handle functions of both continuous and discrete random variables, while RELAX is specialized to functions of discrete variables and can be seen as a version of the recently introduced REBAR with its concrete relaxation based control variate replaced by (or augmented with) a free form function. The experimental section of the paper is adequate but not particularly strong. If Q prop is the most similar existing RL approach, as is state in the paper, why not include it as a baseline? It would also be good to see how RELAX performs at optimizing discrete VAEs using just the free form control variate (instead of combining it with the REBAR control variate).
This paper presents a new, large scale, open domain dataset for on screen audio visual separation, and provides an initial solution to this task. As the setting is quite specialized, the authors proposed a neural architecture based on spatial temporal attentions (while using existing learning objective for audio separation). The reviewers were initially concerned that, while reasonably motivated, the architecture seemed some arbitrary. The authors then provided extensive ablation studies to evaluate the significance of each component with existing datasets, and these efforts are appreciated by reviewers. The authors may consider re organizing the paper and moving some ablation studies to the main text. On the other hand, the reviewers believe that the dataset will be very useful for the community due to its diversity in content and label quality.
Paper proposes adding randomization steps during inference time to CNNs in order to defend against adversarial attacks.  Pros:    Results demonstrate good performance, and the team achieve a high rank (2nd place) on a public benchmark.   The benefit of the proposed approach is that it does not require any additional training or retraining.  Cons:    The approach is very simple, common sense would tend to suggest that adding noise to images would make adversarial attempts more difficult. Though perhaps simplicity is a good thing.   Update: Paper does not cite related and relevant work, which takes a similar approach of requiring no retraining, but rather changing the inference stage: https://arxiv.org/pdf/1709.05583.pdf    Grammatical Suggestions:  This paper would benefit from polishing. For example:    Abstract: sentence 1: replace “their powerful ability” to “high accuracy”   Abstract: sentence 3: replace “I.e., clean images…” with “For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail”   Abstract: sentence 4: replace “utilize randomization” to “implement randomization at inference time” or something similar to make more clear that this procedure is not done during training.   Abstract: sentence 7: replace “also enjoys” with “provides”  Main Text: Capitalize references to figures (i.e. “figure 1” to “Figure 1”).  Introduction: Paragraph 4: Again, please replace “randomization” with “randomization at inference time” or something similar to better address reviewer concerns. 
The presented method proposes to use saliency maps as a component for an additional metric of forgetting in continual learning, and as a tool as additional information to improve learning on new tasks.   Pros:  + R2 & R3: Clearly written and easy to follow.  + R3: New metric to compare saliency masks + R3: Interesting idea to utilize previously learned saliency masks to augment learning new tasks.  + R1: Performance improvements observed.  Cons:   R1 & R2: Novelty is limited in the context of prior works in this field. Unanswered by authors.   R2: Concerns around method s ability to use salient but disconnected components. Unanswered by authors.   R2: Experiments needed on more realistic datasets, such as ImageNet. Unanswered by authors.    R3: Performance gains are small.    R1 & R2: Literature review is insufficient.   Reviewers are leaning reject, and R2 s concerns have not been answered by the authors at all. Idea seems interesting, authors are encouraged to take into careful consideration the feedback from authors and continue their research.
High quality theoretical paper that studies the connection between concentration of the data distribution and adversarial robustness. It contributes a method for more accurate estimation of concentration, which allows drawing stronger conclusions about adversarial robustness compared to previous work. The paper is highly technical, but written clearly and precisely. All reviewers give positive scores, with only minor negative comments.  One minor concern I have is that the potential audience of the paper might be small, given its highly technical nature and very specialized line of research it follows. Still, I believe it s a solid contribution, so I m happy to recommend acceptance.
The authors propose an approach to mitigate mode collapse phenomena in GANs. Motivated by the intuition that mode collapse stems from catastrophic forgetting of the discriminator, the authors propose a solution inspired by recent research in continual learning and dynamically add new discriminators during training. The authors empirically demonstrate that combining the proposed scheme with existing GANs leads to improvements in terms of Inception Score and FID.   This paper is trying to address a significant problem for the generative modeling community. The reviewers appreciated the clarity of writing, the empirical results, and the idea of using normalising flows for an elegant visualisation. However, the reviewers have pointed out several major issues which were not adequately addressed by the authors. The first one is the clear failure to position the work with respect to related work. In fact, the main idea related to catastrophic forgetting was already established in [1,2] and subsequent works. Secondly, the improvements over the baseline come at a significant computational overhead which is extremely challenging and impractical. Finally, given the trend that large scale models achieve significantly better results in practice, the proposed approach is not only impractical, but potentially extremely wasteful. Given very limited novelty, failure to position the work, and impracticality of the proposed solution, I will recommend rejection.  [1] https://arxiv.org/abs/1810.11598  [2] https://arxiv.org/abs/1911.06997
This paper presents sparse attention mechanisms for image captioning. In addition to recent sparsemax based method, authors proposed to extend it by incorporating structural constraints in 2D images, which is called TVMAX. The proposed methods are shown to improve the quality of captioning, particularly in terms of fewer erroneous repetitions, and obtain better human evaluation scores.  Through reviewer discussion, one reviewer updated the score to rejection. A major concern raised by the reviewers is that the motivation of introducing sparse attention is not clear, and the reason why it improves the quality (particularly, why it can reduce repetition) is not convincing. While we understand it is plausible for long sequences as in text domain, we are not convinced that it is really necessary for image captioning problems. Although authors seem to have some ideas, we cannot see how they will be reflected in the paper so I’d like to recommend rejection. I recommend authors to polish the paper with a clearer description of the motivation and high level analysis of the method as well as testing on other visual tasks to show its generality.  
This work suggests using models of the environment as regularizers for performing explicit transfer in RL. Here are some of the highlights from the reviews and subsequent discussions:   * Novel problem    * Unclear to some of the reviewers why the problem setting is in fact important.   * Well written   * Interesting theoretical results   * Somewhat limited experimental results Post rebuttal, while there is not necessarily a great consensus, the reviewers all feel that it s an improved piece of work. While I am myself not fully convinced that the problem setting motivation truly aligns with the kind of empirical results that the work provides, on the balance I think this work is interesting and has sufficient novel contributions to be accepted at ICLR.
This work proposes a VAE based model for learning transformations of sequential data (the main here intuition is to have the model learn changes between frames without learning features that are constant within a time sequence). All reviewers agreed that this is a very interesting submission, but have all challenged the novelty and rigor of this paper, asking for more experimental evidence supporting the strengths of the model. After having read the paper, I agree with the reviewers and I currently see this one as a weak submission without potentially comparing against other models or showing whether the representations learned from the proposed model lead in downstream improvements in a task that uses this representations.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.  The paper    tackles an interesting problem   makes a concerted effort to provide qualititative results that give insight into the models behaviour.   sufficiently cites related work.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The model architecture lacks novelty.   There was also agreement that the contributions   (i) minor modifications of existing sequential attention based models, and (ii) application to the RL domain   are minor.   A lot of space in the paper (section 4.2) is devoted to exploring the use of this model for image classification and video action recognition. However the proposed model performed poorly compared to SOTA methods for this task and no motivation was given for why the proposed model would be useful for such tasks.  All three points impacted the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  There was high agreement between the reviewers on the main drawbacks of the paper, before and after the rebuttal. The AC considered the rebuttals by the authors (in which they argued that there was sufficient contribution) but, in the end, agreed with the reviewers  assessments.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
This paper proposes an algorithm to learn symbolic intrinsic rewards via a symbolic function generator. The policy optimizes this reward function and an evolutionary algorithm selects between a set of such policies. The core idea is that learning with such a symbolic reward function is useful in sparse reward environments and also enables better interpretability.  ${\bf Pros}$: 1. The learnt reward function has a relatively simple form and is therefore interpretable 2. The experimental section is quite extensive ranging from diverse tasks, control systems and agent systems. However there are some issues about showing clear need of the proposed method  ${\bf Cons}$: 1. There was a consensus among reviewers that the paper does not make a strong case for the symbolic reward generator. In the rebuttal the authors argued that as RL scales to real world problems, it will become necessary to use such a method. I can understand how it would be useful in the context of inverse RL or imitation learning. However, as R3 points out, in the cases considered in this paper, the rewards are fairly intuitive and explainable. The paper might become stronger by directly tackling problems with such constraints. 2. There is confusion about the details and scope in the current version of the paper. The paper would become stronger by incorporating all the feedback received during the review period. 
This submission proposes a simple way to improve the stability of training GPT 2: Increase the sequence length of examples over the course of training. It is shown that this simple heuristic can result in using larger learning rates, therefore significantly speeding up convergence. Reviewers agreed that this was a simple and effective approach, but shared various concerns about the paper:   The paper focuses on GPT 2, while stability issues can arise in a much wider range of models. Additional experiments with other models (and ideally other codebases/training setups) would help verify that the proposed method is broadly applicable.   Better analysis of why using the sequence length as the difficulty metric would be helpful. What other criteria would be possible? Why is sequence length the best?  I would suggest that the authors significantly expand the submission based on the above suggestions and resubmit.
Three of the four reviewers recommend rejection; one additional reviewer considers the paper to be marginally above threshold for acceptance but is very uncertain and this is taken into account.  The AC is in consensus with the first three reviewers that this paper is not ready yet for publication.    There is concern from the reviewers that ICLR is not the right venue for this submission.  The author response in "Submission Update" does not clarify this concern.  Training a neural network to solve the problem does not automatically mean that ICLR or other ML conferences are necessarily the right venue.  Regardless, due to the many other raised concerns e.g. limited experimental results and comparisons as well as clarity,  the AC recommends rejection for this paper and resubmission at a more appropriate venue.  
This paper discusses the likelihood ratio estimation using the Bregman divergence.  The authors consider the  train loss hacking , which is an overfitting issue causing minus infinity for the divergence.   They introduce non negative correction for the divergence under the assumption that we have knowledge on the upper bound of the density ratio.  Some theoretical results on the convergence rate are given.  The proposed method shows favorable performance on outlier detection and covariate shift adaptation.  The proposed non negative modification of Bregman divergence is a reasonalbe solution to the important problem of density ratio estimation.  The experimental results as well as theoretical justfication make this work solid.  However, there are some concerns also.  The paper assumes knowledge on the upper bound of density ratio and uses a related parameter essentially in the method.  Assuming the upper bound  is a long standing problem in estimating density ratio, and it is in practice not necesssarily easy to obtain.  Also, there is a room on improvements on readability.   Although this paper has some signicance on the topic, it does not reach the acceptance threshold unfortunately because of the high competition in this years  ICLR.  
This paper introduces a possibly useful new RL idea (though it s a incremental on Liang et al), but the evaluations don t say much about why it works (when it does), and we didn t find the target application convincing. 
This paper presents an interesting and theoretically motivated approach to imposing Lipschitz constraints on functions learned by neural networks. R2 and R3 found the idea interesting, but R1 and R2 both point out several issues with the submitted version, including some problems with the proof probably fixable as well as a number of writing issues. The authors submitted a cleaned up revised version, but upon checking revisions it appears the paper was almost completely re written after the deadline. I do not think reviewers should be expected to comment a second time on such large changes, so I am okay with R1 s decision to not review the updated version. Future reviewers of a more polished version of the paper will be in a better position to assess its merits in detail.
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
The authors present a new benchmark for evaluating a plethora of models on a variety of tasks. In terms of scores, the paper received a borderline rating, with two reviews being rather superficial unfortunately. The last reviewer was positive. The reviewers generally agreed that the benchmark is interesting and carries value, and the AC agrees. Authors certainly invested a significant effort in designing the benchmark and performing a detailed analysis over several tasks and methods. However, the effort seems more engineering in nature and insights are somewhat lacking. For an experimental paper, presenting the results is interesting yet not sufficient. A much more in depth analysis and discuss would warrant a deeper understanding of the results and open directions for future work. This part is currently underwhelming. 
All reviewers assessed this paper as a weak reject. The AC recommends rejection.
This article proposes a novel weakly supervised segmentation method that unifies several annotation types using contractive/metric learning. This method clearly outperforms the current SOTA. While the unified framework itself is not novel enough, the reviewers agree that the contrastive loss formulation is interesting and the extensive experiments show its effectiveness. Overall, I consider that this unified framework is well engineered, the formulations are insightful, and the results advance the SOTA of weakly supervised segmentation. Accordingly, I propose to accept this paper at ICLR 2021.
The authors consider the task of interpretable video classification. First, a set of binary “concepts  is predicted, and these concept features are then used for classifying a video. The set itself is automatically generated from natural language descriptions, instead of relying on expert annotations. The authors collect two datasets to validate the proposed approach and show that the model can match the performance of a standard video classification model, while being interpretable.  The reviewers felt that the paper was well written and that the method and empirical results were clearly outlined. They also appreciated the empirical results whereby interpretability doesn’t necessarily come at the expense of accuracy and consider interpretability as a desirable property. The main reason for the borderline results is the heuristic nature of the proposed automatic concept labeling and the empirical evaluation against alternative baselines. In particular, one needs to **show that the proposed method generalises to other datasets**. Secondly, one of the main contributions, namely the automatic **concept extraction, still ends up requiring human annotation in the form of narrations**, and this cost should be quantified and contextualised.  I suggest the authors address these points and resubmit.
This paper proposes a new paradigm for learning to perform cooperative tasks with partners, which factors the problem into two components: how to perform the task and how to coordinate with the partner according to conventions. The setting is new and the reviewers are excited about the paper. A clear accept.
The paper addresses  an important problem of self supervised learning in the context of time series classification. However, all reviewers raised major concerns regarding the novelty of the approach and the quality of empirical evaluation, including insufficient comparison with the state of art and reproducibility issues. The reviewers agree that the paper, in its current state, does not path the ICLR acceptance threshold, and encourage the authors to improve the paper based on the provided suggestions.
This paper presents new generalized methods for representing sentences and measuring their similarities based on word vectors. More specifically, the paper presents Fuzzy Bag of Words (FBoW), a generalized approach to composing sentence embeddings by combining word embeddings with different degrees of membership, which generalize more commonly used average or max pooled vector representations. In addition, the paper presents DynaMax, an unsupervised and non parametric similarity measure that can dynamically extract and max pool features from a sentence pair.   Pros: The proposed methods are natural generalization of exiting average and max pooled vectors. The proposed methods are elegant, simple, easy to implement, and demonstrate strong performance on STS tasks.  Cons: The paper is solid, no significant con other than that the proposed methods are not groundbreaking innovations per say.   Verdict: The simplicity is what makes the proposed methods elegant. The empirical results are strong. The paper is worthy of acceptance.
The authors show that it is possible to overcome the script barrier in MLLMs by using transliteration. In effect, they show that transliterating all text to a single script improves the performance for low resource languages. They also provide additional analysis in the form of statistical tests and crosslingual representation analysis to substantiate their claims.  The main concerns raised by the reviewers are:  (i) lack of novelty: the idea of using transliteration has been extensively studied in the context of NMT, Speech. It has also been studied in the context of MLLMs by some recent work (which can be considered to be contemporary). IMO, this is a concern. (ii) focus on Indic languages: there are some concerns raised about the broader applicability of the techniques presented in the paper (personally, I disagree with this concern as Indic languages are important   for example, there are numerous papers which only report results on En De, En Ru translation) (iii) limited evaluation: the technique is evaluated only using the ALBERT model and other configurations (such as ROBERTA, XLM, etc) are not considered. IMO, it would have helped if the authors presented results on these models also (at least we would know if transliteration only helps in the case of small/compact models or even in the case of large models) (iv) missing references: there is a large body of related work on NMT, speech, etch which the authors had missed in their initial draft. This has been rectified in the updated version.  The reviewers did participate in the discussion with the meta reviewer (not with the authors though) and even after looking at the revised draft mentioned that the novelty is limited.   To summarise my views, I think the initial draft of the paper did need improvements and the final draft is a significantly improved version of the initial draft. However, I still feel the novelty is missing. Even the empirical novelty claimed by the authors is ;lacking due to the use of a single model (ALBERT).
This paper proposes a theory for double descent phenomena in denoting deep neural networks. There are two major concerns: (1) The assumption that the data lie in a low dimensional subspace is quite strong, and needs to be weaken or better justified. (2) The theory only works for r 1, where the rank is one. For general rank, how to apply the proposed analysis is hand wavy and not convincing. The paper can be significantly strengthen if these two issues could be addressed.
 This paper described a model that improves the performance of LM based pre trained sentence representation on semantic text similarity tasks (STS). The proposed approach is motivated by the observation that top layers in transformer based LMs are quite poor at this task per se. This paper proposes Contrastive Tension, a self supervised objective that drags representations of same sentence together, and pulls away representations of different sentences. The proposed method only relies on unlabelled data, and is relatively simple to implement. The paper demonstrates consistently strong results empirical results on the unsupervised semantic textual similarity (STS) task. Moreover, the paper provides reasonably good analysis.  On a negative side, the reviewers noted that the paper lacks a bit of analysis about the objective. The connection between the observation on layer wise performance of BERT on STS and the proposed contrastive training method is not clear. Second, while the result is interesting, its applicability is limited to STS.  Taking into account all the above, the reviews constitue a case for a solid weak accept. Therefore I recommend acceptance as a Poster contribution.
This paper studies self supervised learning for graph neural networks by proposing a framework called LaGraph. Both theoretical analysis and experimental evaluation are provided in the paper.  We acknowledge the merits of this paper, which include studying a relatively less explored topic, providing theoretical analysis and comparison with other methods, and requiring less memory than a strong baseline.  On the other hand, there are also outstanding concerns (even after the discussions) regarding the novelty and significance of the proposed method (despite the claims of the authors during the discussions), whether the performance improvement over strong baselines is significant across different datasets, and missing a more comprehensive ablation study (beyond the preliminary results provided during the discussion period), among others.  In its current form, this is certainly a borderline paper for a top conference such as ICLR. It would be a better paper if the outstanding concerns could also be addressed before publication.
The paper extends previous work on intrinsic reward design based on curiosity or surprise toward multiple intrinsic rewards based multiple model predictions and fuse the reward using meta gradient optimization.  While most reviewers find the paper clearly written, several reviewers do bring up the concern on limited contribution of the work on top of existing ones. Reviewers also would like to see experiments conducted in environment with sparse reward rather than the delayed reward setting constructed from dense reward environments. More ablation studies on the different design choices will also be helpful. 
This paper provides an interesting strategy for learning to explore, by first training on fully supervised data before deploying that policy to an online setting. There are some concerns, however, on the realism and utility of this setting that should be further discussed. If the offline data is not related to the contextual bandit problem, it would be surprising for this to have much benefit, and this should be better motivated and discussed. Because there are no theoretical guarantees for exploration, a discussion is needed and as suggested by a reviewer the learned exploration policies could be qualitatively examined. For example, the paper says "While these approaches are effective if the distribution of tasks is very similar and the state space is shared among different tasks, they fail to generalize when the tasks are different. Our approach targets an easier problem than exploration in full reinforcement learning environments, and can generalize well across a wide range of different tasks with completely unrelated features spaces." This is a pretty surprising statement, that your idea would not work well in an RL setting, but does work well in a contextual bandit setting.   There should also be a bit more discussion comparing to previous approach to learn how to explore, including in active learning. It is true that active learning is a different setting, but in both a goal is to become optimal as quickly as possible. Similarly, the ideas used for RL could be used here as well, essentially by setting gamma to 0.   Overall, the ideas here are interesting and well written, but need a bit more development on previous work, and motivation for why this approach will be effective. 
The paper analyzes connections between algorithmic fairness and domain generalization literatures. The reviewers found the paper interesting but they also raised some important concerns about it.  The applicability of the method presented in the paper is not clear nor well discussed in the paper.  The papers and the revised version do not not cite important related work.  The mathematical exposition in the paper is a bit hard to read. Even after revision, the reviewers find part of the paper(Appendix F) very hard to read.  Overall, the paper in the current version is below the high acceptance bar of ICLR.
This work introduces a new type of structured attention network that learn latent structured alignments between sentences in a fully differentiable manner, which allows the network to learn not only the target task, but also the latent relationships. Reviewers seem partial to the idea of the work, and it s originality, but have issues with the contributions. In particular:    The reviewers note that the gains in performance from using this approach are quite small and do not outperform previous structured approaches.
This paper offers an interesting and potentially useful approach to robust watermarking.  The reviewers are divided on the significance of the method.  The most senior and experienced reviewer was the most negative.  On balance, my assessment of this paper is borderline; given the number of more highly ranked papers in my pile, that means I have to assign "reject".
Overall, this paper got strong scores from the reviewers (2 accepts and 1 weak accept).  The paper proposes to address the responsibility problem, enabling encoding and decoding sets without worrying about permutations.  This is achieved using permutation equivariant set autoencoders and an  inverse  operation that undoes the sorting in the decoder.  The reviewers all agreed that the paper makes a meaningful contribution and should be accepted.  Some concerns regarding clarity of exposition were initially raised but were addressed during the rebuttal period.  I recommend that the paper be accepted.
This work introduces a method for supervised learning that takes a data generating process into account. While the paper proposes an interesting approach to learning a causally invariant model, the reviewers had several concerns about the proposed method. I thank the authors for having the paper revised, addressing the reviewers  comments. However, there are still legitimate issues unresolved about the specific theoretical results and assumptions made throughout the work.  I share similar concerns, and, therefore, recommend rejection. Still, I would like to encourage the authors to address the reviewers  problems in the paper s next iteration.  
This was a borderline paper with a split recommendation from the reviewers.  The authors took great care to answer the reviewer questions in detail, and the clarity and precision of the technical exposition was strengthened.  However, substantial technical content was added to the paper during the rebuttal process, which the reviewers were not able to fully and properly assess.  Overall, this is worthwhile research, but the paper is still maturing.  The contribution was perceived as incremental in light of previous work using LTL and FSAs in RL, despite the authors extensively re explaining the significance of the work in the rebuttal.  A resubmission is more likely to resonate with reviewers and ultimately achieve higher impact.  For completeness, it would help to also briefly acknowledge and compare to hierarchical RL work that also seeks to capture composable subtask structures, such as:  Sohn et al. "Hierarchical reinforcement learning for zero shot generalization with subtask dependencies", NeurIPS 2018  Sohn et al. "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies", ICLR 2020
After the author response and paper revision, the reviewers all came to appreciate this paper and unanimously recommended it be accepted.  The paper makes a nice contribution to generative modelling of object oriented representations with large numbers of objects.  The authors adequately addressed the main reviewer concerns with their detailed rebuttal and revision.
The paper presents a framework for scalable Deep RL on really large scale architecture, which addresses several problems on multi machine training of such systems with many actors and learners running.  Large scale experiments and impovements over IMPALA are presented, leading to new SOTA results. The reviewers are very positive over this work, and I think this is an important contribution to the overall learning / RL community.
All reviewers found the work interesting but worried about the extension to non bilinear games. This is a point the authors should explicitly address in their work before publication.
This paper shows that DeepSets and PointNet, which are known to be universal for approximating functions, are also universal for approximating equivariant set functions. Reviewer are in agreement that this paper is interesting and makes important contributions. However, they feel the paper could be written to be more accessible.  Based on the reviews and discussions following author response, I recommend accepting this paper. I appreciate the authors for an interesting paper and look forward to seeing it at the conference.
The paper studies factorizations of convolutional kernels. The proposed kernels lead to theoretical and practical efficiency improvements, but these improvements are very, very limited (for instance, Figure 5). It remains unclear how they compare to popular alternative approaches such as group convolutions (used in ResNeXt) or depth separable convolutions (used in MobileNet). The reviewers identify a variety of smaller issues with the manuscript.
This paper introduces a variant of the CycleGAN designed to optimize molecular graphs to achieve a desired quality.  The work is reasonably clear and sensible, however it s of limited technical novelty, since it s mainly just combining two existing techniques.  Overall its specificity and incrementalness make it not meet the bar.
Three knowledgeable referees rate this paper ok but not good enough or borderline positive (4,4,6), and one fairly confident referee rates it borderline positive 6. The referees discussed the authors  responses and, while they considered the idea and some of the theory good, they remain concerned, in particular about the experimental part and generalization discussion. The scores remained unchanged after the discussion. Hence I must reject the article. 
This paper explores a methodology for learning disentangled representations using a triplet loss to find subnetworks within a transformer.  The authors compare against several other methods and find that their method performs well without needing to train from scratch. The reviewers thought this paper was well written and the authors were very responsive during the review period.  However, there were some questions about the experimental setup and empirical performance of the paper, leaving the reviewers wondering if the performance was convincing.  We agree that there is value in exploring disentangled representations even if they do not necessarily improve performance (as the authors point out), but clearly explaining the reasoning behind all analyses (e.g. specifically choosing domains to introduce a spurious correlation), and justifying differences in performance is particularly important in these cases.
The paper proposes an approach for learning a decomposition of a scene into 3D objects using single images without pose annotations as training data. The model is based on Slot Attention and NeRF. Results are demonstrated on CLEVR and its variants.   The reviewers point out that the method is reasonable and the paper is quite good, but even after considering the authors  feedback agree that the paper is not ready for acceptance. In particular, the key concern is around experimental evaluation   that it is performed on one dataset (and variants thereof) and that the evaluation of the 3D properties of the model is not sufficiently convincing: it does not outperform 2D object learning methods on segmentation and is not compared to those on "snitch localization".  Overall, this is a reasonable paper, and the results are promising but somewhat inconclusive, so I recommend rejection at this point, but encourage the authors to improve the paper and resubmit to a different venue.  (One remark. The paper makes a point of not using any annotation. It is technically true, but in practice on CLEVR unsupervised segmentation works so well that it s basically as if segmentation masks were provided. If the authors could demonstrate that their method   possibly with provided coarse segmentation masks   works on more complex datasets, it would be a nice additional experiment)
This paper studies structured pruning methods, called kernel pruning in the paper which is also known as channel pruning for convolutional kernels.  A simple method is proposed that primarily consists of three stages: (i) clusters the filters in a convolution layer into predefined number of groups, (ii) prune the unimportant kernels from each group, and (iii) permute remaining kernels to form a grouped convolution operation and then fine tune the network. Although the novelty of the method is not high, it is simple and effective in experiments after the supplementary sota results in the long rebuttal. Majority of reviewers increase their ratings after the rebuttal (though one reviewer promised this but forgot to act), while some reviewers have concerns on the fairness to other authors by adding lots of new results in unlimited rebuttal and refuse to check more. In terms of the top end of performance, a reviewer thinks that "the authors haven t quite exceeded the results from existing works ("Discrimination aware channel pruning for deep neural network" and "Learning compression” algorithms for neural net pruning" for CIFAR 10 and many others on ImageNet)". In all, this work indeed lies on the boundary. After a discussion with other committee members, we recommend the acceptation of this work, if the authors could incorporate all the new results in rebuttal and get the reproducible codes released in the final version.
The paper investigates a detailed analysis of reduced precision training for a feedforward network, that accounts for both the forward and backward passes in detail.  It is shown that precision can be greatly reduced throughout the network computations while largely preserving training quality.  The analysis is thorough and carefully executed.  The technical presentation, including the motivation for some of the specific choices should be made clearer.  Also, the requirement that the network first be trained to convergence at full 32 bit precision is a significant limitation of the proposed approach (a weakness that is shared with other work in this area).  It would be highly desirable to find ways to bypass or at least mitigate this requirement, which would provide a real breakthrough rather than merely a solid improvement over competing work.  The reviewer disagreement revolves primarily around the clarity of the main technical exposition: there appears to be consensus that the paper is sound and provides a serious contribution to this area.  Although the persistent reviewer disagreement left this paper rated at the borderline, I am recommending acceptance, with the understanding that the authors will not disregard the dissenting review and strive to further improve the clarity of the presentation.
In this paper, the authors enhance the adversarial transferability of vision transformers by introducing two novel strategies specific to the architecture of ViT models: Self Ensemble and Token Refinement method. Comprehensive experiments on various models (including CNN s and ViT s variants) and tasks (classification, detection, and segmentation) successfully verify the effectiveness of the proposed method.  In general, the problem studied is relevant and important. The paper is well written and well motivated with empirical findings. The proposed two strategies are novel, simple to implement, and effective in practice. Following the author s response and discussion, the average score increases from 6 to 7.5, with most concerns well addressed. AC believes that the paper should be highlighted at the ICLR conference.
The work this paper presents is interesting, but it is not quite ready yet for publication at ICLR. Specifically, the motivation of particular choices could be better, such as summing over quantiles, as indicated by Reviewer 1. The inherent trade off between safety and speed of adaptation and how this relates to the proposed method could also use a clearer exposition.
This is an interesting paper discussing differential privacy for multi label classification. The initial reviews rated the paper with rather extreme scores, therefore I have invited an additional reviewer. This review did not clarify the issues raised by the most critical reviewer, but pointed out that the goal of showing how DP can be enforced in MLC is not fully obtained as there is a lack of the discussion concerning the MLC performance. This is also a problem raised in my comments. Taking this into account, I need to state that the paper is not ready for publication.
The paper discusses the auditing of AI systems which is clearly a very important topic. The approach targets mainly image classification systems where the main idea is to do the certification not in pixel space but in a latent space of a generative model. For the certification the authors use the existing CROWN IBP for cubes around test points in latent space. The authors report results on several datasets using existing methods for the generative models.  Most of the reviewers liked this work even though all of them also raised quite similar concerns. The positive reviews were to some extent based on the fact that auditing of AI systems is a very important problem   and I completely agree on this. On the other hand auditing of AI systems, in particular of safety critical systems as discussed with the chest x ray application in this paper, is a serious problem where a paper has to fulfill very high standards. In particular, being transparent about its limitations is crucial which in my opinion does not hold for the current version of this paper.  My main conerns are:   the motivating example (varying the angle from which a face is seen by less than 30 degress) is misleading as such a precise threat model is not feasible with the generative models discussed later on (see last bullet point)   there is no discussion on the potential trade off regarding test accuracy training the classifier i) in latent space, ii) with CROWN IBP using a rather shallow model. This is important to judge the utility of this approach   it has to be transparent what the trade off is regarding test accuracy and what kind of price one has to pay in terms of test accuracy for the certificates.   the deployment check discussed in Section 2.4 is nowhere implemented as later on only boxes in latent space around individual test points are checked but not a fixed box   the main problem connected to what I mentioned above with the motivating example is that the latent dimensions have to be interpreted by humans. I doubt that this can be made fully transparent to humans what such a certificate in latent space meand and thus to give very precise labels to the latent dimensions like "brightness" is in my point of view misleading. It suggests a much more finegrained semantic control than what is available and to use such inprecise and potentially wrong ``labels  is highly problematic for an auditing process as e.g. this paper does not contain any guarantee against "brightness" changes. For safety critical systems this is an important distinction.  As a minor point there is little technical novelty (certification with CROWN IBP is done with an existing library) but this plays no role in the decision.  In total the paper is not ready yet for publication. The authors should revise their manuscript by addressing the points above, being fully transparent about what the paper can do and what it cannot do. I suggest to do less examples but provide more details on how the authors envision the auditing/certification process for the given application, in particular also with the discussion about rejection of inputs not lying in certified areas (which has not been done in the present paper).
This paper proposes to measure the distance of the generator manifold to the training data. The proposed approach bears significant similarity to past studies that also sought to analyze the behavior of generative models that define a low dimensional manifold (e.g. Webster 2019, and in particular, Xiang 2017). I recommend that the authors perform a broader literature search to better contextualize the claims and experiments put forth in the paper.  The proposed method also suffers from some limitations that are not made clear in the paper. First, the measure depends only on the support of the generator, but not the density. For models that have support everywhere (exact likelihood models tend to have this property by construction), the measure is no longer meaningful. Even for VAEs, the measure is only easily applicable if the decoder is non autoregressive so that the procedure can be applied only to the mean decoding.   In this current state, I do not recommend the paper for submission.  Xiang (2017). On the Effects of Batch and Weight Normalization in Generative Adversarial Networks Webster (2019). Detecting Overfitting of Deep Generative Networks via Latent Recovery 
The paper focuses on self supervised learning (SSL) in the federated learning setting (FedSSL). Research in this area is timely and of significance. The authors phrase their work as primarily being an empirical study providing insights into the building blocks of FedSSL. The evaluation in the paper is quite thorough and the authors have been active in a detailed exchange regarding questions raised in the reviews. I would encourage the authors to fully implement the changes they promised into the revised manuscript and work towards timely release of open source code. (I appreciate internal policies of various institutions, but I do agree with the reviewers that it is more important that the code and experimental details be made public for papers such as this one, compared to some other papers.) I have chosen to disagree with some of the concerns raised in one of the reviews, in particular, I do agree with the authors that insights into the building blocks through empirical studies is a significant contribution, and also that FedEMA is a novel contribution. The discussion on this forum will remain for interested readers to come to their own conclusions about the relative performance of various methods.
The paper examined the folk knowledge that there are highly selective units in popular CNN architectures, and performed a detailed analysis of recent measures of unit selectivity, as well as introducing a novel one. The finding that units are not extremely selective in CNNs was intriguing to some (not all) reviewers. Further, they show recent measures of selectivity dramatically over estimate selectivity.  There was not tight agreement amongst the reviewers on the paper s rating, but it trended towards rejection. Weaknesses highlighted by reviewers include lack of visual clarity in their demonstrations, the use of a several generations old CNN architecture, as well as a lack of enthusiasm for the findings.
Two very confident and fairly confident reviewers rate this paper ok but not good enough, and two other fairly confident reviewers rate the article below the acceptance threshold. Therefore I must reject the article. The reviewers provided encouraging comments and suggestions on how the manuscript could be improved, which I hope the authors will find useful. 
As stated by reviewer 3 "This paper introduces a new model to perform image classification with limited computational resources at test time. The model is based on a multi scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al., 2017) and with a classifier at each layer." As stated by reviewer 2 "My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017). ".  The authors assert novelty in the sense that they provide a solution to improve computational efficiency and focus on this aspect of the problem. Overall, the technical innovation is not huge, but I think this could be a very useful idea in practice. 
The paper proposed a refined AIRL method to deal with the reward ambiguity problem in image captioning, wherein the main idea is to refine the loss function in word level instead in sentence level, and introduce a conditional term in the loss function to mitigate mode collapse problem.  The results show the proposed method improves the performance and achieves state of the art performance.  However there are concerns from the reviewers that the motivation of the work was not well explained and some inprecise parts exist in the paper.  The concept of "reward ambiguity problem" is not properly addressed according the opinion of reviewer2.  I would like to see these concerns be well addressed before the paper can be accepted.  
Summary:  Paper addresses the cross domain few shot learning scenario, where meta learning data is unavailable, and approaches are evaluated directly on novel settings. Authors propose a 3 step approach: 1) self supervised pretraining, 2) feature selection, 3) fine tuning, and demonstrate gains over state of art.   Pros:   Approach is novel for this setting   Paper is clear and easy to understand   Performance beats several prior methods   Experiments are thorough   Fundamental problem is worthwhile of investigation   Cons:   Some concerns among multiple reviewers on how hyperparameters are selected. Authors have provided more information and tables in the paper.   Training process is multi step and not unified. Authors provided additional information about unified training results, which yielded poorer results, likely due to overfitting from training many parameters at once.   Overall recommendation based on the consensus of reviewers and AC expertise: accept.
This paper describes a new generative model based on the information theoretic principles for better representation learning. The approach is theoretically related to the InfoVAE and beta VAE work, and is contrasted to vanilla VAEs. The reviewers have expressed strong concerns about the novelty of this work. Some of the very closely related baselines (e.g. Zhao et al., Chen et al., Alemi et a) are not compared against, and the contributions of this work over the baselines are not clearly discussed. Furthermore, the experimental section could be made stronger with more quantitative metrics. For these reasons I recommend rejection.
This paper develops a smoothing procedure to avoid the problem of posterior collapse in VAEs. The method is interesting and novel, the experiments are well executed, and the authors answered satisfactorily to most of the reviewers  concerns. However, there is one remaining issue that would require additional discussion. As identified by Reviewer 1, the analysis in Section 3 is only valid when the number of layers is 2. Above that value, "it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically". Thus, the scope of the analysis in Section 3 deserves further discussion. Given the large number of ICLR submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue.
This paper undertakes an empirical investigation of overparameterized neural networks, studying the last hidden representation and identifying "representation mitosis," a cloning effect whereby neurons split into groups that carry the same information. The effect is observed for a variety of architectural configurations/datasets, and a detailed set of experiments are performed to investigate the behavior.  The reviewers had split opinions about this paper, with most reviewers appreciating the novelty and salience of the observations, but with some reviewers expressing skepticism about the generality of the effect. While the experiments are thorough and revealing, the practical importance of representation mitosis remains somewhat unconvincing.  A primary motivating factor for the analysis is the search for an explanation of the unexpectedly good generalization behavior of oveparameterized networks and the origin of "benign overfitting." As highlighted in the reviews, the sensitivity of the mitosis effect to (1) training to zero loss and (2) optimal regularization suggests that it cannot be the sole explanation for benign overfitting, since the latter can and does occur without these conditions. The authors acknowledge this situation, and respond that their focus is on state of the art models used by the community, rather than on toy settings. For this to be a persuasive response, more compelling results in these state of the art situations should be evidenced   in particular, as several reviewers pointed out, the negative results on ImageNet undermine this point to some extent.  Overall, representation mitosis does seem like an interesting and potentially important phenomenon, but further work is needed to develop persuasive evidence in support of the interpretations and implications. While this is a borderline submission, I believe it falls just short of the mark, and cannot recommend acceptance.
The reviewers develop a novel technique for training neural networks that are provably robust to adversarial attacks, by combining provable defenses using convex relaxations with latent adversarial attacks that lie in the gap between the convex relaxation and the true realizable set of activations at a layer of the network. The authors show that the resulting procedure is computationally efficient and able to train neural networks to attain SOTA provable robustness to adversarial attacks.  The paper is well written and clearly explains an interesting idea, backed by thorough experiments. The reviewers were in consensus on acceptance and relatively minor concerns were clearly addressed in the rebuttal phase.  Hence, I strongly recommend acceptance.
This work performs an analysis of the generalization ability of pre trained models under the condition of vocabulary scrambling. The paper is well written and easy to understand. However, a full story and investigation into the cause of the observed transfer under word scrambling is lacking. For example, do more powerful models transfer less because pre training is more effective?  While the effect described in the paper is interesting, it lacks a solid connection to important areas such as adversarial attacks, cross lingual domain shifts and doesn t seem to have any effect on the application of fine tuning to pre trained models. The experimental section could also be improved with comparisons to other more recent models such as RoBERTa and GPT 2. Even though these more recent models are still Transformer based models it can help answer the question if more powerful models transfer less under word scrambling as raised by reviewer 6a8W. The results on LSTMs seem to imply this case. We thank the authors for including additional results on DeBERTa but this was insufficient to change the authors  opinion of the value of the paper.
All of the reviewers thought that this paper addresses an interesting and important problem.  Several of the reviewers thought that the paper gave a creative approach for training bloom filters and this would be of interest to the community. 
This paper examines conditional GANs, which are found to lead to model collapse in low data settings. The paper proposes what appears to be a simple but effective method that addresses the issue. Reviewers were generally happy with the experiments and the utility of the observations and analysis. Code for the method was provided during the author response period. Only one reviewer did not vote to accept this paper, but they did acknowledge that the authors had addressed their concerns during the discussions with the authors. All others rated the paper as an accept. The AC recommends accepting this paper.
The paper proposed a deep, Bayesian optimization approach to RL with model uncertainty (BAMDP).  The algorithm is a variant of policy gradient, which in each iteration uses a Bayes filter on sampled MDPs to update the posterior belief distribution of the parameters.  An extension is also made to POMDPs.  The work is a combination of existing techniques, and the algorithmic novelty is a bit low.  Initial reviews suggested the empirical study could be improved with better baselines, and the main idea of the proposed method could be expended.  The revised version moves towards this direction, and the author responses were helpful.  Overall, the paper is a useful contribution.
This paper presents the empirical relation between the task granularity and transfer learning, when applied between video classification and video captioning. The key take away message is that more fine grained tasks support better transfer in the case of classification captioning transfer on 20BN something something dataset.  Pros: The paper presents a new empirical study on transfer learning between video classification and video captioning performed on the recent 20BN something something dataset (220,000 videos concentrating on 174 action categories). The paper presents a lot of experimental results, albeit focused primarily on the 20BN dataset.  Cons: The investigation presented by this paper on the effect of the task granularity is rather application specific and empirical. As a result, it is unclear what generalizable knowledge or insights we gain for a broad range of other applications. The methodology used in the paper is relatively standard and not novel. Also, according to the 20BN something something leaderboard (https://20bn.com/datasets/something something), the performance reported in the paper does not seem competitive compared to current state of the art. There were some clarification questions raised by the reviewers but the authors did not respond.  Verdict:  Reject. The study presented by the paper is a bit too application specific with relatively narrow impact for ICLR. Relatively weak novelty and empirical results.
Authors propose an autoencoding echo state machine for a one shot one class time series classification task. Their approach feeds a (one dimensional) error signal over time relative to a reference training datum to SVMs. Training is very fast by design. OFC signal analysis has practical value in neuroscience. But only one benchmark (seq MNIST) was used to evaluate their method. While the performance seem impressive, no explanation of why the internal representation learned by the proposed system is superior and robust to noise was provided. No sequential autoencoders or latent neural trajectory inference methods were compared. Although the manuscript has greatly improved through the review rebuttal process, there are missing key details (e.g. length of E(t) used for classification important for real time application, initial state for the reservoir, choice of W_in  important since it seems to be a chaotic network that s driven by strong input). While there is novelty in the approach, there is a general lack of enthusiasm among the reviewers for the manuscript as is. The reviewers and AC strongly encourage the authors to further developed these ideas and add thorough analyses for another conference.  (BTW, perhaps it s worth citing https://doi.org/10.1109/IJCNN.2016.7727309, since autoencoder combined with reservoir computing has been used for anomaly detection.)
this is an interesting approach that applies the idea of dynamically controlling the amount of information from the input fed into the classifier (some of the earlier approaches have used this idea for, e.g., parsing, real time translation, online speech recognition, and so on...) this is also related to some of the recent work on hierarchical recurrent nets [Chung et al.]. unfortunately, two of the reviewers and other commenters found this manuscript needs more work to clarify motivation, implication and relationship to other existing works, with which i don t necessarily disagree. 
The paper presents a "conceptual  advance connecting causality, disentangled representation learning, invariant representations and robust classification". Authors propose to decompose the image generation process to independent mechanism that can be composed (foreground masks (shapes), forground texture, and backgrounds), allowing for a specific image to generate counterfactuals , by changing some variations factors, while keeping other fixed. One can use interventional data to augment classifiers, this can lead in certain cases to improvement in accuracy and in other in improving the robustness.   There was concerns about the clarity of the paper regarding the structured causal model considered and its applicability beyond image generation, experimental protocol for choosing hyperparameters (loss scaling and ratios of real data and interventional samples ) and some missing references. The rebuttal of the authors and their updated paper reflected comprehensively all those concerns and addressed them, highlighting limitations of the method and adding more examples of its failures.   I liked the ideas and concepts in  this paper , and it will be exciting to generalize such generative approach to other domains, this  work is a first step. I think it will be good addition to ICLR program 
The paper proposes to integrate multiple bit configurations (including pruning) into a single architecture, and then automatically select bit resolution through binary gates. The overall approach can be differentiable and optimized with parameters. However, as pointed out by the reviewers, the novelty of this paper can be the big question. Also, it seems to be hard or unpractical to employ different number of bits for layers, given the standard GPU and CPU hardware. 
This paper generated quite a bit of controversy among reviewers. The main claim of the paper is that Adam and related optimizers are broken because their "weight decay" regularization is not actually weight decay. It proposes to modify Adam to decay all weights the same regardless of the gradient variances.  Calling Adam s weight decay mechanism a mistake seems very far fetched to me. Neural net optimization researchers are well aware of the connection between weight decay and L2 regularization and the fact that they don t correspond in preconditioned methods. L2 regularization is basically the only justification I have heard for weight decay, and despite rejecting this interpretation, the paper does not provide an alternative justification.  Decoupling the optimization from the cost function is a well established principle. This abstraction barrier is not completely clean (e.g. gradient noise has well known regularization effects), and the experiments of this paper perhaps provide evidence that the choices may be coupled in this case. This is an interesting finding, and probably worth following up on. However, the paper seems to sweep the "decoupling optimization and cost" issue under the carpet and take for granted that the decay rate is what should be held fixed. All three reviewers found the presentation to be misleading, and I would agree with them. While there may be an interesting contribution here, I cannot endorse the paper as is. 
The paper proposes a two level hierarchical algorithm for efficient and scalable multi agent learning where the high level policy decides a reduced space for low level to explore in. All the reviewers liked the premise and the experimental evaluation. Reviewers had some clarification questions which were answered in the authors  rebuttal. After discussing the rebuttal, AC as well as reviewers believe that the paper provides insights that will be useful for the multi agent learning community and recommend acceptance.
The reviewers and I all agree that this analysis of multi task and transfer learning from the random matrix perspective is novel and theoretically sound. While some reviewers expressed concern about the restriction to Gaussian mixtures, the strength of the explicit results undoubtedly justifies this assumption, and the generalization to concentrated random vectors significantly mitigates any concerns. I recommend acceptance.
This work reduces the time and memory complexity of Transformer for long sequences by using multiscale pooling to reduce attention from quadratic to linear complexity. Theoretical and experimental results show good results and are very competitive with the state of the art. The paper is well written and experiments are thorough. The additional results in the rebuttal also helped reduce some of the reviewers  concerns. Though the work is somewhat incremental and the experimental settings for the baselines are different, the thoroughness of the experiments and the good results make this a good addition to the conference. For the final version, the authors should provide code, provide error bars and details of the speedup of GLUE.
This paper links OOD generalization with adversarial training and argues that adversarial training can help address the problem of OOD generalization. Based on all responses and reviews, there still are novelty concerns in this paper. In the meantime, this paper lacks theoretical justifications. More importantly, DAT only considers very limited situations regarding DG, which also reflects on its experimental results. In the following, I summarize the drawbacks of this paper for the possible revision in the future.  1. It seems not novel to link AT with OOD generalization since two reviewers show some references related to using AT to address DG.  2. From Eq. (9), DAT is based on perturbations rather than transformations. This means that DAT only considers very limited situations regarding DG. In the ordinary DG, source and target domains are from the same meta distribution, which is clearly a more general case compared to the case considered in this paper. DAT based AT might mislead the research direction of DG. It would be better to consider smart ways to generate adversarial examples, such as "Pixeldefend: Leveraging generative models to understand and defend against adversarial examples" (ICLR2018).  3. Eqs. (7), (8) and (10) are not rigours. It is not convincing to propose a method based on these formulas.  4. The method doesn t show substantial improvements compared to ERM in most tasks (CMNIST dominates the average), which implies the limitations of DAT (see 3).  5. There are no theoretical contributions regarding DG. This paper does not mention the key assumption behind the DAT. For example, DG can be a well defined problem if source and target domains are from the same meta distribution. However, this paper does not clarify what assumptions it assumes and does not show how DAT can address DG in theory.  Based on the above drawbacks, I recommend rejection for this paper.
The reviewers agreed that while this is a well written paper, it is low on novelty and does not make a substantial enough contribution. They also pointed out that although the reported MNIST results are highly competitive, possibly due to the use of a powerful ResNet decoder, the CIFAR10/ImageNet results are underwhelming.
The paper investigates the use of equivariant neural network architectures for model free reinforcement learning in the context of visuomotor robot manipulation tasks, exploiting rotational symmetries in an effort to improve sample efficiency. The paper first provides a formal definition and theoretical evaluation of a class of MDPs for which the reward and transition are invariant to group elements ("group invariant MDPs"). It goes on to describe equivariant versions DQN, SAC, and learning from demonstration (LfD). Experiments on a set of different manipulation tasks reveal that the proposed architectures outperform contemporary baselines in terms of sample complexity and generalizability, while ablations demonstrate the contribution of the different model components.  The idea of structuring a neural architecture to exploit symmetry present in a domain as a means of improving sample complexity is compelling and principled. The contributions of the paper are two fold. First, while the idea of exploiting symmetry in the context of deep RL is not new, the paper describes a variation of equivariant DQN that is effective for visual control domains (visuomotor control) that are more challenging and realistic than those considered previously. Second, the paper proposes novel equivariant versions of SAC and LfD and validates their effectiveness through extensive experiments. Following a detailed author response to the initial reviews together with the inclusion of additional experiments and other updates to the paper, the reviewers largely agree on the significance of these contributions and value of the paper as a whole.
This paper proposes a new idea for performing knowledge distillation by leveraging teacher’s classifier to train student’s penultimate layer feature via proposing suitable loss functions. Reviewers appreciate the simultaneous simplicity and effectiveness of the method. A comprehensive set of studies are performed to empirically show the effectiveness of the method. Specifically, the proposed distillation method is shown to outperform state of the art across various network architectures, teacher student capacities, datasets, and domains. The paper is well written and is easy to follow. All reviewers rate the paper on the accept side (after the rebuttal) and believe the new perspective this work provides on distillation and its simplicity to implement can lead it to gain high impact. I concur with the reviewers and find this submission a convincing empirical work, and thus recommend for accept. 
This paper studies the question of why a network trained to reproduce a single image often de noises the image early in training. This an interesting question and, post discussion, all three reviewers agree that it will be of general interest to the community and is worth publishing. Therefore I recommend it be accepted. 
The paper introduces convex reformulations of problems arising in the training of two and three layer convolutional neural networks with ReLU activations. These formulations allow shallow CNNs to be training in time polynomial in the number of data samples, neurons and data dimension (albeit exponential in filter lengths). These problems are regularized in different ways (L2 regularization for two layers, L1 regularization for three layers), providing new insights into the connection between architectural choices and regularization. The paper also provides experiments showing convex training of neural networks on small datasets.    Pros and cons:  [+] The theoretical results show that globally optimal training of shallow CNNs can be achieved in time fully polynomial, i.e., polynomial in the number of data samples, neurons and data dimension. This is significant theoretical progress, since the corresponding results for fully connected neural networks require time exponential in the rank of the data matrix. There is, however, an exponential dependence on the filter length (or the rank of the patch matrix). In particular, the computational complexity is proportional to $(nK/r_c)^{3r_c}$, where $n$ is the number of data points. While CNNs do use relatively small filters, this becomes prohibitive even when $r_c$ is a moderate constant. E.g., the experiments use filters of length $3$. Here, the comments of the reviewers about generalization may be appropriate; perhaps experiments that evaluate the performance of these networks in terms of generalization may show the disadvantages of using very small filters.   [+] The work provides interesting and rigorous insights into the relationship between architecture and implicit regularization, with different network architectures leading to different regularizers (L1, L2, nuclear). Developing these insights for deeper architectures could lead to important insights even in situations where the convex relaxation is challenging to solve efficiently.   [+] Although the theoretical results require overparameterization, in the sense that strong duality holds when the number of filters is large relative to the number of data points, the authors convincingly argue that this degree of overparameterization is commensurate with, or even smaller than, the degree of overparameterization present in many experimental/theoretical works in the literature.   [+/ ] The paper is mathematically precise and is written in a rigorous fashion, but is occasionally heavy on notation. The paper could be more impactful on empirical work on neural networks if it could provide more intuition about how the various forms of equivalent regularization arise from different architectures.   All three reviewers express appreciation for the paper’s fresh insights into global optimization of shallow CNNs and the connection between architectural choices and regularization. The AC recommends acceptance.  
Does not seem to be a complete submission (only one page), all reviewers agree on rejecting.
The paper proposes the generalization performance of distillation from random networks as a metric of diversity, named RND. Intuitively, the more diverse the generated datasets, the more difficult it should be for a model to learn a random computation. The reviewers agree that the metric has a novel perspective. Unfortunately, the paper is not sufficiently developed to be accepted at this point. It is currently missing a number of experiments that would demonstrate that this metric is indeed a measure of diversity:  1.) RND shows sensitivity to the truncation trick in GANs (for images), and limiting the size of vocabulary in text, but does not show sensitivity to any other changes in diversity (such as human judgment of diversity) 2.) It does not compare to previous metrics of diversity, of which there are many 3.) How sensitive is RND to architecture choice. 4.) It is non obvious to what extent the metric is sensitive to image/text quality  Strong metrics should demonstrate lack of "failure modes", as the utility of a metric is its inability to be gamed. Currently, the paper does not demonstrate this property, though I imagine that more work will help clear up the strengths and weaknesses of the metric.  As a result, I can only recommend rejection.
The major criticism of this paper after the initial reviews was a lack of experimental results on deeper and more modern architectures that include skip connections.  The authors added results to the paper to address these issues.
Pros   Interesting approach to induce sparsity, trains faster than alternative approaches Cons   Fairly complex set of heuristics for pruning weights   Han et al. works well, although the authors claim it takes more time to train, which may not not hold for all training sets and doesn’t seem like a strong enough reason to choose an alternative appraoch  Given these comments, the AC recommends that the paper be rejected. 
This paper suggests that in multi label classification (where there are multiple y that could be correct), the usual conformal prediction setup could be too conservative (too large a set with too many false positives), because it asks for "full" coverage. They propose to change the error metric from coverage to precision, to instead output a smaller set, with higher precision, potentially with the loss of coverage. The paper thus produces two variants of conformal prediction: guaranteeing that the expected number of false positives is at most k, and that the probability that the #false positives being > k is < \delta. The experimental study is interesting.  I followed the extensive discussion thread, and appreciate the authors  and reviewers  willingness to engage. However, in the end, the (excellent) reviewers were somewhat still not enthusiastic about the paper, and with nobody willing to champion the paper, it ended up on the borderline, in the bottom half of my set. Nevertheless, I read through the paper in detail myself to make sure, but I find enough reasons that suggest that the paper is not ready for publication currently, and would benefit from a significant overhaul.  It seems like the final algorithm is a slight variant of nested conformal prediction (a well known and oft cited paper by Gupta et al, 2020 that the authors seem to miss) in the following sense. The usual conformal for classification framework would order the labels in terms of a score (like posterior probability) and then return a set of labels whose score is less than some threshold. (The same style of procedure can be used for the single label and multi label case also.) The nesting appears to be the same high level framework used in this paper, except that the threshold is chosen by a different rule from standard conformal prediction (ie same nesting, different threshold). Writing the algorithm more transparently will make for a tighter connection to the conformal literature   at the moment, the authors claim a fair bit of novelty, but this is partially due to the omission of this reference and the cleaner broad (nested conformal) framework under which this work (as well as standard conformal) sit.  At the same time, I was not fully convinced of the central theoretical claim of the paper, in Proposition 4.6. The authors claim that since Theorem 4.3 is simultaneously valid for all j in B satisfying a condition (the filtered set), the theorem can also be invoked for a data dependent index (chosen via (12)). This does not appear to the true, and at the very least requires careful justification. At a high level, dropping the conditioning for simplicity, Thm 4.3 reads as "forall j in [B], E[A_j] <  c", but this does not imply that for a data dependent j hat, we have E[A_{j hat}] <  c. It would have been true, if the forall j appeared as a sup_j inside the expectation (or as a forall j inside the probability, rather than outside). The authors may want to clarify more carefully, if it is indeed true.  Minor: I also continue to find typos in the main results and proofs. These are minor, but should be corrected. I believe that in the last line of Theorem 4.3, the X_i should be X_{n+1}. In the display after (25), that should be T_k, and not T_{k,\delta}. There appear to be some other potentially missing references as well. For example, while distribution free conformal approaches are cited, distribution free calibration approaches are not, within the related work section. At the same time, some very recent papers, such as by Bates et al, or Angelopoulos et al, appear to be overemphasized in the introduction. A more fair coverage of related work could be useful.
The authors propose a new approach to learning cross lingual embeddings from parallel data. For an overview of this literature, see [0]. Reviews are mixed, and some objections seem unresolved. The authors also ignore a new line of research in which pretrained language models are used to align vocabularies across languages, e.g., [1 2] The paper would also benefit from a discussion of massively parallel resources such as JW300 and WikiMatrix. Finally, it feels odd not to compare to distilled representations from NMT architectures, e.g., [3].   [0] http://www.morganclaypoolpublishers.com/catalog_Orig/product_info.php?products_id 1419 [1] https://www.aclweb.org/anthology/N19 1162.pdf [2] https://www.aclweb.org/anthology/K19 1004.pdf [3] https://arxiv.org/abs/1901.07291
This paper received overall positive scores. All the reviewers agree that the approach presented in the paper is simple yet effective and the results are very impressive with >95% parameter reduction while maintaining the accuracy. The authors promptly revised the paper based on initial reviews. Therefore, I recommend accept and hope the authors incorporate the additional comments from Reviewer3 after the discussion. 
The authors propose to accelerate neural architecture search by using feature similarity with a given teacher network to measure how good a new candidate architecture is. The experiments show that the method accelerates architecture search, and has competitive performance. However, both Reviewers 1 and 3 noted questionable motivation behind the approach, as the method assumes that there already exists a strong teacher network in the domain where we architecture search is performed, which is not always the case. The rebuttal and the revised version of the paper addressed some of the reviewers  concerns, but overall the paper remained below the acceptance bar. I suggest that the authors further expand the evaluation and motivate their approach better before re submitting to another venue. 
The authors have made significant efforts to thoroughly address all the concerns. Due to the amount of discussions, I had to go through the paper myself and agree with the authors on many of the points. In my opinion, this is a solid theoretical work on the pitfalls of IRM. 
Although the paper is clearly written overall and well motivated, reviewers raised several crucial concerns and, unfortunately, the authors did not respond to reviews.  During the discussion, reviewers agree with that this submission is not ready for publication. In particular, empirical evaluation is not thorough as important baselines are not included and discussion is not convincing.  I will therefore reject the paper. For future submission, I strongly recommend the authors to do author response. There are many cases where the reviewers change their scores based on the interaction between the authors and the reviewers, which is healthy for the review process.
This nice paper gives a better understanding of how Curriculum Learning (CL) affects image classification. In particular, it gives insight into cases such as noisy training data and limited training time. It shows that examples can be rated by difficulty to some extent, in that the order in which examples are learned seems to be consistent across runs. The paper is thorough and well written.
This paper proposes a neural architecture search method based on greedily adding layers with random initializations. The reviewers all recommend rejection due to various concerns about the significance of the contribution, novelty, and experimental design. They checked the author response and maintained their ratings. 
The manuscript proposes (TRansfer and Marginalize) TRAM method that integrates the privileged information into the learned network weight through weight sharing at training time and approximately marginalizes over the privileged information at test time. TRAM can also be combined with methods for dealing with noisy labels, distillation (Distilled TRAM) and heteroscedastic output layers (Het TRAM). Experiments are performed on both realistic and synthetic datasets including CIFAR 10H, re labeled ImageNet, and Civil Comments Identities.  Reviewers agreed on several positive aspects of the manuscript, including: 1. The proposed methods have simple architectures (not requiring specific modules, e.g., Gaussian dropout [Lambert et al., 2018], for the marginalization); 2. The proposed method can in principle be applied to any neural network model and has zero overhead at prediction time.  Reviewers also highlighted several major concerns, including: 1. The analysis is performed on edge cases such as linear and non linear sine models. There is no analysis for the classification case that this manuscript is targeted for. The simple cases are only true when the feature extraction network is kept unchanged during training; 2. Empirically, the experiments are conducted in a limited and counter intuitive;  3. Lack empirical evidence suggesting that the representations learned with access to privileged information are more robust against label noise; 4. Lack quantitative (or even qualitative) evidence about how, how much, and what kind of privileged information is transferred through weight sharing in realistic deep neural network models.  Several new experiments have been added to show, among others: representations learned with privileged information outperform representations learned without access to privileged information (using a linear classification model on ImageNet), better quantitatively and qualitatively understanding how and how much privileged information is transferred in realistic deep networks.  Post rebuttal, reviewers stayed with borderline ratings, and they have suggested further improvements: simulating with more annotators by using different checkpoints and/or different hyperparameters, collecting a real world large scale dataset such that the privileged information is insignificantly expensive to obtain along with the main annotations, and disentangling the effect of the pretraining model on the denoising method.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
This paper analyzes the universal approximation power of deep residual neural networks based on nonlinear control theory. Some concerns regarding the clarify, significance and connection to practice were raised, and partially addressed by the rebuttal. After discussions, all the reviewers feel positive about the contribution of the paper.
This paper aims to use pre training to bridge the gap in performance between 2D GNN and 3D GNN. Specifically, during pretraining, it trains both 2D GNNs and 3D GNNs on data equipped with 3D geometry to maximize the mutual information between the 2D GNN representation with the 3D GNN representation. The proposed approach is interesting and novel and the paper presents some promising results showing that the pre training does provide some benefits for downstream tasks where 3D geometry information is not available in comparison to several other baseline pretraining methods. While the reviewers agree that property prediction without only 2D graph is a practically important setting for high throughput screening, there are concerns about whether the current set of results paint a clear picture on the benefits and superiority of the proposed methods to alternatives (e.g., vs conf gen) even after the revision. This is not due to lacking of results, but more of a presentation issue where results are not organized and discussed clearly to provide a coherent story.  We do see clear and strong potential for this paper but it needs a careful rewrite/re organization to tease out the key messages and how the experiments support them.
The paper proposes a new method to train ensembles of classifiers that are robust to adversarial attacks, in particular black box transfer based ones. This is achieved by enforcing the output of early layers of different members of the ensemble to have, on average, gradients with low cosine similarity, which should in turn create different decision boundaries. For this, the authors design a specific loss function, PARL, to be minimized at training time. Two reviewers gave the score of 6 while two reviewers gave the score of 3. The main concerns are: 1) unclear meaning of taking the sum of the gradients of different neurons, and why the similarity of that across models is a proxy for similarity of the decision boundaries; 2) lack of experiments, that is, omitting a simpler baseline like individual robust classifiers. Positive score reviewers also did not champion the paper, thus, the paper should be well addressed these main concerns in the revision and cannot accept to ICLR for now.
All reviewers concur on the fact that the paper contains solid ideas. The discussion helped clarify the case of class imbalance and no major concerns remained after discussion phase. I thank the authors for the additional details on execution time / complexity.  On a separate note and perhaps to dig further in the paper s ideas,  1  the validity of the Gaussian assumption carried in the paper was raised (e.g. ercK), but I would like to point out that Theorem 2 can also be derived for general exponential families given the objective in (2), with perhaps a reformulation of the trace constraint (still, this would imply the knowledge of the exponential family for the KL divergence to simplify).  2  when it comes to protecting labels, the authors might want to have a look at the rich literature on learning from label proportions, which shows that the knowledge of the class is not necessary to learn a supervised model (see for example Patrini et al, NeurIPS / NIPS 2014). Thus, protecting the class could in fact be more achievable than by just considering that learning “needs observed classes”.
The authors present a matrix factorization for the social behavior of honey bees in a hive. All the reviewers appreciated the interesting application.  However, substantial concerns were raised about the model motivation and the interpretation of the learned factors. To quote one reviewer, "Some of these bells and whistles may not even be needed, so simplifying the model and streamlining the text would go a long way for me." Another said, "The paper requires more principled motivation for the choices the authors made as well as cleaning up the notation."  The authors did address some of these concerns in discussion, but there are too many lingering concerns to recommend acceptance.  Given the unique application of this paper, the authors might also consider a journal that specializes in computational biology instead.  
The paper describes a simple method for neural network compression by applying Shannon type encoding. This is a fresh and nice idea, as noted by reviewers. A disadvantage is that the architectures on ImageNet are not the most efficient ones. Also, the review misses several important works on low rank factorization of weights for the compression (Lebedev et. al, Novikov et. al).   But overall, a good paper.
This paper compares “Graph Augmented MLPs” (GA MLP), which augment node features by a single aggregation of neighbors and then pass the resulting features through an MLP, to graph neural networks (GNNs). The paper establishes results on representational power of some GA MLP models being less powerful than GNNs. While practitioners may not change their behavior as a result, the work appears carefully done, is novel, and reviewers are mostly in agreement that the paper is a nice read and good contribution to the field.
The reviewers agree that this is a novel paper with a convincing evaluation.
Pros:   novel method for continual learning   clear, well written   good results   no need for identified tasks   detailed rebuttal, new results in revision  Cons:   experiments could be on more realistic/challenging domains  The reviewers agree that the paper should be accepted.
This paper proposed to theoretically explain why a pre trained embedding network with self supervised training (SSL) can provide representation for downstream few shot learning (FSL) tasks. The review process finds that the paper may over claim the results and that the results seem unsatisfactory. Both Reviewer 4 and Reviewer 5 expressed concerns regarding the writing, organizing, and grammar errors of this paper. The paper needs a substantial revision to improve clarity and accessibility. As pointed out by Nikunj Saunshi’s public comment, this paper may benefit from discussing the differences from the previous works, including [1].    [1] Arora et al., A Theoretical Analysis of Contrastive Unsupervised Representation Learning, ICML 2019
The authors propose to understand spectral bias during training of neural networks from the perspective of the NTK. While reviewers appreciated aspects of the work, the general consensus was that the current version is not ready for publication; some concerns stem from whether the the NTK model and finite neural networks are sufficiently similar that we should be able to gain real practical insights into the behaviour of finite models. This is partly an empirical question, and stronger experiments are required to have a better sense of the answer. Nonetheless, the authors are encouraged to persist with this work, taking into account reviewer comments in future revisions.  
This paper introduces a new adaptive variational dropout approach to balance accuracy, sparsity and computation.   The method proposed here is sound, the motivation for smaller (perhaps sparser) networks is easy to follow. The paper provides experiments in several data sets and compares against several other regularization/pruning approaches, and measures accuracy, speedup, and memory. The reviewers agreed on all these points, but overall they found the results unconvincing. They requested (1) more baselines (which the authors added), (2) larger tasks/datasets, and (3) more variety in network architectures.  The overall impression was it was hard to see a clear benefit of the proposed approach, based on the provided tables of results.  The paper could sharpen its impact with several adjustments. The results are much more clear looking at the error vs speedup graphs. Presenting "representative results" in the tables was confusing, especially considering the proposed approach rarely dominated across all measures. It was unclear how the variants of the algorithms presented in the tables were selected explaining this would help a lot. In addition, more text is needed to help the reader understand how improvements in speed, accuracy, and memory matter. For example in LeNet 500 300 is a speedup of ~12 @ 1.26 error for BB worth it/important compared a speedup of ~8 for similar error for L_0? How should the reader think about differences in speedup, memory and accuracy perhaps explanations linking to the impact of these metrics to their context in real applications. I found myself wondering this about pretty much every result, especially when better speedup and memory could be achieved at the cost of some accuracy how much does the reduction in accuracy actually matter? Is speed and size the dominant thing? I don t know.  Overall the analysis and descriptions of the results are very terse, leaving much to the reader to figure out. For example (fig 2 bottom right). If a result is worth including in the paper it s worth explaining it to the reader. Summary statements like "BB and DBB either achieve significantly smaller error than the baseline methods, or significant speedup and memory saving at similar error rates." Is not helpful where there are so many dimensions of performance to figure out. The paper spends a lot of time explaining what was done in a matter of fact way, but little time helping the reader interpret the results.  There are other issues that hurt the paper, including reporting the results of only 3 runs, sometimes reporting median without explanation, undefined metrics like speedup ,%memory (explain how they are calculated), restricting the batchsize for all methods to a particular value without explanation, and overall somewhat informal and imprecise discussion of the empirical methodology.  The authors did a nice job responding to the reviewers (illustrating good understanding of the area and the strengths of their method), and this could be a strong paper indeed if the changes suggested above were implemented. Including SSL and SVG in the appendix was great, but they really should have been included in the speedup vs error plots throughout the paper. This is a nice direction and was very close. Keep going!
This paper explores extending k means to allow to clusters with non convex shapes.  This paper introduces a new algorithm, relying on empirical comparisons to illustrate its contribution. The main issue with the paper is that the empirical claims do not support that the new method is indeed better. The paper claims the new method outperforms the competitors in most cases. However, the original submission reported median performance and when the authors provided mean performance and additional baseline methods (at the reviewers  request) there appear to be little evidence to support the claim. In addition there are no measures of significance provided. The authors provided no commentary to help the reviewers understand the new results. There might be some important speed gains at the cost of final performance, but on the evidence provided we are not able to evaluate the cost in final performance.  The text changes size after section 5.3 and is 9% smaller. Watch out for this formatting issue in future submissions  
This paper introduces LambdaNetworks, a new method to capture long range interactions in data (such as global context in images). The method is novel and simple, and the experimental results are strong, especially in terms of speed/accuracy tradeoffs. The paper is well written and easy to follow. For these reasons, I recommend to accept the paper.
All reviewers appreciate the empirical analysis and insights provided in the paper. The paper also reports impressive results on SSL. It will be a good addition to the ICLR program. 
The work focuses on the observation that, given a certified epsilon robust model and a certified clean input x, many inputs within the epsilon ball around x are themselves not epsilon certifiable although they are correctly classified. The authors argue that an adversary can exploit this property to produce inputs which are correctly classified by the model yet are not certifiably robust. Reviewers agreed that the paper was overall well written, the methods were clear and overall evaluated thoroughly, and many felt that the main idea was interesting. There were some concerns regarding the significance of the contribution, the primary observation itself is arguably novel but somewhat obvious, and the proposed algorithm for finding non certifiable points isn t a significant contribution when standard techniques like PGD are sufficient. Much of the reviewer discussion concerned whether or not the proposed attack made sense as a threat model. It is the AC s opinion that this discussion did not reach any meaningful conclusions. It is important to remember that the lp threat model is intended as an abstract toy game so that a formal theory of neural network certification can be developed under idealized settings. It is not intended to model any realistic security scenarios, and even more generalized notions of "imperceptible" or "subtle" attacks aren t realistic when for the bulk of applied settings real adversaries are not restricted to small perturbations in the first place [1]. The example provided by the authors regarding small perturbations of a stop sign isn t a relevant example when the adversary has more effective options, e.g. knocking over stop signs [1, Figure 3].   For the sake of discussion, one could consider whether or not a degradation attack would make sense under more general threat models such as content preserving perturbations. An example discussed in [1] concerns adversaries uploading copyrighted content to public streaming services—this attacker defender game is being actively played in the wild where defenders produce statistical models which attempt to flag content as semantically matching existing copyrighted content in a private database, while attackers make large semantically preserving modifications in order to evade statistical detection. An example attack would be cropping 20% of the boundary pixels of a movie and replacing the cropped portion with arbitrary adversarially constructed backgrounds. Epsilon perturbations are possible, but are almost a measure 0 subset of the full attacker action space. Suppose in the far future neural network certification advanced to the point where we could certify that a classifier was robust to all possible content preserving perturbations of a specific movie. In this case the defender would be using the certification method on their private database of copyrighted content, they would not be running the certifier on any content uploaded by users. If a movie in the private database is certified, then we already know that an attacker cannot successfully upload an adversarial version of it, it would be unnecessary to certify whether or not user uploaded content could be further perturbed in a way to become adversarial. Perhaps degradation attacks could be possible as a training poisoning attack, but this seems a bit far fetched when more traditional training poisoning attacks would be preferred. Given this, at least in this example the AC does not see how a degradation attack would make sense as a threat model.   Given that the primary contribution of this work is a novel threat model for ML security, it is crucial that the authors rewrite their work to make more realistic assumptions of the capabilities of realistic adversaries. Starting with some of the examples discussed in [1] may be useful to the authors. Although the example of adversarial attacks on copyright detection classifiers doesn t seem to fit the degradation attack threat model, perhaps other scenarios would.  1. Gilmer et. al, Motivating the Rules of the Game for Adversarial Example Research, https://arxiv.org/pdf/1807.06732.pdf
Very good paper: it proposes a novel parameterization of orthogonal convolutions that uses the Cayley transform in the Fourier domain. The paper discusses several aspects of the proposed parameterization, including limitations and computational considerations, and showcases it in the important application of adversarial robustness, achieving good results. The reviews are all very positive, so I m happy to recommend acceptance.  Also, a big shout out to the reviewers and to the authors for being *outstanding* during the discussion period. The reviewers engaged with the paper to a great depth, and the authors improved the paper considerably as a response. Well done to all of you.
This paper proposes a method to explore neuron interactions within a neural network by deriving rules for the activations of units at different layers. The rules can presumably help interpret the inner workings of the neural network.  The reviewers have very different opinions on the paper and the views did not converge.  However, there is a common concern on the lack of quantitative evaluation on the faithfulness of the rules to the models. I therefore do not recommend accept.   R1[5]: On a related note, I felt the evaluation presented by authors while extensive is rather qualitative in nature. R2[3]: Given that I could provide you with a couple of references that you admit is relevant, and this was just off the top of my head, would you care to comment on a quantitative comparison with the referenced approaches? R3[8]: The examples look very impressive, but my main concern is with whether the examples could have been cherry picked, in the sense that most of the thousands of rules produced may not be useful.   
Reviews for this paper were mixed (6,6,6,5) with one review (Z6sN) being somewhat uninformative. During the rebuttal, some reviewers raised their scores to 6 but overall there was not strong excitement among the reviewers, AC, and SAC. From fresh readings (by SAC and researchers with relevant expertise), this paper’s technical approach looks reasonable but feels quite incremental (novelty is not high) and the experimental results are conducted on a small scale problem where up to 100% success rate is achievable by baseline methods. Therefore, the practical significance of this approach for real world problems with complex and noisy environments is quite unclear. Overall, the paper looks below the ICLR acceptance threshold. For improvement, we suggest providing evidence/demonstration that this method can successfully tackle more challenging real world problems.
This paper makes a key observation that the gradient based method gets more likely to suffer from poor local optima in multi agent reinforcement learning (MARL) with more agents particularly in the offline setting.  The paper proposes the use of zeroth order optimization method to avoid local optima.  Specifically, it samples multiple actions and regularize the policy to get closer to the optimal action among those.  The use of such zeroth order method to avoid poor local optima is not particularly new, although finding its effective in MARL and the empirical support are valuable.  The main discussion point was the insufficiency of experimental support, and the additional experiments during the discussion have addressed the original concerns of the reviewers to some extent.  Overall, given the limited novelty and inefficiency of support (either theoretical or empirical), the paper is slightly below the borderline.
This paper introduces an alternative to self attention, based on matrix factorization, and apply it to computer vision problems such as semantic segmentation. The method is simple and novel and obtains competitive results compared to existing approaches. The reviewers found the paper well written and easy to understand. For these reasons, I recommend to accept the paper.
This paper presents a novel technique for separating signals in a given mixture, a common problem encountered in audio and vision tasks. The algorithm assumes that training samples from only one of the sources and the mixture distributions are available, which is a realistic assumption in a lot of cases. It then iteratively learns a model that can separate the mixture by using the available samples in a clever fashion.  Strengths:   The novelty lies in how the authors formulate the problem, and the iterative approach used to learn the unknown distribution and thereby improve source separation.   The use of existing GLO masking techniques for initialization to improve performance is also novel and interesting.  Weaknesses   There are some concerns around guarantees of convergence. Empirically, the algorithm works well, but it is unclear when the algorithm will fail. Some analysis here would have greatly improved the quality of the paper.   The reviewers also raised concerns around clarity of presentation and consistency of notation. While the presentation improved after revision, there are parts which remain unclear (e.g., those raised by R3) that may hinder readability and reproducibility.    The mixing model assumed by the authors is additive, which may not always be the case, e.g. when noise is convolutive (room reverberation, for instance).   (Minor) Experiments can also be improved. The vision tasks are not very realistic. For the speech separation task, relatively clean speech is easy to obtain. Therefore, it would be worth considering speech as observed, and noise as unobserved. The authors cite separating animal sounds from background, but the task chosen does not quite match that setup.   Overall, the reviewers agree that the paper presents an interesting approach to separation. But given the issues with presentation and evaluations, the recommendation is to reject the paper. We strongly encourage the authors to address these concerns and resubmit in the future.
This paper proposes EXAID, a method to detect adversarial attacks by building on the advances in explainability (particularly SHAP), where activity map like explanations are used to justify and validate decisions. Though it may have some valuable ideas, the execution is not satisfying, with various issues raised in comments. No rebuttal was provided.
The paper studies transferability of adversarial examples between model architectures, and proposes a method to improve this transferability. Whilst it covers an interesting and relevant line of research, the paper does not provide strong evidence for its main underling hypothesis: namely, that adversarial perturbations can be split in a model specified and a data specific part. The paper s presentation also warrants improvements. The authors have not provided a rebuttal.
This paper introduces ICRL, where the RL agent is supposed to maximize the reward under unknown constraints, which should be inferred from the expert demonstration. Reviewers generally agreed that this is an interesting work, and potentially make RL to be applied to more general settings. However, they also would like to see more experimental results with baselines (e.g. agents based on IRL and also related constrained learning approaches) to make the motivation behind the approach more convincing. I hope these concerns are addressed in the future work.
This paper proposes to improve the robustness of computer vision models through a new augmentation strategy. There are two primary contributions of the work, first the use of a bottleneck autoencoder to generate discretized variants of the clean image, and second a slight variant of the task loss, where the task loss is evaluated on the augmented image vs the clean image as is done in prior work. Reviewers argued that the method did not meaningfully improve upon prior work, the method alone underperforms AugMix on existing benchmarks, and when combined with some additional augmentations from AugMix the gains were marginal. Additionally, when there were gains in robustness it was unclear as to the source. The work would be improved with additional experimental evidence that the claimed benefit of the information bottleneck is substantial for improving robustness (for example, when DJMix+RA outperforms AugMix, is this due to the use the of autoencoder or is it primarily due to the new task loss?). I recommend the authors incorporate additional reviewer feedback and resubmit.
This paper describes a new batching strategy for more efficient training of deep neural nets. The idea stems from the observation that some operations can only be batched more efficiently in the backward, suggesting that batching should be different between forward and backward. The results show that the proposed method improves upon existing batch strategies across three tasks. The reviewers find the work novel, but note that it does not properly address the trade offs made by the technique   such as memory consumption. They also argue that the writing should be improved before acceptance at ICLR.
The paper presents an adversarial learning framework for active visual tracking, a tracking setup where the tracker has camera control in order to follow a target object. The paper builds upon Luo et al. 2018 and proposes jointly learning  tracker and target policies (as opposed to tracker policy alone). This automatically creates a curriculum of target trajectory difficulty, as opposed to the engineer designing the target trajectories. The paper further proposes a method for preventing the target to fast outperform the tracker and thus cause his policy to plateau. Experiments presented justify the problem formulation and design choices, and outperform Luo et al. . The task considered is  very important, active surveillance with drones is just one sue case.  A downside of the paper is that certain sentences have English mistakes, such as this one:  "The authors learn a policy that maps raw pixel observation to control signal straightly with a Conv LSTM network. Not only can it save the effort in tuning an extra camera controller, but also does it outperform the..." However, overall the manuscript is well written, well structured, and easy to follow. The authors are encouraged to correct any remaining English mistakes in the manuscript. 
This paper considers the task of web navigation, i.e. given a goal expressed in natural language, the task is to navigate webs by filling up fields and clicking links. The proposed model uses reinforcement learning, introducing a novel extension where the graph embedding of the pages is incorporated into the Q function. The results are sound, and the paper is overall well written.  The reviewers and AC note the following potential weaknesses. The primary concern that was raised was the novelty. Since the task could potentially be framed as semantic parsing, reviewer 4 mentioned there may be readily available approaches for baselines that the authors did not consider. The comparison to semantic parsing required a more detailed discussion, pointing not only the differences but also the similarities, that would encourage the two communities to explore novel approaches to their tasks. Further, reviewer 2 was concerned about the limited novelty, given the extensive work that combines GNN and RL, such as NerveNet.  The authors provided comments and a revision to address these issues. They described why it is not trivial to formulate their setup as a semantic parsing problem, partly due to the fact that the environment is partially observable. Similarly, the authors described the differences between the proposed approach and methods like NerveNet, such as the use of a dynamic graph and off policy RL, making the latter not a viable baseline for the task. These changes addressed most of the concerns raised by the reviewers.  The reviewers agreed that this paper should be accepted.
The paper proposes a framework for generating evaluation tests for feature based explainers. The framework provides guarantees on the behaviors of each trained model in that non selected tokens are irrelevant for each prediction,  and for each instance in the pruned dataset, one subset of clearly relevant tokens is selected.   After reading the paper, I think there are a few issues with the current version of the paper:   (1) the writing can be significantly improved: the motivation is unclear, which makes it difficult for readers to fully appreciate the work. It seems that each part of the paper is written by different persons, so the transition between different parts seems abrupt and the consistency of the texts is poor. For example, the framework is targeted at NLP applications, but in the introduction the texts are more focused on general purpose explainers. The transition from the RCNN approach to the proposed framework is not well thought out, which makes the readers confused about what exactly is the proposed framework and what is the novelty.  (2) the claimed properties of the proposed framework are rather straightforward derivations. The technical novelty is not as high as claimed in the paper.  (3) The experiment results are not fully convincing.   All the reviewers have read the authors  feedback and responded. It is agreed that the current version of the paper is not ready for publication.  
This paper presents an analysis of different tricks for training the super network in NAS. While all reviewers see value in some of the many experiments, all reviewers also have substantial criticisms of the paper, and all reviewers gave weak rejection scores.  Looking at the paper myself, I agree with this assessment. Several of the experiments are valuable, but there are also several substantial issues.  One question that confused two reviewers and myself is about using sparse Kendall s tau as a metric that the authors in the rebuttal again state can be computed during super net training to evaluate the quality, just like super net accuracy. I don t see how that is possible. Kendall s tau measures the correlation between the ranks of the performances of the stand alone architectures and the ordering implied by the super net. Computing this requires access to the performance ranks of the stand alone architectures. For tabular benchmarks this is of course available, but not in practical NAS applications.  I would also like to echo the concern of AnonReviewer2 that too little information is given to fully understand what is shown in Figure 10.  Some reviewers also questioned inhowfar the results generalize to the setting of the Once for all network or BigNAS. This was not a deciding factor for me, since insights based on NAS Bench101, 201 and a DARTS like search spaces are already very useful.  I agree with the reviewers that the authors  use of "proxy" is highly misleading. It is standard to refer to the low fidelity model used for training as the proxy model. In contrast, the authors use it for the final evaluation model.  Concerning the authors  five final take aways: 1) I don t see how sparse Kendall s tau is actionable. 2) The batch normalization part is interesting, and I agree with the authors that it is useful to spell this out and analyze it, rather than just having one sentence in the paper as NB 201 and TuNAS, but the attribution that this has been done before is broken. "In contrast to X", rather than "Like X" 3) This is interesting, although I agree with AnonReviewer3 that I m lacking intuition why a smaller learning rate should be useful for a less smooth space 4) The experiment on low fidelity estimates is very misleading. The proxy settings used during training are already low fidelity evaluations   for the final evaluation, you would increase the number of channels, number of layers and number of epochs. Stating that the use of low fidelities is not useful is highly misleading. The authors  experiments only shows that the proxy model is already well chosen, and that if you reduce #layers or #channels and proportionally increase #epochs, performance gets worse. I encourage the authors to try searching without this proxy model, and I m sure they will find that (which correlations might increase) the search process will be far too slow. 5) The insight on dynamic channeling appears very useful to me.  In summary, I recommend rejection and encourage the authors to address the points raised by the reviewers and in this meta review.
The paper considers the problem of path integration in cognitive maps, where combining proprioception with visual inputs is required to estimate the displacement.  The paper proposes a small mechanism (a resetting path integrator) that extends a conventional LSTM for this purpose.  The resulting networks demonstrate better performance and interpretability than a conventional LSTM on tested problems.  The reviewers raised many issues with the paper.  One concern was whether the problem was to model biological, artificial, or robot problems (reviewer hpxs, PuPV), which the authors successfully addressed by stating that it is a minimal model. Many other minor concerns were also addressed. However, significant concerns remained.  One is the emphasis on the cognitive map (reviewer CUYu, hpxs) for which path integration is a small part.  Another major concern is the significance of the results, with reference to the baselines and $R^2$ (AjJt, CUYu).  A third is on the generalizability of the method beyond single small examples (AjJT, hpxs, CUYu).  All reviewers indicate reject due to concerns that the paper is not ready for publication.  The paper is therefore rejected.
The paper proposes a reinforcement learning algorithm that combines trust region policy optimization and entropy maximization. The starting point is the Lagrangian of a constrained optimization problem that upper bounds the change in the policy and lower bounds the entropy of the policy. The paper proves that the algorithm converges, and evaluates it experimentally in MuJoCo domains.  The main issues raised by the reviewers were related to the proofs (see especially R1) and experimental evaluation (R4). The authors did a great job improving the paper during the discussion phase, but some of the issues remain unresolved, and thus reviewers find the paper not to be ready for publication. Thus, I m recommending rejection.
This paper presents a reinforcement learning architecture that uses an auxiliary k step step loss in the context of continuous control from image based states.  While the topic is relevant and potentially impactful, several reviewers have major concerns about the manuscript. Among these, I highlight:   Reviewers J6YX, 38iT and Qru8 have concerns about the novelty and contribution of the approach compared to existing literature.   Reviewers J6YX, TKuY, 38iT and Qru8 have concerns about the experimental evaluation and the quality of comparisons to baselines.  Overall, it seems that the paper would benefit from further polishing.
The submission proposes optimization with hard threshold activations.  This setting can lead to compressed networks, and is therefore an interesting setting if learning can be achieved feasibly.  This leads to a combinatorial optimization problem due to the non differentiability of the non linearity.  The submission proceeds to analyze the resulting problem and propose an algorithm for its optimization.  Results show slight improvement over a recent variant of straight through estimation (Hinton 2012, Bengio et al. 2013), called saturated straight through estimation (Hubara et al., 2016).  Although the improvements are somewhat modest, the submission is interesting for its framing of an important problem and improvement over a popular setting.
I would like to highlight to the PCs that reviewers highlighted clear evidence of plagiarism from prior work, which I was able to easily verify (a full paragraph of text was copied, word for word, from a paper describing one of the baselines the current work compares against). Further, all reviewers unanimously agreed that the paper was poorly written, and contains no useful advances for the ICLR audience. I recommend a rejection, and further, examination by the PCs of the conduct of the authors.
The authors propose a rank coding scheme for recurrent neural networks (RNNs)   inspired by spiking neural networks   in order to improve inference times at the classification of sequential data. The basic idea is to train the RNN to classify the sequence early   even before the full sequence has been observed. They also introduce a regularisation term that allows for a speed accuracy trade off.  The method is tested on two toy tasks as well as on temporal MNIST and Google Speech Commands.  The results are very good, typically improving inference time with very little loss in accuracy.  Furthermore, the idea seems novel and the paper is well written.  An initial criticism was that experiments with spiking neural networks (SNNs) were missing. The authors added a proof of concept for SNNs, which satisfied the reviewer.   The authors also added some control experiments in response to the initial reviews, which improved the manuscript.  In summary, the manuscript presents a valuable novel idea with good experimental verification and interesting aspects both for ANNs and SNNs. The reviewers consistently vote for acceptance.
AR1 and AR3 have found this paper interesting in terms of replacing the spectral operations in GCN by wavelet operations. However, AR4 was more critical about the poor complexity of the proposed method compared to approximations in Hammond et al. AR4 was also right to find the proposed work similar to Chebyshev approximations in ChebNet and to highlight that the proposed approach is only marginally better than GCN. On balance, all reviewers find some merit in this work thus AC advocates an accept. The authors are asked to keep the contents of the final draft as agreed with AR4 (and other reviewers) during rebuttal without making any further theoretical changes/brushing over various new claims/ideas unsolicited by the reviewers (otherwise such changes would require passing the draft again through reviewers).
The authors introduce a novel probabilistic hierarchical clustering method for graphs. In particular they design an end to end gradient based learning to optimize the Dasgupta cost and Tree Sampling Divergence cost at the same time.  Overall the paper presents solid results both from a theoretical and experimental perspective so I think it is a good fit for the conference and I suggest accepting it.
All the reviewers questioned the significance of the result, in the sense that the qualitatively it is not clear how much of an improvement it is to replace "min(S_T,C_T) with Lipschitz assumption" by "min(S_T,C_T,G_T)". The authors  response on this point did not convince the reviewers. If the authors were to resubmit this work to a future conference, we encourage them to significantly expand on this point.
The authors study the degradation problem observed in KD for large teacher networks and propose to address it by quantifying and adapting to a *sharpness gap* between the student and the teacher. The reviewers generally appreciated the proposed approach in handling larger teachers and found it effective within the scope of the numerical results provided in the paper. That said, the reviewers raised several critical issues concerning the writing and the presentation of several crucial parts of the paper, in particular those related to the sharpness measure and the proposed training method ATKD. Thus, given this, and the exchanges between the reviewers and the authors, in its present form, the paper cannot be recommended for acceptance. The authors are encouraged to incorporate the valuable feedback provided by the knowledgeable reviewers.
The paper proposes a combination of a delay embedding as well as an autoencoder to perform representation learning. The proposed algorithm shows competitive performance with deep image prior, which is a convnet structure. The paper claims that the new approach is interpretable and provides explainable insight into image priors.  The discussion period was used constructively, with the authors addressing reviewer comments, and the reviewers acknowledging this an updating their scores.  Overall, the proposed architecture is good, but the structure and presentation of the paper is still not up to the standards of ICLR. The current presentation seems to over claim interpretability, without sufficient theoretical or empirical evidence. 
Two reviewers recommend acceptance while one is negative. The authors propose t shaped kernels for view synthesis, focusing on stereo images. AC finds the problem and method interesting and the results to be sufficiently convincing to warrant acceptance.
The paper presents a GAN for learning a target distribution that is defined as the difference between two other distributions.  The reviewers and AC note the critical limitation of novelty and appealing results of this paper to meet the high standard of ICLR.   AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
This paper tackles the problem of transferring an RL policy learned in simulation to the real world (sim2real). More specifically, the authors address the situation where the agent can access privileged information available during simulation, for example access to exact states instead of compressed representations. They perform experiments in various simulated domains where different aspects of the environment are modified to evaluate generalization.  Major concerns remain following the rebuttal. First, it is not clear how realistic it is to assume access to such privileged information in practice. Second, the experiments are not convincing since the algorithms do not appear to have reached convergence in the presented results. Finally, a sim2real work would highly benefit from real world experiments.  In light of the above issues, I recommend to reject this paper.
This submission analyses the numerical invertibility of analytically invertible neural networks and shows that analytical invertibility does not guarantee numerical invertibility of some invertible networks under certain conditions (e.g. adversarial perturbation).  Strengths:  The work is interesting and the theoretical analysis is insightful.  Weaknesses:  The main concern shared by all reviewers was the weakness of the experimental section including (i) insufficient motivation of the decorrelation task; (ii) missing comparisons and experimental settings.  The paper clarity could be improved.  Both weaknesses were not sufficiently addressed in the rebuttal. All reviewer recommendations were borderline to reject. 
The paper proposes Reasoning Modulated Representations (RMR). That is, it incorporates how to incorporate (structure) prior knowledge (such as a law in physics) into a pre trained reasoning modules, and investigates how doing so shapes the discovered representations in a number of self supervised learning settings from pixels. The reviews and (short) discussion have presented salient arguments about the suitability of the paper for publication at this stage. One review argues that the "methodological contribution is minimal," another one is asking for "systematic evaluation" of the main claims made. Moreover, while we all agree that the direction is interesting, the RMR approach presented is not shown to "scale well" (yet), as pointed out by one review. This, however, is important since the general idea that prior knowledge shapes the representation learned is common wisdom in the literature. Indeed, one may now argue that the paper is much more about "how best to combine pixel based deep learning and neural algorithmic reasoning algorithms" as one reviewer puts it. From this perspective the ATARI experiments are more interesting but here the benefit compared to C SWM seems to be marginal and one should compare to other deep baseline conditions on the RAM; the significance is not looking at the difference in score and degree in freedoms but just the number of wins. Additionally, there should be other baselines that directly make use of more structured models (structure   prior knowledge, e.g., HMMs or some other way to have bit of a memory), other datasets (where no access to RAM exists) as well as a discussion of other approaches that combines (combinatorial) reasoning with pixel based deep learning. That is, while pushing for a more high level contributions is fine, this also requires some more illustrations and discussion of the broader context. Therefore, my overall recommendation is reject at this stage of the paper.
This paper pursues an ambitious goal to provide a theoretical analysis HRL in terms of regret bounds. However, the exposition of the ideas has severe clarity issues and the assumptions about HMDPs used are overly simplistic to have an impact in RL research. Finally, there is agreement between the reviewers and AC that the novelty of the proposed ideas is a weak factor and that the paper needs substantial revision.
This manuscript proposes a generative model for images, then proposes a training procedure for fitting a convolutional neural network based on this model. One novelty if this result is that the generative procedure seems to be more complex than generative assumptions required for previous work. It is clear that the problem addressed   training methods that may improve on SGD, with convergence guarantees   is of significant interest to the community.  The reviewers and AC note several issue (i) the initial version of the manuscript includes several assumptions that are not clearly stated. This seems to have been fixed in the updated manuscript (ii) reviewers suspect that the accumulation of stated assumptions may result in an easily separable generative model   limiting the generality of the results (iii) experiemental results are underwhelming, and only comparable to much older published results.
The paper presents a way of training 1bit wide resnet to reduce the model footprint while maintaining good performance. The revisions added more comparisons and discussions, which make it much better. Overall, the committee feels this work will bring value to the conference.
This paper presents impressive results on scaling GANs to ILSVRC2012 dataset containing a large number of classes. To achieve this, the authors propose "spectral normalization" to normalize weights and stabilize training which turns out to help in overcoming mode collapse issues.  The presented methodology is principled and well written. The authors did a good job in addressing reviewer s comments and added more comparative results on related approaches to demonstrate the superiority of the proposed methodology. The reviewers agree that this is a great step towards improving the training of GANs.  I recommend acceptance.
Viewing the problem of determining the validity of high dimensional discrete sequences as a sequential decision problem, the authors propose learning a Q function that indicates whether the current sequence prefix can lead to a valid sequence. The paper is fairly well written and contains several interesting ideas. The experimental results appear promising but would be considerably more informative if more baselines were included. In particular, it would be good to compare the proposed approach (both conceptually and empirically) to learning a generative model of sequences. Also, given that your method is based on learning a Q function, you need to explain its exact relationship to classic Q learning, which would also make for a good baseline.
Given the increasing scale of large models (e.g. CLIP), there s an argument that we need better automated techniques for properly utilizing (prompting) these models. Given the success of prompt learning within pure NLP models, the authors apply the same approach to the V+L domain and show that it also is applicable here.  Generally, reviewers felt that the results were clear and thorough, yet technically limited.  The approach is not novel and the result not surprising.  There is a documentary benefit to having this work out in the community for others to reference and extend.
This work explores how to leverage structure of this input in decision trees, the way this is done for example in convolutional networks. All reviewers agree that the experimental validation of the method as presented is extremely weak. Authors have not provided a response to answer the many concerns raised by reviewers. Therefore, we recommend rejection.
The submission modifies the SPEN framework for structured prediction by adding an inference network in place of the usual combinatorial optimization based inference.  The resulting architecture has some similarity to a GAN, and significantly increases the speed of inference.  The submission provides links between two seemingly different frameworks: SPENs and GANs.  By replacing inference with a network output, the connection is made, but importantly, this massively speeds up inference and may mark an important step forward in structured prediction with deep learning.  
The paper addresses normalisation and conditioning of GANs. The authors propose to replace class conditional batch norm with whitening and class conditional coloring. Evaluation demonstrates that the method performs very well, and the ablation studies confirm the design choices. After extensive discussion, all reviewers agreed that this is a solid contribution, and the paper should be accepted. 
The paper proposes a new approach for scalable training of deep topic models based on amortized inference for the local parameters and stochastic gradient MCMC for the global ones.  The key aspect of the method involves using Weibull  distributions (instead of Gammas) to model the variational posteriors over the local parameters, enabling the use of the reparameterization trick. The resulting methods perform slightly worse that the Gibbs sampling based approaches but are much faster at test time. Amortized inference has already been applied to topic models, but the use of Weibull posteriors proposed here appears novel. However, there seems to be no clear advantage to using stochastic gradient MCMC instead of vanilla SGD to infer the global parameters, so the value of this aspect of WHAI unclear.
This paper presents a method for target side data augmentation for sequence to sequence models.  The authors of the paper use a relatively straightforward method to generate pseudo tokens that are used for enhanced training.  The authors present results on dialog generation, MT and summarization where automatic metrics show improvements.  For really robust results, I would have preferred to see more human evaluations since BLEU and ROUGE are metrics that the NLP community is moving away from.  Overall, the majority of the reviewers are happy with the paper and there is significant back and forth between the reviewers and authors that have improved the paper;  I think the authors went to significant lengths to allay all concerns from the reviewers and the paper should be accepted.
While the paper contains significant information, most insights have already been revealed in previous work as noted by R1.  The empirical novelty is therefore limited and the authors do not provide theoretical analysis to complement this.
Most reviewers found the method proposed to be technically sound, well motivated and particularly interesting due to the interpretability of its results. Indeed, the extraction of interpretable motifs from NAS is a valuable contribution. One of the reviewers was particularly concerned by the lack of guarantees of the proposed method and a perceived failure mode of averaged gradients. We thank both the reviewer and the authors for the detailed discussion on these points. Ultimately, the benefits of the method proposed and the magnitude of the contributions in the paper outweigh these concerns.
The paper shows how techniques introduced in the context of unsupervised machine translation can be used to build a style transfer methods.  Pros:     The approach is simple and questions assumptions made by previous style transfer methods (specifically, they show that we do not need to specifically enforce disentanglement).       The evaluation is thorough and shows benefits of the proposed method     Multi attribute style transfer is introduced and benchmarks are created     Given the success of unsupervised NMT, it makes a lot of sense to see if it can be applied to the style transfer problem  Cons:    Technical novelty is limited     Some findings may be somewhat trivial (e.g., we already know that offline classifiers are stronger than the adversarials, e.g., see Elazar and Goldberg, EMNLP 2018).     
The paper proposes a novel method for conditional image generation which is based on nearest neighbor matching for transferring high frequency statistics. The evaluation is carried out on several image synthesis tasks, where the technique is shown to perform better than an adversarial baseline.
This paper proposes a relation based model that extends VAE to explicitly alleviate the domain bias problem between seen and unseen classes in the setting of generalized zero shot learning.  Reviewers and AC think that the studied problem is interesting, the reported experimental results are strong, and the writing is clear, but the proposed model and its scientific reasoning for convincing why the proposed method is valuable is somewhat limited. Thus the authors are encouraged to further improve in these directions. In particular:    The idea of using a variant of the widely used domain discriminator to make seen and unseen classes distinguishable is somewhat contradicted to the basic principle of zero shot learning. How to trade off the balance between seen and unseen classes has been an important problem in generalized ZSL. These problems need further elaboration.    The proposed model itself is not a real "VAE", making the value of an extensive derivation based on variational inference less prominent.     There is also the need to compare with the baselines mentioned by the reviewers.   Overall, this is a borderline paper. Since the above concerns were not addressed convincingly in the rebuttal, I am leaning towards rejection.
The paper proposes a modulation layer to address the problem of missing data.  The results do not show that the approach outperforms existing sota approaches. The results do not demonstrate that the proposed modulation layer is an improvement over attention layer. Many smaller errors (spelling, etc.) where found in the manuscript. Experimental details are insufficient to make the results reproducible. The authors have not provided a response to the reviewers.
Three reviewers recommended rejection and there was no rebuttal.
This work makes the observation that gradients in neural network training are approximately distributed according to a log normal distribution. This observation is then used to compress and sparsify the gradients, which can be useful in distributed optimization of neural nets. The reviewers indicate that this contribution is novel and useful and they do not find any major issues with the presented work. I recommend accepting the paper for a poster presentation.
The manuscrupt studies an unexplored problem: How to reverse engineer adversarial perturbations from an adversarial image? This leads to a new adversarial learning paradigm—Reverse Engineering of Deceptions (RED). The authors formalize the RED problem and identify a set of principles crucial to the RED approach design. By integrating these RED principles with image denoising, they propose a new Class Discriminative Denoising based RED framework, termed CDD RED.  The reviewers recognize that this topic is important and a promising research direction. The reviewers are also satisfied with the respones from the authors. In summary, this paper is recommended to be accepted as it is well formulated, easy to follow, and has some merits, despite that it needs to be evaluated further.
This paper proposed to use an autoencoder based approach for anomaly localization. The method shows promising on inpainting task compared with traditional auto encoder.  First two reviewers recommend this paper for acceptance. The last review has some concerns about the experimental design and whether VAE is a suitable baseline. The authors provide reasonable explanation in rebuttal while the reviewer did not give further comments.  Overall, the paper proposes a promising approach for anomaly localization; thus, I recommend it for acceptance. 
The paper proposes a method to map input probability distributions to output probability distributions with few parameters. They show the efficacy of their method on synthetic and real stock data. After revision they seemed to have added another dataset, however, it is not carefully analyzed like the stock data. More rigorous experimentation needs to be done to justify the method.
The paper addresses semi supervised learning with unbalanced class distribution, a.k.a long tail. The main idea is to alternate learning of the representation and the classifier.  Reviewers pointed out that several papers already addressed this learning setup, often under the name "imbalanced semi supervised learning". No rebuttal was submitted.   The paper should make direct comparison to recent papers listed by reviewers, both in terms of the technical approach and in terms of empirical experiments. It cannot be accepted tot ICLR.
The paper presents an SGD based learning of a Gaussian mixture model, designed to match a data streaming setting.  The reviews state that the paper contains some quite good points, such as * the simplicity and scalability of the method, and its robustness w.r.t. the initialization of the approach; * the SOM like approach used to avoid degenerated solutions;  Among the weaknesses are * an insufficient discussion wrt the state of the art, e.g. for online EM; * the description of the approach seems yet not mature (e.g., the constraint enforcement boils down to considering that the $\pi_k$ are obtained using softmax; the discussion about the diagonal covariance matrix vs the use of local principal directions is not crystal clear); * the fact that experiments need be strengthened.  I thus encourage the authors to rewrite and polish the paper, simplifying the description of the approach and better positioning it w.r.t. the state of the art (in particular, mentioning the data streaming motivation from the start). Also, more evidence, and a more thorough analysis thereof, must be provided to back up the approach and understand its limitations.
The submission proposes instance level and episode level pretext tasks as an unsupervised data augmentation mechanism for few shot learning. Furthermore, transformer are proposed to integrate features from different images and augmentations. The paper received one clear accept, one accept, one borderline accept and two borderline reject recommendations. The main concerns of the R5 and R2 were weak ablation study and the lack of a clear advantage of the method in terms of results compared to the prior state of the art. In the rebuttal, the authors provided more ablation studies. Similarly, the reviewers were concerned about the novelty of the paper being incremental compared to the prior works. Based on the majority vote, the meta reviewer recommends acceptance. 
This paper proposes to learn a latent space representation such that some linear equivariance and symmetry constraints are respected in the latent space, with the goal to improve sample efficiency. One core idea is that the latent space is also the same as the space of linear transformation used in the constraints, which is shown to simplify some of the mathematical derivations. Experiments on the Atari 100K benchmark demonstrate a statistical improvement over the SPR baseline when using the SE(2) group of linear transformations as latent space.  Following the discussion period, most reviewers were in favor of acceptance. However, one reviewer remained unconvinced, and after carefully reading the paper, I actually share the same concerns, i.e., that it is unclear under which conditions the proposed approach actually works, and what makes it work. I believe that, as a research community, we should value understanding over moving the needle on benchmarks, especially when proposing such a complex method as this one (see Fig. 5).  More specifically:  1. The method is only evaluated on Atari games, showing some improvements when using SE(2), and arguing that there are corresponding symmetries in such games. There is however no analysis demonstrating (or even hinting at the fact) that the proposed technique is actually learning to take advantage of such symmetries (NB: I had a quick look at the animation added by the authors in the supplementary material, but I do not see if/how they help on this point). Even if analyzing representations on Atari may be tricky, I believe that given the motivation of this new algorithm, it *must* be evaluated on some toy example (e.g., the pendulum mentioned throughout the paper) to validate that it is learning what we want it to learn (although I also agree with the authors that experimenting on a more complex benchmark like Atari is equally important).  2. The idea of embedding states into the same space as transformations is interesting, and brings some advantages when writing down equations, as demonstrated by the authors. However, there is no justification besides mathematical convenience, and it doesn t seem intuitive to me at all that why this should be a good idea, considering that it ties the state representation to the mathematical representation of group transformations. For instance, what does the spcial group element $e$ mean for a state? And this coupling makes it difficult to interpret the effect of using a different group of transformations: for instance when moving from GL(2) to SE(2), is the observed benefit because we are using only specific transformations, or simply because we are reducing the dimensionality of the state embedding? (note that in Fig. 4(c) the MLP variant has similar performance to GL(2), and based on my understanding they use the same embedding dimensionality   > I believe it would be important to check what would happen with an MLP variant using the same dimensionality as SE(2))  3. The effect of the $L_{GET}$ loss is not convincing, as pointed out by several reviewers. I think it would have been an opportunity for the authors to investigate why, especially since it seems to work in some games and not others. But just focusing on "here are the 17/26 games where it works better" doesn t really bring added value here. Do these games have some specific properties that make them better candidates to take advantage of $L_{GET}$? This could have been a very interesting insight if that was the case, but as it is now, I am not sure what we can learn from that.  4. There are several implementation "details", some moving the final algorithm farther from its theoretical justification, that are not ablated, making it difficult to understand their impact (ex: using target networks, the choice of the value of M, using projections onto the unit sphere of some arbitrary dimensionality, how the $s $ state is chosen in $L_{GET}$)  As a result, we have here an algorithm with some interesting theoretical background, but with a lot of moving components which   when properly tweaked   can lead to a statistically meaningful improvement on Atari 100K   without really understanding why. I believe this is not quite enough for publication at ICLR, and I would encourage the authors to delve deeper into the understanding of their algorithm, which I hope will bring useful insights to the research community working on representation learning.
The authors carefully study a class of unsupervised learning models called self expressive deep subspace clustering (SEDSC) models,  which involve clustering data arising from mixtures of complex nonlinear manifolds. The main contribution is to show that the SEDSC formulation itself suffers from fundamental degeneracies, and that the experimental gains reported in the literature may be due to ad hoc preprocessing.  The contributions are compelling, and all reviewers appreciated the paper. Despite the paper being of somewhat narrow focus, my belief is that negative results of this nature are useful and timely. I recommend an accept.
The paper shows that most variance of gradients used in FL and distributed learning in general is in very low rank subspaces, an observation also made in Konecny et al 2016 and some other related works in deep learning, though sometimes for a different purpose. The paper then proposes lightweight updates combining a fresh gradient with old updates. Experiments and a theoretical convergence guarantee complement the results, which are mostly convincing.   The experiments compare against ATOMO but strangely not against the more common PowerSGD, which would also work with partial client participation.  Overall, reviewers all agreed that the paper is interesting, well motivated and deserves acceptance. We hope the authors will incorporate the open points as mentioned by the reviewers.
This paper has been independently evaluated by four expert reviewers. After discussion with authors, three of them set their recommendations at marginal acceptance, one at straight accept. Perhaps the key criticism involved limited rigor of theoretical justification for the proposed method, but it appears to be applicable in practice as the empirical results suggest. All things considered, I am leaning towards recommending that this paper is accepted for ICLR 2022.
This paper proposes a new metric to measure symmetry based disentanglement and uses this metric to optimize diffusion VAEs on a set of small, synthetic datasets. In general, reviewers found the theoretical framework introduced to be interesting and relevant, but there were a number of concerns regarding the empirical evaluation in the paper and the clarity of many of the claims, particularly wrt the need for strong supervision (pairs of data points with a known transformation between them) for both evaluating the metric and for training by regularizing the proposed metric. I d encourage the authors to focus on the improvement points suggested by reviewers, most notably by improving the empirical evaluation by adding detailed ablations and comparisons (e.g., exploring the relative amount of supervision needed, comparisons to previous approaches) and clarity regarding the supervision required. As such, I recommend that it be rejected in its current form.  
This paper addresses an important problem of semi supervised learning of time series data.  Their approach is based on a convolution autoencoder for learning a time series latent space.  To guide learning an appropriate embedding, they explore three alternative internal clustering metrics (prototype loss, Silhouette loss, and DB index loss) coupled with the autoencoder reconstruction loss.   The approach is reasonable and interesting, however as pointed out by the reviewers, the current submitted version needs major revision for it to be accepted.  Key weaknesses are: 1.	It lacks an extensive literature review.  The reviewers have made several suggestions for improving this. 2.	The experiments are weak.  First, state of the art baselines are missing.  Second, additional alternative evaluation metrics will strengthen the evaluation of unsupervised methods (e.g., adding NMI, accuracy, cross validation of internal metrics).  Providing evaluation of when which loss would work better on what types of data would provide insight to the various losses proposed. 
BMIs need per patient and per session calibration, and this paper seeks to amend that.  Using VAEs and RNNs, it relates sEEG to sEMG, in principle a ten year old approach, but do so using a novel adversarial approach that seems to work.  The reviewers agree the approach is nice, the statements in the paper are too strong, but publication is recommended.  Clinical evaluation is an important next step.
The paper presents PIVEN, a deep neural network that produces a prediction interval in addition to a specific point prediction.  PIVEN is distribution free and does not assume symmetric intervals.    All the reviewers agree that the paper investigates an important problem and the paper is well written. The reviewers also identified a couple of weak points, namely:   Novelty: The key idea seems to a combination of prediction loss (common) and prediction interval loss which has been proposed by Pearce et al. 2018.   Claims that PIVEN outperforms existing methods (QD and DE) empirically as some of the improvements seem marginal.   Given these concerns, I think the current version falls a bit short of the acceptance threshold unfortunately. I encourage the authors to revise the draft and resubmit to a different venue. 
This paper proposes an adaptive sparse Huber additive model for for forecasting non stationary time series. The prior work has considered similar models for Gaussian innovations which is overly restrictive for a variety of applications such as finance. The results are supported both by theory and experiments. The results are novel and are of interest to ICLR and machine learning communities in general.
The paper studies the unsupervised RL problem, where the agent is allowed to interact with the environment for a certain amount of time without any extrinsic reward. The main idea is that the initial unsupervised training phase can be used to learn a set of "skills" that could help both in exploration and zero shot transfer for any downstream task.  There is general consensus among the reviewers that the paper is studying an important problem and that the empirical validation is solid. Nonetheless, the technical contribution and the positioning wrt to the relevant literature are relatively weak for proposing acceptance for the paper. Before entering into details, I would like to acknowledge the fact that the rebuttal and the revised version did improve the original submission and clarified some aspects (eg, the structure of the algorithm), yet the contribution does not seems strong enough.  The idea of state space coverage is indeed not novel, either for exploration or for transfer (as properly reviewed in the paper). The authors identified some weaknesses of existing methods (eg, estimating state distributions), but it remains unclear whether the algorithm they propose has any significant technical contribution. In fact, as confirmed by the authors, CPT is rather applying any algorithm that *could* perform a good state space coverage and learn policies at the same time and then use the learned policy during the downstream task combined with a relatively basic "option level" eps greedy strategy. In this sense, it seems like CPT overcomes the limitations of previous algorithms, just by applying another existing algorithm (NGU) that is more scalable. While the evidence that this is "enough" to obtain good results is indeed interesting, it doesn t seems like it is pushing the algorithmic state of the art forward.  A more substantial contribution would be to dive deeper into the state coverage problem and provide an algorithm that is more specifically designed for the transfer setting considered in the paper. In fact, there is no clear evidence that NGU is the *right* approach to perform good coverage and return "useful" skills. Since this is the core concern of the paper, the technical contribution should be more significant on this part. The "meta" algorithm in itself seems rather standard otherwise.  
Meta Review for Variational Neural Cellular Automata  This paper proposes a generative model, a VAE whose decoder is implemented via neural cellular automata (NCA). The authors show that this model performs well for reconstruction, but they also show that the architecture has some robustness properties against damage during generation.   Experiments were conducted on 3 datasets: MNIST, Noto Emoji, and CelebA, and while experimental results were great on MNIST, the method was less performant so on the other two datasets, although there is clear evidence that the model can learn to generate meaningful images. For the robustness experiments, the authors show that VNCA is robust to perturbations (occlusions) and show that the model has a reasonable degree of robustness even without ever seeing any perturbations at training time.  All authors agree that this model is an improvement over neural cellular automata, and that the approach is interesting and the results are sound (and even useful). Initially, there were concerns that NCA s were simply convolutional neural networks (the connection is already known, and not the point of the paper), and issues with comparison with baselines for damage reconstruction tasks, but these were addressed by the authors (which the reviewers have acknowledged, and have improved their scores). The authors have also responded to the concerns of reviewer cp9d, and due to the lack of response from cp9d, I assessed the authors  response myself and find that they do address the concerns (in particular, they removed claims of super resolution, and improved the clarity of the work). With that in mind, the score of 5 is viewed as a score of 6 from my perspective (giving this work effectively an average score of 6).  After my assessment of the paper and reviews, I agree with reviewer kwgv, as they have summarized the work in their original review:   The authors propose a variational neural cellular automaton, which learns to generate images by iterating the transition rule.   The paper is interesting, with good results, and a good fit for ICLR.   The paper solves an interesting problem on the topic of neural cellular automata.   There are some doubts/limitations that I have asked the authors to address (mainly concerning the architecture of the model).   There are some missing references and details that would help the readers to get a better sense of the subject.  Crucially, kwgv have acknowledged that the *authors have improved the paper significantly after the reviews, and they have addressed all questions and comments that [they] raised* (especially with regards to the last 2 points), and kwgv has subsequently championed the work with a score of 8. With the increased scores from kwgv and AnwX in mind, and also with what I view as an increased score of 6 from cp9d (in the lack of response from the reviewer, the authors have addressed the concerns in my judgement), my conclusion is that this is a nice work that bridges NCAs with generative models, and I think the work will be a useful addition to the growing literature in this space. I will recommend it for acceptance at ICLR 2022 as a poster.
This paper propose to learn the embedding of audio segment in the framework of stochastic neighbor embedding (SNE), where the embeddings of the same word shall be close to each other. The method was initially demonstrated for name recognition. The use of SNE for acoustic embedding is novel and this is recognized by all reviewers. There has been quite some discussions between the authors and reviewers/AC, and the papers has got improved since. To summarize: 1. The discussion of the properties of SNE (the reduction from SNE to weighted least squares terms) was not accurate and confused multiple reviewers. The authors have made some clarifications and added citations. As the author claim this to be a contribution, I feel this part of the main text can be further strengthened and made more accurate. 2. For the experiment in main paper, the comparison between proposed method and prior work was not fair since the proposed method use outputs from an ASR system to obtain phone posteriors. The authors then added more results for the word discrimination task in the appendix. But reviewers are still concerned that the authors are not comparing with the strongest variant of He et al, 2017, and that comparisons are shown for embeddings with low dimensions. The reviewers believe this set of experiments shall be more illuminating, and be moved to the main text. 3. The biggest concern at the intuition level is whether it is the best choice to make the affinity binary, which does not take into account the more fine grained similarity between different words. Quoting the comment from Reviewer 6:  "The argument of trying to be as simple as possible is reasonable, but we would have liked to see it motivated by some experiments. Something along the lines of a method which introduced rudimentary edit distance based affinities and then presented their version with hard affinities, and then show some results comparing them. These edit distance based versions could be as simple or complicated as they felt necessary, but it would be nice for some comparison to be made in order to then dismiss them."  Overall, we think the paper is borderline in the current stage, and the paper can be further improved if the above concerns are properly addressed. 
The paper describes the use of differentiable physics based rendering schemes to generate adversarial perturbations that are constrained by physics of image formation.  The paper puts forth a fairly novel approach to tackle an interesting question. However, some of the claims made regarding the "believability" of the adversarial examples produced by existing techniques are not fully supported. Also, the adversarial examples produced by the proposed techniques are not fully "physical" at least compared to how "physical" adversarial examples presented in some of the prior work were.  Overall though this paper constitutes a valuable contribution. 
This paper proposes a new method for understanding the role and importance of individual units in convolutional neural networks. The reviewers were in agreement that the technique is novel and provides potentially valuable insights into neural network behavior. The reviewers were less certain about the utility or significance of this idea; however, the authors partially addressed this concern by adding studies of using this technique as a pruning heuristic, and future researchers will be the best judge of the paper s eventual significance. With that in mind, I recommend acceptance so that this intriguing idea can become part of the research literature and future researchers will have this opportunity.
The reviewers have some following concerns:   1) There is lack of experimental result. The experiment on MNIST with small CNN architecture is definitely not sufficient to verify the efficiency of the proposed method. Moreover, the advantage of the proposed method is not very clear due to the choices of the parameters. The choice of the learning rates is quite sensitive.   2) It is not clear why the authors could argue that $ \mathbb{E}(V_T)   \mathcal{O}(T)$ without any theoretical and empirical support. Even if this is correct, this term could dominate the first term unless $ \mathbb{E}(V_T) \leq \mathcal{O}(\sqrt{T})$, which is too strong. If assuming $\mathbb{E}(V_T)   \mathcal{O}(T)$, the convergence results are upper bounded by some constant (note that $\epsilon$ is a constant in this scenario, not arbitrarily small). Hence, the authors failed to show the convergence to a stationary point.   There are some suggestions to improve the paper as follows:   1) Show $\mathbb{E}(V_T)   \mathcal{O}(T)$ and revise the theory properly to make it rigorously by showing upper bounded by some function $R(T) \to 0, T \to \infty$ rather than showing the convergence to some fixed neighborhood. (Note that $\frac{C_4}{\sqrt{N}}$ is a fixed constant).   2) Do more experiments on various datasets and network architectures to verify the efficiency of the proposed method and show the clear advantages compared to others.   3) Provide convergence rate comparisons with other decentralized algorithms (e.g., as a table). It would be nice if the authors also provide the assumptions and the dependent constants so that the readers could really see the differences.   4) Explicitly derive the convergence measure based on the standard one, that is, $\frac{1}{T} \sum_{t 1}^{T} \mathbb{E} [ \| \nabla  f (  X_t )  \|^2 ] $ and add the dependency of $G_{\infty}^2$ to the bound.   5) Revise the paper and implement all necessary comments from the reviewers consistently with the content.  
This paper introduces an ensemble method to few shot learning.  Although the introduced method yields competitive results, it is fair to say it is more complicated than much simpler algorithms and does not necessarily perform better. Given that ensembling for few shot learning has been around for a while, it is not clear that this paper will have a significant audience at ICLR.  Sorry about the bad news,   AC.  
This paper presents a compelling mechanism for reducing the neural architecture search process based on accumulated experience  that the reviewers found compelling with significant improvements in performance.  This is an intriguing idea. However, there were concerns about clarity that need to be addressed, and more concerning, the paper lacked technical depth or details in several aspects described in the reviews.  The authors subsequent response and revisions have somewhat addressed these issues.  The reviewer discussion had mixed opinions, with some for weak acceptance and others for weak rejection.  There were compelling points that the contribution is significant, but overall this paper would benefit from thoroughly addressing the shortcomings mentioned in the reviews before it is ready for publication. 
This paper propses a slice method for approximaing the Kernel Stein Discrepancy, which has been popularly used for learning and inference with unnormalized density models.  The proposed method uses a finite set from the orthogonal bases for the slice to approximate the Stein Discrepancy.  The experimental results show that they outperform exsiting methods in high dimensional cases in the applications of goodness of fit tests and learning of energy based models.    The proposed slice idea is novel and significant.  Especially, unlike sliced Wasserstein, the slices are taken from the limited number of vectors, which should be an advantageous feature of the method.  Experiments demonstrate clear advantages in high dimensional cases, as expected.  The paper is worth accepting in ICLR. 
This paper analyzes local SGD under the random reshuffling data selection setting. As is the case for standard random reshuffling, better rates are shown for local SGD when random reshuffling is used. This would already be a nice contribution to a line of work on random shuffling methods—but the paper goes beyond that by showing a matching lower bound and designing a (theoretically) better variant algorithm. The reviewers were all in agreement that this paper should be accepted (as a result not much further discussion happened after the original reviews), and I agree with this consensus. The modification seems to improve the paper, although I did not look through it in detail.
A well written paper proposing some reasonable approaches to counter adversarial images. Proposed approaches include non differentiable and randomized methods. Anonymous commentators pushed upon and cleared up some important issues regarding white, black and gray "box" settings. The approach appears to be a plausible defence strategy. One reviewers is a hold out on acceptance, but is open to the idea. The authors responded to the points of this reviewer sufficiently. The AC recommends accept.
This paper proposes an early exit method that uses class means of samples that is gradient free and is aimed for low compute cases such as mobile and edge data. The idea is novel in this setting (though class means have been used for other settings such as few shot classification) and empirical results show that it works well. There are two main concerns from reviewer concerns that were not addressed by the author rebuttal. First, applicability of the model in real world due to its memory requirements and two, experiments that show performance on more realistic datasets such as Imagenet. The reason the latter is required is the promise of mobile application for the proposed method. I suggest the authors explain the first concern more and add the requested experiments in the upcoming version of the paper.
This paper proposes a tree structured tensor factorisation method for parameter reduction. The reviewers felt the paper was somewhat interesting, but agreed that more detail was needed in the method description, and that the experiments were on the whole uninformative. This seems like a promising research direction which needs more empirical work, but is not ready for publication as is.
The paper suggests a procedure to efficiently adapting a learned neural compression model to a new test distribution. If this test distribution has low entropy (e.g., a video as a sequence of interrelated frames), large compression gains can be expected. To achieve these gains, the method adapts the decoder model to the new instance, transmitting not only the data but also a compressed model update. Experiments are carried out on compressing I frames from videos, while comparisons comprise baseline approaches that finetune the latent representations of videos as opposed to the decoder.   The paper’s main contribution is very timely and relevant. While it was well known in the classical compression literature that model updates could be sent along with the data (e.g., as already done in “optimized JPEG”), this is the first time the idea was implemented in neural compression. The experiments are arguably the paper’s weaker part and were originally a concern, but they have been significantly improved during the review period such that all reviewers voted for acceptance. We encourage the authors to further strengthen their experimental results by adding more challenging baselines on well established tasks (e.g., image compression). 
All reviewers are for accepting the paper: in particular, R1 and R3 found the rebuttal sufficiently convincing to increase their scores from their initial assessment leaning towards rejection.   Strengths: + Clarity  + Simplicity of the proposed approach  + Convincing experiments outperforming reasonable baselines across all problem instances  Weaknesses: + Scale (as noted by R2 and R3) to larger problem sizes, beyond the setting of less than a dozen.  I agree with some hesitation that the paper is narrow in scope (both in interest from the community and scale and ultimately whether it would interest the overall quantum computing audience). However, I think the paper makes significant advances toward the area of adiabatic quantum computation.
To overcome the challenge of lacking task specific unlabeled data in semi supervised learning (SSL) or knowledge distillation (KD) tasks, this paper presents a new framework called "generate, annotate, and learn (GAL)" that uses unconditional language models to synthesize in domain unlabeled data to advance SSL and KD. Extensive experiments on both NLP and tabular tasks demonstrate positive results of the proposed method.    Reviewers generally agree on several key strengths of the paper, e.g., the paper is well written, literature review is comprehensive, experimental results are generally positive (the improvements over the standard baselines on GLUE benchmark looks solid despite not very significant). On the negative side, some reviewers did raise some major concerns about the novelty of the proposed framework and the lack of strong baselines for comparison. For example, the proposed GAL framework doesn’t seem particularly novel as neither of the proposed components is new, and the key value of the work seems on the contribution of evaluating the large LM s ability to generate good in domain unlabeled data (as agreed by authors). Therefore, it is very important to compare with other existing data augmentation baselines in the empirical studies. While authors did try to add one round trip translation (RT) data augmentation baseline for comparison during the rebuttal, more stronger SOTA data augmentation baselines should be compared.   Overall, this is a good paper which is worthy of publication in near future but it still needs some more work on more extensive comparison of more baselines and improvements on the writing of novelty and contribution claims.
This paper presents an analysis showing the equivalence between gradient and data poisoning attacks in personalized federated learning settings. The paper contains an analysis of an attack that requires only a single corrupt learning agent, providing results in the setting of PAC learnable models. The reviewers had several criticisms of the paper, some of which were addressed in the rebuttal.  The first is that the presentation of the paper was at times confusing, and the theoretical results were hard to interpret.  This has been addressed by several changes to the paper writing, including major changes to the layout.  The reviewers feel that other criticisms were not entirely addressed.  This includes the criticism that the experiments are in a fairly simplistic setting (GD on MNIST and Fashion MNIST), and that the theoretical results require strong assumptions and focus mostly on classical models that are learnable in convex frameworks.  While the reviewers agree there are interesting questions posed in this paper, the consensus seems to be that the experimental and theoretical results in this paper should be further revised, and that a future version of this paper will be a great candidate for publication.
This paper first makes the observation that incidental supervisory data can be used to define a new prior from which to calculate a PAC Bayes generalization guarantee.  This observation can be applied to any setting where there is unsupervised or semi supervised pre training followed by fine tuning on labeled data.  The PAC Bayes bound is valid when applied to the fine tuning.  For example, one could use an L2 bound (derived from PAC Bayes) on the difference between the fine tuned parameters and pre trained parameters.  But the paper proposes evaluating the value of pre training before looking at any labeled data. Let $\pi_0$ be the prior before unsupervised or semi supervised training and let $\tilde{\pi}$ be the prior after pre training.  The paper proposes using the entropy ratio $H(\pi_0)/H(\tilde{\pi})$ as a measure of the value of the pre training.  As the reviewers note, this is not really related to PAC Bayes bounds.  Furthermore, it is clearly possible that the pre training greatly focuses the prior but in a way that is detrimental to learning the task at hand.  I have to side with the reviewers that feel that this is below threshold.
The reviewers agree that while the presented result looks interesting, it is but one result. Further, one of the reviewer finds this to be a weak comparison as well. The novelty of the approach over the paper by Ba et. al. also is in question   good results on multiple tasks might have made it worth exploring, but the authors did not establish this to be the case convincingly.
This manuscript proposes an approach for fair and robust training of predictive modeling   both of which are implemented using adversarial methods, i.e., an adversarial loss for fairness and an adversarial loss for robustness. The resulting model is evaluated empirically and shown to improve fairness and robustness performance.  The reviewers and AC agree that the problem studied is timely and interesting, as there is limited work on joint fairness and robustness. However, the reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. In reviews and discussion, the reviewers also noted insufficient motivation for the approach. 
All three reviewers found that the motivation for the proposed method was lacking and recommend rejection. The AC thus recommends the authors to take these comments in consideration when revising their manuscript.
This paper proposes a new decaying momentum rule to improve existing optimization algorithms for training deep neural networks, including momentum SGD and Adam. The main objections from the reviewers include: (1) its novelty is limited compared with prior work; (2) the experimental comparison needs to be improved (e.g., the baselines might not be carefully tuned, and learning rate decay is not applied, while it usually boosts the performance of all the algorithms a lot). After reviewer discussion, I agree with the reviewers’ evaluation and recommend reject.
The authors propose to use identity + some weights in the recurrent connections to prevent vanishing gradients. The reviewers found the experiments to have weak baselines, weakening the claims of the paper.
This paper presents a novel method for identifying simuli induced patterns in MEG and EEG signals.  The authors develop a novel statistical point process model and a fast EM algorithm to learn the parameters.  Discussion of this paper centered around: how to fit hyperparameters, and similarity and comparison with other algorithms, especially ICA, as well as the small number of subjects  Comparison to other methods would make the work stronger, as would adding more datasets but this novel algorithm seems worth publishing. I recommend acceptance as a poster.
The reviewers agree that the method is original and mostly well communicated, but have some doubts about the significance of the work.   
This paper investigates a new approach to machine reading for procedural text, where the task of reading comprehension is formulated as dynamic construction of a procedural knowledge graph. The proposed model constructs a recurrent knowledge graph (as a bipartite graph between entities and location nodes) and tracks the entity states for two domains: scientific processes and recipes.  Pros: The idea of formulating reading comprehension as dynamic construction of a knowledge graph is novel and interesting. The proposed model is tested on two different domains: scientific processes (ProPara) and cooking recipes.  Cons: The initial submission didn t have the experimental results on the full recipe dataset and also had several clarity issues, all of which have been resolved through the rebuttal.   Verdict: Accept. An interesting task & models with solid empirical results. 
This paper presents a nice set of results on a new RL algorithm. The main downside is the limitation to the Atari domain, but otherwise the ablation studies are nice and the results are strong.
The paper proposes numerical method for solving SDEs that empirically are faster than previous approaches. Two reviewers felt the paper was above threshold, while two felt it was below threshold for acceptance. While the paper is borderline in this sense, all four reviewers noted that the paper lacked a theoretical justification and rested on empirical evidence for the usefulness of the approach. Several reviewers also pointed out that these empirical results are on the weak side. While the paper may add a potentially useful learning trick to the optimization literature, these two significant concerns put it on the side of a borderline reject.
The paper proposes an approach to automatically tune the learning rate by using a statistical test that detects the stationarity of the learning dynamics. It also proposes a robust line search algorithm to reduce the need to tune the initial learning rate. The statistical test uses a test function which is taken to be a quadratic function in the paper for simplicity, although any choice of test function is valid. Although the method itself is interesting, the empirical benefits over SGD/ADAM seem to be minor.  
The paper introduces the CareGraph, a knowledge graph based recommendation approach.  CareGraph is a deep neural network based recommender that can be used a mobile healthcare platform for nudge recommendation. The main motivation is to use the knowledge graph to  mitigate cold start problems when recommending nudge messages.   The papers  main strength is the topic of interest. Research on recommending systems in the healthcare context is of great interest. However, the reviews raised concerns that outweigh the strengths.  The majority reviewers agree that the work is not ready for publication. Main concerns focus on weak experimental section and lack of technical details.  I recommend the authors to incorporate all the reviewers  comments and make a  stronger submission to a future conference!
This paper is essentially an application of dual learning to multilingual NMT. The results are reasonable.  However, reviewers noted that the methodological novelty is minimal, and there are not a large number of new insights to be gained from the main experiments.  Thus, I am not recommending the paper for acceptance at this time.
The paper improves the Bloom filter learning by utilizing the complete spectrum of the scores regions.   The paper is nicely written with strong motivation and theoretical analysis of the proposed model. The evaluation could be improved: all the experiments are only tested on the small datasets, which makes it hard to assess the practicality of the proposed method. The paper could lead to a strong publication in the future if the issue on evaluation can be addressed.  
The paper proposes a model and a training mechanism for multimodal generation. The reviews are generally positive: they praise the generality of the method, the extensive experimental evaluation, and the good empirical results. Overall, no major concerns were raised, and all reviewers recommend acceptance.  A couple of concerns remain, in my view:   The method is generally heuristic, and intuitively rather than theoretically motivated. This is compensated of course by the empirical evaluation, which is thorough.   The paper could be better written. The reviewers suggested some minor improvements which were implemented in the updated version, but I believe there is room for further improvement.  Due to the above concerns, I consider the rating of reviewer #3 (10: Top 5% of accepted papers, seminal paper) to be unjustifiably high. On balance, however, I m happy to recommend acceptance.  Message to the authors:  In the abstract you write: "a simple generic model that can beat highly engineered pipelines". Please be aware that the word "beat" evokes competition, winners and losers, so it s not appropriate in the context of scientific evaluation. Please consider replacing it with something neutral, such as "a simple generic model that can perform better than ...".
Given two distributions, source and target, the paper presents an upper bound on the target risk of a classifier in terms of its source risk and other terms comparing the risk under the source/target input distribution and target/source labeling function. In the end, the bound is shown to be minimized by the true labeling function for the source, and at this minimum, the value of the bound is shown to also control the "joint error", i.e., the best achievable risk on both target and source by a single classifier.   The point of the analysis is to go beyond the target risk bound presented by Ben David et al. 2010 that is in terms of the discrepancy between the source and target and the performance of the source labeling function on the target or vice versa, whichever is smaller. Apparently, concrete domain adaptation methods "based on" the Ben David et al. bound do not end up controlling the joint error. After various heuristic arguments, the authors develop an algorithm for unsupervised domain adaptation based on their bound in terms of a two player game.  Only one reviewer ended up engaging with the authors in a nontrivial way. This review also argued for (weak) acceptance. Another reviewer mostly raised minor issues about grammar/style and got confused by the derivation of the "general" bound, which I ve checked is ok. The third reviewer raised some issues around the realizability assumption and also asked for better understanding as to what aspects of the new proposal are responsible for the improved performance, e.g., via an ablation study.  I m sympathetic to reviewer 1, even though I wish they had engaged with the rebuttal. I don t believe the revision included any ablation study. I think this would improve the paper. I don t think the issues raised by reviewer 3 rise to the level of rejection, especially since their main technical concern is due to their own confusion. Reviewer 2 argues for weak acceptance. However, if there was support for this paper, it wasn t enough for reviewers to engage with each other, despite my encouragement, which was disappointing.
There is a pretty good consensus that this paper should not be accepted at ICLR. The reviewers do not seem think that extending MuZero to non deterministic MuZero constitutes a significant advance.  Three reviewers give clear rejects with scores (3, 4, 5) all with good confidence (4).  A fourth reviewer gave a score of 6, i.e., borderline accept.  While the fifth reviewer recommends, he does not seem to be very confident and did not step in to champion the paper.  The program committee decided that the paper in its current form does not meet the acceptance bar.
The submission proposes "feature flow regularization" during training to enforce (approximately) sparse network weights which can then be post hoc pruned.  The form of the regularizer is reasonably well motivated, and the method seems interesting.  The reviewers were split on this, with two recommendations for "marginally above" and one for "marginally below" the threshold of acceptance.  I therefore read the paper in detail, in addition to reading the reviews, rebuttal, and private reviewer comments.  The appendix on the sparsity accuracy tradeoff and its relationship to the hyperparameters k1 and k2 is an interesting experiment, and overall the authors were very engaged in the reviewing process.  In an initial reading, the term trajectory is indeed vague, although it is presented as a definition.  This ambiguity is reflected in the reviewer discussion where in response to Reviewer wyu7, the authors indicate that there are two different meanings of the word in different papers that are being confused.  In a mature presentation, these definitions should probably be given mathematically early on in a formal definition box, but this would require significantly tightening up the mathematical notation early on.  The results table does not show that the proposed method Pareto dominates other methods (accuracy, sparsity, and latency), which themselves are necessarily limited due to the very high number of published papers on network pruning.  Furthermore, some of the selected comparisons appear to be optimizing for different metrics rather than network sparsity, e.g. DCP reports better accuracy for VGG 16 after pruning is applied.  This indicates that the proposed method is somehow in the crowd, but does not seem to show a clear consistent improvement over SOTA.  Analysis in Appendix A.3 does not really depend on which kind of norm is used   the same conclusion will be reached that ||X|| decreases, while structured sparsity, e.g. with expected sparsity rates, is dependent on the kind of norm.  As such, it s OK, but not a particularly specific result.  On the whole, this indicates that the paper is interesting, but borderline with room for concrete improvements that go beyond the scope of a simple refinement for a camera ready presentation.
This paper proposes a method called approximate empirical Bayes to learn both the weights and hyperparameters. Reviewers have had a mixed feeling about this paper. Reviewers agree that the novelty of this paper is limited since AEB is already a well known method (in fact, iterative conditional modes is a well known algorithm). Unfortunately, the paper completely ignores the huge literature on this topic; the previous reference to use AEB is from McInerney (2017).  Another issue is that the paper seems to be unaware of any issues that this type of approach might have. Here is a reference that discusses some problems with this type of approach:  "Deterministic Latent Variable Models and their Pitfalls" Max Welling∗ Chaitanya Chemudugunta, Nathan Sutter, 2008  The experiments presented in the paper are interesting, but then are not really doing a good job to assess why the method works well here even though in theory it should not be as good as the exact empirical Bayes method.   This paper does not meet the bar for acceptance at ICLR and therefore I recommend a reject for this paper. 
This paper presents a new method of employing some existing techniques to improve robustness, which was verified through experiments. According to the reviewers’ comments and the authors’ responses to these comments, the reviewers generally appreciate the authors’ effort in properly improving and clarifying the proposed method. However, their major concerns still rely on the novelty of this paper, which is identified as a combination of some existing techniques. In addition, the proposed method at its current status still contains some un convincing points. Hence, the paper is recommended rejected.
The manuscript focuses on model robustness under distribution shift, specifically domain shifts and subpopulation shifts. Domain shift is where the test domain and train domain are disjoint. Subpopulation shift is where test distribution has different mixture proportion than train distribution. The assumption is that domain identification spuriously correlates with labels. The proposed framework learns an invariant representation by using mixup strategies and interpolates samples either with the same labels but different domains or with the same domain but different labels to. Experiments are performed on a variety of domain shift and subpopulation shift benchmarks, and results showed that the proposed framework is better than empirical risk minimization (ERM) and alternative data augmentation methods. Theoretical analysis is also provided and it is shown that, under certain conditions, the proposed framework has asymptotically smaller worst case classification errors than ERM and vanilla mixup.  Reviewers agreed on several positive aspects of the manuscript, including: 1. The manuscripts addresses a critical point that prevent models from generalization, namely spurious correlation;  2. The proposed method is simple and easy to implement, and the empirical results are within expectation.  Reviewers also highlighted several major concerns, including: 1. Different recent approaches introduce methods that use some sort of mixup across domains in similar settings; 2. Ablation study on datasets without spurious correlations are missing; 3. Evaluation of domain invariance representations and prediction level invariance needs clarifications;  Authors clarified different motivations of the two selection strategies in relation to spurious correlation between domains and labels, and provided an ablation study on datasets with no spurious correlation. Post rebuttal, reviewers stayed with borderline ratings, and they have suggested further improvements: improving results analysis and the conclusion that “existing domain information may not fully reflect the spurious correlation”, understanding the implication and the reasons that invariance is achieved at the prediction level instead of at the representation level despite the original goal is to learn an invariant representation, and improving presentation of the manuscript including settings and assumptions.
The paper proposes a variant of Sparse Transformer where only top K activations are kept in the softmax. The resulting transformer model is applied to NMT, image caption generation and language modeling, where it outperformed a vanilla Transformer.  While the proposed idea is simple, easy to implement, and it does not add additional computational or memory cost, the reviewers raised several concerns in the discussion phase, including: several baselines missing from the tables; incomplete experimental details; incorrect/misleading selection of best performing model in tables of results (e.g. In Table 1, the authors boldface their results on En De (29.4) and De En (35.6) but in fact, the best performance on these is achieved by competing models, respectively 29.7 and 35.7. The caption claims their model "achieves the state of the art performances in En Vi and De En" but this is not true for De En (albeit by 0.1). In Table 3, they boldface their result of 1.05 but the best result is 1.02; the text says their model beats the Transf XL "with an advantage" (of 0.01) but do not point out that the advantage of Adaptive span over their model is 3 times as large (0.03)).  This prevents me from recommending acceptance of this paper in its current form. I strongly encourage the authors to address these concerns in a future submission.
The submission proposes a method for adversarial imitation learning that combines two previous approaches   GAIL and RED   by simply multiplying their reward functions. The claim is that this adaptation allows for better learning   both handling reward bias and improving training stability.   The reviewers were divided in their assessment of the paper, criticizing the empirical results and the claims made by the authors. In particular, the primary claims of handling reward bias and reducing variance seem to be not well justified, including results which show that training stability only substantially improves when SAIL b, which uses reward clipping, is used.   Although the paper is promising, the recommendation is for a reject at this time. The authors are encouraged to clarify their claims and supporting experiments and to validate their method on more challenging domains.
This paper attempts at modeling text matching and also generating rationales.  The motivation of the paper is good.  However there is some shortcomings of the paper, e.g. there is very little comparison with prior work, no human evaluation at scale and also it seems that several prior models that use attention mechanism would generate similar rationales.  No characterization of the last aspect has been made here.  Hence, addressing these issues could make the paper better for future venues.  There is relative consensus between the reviewers that the paper could improve if the reviewers  concerns are addressed when it is submitted to future venues.
The paper investigates the performance of low precision Stochastic Gradient Langevin Dynamics (SGLD). While similar low precision techniques have been widely used in optimization, much less is known for Markov Chain Monte Carlo (MCMC) methods. The paper develops a new quantization function to make SGLD suitable for low precision setups and argues for its use in deep learning.   The main concerns among the reviewers were related to the paper presentation (separation and comparison between optimization and sampling), comparison to Dalalyan Karagulyan 19 and overview of this work, technical depth, and numerical experiments. The authors have adequately responded to the reviewers  comments and addressed them to the extent possible. However, there was ultimately not enough support to lead this paper to acceptance.  I find low precision sampling a worthy topic of study and the contributions of the paper are interesting. The authors are encouraged to revise the paper based on the reviewers  comments, more clearly highlight the contributions, and resubmit.
The paper presents a new algorithm for byzantine resilient nonconvex distributed optimization. The presentation is clear, the motivation is solid, and the problem setting is interesting. The novelty of the present work is sufficient for publicaiton. The new scheme comes with some provable guarantees, improving the prior state of the art. Some of these guarantees are arguably not corresponding to strong operational robustness guarantees, however they compare well with convergence proofs of the related literature. Some concerns were raised with regards to comparison with some prior work, but the authors addressed it in the rebuttal.
This paper analyzes local SGD optimization for strongly convex functions, and proves that local SGD enjoys a linear speedup (in the number of workers and minibatch size) over vanilla SGD, while also communicating less than distributed mini batch SGD. A similar analysis is also provided for the asynchronous case, and limited empirical confirmation of the theory is provided. The main weakness of the current revision is that it does not yet properly relate this work to two prior publications: Dekel et al., 2012 (https://arxiv.org/pdf/1012.1367.pdf) and Jain et al., 2016 (https://arxiv.org/abs/1610.03774). It is critical that these references and suitable discussion be added in the camera ready paper, since this issue was the subject of considerable discussion and the authors promised to include the references and discussion in the final paper.
This paper demonstrates a framework for optimizing designs in auction/contest problems. The approach relies on considering a multi agent learning process and then simulating it.   To a large degree there is agreement among reviewers that this approach is sensible and sound, however lacks substantial novelty. The authors provided a rebuttal which clarified the aspects that they consider novel, however the reviewers remained mostly unconvinced. Furthermore, it would help if the improvement over past approaches is demonstrated in a more convincing way, for example with increased scope experiments that also involve richer analysis. 
All of the reviewers agree the paper has an interesting idea (using rotations of the representation as regularization). However, the reviewers also agree the empirical gains are too insignificant. While the paper shows results on CIFAR, the reviewers mentioned a few other ways to improve performance, such as more complex and unconstrained datasets. These additional experiments would make the effectiveness of proposed approach more convincing.
This paper introduces a modified GAN architecture that looks a lot like a mixture of experts, to address the problem of learning multiple disconnected manifolds.  They show this method helps on 2D toy experiments, and artificial tasks where different datasets are combined, but not on CIFAR.  They also introduced a new variant of FID that they claim is more sensitive to the improvements made by their model.  R2 didn t seem to think too hard about the paper, and R3 seemed a bit dismissive.  Overall the idea seems sensible but the particulars of this approach aren t all that well motivated in my opinion, especially since the cost of the generator is increased.  Why not just use a mixture of Gaussians in the original untransformed space?  I also found the toy experiments unconvincing, particularly the claim that a standard GAN couldn t learn a mixture of 3 Gaussians.  Learning a mixture of 8 Gaussians was one of the results in the unrolled GAN paper, for instance.  The results on the mixed datasets experiments seem encouraging, but I m afraid that proposing a new GAN architecture in 2019 requires even more baselines than the authors compared against, and the fact that the task was artificially constructed undercuts its importance.
The authors introduce a method for disentangling effects of correlated predictors in the context of high dimensional outcomes. While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR due to its limitations in terms of limited applicability and experiments. The paper will benefit from a revision and resubmission to another venue.
The paper has merits on providing a particular way of understanding a prediction model based on auxiliary data (concepts). I have a generally more positive view of it, aligned with the higher scoring reviews. However, I feel a bit uncomfortable of framing it as "causal" in the sense it does not aim to provide any causal predictions, but it is more of a smoothing method for capturing signal contaminated with "uninteresting" latent sources   this is more akin to regression with measurement error (see e.g. Carroll, Ruppert and Stefanski s "Nonlinear regression with measurement error") where, like in this paper, different definitions of "instrumental variables" also exist and are different from the causal inference definition. I can see though why we may want to provide a causal interpretation in order to justify particular assumptions, not unlike interesting lines of work from Scholkopf s take on causality. The paper can be strengthened by some further discussion on the assumptions made about additivity on equations (2) and (3), which feel strong and not particularly welcome in many applications.  The proposed title is still a bit clunky, I feel that the two stage approach is less important than the structural assumptions made, perhaps a title emphasizing the latter rather than the former would be more promising.
The paper presents a method for increasing the "model compatibility" of Generative Adversarial Networks by adding a term to the loss function relating to classification boundaries. The reviewers recognized the importance of the problem, but several concerns were raised about the clarity of the paper, as well as the significance of the experimental results.
The paper proposes a method to improve PROVEN, which gives a certification for probabilistic robustness. However, reviewers think the paper is below the acceptance bar due to unclear motivation and insufficient experiments. In particular, a clear use case of probabilistic robustness certification is crucial for the paper.
The manuscript proposes a novel estimation technique for generative models based on fast nearest neighbors and inspired by maximum likelihood estimation. Overall, reviewers and AC agree that the general problem statement is timely and interesting, and the subject is of interest to the ICLR community  The reviewers and ACs note weakness in the evaluation of the proposed method. In particular, reviewers note that the Parzen based log likelihood estimate is known to be unreliable in high dimensions. This makes a quantitative evaluation of the results challenging, thus other metrics should be evaluated. Reviewers also expressed concerns about the strengths of the baselines compared. Additional concerns are raised with regards to scalability which the authors address in the rebuttal. 
The paper describes very interesting work that advances the state of the art in Zork by going beyond an important state bottleneck.  While there is an important engineering contribution, the reviewers raised important concerns regarding the novelty of the question answering approach to construct knowledge graphs, the clarity of the backtracking heuristic and the extent to which the proposed approach outperforms previous work.  I also read the paper and I agree with the concerns of the reviewers.  In particular, I encourage the authors to provide more details about the backtracking procedure, a formal description of the algorithm and its assumptions to help readers apply the approach in other domains, as well as a formal analysis to better understand when it will or will not pass a bottleneck state.
Strengths: * Theoretical foundation provided to knowledge integration problem * Findings from the empirical studies are interesting * Authors dedicated significant time and energy to coordinating with reviewers in the rebuttal period  Weaknesses: * It is not clear whether the GCS is a suitable approximation for measuring KI. For example, relation types are not supported in the GCS architecture making it unclear whether GCS adequately approximates knowledge integration. As reviewer 4qCM mentions, (X, born_in, Zurich) is very different knowledge from (X, died_in, Zurich). The current formulation only learns co occurrence between entities rather than relational knowledge.  * Empirical study is limited to two knowledge integration methods (ERNIE & K Adapter) and only evaluated on entity typing datasets, which are likely to be well suited for their method which ignores relation information. * The presentation and takeaways of the results could be clearer. Authors should explain in depth why experiments that drop knowledge randomly are not suitable baselines.  This paper is promising and the topic explored by the authors is interesting. I think it would benefit from integrating the comments from the reviewers and will make for a strong submission at a future venue.
The reviewers all believe that this paper is not yet ready for publication. All agree that this is an important application, and an interesting approach. The methodological novelty, as well as other parts of exposition, involving related work, or further discussion of what this solution means for patients, is right now not completely convincing to reviewers. My recommendation is to work on making sure the exposition best explains the methodology, and making sure this venue is the best for the submitted line of work.
A new method for dynamic token normalization in ViTs (both within and across tokens) is introduced in the paper. As noted by the reviewers, the proposed method is technically sound, with a clear and solid motivation. The main raised concerns included the lack of experiments using larger models, unclear reason for the accuracy gains, and lack of experiments on other tasks beyond classification, such as detection and segmentation. The authors’ response was strong, clarifying other questions and providing additional experiments, for example, showing the effectiveness of the method on object detection, and when applied to larger models or architectures that explicitly model local context. Two reviewers recommend borderline rejection, but they did not participate in the discussion nor updated their reviews after the author response. The AC considers that their concerns were adequately addressed by the rebuttal, and agrees with the other two reviewers that the paper passes the acceptance bar of ICLR. The authors should carefully proofread the paper for the final version.
All three reviewers suggest acceptance of the paper. The authors study an interesting problem (understanding non stationary and reactionary policies) and propose a solution to the problem which compares favorably to baselines in experiments. However, some of the reviewers also criticize unclarities in the presentation of the paper and the made assumptions. The authors clarified those points quite well in their rebuttal. Further concerns regarded design decisions and the comparison to failure cases of baselines. The authors addressed those in their rebuttal and promised to include corresponding material in their updated paper. Hence I am suggesting acceptance of the paper. Nevertheless, I would like to urge the authors to carefuly revise their problem presentation in the paper in order to improve clarity and add the promised additional insights to the final version of the paper.
This paper studies the problem of dealing with long contexts within a Transformer architecture. The key contribution is a  kNN memory module that works in concert with  a Transformer by integrating upper layers with additional retrieved context.  The idea is simple  but the execution is good.  While the  idea is reminiscent of other recent work on this topic, and novelty is somewhat borderline, it is practically useful. Overall, though ambivalent, my recommendation is that the paper should probably be accepted
This work appears to be a promising start to a research direction. However, as the reviewers noted, the work does not compare to alternative approaches and the presentation of the work overall is incomplete.
Gradient clipping is increasingly popular and it s nice to see a paper theoretically exploring its nice performance. All reviewers appreciated the work and the results.  Please make sure to incorporate all of their comments for the final version.
This paper proposes an addition to seq2seq models to allow the model to copy spans of tokens of arbitrary length in one step. The authors argue that this method is useful in editing applications where long spans of the output sequence will be exact copies of the input. Reviewers agreed that the problem is interesting and the solution technically sound. However, during the discussion phase there were concerns that the method was too incremental to warrant publication at ICLR. The work would be strengthened with a more thorough discussion of related work and additional experiments comparing with the relevant baselines as suggested by Reviewer 2.
This paper applies reinforcement learning to text adventure games by using knowledge graphs to constrain the action space. This is an exciting problem with relatively little work performed on it. Reviews agree that this is an interesting paper, well written, with good results. There are some concerns about novelty but general agreement that the paper should be accepted. I therefore recommend acceptance.
This paper presents a method to model uncertainty in deep learning regressors by applying a post hoc procedure.  Specifically, the authors model the residuals of neural networks using Gaussian processes, which provide a principled Bayesian estimate of uncertainty.  The reviewers were initially mixed and a fourth reviewer was brought in for an additional perspective.  The reviewers found that the paper was well written, well motivated and found the methodology sensible and experiments compelling.  AnonReviewer4 raised issues with the theoretical exposition of the paper (going so far as to suggest that moving the theory into the supplementary and using the reclaimed space for additional clarifications would make the paper stronger).  The reviewers found the author response compelling and as a result the reviewers have come to a consensus to accept.  Thus the recommendation is to accept the paper.    Please do take the reviewer feedback into account in preparing the camera ready version.  In particular, please do address the remaining concerns from AnonReviewer4 regarding the theoretical portion of the paper.  It seems that the methodological and empirical portions of the paper are strong enough to stand on their own (and therefore the recommendation for an accept).  Adding theory just for the sake of having theory seems to detract from the message (particularly if it is irrelevant or incorrect as initially pointed out by the reviewer).
The main contribution of this work is introducing the uncertainty aware value function prediction into model based RL, which can be used to balance the risk and return empirically.   The reviewers generally agree that this paper addresses an interesting problem, but there are some concerns that remain (see reviewer comments).   I also want to highlight that in terms of empirical results, it is insufficient to present results for 3 different random seeds. To highlight any kind of robustness, I suggest *at least* 10 20 different random seeds; otherwise the findings can/will be misleading. 
The reviewers were split about this paper: on one hand they would have liked to see more experiments on different problem settings on the other they appreciated the elegance of graph encoding methods and current results. After going through the paper and discussion I have voted to accept for the following reason:  the additional experiments and discussion posted during the rebuttal phase have addressed many of the main concerns of the reviewers (i.e., training time, message passing figure, discussion on encoding and SAT solvers). The only remaining one I see is the request for additional experiments which I don t think is grounds for rejection: current results are comprehensive and an additional experiment I think would not alter the main conclusions. I urge the authors to take all of the reviewers changes into account (if not already done so).
This paper proposes a framework for using invertible neural networks to study inverse problems, e.g., recover hidden states or parameters of a system from measurements. This is an important and well motivated topic, and the solution proposed is novel although somewhat incremental. The paper is generally well written. Some theoretical analysis is provided, giving conditions under which the proposed approach recovers the true posterior. Empirically, the approach is tested on synthetic data and real world problems from medicine and astronomy, where it is shown to compared favorably to ABC and conditional VAEs. Adding additional baselines (Bayesian MCMC and Stein methods) would be good. There are some potential issues regarding MMD scalability to high dimensional spaces, but overall the paper makes a solid contribution and all the reviewers agree it should be accepted for publication.
The paper proposes a domain adaptive filter decomposition method via separating domain specific and cross domain features, towards learning invariant representations for unsupervised domain adaptation.  Overall, this well written paper is well motivated with a better technique for learning invariant representations using convolutional filters. Nonetheless, reviewers still have major concerns: 1) the novelty of the paper may be marginal given the significant line of recent work on learning domain invariant representations; 2) when the label distributions differ, learning invariant representations can only lead to worse target generalizations; 3) the provided theory has an unclear connection to the presented filter decomposition method. The paper can be strengthened by further discussions on how to mitigate the aforementioned negative results.   Hence I recommend rejection.
This paper presents a method for improving optimization in multi task learning settings by minimizing the interference of gradients belonging to different tasks.   While the idea is simple and well motivated, the reviewers felt that the problem is still not studied adequately. The proofs are useful, but there is still a gap when it comes to practicality.  The rebuttal clarified some of the concerns, but still there is a feeling that (a) the main assumptions for the method need to be demonstrated in a more convincing way, e.g. by boosting the experiments as suggested with other MTL methods (b) by placing the paper better in the current literature and minimizing the gap between proofs/underlying assumptions and practical usefulness.  
This paper extends prototypical classification networks to handle class hierarchies and fairness. New neural architecture is proposed and experimental results in support of it are presented.  Unfortunately, reviewers found that paper in its current for is not sufficiently strong to be accepted at ICLR. Authors have made a significant attempt to clarify and improve the paper in their response. However, reviewers believe that contributions and motivation can be clarified further. We encourage authors to improve their work according to the specific suggestions made by the reviewers and resubmit.
The paper proposes to speed up self supervised learning for semi supervised learning by combining self supervised pretraining and supervised fine tuning into a single objective. The proposed supervised loss builds on Neighbourhood Components Analysis and soft nearest neighbor losses. Most reviewers are concerned about the novelty of the approach and the significance of empirical results. I agree with both concerns. I appreciate the comparison between $\log \sum \exp$ and $\sum \log \exp$, but it seems a simpler cross entropy loss also achieves a similar goal (potentially somewhat slower). I believe adding more experiments comparing different supervised loss functions across different architectures can help improve the paper. 
Reviewers raise the serious issue that the proof of Theorem 2 is plagiarized from Theorem 1 of "Demystifying MMD GANs" (https://arxiv.org/abs/1801.01401). With no response from the authors, this is a clear reject. 
This paper studies group equivariant neural posterior estimation which seeks to endow conventional neural posterior estimation method with equivariance of both the data and parameters simultaneously. To test the efficacy of the proposed approach the authors experiment with gravitational wave data and show that the proposed GNPE achieves considerable performance gains.   Strengths:    The approach is independent of neural architectures and does not necessitate knowledge of exact equivariances.   The method seems to be much better than regular NPE in cases where there are known equivariances.  Weaknesses:    the writing of the paper is not clear, which makes the paper difficult to read and evaluate.   It is hard to check the correctness of the conclusions and algorithm due to lack of necessary assumptions and derivation steps.   the authors are knowledgeable about the subject but present material in a slightly callous way which prevents a precise understanding of their techniques.   This is a borderline paper with two reviewers in favor of acceptance and two with a slight tendency to reject. The two negative reviewers did not engage in a discussion with the authors or did not complete that discussion, failing to confirm their ratings or provide an update of those ratings. They also do not seem to give strong arguments for rejection. Based on this, I recommend the paper for acceptance. However, I encourage the authors to take into account the reviewers  comments, especially the part on clarity and rigor, to improve the paper for its camera ready version.
This paper deals with the important topic of learning better graph representations and shows promise in helping to detect critical substructures of graph that would help with the interpretability of representations. Unfortunately, this work fails to accurately portray how it relates to previous work (in particular, Niepert et al, Kipf et al, Duvenaud et al) and falls short of providing clear and convincing explanations of what it can do that these models can t, without including all of them in experimental comparisons. 
This paper proposes an approach for architecture search by framing it as a differentiable optimization over directed acyclic graphs. While the reviewers appreciated the significance of architecture search as a problem and acknowledged that the paper proposes a principled approach for this problem, there were concerns about lack of experimental rigor, and limited technical novelty over some existing works. 
This main focus of this paper is graph modeling. Specifically, this paper considers a setting in which data is generated under continuous time dynamics based on neural ODE. Theoretical results regarding parameter estimation are provided. The results are also supported by experiments.  The reviewers appreciate a thorough response to their questions and think that this paper would be of interest to ICLR and ML community. Please address reviewers comments in your final version.
This paper analyzes the data scaling laws in NMT tasks with different network architectures and data qualities. The main purpose of this paper is to investigate how such different experimental setup affects the scaling law. The authors found that those difference does not have strong impact on the scaling exponent, and a small difference of model architecture and data noise can be compensated by larger data size.  This paper gives nice justification of data scaling law from some different aspects which is instructive to some extent. On the other hand, the paper has some weakness as listed in the following: (1) The scaling law itself has been analyzed by many papers, and its novelty is rather limited. I acknowledge that this paper investigates different aspects of the data scaling law and the size of experiments are larger than existing work. However, the result is rather unsurprising. (2) The experiments are conducted mostly on one language pair (English to German), it is still unclear whether the findings are universal to other language pairs. As the authors responded, exhaustive experiments over all language pairs are unrealistic but some more investigation to more general data sets could be conducted to strengthen the paper.    This paper is around the borderline. Some reviewers were rather positive to this paper. However, they also pointed out the concerns I listed above and they do not show strong support on the paper.   In summary, although this paper shows some instructive findings, it is still a bit below the threshold of acceptance.
This paper set out to show that increasing task diversity during meta training process does not boost performance. The reviewers mostly  agreed (only reviewer wVFn dissented) that the empirical set up of the paper was convincing, but they also felt it over emphasized empirics over a deeper understanding of the phenomena observed. In turn, this resulted in discussions around how the experiments and the explanations didn t fully prove that increasing task diversity does not help. Overall, the discussion and the additional analysis tools provided by the authors (such as the diversity metric) will greatly improve the paper.
This paper report empirical implications of privacy ‘leaks’ in language models. Reviewers generally agree that the results look promising and interesting, but the paper isn’t fully developed yet. A few pointed out that framing the paper better to better indicate broader implications of the observed symptoms would greatly improve the paper. Another pointed out better placing this work in the context of other related work. Overall, this paper could use another cycle of polishing/enhancing the results.  
This paper proposes a unified way of data augmentation using a latent embedding space   it learns a continuous latent space for transformation, and finds effective directions to traverse in this space for data augmentation. The proposed approach combines existing approaches for data augmentation, e.g., adversarial training, triplet loss, and joint training.  The paper also identifies input examples where the model had low performance and creates harder examples that help the model improve its performance. It is evaluated on multiple corresponding to text, table, time series and image modalities and outperforms SOTA except on image data.  The paper has responded to the reviewers  feedback to provide more detailed experiments with stronger baselines and also ablation studies to show the effectiveness of different components of the approach. The results can be further improved by thorough empirical comparison to other SOTA methods, and by using other loss functions (e.g.,center loss, large margin loss and other contrastive losses) as alternatives to the triplet loss proposed in the paper.  Some reviewers have pointed out that the paper is somewhat limited in it s novelty, since it combines existing off the shelf modules/losses and similar methods have been tried in the past   the novel contributions of the paper should be clearly highlighted in the revised submission.
This paper received borderline scores, R1, R3, R4 gave a score of 6 and recommended a borderline acceptance. R2 provided by far the most detailed review and recommended a score of 5 (i.e., borderline reject). After the rebuttal, R2 comments, "I believe that the paper is still below the acceptance threshold, although only marginally". Overall, I concur with R2. The reasons are detailed below:   The paper proposes a method for communication between two agents, wherein one agent actuates its joints to communicate intent. Intuitively, this resembles making a gesture. The paper considers the setting of a discrete number of intents. The sender agent is modeled as a neural network that takes as input the intent and outputs a trajectory of joints. The receiver observes a noisy version of the trajectory and outputs the intent. The parameters of the sender policy and receiver discrimination network are optimized to maximize classification accuracy. It is shown that if the intents are sampled from Zipf distribution and trajectories are penalized based on their energy, then a receiver agent initialized from scratch is better at inferring the intent from a pre trained speaker agent, as opposed to when the distribution is uniform or when the energy regularization is not used (Figure 2).  Further, section 5.2 shows that when the listener is provided with the energy of the trajectory then it is better at recognizing the intent as opposed to being provided with the entire trajectory when a number of intents are small. With a larger number of intents (N 10), the performance is at chance accuracy.   The biggest challenge with the paper is that it is very poorly written. Large parts of the method and experimental setup are in the appendix (A.2 / A.3), which makes it hard to understand the paper. Section 4.2 is rather confusing because the ideas introduced are not used for training, but simply for evaluation. Further, the authors point out in the rebuttal that torque curriculum is not required, but it is still there in the paper and makes it more confusing. I recommend the authors to substantially rewrite the paper and focus on relevant parts instead of philosophical arguments. Lastly, I am confused by results in Table 2   the authors mention in the text that with 10 intents, intent identification is at chance (i.e. 34% accuracy), but the table shows 56% accuracy. A clarification would be helpful here.   The problem of communicating intents via gestures, when the agents are unaware of mapping from intents to gestures is an exciting area of research. From the perspective of emergent gestures, this paper has a novel contribution. However, the settings are toy and even in such a setup, the results are underwhelming. The assumptions that make the setup toy are: the listener agent knows about all joint locations of the sender (with some noise) and also has access to the energy exerted by the agent. Without access to energy, the performance is poor. In real world scenarios, these are big assumptions. Furthermore, even when the energy is known For instance, even when the number of intent is small (i.e.,  N 10,) the performance is bad. The authors argue that is due to local minima in the optimization   but that s exactly where the contribution could have been.   I will reiterate, that the authors claim their contribution is in using energy minimization + Zipf intent distribution as a mechanism for communicating intent   which I agree to. However, as pointed out earlier, the paper is not well executed or written and therefore I recommend rejection.   
This paper focuses on a notion of privacy in learning representations.   One of the primary concerns of the reviewers was clarity of the writing and results. Numerous concerns are mentioned in the reviews, and also more engagement with the fairness literature was desired. One reviewer felt that some of the claims in the paper were unsubstantiated, for example: understanding the sanitization process in a human understandable visual way", "integration of a notion of interpretability". It was felt that the changes required were more than could be expected for a camera ready version. The authors are recommended to revise the paper with a particular eye for clarity to a new reader.  The notion and measurement of privacy was also considered to be somewhat shaky. It is understood that the nature of privacy considered in this paper is different from differential privacy. That said, the latter is a rigorous definition, and the one in this paper seems to be rather empirical in nature. There are no formal guarantees in terms of privacy preservation, and it is not clear whether the representations could leak information when evaluated with a different network. As privacy is a mission critical property, some justification of why the heuristic measurement of privacy is acceptable.  As a side note, the authors should consider using the \citep command for parenthetical citations in the text.
Reviewers agreed that connecting neural networks with dynamical systems to create a new kind of optimizer is an interesting idea. After the authors  improvements, this is a strong submission of wide interest.
Thank you for submitting you paper to ICLR. The paper presents an interesting analysis, but the utility of this analysis is questionable e.g. it is not clear how this might lead to improved VAEs/GANs. The authors did add an additional experimental result in their revised paper, but questions still remain. In light of this the significance of the paper is on the low side and it is therefore not ready for publication in ICLR without more work. 
All the reviewers recommend rejecting the submission. There is no basis for acceptance.
The reviewers agree that the problem of learning learning credit assignment from terminal rewards is interesting, and that the presented approach is promising. There are some concerns regarding the rigor and correctness of the theoretical results, and I ask the authors to improve those aspects of the paper. I also ask the authors to the result figures easier to read. The chosen colors are not ideal and the use of log scale x axis is not standard. Finally, including DAgger in the same plot is confusing assuming that DAgger user more information.
The paper gives a new method for code generation from natural language queries using pretrained models. The approach follows two steps: (1) given a query, it selects a set of similar training examples using a method called Target Similarity Tuning, and (2) it then uses a method called Constrained Semantic Decoding (built on top a frozen language model) to adapt these examples into syntactically/semantically correct code.  The reviewers found the paper interesting. There were some concerns about the method s scope and its relationship to prior work but these were mostly addressed during the author response period. Given this, I am delighted to recommend acceptance. Please incorporate the feedback in the reviews (in particular, the review by vLDx) in the final version of the paper.
Three reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera ready submission.
The paper presents a really interesting take on the mode collapse problem and argue that the issue arises because of the current GAN models try to model distributions with disconnected support using continuous noise and generators. The authors try to fix this issue by training multiple generators with shared parameters except for the last layer.  The paper is well written and authors did a good job in addressing some of the reviewer concerns and improving the paper.  Even though arguments presented are novel and interesting, reviewers agree that the paper lacks sufficient theoretical or experimental analysis to substantiate the claims/arguments made in the paper. Limited quantitative and subjective results are not always in favor of the proposed algorithm. More controlled toy experiments and results on larger datasets are needed. The central argument about "tunneling" is interesting and needs deeper investigation. Overall the committee recommends this paper for workshop.
This paper studies how to efficiently expose failures of "top performing" segmentation models in the real world and how to leverage such counterexamples to rectify the models. The key idea is to discover most "controversial" samples from massive online unlabeled images. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness.  However, there exists some limitations coming from R2 and R3, for example, 1) Segmentation benchmarks may not require pixel level dense annotation. There are also examples of benchmarks where the groundtruth consists of computer segmentations corrected by humans. 2) It is much harder for segmentation data to be class balanced in the pixel level, making highly skewed class distributions common for this particular task. 3) Citing the field of computer assisted annotation as relevant work.  In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed.
The paper proposes to use greedy core set sampling to improve GAN training with large batches. Although the problem is clear and the solution works, reviewers have raised several concerns. One concern is that the technical novelty is limited; another (in the first version) that even a simpler version of gradient accumulation can solve the  main task (rather that computing core sets). In the end, some discussion was done, with quite a few additions and experiments done by the authors. The final concern that seemingly was not addressed: the gradient accumulation seems to give the same numbers as large batches, thus you can  mimic  large batch sizes with smaller ones and gradient accumulation, making the main claim of the paper questionable. The achievement of SOTA is good, but it is not clear wether it is due to the proposed technique, or rather smart tuning of a larger set of hyperparameters. Thus, I would agree with the concern of Reviewer1.
## Summary  The paper advances the state of the art in training binary neural networks coming out to first place on ImageNet with a controlled computation budget. While any paper making a new record on ImageNet would be a serious candidate for acceptance, it is positive that this one achieves the goal by putting at work the mechanism of conditional computation, innovative for binary networks, and studying in a systematic and clear way how the network width and configuration can be varied while maintaining the computation budged.  ## Review Process and Decision  The paper was thoroughly discussed by reviewers from different aspects. Several weaker spots have been identified (see below and final reviews), but no critical issues that would indicate a necessity of a major revision. In the end, reviewers agreed on acceptance although in some cases they have decided to keep their original < 5 ranking to reflect the scientific value to them from a more global perspective. I think it is an example of well done modeling and experimental work: the work is very clear, uses sound methods, the experimental results are systematic and give interpretable evidence, which is in my experience is rather exceptional for the overall very empirical binary NNs field. I estimate high interest because of the concept of conditional computation put to work here and because of making a new record on ImageNet.  ## Details  * Computation Cost  If such networks are to be deployed in low power devices, the computation cost might need to be estimated more accurately. An example of such estimation is the work by Ding et al. (2019) Regularizing Activation Distribution for Training Binarized Deep Networks, where the energy and area are estimated using information from a semiconductor process design kit.  There is indeed a number of floating point operations around the binary convolutions: first and last layers, experts, skip connections with scale factors and non linearities. The latency and cost of these operations may not be negligible on target devices. In particular Ding et al. (2019)  argue that XNOR Net architecture is 3 times more costly because of floating point scale factors. However the paper does a fair job in comparing in operation counts, which is a good proxy for many devices. The floating point computations needed in various places can be indeed further reduced to lower bit width, the research on quantization techniques shows this is possible and orthogonal to the contribution.  * Novelty of grouped convolutions design and search  The work of Phan et al. (CVPR 2020): Binarizing MobileNet via Evolution based Searching also proposed to search for best grouped convolution under computation budget constraints (evolutionary search method). Strict budget constraint and merging results from different groups are somewhat novel and the prior work can be objectively contemporaneous.  * Clarity  Clarity of the paper has been improved by the revision. One remaining mysticism is still about the gradient estimator for the experts. The paper states: "we wish to back propagate gradients for the non selected experts", "allows meaningful gradients to flow to all experts during training". The problem is that since $\varphi(z)$ is binary one hot on the forward pass, the gradient of the scalar product with $\varphi$ in (2) results in that in the backward pass only the selected expert receives the training signal and by no means all of them. This is regardless of how the gradient is propagated through $\varphi$. Maybe something is missing? I hope the authors can clarify in the final version. I do not consider it as a serious flow since this training scheme is not claimed as a contribution in any case.  One more point on the clarity: The paper claims that using experts increases the network representation power / capacity. While this seems logical, and follows the preceding work in real valued NNs, the paper could provide additional evidence in terms of training performance of these models. Since the teacher with 76% accuracy is used in the distillation, I assume the training never reaches 100% training accuracy in any of the settings. Does the training accuracy improves with experts? This would be a helpful evidence for further work.  * Search method  The paper was further criticized for that the manual search of the architecture is a step back from automated search methods (NAS, BATS). However these methods are themselves a relaxation of discrete choices (experts, if you like), that need to keep all possible configurations at the same time, which may be less stable and too costly for real architectures and datasets. The principles of gradient based architecture search are not entirely clear and the resulting models coming out of these methods typically give no insights regarding good (intelligent) design choices. At present, the systematic exploration with analysis of tradeoffs conducted is seen to have advantages.   
Dear authors,  Thank you for your submission to ICLR. Sadly, the reviewers were not convinced by the novelty of your approach nor by its experimental results. Thus, your paper cannot be accepted to ICLR.
The paper focused on deep regression problems and proposed a label encoding technique which can be thought as a sibling of the famous error correcting output codes but designed for regression problems. The main idea is well illustrated in Figure 1 at the top of page 3, where the encoder and decoder are the main objects of the proposal (and a quantizer is also needed for using the encoder/decoder which is a uniform quantizer in the paper). The idea/proposal is supported by solid theoretical arguments and convincing empirical evidences (not only the paper but also the rebuttal). While there were some concerns in the beginning, the authors have successfully clarified all the concerns and then the average score has been increased from 5.5 to 7.5. As a result, the paper is clearly above the bar of acceptance. What is more, an advantage is that the proposed method is task agnostic and can be combined with different task specific feature extractors borrowed from very complex regression problems (e.g., head pose estimation, facial landmark detection, age estimation, and end to end autonomous driving), making its significance and potential impact high. Given these facts, I think the paper might be selected as a spotlight presentation at the conference.
Reviewers acknowledged that the problem addressed in this paper is interesting and is not solved by the existing literature. They appreciated that the setup was well defined and the paper was clearly written. Yet they kept several concerns after the rebuttal. Especially, they expected the comparison to be done with algorithms using both demonstrations and rewards and the current empirical evaluation was not judged as fair. Also, the simple baseline consisting of adding an LSTM to BC to integrate past observations has not been considered either. This baseline is still missing to assess the quality of the proposed method. 
There has been a long discussion on the paper, especially between the authors and the 2nd reviewer. While the authors  comments and paper modifications have improved the paper, the overall opinion on this paper is that it is below par in its current form. The main issue is that the significance of the results is insufficiently clear.  While the sender receiver game introduced is interesting, a more thorough investigation would improve the paper a lot (for example, by looking if theoretical statements can be made).
**Summary**  This paper proposes a method to do a sample efficient offline domain adaptation method where the setting requires one to have abundant amount of data from the source domain but limited amount of data available in the target domain. The proposed approach DARA achieves that by accounting for the the dynamics shift between the source and the target domain via a reward penalty. The paper shows promising experimental results.  **Final Thoughts** Overall the paper is well written, and it addresses an important problem. The reviewers are mostly positive about the paper at the end. The authors did a very good job addressing the concerns raised by the reviewers. Reviewer 73ni had concerns about the novelty of the approach, it would be nice if the paper can make it more clear about the novelty of the proposed approach compared to the other existing methods in the paper. I  would also recommend the authors to incorporate the feedback and the suggestions made by the reviewer into the camera ready version of the paper.
Reviewers were concerned with the novelty, although appreciated sota results in extensive experiments.
The authors claim that backdoored classifiers are "fundamentally broken" by demonstrating that other backdoors can be generated for such classifiers without the knowledge of the original backdoors. The proposed method, however, requires manual intervention and is not justified by theoretical arguments. Numerous questions asked by the reviewers were not addressed in the rebuttal period.
This paper studies the important problem of adding structured knowledge (in this case from Wikidata) to pretrained language models. The reviewers do not see this paper as ready for ICLR and recommend a number of revisions. Unfortunately the authors did not respond during the author response period. The area chair hence agrees with the reviewers.
This paper proposed an extension of the SIGN model as an efficient and scalable solution to handle prediction problems on heterogeneous graphs with multiple edge types.  The approach is quite simple: (1) sample subsets of edge types, then construct graphs with these subsets of edge types and (2) compute node features on each such graph as if they have only a single edge type, (3) then aggregate the representations from multiple graphs into one using an attention mechanism, and (4) train MLPs on node representations as in SIGN.  Results show that such a simple method can produce quite good results, and is very efficient and scalable.  The reviewers of this paper put it on the borderline, with 3 out of 4 leaning toward rejection.  The most common criticism is the lack of novelty.  Indeed this paper is an extension of prior work SIGN, and the proposed approach is simple.  However, I personally think the simplicity and the great empirical results is rather the strength of this paper.  The authors also did a good job addressing reviewers’ comments and concerns in the discussions, but a few reviewers unfortunately didn’t actively engage in the process.  I d really encourage the authors to improve and highlight the strength of this paper more and submit to the next venue.
Canonical correlation analysis is a method for studying associations between two sets of variables. However these methods lose their effectiveness when the number of variables is larger than the number of samples. This paper proposes a method, based on stochastic gating, for solving a $\ell_0$ CCA problem where the goal is to learn correlated representations based on sparse subsets of variables. Essentially, this paper combines ideas from Yamada et al. and Suo et al. who introduced Gaussian based relaxations of Bernoulli random variables, and sparse CCA respectively. They also extend their methods to work with nonlinear functions by integrating deep neural networks into the $\ell_0$ CCA model. They gave experimental results on various synthetic and real examples, including to feature selection on biological data. The author response addressed a number of the reviewers  concerns, including by providing additional experiments and analyzing the genes selected by their model on the METABRIC dataset. Overall this is a solid contribution both from a theoretical and experimental standpoint.
The paper studies subpopulation shift in object recognition when classes obey a hierarchy. It proposes an architecture, a relevant metric and a dataset (subset of imagenet).  The problem of classification in hierarchical label spaces is important and of great interest, and the effect on domain shift is interesting. Naturally, this problem was studied quite intensively over the years.   Reviewers were concerned that the current proposal was not placed well enough in context of previous literature, both in terms of the method and in terms of experimental results.  Also, the paper would be strengthen if it provides more theoretical analysis about how the hierarchy helps with the domain shift. The authors addressed some of these issues in the rebuttal, adding references and highlighting the differences from previous methods, but the paper would need more time to make the proper experimental comparisons with previous work and subsequent analysis. As a result, the paper is still not ready for acceptance to ICLR in its current form.
The paper proposes a method  to generate conversations for evaluate dialog systems using counterfactual generation.   Pros:   The reviewers agree that the paper makes a good contribution towards evaluation of DST models.    The paper adds to a growing body of work on robust evaluation of NLP models  Cons:   One reviewer has commented on the lack of novelty. However, I believe that the authors have adequately addressed it. In particular, I do not see any harm in using heuristics/templates to generate counterfactuals as long as the final goal of robust evaluation is achieved.    It would have been good to evaluate the method on other datasets. However, I agree with the authors  rebuttal that this is indeed the most popular dataset for the task and most SOTA methods evaluate on this dataset.   The authors have adequately addressed all reviewer concerns and have clearly highlighted their contributions and novelty.  I think of this as a valuable contribution and would like to see the paper accepted. 
This paper explores losses and other training details to produce a model based agent for pixel input continuous control problems.  The authors present a rainbow like approach that combines various separate innovations into a single system.  They show an improvement over a previous baseline on this class of problem, and break down the contributions of the various components.  Though the paper was seen as clearly written, fundamentally, the reviewers did not feel they gained insight through the presentation of the experiments.  For example, one quirk brought up by multiple reviewers is that some combinations of methods show worse performance, but then adding yet another method makes things improved relative to baseline (the authors clarified that this was with the same hyperparameters).  Reviewers found this a bit confusing and insufficiently explored (i.e. was this just hyperparameter tuning or does just the right selection tricks actually need to be combined).  This confusion around method combinations is perhaps relatively minor by itself but indicative of how this paper did not build intuition for the reviewers.  Moreover, none of the reviewers were impressed by the magnitude of improvement over the baseline dreamer agent.  While it was acknowledged the the set of methods improved things, the reviewers felt that each innovation had already been independently validated as likely to improve sample efficiency, so the fact that they did so together was not especially insightful.      I d like to clarify for the authors that I believe this work was, in many respects, technically well executed.  Ultimately, based on the reviews and my own assessment, I don t think the scope was sufficiently ambitious considering the competitiveness of this conference.  While it is useful to occasionally produce summary works which pool a set of separate innovations, such papers must be insightful to readers, aggregate a sufficiently large number of innovations, and/or show striking performance gains.  The final reviewer scores are 4, 5, 6, 4.    
The paper investigates a novel initialisation method to improve Equilibrium Propagation.  In particular, the results are convincing, but the reviewers remain with small issues here and there.  An issue with the paper is the biological plausibility of the approach.  Nonetheless publication is recommended.  
This paper offers a new dataset and accompanying metric to measure the degree to which NLI (textual entailment) systems are aware of gender–occupation associations.  Pros:   The paper deals with an important issue in the context of a visible set of models and datasets.  Cons:   The metric is designed to evaluate bias on models trained for a specific, precisely defined task, but it does not conform to the standard formulation of that task, which makes results on those metric untrustworthy and potentially arbitrary. Reviews had concerns about both the data (the use of references to the form of the premise text) and the metric (the handling of  neutral  predictions).   The proposed definition of bias is not clearly mapped onto a concrete potential harm.   There has been substantial similar prior work on this problem. This doesn t invalidate this work, but it does raise the bar a bit, since arguments of the form  we need to start a conversation about bias in models  are not pursuasive. 
The paper provides a method to accelerate training by choosing a subset of points. After the initial submission, the reviewers raised a major concern about the practicality of the method. In the rebuttal phase the authors provided additional experiments on a large datasets that addressed this concern. That being said, the reviews are still quite borderline. The biggest remaining concern is about the quality of writing. Specifically, there are still requests to “fix the narrative” (NdhY, DHeZ). In addition, some details seem to remain vague regarding the positioning of the paper when compared to the active learning literature (BBTj). Overall, the paper seems to have potential, especially with the new experiments. However, the changes it required when compared to the originally submitted version are simply too extensive to be thoroughly reviewed in a rebuttal phase.
This paper introduces a method to estimate dynamics parameters in recurrent structured models during the learning process. All three reviewers agreed that the idea is interesting and the proposed method could be potentially useful. However, two of the three reviewers have a serious concern about the lack of comparison with other approaches. I agree with these two reviewers; due to the lack of discussion and comparison with existing studies, I cannot recommend accepting this submission in its current form. 
The paper proposed a sketching algorithm for empirical risk minimization (ERM) for linear regression and classification. The technique is based on LSH with non standard hash functions. The reviews indicate that the paper is well written and easy to follow. However, there are several concerns raised regarding its quality. A major one regards the novelty of the paper.   MTPW: “The technique novelty is limited since previous work (Coleman & Shrivastava, 2020) has used LSH to approximate kernel density estimation on streaming setting.“, 7LQM: “My review of the theoretical results and data structure design is that the results are believable and seem correct, but lack technical novelty.” “Other than using non standard hashing functions, what distinguishes the STORM sketch from the RACE sketch?”  An additional concern is a claim of weak experimental evidence. There seems to be a need for more thorough experiments isolating different components rather than the system as a whole, and in addition the bottom line results provide only a slight lift over a naive baseline (7LQM:  “The experiments suggest that using the STORM estimator is only slightly better than returning the mean of your data.”).   Whether it is the case that the techniques should be improved or that these concerns could be addressed by improving the presentation of the paper, the conclusion is that the paper now is not ready to be published.
This paper proposes an extension of the monotonic policy improvement approach to the average reward case. Although the reviewers acknowledge that this work has merits (well written, clearly organized, well motivated, technically sound) the reviewers have raised several concerns, which have been only partially addressed by the authors  responses. In particular, Reviewer4 is still concerned about the discrepancy between the theorem and the implemented algorithm, and the proposed simplification used in the implementation boils down to an algorithm that is very similar to TRPO, thus making the contribution quite incremental as also stressed by Reviewer1. Furthermore, I share the concerns raised about the fairness of comparing algorithms that optimize different objective functions. I suggest the authors take into serious consideration the suggestions provided by the reviewers in order to produce an improved version of their work. The paper is borderline and I think that it needs another round of fresh reviews before being ready for publication. 
The paper is extremely difficult to read, even given that both reviewers have very strong math / theoretical background. Although it may potentially include interesting ideas, nothing in the work could not be understood by the ICLR audience.  
meta score: 7  The paper introduces an online distillation technique to parallelise large scale training.  Although the basic idea is not novel, the presented experimentation indicates that the authors  have made the technique work.  Thus this paper should be of interest to practitioners.  Pros:    clearly written, the approach is well explained    good experimentation on large scale common crawl data with 128 256 GPUs    strong experimental results  Cons:    the idea itself is not novel    the range of experimentation could be wider (e.g. different numbers of GPUs) but this is expensive!  Overall the novelty is in making this approach work well in practice, and demonstrating it experimentally.
Discussions and additional baseline experiments added during the author response period were enough to motivate multiple reviewers to change their recommendation to an accept during the author response. Multiple reviewers felt that the technical novelty of the work was limited, but the rebuttal cleared up their concerns enough to motivate them to switch their assessments to accept.  The claim of this work is that it provides a simpler, sparser, and faster algorithms for differentially private fine tuning of LLMs, yielding SOTA privacy results vs. utility on a number of standard NLP tasks. The work proposes a meta framework.   In the end, all reviewers rated this paper as an accept and the AC also recommends acceptance.
This paper proposes a new way to formulate the design of the deep reinforcement learning that automatically shrinks or expands decision processes.  The paper is borderline and all reviewers appreciate the paper and gives thorough reviews. However, it not completely convince that it is ready publication.   Rejection is recommended. This can become a nice paper for next conference by taking feedback into account. 
The paper seeks to understand how training over parametrized models (e.g., those based on neural networks) to zero training accuracy even when the test error is small (i.e., benign overfitting) can introduce vulnerabilities in the form of adversarial examples and how to remedy the situation. The paper implicates label noise as one of the causes of adversarial robustness, and suboptimal representations learned as part of the training as another. The claims are supported both theoretically and empirically. A good paper overall, accept! 
This article introduces an interesting variant of the work of Nakkiran & Bansal (2020). It shows empirically that the test error of deep models can be approximated from the disagreement on the unlabelled test data between two different trainings on the same data. The authors then show theoretically that a calibration property can explain such behaviour, and they report experiments showing that the relationship does exist in practical situations.  All reviewers agree on the practical and theoretical value of the article, which is very well organised and written. The ideas developed here are likely to lead to further work in the future, and they clearly deserve to be published at ICLR.  I agree with one of the reviewers that the title is somewhat misleading, as the reader expects an analysis based on SGD. The title could be shortened to "Assessing Generalization via Disagreement", and the experimental restriction to SGD could be mentioned in the abstract.
In this paper, the authors consider two algorithms for solving (strongly) monotone variational inequalities with compressed communication guarantees, MASHA1 and MASHA2. MASHA1 is a variant of a recent algorithm proposed by Alacaoglu and Malitsky, while MASHA2 is a variant of MASHA1 that relies on contractive compressors (by contrast, MASHA1 only involves unbiased compressors). The authors then show that   MASHA1 converges at a linear rate (in terms of distance to a solution squared), and at a $1/k$ rate when taking its ergodic averge (in terms of the standard VI gap function).   MASHA2 converges at a linear rate (in terms of distance to a solution squared).  Even though the paper s premise is interesting, the reviewers raised several concerns which were only partially addressed by the authors  rebuttal. One such concern is that the improvement over existing methods is a multiplicative factor of the order of $\mathcal{O}(\sqrt{1/q + 1/M})$ in terms of communication complexity (number of transmitted bits) for the RandK compressor, which was not deemed sufficiently substantive in a VI setting (relative to e.g., wall clock time, which is not discussed).  After the discussion with the reviewers during the rebuttal phase, the paper was not championed and it was decided to make a borderline "reject" recommendation. At the same time, I would strongly urge the authors to resubmit a properly revised version of their paper at the next opportunity (describing in more detail the innovations from the template method of Alacaoglu and Malitsky, as well as including a more comprehensive cost benefit discussion of the stated improvements for the RandK/TopK compressors).
This paper shows an theoretical equivalence between the L2 PGD adversarial training and operator norm regularization. It gives an interesting observation and support it from both theoretical arguments and practical experiments. There has been a significant discussion between the reviewers and authors. Although the authors made efforts in rebuttal, it still leaves many places to improve and clarify, especially in improving the mathematical rigor of the  proof and experiments using state of the art networks.   
### Description  The paper develops a new automatic scheduler to schedule the learning rate during the training. The scheduler has access to the current training state summarized by certain statistics of weights and gradients in all layers and the loss history. It is trained by reinforcement learning with the reward derived from the progress with respect to the performance measure, such as validation accuracy. The key innovations are the design of the state vector using graph convolutional neural networks and empirical improvements to the reward function. The main claim is that GCNs allow to take into account the architecture of the network to be trained and the state of all layers, which, authors hypothesize and demonstrate experimentally, improves performance and transferability of the scheduler across networks and tasks.  ### Decision The reviewers recommendations after the rebuttal settled on 4 x "marginally above" and one "accept". Respectively, I recommend to accept. I recommend a poster based on the reception by reviewers: the novelty was assessed as limited because the idea of an automatic schedulers based on RL with a similar learning strategy belongs to the prior work. On the strong side, the paper satisfied all requests by reviewers for the experiments, regarding alternative methods, large datasets and ablation studies demonstrating that it is indeed the new architecture that allows to achieve a significant improvement, making it a solid improvement step. Amongst alternative methods the paper considers all viable alternatives: a function based schedulers, a hyper gradient method, and a the RL based scheduler, optimized in hyperparameters.  ### Discussion There was no significant non public discussion. As an additional feedback, let me just share my observations.  What is somewhat unclear in that the paper starts by discussing the directed graph of a feed forward network, then it proposes to run GCN on it, which is undirected. Then the hierarchical method is proposed, which runs GCN on each block sequentially while taking the aggregated input from the preceding block. This makes it a directed processing method on the level of block. I wonder whether the directed processing is desired or not desired here? Can a sequential processing summarize the network state efficiently on its own, similar to feedforward propagation, without the global averaging proposed?  It was not very clear to me from the paper what is the meaning of the batch size for training the GSN, and respectively what the batch normalization is doing there.  From some 100 mile perspective, it seems to me that whatever efficient optimization can be performed on the validation set, it helps. It does not matter so much what is varied: the learning rate schedule, other hyperparameters or even the network architecture. So in a sense it is not surprising that one can improve. What is more interesting is that the learned schedulers are generalizable / transferable, as demonstrated in Section 5.4 (changing the architecture or going from CIFAR to ImageNet while keeping the scheduler).  The work has done quite a lot on the experimental side with the baseline GCN model that they proposed. It seems to have still lots of potential via different possible enhancements. For example, what authors mentioned, including exponentially weighted running averages of gradients and squared gradients into the features. They already tried GAT instead of GCN as proposed by reviewers. There may be hyperparameters other than those controlling the learning rate. Some such hyperparameters, e.g. momentum, are apparently tightly coupled with the learning rate. The paper does not discuss how to tune them together with GNS. It could be a difficulty. On the other hand, they can be potentially scheduled with the same GNS.   When considering SGD with momentum (which is not used in the paper), please note that the common use of a momentum parameter $\mu$ actually mixes the learning rate together with the smoothing parameter controlling the exponentially weighted averaging. So if one wants to control the learning rate alone, it is better to implement the gradient smoothing is done in .e.g. Adam, with its hyperparameter $\beta_1$.
This paper proposes a new approach to enforce monotonicity in the context of risk minimization, or to promote it as an inductive bias.  This improves upon existing point wise gradient based methods by expanding the region where monotonicity is enforced.  Group monotonicity is found valuable as a regularization for convolutional models, and multiple applications were shown where the approach appears effective.  The paper is well written, and received detailed discussion. Despite the rebuttal, some major concerns remain, such as drop in accuracy, and empirical estimate of the probability that Definition 1 would not hold over the distribution in question.  Overall, revisions are needed to make the paper publishable.
This paper proposes using first order logic to rule out superficial information for improved natural language inference. While the topic is of interest, reviewers find that the paper misses much of the previous literature on semantics which is highly relevant.   I thank the authors for submitting this paper to ICLR. Please take the reviewers  comments, especially recommended references, to improve the paper for future submission.
This work proposed a nested evolutionary algorithm to choose image filters and filter parameters for back box attacks, with the emphasize of high transferability.   After reading the manuscript, the comments of reviewers and the authors  responses, I think the main issues of this work include:  1. The limited novelty of the main idea, since there have been many filter based attacks, and this work is very close to an existing work; 2. The solution is not new, since the evolutionary method is also well adopted in adversarial attacks;  3. Many many black box attack methods are not cited and compared, though the authors argued that their perturbation upper bound are different such that they cannot be compared, which is not convincing;  4. The claimed high transferability is not well explained, maybe due to the model ensemble (as indicated by reviewer eN8o). Besides, many existing works that studied transferability are not cited and compared.  5. Experiments are inadequate. The authors added some results in the revised version, but the current shape is still not ready for publication.   Thus, my recommendation is reject. Hope the reviews can help to improve this work in future.
Evolutionary strategies are a popular class of method for black box gradient free optimization and involve iteratively fitting a distribution from which to sample promising input candidates to evaluate.  CMA ES involves fitting a Gaussian distribution and has achieved state of the art performance on a variety of black box optimization benchmarks when the underlying function is cheap to evaluate.  In this work the authors replace this distribution instead with a much more flexible deep generative model (i.e. NICE). They demonstrate empirically that this method is effective on a number of synthetic global optimization benchmarks (e.g. Rosenbrock) and three direct policy search reinforcement learning problems.  The reviewers all believe the paper is above borderline for acceptance.  However, two of the reviewers said they were on the low end of their respective scores (i.e. one wanted to give a 5 instead of a 6 and another a 7 instead of 8.)  A major issue among the reviewers was the experiments, which they noted were simple and not very convincing (with one reviewer disagreeing).  The synthetic global optimization problems do seem somewhat simple.  In the RL problems, it s not obvious that the proposed method is statistically significantly better, i.e. the error bars are overlapping considerably.   Thus the recommendation is to reject.  Hopefully stronger experiments and incorporating the reviewer comments in the manuscript will make this a stronger paper for a future conference.
+ Interesting method for binaural synthesis from moving mono audio + Nice insight into why l2 isn t the best loss for binaural reconstructions.  + Interesting architectural choice with nice results. + Nicely motivated and clearly presented idea   especially after addressing the reviewers comments.  I agree with the idea of a title change. While I think its implied that the source is probably single source, making it explicit would make it clearer for those not working in a closely related topic. Hence, "Neural Synthesis of Binaural Speech from Mono Audio" as suggested in the review process sounds quite reasonable. 
This paper introduces a framework for specifying the model search space for exploring over the space of architectures and hyperparameters in deep learning models (often referred to as architecture search).  Optimizing over complex architectures is a challenging problem that has received significant attention as deep learning models become more exotic and complex.  This work helps to develop a methodology for describing and exploring the complex space of architectures, which is a challenging problem.  The authors demonstrate that their method helps to structure the search over hyperparameters using sequential model based optimization and Monte Carlo tree search.  The paper is well written and easy to follow.  However, the level of technical innovation is low and the experiments don t really demonstrate the merits of the method over existing strategies.  One reviewer took issue with the treatment of related work.  The underlying idea is compelling and addresses an open question that is of great interest currently.  However, without experiments demonstrating that this works better than, e.g., the specification in the hyperopt package, it is difficult to assess the contribution.  The authors must do a better job of placing this contributing in the context of existing literature and empirically demonstrate its advantages.  The presented experiments show that the method works in a limited setting and don t explore optimization over complex spaces (i.e. over architectures   e.g. number of layers, regularization for each layer, type of each layer, etc.).  There s nothing presented empirically that hasn t been possible with standard Bayesian optimization techniques.  This is a great start, but it needs more justification empirically (or theoretically).  Pros:   Addresses an important and pertinent problem   architecture search for deep learning   Provides an intuitive and interesting solution to specifying the architecture search problem   Well written and clear  Cons:   The empirical analysis does not demonstrate the advantages of this approach over existing literature   Needs to place itself better in the context of existing literature
This paper proposes a method for tracing activations in a capsule based network in order to obtain semantic segmentation from classification predictions.  Reviewers 1 and 2 rate the paper as marginally above threshold, while Reviewer 3 rates it as marginally below. Reviewer 3 particularly points to experimental validation as a major weakness, stating: "not sure if the method will generalize well beyond MNIST", "I’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only."  The AC shares these concerns and does not believe the current experimental validation is sufficient. MNIST is a toy dataset, and may have been appropriate for introducing capsules as a new concept, but it is simply not difficult enough to serve as a quantitative benchmark to distinguish capsule performance from U Net. U Net and Tr CapsNet appear to have similar performance on both MNIST and the hippocampus dataset; the relatively small advantage to Tr CapsNet is not convincing.  Furthermore, as Reviewer 1 suggests, it would seem appropriate to include experimental comparison to other capsule based segmentation approaches (e.g. LaLonde and Bagci, Capsules for Object Segmentation, 2018). This related work is mentioned, but not used as an experimental baseline. 
This paper presents a number of improvements on existing approaches to neural logic programming. The reviews are generally positive: two weak accepts, one weak reject. Reviewer 2 seems wholly in favour of acceptance at the end of discussion, and did not clarify why they were sticking to their score of weak accept. The main reason Reviewer   1 sticks to 6 rather than 8 is that the work extends existing work rather than offering a "fundamental contribution", but otherwise is very positive. I personally feel that a) most work extends existing work b) there is room in our conferences for such well executed extensions (standing on the shoulders of giants etc).  Reviewer 3 is somewhat unconvinced by the nature of the evaluation. While I understand their reservations, they state that they would not be offended by the paper being accepted in spite of their reservations.  Overall, I find that the review group leans more in favour of acceptance, and an happy to recommend acceptance for the paper as it makes progress in an interesting area at the intersection of differentiable programming and logic based programming.
This paper explores what might be characterized as an adaptive form of ZoneOut. With the improvements and clarifications added to the paper during the rebuttal the paper could be accepted. 
The paper presents an approach that supports better performance when out of distribution cases occur, by letting neurons be of only compact support and thus if the input is out of distribution (OOD).   Pros:   The proposed strategy is interesting and may be useful.  Cons:   The choice of the parameter alpha, whose value is crucial to the success in experiments, is left murky. The approach suggested by the authors was not validated experimentally.    There is insufficient comparison to recent works.
This paper presents the application of the hierarchical latent variable model, CW VAE which is originally developed in the vision community, to the speech domain with meaningful modifications, and provide empirical analysis of the likelihood as well as discussions on the likelihood metrics. The reviewers tend to agree that it is a promising direction to study hierarchically structured LVMs for speech, and the introduction/adaptation of CW VAE is useful. There were some discussion on the suitability of the likelihood evaluation, and it appears a fair comparison with wavenet shall take place at s 1 (single sample), a resolution level the proposed method does not yet scale up to. On the other hand, an important potential use case of the model is representation learning for speech, as it is a common belief that at suitable resolution the features shall discover units like phoneme. But I find the current evaluation of latent representations by LDA and KNN to be somewhat limited, and in fact there is no comparison with suitable baselines in Sec 3.2 in terms of feature quality. A task closer to modern speech recognition (e.g., with end to end models) would be preferred.
The paper proposes a method for solving challenging sparse reward problems by performing task reduction followed by self imitation learning from solution trajectories to the reduced tasks.  The core innovation seems to me to be the uses of the reduction search, which is essentially a form of recursive subgoal selection, but where the subgoals are sure to be achievable as assessed by leveraging the learned value function.  This idea seems rather general, though its use is strongly facilitated in this paper by definition of the space (i.e. that object target is the space, is pre specified; there is only one, rather limited result on a pixel based task).   Note: Another submission to this conference also explores a quite similar idea to the task reduction proposed in this paper   see "Divide and Conquer Monte Carlo Tree Search".  The breaking down of the problem into sub problems using the value function is similar, but the details of how the papers proceed from there is quite distinct.  This is a difficult meta review decision due to the fairly mixed reviews, coupled with limited engagement in the discussion phase.  Two reviewers felt the paper was solid and could be accepted (R1 and R2 with scores 7 and 6 respectively).  R3 gave a borderline review that leaned towards reject (score 5).  R3 replied to the initial author response, which provided helpful feedback to the authors. Ultimately, in my assessment, the authors did a fairly thorough job of addressing some of the points raised by R3, including by adding an additional comparison even where they didn t agree with the reviewer. R4 assigned the paper the lowest score of 3.  The authors provided a lengthy reply to this review asserting that the review may have reflected misunderstanding of paper details, but the reviewer did not respond to the authors.      Two core issues raised about this paper relate to the definition of the space for subgoals and the limited difficulty of the tasks. However, this method does not claim to be entirely ignorant of the task space so I don t see the fact that they do include some domain knowledge in designing the goal space to be totally undermining of the method.  They focus on the complementary issue of how to break down difficult problems into sub problems.  While it would be considerably more impressive if the goal space were learned, I think this harder version of the problem remains a fundamental and deep problem within AI, so it seems to me too much to ask of the present paper (especially given that it was not the stated focus of the paper).  And while the tasks explored in the paper are a little contrived (some repetitive motifs and designed with a relatively small search space over subtasks), these problems do have some complex structure.  Compared to many works in this field, I applaud the authors for engaging with problems with both long horizon task structure as well as complex high DoF continuous control component.  While I agree with some of the concerns raised, my overall assessment is that I find the contributions sufficiently innovative and substantial to justify acceptance.  The authors proposed a specific innovation and evaluated that innovation.  Insofar as their innovation is somewhat general, I don t think this paper can be the last word on how well it compares with the diverse approaches it could be set against.  And while the experiments are not definitive, I do think they do constitute a fairly ambitious initial validation of the core idea.   
This paper studies the relationship between adversarial transferability and knowledge transferability. It develops two metrics to measure adversarial transferability and a theoretical framework to justify the positive correlation between adversarial transferability and knowledge transferability. Synthetic experiments show that adversarial transferability measured by the proposed metrics indicates knowledge transferability.  While the paper studies an interesting and fundamental problem, with a sound theoretical analysis and a clear presentation, reviewers still have several reservations to directly accept it.   Lack of interpretation. How this observation can be used to gain better understanding of either fields of adversarial examples or knowledge transfer?   Lack of inspiration. How the insights can lead to better transfer techniques, apply to practical applications, and foster future research?   Lack of justification. Why such definitions of metrics are the intrinsic ways of measuring adversarial transferability? How well do they correlate with the practical experience with advanced attack, defense, and transfer methods?  AC believes the endeavor made by this paper towards a fundamental problem is highly necessary to our field. But given the above reservations, AC would encourage the authors to further strengthen their work to make it more inspiring and useful.
This paper proposes an interesting method for combining retrieval based models and graph neural networks for source code summarization. Finding new ways of bringing in additional context for graph based models is an important research direction in this space, and the paper presents a novel and effective approach. The initial submission was missing experiments on existing benchmarks, but new experiments presented in the discussion phase are enough to resolve that concern. Reviewers are unanimously in support of acceptance. 
There was a consensus among the reviewers to reject the paper. While they noted that the paper proposed a new interesting stochastic algorithm for deep learning, they think the paper needs to be substantially improved in both theory and empirical study. The paper was judged quite incremental in comparison to the work of Öztoprak et al 2018 (where most of the theory was developed), while not showing improved empirical performance on the benchmarks.
This paper presents a new formulation for the infinitely wide limiting case of deep networks as Gaussian processes, i.e. NNGPs.  The authors extend the existing case to incorporate a scale term at the penultimate layer of the network, which results in a scale mixture of NNGPs or a Student t process in a specific case.  This formulation allows for a more heavy tailed output distribution which e.g. can be more robust to outliers.  The four reviews averaged just above borderline, with a 5, 8, 6, 6.  The reviewers found the approach to be sensible, technically correct and timely given the recent literature.  They found the experiments to be compelling for the most part, demonstrating the added robustness of this approach over the baseline NNGP.  The main concern raised by the reviewers is that the work is incremental, given that both NNGPs and Student t processes are already established.
This paper presents a framework where GANs are used to improve detection of outliers (in this context, instances of the “background class”). This is a very interesting and, as demonstrated, promising idea. However, the general feeling of the reviewers is that more work is needed to make the technical and evaluations parts convincing. Suggestions for further work towards this direction include: theoretical analysis, better presentation of the manuscript and, most importantly, stronger experimental section. 
This paper presents a method for producing a mixture of (disjoint) predictive distributions for deep learning models rather than a single predictive distribution.  The reviewers in general found that the idea had strong potential, was well motivated and addresses an important and under appreciated problem in deep learning.  They seemed to find the proposed approach of using mixture density networks to be sensible.  However, the reviewers seemed to find that the paper was unclear in presentation and grammatically, as if hastily written.  One reviewer noted that they would not be able to reproduce the method given the confusing presentation.  The reviewers also found that the experiments didn t adequately evaluate their method empirically.  Unfortunately, the reviewers all agreed that the paper is not quite ready for publication (5, 3, 5).  Careful rewriting of the paper and the technical contributions and strengthening the experiments would go a long way towards improving this paper for a future submission.
All reviewers concur that the paper has promise, but fails to deliver on that promise.  The idea of learning potentials based on DNNs is appreciated, but the evaluation of the contribution is considered lacking by all reviewers.  In addition, reviewers note that the training is not differentiable, which the rebuttal acknowledges is future work.  I do not reject the paper simply for failing to beat a deep learning baseline, but for having chosen applications which do not even test the paper s hypotheses: reviewers note that the models are tree structured, so loopy BP is not tested, despite the revised paper s claim that "the inference strategy is compatible with graphs containing cycles".
This work proposes a shortest path constraint for the reinforcement learning algorithm to improve efficiency in sparse reward scenarios. The experiments are shown in navigation tasks in first person maze and grid world. Reviewers found the idea interesting and the paper well written but none of them championed the paper for clear acceptance. The authors provided a detailed thoughtful rebuttal. All the reviewers acknowledged the rebuttal followed by discussion. After considering rebuttal, review, and discussion, both AC and reviewers feel that experiments don t fully support and justify the algorithm. The main issue is that the results are shown only for the shortest pathfinding problems where the shortest path constraint is shown to be helpful. Hence, it is recommended to run it on diverse scenarios and standard benchmarks like the Atari games suite. Please refer to the reviews for final feedback and suggestions to strengthen the future submission.
The reviewers have ranked this paper as borderline accept. On the negative side, the main claim of the paper (the more categories for training a one shot detector, the better) has already been observed in several works and very intuitive. However, the paper has done significant experimental work to support this claim. The paper is very well written, it carefully explores the existing setups for one shot detection and highlights their weaknesses. The paper also gives advice on how to construct better datasets for one shot detection (the conclusion "add more diverse categories" is somewhat obvious but the paper demonstrates how important that is).
Nominally, the scores on this paper were pretty split.  In reality, I concur with the 2 and the 3.  The 6 acknowledges being unfamiliar w/ the GAN literature, and I think the 7 is being too permissive about the baselines.   The empirical evaluation here is simply not up to par for a major machine learning conference.  As reviewers have mentioned, the baselines are out of date, and even then the improvements are marginal.  It s totally fine to have a marginal improvement if the proposed technique is very new and interesting and the baselines  are taken seriously, but unfortunately I don t believe that s the case here. Thus, I recommend rejection.
The idea of extending deep nets to infinite dimensional inputs is interesting but, as the reviewers noted, the execution does not have the quality we can expect from an ICLR publication. I encourage the authors to consider the meaningful comments that were made and modify the paper accordingly.
This paper received borderline scores, which makes for a difficult recommendation. Unfortunately, two of the reviews were too short and thus were of limited use in forming a recommendation. That includes the high scoring one, which did not adequately substantiate its score.  There is much to admire in this submission. Reviewers appreciated the originality of this research, linking rate reduction optimization to deep network architectures: * R1: "The paper proposes a novel perspective" * R4: "The novelty of the paper is in that formulation of the feature optimisation is baked in into a deep architecture" * R5: " I think the construction seems interesting and the rate reduction metric seems like a reasonable thing to optimize. I found the relationship of coding rate maximization to ReduNet to be quite clever" * R3 (short): "The innovative method allows the inclusion of a new layer structure named ReduNet"  Reviewers also applauded the paper s clarity, including R4 who raised their score to 6 based on satisfying clarity revisions from the authors: * R1: "The writing is good and easy to follow" * R4 post discussion: "Clarity is not an issues anymore   additional explanations provided by the authors and one more careful reading of the paper helped in understanding of all the aspects of the model" * R2 (short): "The paper is well structured."  However, there were some core questions around how well the main significance claims of the paper are supported. The most in depth discussion on these topics is in the detailed thread with R5. In that thread there are many points discussed, but the two issues seem to be: 1. whether the connection between ReduNet and standard neural net architectures is sufficiently substantiated so as to constitute an explanation for behaviors of those standard architectures, like CNNs; and 2. whether the emergence of ReduNet s group invariance/equivariance is surprising or qualitatively new.  The first is much more central. On the first issue, R5 writes in summary: "Fundamentally I think the authors propose a hypothesis: that ReduNets explain DL models. However, the authors do not take meaningful steps towards validating this hypothesis. [...] I would contrast this with, for example, the scattering networks paper (https://arxiv.org/abs/1203.1513) which did an exceptional job of arguing for an ab initio explanation of convolutional networks."  I find R5 s perspective on this point to be compelling, in that the paper currently doesn t do enough to justify these main claims, either through drawing precise nontrivial mathematical connections or through experimental validation. (The thread has a much more detailed and nuanced discussion.)  The second issue is not quite as central to the significance of the paper, but it was noted by multiple reviewers: * R5: "I may be missing something, but given the construction of ReduNet, I feel as though the emergence of a convolutional structure subject to translation invariance is not terribly surprising." * R4: "Finally, I am not sure if the result of obtaining a convnet architecture in ReduNet when translation invariance constraint is added the embedding is all that surprising." * R4 post discussion: "Reading the exchange between the authors and R5 I am still not fully convinced that translation invariance property is all that surprising, but for me that s not a reason to reject."  At the least, the paper as written hasn t yet convinced some readers (myself included) on these claims.  As I mentioned at the start, this paper is borderline, but because I am largely aligned with R5 s perspectives, I think this paper does not quite pass the bar for acceptance. I recommend a rejection, but I look forward to seeing a strengthened version of this work in the future. I hope the feedback here has been useful to bringing about that stronger version.
The reviewers agree that the problem being studied is important and relevant but express serious concerns. I recommend the authors to carefully go through the reviews and significantly scale up their experiments.
This paper considers graph neural representations that use Cayley polynomials of the graph Laplacian as generators. These polynomials offer better frequency localization than Chebyshev polynomials. The authors illustrate the advantages of Cayleynets on several benchmarks, producing modest improvements.  Reviewers were mixed in the assessment of this work, highlighting on the one hand the good quality of the presentation and the theoretical background, but on the other hand skeptical about the experimental section significance. In particular, some concerns were centered about the analysis of complexity of Cayley versus the existing alternatives.  Overall, the AC believes this paper is perhaps more suited to an audience more savvy in signal processing than ICLR, which may fail to appreciate the contributions. 
New effective kernel learning methods are very well aligned with ICLR s focus on Representation Learning. As a reviewer pointed out, not all aspects of the paper are algorithmically "clean". However, the proposed approach is natural and appears to give consistent improvements over a couple of expected baselines. The paper could be strengthened with more comparisons against other kernel learning methods, but acceptance at ICLR 2018 will increase the diversity of the conversation around advances in Representation Learning.
All in all, while the reviewers found that the problem at hand is interesting to study, the submission s contributions in terms of significance/novelty did not rise to the standards for acceptance. The reasoning is most succinctly discussed by R3 who argues that IFS and EFS are basically feature selection and applying them to feature attribution is not particularly novel from a methodological point of view. 
The paper proposes an approach for unsupervised learning of keypoint landmarks from images and videos by decomposing them into the foreground and static background. The technical approach builds upon related prior works such as Lorenz et al. 2019 and Jakab et al. 2018 by extending them with foreground/background separation. The proposed method works well for static background achieving strong pose prediction results. The weaknesses of the paper are that (1) the proposed method is a fairly reasonable but incremental extension of existing techniques; (2) it relies on a strong assumption on the property of static backgrounds; (3) video prediction results are of limited significance and scope. In particular, the proposed method may work for simple data like KTH but is very limited for modeling videos as it is not well suited to handle moving backgrounds, interactions between objects (e.g., robot arm in the foreground and objects in the background), and stochasticity. 
The paper develops linear over parameterization methods to improve training of small neural network models. This is compared to training from scratch and other knowledge distillation methods.   Reviewer 1 found the paper to be clear with good analysis, and raised concerns on generality and extensiveness of experimental work. Reviewer 2 raised concerns about the correctness of the approach and laid out several other possibilities. The authors conducted several other experiments and responded to all the feedback from the reviewers, although there was no final consensus on the scores.  The review process has made this a better paper and it is of interest to the community. The paper demonstrates all the features of a good paper, but due to a large number of strong papers, was not accepted at this time.
although the authors argue that their experiments were selected from the earlier work from which major comparing approaches were taken, the reviewers found the empirical result to be weak. why not some real tasks (i do not believe bAbI nor PTB could be considered real) that could clearly reveal the superiority of the proposed unit against existing ones?
The paper makes an attempt towards byzantine resilient federated learning, in the pressneece of backdoor attacks.   The method presented combines a clustering step with a poison elimination step, and seems to be effective against a range of current attacks.   Both steps are a bit ad hoc in nature, and do not come with provable guarantees.  Moreover, the algorithms presented will have a big negative impact on personalization as several models may be incorrectly discarded during and FL round.  The authors further point in their response that " no existing defense against backdoor attacks preserves the privacy of the clients’ data." This is in fact not true, as the differential privacy defense presented by the "Can you really backdoor FL" paper is in fact fully respective of user privacy.  At the same time, the work on backdoor attacks and defenses is reminiscent of the "cat and mouse" work in adversarial examples: an attack comes out, then a defense claims to protect against it, then an attack that incorporates that defense can be made stronger, and so on. This is similar in the context of backdoor attacks.  In fact, a recent work [1] proposes that detecting backdoors is in the general computationally unlikely, rendering the generality of the proposed algorithm questionable, and also suggest a set of attacks that seem very hard to defend against. (it is fine that the authors do not reference this work as it was published just recently)  As the paper lacks significant algorithmic novelty, solid guarantees, and also is unclear whether it is universally sound, the overall contribution is limited.   [1] Wang et al. Attack of the tails: Yes, you really can backdoor federated learning, neurips 2020 https://papers.nips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb Paper.pdf 
This paper present a model for reconstructing images from fMRI recordings, based on an encoder and decoder used in a loop. The reviewers were unanimous in their opinion that this paper is not ready for publication at this stage. They raised concerns ranging from the quality of the result and how to compare them to previous methods, to the justification behind different modeling choices. The authors were gracious in their responses to the reviewers. I do not recommend acceptance at this stage,
Symmetries play an important role in physics, and more and more papers show that they also play an important role in statistical machine learning. In particular, employing symmetries might be the key to improve training and predictive performance of machine learning models.  In this context, the present paper shows how previous physical knowledge can be leveraged to improve neural network performance, in particular within Deep dynamic models. To this end, they show how to incorporate equivariance into resnets and u nets for dynamical systems. On a technical level, as pointed out by the reviews and also clearly mentioned by the authors, the basic building blocks are well known in the literature. However, dynamical systems also raises their own challenges resp. laws when it comes to modelling symmetries, as the authors argue in the paper and also clarified in the rebuttal. For instance, it pays off to adapt the techniques known from the literature deal better with scale, magnitude and uniform motion equivariance. This is a solid contributions and will help many other who want to apply DNNs to dynamic and physical models. 
The paper uses a multimodal prior in GANs and reconstructs the latents back from images in two stages to match the generated data modes to the latent space modes. It is empirically shown that this can prevent mode collapse to some extent (including intra class collapse). However the paper lacks a comparison with state of the art GANs that have been shown to get better FID scores (~21 for SN GAN [1] vs ~28 in the paper) so the benefit here is unclear, particularly in cases when the mode prior is unknown. Similarly for other applications used in the paper such as inference and attribute discovery, it falls short of demonstrating quantitative improvements with the approach. For example, there is a growing body of work on unsupervised disentanglement in generative models with several metrics to measure it, which could be used to evaluate the attribute discovery performance. R1 has brought up the point of lack of comparisons which the AC agrees with. Authors have made revisions in the paper including some comparisons but these feel insufficient to establish the benefits of the method over state of the art in preventing mode collapse.   A borderline paper as reflected in the reviewer scores but can be made stronger with experiments showing convincing improvements over state of the art in at least one of the applications considered in the paper.    [1] Miyato, T., Kataoka, T., Koyama, M., & Yoshida, Y. (2018). Spectral normalization for generative adversarial networks. ArXiv Preprint ArXiv:1802.05957.
The paper is proposing Risk Extrapolation (REX) as a domain generalization algorithm. Authors extends the distributionally robust learning to affine mixture of distributions from convex mixture. Authors later uses variances instead of this extension and demonstrate various empirical and theoretical properties. The paper is reviewed by four expert reviewers and the reviewers did not reach to a consensus. Hence, I also read the paper in detailed and reviewed it. In summary, reviewers argue the following:    R#2: Main argument is the lack of justification of the claim "Rex could deal with both covariate and concept shift together". Authors try to address this in their response. Moreover, reviewer also argues in the private discussion that manuscript is not updated and authors did not address any of the issues during the discussion period.   R#3: Argues that (similar to R#2), dealing with covariate shift is not explained properly. Reviewer is not persuaded that REX results in invariant prediction.   R#1 and R#4: Largely positive about the paper. In the mean time, argue that organization of the paper is lacking and some of the material in the supplement is relevant and should be moved to the main text. R#1 decreases their score due to the lack of re organization during the discussion.  The value of the paper is clear to me, the joint treatment of minimax perspective, domain generalization and invariances is definitely interesting and valuable. Hence, the paper has merit to be published. However, the presentation is lacking significantly.  The main contribution of the paper lies in Table 1 but the invariant prediction property is not justified at all in the main text. Hence, Table 1 is not justified properly. Authors discuss Thm 1&2 in their response but they both are in the supplement. From reading only the main text, confusion of the reviewers are well justified. ICLR guidelines clearly states that "...Note that reviewers are encouraged, but not required to review supplementary material during the review process..." It is authors  responsibility to make the main paper self contained. Even more worrisome is the fact that authors dismiss this concern in their response to R#1 which eventually leads to R#1 decreasing their score. Hence, I decided to reject the paper since the presentation is subpar and authors did not persuaded reviewers that they can fix this presentation issue by the camera ready deadline. On the other hand, I think the paper can be really influential if it was written clearly. I suggest authors to revise the claims more precisely, extended the discussion on the claims and move the theorems to the main paper.
The authors have proposed a new consistency loss for improving model robustness to common corruptions. With a student teacher training setup, only the student network uses batch normalization at training time. Improvements are shown on small scale corruption datasets (CIFAR C), a single domain generalization dataset (VLCS), and RobustPointSet.  Though, positive feedback were given on the quality of the story telling, and on an interesting motivation by a few toy examples, some concerns remained among the reviewers. In particular applicability of the method as model and data sizes increases, e.g., on ImageNet C, was questioned. After Additional results were provided by the authors, the method seems to break as scales increases. The way relevant baselines from previous work was also judged light and should be improved. Hence, the paper could be improved to include more comparisons and more convincingly showing advantages of the method.
This paper considers an important problem, graph partitioning, from a transductive viewpoint: assuming that the graphs are generated by independent draws from an unknown distribution, learn some parameters in an ``offline” phase, and use these in the ``online” phase (much as in PAC learning). The authors have also answered many of the reviewer questions. In particular, the comparison with existing work is substantial.   While I laud the positives of this work and the importance of the transductive approach, I see an issue: as a reviewer points out and as agreed by the authors, the paper does not provide a theoretical guarantee of the quality of the generalization to unseen graphs. It would have been useful, e.g., to consider this on Erdos Renyi G(n,p) models, stochastic block models etc.
Reviewers raised several valid concerns about novelty of quantization idea and lack of discussions related to prior art (AISTATS 2020 paper). The rebuttal did not convince the reviewers to raise their score. We hope the authors will benefit from the feedback and improve the paper for future submission.
The author s propose to use swish and show that it performs significantly better than Relus on sota vision models. Reviewers and anonymous ones counter that PRelus should be doing quite well too. Unfortunately, the paper falls in the category where it is hard to prove the utility of the method through one paper alone, and broader consensus relies on reproduction by the community. As a results, I m going to recommend publishing to a workshop for now.
This paper seemingly joins a cohort of ICLR submissions which attempt to port mature concepts from physics to machine learning, make a complex and non trivial theoretical contribution, and fall short on the empirical front. The one aspect that sets this apart from its peers is that the reviewers agree that the theoretical contribution of this work is clear, interesting, and highly non trivial. While the experiment sections (MNIST!) is indubitably weak, when treating this as a primarily theoretical contribution, the reviewers (in particular 6 and 3) are happy to suggest that the paper is worth reading. Taking this into account, and discounting somewhat the short (and, by their own admission, uncertain) assessment of reviewer 5, I am leaning  towards pushing for the acceptance of this paper. At very least, it would be a shame not to accept it to the workshop track, as this is by far the strongest paper of this type submitted to this conference.
 Pros:   Great work on getting rid of the need for QP and the corresponding proof of the update rule   Mostly clear writing   Good experimental results on relevant datasets   Introduction of a more reasonable evaluation methodology for continual learning  Cons:   The model is arguably a little incremental over GEM.  In the end I think all the reviewers agree though that the practical value of a considerably more efficient and easy to implement approach largely outweighs this concern.  I think this is a good contribution in this area and I recommend acceptance.
All reviewers agree that the presented audio data augmentation is very interesting, well presented, and clearly advancing the state of the art in the field. The authors’ rebuttal clarified the remaining questions by the reviewers. All reviewers recommend strong acceptance (oral presentation) at ICLR. I would like to recommend this paper for oral presentation due to a number of reasons including the importance of the problem addressed (data augmentation is the only way forward in cases where we do not have enough of training data), the novelty and innovativeness of the model, and the clarity of the paper. The work will be of interest to the widest audience beyond ICLR.
The paper addresses the problem of uncertainty quantification in deep neural nets. The authors introduces the CaPE calibration loss to deal with the inherent uncertainty in probabilistic prediction, e.g. medical prognosis, weather prediction or collision prediction.  The paper initially received contrasted reviews: two weak acceptance, one weak rejection, and one strong rejection recommendation. The main limitation pointed out by reviewers related to the unclear definition of the problem setting, the limited contributions, and clarifications on experiments (comparison with deep ensembles). After authors  feedback, the reviewers were not convinced by the clarification on the problem setting, and there was a consensus among reviewers to reject the paper.   The AC s own readings confirmed the concerns raised by the reviewers, and also identifies additional shortcomings of the current submission. The paper addresses the problem of proper quantification of data uncertainty (generally referred as aleatoric uncertainty), and the CaPE calibration loss should be positioned with respect to the literature on the topic. The AC thus recommends rejection, but encourages the authors to re submit their work after specifying the focus and motivation of their work.
Strengths: * Strong empirical study across multiple datasets. However, the gains are not as impressive as for other pretraining domains, such as text or images. * Interesting formulation of pseudo homophily as an objective to optimize in the self supervision stage * Well written paper  Weaknesses: * Novelty may be limited by the fact that the method is essentially learning (or searching) for a weighted average of self supervised training objectives * In that case, while the pseudo homophily angle is interesting, there may be other appropriate baselines for yielding this weighted combination of tasks that are not explored * There is concern about the degree of empirical improvements on certain datasets
Although all reviewers agree that the work is interesting and has potential, several issues in the presentation and the experimental section (especially regarding the ablation) need to be worked on before granting acceptance to the paper. 
The paper proposes a neurally inspired model that is a variant of conv LSTM called V1net. The reviewers had trouble gleaning the main contributions of the work. Given that it is hard to obtain state of art results in neurally inspired architectures, the bar is much higher to demonstrate that there is value in pursuing these architectures. There are not enough convincing results in the paper to show this. I recommend rejection.
This paper studies mixed precision quantization in deep networks where each layer can be either binarized or ternarized. The proposed regularization method is simple and straightforward. However, many details and equations are not stated clearly. Experiments are performed on small scale image classification data sets. It will also be more convincing to try larger networks or data sets. More importantly, many recent methods that can train mixed precision networks are not cited nor compared. Figures 3 and 4 are difficult to interpret, and sensitivity on the new hyper parameters should be studied. The use of "best validation accuracy" as performance metric may not be fair. Finally, writing can be improved. Overall, the proposed idea might have merit, but does not seem to have been developed enough.
The method presented, the simplified action decoder, is a clever way of addressing the influence of exploratory actions in multi agent RL. It s shown to enable state of the art performance in Hanabi, an interesting and relatively novel cooperative AI challenge. It seems, however, that the method has wider applicability than that.  All reviewers agree that this is good and interesting work. Reviewer 2 had some issues with the presentation of the results and certain assumptions, but the authors responded so as to alleviate any concerns.  This paper should definitely be accepted, if possible as oral.       
The reviewer scores are fairly close, and the comments in their reviews are likewise similar.  All reviewers indicate that they find this to be an interesting learning domain.  However, they also agree in assessing the proposed method as having limited novelty and significance.  They also critiqued the empirical evaluation as being too specific to Starcraft and not comprehensive, without providing evidence that the defogger contributes to winning at StarCraft.  The authors wrote a substantial rebuttal to the reviews, but it did not convince anyone to increase their scores.
In contrast to many current hierarchical reinforcement learning approaches, the authors present a decentralized method that learns low level policies that decide for themselves whether to act in the current state, rather than having a centralized higher level meta policy that chooses between low level policies.  The reviewers primarily had minor concerns about clarity, reward scaling, and several other issues that were clarified by the authors.  The only outstanding concern is that of whether transfer/pretraining is required for the experiments to work or not.  While this is an interesting question that I would encourage authors to address as much as possible, it does not seem like a dealbreaker in light of the reviewers  agreement on the core contribution.  Thus, I recommend this paper for acceptance.
This paper proposes an ensemble method to identify noisy labels in the training data of supervised learning.  The underlying hypothesis is that examples with label noise require memorization.  The paper proposes methods to identify and remove bad training examples by retaining only the training data that maintains low losses after perturbations to the model parameters.  This idea is developed in several candidate ensemble algorithms.  One of the proposed ensemble methods exceeds the performance of state of the art methods on MNIST, CIFAR 10 and CIFAR 100.  The reviewers found several strengths and a few weaknesses in the paper.  The paper was well motivated and clear.  The proposed solution was novel and plausible.  The experiments were comprehensive.  The reviewers identified several parts of the paper that could be more clear or where more detail could be provided, including a complexity analysis and  extended experiments.  The author response addressed the reviewer questions directly and also in a revised document.  In the discussion phase, the reviewers were largely satisfied that their concerns were addressed.  This paper should be accepted for publication as the paper presents a clear problem and solution method along with convincing evidence of method s merits. 
This paper presents an adaptive computation time method for reducing the average case inference time of a transformer sequence to sequence model.   The reviewers reached a rough consensus: This paper makes a proposes a novel method for an important problem, and offers reasonably compelling evidence for that method. However, the experiments aren t *quite* sufficient to isolate the cause of the observed improvements, and the discussion of related work could be clearer.  I acknowledge that this paper is borderline (and thank R3 for an extremely thorough discussion, both in public and privately), but I lean toward acceptance: The paper doesn t have any fatal flaws, and it brings some fresh ideas to an area where further work would be valuable.
Somewhat borderline paper given the scores, but leaning on the side of accepting mostly because the positive (and weak positive) reviews are a little more persuasive. The negative review is a bit of an outlier; the main issues raised in the negative review are that the novelty is on the lower side or otherwise that the work is incremental. These complaints are largely not shared by the other reviewers, and furthermore seem not like deal breakers. Still a borderline paper, but fairly safe to accept.
This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets.  The strengths of the paper are as follows: + In terms of significance of the topic, the problem tackled in the paper is significant and highly relevant.  + The motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis.  + Algorithmically, the paper proposes Adaptive Sampling and K consensus algorithms to reduce the computational cost, making the method more practical. + Experimentally, the paper exhibits competitive results against several frameworks for training smooth classifiers and on several datasets.
This paper analyzes deep networks optimized using non convex noisy gradient descent. The main result shows that in a teacher student setting, the excess risk converges in a fast rate and is stronger than any linear estimators (which include kernel methods). The paper also gives a convergence rate result that depends on some spectral gaps (which can be very small) but not on dimension. Overall the paper is interesting. It should probably emphasize that the dependency on spectral gaps (and the fact that they could be exponentially small) on the convergence as the current abstract suggests efficient convergence.
This paper finally received divergent and borderline reviews with one positive (6) and two negative (5) rates. Based on the reviews, authors’ responses and updated manuscript, we would like to decide to reject this work at this time even though this submission has a lot of potentials such as simplicity and efficiency. Positively, all the reviews agree that the proposed approach is simple but effective to improve the robustness of few shot classifiers. However, there is some room for improvement to be a stronger submission: (i) the technical novelty may need to be better presented, and (ii) the improved performance may need to be better justified (e.g., the effect of the pretrained stage).
This paper proposes a new and general formulation for supernet, which encodes supernet with tensor network(TN). The idea is interesting and motivated.  However, the paper is well presented and the clarify needs to be further improved.  The effectiveness of algorithm is not well justified and experimental results are less convincing even after additional results provided in the revision. Most importantly, it is not clear that the  TENSORIZING  method can solve the current NAS s ineffectiveness problem.  It is confirmed that the reference to ICLR 2021 paper is not used for the decision of paper. 
This paper was assessed by three reviewers who scored it as 6/1/6. The main criticism included somewhat weak experiments due to the manual tuning of bandwidth, the use of old (and perhaps mostly solved/not challenging) datasets such as Mnist and Cifar10, lack of ablation studies. The other issue voiced in the review is that the proposed method is very close to a MMD GAN with a kernel plus random features. Taking into account all positives and negatives, we regret to conclude that this submission falls short of the quality required by ICLR2020, thus it cannot be accepted at this time.  
This work presents a learnable activation function based on adaptive piecewise linear (APL) units. Specifically, it extends APL to the symmetric form. The authors argue that S APL activations can lead networks that are more robust to adversarial attacks. They present an empirical evaluation to prove the latter claim. However, the significance of these empirical results were not clear due to non standard threat models used in black box setting and the weak attacks used in open box setting. The authors revised the submission and addressed some of the concerns the reviewers had. This effort was greatly appreciated by the reviewers. However, the issues related to the significance of robustness results remained unclear even after the revision. In particular, as pointed by R4, some of the revisions seem to be incomplete (Table 4). Also, the concern R4 had initially raised about non standard black box attacks was not addressed. Finally, some experimental details are still missing. While the revision indeed a great step, the adversarial experiments more clear and use more standard setup be convincing. 
The reviewers all appreciate the idea, and the competitive performance, however the consensus is that this is a simple extension of the work of Han et al. and therefore the current submission contains little novelty. There are also numerous issues regarding clarity that the reviewers have pointed out. It is unfortunate that the authors have not engaged in discussion with the reviewers to resolve these, however they are encouraged to consider the reviewer feedback in order to improve the paper.
This paper proposes a new differentiable physics benchmark for soft body manipulation. The proposed benchmark is based on the  DiffTaichi system. Several existing reinforcement learning algorithms are evaluated on this benchmark. The paper identify a set of key challenges that are posed by this specific benchmark to RL algorithms. Short horizon tasks are shown to be feasible by optimizing the physics parameters via gradient descent. The reviewers agree that this paper is very well written, the  problem tackled in it is quite interesting and challenging, and the use of differentiable physics in RL for manipulating soft objects quite intriguing.
The paper addresses the problem of improving generalization when few annotated data is available by leveraging available auxiliary information. The authors consider the respective merits of two alternatives: using auxiliary information as complementary inputs or as additional outputs in a multi task or transfer setting.  For linear regression, they show theoretically that the former can help improve in distribution error but may hurt OOD error, while the latter may help improve OOD error. They propose a framework for combining the two alternatives and show empirically that it does so on three different datasets.    All the reviewers agree on the novelty, interest and impact of the method. The rebuttal clarified the reviewers’ questions. I propose an accept. 
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with reviewers that the paper has limited novelty and could be more clear about mix precision training. Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.  
This paper proposes a pre training technique for improving the logical abilities of pre trained language models. Reviewers point to many issues with clarity and experimental evaluation. No response was given by authors.
This paper seeks to find an answer to some quite interesting research question: can deep vanilla networks without skip connections or normalization layers be trained as fast and accurately as ResNets? In this regard, the authors extend Deep Kernel Shaping and show that a vanilla network with leaky RELU family activations can match the performance of a deep residual network.  Four reviewers unanimously suggested acceptance of the paper. There were concerns about the clarity or marginal performance improvement. However, they all including myself agree: achieving the competitive performance with the vanilla deep model itself can be seen as a big contribution and the clarity has been improved to some extent through revision.
Main content: Paper proposes a fast network adaptation (FNA) method, which takes a pre trained image classification network, and produces a network for the task of object detection/semantic segmentation  Summary of discussion: reviewer1: interesting paper with good results, specifically without the need to do pre training on Imagenet. Cons are better comparisons to existing methods and run on more datasets.  reviewer2:  interesting idea on adapting source network network via parameter re mapping that offers good results in both performance and training time. reviewer3: novel method overall, though some concerns on the concrete parameter remapping scheme. Results are impressive Recommendation: Interesting idea and good results. Paper could be improved with better comparison to existing techniques. Overall recommend weak accept.
The paper analyzes a variant of the Q Learning algorithm with two modifications: Online Target Learning (OTL), and Reverse Experience Replay (REP). OTL is essentially the same as using the target network. REP is a new modification of ER, which instead of randomly selecting samples from the buffer, replays them in the reverse order.  Most reviewers are positive about this paper, so I am going to recommend acceptance. There are, however, several concerns that have been raised by the reviewers. As the authors have not revised the paper during the discussion period, my acceptance recommendation is under the good faith expectation that the authors make a serious effort in improving their work based on the reviews. Some of the concerns are:    The intuition of why REP breaks the correlation is not clear enough. This has been brought up several times by the reviewers.   What are the technical differences in the analysis compared to previous work such as Zou et al., 2019?   The kappa appearing in Assumption 4, and showing up in the error bounds, can be dimension dependent. Please clarify this and its effect on the results.   Much of the paper is in the appendix. It helps if the authors can include more about the proof technique in the main body of the paper.   Describe the relation between the error in the value function vs. the performance of its greedy policy.
The paper tackles the problem of covariate shift in adaptive curriculum learning.  Unfortunately, the paper lacks clarity and the experiments are insufficient.  The author response clarified the notation and corrected many typos, however, the paper remains conceptually unclear as pointed out by the reviewers.  Hence this work is not ready for publication.
The authors analyze linear regression with gaussian covariates in an asymptoptic setting, where the number of examples and the number of covariates go to infinity together.  They identify conditions on the covariance under which "multiple descent" occurs, and conditions under which a regularization removes this effect.  Concerns were raised that the overlap between this paper and previous research was too substantial for it to be published in ICLR.  These persisted after the authors  response and the discussion period.
The paper studies network architecture search in the context of reinforcement learning. In particular it applies the DARTS method to the Procgen RL benchmark, and conducts extensive experimental evaluations. It identifies a number of issues that could potentially prevent DARTS from working well in the RL setting (such as nonstationarity and high variance), but in the end shows good performance without needing to modify DARTS substantially.  The reviewers agreed that a key strength of the paper is in its experiments. But they also identified a weakness in novelty: if a paper s main contribution is to combine two previously well explored ideas (in this case, RL and DARTS) then there is a high bar for the quality of exposition and positioning, and the reviewers did not feel that this bar was met. (Though the authors  updates during the rebuttal period did help substantially with clarity and relationship to other methods   thank you for these!)  Recommended decision: while the paper makes a worthwhile contribution, it does not in its current form rise to the level of novelty and general interest that is needed for publication in ICLR.
This paper presents a promising model to avoid catastrophic forgetting in continual learning. The model consists of a) a data generator to be used at training time to replay past examples (and removes the need for storage of data or labels), b) a dynamic parameter generator that given a test input produces the parameters of a classification model, and c) a solver (the actual classifier). The advantages of such combination is that no parameter increase or network expansion is needed to learn a new task, and no previous data needs to be stored for memory replay.  There is reviewer disagreement on this paper. AC can confirm that all three reviewers have read the author responses and have significantly contributed to the revision of the manuscript.  All three reviewers and AC note the following potential weaknesses: (1) presentation clarity needed substantial improvement. Notably, the authors revised the paper several times while incorporating the reviewers suggestions regarding presentation clarity. R2 has raised the final rating from 4 to 5 while retaining doubts about clarity.  (2) weak empirical evidence: evaluation with more than three tasks and using more recent/stronger baseline methods would substantially strengthen the evaluation (R2, R3). AC would like to report the authors added an experiment with five tasks and provided a verbal comparison with "Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence", ECCV 2018 by reporting the authors results on the MNIST dataset.  (3) as noted by R2, an ablation study of different model components could strengthen the evaluation. The authors included such ablation study in Table 4 of the revised paper.  (4) reproducibility of the model could be difficult (R1). In their response, the authors promised to make the code publicly available.  AC can confirm that all three reviewers have contributed to the final discussion. Given the effort of the reviewers and authors in revising this work and its potential novelty, the AC decided that the paper could be accepted, but the authors are strongly urged to further improve presentation clarity in the final revision if possible. 
This paper was controversial amongst the reviewers. There is clear utility to the ICLR community: a new model of grid cells based on well known technique (SR) used frequently in ML; good science careful analysis showing the proposed model exhibits key properties and useful in synthetic navigation domains;  such work reminds of the important concerns in natural learning systems which is relevant to those that wish to simulate and build intelligence. Two of the reviewers with subject matter experience in the area advocated for acceptance.  On the other hand, many readers of ICLR may find the paper confusing and unsatisfying as some of the reviewers did. The empirical work was limited to small domains and mostly in the form of demonstrations a typically ICLR reader would expect a performance improvement claim or a scientific hypothesis tested by each experiment. Presented as a new algorithm for ML the paper might appear too limited and simple (e.g., relying on state aggregation). The reviewers with neuro background found the paper clear and well organized, while the ML reviewers found it confusing. The relevance of the work will be limited to a smaller subset of researchers but this is true of many ML works also. Finally, ML readers might be more familar with neuro work which propose computational models and then validate those models against real neural activity data from brains. This is work is not like that, rather using synthetic data to demonstrate important properties and explore empirical conjectures about the model.  In the end the paper is boarder line: the subject matter experts both listed issues that should be addressed (e.g., band cells issue), while the reaction of the ML reviewers suggests the impact of the work might be reduced at ICLR (compared to other venues). Additional text clearly articulating the scope and managing reader expectation could mitigate this concern, but it s not a small task to change the tone and pitch this way. Scientific conferences are about insights and understanding, this paper provides both. Please consider the suggested edits to maximize the impact of your work at ICLR this year.     
This paper proposes to use the grey level co occurrence matrix method (GLCM) for both the performance evaluation metric and an auxiliary loss function for single image super resolution. Experiments are conducted on X ray images of rock samples. Three reviewers provide comments. Two reviewers rated reject while one rated weak reject. The major concerns include the lack of clear and detailed description, low novelty, limited experiment on only one database, unconvincing improvement over the prior work, etc. The authors agree that the limited experiment on one database does not demonstrate the generalization capability of the proposed method. The AC agrees with the reviewers’ comments, and recommend rejection.
A defense against of adversarial attacks is presented, which builds mostly on combining known methods in a novel way. While the novelty is somewhat limited, this would be fine if the results were unequivocally good and other parts of the problematic. However, reviewers were not entirely convinced by the results, and had a number of minor complaints with various parts of the paper.  In sum, this paper is not currently at a stage where it can be accepted.
Reviewers agreed that overall the two pronged message of the submission has utility.  1. That ObjectNet is continues to be difficult for models to understand and is a challenging test platform even when objects are isolated from their backgrounds. This is significant and not obvious. Cropping objects makes the distribution shift between ObjectNet and ImageNet far smaller, but the large remaining performance gap points to the fact that detectors are limited by their ability to recognize the foregrounds of objects not by their ability to isolate objects from their backgrounds.  2. That segmentation could be a promising direction for robustness to adversarial perturbations which has so far been overlooked.
In this work, the authors address a multi task learning setting and propose to enhance the estimation of task dependency with an attention mechanism capturing sample dependant measure of task relatedness. All reviewers and AC agree that the current manuscript lacks clarity and convincing empirical evaluations that clearly show the benefits of the proposed approach w.r.t. state of the art methods. Specifically, the reviewers raised several important concerns that were viewed by AC as critical issues: (1) the empirical evaluations need to be significantly strengthened to show the benefits of the proposed methods over SOTA   see R2’s request to empirically compare with the related recent work [Taskonomy, 2018] and R4’s request to compare with the work [End to end multi task learning with attention, 2018]. R4 also suggested to include an ablation study to assess the benefits of the attention mechanism. Pleased to report that the authors addressed the ablation study in their rebuttal and confirmed that the proposed attention mechanism plays an important role in the performance of the proposed method.  (2) All reviewers see an issue with the presentation clarity of the conceptual and technical contributions    see R4’s and R2’s detailed comments and questions regarding technical contributions; see R3’s and R4’s comments that the distinction between the general task dependency and the data driven dependency is either not significant or is not clearly articulated; finding better examples to illustrate the difference (instead of reiterating the current ones) would strengthen the clarity and conceptual contributions.   A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs more clarifications, empirical studies and polish to achieve the desired goal.  
The paper provides a method to train boosted decision trees to satisfy individual fairness. All of the reviews suggest that this paper is well written and gives novel techniques for solving an interesting problem. The authors have addressed most of the concerns raised by the reviewers during their response. However, the authors should follow a suggestion in the reviews and include the running time in the empirical evaluation.
This paper proposes a regularization term that enforces the orthogonality between (i) a residual between the observed outcome and its estimator and (ii) the treatment and propensity score. The method empirically performs competitively. However, there seems to exist a gap between the proposed method and the assumptions made to provide theoretical guarantees (e.g., R3, R2). R4 was also concerned about the issue and adjusted his/her score accordingly. Even though the authors provide a detailed discussion on most of the reviewers  concerns, some of the problems remain unresolved. Further, unlike other papers submitted to ICLR, the authors did not actually update the paper such that we could check whether the revisions were adequately made. As such, I believe this paper is not quite ready for publication in its current form. 
The paper uses a transformer model to generate CNN models and use it for few shot learning.  Although the reviewers appreciate the ideas and the good benchmarking results presented in the paper they are find the paper somewhat incremental compared to previous work in the hyper network literature. This also despite the authors thorough rebuttal with additional results. This shows that the authors could have done a better job in presenting their work.  Rejection is therefore recommended with a strong encouragement to rework the paper to counter future reviewers having similar reservations.
The paper presents an interesting idea, but there are significant concerns about the presentation issues and experimental results (e.g., comparisons with baselines). Overall, it is not ready for publication. 
This paper tackles an interesting problem: distribution shift generalization often requires parameter identification but this is not possible for over parameterized neural networks. This paper shows for quadratic neural networks, it is possible to identify the function without identifying the parameter.   This is an interesting result. However, reviewers raise concerns about the assumption and technical details. The meta reviewer agrees with these concerns.
The authors consider the use of MAML with model based RL and applied this to robotics tasks with very encouraging results. There was definite interest in the paper, but also some concerns over how the results were situated, particularly with respect to the related research in the robotics community. The authors are strongly encouraged to carefully consider this feedback, as they have been doing in their responses, and address this as well as possible in the final version. 
The paper proposes a model for heart rate estimation from video. Unlike previous approaches, the method does not perform pre processing of the video (face detection, cropping, etc). The empirical results are good   on par with state of the art or sometimes better.  Most reviewers are negative even after considering the authors  responses, but there is no full consensus. Some mentioned pros of the paper are that it s practically useful to have a model without pre processing steps and that overall the technical side of the paper seems sound. Some cons are that the technical novelty is limited, the empirical evaluation is somewhat limited, and appropriate ablation studies are missing.  Overall, I recommend rejection at this point. While the paper goes in a promising direction, it is an application paper and as such it would be expected to have a more solid experimental evaluation   ideally on multiple tasks (as the paper title might suggest) and on more datasets as mentioned by the reviewers and the commenters. Moreover, an ablation study (and/or other analysis) of the proposed model would be crucial to let the readers know what components of the model actually contribute to its performance.
 Though the reviewers thought the ideas in this paper were interesting, they questioned the importance and magnitude of the contribution.  Though it is important to share empirical results, the reviewers were not sure that there was enough for this paper to be accepted.
This paper proposes a hardware aware pruning method which structurally prunes the given deep neural networks to retain their accuracy while satisfying the latency constraints. Specifically, the authors formulate the latency constrained pruning problem as a combinatorial optimization problem to find the optimal combination of neurons to maximize the sum of the importance scores, and propose an augmented knapsack solver to solve it, as well as a neuron grouping technique to speed up the training. The proposed method is validated for its classification tasks on two devices, namely Titan V and Jetson TX2, and for object detection performance on Titan V, and is shown to achieve superior accuracy/latency tradeoff compared to existing pruning methods, including latency aware ones.  The paper received split reviews initially, and the following is the summary of the pros and cons mentioned by the reviewers.  Pros   The proposed formulation of the latency constrained pruning problem as a constrained knapsack problem is novel.    The method achieves competitive performance against existing latency constrained pruning methods.    The paper is written well, with clear motivation and descriptions of the proposed method.  Cons    The idea is not very exciting since posing pruning as a combinatorial optimization problem, or a knapsack problem is not new, and the proposed method only adds in additional latency constraints.   The title “hardware aware” is vague and misleading since what the authors do are latency constrained pruning.   The experimental validation is only done on two devices, which makes the method less convincing as a “hardware aware” method and how it generalizes to other devices (e.g. CPU, FPGA)   Use of lookup tables to obtain the latency constraints is not novel, has a limited scalability, and is inefficient.     Missing discussion of design choices.   During the discussion period, the authors cleared away some of the concerns, which resulted in two of the reviewers increasing their scores. However, one reviewer maintained the negative rating of 5, and the positive reviewers were still concerned with limited novelty.  I believe that this is a good paper that proposes a neat solution for latency pruning, which may have some practical impact. However, the novelty of the idea is limited, as pointed out by the reviewers. The use of lookup tables also does not seem to be an efficient solution for adapting to edge devices for which the collection of latency measurements could be slow. The experimental validation on only two devices of the same type (GPU) also seems insufficient, as how the method generalizes to diverse devices is uncertain. It would be worthwhile to consider using a latency predictor (e.g. BRP NAS [Dudziak et al. 20]), and perform experimental validation on diverse hardware platforms (e.g. CPU and FPGA). Comparing against recently proposed hardware aware NAS methods could be also interesting, as there has been a rapid progress on the topic recently.  Thus, despite the overall practicality and the quality of the paper, the paper may benefit from another round of revision, since both the method and the experimental validation part could be improved.   [Dudziak et al. 20] BRP NAS: Prediction based NAS using GCNs, NeurIPS 2020
The paper proposes the anisotropic version of randomized smoothing. Evaluation metrics based on the volume of the certified region are proposed, allowing comparisons with the certified regions provided from isotropic randomized smoothing. Experimental results show the usefulness of introducing anisotropic randomized smoothing as it certifies larger regions.  Strengths: + The paper is well written, polished, and easy to follow. + The anisotropic part of the proposed approach is well motivated. + The evaluation section is quite thorough and obtains SOTA results.  Weaknesses:   The sample wise (data dependent) part has several issues making it unsuitable to use in practice. The authors already discuss a know issue of data dependent classifiers which when not tackled can lead to certificates that are not sound. To address the issue they adapt the memory based procedure introduced in Alfarra et al. While this procedure does make the certificate sound it has other problems. For example, an issue is that the memory makes the certificate dependent on the order of the incoming test samples. This provides a new avenue for attack, i.e. the adversary can optimize the order of the test samples to decrease the utility of the final obtained smoothed classifier. In addition, the success of this memory approach also somewhat depends on the "sparsity" of the test samples. Namely, by using a small test set since the samples are in a high dimensional space the distance between them tends to be bigger than the (proxy) radii of the certified regions. However, in a real world application we are likely to have many more test samples which would increase the number of intersections when running Algorithm 1. Although the authors provide opposite empirical evidence on a specific dataset, it is not very clear how general it is for other datasets.    Some of the theoretical results are not novel as they follow directly from prior work (as acknowledged in the paper).     Another issue with the proposed approach is the optimization procedure described in section C. The optimization suffers from issues such as: inconsistent estimation due to clamping and not using confidence bounds, sensitivity to initialization, high gradient variance, etc.
The paper considers using local spectral graph clustering methods such at the PPR Nibble method for graph neural networks.  These local spectral methods are widely used in social networks, and understanding neural networks from them is interesting.  In many ways, the results are interesting and novel, and they deserve to be more widely known, but there are several directions to make the work more useful to the community.   These are outlined in the reviewer comments, which the authors answered partially but not completely satisfactorily.  Much of this has to do with explaining how/where these (these very fundamental and ubiquitous) methods are useful in a particular application (GNNs here, and node embeddings below).  An example of a paper that successfully did this is "LASAGNE: Locality And Structure Aware Graph Node Embedding, E. Faerman, et al.  Proc. 2018 Conference on Web Intelligence."  (That is mentioned not since it is directly relevant to this paper, but since it provides an example of how to present the use of a method such as PPR Nibble for the community.
The manuscript performs an empirical analysis of existing bias mitigation methods on two large datasets CelebA and ImageNet People Subtree where there are multiple sensitive attributes and some unavailable sensitive attribute labels. The results show that existing methods can mitigate intersectional bias at scale but unlabeled mitigation methods generalize poorly. The manuscript further proposes a knowledge distillation approach which can augment other labeled mitigation approaches.  On the positive aspect, the manuscript studies an important problem: intersectional subgroups on deep learning methods. Reviewers acknowledged that an empirical study on this problem is as an opportunity to make a contribution as it can highlight previously unknown issues.  There are however several major concerns including: 1. Methodological contribution (knowledge distillation) is under developed, while empirical investigation is interesting but can be further developed; 2. The fairness metrics adopted in this manuscript need to be clarified; 3. A discussion on the hyperparameter tuning, maybe involving a fairness accuracy tradeoff; 4. The claimed O(1) complexity for the knowledge distillation approach is implausible because it assumes the availability of G group specific models. This has been clarified in the rebuttal that the claim is only for the inference complexity, and the approach does not improve the training complexity.   Reviewers also concluded that while the empirical analysis is interesting, the results on CelebA to be of limited use because the sensitive attributes are "purely illustrative." It s not clear that the insights from these illustrative intersectional groups (e.g. big nose & attractive) will hold for groups that are meaningful in a fairness sense.
The paper presents a reinforcement learning based approach for object localization given an exemplary set of images.  The paper shows that test time policy adaptation to new environments is possible with object detection experiments. All four reviewers find the proposed approach interesting. Most of the reviewers feel their initial concerns (including the clarity of the paper and the approach details) addressed after the discussion with the authors and the revision. One reviewer still finds the experiments limited even after the author rebuttal, but the reviewers all agree that there is value in the paper.  We recommend accepting the paper.
This paper tackles neural response generation with Generative Adversarial Nets (GANs), and to address the training instability problem with GANs, it proposes a local distribution oriented objective. The new objective is combined with the original objective, and used as a hybrid loss for the adversarial training of response generation models, named as LocalGAN. Authors responded with concerns about reviewer 3 s comments, and I agree with the authors explanation, so I am disregarding review 3, and am relying on my read through of the latest version of the paper. The other reviewers think the paper has good contributions, however they are not convinced about the clarity of the presentations and made many suggestions (even after the responses from the authors).  I suggest a reject, as the paper should include a clear presentation of the approach and technical formulation (as also suggested by the reviewers).
This paper experimentally analyzes the double descent phenomenon for deep models. While, as the reviewers have mentioned, this phenomenon has been observed for some time, some of its specificities still elude us. As a consequence, I am happy to see this paper presented to ICLR.  That being said, given the original lack of proper references as well as the recent public announcements about this paper giving it visibility, I want to make it absolutely clear that this paper is accepted with the assumption that proper credit will be given to past work and that efforts will be made to draw connections between all these works.
The paper investigates various approaches, and a unifying framework, for sequence design. There were a variety of opinions about the paper. It was felt, after discussion, that the paper would benefit from a sharper focus, and somewhat suffers from being overwhelmed by various approaches, lacking a clear narrative. But overall all reviewers had a positive sentiment, and the paper makes a nice contribution to the growing body of work on protein design.
This paper has been thoroughly evaluated by four expert reviewers and it had received one public comment. The authors provided extensive explanations and added technical updates to the contents of their submission in response to constructive critiques from the reviewers. Even though some minor issues have not been fully resolved in the discussion between the authors and the reviewers, I consider this paper worthy of inclusion in the program of ICLR 2021 since, albeit marginally, the apparent strengths outweigh its outstanding limitations.  
The paper considers an attack of the recently proposed InstaHide algorithm mixing up public and private images by convex combination to achieve security of sensitive data. The paper formulates the problem as a multi task phase retrieval problem with missing data, and shows that under Gaussian data distribution setting, we can recover a small number of private data samples given sufficiently large dimensionality and number of synthetic samples output by InstaHide.  Theoretically, the Gaussian data distribution is quite restrictive in practice, but it could be a good start. The paper also uses some novel techniques in the analysis, which meets the technical standard of ICLR. The reviewer mainly concerns about the general motivation and formulation of "security" studied in the paper, since attacks can be trivial in practical scenarios where data is non Gaussian, which reveals a possible weakness on practical value of this work.   Although the work is probably better suited for a theoretical oriented conference, I nevertheless feel it should be also acceptable for ICLR because it specifically addresses a recent distributed learning problem and the results are non trivial and improving our understanding of the InstaHide s security.
This paper presents a hyperparameter optimization (HPO) method in which two search strategies: global and local optimizations, are effectively combined. All reviewers evaluated the proposed method positively. The experimental results clearly show the effectiveness of the proposed method, and it could be an important contribution to the AutoML research community. On the other hand, since there is no theoretical justification for the proposed method, it is not clear why the performance of the proposed method is improved so much. The author s rebuttal has alleviated some of our concerns on this point, but the further theoretical analysis is desirable.
Thank you for submitting you paper to ICLR. ICLR. The consensus from the reviewers is that this is not quite ready for publication.
The proposed method suggests a way to do robust conditional image generation with GANs. The premise is to make the image to image translation model resilient to noise by leveraging structure in the output space, with an unsupervised "pathway".  In general, the qualitative results seem reasonable on a a number of datasets, including those suggested by reviewers. The method appears simple, novel and easy to try.  The main concerns seem to be that the idea is maybe too simple, but I m not particularly bothered by that. The authors showed it working well on a variety of tasks (synthetic and natural), provide SSIM numbers that look compelling (despite SSIM s short comings) and otherwise give compelling arguments for the technical soundness of the approach.  Thus, I recommend acceptance.
This work proposes to train large scale graph neural networks by replacing the moving averages used in the stochastic compositional optimization (SCO) framework with sparse moving averages. This reduces the memory required for SCO, allowing their algorithm to scale  to larger graphs.   The consensus is that the approach is reasonable, but incremental both in the change over SCGD and the change in the analysis. More importantly, the reviewers identified several sampling based methods for scaling up training of GNNs that are important baselines for the proposed algorithm; the relative merits of the method against these approaches should be established with further experiments.
### Description The paper investigates the choice of a fixed quantization grid for weights. Namly, the paper observes that symmetric uniform quantization levels such as { 1.5, 0.5,0.5,1.5} lead to better results than non symmetric ones, e.g. { 2, 1,0,1}. While it is a small thing, it can be appreciated that it is investigated systematically and pedantically, proposing an explanation and showing experimentally that the effect is constantly present in favour of symmetric quantization. While the improvement is small, it comes almost at no cost. A part of the contribution proposes an efficient implementation.   ### Decision Reviewers and AC came to a consensus that the contribution of the paper is marginal. Symmetric quantization schemes themseleves were already employed by many models, albeit without analysis or even a discussion of such choice. The analysis presented in the paper was found unconvincing by the reviewers (see below). The efficient implementation follows from basic linear algebra (see below). The potential impact of the work was considered as limited due to a rather marginal observed improvement. The average rating of the paper was 4.5. Therefore must reject.  ### Details Regarding the proposed analysis of CSQ, it is not clear, why the number of quantization levels of an elementary product matters, given that these numbers are then summed over all corresponding input channels and spatial dimensions of a convolution kernel applied at a single location. It is questionable whether the number of these quantization levels indeed corresponds to the representation capacity. Finally, the paper misses to demonstrate the effect on binary (1 bit) networks. In this case the standard approach is to use { 1,1} weights and { 1,1} activatinos. The paper could investigate the case of {0,1} activations, where there would be 50% more unique possible outputs from the product, namely { 1,0,1} to validate their hypothesis. If the hypothesis holds, an improvement in the binary case would be observed. This is important since the binary case is know to be the hardest and since the respective recommendation of representations would be non standard. It could be further questioned why the distribution of real valued weights has any relevance (such as in the arguments in appendix E) if the model is trained from scratch? A training method need not keep any real valued latent weights in the first place.  The technical part in section 5 "efficient realization" adds very little, if anything, to the paper s contribution. A simple linear algebra suffices to see that   $(W 0.5) \ast x   W* x   0.5 I \ast x,$  where $I$ is the kernel of ones of the same shape as $W$. It is clear that the convolution $I \ast x$ can be implemented efficiently (e.g. it is just a sum over channels followed by a separable spatial only convolution) and is not a bottleneck and. The final detail such as whether to slice by bits and use popcount for it or to use 8 bit addition, depend very much on the choice of the bit packed representation and the hardware available. It would be known to engineers in the field how to implement it efficiently.
The paper shows a connection between Potts model and Transformers and uses the connection to propose a factored attention energy to use in an MRF. Results are shown, using this energy based on factored attention. Also, pretrained BERT models are used to predict contact maps as a comparison. The reviewers found the paper interesting from a protein structures prediction point of view, but from a machine learning perspective their opinion was that the paper does not offer a coherent, compelling method that is very novel, and the connection between Potts and an energy based attention model is not that overwhelming.  In addition the presentation was somewhat circuitous.    The authors made improvements to the paper over the course of the review, which is appreciated, but the method presented does not match the target for an ICLR paper in terms of methodological contributions.  
The paper studies the credit assignment problem in meta RL, proposes a new algorithm that computes the right gradient, and demonstrates its superior empirical performance over others.  The paper is well written, and all reviewers agree the work is a solid contribution to an important problem.
This paper aims to develop a simple yet efficient deep RL algorithm for off policy RL. The proposed method uses advantages to as weight in regression, which is an extension of the known method of reward weighted regression. The paper is in general nicely written, and it comes with a set of theoretical analyses and experiments. While all reviewers admit that the approach is interesting and the work makes an attempt to solve an important yet open problem, there are several aspects of the paper that make it not ready for publication in its current form:    Novelty: As pointed out by reviewers, the proposed method appears to be a minor modification of existing off policy solvers. Although the use of advantages as weights makes intuitive sense, it is unclear why and how the new method significantly differs from and outperforms existing methods. Going forward, it would be helpful if the authors could present more convincing arguments/experiments to demonstrate the power of ARW, relative to similar existing methods.    Experiments provide some insights into the difference between several algorithms, but the results are not strong enough to support the claim of the paper. Please see reviewers  comments for more details. We strongly recommend the authors to take these comments into consideration and develop more rigorous experiments to demonstrate advantages of AWR.    Theoretical analysis is limited. As R#2, R#3 mentioned, the theory analysis in the paper seems to not match the algorithm, and there remain bugs, this it doesn t add to the paper. Although theory might not be the focus of the paper, if the authors decide into include theoretical analysis, the analysis would hopefully provide insights into why and by how much the approach is better.  
This paper is concerned with sequence segmentation. The authors introduce a framework which they call  lexical unit analysis    a neural network is used to score spans and then dynamic programming is used to find the best scoring overall segmentation. The authors present extensive experiments on various Chinese NLP tasks, obtaining better results than the systems they compare to.  Reviewers raised concerns, including about novelty. In my view, beyond beating the state of the baselines on the chosen tasks, it is hard to extract an actionable insight or novel conceptual understanding. Therefore, the paper is not recommended for acceptance in its current form.
Pros:   novel idea of intra life curiosity that encourages diverse behavior within each episode rather than across episodes.  Cons:   privileged/ad hoc information (RAM state, distinguishing rooms)   lack of sufficient ablations/analysis   insufficient revision/rebuttal  The reviewers reached consensus that the paper should be rejected in its current form.
This paper is a very borderline case. Mixed reviews. R2 score originally 4, moved to 5 (rounded up to WA 6), but still borderline. R1 was 6 (WA) and R3 was 3 (WR).  R2 expert on this topic, R1 and R3 less so. AC has carefully read the reviews/rebuttal/comments and looked closely at the paper. AC feels that R2 s review is spot on and that the contribution does not quite reach ICLR acceptance level, despite it being interesting work. So the AC feels the paper cannot be accepted at this time. But the work is definitely interesting   the authors should improve their paper using R2 s comments and resubmit.  
All reviewers gave a 5 rating. The author rebuttal was not able to alter the consensus view of reviewers. See below for details.
This paper presents a faster sampling method for diffusion based generative models which are usually slow in practice. The key idea is based a progressive distillation approach (e.g., how to distill a 4 step sampler into a 1 step sampler). The paper studies the various design choices for diffusion models which existing work hasn t looked at that deeply and sheds light on the effects of these choices. The paper also shows that DDIM can be seen as a numerical integrator for probability flow ODE. The experimental results are impressive.   There were some concerns such as the effect of progressive distillation and the overhead of distilling the diffusion model but the authors provided a satisfactory response and backed it up with additional results.  Overall, this is a nice paper on making diffusion based generative models generate faster samples and also provides novel insights into the behavior of these models under various design choices. Given the significant recent interest in these models which are pretty impressive in terms of generation quality but slow, the paper indeed makes a timely contribution which will fuel further interest in these models.  All the reviewers have voted for acceptance. Based on my own reading, the reviewers  assessments, the discussions, and the authors  response, I would vote for acceptance.
The reviewers have the following remain concerns: 1. The bounded function value assumption is strong. Note that the previous works for SGD and SGD M for other LR schemes do not necessarily need this assumption, hence it may be unfair to compare with existing results and say that this work has improvements for non monotonic schemes. The authors also agree that it is not easy to prove and remove this assumption.  2. The novelty is limited, and the contributions are somewhat incremental. The bandwidth step size scheme was already introduced in a previous work with a very similar setting. The convergence rate for the proposed LR scheme is the same as previous works for other schemes (or only better by a logarithmic term), which makes the results incremental.  3. Some of the claims are not well supported. For example, the reviewers comment that it is not clear how the proposed bandwidth step size can help to escape local minima. Although the authors aim to show this empirically, the toy setting is not strong enough to conclude the superior performance of the proposed scheme.  We encourage the authors to improve their paper and resubmit to another venue. Here are the related suggestions: 1. The authors might try to investigate and provide a rigorous proof of how the non monotonic step size can help to escape local minima. It also helps to characterize the effectiveness of each cyclic rule (cosine/ triangular or any other) and make clear what property (cosine/linear rules or bandwidth or non monotonicity) contributes most in the good performance of a LR scheme. 2. It is better if the assumption on the bounded function value can be removed. In addition, a theoretical/empirical analysis on the generalization performance of the proposed scheme might also be helpful.
 The paper attempts at controllable summarization in two dimensions: Length, and content. Authors try to achieve this through training data generation approach, where they provide a standard BART model with additional keywords (extracted using a BERT model) in training.  The paper s main motivation on controllable summarization is important and interesting, and despite simplicity, the results are generally positive on multiple datasets. However, despite positive results, reviewers raised several critical concerns, some of which remained unresolved after reviewer/author discussion period. Examples include concerns regarding lack of methodological novelty over prior work (R1, R2, R4), unfair/incomplete comparisons with prior work (R2, R4, R5), and not evaluating on a real user controlled setting instead of automatic keywords (R1, R4). Although the authors tried addressing human evaluation in their revision, some reviewers remained unconvinced.  Some quotes from reviewer discussions:  > I m not convinced the human eval was done properly.  > My concerns are not completely addressed and the score remains unchanged. For human evaluation, I agreed with Reviewer X. 
The proposed method for set representation learning with an application to mete learning is well motivated and reasonable. Reviewers  original concerns about novelty and technical presentation have been well explained and addressed in the revision. If some theoretical analysis can be provided regarding the proposed method, it would make this work stronger.  In summary, a positive recommendation is given here.
This submission presents a theoretical study of phase transitions in IB: adjusting the IB parameter leads to step wise behaviour of the prediction.  Quoting R3: “The core result is given by theorem 1: the phase transition betas necessarily satisfy an equation, where the LHS is expressed in terms of an optimal perturbation of the encoding function X >Z.” This paper received a borderline review and two votes for weak accept.  The main comment for the borderline review was about the rigor of a proof and the use of << symbols.  The authors have updated the proof using limits as requested, addressing this primary concern.  On the balance, the paper makes a strong contribution to understanding an important learning setting and a contribution to theoretical understanding of the behavior of information bottleneck predictors.  
The paper proposes a fast training method for extreme classification problems where number of classes is very large. The method improves the negative sampling (method which uses uniform distribution to sample the negatives) by using an adversarial auxiliary model to sample negatives in a non uniform manner. This has logarithmic computational cost and minimizes the variance in the gradients. There were some concerns about missing empirical comparisons with methods that use sampled softmax approach for extreme classification. While these comparisons will certainly add further value to the paper, the improvement over widely used method of negative sampling and a formal analysis of improvement from hard negatives is a valuable contribution in itself that will be of interest to the community. Authors should include the experiments on small datasets to quantify the approximation gap due to negative sampling compared to full softmax, as promised.
The submission combines meta learning and attention mechanism for generalised zero shot learning. The image guided attention on the semantic space helps to adapt the better class specific semantic information while separate experts operate on the seen and unseen classes. The unseen class expert is trained with the pseudo negative samples with pseudo negative labels. Meta learning based training adapts the model to few shot learning scenario. The submission has received two accept, two weak accept and one weak reject reviews. All reviewers found the methodology interesting but they found it moderately novel. The experimental evaluation has been found strong. The rebuttal addressed all the reviewers  concerns and during the discussion phase all reviewers recommended acceptance. The meta reviewer follows the consensus of all the reviewers and recommends acceptance.
In this paper, the authors aim to work within the path specific framework to implement fair predictions by learning a causal graph in such a way that some path specific effect is removed.  Generally, the paper was not received very well by reviewers, with the primary concern being lack of novelty in particular in comparison with (Kyono et al.)  One additional comment I wanted to make is this: any prediction task that is a part of a pipeline that contains a graphical model selection step is properly a "post selection inference problem."  Such problems are very challenging because:  (a) Learning a graph from data is known to lack consistency at any rate (meaning that the algorithm is only pointwise consistent, but not uniformly consistent).  This issue propagates to "downstream" tasks in the pipeline, including prediction problems.  Probably, the way this issue would manifest in this work is unless sample sizes were very large, there would be no particular reason to assume the correct causal path is removed.  (b) Even if uniformly consistent modifications of structure learning algorithms were used, the uncertainty in learning the graph with error must be propagated to all subsequent steps in the pipeline.  Doing so appropriately is very challenging.  When revising the paper, in addition to taking reviewer comments into account, please consider how your method deals with post selection inference issues   I think this is a very interesting but challenging question that is likely to come in peer review.
This paper proposes new GAN training method with multi generator architecture inspired by Stackelberg competition in game theory. The paper has theoretical results showing that minmax gap scales to \eps for number of generators O(1/\eps), improving over previous bounds. Paper also has some experimental results on Fashion Mnist and CIFAR10 datasets.   Reviewers find the theoretical results of the paper interesting.  However, reviewers have multiple concerns about comparison with other multi generator architectures, optimization dynamics of the new objective and clarity of writing of the original submission. While authors have addressed some of these concerns in their response reviewers still remain skeptical of the contributions. Perhaps more experiments on imagenet quality datasets with detailed comparison can help make the contributions of the paper clearer. 
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
Description: The paper presents a weakly supervised model CICGMO for disentangling category, shape and view information from images. Label information is not need as  the weak supervision is done by grouping together different views of the same object. They show that this outperforms other techniques on tasks such as invariant clustering and one shot classification.  Strengths:   Paper is well written   Data category is explicitly modeled    The weakly supervision approach is appealing,  since the grouped data used as supervision information is easy to obtain   Invariant clustering and one shot classification results outperforms other methods significantly, showing CIGMO is doing a decent job at those tasks. This could be explained by CIGMO ability to better  disentangle category shape view.  Weaknesses:   It is unclear how well (quality) the generative model is able to disentangle shape from view   The reconstruction quality is quite low, such that it is difficult often times, in the MULTIPIE example, to clearly identify a face geometry.   Generated results are not evaluated directly, but rather evaluation is done through down stream tasks such as invariant clustering. This makes it difficult to show the quality of shape and view information.
This paper addresses the question of RL in high dimensional spaces by learning lower dimensional representations for control purposes. The work contains both theoretical and empirical results that shows the promise of the proposed approach.  While the reviewers had initial concerns, including with a problem in a proof and questions around the contributions, after robust responses and discussions this paper is now in good shape.
The paper proposes an interesting idea for more effective imitation learning.  The idea is to include short actions sequences as labels (in addition to the basic actions) in imitation learning.  Results on a few Atari games demonstrate the potential of this approach.  Reviewers generally like the idea, think it is simple, and are encouraged by its empirical support.  That said, the work still appears somewhat preliminary in the current stage: (1) some reviewer is still in doubt about the chosen baseline; (2) empirical evidence is all in the similar set of Atari games   how broadly is this approach applicable?
The submission proposes a hierarchical clustering approach (nested means clustering) to determine good quantization intervals for non uniform quantization.  An empirical validation shows improvement over a very closely related approach (Zhu et al, 2016).  There was an overall consensus that the literature review was insufficient in its initial form.  The authors have proposed to extend it somewhat.  Other concerns are related to the novelty of the technique (R4 was particularly concerned about novelty over Zhu et al, 2016).  Two reviewers were against acceptance, and one was positive about the paper.  Due to the overall concerns about the novelty of the approach, and that these concerns were confirmed in discussion after the rebuttal, this paper is unlikely to meet the threshold for acceptance to ICLR.
This paper proposes a policy gradient algorithm based on the Bregman divergence and momentum method. While one reviewer was initially concerned about the technical novelty of the paper given some existing works, after the author s response and paper revision, the reviewers are all convinced and have reached a consensus to accept this paper. Thus I recommend acceptance.
The reviewers found that paper is well written, clear and that the authors did a good job placing the work in the relevant literature.  The proposed method for using multiple discriminators in a multi objective setting to train GANs seems interesting and compelling.  However, all the reviewers found the paper to be on the borderline.  The main concern was the significance of the work in the context of existing literature.  Specifically, the reviewers did not find the experimental results significant enough to be convinced that this work presents a major advance in GAN training.  
The reviewers present strong concerns regarding presentation of the paper. The approach appears overly complex, some design choices are not clear and the experiments are not conducted properly. I recommend the authors to carefully go through the reviews.
The paper introduces the new task of few shot semantic edge detection by adapting existing datasets. It proposes a new method which is compared to a baseline.  Pros:   Clear writing.    Extensive ablation experiments.   Good architectural choices.  Mixed:   The value of the new task raises a mix of opinions. For example R1 sees it as a "relevant problem, and is well suited for few shot tasks", but R2 finds is very similar to few shot segmentation. I think a more interesting version of the problem (that would also create more separation to few shot segmentation) would be to also consider internal edges, not just "semantic boundaries". For example the original BSDS dataset has pure edge annotations.    Besides the task, another novelty of the paper is the proposed multi split matching technique, but while it is well demonstrated empirically (as backed by additional results given by authors in rebuttal), R3 would like to have seen "theoretical or analytical reasoning" and R1 says it is an "ad hoc technique".  Cons:   the PANet+Sobel baseline. All 4 reviewers are unhappy with this baseline: 3 of them find it unfair because of the non standard edge thickening employed and 2 think there would be more recent and better baselines. The authors provided a rebuttal arguing that their GT edges are "not too thick to be unfair" but two of the reviewers mentioned they remained unconvinced   R1 hopes "the authors will work on cleaner evaluation of the baseline" and R4 find the baseline "still unconvincing in the revised version".  Overall the paper would benefit from one more iteration focusing on the evaluation procedure to be convincing and impactful.  
The paper presents a novel perspective on optimizing lists of documents ("slates") in a recommendation setting. The proposed approach builds on progress in variational auto encoders, and proposes an approach that generates slates of the desired quality, conditioned on user responses.   The paper presents an interesting and promising novel idea that is expected to motivate follow up work. Conceptually, the proposed model can learn complex relationships between documents and account for these when generating slates. The paper is clearly written. The empirical results show clear improvements over competitive baselines in synthetic and semi synthetic experiments (real users and clicks, learned user model).  The reviewers and AC also note several potential shortcomings. The reviewers asked for additional baselines that reflect current state of the art approaches, and for comparisons in terms of prediction times. There are also concerns about the model s ability to generalize to (responses on) slates unseen during training, as well as concerns about the realism of the simulated user model in the evaluation. There were questions regarding the presentation, including model details / formalism.  In the rebuttal phase, the authors addressed the above as follows. They added new baselines that reflect sequential document selection (auto regressive MLP and LSTM) and demonstrate that these perform on par with greedy approaches. They provide details on an experiment to test generalization, showing both when the model succeeds and where it fails   which is valuable for understanding the advantages and limitations of the proposed approach. The authors clarified modeling and evaluation choices.   Through the rebuttal and discussion phase, the reviewers reached consensus on a borderline / lean to accept decision. The AC suggests accepting the paper, based on the innovative approach and potential directions for follow up work.  
Congratulations!  The reviewers unanimously viewed this work positively and were in favor of acceptance to ICLR.  While the current revision already addresses many reviewer concerns, it may be worth adding some of the datasets pointed out by R3 or comparing to some of the papers suggested by R1.
The paper closes an important gap in our understanding of neural tangent kernels.  In addition, the used techniques are novel.  My low confidence is mainly based on the fact, that the review process at conference is not perfectly suited to deal with such papers, since their review would actually require both expert reviewers and substantially longer reviewing periods.
This paper presents a model for video action recognition.  The reviewers appreciated the development of a novel dynamic fusion method that examines channels from feature maps for use in temporal modeling.  After reading the authors  responses, the reviewers converged on an accept rating.  The solid empirical results and analysis, the fact that is is a plug in method that could be used in other models, and the clear exposition were deemed to be positives.  As such, this paper is accepted to ICLR 2021.
# Paper Summary  This paper considers the problem of distributionally robust optimization (DRO), in which one is attempting to minimize a loss on the worst of all distributions that are some distance (here, measured in terms of KL divergence) from the training set. The main novelty here is that this adversarial distribution is represented as a model, with parameters that are learned jointly with the primary model.  This is an intuitive idea, but as the authors explain, attempting to implement it leads to a number of complications. One of these is that it is challenging to constrain the adversarial distribution model to be a certain KL divergence away from the training set. To address this, they write down the Lagrangian, but do not actually optimize over the Lagrange multiplier resulting from this constraint: instead, they keep it at a fixed constant value (a hyperparameter). A second, and potentially more worrisome, issue is that it is difficult to optimize the KL divergence as written instead, they swap the two parameters, which is of course incorrect but they claim leads to much nicer convergence behavior.  They also propose a stopping condition, which terminates optimization once the robust validation loss (i.e. the validation loss w.r.t. the worst permissible distribution) stops decreasing. Normally, this would require a search for the worst such distribution at every iteration, which would be prohibitively expensive, so they propose instead only checking the distributions that have been found by the adversary during the course of optimization.  They close with a set of experiments that is nicely designed to narrow in on and explore particular details of their approach (e.g. they have an experiment that validates their stopping criterion), and have a realistic experiment on two NLP datasets.  # Pros  1. Reviewers agreed that it was very well written, well organized, and comprehensive 1. Good discussion of background material. The paper is very accessible 1. Intuitive idea, although the details of the approach become somewhat complex 1. Aside from the "realistic" experiment, each is designed to explore a particular facet of their approach  # Cons  1. Some reviewers were concerned that the baselines were insufficient. In response the authors added the new Hu et al. baseline (NonParam), which seemed to be satisfactory 1. While the approach is more general, one reviewer noted that the experiments only consider NLP problems. This is a minor negative point, in my view 1. One reviewer was concerned that the results were "too good", and encouraged the authors to double check their results. My belief is that, at least on the non "realistic" experiments (which were mostly intended to drill down into specific attributes of their approach, rather than demonstrate its overall performance), this is because the problem was constructed to perform especially poorly with a non DRO approach 1. One reviewer was unsatisfied with the idea of swapping the parameters to the KL divergence (I share this concern). The authors clarified, both in the response and in the paper, that swapping the parameters is indeed incorrect, and may in fact be a very bad approximation to the true quantity of interest, but that the performance difference was so dramatic that it couldn t be undone. This seemed to partially satisfy the reviewer  # Conclusion  All four reviewers ultimately recommended acceptance. The major concerns were (i) that the baselines weren t good enough (which the authors addressed by adding a new baseline), and (ii) that swapping the parameters to the KL divergence results in a very poor approximation to the original KL divergence (which the authors now explicitly acknowledge in the paper, with an explanation for why they feel it is necessary). Overall, this is a nice idea, and while bringing it into practice may require more hand waving than would be ideal (which is the main reason I suggested a poster acceptance instead of a spotlight or oral), it seems to work well experimentally, and the experiments are overall very careful and well thought out. Additionally, the writing quality is excellent, as is the organization and presentation of background material.
This paper tackles the problem of learning under data shift, i.e. when the training and testing distributions are different. The authors propose an approach to improve robustness and uncertainty of image classifiers in this situation. The technique uses synthetic samples created by mixing multiple augmented images, in addition to a Jensen Shannon Divergence consistency loss. Its evaluation is entirely based on experimental evidence.  The method is simple, easy to implement, and effective. Though this is a purely empirical paper, the experiments are extensive and convincing.   In the end, the reviewers didn t show any objections against this paper. I therefore recommend acceptance.
The reviewers agree that the submitted paper is of high quality and provides a promising approach/framework for Bayesian IRL. Certain concerns regarding details of the implementation and evaluation have already been addressed by the authors during the rebuttal phase, and also the title of the paper was adjusted in line with discussions with the reviewers. For the final paper, the authors should make sure to clearly highlight the advances of inferring a distribution over rewards (this is already partly done by the added grid world experiments) and discuss relations to VAEs as the initially had in mind and even in the paper title. Beyond that, the should of course also address other reviewers’ comments.
This paper tackles the task of end to end systems for dialogue generation and proposes a novel, improved GAN for dialogue modeling, which adopts conditional Wasserstein Auto Encoder to learn high level representations of responses. In experiments, the proposed approach is compared to several state of the art baselines on two dialog datasets, and improvements are shown both in terms of objective measures and human evaluation, making a strong support for the proposed approach. Two reviewers suggest similarities with a recent ICML paper on ARAE and request including reference to it and also request examples demonstrating differences, which are included in the latest version of the paper.
The authors introduce a method for improving reinforcement learning in sparse reward settings. In particular, they propose to take advantage of a suboptimal behavior policy as a guidance policy that is incorporated in a TRPO like update. The reviewers agree that this is a novel and interesting idea and given the authors  rebuttal with additional experiments, clarifications and discussions, they agreed to accept the paper. However, they also point out several flaws (e.g. evaluation on a more challenging sparse reward task such as Adroid) that I encourage the authors to address in the final version of the paper.
The topic of this paper is non uniform priors and exploration in reinforcement learning with the graph Laplacian.  All reviewers appreciated several aspects of this work but they all also have several reservations.   Looking at the paper, reviews and discussions, I see the potential a very nice more general contribution. This potential is not fully realised as the paper stands now. Acceptance can therefore not be recommended.
This manuscript was reviewed by 3 expert reviewers and their evaluation is generally positive. The authors have responded to the questions asked and the reviewers are satisfied with the responses. Although the 2D environments are underwhelming (compared to 3D environments such as SUNCG, Doom, Thor, etc), one thing that distinguishes this paper from other concurrent submissions on the similar topics is the demonstration that "words learned only from a VQA style supervision condition can be successfully interpreted in an instruction following setting." 
This paper receives positive reviews. The authors provide additional results and justifications during the rebuttal phase. All reviewers find this paper interesting and the contributions are sufficient for this conference. The area chair agrees with the reviewers and recommends it be accepted for presentation.
The reviewers were unanimous that this submission is not ready for publication at ICLR. Concerns were raised about clarity of the exposition, as well as lack of sufficient experiments comparing to related work.
New generative model to come up with data that is needed when doing contrastive learning. Like the fact that multiple modalities were considered and evaluated. The Viewmaker methods appears to do well on CIFAR 19 and outperforms baselines on speed and wearable domains. The reviewers praise the method for being simple, well described and well motivated. The main drawbacks stem from the fact that viewmaker cannot make certain types of image specific augmentations (crop & rescale, as an example), but it s fair that the authors argue that their method is more domain agnostic; and one can indeed add more domain specific stuff if needed.  All in all, this seems like a solid paper with an easy to implement idea that is quite general and that has been shown to work in a variety of settings. It definitely belongs at ICLR.
I m inclined to recommend accepting this paper, although it is borderline given the strong dissenting opinion. The revisions have addressed many of the concerns about quality, clarity, and significance. The paper gives an end to end explanation in Bayesian terms of generalization in neural networks using SGD.  However, it is my opinion that Bayesian statistics is not, at present, a theory that can be used to explain why a learning algorithm works. The Bayesian theory is too optimistic: you introduce a prior and model and then trust both implicitly. Relative to any particular prior and model (likelihood), the Bayesian posterior is the optimal summary of the data, but if either part is misspecified, then the Bayesian posterior carries no optimality guarantee. The prior is chosen for convenience here. And the model (a neural network feeding into cross entropy) is clearly misspecified.  However, there are ways to sidestep both these issues using a frequentist theory closely related to Bayes, which can explain generalization. Indeed, you cite a recent such paper by Dzugate and Roy who use PAC Bayes. However, you citation is disappointingly misleading: a reader would never know that these authors are also responding to Zhang, have already proposed to explain "broad minima" in (PAC )Bayesian terms, and then even get nonvacuous bounds. (The connection between PAC bayes and marginl likelihood is explained by Germain et al. "PAC Bayesian Theory Meets Bayesian Inference").  Dzugate et al don t propose to explain why SGD finds such "good" minima. So I would say, your work provides the missing half of their argument. This work deserves more prominent placement and shouldn t be buried on page 5. Indeed, it should appear in the introduction and a proper description of the relationship should be given. 
This paper proposes a deep RL framework that incorporates motivation as input features, and is tested on 3 simplified domains, including one which is presented to rodents.   While R2 found the paper well written and interesting to read, a common theme among reviewer comments is that it’s not clear what the main contribution is, as it seems to simultaneously be claiming a ML contribution (motivation as a feature input helps with certain tasks) as well as a neuroscientific contribution (their agent exhibited representations that clustered similarly to those in animals). In trying to do both, it’s perhaps doing both a disservice.   I think it’s commendable to try to bridge the fields of deep RL and neuroscience, and this is indeed an intriguing paper. However any such paper still needs to have a clear contribution. It seems that the ML contributions are too slight to be of general practical use, while the neuroscientific contributions are muddled somewhat. The authors several times mentioned the space constraints limiting their explanations. Perhaps this is an indication that they are trying to cover too much within one paper. I urge the authors to consider splitting it up into two separate works in order to give both the needed focus.   I also have some concerns about the results themselves. R1 and R3 both mentioned that the comparison between the non motivated agent and the motivated agent wasn’t quite fair, since one is essentially only given partial information. It’s therefore not clear how we should be interpreting the performance difference. Second, why was the non motivated agent not analyzed in the same way as the motivated agent for the Pavlovian task? Isn’t this a crucial comparison to make, if one wanted to argue that the motivational salience is key to reproducing the representational similarities of the animals?  (The new experiment with the random fixed weights is interesting, I would have liked to see those results.) For these reasons and the ones laid out in the extensive comments of the reviewers, I’m afraid I have to recommend reject. 
The authors consider the problem of causal inference from multiple conditionally ignorable models that yield different observed data distributions.  This problem is distinct from transportability (which assumes some types of causal invariance across domains, and aims to move causal conclusioned learned in one context to another using this invariance).  The authors adapt a machine learning approach from (Shalit et al, 2017).  Because the authors describe an algorithm rather than a model, it was a bit difficult to understand what assumptions tie the different observed data distributions together (I am guessing there is a way to formulate a  global model  tying all datasets together in terms of the algorithm hyperparameters but the authors do not discuss this).  The authors evaluate their method via a simulation study.  Moreover, in response to reviewer criticism, the authors uploaded additional results from semi synthetic data.  Some of the concerns of reviewers were about novelty and scope of evaluation (in addition, some complained about writing and notation). 
The authors propose to improve the LMs ability on modelling entities by signalling the existence of entities and also allowing the model to represent entities also as units. The embeddings of the surface form and then entity unit are then added and passed through a layer to predict the next word. The paper evaluates on QA and conducts probing tasks and shows that such an entity modelling results in better performance.  All reviewers have found the idea conceptually simple and novel. At the same time a number of concerns are raised, with the most important being the lack of understanding around which and how hyper parameters matter for this model and, most importantly, the confounder introduced to the model by the much larger size of parameters introduced by the embedding layers. While the authors comment that not all the parameters are used all the time, the size of the embeddings still count at the total size of parameters a model has. Thus, without properly controlling for this (e.g., have an another model where the extra embedding params are given to another part of the model), it is difficult to determine whether adding more parameters was the solution or adding more parameters for modelling the entities. 
This paper presents an approach based on conditional denoising diffusion models for point cloud completion. The reviewers have recognized the significance of contributions, the clarity of presentation, and the comprehensivity of experiments. I am happy to recommend this paper for presentation at ICLR.
The reviewers acknowledge the value of the careful analysis of Gaussian encoder/decoder VAE presented in the paper. The proposed algorithm shows impressive FID scores that are comparable to those obtained by state of the art GANs. The paper will be a valuable addition to the ICLR program.   
This work takes ReQuest, an approach for safe deep reinforcement learning utilizing human feedback, and studies it s feasibility in pixel based 3D environments (previously it was only shown to work in simple 2D environments). In order to apply ReQuest in this much more challenging settings, this novel instantiation of ReQuest learns a pixel based dynamics model from a lot of human demonstration data, and a different (as compared to the "base" ReQuest) reward sketching approach to infer the reward function from human feedback.   **Strengths** globally a well motivated and well written/structured manuscript Adresses an important problem, and shows promising results  **Weaknesses** on the more detailed level, there are some clarity concerns (even with the lengthy appendix) evaluation was missing some more relevant comparison (partially fixed after rebuttal/revision) lack of technical novelty, and lack of in depth analysis of results motivation of algorithmic choices : why did you choose the reward sketching approach that you chose? How is it different, and does it improve performance?  **Rebuttal** The authors addressed most questions/things that were unclear and updated the paper to include an additional baseline.    Additional baseline: First, it s great that you added this additional baseline! Yet, to me it s unclear what that additional baseline really represents (Request + sparse rewards). The original ReQuest paper also learns reward from human feedback, is that what you did for this paper? If yes then what does the sparse reward mean? Why does this version of request perform worse than your proposed version?  **Summary** I agree with the reviewers and authors that this is a promising direction. However, in it s current form this manuscript is not ready yet for publication. My  concern are centered around motivation of algorithmic choices: The reward sketching part (while not novel in itself) is a novel component of the ReQuest pipeline, but you do not evaluate what it adds, and neither do you motivate that choice. Furthermore, the additional baseline is not clearly described, it s unclear how it s different for your proposed approach and why we see the performance improvement of your ReQuest version vs the baseline ReQuest version.
The authors propose an efficient LSH based method for computing unbiased gradients for softmax layers, building on (Mussmann et al. 2017). Given the somewhat incremental nature of the method, a thorough experimental evaluation is essential to demonstrating its value. The reviewers however found the experimental section weak and expressed concerns about the choice of baselines and their surprisingly poor performance.
The reviewers all consider the paper to be below the acceptance bar. While the revision addressed some concerns, several critical ones remain open. This includes empirical concerns with regard to the extremely simple grid world environments used, and with regard to the vague distinction between instructions and goal specifications. To improve the submission, the authors should seek stronger empirical foundations, and either refine or remove vague distinctions with regard to the phenomena they aim to study.  Special thanks to the reviewers for an extremely productive discussion.
The paper considers the important problem of tensor network optimization. Unfortunately the authors did not respond to the reviewers comments. Hence, several concerns remain about the proposed greedy algorithm, including its relationship with prior work and the issue of the ALS method being stuck in local minima for important classes of problems. We strongly encourage the authors to carefully examine the reviewers points and revise their work accordingly.
This provides a simple analysis of an existing algorithm for min max optimization under some favorable assumptions.  The paper is clean and nice, though unfortunately lands just below borderline.  I urge the authors to continue their interesting work, and amongst other things address the reviewer comments, for example those on stochastic gradient descent. 
The paper considers input dependent randomized smoothing to obtain certified robust classification. The main contribution is the derivation of necessary conditions on how the variance of the smoothing distributions (assumed to be spherically symmetric Gaussian distributions) has to change to achieve certified robustness. All reviewers like this result, as it provides guidance on designing input dependent smoothing, which is an interesting result for the community and certainly helps future research.  On the negative side, the smoothing method derived based on the theory provides little (if at all) improvement in practice, it cannot be scaled to higher dimensions, it does not address the problems it claims to address (the "waterfall" effect, as also admitted by the authors in the discussion), and the presentation should be significantly improved.   The paper received mixed reviews. While I think that the presented theoretical results are useful and interesting, the problems mentioned above make me to side with the negative reviewers and suggest rejection of the paper at this point (although this was not an easy decision).  While this is only lightly touched in the reviews, I strongly recommend the authors to make the presentation of the theoretical results more comprehensible. It is quite hard to follow the paper as notation is introduced continuously in an ad hoc and confusing way (e.g., in the proof of Theorem 2, $a$ denotes $\delta$ and $\|\delta\|$), and things are often not adequately defined (e.g., the certified robust radius is not defined formally; in Lemma 1, $x$ is undefined and used for $x_0$ as well as a free parameter, $\chi_N^2$ is only implicitly defined, etc.)
All reviewers recommended rejecting this submission so I will as well. However, I do not believe it is fundamentally misguided or anything of that nature.  Unfortunately, reviewers did not participate as much in discussions with the authors as I believe they should. However, this paper concerns a relatively niche problem of modest interest to the ICLR community. I believe a stronger version of this work would be a more application focused paper that delved into practical details about a specific case study where this work provides a clear benefit.
The paper was evaluated by 3 knowledgeable reviewers, where 2 reviewers were leaning for acceptance and one reviewer argued for rejection, rendering the paper a borderline paper. The positive negative points that were raised about the paper during the discussion are summarized below:  Strength:   The presented policy optimization method provides strong results   It provided strong insights into the benefits of differentiable simulators for trajectory guided reinforcement learning   The direction of differentiable physics simulators is very promising and the provided benchmarks are interesting  Weak points:   (i) The main contribution of the paper is a novel trajectory optimization method that uses analytical gradients. In a second step, a neural network is fitted to generalize the control from the single trajectories. The given approach is very much related to existing methods such as GPS or IGOR, just that the trajectory optimization is different. A comparison to these methods is needed. For example, how does the algorithm compare to using iLQG as trajectory optimization method ( we could also use analytical gradients for the linierazations used in iLQG)?   (ii) While the presented tasks are very interesting, there is no benchmark on a more well known task. Hence, it is hard to evaluate the performance in comparison to other algorithms.  The paper defintely has interesting contributions in terms of the new trajectory optimization method and I could live with (ii) as the presented experiments are challenging and interesting, the contribution needs to be better evaluated as comparisions to other trajectory optimization methods are missing. I am sure that the paper will be accepted at another conference with this additional experiments, however, without it the paper is incomplete and I can unfortunately not recommend acceptance.  
The reviewers have mixed views about this paper.   However, it seems to me that the paper is missing some important related work on near optimal exploration and it is only picking a couple of superficially similar approaches to look at. In particular, the standard benchmarks of Rmax, UCRL and Posterior Sampling do are not mentioned. I encourage the authors to look at such methods closely. They perform exploration byplanning over a sample or set of possible MDPs.   I also want to raise another issue mentioned by the reviewers. The paper focuses extensively on neural networks, however a count based metric is inherently for the tabular case. Why would a neural network be appropriate in such a setting? (The authors use a hash table because they are using a large discrete space. However, does it makes sense to essentially uniformly randomly cluster states together? Could there be another, better method? How about continuous spaces?)  The algorithm idea is interesting, and the core is given already in (1) as:   give reward in newly visited states . However, the algorithm as described is incomplete.  It is OK as a high level description, but normally we d require sufficient detail to reimplement the method from scratch. You should for example specify how this intrinsic reward value is going to be used. Most of the reviewers, including me, could not understand how a student/teacher network would be  combined with (1) to produce the intended exploration. Please try to explain in as much detail as possible your algorithm in order for the reviewers to be able to make an informed decision. 
This paper proposes a method for tool synthesis by jointly training a generative model over meshes and a task success predictor. Gradient based planning is then used to find a latent space tool representation which maximizes task success, given a starting tool and an input scene. The results indicate that this method can successfully generate simple tools, and that it performs better than either a random baseline or a version where the generative model and success predictor are trained independently.  The reviewers unanimously felt that this paper was not quite ready for publication at ICLR. While I m a strong believer that unique and creative papers which tackle understudied problems such as this one ought to be encouraged, and that the authors  rebuttal satisfactorily addressed most of the reviewers  concerns, there was one major point that was not. In particular, all reviewers noted that the paper lacks comparison to convincing baselines and/or sufficiently extensive experiments. While I do not think baselines are necessary per se, especially in such a unconventional setting such as this, I believe what the reviewers are getting at (and I agree) is that the results as presented don t really help the reader understand the contours of the method and/or problem space, and as a result, the contributions of the paper feel thin. For example, here are some questions that the reviewers raised, which I do not feel were adequately addressed:    R3: What are the failure cases of the model?   R2: How important is the particular representation of the task and tool (i.e., visual for the task, meshes for the tool)?   R4: How do the imagined tool trajectories compare between the task aware and task unaware cases?   R4: Is the success classifier trained to the same level of performance in both task aware and task unaware settings? (In general, it would be helpful to include learning curves in the appendix.)   R1: How important is the choice of the particular planning/optimization method (i.e. gradient descent)?   R1: What is the generalization performance of the model along affordance directions (e.g. needing to synthesize longer/shorter tools than seen during training)?  Taken individually, such questions might not be an issue, but together they illustrate a larger concern that the paper has not done a thorough enough job of analyzing and evaluating the proposed method. Therefore, at this stage I recommend rejection. I think that by fleshing the paper out with some answers to the above questions, this could make an excellent submission to a future conference.
This paper addresses important general questions about how linear classifiers use features, and about the transferability of those features across tasks. The paper presents a specific new analysis method, and demonstrates it on a family of NLP tasks.   All four reviewers (counting the emergency fourth review) found the general direction of research to be interesting and worthwhile, but all four shared several serious concerns about the impact and soundness of the proposed method.   The impact concerns mostly dealt with the observation that the method is specific to linear classifiers, and that it s only applicable to tasks for which a substantial amount of training data is available.   As the AC, I m willing to accept that it should still be possible to conduct an informative analysis under these conditions, but I m more concerned about the soundness issues: The reviewers were not convinced that a method based on the counting of specific features was appropriate for the proposed setting (due to rotation sensitivity, among other issues), and did not find that the experiments were sufficiently extensive to overcome these doubts.
The paper shows that overparameterized autoencoders can be trained to memorize a small number of training samples, which can be retrieved via fixed point iteration. After rounds of discussion with the authors, the reviewers agree that the idea is interesting and overall quality of writing and experiments is reasonable, but they were skeptical regarding the significance of the finding and impact to the field and thus encourage studying the phenomenon further and resubmitting in a future conference. I thus recommend rejecting this submission for now.
This paper introduces All Alive Pruning, an approach which checks for and removes the connections to and from units with zero gradient ("dead" units). The method is shown to improve performance of IMP at extreme (>128x) compression ratios on MNIST, CIFAR 10, and Tiny ImageNet. All reviewers felt that the problem the authors study   how to identify and remove dead units   is an interesting one.   However, there were concerns about the practical utility of the method, given that AAP only improves performance for extreme compression ratios in which performance is already substantially degraded relative to unpruned models. I share these concerns, which mute the practical impact of this work. There were also concerns about a lack of proper baseline comparisons to more simple approaches to removing dead units. As mentioned by R1, given that the problem the study is an interesting one, the paper could make up for the lack of practical utility by providing detailed analyses of the settings in which dead units emerge, differences among pruning approaches, etc., but analyses provided here are limited.   I would encourage the authors to explore these areas in a future revision of the paper, but recommend that the paper be rejected in its current form. 
This paper presents a new approach, SlowMo, to improve communication efficient distribution training with SGD. The main method is based on the BMUF approach and relies on workers to periodically synchronize and perform a momentum update. This works well in practice as shown in the empirical results.   Reviewers had a couple of concerns regarding the significance of the contributions. After the rebuttal period some of their doubts were clarified. Even though they find that the solutions of the paper are an incremental extension of existing work, they believe this is a useful extension. For this reason, I recommend to accept this paper.
This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. TopicGAN operates in two steps: it first generates latent topics and produces bag of words corresponding to those latent topics. In the second step, the model generates text conditioning on those topic words.  Pros:  It combines the strength of topic models (interpretable topics that are learned unsupervised) with GAN for text generation.  Cons:  There are three major concerns raised by reviewers: (1) clarity, (2) relatively thin experimental results, and (3) novelty. Of these, the first two were the main concerns. In particular, R1 and R2 raised concerns about insufficient component wise evaluation (e.g., text classification from topic models) and insufficient GAN based baselines. Also, the topic model part of TopicGAN seems somewhat underdeveloped in that the model assumes a single topic per document, which is a relatively strong simplifying assumption compared to most other topic models (R1, R3). The technical novelty is not extremely strong in that the proposed model combines existing components together. But this alone would have not been a deal breaker if the empirical results were rigorous and strong.  Verdict: Reject. Many technical details require clarification and experiments lack sufficient comparisons against prior art.
The reviewers are concerned about the novelty of the proposed learning rate schedule, the rigor of the empirical validation, and the relationship between the results and the discussion of sharp vs. local minima. I invite the authors to incorporate reviewers  comments and resubmit to other ML venues.
The paper shows interesting and discussion inspiring results on multi agent trajectory prediction, as needed, for instance, in autonomous driving. Among the key technical ideas is a “conditional scene transformer” approach for flexible predictions for different agents.  Results on two  public benchmarks are impressive. Some reviewers are a bit torn about the significance of the technical contributions and the analyses of the results. Nevertheless, on average, the reviewers vote the paper to be above the acceptance threshold.
In this paper, the authors introduce a simple mixture of experts model, by greatly simplifying the routing mechanism: experts are randomly activated both at train and inference time. A consistency loss function is added for training the proposed models, enforcing all experts to make consistent predictions. The proposed method, called THOR, is evaluated on machine translation tasks, including multi lingual MT, and outperforms the recently proposed Switch Transformer MoE.  The reviews note that the paper is well written and easy to follow, and that the proposed method is simple. While the results look promising, the reviewers also raised concerns regarding comparisons to previous work, some of which were addressed in the rebuttal. Finally, a reviewer raised the concern that this method is related to ensembles, which work well for machine translation, but are not discussed or compared to. For these reasons, I believe that the paper is borderline, leaning toward acceptance.
This paper presents a software library for dealing with neural networks either in the (usual) finite limit or in the infinite limit. The latter is obtained by using the Neural Tangent Kernel theory.   There is variance in the reviewers  scores, however there has also been quite a lot of discussion, which has been facilitated by the authors  elaborate rebuttal. The main points in favor and against are clear: on the positive side, the library is demonstrated well (especially after rebuttal) and is equipped with desirable properties such as usage of GPU/TPU, scalability etc. On the other hand, a lot of the key insights build heavily on prior work of Lee et al, 2019. However, judging novelty when it comes to a software paper is more tricky to do, especially given that not many such papers appear in ICLR and therefore calibration is difficult. This has been discussed among reviewers.   It would help if some further theoretical insights were included in this paper; these insights could come by working backwards from the implementation (i.e. what more can we learn about infinite width networks now that we can experiment easily with them?).  Overall, this paper should still be of interest to the ICLR community. 
The main concern raised by the reviewers is limited experimental work, and there is no rebuttal.
The work presented in this submission is focused on a new approach for learning a model that can perform well at any point in time, and called Anytime Learning at Macroscale (ALMA). The algorithm processes data through a series of training batches, each of these processing steps being followed to a model evaluation. The total loss is the average (or sum) of the losses computed at each step.  Reviewers agreed that the paper is not ready for acceptance at ICLR 2022 as the presentation of the work lacks of clarity, especially w.r.t. to the similarities with online learning and the learning of streams of data, and the fundamental difference between small or moderate batch sizes and very large batches.
This paper presents a novel hierarchical reinforcement learning framework, based on learning temporal abstractions from past experience or expert demonstrations using recurrent variational autoencoders and regularising the representations.  This is certainly an interesting line of work, but there were two primary areas of concern in the reviews: the clarity of details of the approach, and the lack of comparison to baselines. While the former issue was largely dealt with in the rebuttals, the latter remained an issue for all reviewers.  For this reason, I recommend rejection of the paper in its current form.
The paper presents a graph neural network that represents the movements of electrons during chemical reactions, trained from a dataset to predict reactions outcomes.  The paper is clearly written, the comparisons are sensical. There are some concerns by reviewer 3 about the experimental results: in particular the lack of a simpler baseline, and the experimental variance. I think the some of the important concerns from reviewer 3 were addressed in the rebuttal, and I hope the authors will update the manuscript accordingly.  Overall, this is fitting for publication at ICLR 2019.
The paper makes its contribution by deriving an accelerated gradient flow for the Wasserstein distances. It is technically strong and demonstrates it applicability using examples fo Gaussian distributions and logistic regression.  Reviewer 3 provided a deep technical assessment, pointing out the relevance to our ML community since these ideas are not yet widespread, but had concerns about the clarity of the paper. Reviewer 2 had similar concerns about clarity, and was also positive about its relevance to the ML community. The authors provided details responses to the technical questions posed by the reviewers. The AC believes that such work is a good fit for the conference. The reviewers felts that this paper does not yet achieve the aim of making this work more widespread and needs more focus on communication.  This is a strong paper and the authors are encouraged to address the accessibility questions. We hope the review offers useful points of feedback for their future work.
The paper introduces a new formulation for learning low dimensional manifold representations via autoencoder mappings that are (locally) isometric by design. The key technical ingredient is the use of a particular (theoretically motivated) weight tied architecture coupled with isometry promoting loss terms that can be approximated via Monte Carlo sampling. Representative results on simple manifold learning experiments are shown in support of the proposed formulation.  The paper was generally well received; all reviewers appreciated the theoretical elements as well as the presentation of the ideas.  However, there were a few criticisms. First, the fact that the approach requires Monte Carlo sampling in very high dimensions automatically limits its scope. Second, the experiments seemed somewhat limited to simple (by ICLR standards) datasets. Third and most crucially, the approach lacks a compelling enough use case. It is not entirely clear what local isometry enables, beyond nice qualitative visualizations (and moreover, what the isometric autoencoder provides over other isometry preserving manifold learning procedures such as ISOMAP). Some rudimentary results are shown on k NN classification and linear SVMs, but the gains seem to be in the margins.   The authors are encouraged to consider the above concerns (and in particular, identifying a unique use case for isometric autoencoders) while preparing a future revision.
This paper focuses on the extrapolation ability of graph neural networks and proposes a new pooling function based on vector norm. The proposed method can be applied to replace the commonly used pooling function like max/mean/sum, and is proved able for extrapolation in a simple example.   Overall, all reviewers tend to reject this submission due to the following reasons   The contribution to this paper is incremental. It builds on top of the well known L p norm pooing function and extends it to allow negative values of p and an additional learnable parameter q.   However, this simple extension is not a well behaved function for gradient based optimization, which leads to unconvinced experiments, i.e., diverse performance compared with min/max.   more recent baselines should be compared with and it would be better to see how GNP works on state of the art model architectures on real world applications