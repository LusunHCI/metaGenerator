This paper endeavors to combine genetic evolutionary algorithms with subsampling techniques. As noted by reviewers, this is an interesting topic and the paper is intriguing, but more work is required to make it convincing (fairer baselines, more detailed / clearer presentation, ablation studies to justify the claims made int he paper). Authors are encouraged to strengthen the paper by following reviewers  suggestions.
The submission proposes two new things: a repulsive loss for MMD loss optimization and a bounded RBF kernel that stabilizes training of MMD GAN. The submission has a number of unsupervised image modeling experiments on standard benchmarks and shows reasonable performance. All in all, this is an interesting piece of work that has a number of interesting ideas (e.g. the PICO method, which is useful to know). I agree with R2 that the RBF kernel seems somewhat hacky in its introduction, despite working well in practice.  That being said, the repulsive loss seems like something the research community would benefit from finding out more about, and I think the experiments and discussion are sufficiently extensive to warrant publication.
This paper proposes an approach for learning to generate 3D views, using a surfel based representation, trained entirely from 2D images.  After the discussion phase, reviewers rate the paper close to the acceptance threshold.  AnonReviewer3, who initially stated "My second concern is the results are all on synthetic data, and most shapes are very simple", remains concerned after the rebuttal, stating "all results are on synthetic, simple scenes. In particular, these synthetic scenes don t have lighting, material, and texture variations, making them considerably easier than any types of real images."  The AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper.  Particularly relevant is the criticism that "While the paper is called ‘pix2scene’, it’s really about ‘pix2object’ or ‘pix2shape’." 
This paper shows a promising new variational objective for Bayesian neural networks. The new objective is obtained by effectively considering a functional prior on the parameters. The paper is well motivated and the mathematics are supported by theoretical justifications.   There has been some discussion regarding the experimental section. On one hand, it contains several real and synthetic data which show the good performance of the proposed method. On the other hand, the reviewers requested deeper comparisons with state of the art (deep) GP models and more general problem settings. The AC decided that the paper can be accepted with the experiments contained in the new revision, although the authors would be strongly encouraged to address the reviewers’ comments in a “non cosmetic manner (as R2 put it).  
 pros:   nicely written paper   clear and precise with a derivation of the loss function  cons:  novelty/impact: I think all the reviewers acknowledge that you are doing something different in the neural brainwashing (NB) problem than is done in the typical catastropic forgetting (CF) setting.  You have one dataset and a set of models with shared weights; the CF setting has one model and trains on different datasets/tasks.  But whereas solving the CF problem would solve a major problem of continual machine learning, the value of solving the NB problem is harder to assess from this paper...  The main application seems to be improving neural architecture search.  At the meta level, the techniques used to derive the main loss are already well known and the result similar to EWC, so they don t add a lot from the analysis perspective.  I think it would be very helpful to revise the paper to show a range of applications that could benefit from solving the NB problem and that the technique you propose applies more broadly.
The authors propose an efficient scheme for encoding sparse matrices which allow weights to be compressed efficiently. At the same time, the proposed scheme allows for fast parallelizable decompression into a dense matrix using Viterbi based pruning.  The reviewers noted that the techniques address an important problem relevant to deploying neural networks on resource constrained platforms, and although the work builds on previous work, it is important from a practical standpoint.  The reviewers noted a number of concerns on the initial draft of this work related to the experimental methodology and the absence of runtime comparison against the baseline, which the reviewers have since fixed in the revised draft. The reviewers were unanimous in recommending that the revision be accepted, and the authors are requested to incorporate the final changes that they said they would make in the camera ready version. 
This paper proposes a new batching strategy for training deep nets. The idea is to have the properties of sampling with replacement while reducing the chance of not touching an example in a given epoch. Experimental results show that this can improve performance on one of the tasks considered. However the reviewers consistently agree that the experimental validation of this work is much too limited. Furthermore the motivation for the approach should be more clearly established.
Reviewers mostly recommended to reject after engaging with the authors, with one reviewer slightly suggesting to accept, but with confidence 1. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit.
The reviewers appreciated this contribution, particularly its ability to tackle nonstationary domains which are common in real world tasks.   
This paper was reviewed by three experts. Initially, the reviews were mixed with several concerns raised. After the author response, there continue to be concerns about need for significantly more experiments. If this were a journal, it is clear that recommendation would be "major revision". Since that option is not available and the paper clearly needs another round of reviews, we must unfortunately reject. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue. 
Average score of 3.33, highest score of 4. The AC recommends rejection. 
The paper presents a novel architecture, reminescent of mixtures of experts, composed of a set of advocates networks providing an attention map to a separate "judge" network. Reviewers have several concerns, including lack of theoretical justification, potential scaling limitations, and weak experimental results. Authors answered to several of the concerns, which did not convinced reviewers. The reviewer with the highest score was also the least confident, so overall I will recommend to reject the paper.
* Strengths  The paper proposes a novel and interesting method for detecting adversarial examples, which has the advantage of being based on general “fingerprint statistics” of a model and is not restricted to any specific threat model (in contrast to much of the work in the area which is restricted to adversarial examples in some L_p norm ball). The writing is clear and the experiments are extensive.  * Weaknesses  The experiments are thorough. However, they contain a subtle but important flaw. During discussion it was revealed that the attacks used to evaluate the method fail to reduce accuracy even at large values of epsilon where there are simple adversarial attacks that should reduce the accuracy to zero. This casts doubt on whether the attacks at small values of epsilon really are providing a good measure of the method’s robustness.  * Discussion  There was substantial disagreement about the paper, with R1 feeling that the evaluation issues were serious enough to merit rejection and R3 feeling that they were not a large issue. In discussion with me, both R1 and R3 agreed that if an attack were demonstrated to break the method, that would be grounds for rejection. They also both agreed that there probably is an attack that breaks the method. A potential key difference is that R3 thinks this might be quite difficult to find and so merits publishing the paper to motivate stronger attacks.  I ultimately agree with R1 that the evaluation issues are indeed serious. One reason for this is that there is by now a long record of adversarial defense papers posting impressive numbers that are often invalidated within a short period (often less than a month or so) of the paper being published. The “Obfuscated Gradients” paper of Athalye, Carlini, and Wagner suggests several basic sanity checks to help avoid this. One of the sanity checks (which the present paper fails) is to test that attacks work when epsilon is large. This is not an arbitrary test but gets at a key issue any given attack provides only an *upper bound* on the worst case accuracy of a method. For instance, if an attack only brings the accuracy of a method down to 80% at epsilon 1 (when we know the true accuracy should be 0%), then at epsilon 0.01 we know that the measured accuracy of the attack comes 80% from the over optimistic accuracy at epsilon 1 and at most 20% from the true accuracy at epsilon 0.01. If the measured accuracy at epsilon 1 is close to 100%, then accuracy at lower values of epsilon provides basically no information. This means that the experiments as currently performed give no information about the true accuracy of the method, which is a serious issue that the authors should address before the paper can be accepted.
I appreciate that the authors are refuting a technical claim in Poole et al., however the paper has garnered zero enthusiasm the way it is written. I suggest to the authors that they rewrite the paper as a refutation of Poole et al., and name it as such.
The paper proposed to add the sliced Wasserstein distance between the distribution of the encoded training samples and a samplable prior distribution to the auto encoder (AE) loss, resulting in a model named sliced Wasserstein AE. The difference compared to the Wasserstein AE (WAE) lies in using the usage of the sliced Wasserstein distance instead of GAN or MMD based penalties. The idea of the paper is interesting, and a theoretical and an empirical analysis supporting the approach are presented. As reviewer 1 noticed, „the advantage of using sliced Wasserstein distance is twofold: 1)parametric free (compared to GANs); 2) almost hyperparameter free (compared to the MMD with RBF kernels), except setting the number of random projection bases.“ However, the empirical evaluation in the paper and concurrent ICLR submission on Cramer World AEs the authors refer to shows no clear practical advantage over the WAE, which leads to better results at least regarding the FID score. On the other hand, the Cramer World AE is based on the ideas presented in this paper (which was previously available on arxive) proving that the paper presents interesting ideas which are of value to the communty. Therefore, the paper is a bit boarderline, but I recommand to accept it. 
The paper contributes to the understanding of straight through estimation for single hidden layer neural networks, revealing advantages for ReLU and clipped ReLU over identity activations.  A thorough and convincing theoretical analysis is provided to support these findings.  After resolving various issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Valid criticisms of the presentation quality were raised during the review and response period, and the authors would be well served by continuing to improve the paper s clarity.
This paper proposes a new method for verifying whether a given point of a two layer ReLU network is a local minima or a second order stationary point and checks for descent directions. All reviewers agree that the algorithm is based on number of new techniques involving both convex and non convex QPs, and is novel. The method proposed in the paper has significant limitations as the method is not robust to handle approximate stationary points. Given these limitations, there is a disagreement between reviewers about the significance of the result . While I share the same concerns as R4, I agree with R3 and believe that the new ideas in the paper will inspire future work to extend the proposed method towards addressing these limitations. Hence I suggest acceptance. 
The reviewers agree the paper is not ready for publication at ICLR.
This paper formulates the recommendation as a model based reinforcement learning problem. Major concerns of the paper include: paper writing needs improvement; many decisions in experimental design were not justified; lack of sufficient baselines; results not convincing. Overall, this paper cannot be published in its current form. 
The paper proposes a new unsupervised learning scheme via utilizing local maxima as an indicator function.  The reviewers and AC note the novelty of this paper and good empirical justifications. Hence, AC decided to recommend acceptance.  However, AC thinks the readability of the paper can be improved.
The main novelty of the paper lies in using multiple noise vectors to reconstruct the high resolution image in multiple ways. Then, the reconstruction with minimal loss is selected and updated to improve the fit against the target image. The most important control experiment in my opinion should compare this approach against the same architecture with only with m 1 noise vector (i.e., using a constant noise vector all the time). Unfortunately, the paper does not include such a comparison, which means the main hypothesis of the paper is not tested. Please include this experiment in the revised version of the paper.  PS: There is another high level concern regarding the use of PSNR or SSIM for evaluation of super resolution methods. As shown by "Pixel recursive super resolution (Dahl et al.)" and others, PSNR and SSIM metrics are only relevant in the low magnification regime, in which techniques based on MSE (mean squared error) are very competitive. Maybe you need to consider large magnification regime in which GAN and normalized flow based models are more relevant.
This is an interesting paper that develops new techniques for analyzing the loss surface of deep networks, allowing the existence of spurious local minima to be established under fairly general conditions.  The reviewers responded with uniformly positive opinions.
This paper introduces a planning phase for NMT.  It first generates a discrete set of tags at decoding time, and then the actual words are generated conditioned on those tags.  The idea in the paper is interesting.  However, the paper s experimental settings could improve by comparing on larger datasets and also using stronger baselines.  The writing could also improve   why were only the few coarse POS tags used?  Have the authors tried a larger set?  I think without such controlled comparisons, it would be hard to understand why only those coarse tags are used.  The reviewers express concern about some of the above issues and there is consensus that the paper should be improved for acceptance at a venue like ICLR.
To borrow the succinct summary from R1, "the paper suggests a method for generating representations that are linked to goals  in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the  policies leading to them are similar." The reviewers and AC agree that this is a novel and worthy idea.  Concerns about the paper are primarily about the following. (i) the method already requires good solutions as input, i.e., in the form of goal conditioned policies, (GCPs) and the paper claims that these are easy to learn in any case. As R3 notes, this then begs the question as to why the actionable representations are needed. (ii) reviewers had questions regarding the evaluations, i.e., fairness of baselines, additional comparisons, and  additional detail.   After much discussion, there is now a fair degree of consensus.  While R1 (the low score) still has a remaining issue with evaluation, particularly hyperparameter evaluation, they are also ok with acceptance. The AC is of the opinion that hyperparameter tuning is of course an important issue, but does not see it as the key issue for this particular paper.  The AC is of the opinion that the key issue is issue (i), raised by R3. In the discussion, the authors reconcile the inherent contradiction in (i) based on the need of additional downstream tasks that can then benefit from the actionable representation, and as demonstrated in a number of the evaluation examples (at least in the revised version). The AC believes in this logic, but believes that this should be stated more clearly in the final paper. And it should be explained the extent to which training for auxiliary tasks implicitly solve this problem in any case.  The AC also suggests nominating R3 for a best reviewer award.
This paper proposes a method to learn representations to infer simple local models that can be used for policy improvement. All the reviewers agree that the paper has interesting ideas, but they found the main contribution to be a bit weak and the experiments to be insufficient.  Post rebuttal, the reviewers discussed extensively with each other and agreed that, given more work is done on a clear presentation and improving the experiments, this paper can be accepted. In its current form however, the paper is not ready to be accepted. I have recommended to reject this paper, but I will encourage the authors to resubmit after improving the work. 
This paper provides an interesting strategy for learning to explore, by first training on fully supervised data before deploying that policy to an online setting. There are some concerns, however, on the realism and utility of this setting that should be further discussed. If the offline data is not related to the contextual bandit problem, it would be surprising for this to have much benefit, and this should be better motivated and discussed. Because there are no theoretical guarantees for exploration, a discussion is needed and as suggested by a reviewer the learned exploration policies could be qualitatively examined. For example, the paper says "While these approaches are effective if the distribution of tasks is very similar and the state space is shared among different tasks, they fail to generalize when the tasks are different. Our approach targets an easier problem than exploration in full reinforcement learning environments, and can generalize well across a wide range of different tasks with completely unrelated features spaces." This is a pretty surprising statement, that your idea would not work well in an RL setting, but does work well in a contextual bandit setting.   There should also be a bit more discussion comparing to previous approach to learn how to explore, including in active learning. It is true that active learning is a different setting, but in both a goal is to become optimal as quickly as possible. Similarly, the ideas used for RL could be used here as well, essentially by setting gamma to 0.   Overall, the ideas here are interesting and well written, but need a bit more development on previous work, and motivation for why this approach will be effective. 
This paper proposes to approximate arbitrary conditional distribution of a pertained VAE using variational inferences. The paper is technically sound and clearly written. A few variants of the inference network are also compared and evaluated in experiments.  The main problems of the paper are as follows: 1. The motivation of training an inference network for a fixed decoder is not well explained. 2. The application of VI is standard, and offers limited novelty or significance of the proposed method. 3. The introduction of the new term cross coding is not necessary and does not bring new insights than a standard VI method.  The authors argued in the feedback that the central contribution is using augmented VI to do conditioning inference, similar to Rezende at al, but didn t address reviewers  main concerns. I encourage the authors to incorporate the reviewers  comments in a future revision, and explain why this proposed method bring significant contribution to either address a real problem or improve VI methodology.
The authors have proposed 3 continual learning variants which are all based on MNIST and which vary in terms of whether task ids are given and what the classification task is, and they have proposed a method which incorporates a symmetric VAE for generative replay with a class discriminator. The proposed method does work well on the continual learning scenarios and the incorporation of the generative model with the classifier is more efficient than keeping them separate. The discussion of the different CL scenarios and of related work is nice to read. However, the authors imply that these scenarios cover the space of important CL variants, yet they do not consider many other settings, such as when tasks continually change rather than having sharp boundaries. The authors have also only focused on the catastrophic forgetting aspect of continual learning, without considering scenarios where, e.g., strong forward transfer (or backwards transfer) is very important. Regarding the proposed architecture that combines a VAE with a softmax classifier for efficiency, the reviewers all felt that this was not novel enough to recommend publication.
This paper provides a number of interesting experiments for few shot learning using the CUB and miniImagenet datasets. One of the especially intriguing experiments is the analysis of backbone depth in the architecture, as it relates to few shot performance. The strong performance of the baseline and baseline++ are quite surprising. Overall the reviewers agree that this paper raises a number of questions about current few shot learning approaches, especially how they relate to architecture and dataset characteristics.  A few minor comments:   In table 1, matching nets are mistakenly attributed to Ravi and Larochelle. Should be Vinyals et al.   The notation for cosine similarity in section 3.2 is odd. It looks like you’re computing some cosine function of two vectors which doesn’t make sense. Please clarify this.   There are a few results that were promised after the revision deadline, please be sure to include these in the final draft. 
This paper provides an RL environment defined over Coq, allowing for RL agents and other such systems to to be trained to propose tactics during the running of an ITP. I really like this general line of work, and the reviewers broadly speaking did as well. The one holdout is reviewer 3, who raises important concerns about the need for further evaluation. I understand and appreciate their points, and I think the authors should be careful to incorporate their feedback not only in final revisions to the paper, but in deciding what follow on work to focus on. Nonetheless, and with all due respect to reviewer 3, who provided a review of acceptable quality, I am unsure the substance of their review merits a score as low as they have given. Considering the support the other reviews offer for the paper, I recommend acceptance for what the majority of reviewers believes is a good first step towards one day proving substantial new theorems using ITP ML hybrids.
With ratings of 6, 5 & 3 the numerical scores are just not strong enough to warrant acceptance. The author rebuttal was not able to sway opinions. 
 pros:   The paper presents an interesting forward chaining model which makes use of meta level expansions and reductions on predicate arguments in a neat way to reduce complexity.  As Reviewer 3 points out, there are a number of other papers from the neuro symbolic community that learn relations (logic tensor networks is one good reference there). However using these meta rules you can mix predicates of different arities in a principled way in the construction of the rules, which is something I haven t seen.   The paper is reasonably well written (see cons for specific issues)   There is quite a broad evaluation across a number of different tasks.  I appreciated that you integrated this into an RL setting for tasks like blocks world.   The results are good on small datasets and generalize well  cons:   (scalability) As both Reviewers 1 and 3 point out, there are scalability issues as a function of the predicate arity in computing the set of permutations for the output predicate computation.   (interpretability) As Reviewer 2 notes, unlike del ILP, it is not obvious how symbolic rules can be extracted.  This is an important point to address up front in the text.    (clarity) The paper is confusing or ambiguous in places:   Initially I read the 1,2,3 sequence at the top of 3 to be a deduction (and was confused) rather than three applications of the meta rules.  Maybe instead of calling that section "primitive logic rules" you can call them "logical meta rules".   Another confusion, also mentioned by reviewer 3 is that you are assuming that free variables (e.g. the "x" in the expression "Clear(x)") are implicitly considered universally quantified in your examples but you don t say this anywhere.  If I have the fact "Clear(x)" as an input fact, then presumably you will interpret this as "for all x Clear(x)" and provide an input tensor to the first layer which will have all 1.0 s along the "Clear" relation dimension, right?   It seems that you are making the assumption that you will never need to apply a predicate to the same object in multiple arguments?  If not, I don t see why you say that the shape of the tensor will be m x (m 1) instead of m^2.  You need to be able to do this to get reflexivity for example: "a <  a".   I think you are implicitly making the closed world assumption (CWA) and should say so.   On pg. 4 you say "The facts are tensors that encode relations among multiple objectives, as described in Sec. 2.2.".  What do you mean by "objectives"?  I would say the facts are tensors that encode relations among multiple objects.   On pg. 5 you say "We finish this subsection, continuing with the blocks world to illustrate the forward propagation in NLM".  I see no mention of blocks world in this paragraph. It just seems like a description of what happens at one block, generically.   In many places you say that this model can compute deduction on first order predicate calculus (FOPC) but it seems to me that you are limited to horn logic (rule logic) in which there is at most one positive literal per clause (i.e. rules of the form: b1 AND b2 AND ... AND bn  > h).  From what I can tell you cannot handle deduction on clauses such as b1 AND b2  > h1 or (h2 and h3).   There is not enough description of the exact setup for each experiment. For example in blocks world, how do you choose predicates for each layer?  How many exactly for each experiment?  You make it seem on p3 that you can handle recursive predicates but this seems to not have been worked out completely in the appendix.  You should make this clear.   In figure 1 you list Move as if its a predicate like On but it s a very different thing. On is  predicate describing a relation in one state.  Move is an action which updates a state by changing the values of predicates.  They should not be presented in the same way.   You use "min" and "max" for "and" and "or" respectively.  Other approaches have found that using the product t norm t norm(x,y)   x * y helps with gradient propagation.  del ILP discusses this in more detail on p 19.  Did you try these variations?   I think it would be helpful to somewhere explicitly describe the actual MLP model you use for deduction including layer sizes and activation functions.   p. 5. typo: "Such a parameter sharing mechanism is crucial to the generalization ability of NLM to problems ov varying sizes." ("ov"  > "of")   p. 6. sec 3.1 typo: "For ∂ILP, the set of pre conditions of the symbols is used direclty as input of the system." ("direclty"  > "directly")  I think this is a valuable contribution and novel in the particulars of the architecture (eg. expand/reduce) and am recommending acceptance.  But I would like to see a real effort made to sharpen the writing and make the exposition crystal clear.  Please in particular pay attention to Reviewer 3 s comments.  
The paper proposes an approach to hyperparameter tuning based on bilevel optimization, and demonstrates promising empirical results. Reviewer s concerns seem to be addressed well in rebuttals and extended version of the paper.
This paper proposes a hypothesis about the kinds of visual information for which popular neural networks are most selective.  It then proposes a series of empirical experiments on synthetically modified training sets to test this and related hypotheses.  The main conclusions of the paper are contained in the title, and the presentation was consistently rated as very clear.  As such, it is both interesting to a relatively wide audience and accessible.  Although the paper is comparatively limited in theoretical or algorithmic contribution, the empirical results and experimental design are of sufficient quality to inform design choices of future neural networks, and to better understand the reasons for their current behavior.  The reviewers were unanimous in their appreciation of the contributions, and all recommended that the paper be accepted.  
The reviewers raise an important issue about the parameters in the proposed gradient in Theorem 1. There could be different parameters for each policy in the gradient (though some parameter sharing could be possible), and computing this gradient would be prohibitive. The solution is to just use the most recent parameters, but then the gradients become off policy again without motivation for why this is acceptable. This approximation needs to be better justified.   As an additional point, there are other off policy policy gradient methods, than just DPG. The authors could consider comparing to these strategies (which can use replay buffers) and explain why the proposed strategy provides benefits beyond these. What is inadequate about these methods? Further motivation is needed for the proposed strategy. This is additionally true because the proposed strategy requires entire sampled trajectories for a fixed policy (to make the policy gradient sound, with weighting dpi_n(s)), whereas DPG and other off policy AC methods do not need that.
All reviewers agree in their assessment that this paper is not ready for acceptance into ICLR and the authors did not respond during the rebuttal phase.
There were several ambivalent reviews for this submission and one favorable one. Although this is a difficult case, I am recommending accepting the paper.  There were two main questions in my mind. 1. Did the authors justify that the limited neighborhood problem they try to fix with their method is a real problem and that they fixed it? If so, accept.  Here I believe evidence has been presented, but the case remains undecided.  2. If they have not, is the method/experiments sufficiently useful to be interesting anyway?  This question I would lean towards answering in the affirmative.  I believe the paper as a whole is sufficiently interesting and executed sufficiently well to be accepted, although I was not convinced of the first point (1) above. One review voting to reject did not find the conceptual contribution very valuable but still thought the paper was not severely flawed. I am partly down weighting the conceptual criticism they made. I am more concerned with experimental issues. However, I did not see sufficiently severe issues raised by the reviewers to justify rejection.  Ultimately, I could go either way on this case, but I think some members of the community will benefit from reading this work enough that it should be accepted.
This paper proves that gradient descent with random initialization converges to global minima for a squared loss penalty over a two layer ReLU network and arbitrarily labeled data. The paper has several weakness such as, 1) assuming top layer is fixed, 2) large number of hidden units  m , 3) analysis is for squared loss. Despite these weaknesses the paper makes a novel contribution to a relatively challenging problem, and is able to show convergence results without strong assumptions on the input data or the model. Reviewers find the results mostly interesting and have some concerns about the \lambda_0 requirement. I believe the authors have sufficiently addressed this issue in their response and  I suggest acceptance. 
This paper was reviewed by three experts. After the author response, R2 and R3 recommend rejecting this paper citing concerns of experimental evaluation and poor quality of the manuscript. All three reviewers continue to have questions for the authors, which the authors have not responded to. The AC finds no basis for accepting this paper in this state. 
This paper attempts at ranking of tasks handled by deep learning methods based on learning curves.  A main premise of the paper is "fitting learning curves to a power law, and then sorting tasks by empirical estimates of exponents".   The idea of the paper is quite interesting.  However, the paper makes some bold claims which are a bit distant from the empirical study it conducts.  It is hard to line up the order in Table 2 with the Chomsky hierarchy.  Also, for various tasks, various different deep models are used (ResNets for image classification, LSTMs for LM, and so on).  I was not convinced that the beta parameter is model agnostic.  Similar concerns are expressed by the reviewers, and they agree that the paper should address the criticism that they express in their feedback.
Reviewers all agree that this is a strong submission. I also believe it is interesting that only by changing the geometry of embeddings, they can use the space more efficiently without increasing the number of parameters.
The paper conveys interesting idea but need more work in terms of fair empirical study and also improvement of the writing. The AC based her summary only on the technical argumentation presented by reviewers and authors. 
This paper proposes to compress the deep learning model using both activation pruning and weight pruning. The reviewers have a consensus on rejection due to lack of novelty. 
This paper explores an approach to testing the information bottleneck hypothesis of deep learning, specifically the idea that layers in a deep model successively discard information about the input which is irrelevant to the task being performed by the model, in full scale ResNet models that are too large to admit the more standard binning based estimators used in other work. Instead, to lower bound I(x;h), the authors propose using the log likelihood of a generative model (PixelCNN++). They also attempt visualize what sort of information is lost and what is retained by examining PixelCNN++ reconstructions from the hidden representation at different positions in a ResNet trained to perform image classification on the CINIC 10 task. To lower bound I(y;h), they perform classification. In the experiments, the evolution of the bounds on I(x;h) and I(y;h) are tracked as a function of training epoch, and visualizations (reconstructions of the input) are shown to support the argument that color invariance and diversity of samples increases during the compression phase of training. These tests are done on models trained to perform either image classification or autoencoding. This paper enjoyed a good discussion between the reviewers and the authors. The reviewers liked the quantitative analysis of "usable information" using PixelCNN++, though R2 wanted additional experiments to better quantify the limitations of the PixelCNN++ model to provide the reader with a better understanding of plots in Fig. 3, as well as more points sampled during training. Both R2 and R3 had reservations about the qualitative analysis based on the visualizations, which constitute the bulk of the paper. Unfortunately, the PixelCNN++ training is computationally intensive enough that these requests could not be fulfilled during the ICLR discussion phase. While the AC recommends that this submission be rejected from ICLR, this is a promising line of research. The authors should address the constructive suggestions of R2 and R3 and submit this work elsewhere.
This work is novel, reasonably clearly written with a thorough literature survey. The proposed approach also empirically seems promising. The paper could be improved with a bit more discussion about the sensitivity, particularly as a two timescale approach can be more difficult to tune.
This paper combines Prolog like reasoning with distributional semantics, applied to natural language question answering. Given the importance of combining neural and symbolic techniques, this paper provides an important contribution. Further, the proposed method complements standard QA models as it can be easily combined with them.  The reviewers and AC note the following potential weaknesses: (1) The evaluation consisted primarily on small subsets of existing benchmarks,  (2) the reviewers were concerned that the handcrafted rules were introducing domain information into the model, and (3) were unconvinced that the benefits of the proposed approach were actually complementary to existing neural models.   The authors addressed a number of these concerns in the response and their revision. They discussed how OpenIE affects the performance, and other questions the reviewers had. Further, they clarified that the rule templates are really high level/generic and not "prior knowledge" as the reviewers had initially assumed. The revision also provided more error analysis, and heavily edited the paper for clarity. Although these changes increased the reviewer scores, a critical concern still remains: the evaluation is not performed on the complete question answering benchmark, but on small subsets of the data, and the benefits are not significant. This makes the evaluation quite weak, and the authors are encouraged to identify appropriate evaluation benchmarks.   There is disagreement in the reviewer scores; even though all of them identified the weak evaluation as a concern, some are more forgiving than others, partly due to the other improvements made to the paper. The AC, however, agrees with reviewer 2 that the empirical results need to be sound for this paper to have an impact, and thus is recommending a rejection. Please note that paper was incredibly close to an acceptance, but identifying appropriate benchmarks will make the paper much stronger.
 pros:   interesting application of graph networks for relational inference in MARL, allowing interpretability and, as the results show, increasing performance   better learning curves in several games   somewhat better forward prediction than baselines  cons:   perhaps some lingering confusion about the amount of improvement over the LSTM+MLP baseline  Many of the reviewer s other issues have been addressed in revision and I recommend acceptance.
The paper trains a classifier to decide if a program is a malware and when to halt its execution. The malware classifier is mostly composed of an RNN acting on featurized API calls (events). The presentation could be improved. The results are encouraging, but the experiments lack solid baselines, comparisons, and grounding of the task usefulness, as this is not done on an established benchmark.
The paper introduces a new dataset that contains multiple landings from the X plane simulator, and each includes readings from multiple sensors for aircraft landing. The  paper also trains a set of self supervised methods presented in previous works in order to learn sensory representations, and evaluates the learnt representations in terms of disentanglement and re purposing to a discriminative task.   Though the evaluations presented are interesting, they are not convincingly useful, as noted by the reviewers. Overall, it is not clear why this dataset is particularly well suited for representation learning. Furthermore, it is difficult to evaluate representation learning methods without relating them to an end task, e.g., that of landing the aircraft.  The paper writing would also benefit from restructuring and improving on English expressions. In particular, the conclusion section contains half finished sentences. 
The reviewers all feel that the paper should be accepted to the conference.  The main strengths that they noted were the quality of writing, the wide applicability of the proposed method and the strength of the empirical evaluation.  It s nice to see experiments across a large number of problems (100), with corresponding code, where baselines were hyperparameter tuned as well.  This helps to give some assurance that the method will generalize to new problems and datasets.    Some weaknesses noted by the reviewers were computational cost (the method is significantly slower than the baselines) and they weren t entirely convinced that having more concise representations would directly lead to the claimed interpretability of the approach.  Nevertheless, they found it would make for a solid contribution to the conference.
As all the reviewers have highlighted, there is some interesting analysis in this paper on understanding which models can be easier to complete. The experiments are quite thorough, and seem reproducible. However, the biggest limitation and the ones that is making it harder for the reviewers to come to a consensus is the fact that the motivation seems mismatched with the provided approach. There is quite a lot of focus on security, and being robust to an adversary. Model splitting is proposed as a reasonable solution. However, the Model Completion hardness measure proposed is insufficiently justified, both in that its not clear what security guarantees it provides nor is it clear why training time was chosen over other metrics (like number of samples, as mentioned by a reviewer). If this measure had been previously proposed, and the focus of this paper was to provide empirical insight, that might be fine, but that does not appear to be the case. This mismatch is evident also in the writing in the paper. After the introduction, the paper largely reads as understanding how retrainable different architectures are under which problem settings, when replacing an entire layer, with little to no mention of security or privacy.   In summary, this paper has some interesting ideas, but an unclear focus. The proposed strategy should be better justified. Or, maybe even better for the larger ICLR audience, the provided analysis could be motivated for other settings, such as understanding convergence rates or trainability in neural networks.
This paper proposes a novel framework for tractably learning non eucliean embeddings that are product spaces formed by hyperbolic, spherical, and Euclidean components, providing a heterogenous mix of curvature properties.  On several datasets, these product space embeddings outperform single Euclidean or hyperbolic spaces. The reviewers unanimously recommend acceptance.
This  paper presents an LLE based unsupervised feature selection approach. While one of the reviewers has acknowledged that the paper is well written with clear mathematical explanations of the key ideas, it also lacks a sufficiently strong theoretical foundation as the authors have acknowledged in their responses; as well as novelty in its tight connection to LLE. When theoretical backbone is weak, the role of empirical results is paramount, but the paper is not convincing in that regard.
The paper presents a novel problem formulation, that of generating 3D object shapes based on their functionality. They use a dataset of 3d shapes annotated with functionalities to learn a voxel generative network that conditions on the desired functionality to generate a voxel occupancy grid. However, the fact that the results are not very convincing  resulting 3D shapes are very coarse  raises questions regarding the usefulness of the proposed problem formulation.  Thus, the problem formulation novelty alone is not enough for acceptance. Combined with a motivating application to demonstrate the usefulness of the problem formulation and results, would make this paper a much stronger submission. Furthermore, the authors have greatly improved the writing of the manuscript during the discussion phase.
The authors have proposed a new method for exploration that is related to parameter noise, but instead uses Gaussian dropout across entire episodes, thus allowing for temporally consistent exploration. The method is evaluated in sparsely rewarded continuous control domains such as half cheetah and humanoid, and compared against PPO and other variants. The method is novel and does seem to work stably across the tested tasks, and simple exploration methods are important for the RL field. However, the paper is poorly and confusingly written and really really needs to be thoroughly edited before the camera ready deadline. There are many approaches which are referred to without any summary or description, which makes it difficult to read the paper. The three reviewers all had low confidence in their understanding of the paper, which makes this a very borderline submission even though the reviewers gave relatively high scores. 
The paper proposes learning a hash function that maps high dimensional data to binary codes, and uses multi index hashing for efficient retrieval. The paper discusses similar results to "Similarity estimation techniques from rounding algorithms, M Charikar, 2002" without citing this paper. The proposed learning idea is also similar to "Binary Reconstructive Embedding, B. Kulis, T. Darrell, NIPS 09" without citation. Please study the learning to hash literature and discuss the similarities and differences with your approach.  Due to missing citations and lack of novelty, I believe the paper does not pass the bar for acceptance at ICLR.   PS: PQ and its better variants (optimized PQ and cartesian k means) are from a different family of quantization techniques as pointed out by R3 and multi index hashing is not directly applicable to such techniques. Regardless, I am also surprised that your technique just using hamming distance is able to outperform PQ using lookup table distance.  
 This paper introduces conditional graph neural fields, an approach that combines label compatibility scoring of conditional random fields with deep neural representations of nodes provided by graph convolutional networks. The intuition behind the proposed work is promising, and the results are strong.  The reviewers and the AC note the following as the primary concerns of the paper: (1) The novelty of this work is limited, since a number of approaches have recently combined CRFs and neural networks, and it is unclear whether the application of those ideas to GCNs is sufficiently interesting, (2) the losses, especially EBM, and the use of greedy/beam search inference was found to be quite simple, especially given these have been studied extensively in the literature, and (3) analysis and adequate discussion of the results is missing (only a single table of numbers is provided). Amongst other concerns, the reviewers identified issues with writing quality, lack of clear motivation for CRFs, and the selection of the benchmarks.  Given the feedback, the authors responded with comments, and a revision that removes the use of EBM loss from the paper, which the reviewers appreciated. However, most of the concerns remain unaddressed. Reviewer 2 maintains that CRFs+NNs still need to be motivated better, since hidden representations already take the neighborhood into account, as demonstrated by the fact that CRF+NNs are not state of art in other applications. Reviewer 2 also points out the lack of a detailed analysis of the results. Reviewer 2 focuses on the simplicity of the loss and inference algorithms, which is also echoed by reviewer 2 and reviewer 1. Finally, reviewer 1 also notes that the datasets are quite simple, and not ideal evaluation for label consistency given most of them are single label (and thus need only few transition probabilities).  Based on this discussion, the reviewers and the AC agree that the paper is not ready for acceptance.
despite the (significant) improvement in language modelling, it has always been a thorny issue whether better language models (at this level) lead to better performance in the downstream task or whether such a technique could be used to build a better conditional language model which often focuses on the aspect of generation. in this context, the reviewers found it difficult to see the merit of the proposed approach, as the technique itself may be considered a rather trivial application of earlier approaches such as truncated backprop. it would be good to apply this technique to e.g. document level generation and see if the proposed approach can strike an amazing balance between computational efficiency and generation performance.
This paper developed an accelerated gradient flow in the space of probability measures. Unfortunately, the reviewers think the practical usefulness of the proposed approach is not sufficiently supported by realistic experiments, and the clarity of the paper need to be significantly improved. The authors  rebuttal resolved some of the confusion the reviewers had, but we believe further substantial improvement will make this work a much stronger contribution. 
This paper extends the DiCE estimator with a better control variate baseline for variance reduction.  The reviewers all think the paper is fairly clear and well written. However, as the reviews and discussion indicates,  there are several critical issues, including lack of explanation of the choice of baseline, the lack more realistic experiments and a few misleading assertions.  We encourage the authors to rewrite the paper to address these criticism. We believe this work will make a successful submission with proper modification in the future.  
Two of the reviewers raised their scores during the discussion phase noting that the revised version was clearer and addressed some of their concerns.  As a result, all the reviewers ultimately recommended acceptance.  They particularly enjoyed the insights that the authors shared from their experiments and appreciated that the experiments were quite thorough.  All the reviewers mentioned that the work seemed somewhat incremental, but given the results, insights and empirical evaluation decided that it would still be a valuable contribution to the conference.  One reviewer added feedback about how to improve the writing and clarity of the paper for the camera ready version.
This work proposes a method for both instance and feature based transfer learning.  The reviewers agree that the approach in current form lacks sufficient technical novelty for publication. The paper would benefit from experiments on larger datasets and with more analysis into the different aspects of the proposed model.  
The paper has some nice ideas for efficient exploration, but reviewers think more work is needed before it is ready for publication.  In particular, the paper should have an improved discussion of state of the art work on exploration, compare the difference and benefits of the proposed approach, and then conduct proper experiments to validate the claims.
Several reviewers thought the results were not surprising in light of existing universality results, and thought the results were of limited relevance, given that the formalization is not quite in line with real world networks for MIL. The authors draw out some further justifications in the rebuttal. These should be reintegrated. I agree with the general criticisms regarding relevance to ICLR. Ultimately, this work may belong in a journal.
 pros:   well written and clear   good evaluation with convincing ablations   moderately novel  cons:   Reviewers 1 and 3 feel the paper is somewhat incremental over previous work, combining previously proposed ideas.  (Reviewer 2 originally had concerns about the testing methodology but feels that the paper has improved in revision) (Reviewer 3 suggests an additional comparison to related work which was addressed in revision)  I appreciate the authors  revisions and engagement during the discussion period.  Overall the paper is good and I m recommending acceptance.
The paper proposes a set of tricks leading to a new SOTA for sampling high resolution images. It is clearly written and the presented contribution will be of high interest for practitioners.
A well written paper that proposes an original approach for leaning a structured prior for VAEs, as a latent tree model whose structure and parameters are simultaneously learned. It describes a well principled approach to learning a multifaceted clustering, and is shown empirically to be competitive with other unsupervised clustering models.  Reviewers noted that the approach reached a worse log likelihood than regular VAE (which it should be able to find as a special case), hinting towards potential optimization difficulties (local minimum?). This would benefit form a more in depth analysis.  But reviewers appreciated the gain in interpretability and insights from the model, and unanimously agreed that the paper was an interesting novel contribution worth publishing. 
although the idea is a straightforward extension of the usual (flat) attention mechanism (which is positive), it does show some improvement in a series of experiments done in this submission. the reviewers however found the experimental results to be rather weak and believe that there may be other problems in which the proposed attention mechanism could be better utilized, despite the authors  effort at improving the result further during the rebuttal period. this may be due to a less than desirable form the initial submission was in, and when the new version with perhaps a new set of more convincing experiments is reviewed elsewhere, it may be received with a more positive attitude from the reviewers.
there is a disagreement among the reviewers, and i am siding with the two reviewers (r1 and r3) and agree with r3 that it is rather unconventional to pick learning to learn to experiment with modelling variable length sequences (it s not like there s no other task that has this characteristics, e.g., language modelling, translation, ...) 
All of the reviewers agree that this is a well written paper with the novel perspective of minimizing energy consumption in neural networks, as opposed to maximizing sparsity, which does not always correlate with energy cost. There are a number of promised clarifications and additional results that have emerged from the discussion that should be put into the final draft. Namely, describing the overhead of converting from sparse to dense representations, adding the Imagenet sparsity results, and adding the time taken to run the projection step.
The paper proposes to use a convolutional/de convolutional Q function over on screen goal locations, and applied to the problem of structured exploration. Reviewers pointed out the similarity to the UNREAL architecture, the difference being that the auxiliary Q functions learned are actually used to act in this case.  Reviewers raised concerns regarding novelty, the formality of the writing, a lack of comparisons to other exploration methods, and the need for ground truth about the sprite location at training time. A minor revision to the text was made, but the reviewers did not feel their main criticisms were addressed. While the method shows promise, given that the authors acknowledge that the method is somewhat incremental, a more thorough quantitative and ablative study would be necessary in order to recommend acceptance.
The paper provides a novel analysis of the robustness to adversarial attacks in network representation learning. It appears to be a useful contribution for important class of models; however,  the detailed reviews (1 and 2) raise some concerns that may require a bit of further work (though partially addressed in revised version).
The submission proposes a strategy for quantization of neural networks with skip connections that quantizes only the convolution paths, while leaving the skip paths at full precision.  The approach can save computation through compressing the convolution kernels, while spending more on the skip connections. Empirical results show improved performance at 2 bit quantization compared to a handful of competing methods.  Figure 5 provides some interpretation of why the method might be working in terms of "smoothness" of the loss surface (term not used in the traditional mathematical sense).  The paper seems to focus too much on selling the name "precision highway" rather than providing proper definitions of their strategy (a definition block would be a good first step), and there is little mathematical analysis of the consequences of the chosen approach. There are concerns about the novelty of the method, specifically compared to Liu et al. (2018) and Choi et al. (2018b), which propose approximately the same strategy.  Footnote 1 claims that these works were conducted in parallel with the current submission, but it is unambiguously the case that Choi et al appeared on arXiv in May, and Liu et al. appeared in ECCV 2018 and on arXiv more than 30 days before the ICLR deadline, and can fairly be considered prior work https://iclr.cc/Conferences/2019/Reviewer_Guidelines  The reviewer scores were on aggregate borderline for the ICLR acceptance threshold.  On the balance, the paper seems to fall under the threshold due to insufficient novelty and analysis of the method. 
This paper proposes a self attention based approach for learning representations for the vertices of a dynamic graph, where the topology of the edges may change. The attention focuses on representing the interaction of vertices that have connections. Experimental results for the link prediction task on multiple datasets demonstrate the benefits of the approach. The idea of attention or its computation is not novel, however its application for estimating embeddings for dynamic graph vertices is new. The original version of the paper did not have strong baselines as noted by multiple reviewers, but the paper was  revised during the review period. However, some of these suggestions, for example, experiments with larger graph sizes and other related work i.e., similar work on static graphs are left as a future work.
The paper proposes an interesting idea for more effective imitation learning.  The idea is to include short actions sequences as labels (in addition to the basic actions) in imitation learning.  Results on a few Atari games demonstrate the potential of this approach.  Reviewers generally like the idea, think it is simple, and are encouraged by its empirical support.  That said, the work still appears somewhat preliminary in the current stage: (1) some reviewer is still in doubt about the chosen baseline; (2) empirical evidence is all in the similar set of Atari games   how broadly is this approach applicable?
The aim of this paper is to interpret various optimizers such as RMSprop, Adam, and NAG, as approximate Kalman filtering of the optimal parameters. These algorithms are derived as inference procedures in various dynamical systems. The main empirical result is the algorithms achieve slightly better test accuracy on MNIST compared to an unregularized network trained with Adam or RMSprop.  This was a controversial paper, and each of the reviewers had a significant back and forth with the authors. The controversy reflects that this is a pretty interesting and relevant topic: a proper Bayesian framework could provide significant guidance for developing better optimizers and regularizers. Unfortunately, I don t think this paper delivers on its promise of a unifying Bayesian framework for these various methods, and I don t think it s quite ready for publication at ICLR.  There was some controversy about relationships to various recently published papers giving Bayesian interpretations of optimizers. The authors believe the added value of this submission is that it recovers features such as momentum and root mean square normalization. This would be a very interesting contribution beyond those works. But R2 and R3 feel like these particular features were derived using fairly ad hoc assumptions or approximations almost designed to obtain existing algorithms, and from reading the paper I have to say I agree with the reviewers.  There was a lot of back and forth about the correctness of various theoretical claims. But overall, my impression is that the theoretical arguments in this paper exceed the bar for a primarily practical/empirical paper, but aren t rigorous enough for the paper to stand purely based on the theoretical contributions.   Unfortunately, the empirical part of the paper is rather lacking. The only experiment reported is on MNIST, and the only result is improved test error. The baseline gets below 99% test accuracy, below the level achieved by the original LeNet, suggesting the baseline may be somehow broken. Simply measuring test error doesn t really get at the benefits of Bayesian approaches, as it doesn t distinguish it from the many other regularizers that have been proposed. Since the proposed method is nearly identical to things like Adam or NAG, I don t see any reason it can t be evaluated on more challenging problems (as reviewers have asked for).   Overall, while I find the ideas promising, I think the paper needs considerable work before it is ready for publication at ICLR. 
This is a tough choice as it is a reasonably strong paper. I am similar to another reviewer quite confused how this graph matching can "only focus on important nodes in the graph" This seems counter intuitive and the only reason given in the rebuttal is that other people have done it also..  Relatedly: "In graph matching, we not only care about the overall similarity of two graphs but also are interested in finding the correspondence between the nodes of two graphs"  I am sorry for the authors and hope they will get it accepted at the next conference.
This paper proposes a GAN model to synthesize raw waveform audio by adapting the popular DC GAN architecture to handle audio signals. Experimental results are reported on several datasets, including speech and instruments.   Unfortunately this paper received two low quality reviews, with little signal. The only substantial review was mildly positive, highlighting the clarity, accessibility and reproducibility of the work, and expressing concerns about the relative lack of novelty. The AC shares this assessment. The paper claims to be the first successful GAN application operating directly on wave forms. Whereas this is certainly an important contribution, it is less clear to the AC whether this contribution belongs to a venue such as ICLR, as opposed to ICASSP or Ismir.  This is a borderline paper, and the decision is ultimately relative to other submissions with similar scores. In this context, given the mainstream popularity of GANs for image modeling, the AC feels this paper can help spark significant further research in adversarial training for audio modeling, and therefore recommends acceptance. I also encourage the authors to address the issues raised by R1.  
This paper analyzes local SGD optimization for strongly convex functions, and proves that local SGD enjoys a linear speedup (in the number of workers and minibatch size) over vanilla SGD, while also communicating less than distributed mini batch SGD. A similar analysis is also provided for the asynchronous case, and limited empirical confirmation of the theory is provided. The main weakness of the current revision is that it does not yet properly relate this work to two prior publications: Dekel et al., 2012 (https://arxiv.org/pdf/1012.1367.pdf) and Jain et al., 2016 (https://arxiv.org/abs/1610.03774). It is critical that these references and suitable discussion be added in the camera ready paper, since this issue was the subject of considerable discussion and the authors promised to include the references and discussion in the final paper.
This paper suggests a method for defending against adversarial examples and out of distribution samples via projection onto the data manifold. The paper suggests a new method for detecting when hidden layers are off of the manifold, and uses auto encoders to map them back onto the manifold.   The paper is well written and the method is novel and interesting. However, most of the reviewers agree that the original robustness evaluations were not sufficient due to restricting the evaluation to using FGSM baseline and comparison with thermometer encoding (which both are known to not be fully effective baselines).   After rebuttal, Reviewer 4 points out that the method offers very little robustness over adversarial training alone, even though it is combined with adversarial training, which suggests that the method itself provides very little robustness. 
This paper provides a theoretical analysis of GANs, showing its advantages when the measure satisfies the disconnected support property. Its main theoretical results are interesting, but the reviews and discussion shows some misleading places.  It was also found some of the claims and proof are not mathematically rigorous. 
The paper extends MAML so that a learned behavior can be quickly (sample efficiently) adapted to a new agend (allied or opponent). The approach is tested on two simple tasks in 2D gridworld environments: chasing and path blocking.  The experiments are very limited, they do not suffice to support the claims about the method. The authors did not enter a rebuttal and all the reviewers agree that the paper is not good enough for ICLR.
This paper examines ways of encoding structured input such as source code or parsed natural language into representations that are conducive for summarization. Specifically, the innovation is to not use only a sequence model, nor only a tree model, but both. Empirical evaluation is extensive, and it is exhaustively demonstrated that combining both models provides the best results.  The major perceived issue of the paper is the lack of methodological novelty, which the authors acknowledge. In addition, there are other existing graph based architectures that have not been compared to.  However, given that the experimental results are informative and convincing, I think that the paper is a reasonable candidate to be accepted to the conference.
. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The proposed method is novel and effective   The paper is clear and the experiments and literature review are sufficient (especially after revision).   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.  The original weaknesses (mainly clarity and missing details) were adequately addressed in the revisions.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted.
The paper studies population based training for MARL with co play, in MuJoCo (continuous control) soccer. It shows that (long term) cooperative behaviors can emerge from simple rewards, shaped but not towards cooperation.  The paper is overall well written and includes a thorough study/ablation. The weaknesses are the lack of strong comparisons (or at least easy to grasp baselines) on a new task, and the lack of some of the experimental details (about reward shaping, about hyperparameters).  The reviewers reached an agreement. This paper is welcomed to be published at ICLR.
This paper presents good empirical results on an important and interesting task (translation between several language pairs with a single model). There was solid communication between the authors and the reviewers leading to an improved updated version and consensus among the reviewers about the merits of the paper.
This paper proposes a generalization of the translation style embedding approaches for link prediction to Riemannian manifolds. The reviewers feel this is an important contribution to the recent work on embedding graphs into non Euclidean spaces, especially since this work focuses on multi relational links, thus supporting knowledge graph completion. The results on WN11 and FB13 are also promising.  The reviewers and AC note the following potential weaknesses: (1) the primary concern is the low performance on the benchmarks, especially WN18 and FB15k, and not using the appropriate versions (WN18 RR and FB15k 237), (2) use of hyperbolic embedding for an entity shared across all relations, and (3) lack of discussion/visualization of the learned geometry.  During the discussion phase, the authors clarified reviewer 1 s concern regarding the difference in performance between HolE and ComplEx, along with providing a revision that addressed some of the clarity issues raised by reviewer 3. The authors also justified the lower performance due to (1) they are focusing on low dimensionality setting, and (2) not all datasets will fit the space of the proposed model (like FB15k). However, reviewers 2 and 3 still maintain that the results provide insufficient evidence for the need for Riemannian spaces over Euclidean ones, especially for larger, and more realistic, knowledge graphs.  The reviewers and the AC agree that the paper should not be accepted in the current state. 
Positives:  The paper proposes an interesting idea: to study the effect on vulnerability to adversarial attacks of training for invariance with respect to rotations. Experiments on MNIST, FashionMNIST, and CIFAR10. An interesting hypothesis partially borne out in experiments.  Negatives:  no accept recommendation from any reviewer insufficient empirical results not a clear enough message very limited theoretical contribution  Although additional experimental results on FashionMNIST and CIFAR10 were added to the initial very limited results on MNIST, the main claim of the paper seems to be somewhat weakened.  The effect of increased vulnerability to adversarial attacks as invariance is increased is less pronounced on the additional datasets.  This calls into question how relevant this effect is on more realistic data than the toy problems considered here.  The size of the network is not varied in the experiments.  If increased invariance results in poorer performance with respect to attacks, one possible explanation is that the invariance taxes the capacity of the network architecture.  Varying architecture depth could partially answer whether this is relevant.  Given the lack of theoretical contribution, more insights along these lines would potentially strengthen the work.  The title uses the term "equivariance," which strictly speaking is when the inputs and outputs of a function vary equally, e.g. an image and its segmentation are equivariant under rotations, but classification tasks should probably be called "invariant."  The reviewers were unanimous in not recommending the paper for acceptance.  The key concerns remain after the author response. 
This paper presents "BabyAI", a research platform to support grounded language learning. The platform supports a suite of 19 levels, based on *synthetic* natural language of increasing difficulties. The platform uniquely supports simulated "human in the loop" learning, where a human teacher is simulated as a heuristic expert agent speaking in synthetic language.     Pros: A new platform to support grounded natural language learning with 19 levels of increasing difficulties. The platform also supports a heuristic expert agent to simulate a human teacher, which aims to mimic "human in the loop" learning. The platform seems to be the result of a substantial amount of engineering, thus nontrivial to develop. While not representing the real communication or true natural language, the platform is likely to be useful for DL/RL researchers to perform prototype research on interactive and grounded language learning.   Cons: Everything in the presented platform is based on synthetic natural language. While the use of synthetic language is not entirely satisfactory, such limit is relatively common among the simulation environments available today, and lifting that limitation is not straightforward. The primary contribution of the paper is a new platform (resource). There are no insights or methods.  Verdict: Potential weak accept. The potential impact of this work is that the platform will likely be useful for DL/RL research on interactive and grounded language learning.
This paper studies the behavior of weight parameters for linear networks when trained on separable data with strictly decreasing loss functions. For this setting the paper shows that the gradient descent solution converges to max margin solution and each layer converges to a rank 1 matrix with consequent layers aligned.  All reviewers agree that the paper provides novel results for understanding implicit regularization effects of gradient descent for linear networks. Despite the limitations of this paper such as studying networks with linear activation, studying gradient descent not with practical step sizes, assuming data is linearly separable, reviewers find the results useful and a good addition to existing literature.
although the way in which the authors characterize existing rnn variants and how they derive a new type of rnn are interesting, the submission lacks justification (either empirical or theoretical) that supports whether and how the proposed rnn s behave in a "learning" setting different from the existing rnn variants.
Strengths of the paper:  Based on previous work suggesting that radial basis features can help defend against adversarial attacks, the paper proposes a concrete method for incorporating them in deep networks.  The paper evaluates the method on multiple datasets, including MNIST and  ISBI International Skin Imaging Collaboration (ISIC) Challenge.  Weaknesses:  Reviewers 2 and 3 felt that the paper was not clearly written, and cited several concrete questions about the method that could not be understood from the paper.  There were additional concerns of lacking comparison to existing methods, and Reviewer 1 pointed out that a competing method gave higher performance, although this was not reported in the present submission.  Points of contention:  The authors did not provide a response to the reviewer concerns.  Consensus:  All reviewers recommended that the paper be rejected, and the authors did not provide a rebuttal.
The reviewers generally had concerns that the goal of recovering only the model architecture was unmotivated (given that knowing the architecture is not a large threat on its own, and there are existing attacks that work without knowledge of the model architecture). Moreover, given the strength of the assumed attack model, recovering model architecture is a fairly unambitious goal (again, more serious attacks have already been demonstrated under weaker attack models). Finally, though less seriously, the analysis is fairly preliminary, e.g. it is unclear if the attack can generalize to nearby architectures that were outside the training set.
This paper addresses an important topic and was generally well written. However, reviewers pointed out serious issues with the evaluation (using weak or poorly chosen attacks), and some conceptual confusions (e.g. conflating adversarial examples with out of distribution examples, unsubstantiated claim that adversarial examples lie off the data manifold).
Because of strong support from two of the reviewers I am recommending accepting this paper. However, I believe reviewer 1 s concerns should be taken seriously. Although I disagree with the reviewer that a general "framework" method is a bad thing, I agree with them that additional experiments would be valuable.
Reviewers have concerns about poor writing of the paper, lack of technical novelty, and the methodology taken by the paper not being very principled. 
This paper attempts at modeling coherence of generated text, and proposes two kinds of discriminators that tries to measure whether a piece of text is coherent or not.  However, the paper misses several related critical references, and also lacks extensive evaluation (especially manual evaluation).  There is consensus between the reviewers that this paper needs more work before it is accepted to a conference such as ICLR. 
This paper addresses important general questions about how linear classifiers use features, and about the transferability of those features across tasks. The paper presents a specific new analysis method, and demonstrates it on a family of NLP tasks.   All four reviewers (counting the emergency fourth review) found the general direction of research to be interesting and worthwhile, but all four shared several serious concerns about the impact and soundness of the proposed method.   The impact concerns mostly dealt with the observation that the method is specific to linear classifiers, and that it s only applicable to tasks for which a substantial amount of training data is available.   As the AC, I m willing to accept that it should still be possible to conduct an informative analysis under these conditions, but I m more concerned about the soundness issues: The reviewers were not convinced that a method based on the counting of specific features was appropriate for the proposed setting (due to rotation sensitivity, among other issues), and did not find that the experiments were sufficiently extensive to overcome these doubts.
This paper proposes a genetic algorithm to search neural network architectures with locally dense and globally sparse connections. A population based genetic algorithm is used to find the sparse, connections between dense module units. The local dense but global sparse architecture is an interesting idea, yet is not well studied in the current version, e.g. overfitting and connections with other similar architecture search methods. Based on reviewers’ ratings (5,5,6), the current version of paper is proposed as borderline lean reject.  
This paper proposes a framework of image restoration by searching for a MAP in a trained GAN subject to a degradation constraint. Experiments on MNIST show good performance in restoring the images under different types of degradation.  The main problem as pointed out by R1 and R3 is that there has been rich literature of image restoration methods and also several recent works that also utilized GAN, but the authors failed to make comparison any of those baselines in the experiments. Additional experiments on natural images would provide more convincing evidence for the proposed algorithm.  The authors argue that the restoration tasks in the experiments are too difficult for TV to work. It would be great to provide actual experiments to verify the claim.
The paper presents an interesting technique for constrained policy optimization, which is applicable to existing RL algorithms such as TRPO and PPO. All of the reviewers agree that the paper is above the bar and the authors have improved the exposition during the review process. I encourage the authors to address all of the comments in the final version.
This paper provides a new approach for progressive planning on discrete state and action spaces. The authors use LSTM architectures to iteratively select and improve local segments of an existing plan. They formulate the rewriting task as a reinforcement learning problem where the action space is the application of a set of possible rewriting rules. These models are then evaluated on a simulated job scheduling dataset and Halide expression simplification. This is an interesting paper dealing with an important problem. The proposed solution based on combining several existing pieces is novel. On the negative side, the reviewers thought the writing could be improved, and the main ideas are not explained clearly. Furthermore, the experimental evaluation is weak.
Reviewers largely agree that the paper proposes a novel and interesting idea for unsupervised learning through meta learning and the empirical evaluation does a convincing job in demonstrating its effectiveness. There were some concerns on clarity/readability of the paper which seem to have been addressed by the authors. I recommend acceptance. 
The authors have proposed a language+vision  dual  attention architecture, trained in a multitask setting across SGN and EQA in vizDoom, to allow for knowledge grounding. The paper is interesting to read. The complex architecture is very clearly described and motivated, and the knowledge grounding problem is ambitious and relevant. However, the actual proposed solution does not make a novel contribution and the reviewers were unconvinced that the approach would be at all scalable to natural language or more complex tasks. In addition, the question was raised as to whether the  knowledge grounding  claims by the authors are actually much more shallow associations of color and shape that are beneficial in cluttered environments. This is a borderline case, but the AC agrees that the paper falls a bit short of its goals.
All reviewers agree that the proposed is interesting and innovative. One reviewer argues that  some additional baseline comparisons could be beneficial and the other two suggest inclusion of additional explanations and discussions of the results. The authors’ rebuttal alleviated most of the concerns. All reviewers are very appreciative of the quality of the work overall and recommend probable acceptance. I agree with this score and recommend this work for poster presentation at ICLR.
BMIs need per patient and per session calibration, and this paper seeks to amend that.  Using VAEs and RNNs, it relates sEEG to sEMG, in principle a ten year old approach, but do so using a novel adversarial approach that seems to work.  The reviewers agree the approach is nice, the statements in the paper are too strong, but publication is recommended.  Clinical evaluation is an important next step.
The paper proposes a new attentional pooling mechanism that potentially addresses the issues of simple attention based weighted averaging (where discriminative parts/frames might get disportionately high attentions). A nice contribution of the paper is to propose an alternative mechanism with theoretical proofs, and it also presents a method for fast recurrent computation. The experimental results show that the proposed attention mechanism improves over prior methods (e.g., STPN) on THUMOS14 and ActivityNet1.3 datasets. In terms of weaknesses: (1) the computational cost may be quite significant. (2) the proposed method should be evaluated over several tasks beyond activity recognition, but it’s unclear how it would work.   The authors provided positive proof of concept results on weakly supervised object localization task, improving over CAM based methods. However, CAM baseline is a reasonable but not the strongest method and the weakly supervised object recognition/segmentation domains are much more competitive domains, so it s unclear if the proposed method would achieve the state of the art by simply replacing the weighted averaging attentional pooling with the proposed attention mechanism. In addition, the description on how to perform attentional pooling over images is not clearly described (it’s not clear how the 1D sequence based recurrent attention method can be extended to 2 D cases). However, this would not be a reason to reject the paper.   Finally, the paper’s presentation would need improvement. I would suggest that the authors give more intuitive explanations and rationale before going into technical details. The paper starts with Figure 1 which is not really well motivated/explained, so it could be moved to a later part. Overall, there are interesting technical contributions with positive results, but there are issues to be addressed. 
This paper proposes a new method for combining previous state representation learning methods and compares to end to end learning without without separately learning a state representation. The topic is important, and the authors have made an extensive effort to address the reviewer s concerns, particularly regarding clarity, related work, and accuracy of the drawn conclusions. The reviewers found that the main weakness of the paper was the experiments not being sufficiently convincing that the proposed approach is better than the alternatives. Hence, it does not currently meet the bar for publication.
The reviewers had some concerns regarding clarity and evaluation but in general liked various aspects of the paper. The authors did a good job of addressing the reviewers  concerns so acceptance is recommended.
In light of the reviews and the rebuttal, it seems that the paper needs to be rewritten to head off some of the confusions and criticisms that the reviewers have made. That said, the main argument seems to contradict some of the lower bounds recently established by Madry and colleagues, showing the existence of distributions where the sample complexity for finding robust classifiers is arbitrarily larger than that for finding low risk classifiers. I recommend the authors take a closer look at this apparent contradiction when revising.
The paper presents an action conditioned video prediction method that combines previous losses in the literature, such as, perceptual, adversarial and infogan type of losses. The reviewers point out the lack of novelty in the formulation, as well as the lack of experiments that would verify its usefulness in model based RL. There is no rebuttal thus no ground for discussion or acceptance.
Existing PAC Bayes analysis gives generalization bounds for stochastic networks/classifiers. This paper develops a new approach to obtain generalization bounds for the original network, by generalizing noise resilience property from training data to test data.  All reviewers agree that the techniques  developed in the paper (namely Theorem 3.1) are novel and interesting.  There was disagreement between reviewers on the usefulness of the new generalization bound (Theorem 4.1) shown in this paper using the above techniques. I believe authors have sufficiently addressed these concerns in their response and updated draft. Hence, despite the concerns of R3 on limitations of this bound and its dependence on pre activation values, I agree with R2 and R4 that the techniques developed in the paper are of interest to the community and deserve publication. I suggest authors to keep comments of R3 in mind while preparing the final version. 
There was a significant amount of discussion on this paper, both from the reviewers and from unsolicited feedback.  This is a good sign as it demonstrates interest in the work.  Improving exploration in Deep Q learning through Thompson sampling using uncertainty from the model seems sensible and the empirical results on Atari seem quite impressive.  However, the reviewers and others argued that there were technical flaws in the work, particularly in the proofs.  Also, reviewers noted that clarity of the paper was a significant issue, even more so than a previous submission.    One reviewer noted that the authors had significantly improved the paper throughout the discussion phase.  However, ultimately all reviewers agreed that the paper was not quite ready for acceptance.  It seems that the paper could still use some significant editing and careful exposition and justification of the technical content.  Note, one of the reviews was disregarded due to incorrectness and a fourth reviewer was brought in.
The paper proposed an optimal margin distribution loss and applied PAC Bayesian bounds that are from Sanov large deviation inequalities to give generalization error bounds for such a loss. Some interesting empirical results are shown to support the proposed method.   The majority of reviewers think the paper’s empirical results are encouraging, although still in premature stage. The theoretical analysis is a kind of being standard. After reading the authors’ response and revision, the reviewers do not change much of their opinions and think the paper better undergoes systematic further study on their proposal for big improvement.    Based on current ratings, the paper is therefore proposed to borderline lean rejection.  
The paper proposes several subsampling policies to achieve a clear reduction in the size of augmented data while maintaining the accuracy of using a standard data augmentation method. The paper in general is clearly written and easy to follow, and provides sufficiently convincing experimental results to support the claim. After reading the authors  response and revision, the reviewers have reached a general consensus that the paper is above the acceptance bar. 
This paper considers the information bottleneck Lagrangian as a tool for studying deep networks in the common case of supervised learning (predicting label Y from features X) with a deterministic model, and identifies a number of troublesome issues. (1) The information bottleneck curve cannot be recovered by optimizing the Lagrangian for different values of β because in the deterministic case, the IB curve is piecewise linear, not strictly concave. (2) Uninteresting representations can lie on the IB curve, so information bottleneck optimality does not imply that a representation is useful. (3) In a multilayer model with a low probability of error, the only tradeoff that successive layers can make between compression and prediction is that deeper layers may compress more. Experiments on MNIST illustrate these issues, and supplementary material shows that these issues also apply to the deterministic information bottleneck and to stochastic models that are nearly deterministic. There was a substantial degree of disagreement between the reviewers of this paper. One reviewer (R3) suggested that all the conclusions of the paper are the consequence of P(X,Y) being degenerate. The authors responded to this criticism in their response and revision quite effectively, in the opinion of the AC. Because R3 failed to participate in the discussion, this review has been discounted in the final decision. The other two reviewers were considerably more positive about the paper, with one (R1) having basically no criticisms and the other (R2) expression some doubts about the novelty of the observations being made in the paper and their importance for practical machine learning scenarios.  Following the revision and discussion, R2 expressed general satisfaction with the paper, so the AC is recommending acceptance. The AC thinks that the final paper would be clearer if the authors were to carefully distinguish between ground truth labels used in training and the labels estimated by the model for a given input.  At the moment, the symbol Y appears to be overloaded, standing for both.  Perhaps the authors should place a hat over Y when it is standing for estimated labels?
This paper targets improving the computation efficiency of super resolution task. Reviewers have a consensus that this paper lacks technical contribution, therefore not recommend acceptance. 
This paper addresses the problem of few shot learning and then domain transfer. The proposed approach consists of combining a known few shot learning model, prototypical nets, together with image to image translation via CycleGAN for domain adaptation.  Thus the algorithmic novelty is minor and amounts to combining two techniques to address a different problem statement. In addition, as mentioned by Reviewer 2, though meta learning could be a solution to learn with few examples, the solution being used in this work is not meta learning and so should not be in the title to avoid confusion.   As this is a new problem statement the authors apply multiple existing works from few shot learning (and now adaptation) to their setting. The proposed approach does outperform prior work, however this is not surprising as the prior work was not designed for this task. Despite improvements during the rebuttal to address clarity the specific experimental setting is still unclear   especially the setup of meta test data vs unsupervised da data.   This paper is borderline. However, since the main contribution consists of proposing a new problem statement and suggesting a combination of prior techniques as a first solution, the paper needs a more thorough ablation of other possible combination of techniques as well as a clearly defined experimental setup before it is ready for publication.
The paper presents an extension of MADDPG, adding communication between agents. The methods targets extremely noisy observations settings, so that agents need to decide if they communicate their private observations (or not). There is no intrinsic/explicit reward to guide the learning of the communication, only the extrinsic/implicit reward of the downstream task.  The paper is clear and easy to follow, in particular after the updated writing. I believe some of the reviewers  points were addressed by the rebuttal. Nonetheless, some of the weaknesses of the paper still hold: namely the complexity of the approach compounded with a very specific experimental evaluation. The more complex an approach is (and it may be justified by the complexity of the setting!), the more varied its supporting evidence should be.  In its current form, the paper would constitute a good workshop contribution (to discuss the approach), but I believe it needs more varied (and/or harder) experiments to be published at ICLR.
{418}; {Classifier agnostic saliency map extraction}; {Avg: 4.33}; {}  1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.  The paper is well written and the method is simple, effective, and well justified.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.  1. The introduction, in particular the last row of pg 1, implies that this work is the first to show that a class agnostic saliency estimation method can produce higher quality saliency maps than class dependent ones. However, Fan et al. have already shown this. For this reason, AR1 recommended that the authors reword the introduction to reflect prior work on this aspect but the authors declined to do so. The AC would have liked to see a discussion of how the different points of view of the two works (robustness to corruption vs class agnosticism) both address the same issue (poor segmentation of the salient image regions). 2. The work of Fan et al has a very similar approach and a deeper comparison is needed. While the authors dedicated two paragraphs of discussion to this work, they should have gone further. For example, the work of Fan et al. uses a very simple saliency map extraction network and it s unclear how much this impacts their performance when compared to the proposed method, which uses ResNet50. The AC agrees with the authors that re implementing the method of Fan et al. is asking a lot but a discussion of the potential impact would have sufficed. 3. The authors didn t mention at all the vast body of work on salient object detection (for a somewhat recent review see Borji et al. "Salient object detection: A benchmark." IEEE TIP). The differences to this line of work should have been discussed.  Points 1 and 2 were particularly salient for the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Two major points of contention were:   The discussion of differences between the proposed method and the method of Fan et al.   The fairness of the comparison to Fan et al. AR1 felt that the paper was deficient on both counts (AR2 had similar concerns) and the authors disagreed, arguing that the discussion was complete and the quantitative comparison fair.  The AC was sympathetic to these concerns and found the authors  responses to be dismissive of those concerns. In particular, the AC agrees that the paper, as currently organized, minimizes the degree to which the work is derived from Fan et al.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
This paper presents a method for building representations of logical formulae not by propagating information upwards from leaves to root and making decisions (e.g. as to whether one formula entails another) based on the root representation, but rather by propagating information down from root to leaves.  It is a somewhat curious approach, and it is interesting to see that it works so well, especially on the "massive" train/test split of Evans et al. (2018). This paper certainly piques my interest, and I was disappointed to see a complete absence of discussion from reviewers during the rebuttal period despite author responses. The reviewer scores are all middle of the road scores lightly leaning towards accepting, so the paper is rather borderline. It would have been most helpful to hear what the reviewers thought of the rebuttal and revisions made to the paper.  Having read through the paper myself, and through the reviews and rebuttal, I am hesitantly casting an extra vote in favour of acceptance: the sort of work discussed in this paper is important and under represented in the conference, and the results are convincing. I however, share the concerns outlined by the reviewers in their first (and only) set of comments, and invite the authors to take particular heed of the points made by AnonReviewer3, although all make excellent points. There needs to be some further analysis and explanation of these results. If not in this paper, then at least in follow up work. For now, I will recommend with medium confidence that the paper be accepted.
The paper proposes a filtering technique to use less training examples in order to train faster; the filtering step is done with an autoencoder. Experiments are done on CIFAR 10. Reviewers point to a lack of convincing experiments, weak evidence, lack of experimental details. Overall, all reviewers converge to reject this paper, and I agree with them.
This manuscript proposes a technique for co manifold learning that exploits smoothness jointly over the rows and columns of the data. This is an important topic worth further study in the community.  The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the clarity of the presentation. Further improvement of the clarity   particularly clarification of the learning goals, combined with additional convincing experiments would significantly strengthen this submission.
The reviewers find the per difficult to read. Reviewers also had concerns regarding the correctness of various claims in the paper. The paper was also found lacking in experimental analysis, as it only tested on relatively small datasets, and only no a CNN architecture. Overall, the paper appears to be lacking in quality and clarity, and questionable in correctness and originality.
The paper extends the results in Yarotsky (2017) from Sobolev spaces to Besov spaces, stating that once the target function lies in certain Besov spaces, there exists some deep neural networks with ReLU activation that approximate the target in the minimax optimal rates. Such adaptive networks can be found by empirical risk minimization, which however is not yet known to be found by SGDs etc. This gap is the key weakness of applying approximation theory to the study of constructive deep neural networks of certain approximation spaces, which lacks algorithmic guarantees. The gap is hoped to be filled in future studies.   Despite the incompleteness of approximation theory, this paper is still a good solid work. Based on fact that the majority of reviewers suggest accept (6,8,6), with some concerns on the clarity, the paper is proposed as probable accept. 
This paper relates deep learning to convex optimization by showing that the forward pass though a dropout layer, linear layer (either convolutional or fully connected), and a nonlinear activation function is equivalent to taking one τ nice proximal gradient descent step on a a convex optimization objective. The paper shows (1) how different activation functions correspond to different proximal operators, (2) that replacing Bernoulli dropout with additive dropout corresponds to replacing the τ nice proximal gradient descent method with a variance reduced proximal method, and (3) how to compute the Lipschitz constant required to set the optimal step size in the proximal step. The practical value of this perspective is illustrated in experiments that replace various layers in ConvNet architectures with proximal solvers, leading to performance improvements on CIFAR 10 and CIFAR 100. The reviewers felt that most of their concerns were adequately addressed in the discussion and revision, and that the paper should be accepted.
The authors propose to accelerate neural architecture search by using feature similarity with a given teacher network to measure how good a new candidate architecture is. The experiments show that the method accelerates architecture search, and has competitive performance. However, both Reviewers 1 and 3 noted questionable motivation behind the approach, as the method assumes that there already exists a strong teacher network in the domain where we architecture search is performed, which is not always the case. The rebuttal and the revised version of the paper addressed some of the reviewers  concerns, but overall the paper remained below the acceptance bar. I suggest that the authors further expand the evaluation and motivate their approach better before re submitting to another venue. 
The work presents a method to back propagate and visualize bias distribution in network as a form of explainability of network decisions. Reviewers unanimous reject, no rebuttal from authors. 
The paper is overally interesting and addresses an important problem, however reviewers ask for more rigorous empirical study and less restrictive settings.
The authors propose an algorithm for generating adversarial examples for ASR systems treating them as black boxes.   Strengths   One of the early works to demonstrate black box attacks on ASR system that recognize phrases instead of isolated words.  Weaknesses   The approach assumes that the logits are available, which may not be realistic for most ASR systems when they are used in practice   typically only the final transcription is available.   Although the technique is applied to continuous speech, algorithmic improvements over prior work of Alzanot et al. is minimal.   Evaluation is weak. For example, cross correlation cannot completely capture the adversarial nature of a generated audio sample.    The authors use a genetic algorithm for generating new set of examples which are pruned and mutated. It’s not clear what guarantees exist that the algorithm will eventually succeed.   The reviewers agree that the presented work puts forth an interesting research direction. But given the deficiencies of the current submission as pointed out by the reviewers, the recommendation is to reject the paper.
This paper proposes to quantify the uncertainty of neural network models with Beta, Dirichlet and Dirichlet Multinomial likelihood. This paper is clearly written with a sound main idea. However, it is a common practice to model different types of data with different likelihood, although the proposed distributions are not usually used for network output. All the reviewers therefore considered this paper to be of limited novelty. Reviewer 2 also had a concern about the mixed experimental results of the proposed method.  Reviewer 3 raised the concern that this paper did not model the uncertainty of prediction from the uncertainty of the model parameters. It is a common consideration in a Bayesian approach and I encourage the authors to discussed different sources of uncertainty in future revisions.
This paper tackles the problem of using auxiliary losses to help regularize and aid the learning of a "goal" task. The approach proposes avoiding the learning of irrelevant or contradictory details from the auxiliary task at the expense of the "goal" tasks by observing cosine similarity between the auxiliary and main tasks and ignore those gradients which are too dissimilar.   To justify such a setup one must first show that such negative interference occurs in practice, warranting explicit attention. Then one must show that their algorithm effectively mitigates this interference and at the same time provides some useful signal in combination with the main learning objective.   During the review process there was a significant discussion as to whether the proposed approach sufficiently justified its need and usefulness as defined above. One major point of contention is whether to compare against the multi task literature. The authors claim that prior multi task learning literature is out of scope of this work since their goal is not to measure performance on all tasks used during learning. However, this claim does not invalidate the reviewer s request for comparison against multi task learning work. In fact, the authors *should* verify that their method outperforms state of the art multi task learning methods. Not because they too are studying performance across all tasks, but because their method which knows to prioritize one task during training should certainly outperform the learning paradigms which have no special preference to one of the tasks.   A main issue with the current draft centers around the usefulness of the proposed algorithm. First, whether the gradient co sine similarity is a necessary condition to avoid negative interference and 2) to show at least empirically that auxiliary losses do offer improved performance over optimizing the goal task alone. Based on the experiments now available the answers to these questions remains unclear and thus the paper is not yet recommended for publication.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    improvements to a transformer model originally designed for machine translation   application of this model to a different task: music generation   compelling generated samples and user study.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    lack of clarity at times (much improved in the revised version)  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  The main contention was novelty. Some reviewers felt that adapting an existing transformer model to music generation and achieving SOTA results and minute long music sequences was not sufficient novelty. The final decision aligns with the reviewers who felt that the novelty was sufficient.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  A consensus was not reached. The final decision is aligned with the positive reviews for the reason mentioned above. 
a major issue or complaint from the reviewers seems to come from perhaps a wrong framing of this submission. i believe the framing of this work should have been a better language model (or translation model) with word discovery as an awesome side effect, which i carefully guess would ve been a perfectly good story assuming that the perplexity result in Table 4 translates to text with blank spaces left in (it is not possible tell whether this is the case from the text alone.) even discounting R1, who i disagree with on quite a few points, the other reviewers also did not see much of the merit of this work, again probably due to the framing issue above.   i highly encourage the authors to change the framing, evaluate it as a usual sequence model on various benchmarks and resubmit it to another venue.
The reviewers are unanonymous in their assessment that the paper is not ICLR quality in its current form.
 * Strengths  This paper studies adversarial robustness to perturbations that are bounded in the L2 norm. It is motivated by a theoretical sufficient condition (non expansiveness) but rather than trying to formally verify robustness, it uses this condition as inspiration, modifying standard network architectures in several ways to encourage non expansiveness while mostly preserving computational efficiency and accuracy. This “theory inspired practically focused” hybrid is a rare perspective in this area and could fruitfully inspire further improvements. Finally, the paper came under substantial scrutiny during the review period (there are 65 comments on the page) and the authors have convincingly answered a number of technical criticisms.  * Weaknesses  One reviewer and some commenters were concerned that the L2 norm is not a realistic norm to measure adversarial attacks in. There were also concerns that the empirical level of robustness of the network was too weak to be meaningful. In addition, while some parts of the experiments were thorough and some parts of the paper were well presented, the quality was not uniform throughout. Finally, while the proposed changes improve adversarial robustness, they also decrease the accuracy of the network on clean examples (this is to be expected but may be an issue in practice).  * Discussion  There was substantial disagreement on whether to accept the paper. On the one hand, there has been limited progress on robustness to adversarial examples (even under simple norms such as the L2 norm) and most methods that do work are based on formal verification and therefore quite computationally expensive. On the other hand, simple norms such as the L2 norm are somewhat contrived and mainly chosen for convenience (although doing well in the L2 norm is a necessary condition for being robust to more general attacks). Moreover, the empirical results are currently too weak to confer meaningful robustness even under the L2 norm.  * Decision  While I agree with the reviewers and commenters who are skeptical of the L2 norm model (and would very much like to see approaches that consider more realistic threat models), I decided to accept the paper for two reasons: first, doing well in L2 is a necessary condition to doing well in more general models, and the ideas and approach here are simple enough that they might provide inspiration in these more general models as well. Additionally, this was one of the strongest adversarial defense papers at ICLR this year in terms of credibility of the claims (certainly the strongest in my pile) and contains several useful ideas as well as novel empirical findings (such as the increased success of attacks up to 1 million iterations).
This paper introduces a recurrent neural network approach for learning diffusion dynamics in networks. The main advantage is that it embeds the history of diffusion and incorporates the structure of independent cascades for diffusion modeling and prediction. This is an important problem, and the proposed approach is novel and provides some empirical improvements.  However, there is a lack of theoretical analysis, and in particular modeling choices and consequences of these choices should be emphasized more clearly. While there wasn t a consensus, a majority of the reviewers believe the paper is not ready for publication.  
The paper proposes an architecture to learn over sets, by proposing a way to have permutations differentiable end to end, hence learnable by gradient descent. Reviewers pointed out to the computational limitation (quadratic in the size of the set just to consider pairwise interactions, and cubic overall). One reviewer (with low confidence) though the approach was not novel but didn t appreciate the integration of learning to permute with a differentiable setting, so I decided to down weight their score. Overall, I found the paper borderline but would propose to accept it if possible.
The authors propose a new method of securely evaluating neural networks.   The reviewers were unanimous in their vote to accept. The paper is very well written, the idea is relatively simple, and so it is likely that this would make a nice presentation.
Dear authors,  All reviewers commented that the paper had issues with the presentations and the results, making it unsuitable for publication to ICLR. Please address these comments should you decide to resubmit this work.
This paper focuses on scaling up neural theorem provers, a link prediction system that combines backward chaining with neural embedding of facts, but does not scale to most real world knowledge bases. The authors introduce a nearest neighbor search based method to reduce the time/space complexity, along with an attention mechanism that improves the training. With these extensions, they scale NTP to modern benchmarks for the task, including ones that combine text and knowledge bases, thus providing explanations for such models.  The reviewers and the AC note the following as the primary concerns of the paper: (1) the novelty of the contributions is somewhat limited, as nearest neighbor search and attention are both well known strategies, as is embedding text+facts jointly, (2) there are several issues in the evaluation, in particular around analysis of benefits of the proposed work on new datasets. There were a number of other potential weaknesses, such the performance on some benchmarks (Fb15k) and clarity and writing quality of a few sections.  The authors provided significant revisions to the paper that addressed many of the clarity and evaluation concerns, along with providing sufficient comments to better contextualize some of the concerns. However, the concerns with novelty and analysis of the results still hold. Reviewer 3 mentions that it is still unclear in the discussion why the accuracy of the proposed approach matches/outperforms that of NTP, i.e. why is there not a tradeoff. Reviewer 4 also finds the analysis lacking, and feels that the differences between the proposed work and the single link approaches, in terms of where each excels, are described in insufficient detail. Reviewer 4 focused more on the simplicity of the text encoding, which restricts the novelty as more sophisticated text embeddings approaches are commonplace.  Overall, the reviewers raised different concerns, and although all of them appreciated the need for this work and the revisions provided by the authors, ultimately feel that the paper did not quite meet the bar.
This paper proposes an approach for incremental learning of new classes using meta learning. Strengths: The framework is interesting. The reviewers agree that the paper is well written and clear. The experiments include comparisons to prior work, and the ablation studies are useful for judging the performance of the method. Weaknesses: The paper does not provide significant insights over Gidaris & Komodakis  18. Reviewer 1 was also concerned that the motivation for RBP is not entirely clear. Overall, the reviewers found that the strengths did not outweigh the weaknesses. Hence, I recommend reject. 
This paper proposes a solution for the well known problem of posterior collapse in VAEs: a phenomenon where the posteriors fail to diverge from the prior, which tends to happen in situations where the decoder is overly flexible.  A downside of the proposed method is the introduction of hyper parameters controlling the degree of regularization. The empirical results show improvements on various baselines.  The paper proposes the addition of a regularization term that penalizes pairwise similarity of posteriors in latent space. The reviewers agree that the paper is clearly written and that the method is reasonably motivated. The experiments are also sufficiently convincing.
This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc. Although the novelty of the paper may be limited, empirical performance seems impressive. The source code is not available. I think this is a borderline paper but maybe good enough for acceptance. 
The paper presents adversarial "attacks" to maze generation for RL agents trained to perform 2D navigation tasks in 3D environments (DM Lab).  The paper is well written, and the rebuttal(s) and additional experiments (section 4) make the paper better. The approach itself is very interesting. However, there are a few limitations, and thus I am very borderline on this submission:     the analysis of why and how the navigation trained models fail, is rather succinct. Analyzing what happens on the model side (not just the features of the adversarial mazes vs. training mazes) would make the paper stronger.    (more importantly) Section 4: "adapting the training distribution" by incorporating adversarial mazes into training feels incomplete. That is a pithy as giving an adversarial attack for RL trained navigation agents would be much more complete of a contribution if at least the most obvious way to defend the attack was studied in depth. The authors themselves are honest about it and write "Therefore, it is possible that many more training iterations are necessary for agents to learn to perform well in each adversarial setting." (under 4.4 / Expensive Training).  I would invite the authors to submit this version to the workshop track, and/or to finish the work started in Section 4 and make it a strong paper.
The paper discusses layer wise training of deep networks. The authors show that it s possible to achieve reasonable performance by training deep nets layer by layer, as opposed to now widely adopted end to end training. While such a training procedure is not novel, the authors argue that this is an interesting result, considering that such a training procedure is often dismissed as sub optimal and leading to inferior results. However, the results show exactly that, as the performance of the models is significantly worse than the state of the art, and it is unclear what other advantages such a training scheme can offer. The authors mention that layer wise training could be useful for theoretical understanding of deep nets, but they don’t really perform such analysis in this submission, and it’s also unclear whether conclusions of such analysis would extend to deep nets trained end to end.  In its current form, the paper is not ready for acceptance. I encourage the authors to make a more clear case for the method: either by improving results to match end to end training, or by actually demonstrating that layer wise training has certain advantages over end to end learning. 
The reviewers disagree strongly on this paper. Reviewer 2 was the most positive, believing it to be an interesting contribution with strong results. Reviewer 3 however, was underwhelmed by the results. Reviewer 1 does not believe that the contribution is sufficiently novel, seeing it as too close to existing multi task learning approaches.  After considering all of the discussion so far, I have to agree with reviewer 2 on their assessment. Much of the meta learning literature involves changing the base learner *for a fixed architecture* and seeing how it affects performance. There is a temptation to chase performance by changing the architecture, adding new regularizers, etc., and while this is important for practical reasons, it does not help to shed light on the underlying fundamentals. This is best done by considering carefully controlled and well understood experimental settings. Even still, the performance is quite good relative to popular base learners.  Regarding novelty, I agree it is a simple change to the base learner, using a technique that has been tried before in other settings (linear regression as opposed to classification), however its use in a meta learning setup is novel in my opinion, and the new experimental comparison regression on top of pre trained CNN features helps to demonstrate the utility of its use in the meta learning settings.  While the novelty can certainly be debated, I want to highlight two reasons why I am opting to accept this paper: 1) simple and effective ideas are often some of the most impactful. 2) sometimes taking ideas from one area (e.g., multi task learning) and demonstrating that they can be effective in other settings (e.g., meta learning) can itself be a valuable contribution. I believe that the meta learning community would benefit from reading this paper. 
This paper proposes to pre train hierarchical document representations for use in downstream tasks. All reviewers agreed that the results were reasonable.  However, the methodological novelty is limited. While I believe there is a place for solid empirical results, even if not incredibly novel, there is also little qualitative or quantitative analysis to shed additional insights.  Given the high quality bar for ICLR, I can t recommend the paper for acceptance at this time.
Although one review is favorable, it does not make a strong enough case for accepting this paper. Thus there is not sufficient support in the reviews to accept this paper.  I am recommending rejecting this submission for multiple reasons.  Given that this is a "black box" attack formalized as an optimization problem, the method must be compared to other approaches in the large field of derivative free optimization. There are many techniques including: Bayesian optimization, (other) evolutionary algorithms, simulated annealing, Nelder Mead, coordinate descent, etc. Since the method of the paper does not use anything about the structure of the problem it can be applied to other derivative free optimization problems that had the same search constraint. However, the paper does not provide evidence that it has advanced the state of the art in derivative free optimization.  The method the paper describes does not need a new name and is an obvious variation of existing evolutionary algorithms. Someone facing the same problem could easily reinvent the exact method of the paper without reading it and this limits the value of the contribution.  Finally, this paper amounts to breaking already broken defenses, which is not an activity of high value to the community at this stage and also limits the contribution of this work. 
This paper investigates composition and decomposition for adversarially training generative models that work on composed data. Components that are sampled from component generators are then fed into a composition function to generate composed samples, aiming to improve modularity, extensibility, and interpretability of GANs. The paper is written very clearly and is easy to follow. Experiments considered application to both images (MNIST) and text (yelp reviews). The original version of the paper lacks any qualitative analysis, even though experiments were described. Authors revised the paper to include some experimental results, however, they are still not sufficient. State of the art baselines, from previous work suggested by the reviewers should be included for comparison.
 * Strengths  This paper presents a very interesting connection between GANs and robust estimation in the presence of corrupted training data. The conceptual ideas are novel and can likely be extended in many further directions. I would not be surprised if this opens up a new line of research.  * Weaknesses  The paper is poorly written. Due to disagreement among the authors and my interest in the topic, I read the paper in detail myself. I think it would be difficult for a non expert to understand the key ideas and I strongly encourage the authors to carefully revise the paper to reach a broader audience and highlight the key insights. Additionally, the experiments are only on toy data.  * Discussion  One of the reviewers was concerned about the lack of efficiency guarantees for the proposed algorithm (indeed, the algorithm requires training GANs which are currently beyond the reach of theory and finicky in practice). That reviewer points to the fact that most papers in the robustness literature are concerned with computational efficiency and is concerned that ignoring this sidesteps one of the key challenges. The reviewer is also concerned about the restriction to parametric or nearly parametric families (e.g. Gaussians and elliptical distributions). Other reviewers were more positive and did not see these as major issues.  * Decision  In my opinion, the lack of efficiency guarantees is not a huge issue, as the primary contribution of the paper is pointing out a non obvious conceptual connection between two literatures. The restriction to parametric families is more concerning, but it seems possible this could be removed with further developments. The main reason for accepting the paper (despite concerns about the writing) is the importance of the conceptual connection. I think this connection is likely to lead to a new line of research and would like to get it out there as soon as possible.  * Comments  Despite the accept decision, I again urge the authors to improve the quality of exposition to ensure that a large audience can appreciate the ideas.
The paper considers an important problem of investigating the effects different statistical characteristics of representations (hidden unit activations) , such as sparsity, low correlation, etc, have on the neural network performance; while all reviewers agree that this is clearly a very important topic, there is also a consensus that perhaps the authors must strengthen and emphasize their contribution more clearly.  
The manuscript proposes deterministic approximations for Bayesian neural networks as an alternative to the standard Monte Carlo approach. The results suggest that the deterministic approximation can be more accurate than previous methods. Some explicit contributions include efficient moment estimates and empirical Bayes procedures.   The reviewers and ACs note weakness in the breadth and complexity of models evaluated, particularly with regards to ablation studies. This issue seems to have been addressed to the reviewer s satisfaction by the rebuttal. The updated manuscript also improves references to related prior work.  Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. We recommend acceptance.
The authors present an interesting approach but there were multiple significant concerns with the clarity of the presentation, and some concern with the significance of the experimental results.
The reviewers mostly raised two concerns regarding the paper: a) why this algorithm is more interpretability than BP (which is just gradient descent); b) the exposition of the paper is somewhat confusing at various places; c) the lack of large scale experiment results to show this is practically relevant. In the AC s opinion, a principled kernel based approach can be counted as interpretable, and there the AC would support the paper if a) is the only concern. However, c) seems to be a serious concern since the paper doesn t seem to have experiments beyond fashion MNIST (e.g., CIFAR is pretty easy to train these days) and doesn t have experiments with convolutional models. Based on c), the AC decided that the paper is not quite ready for acceptance. 
The submission proposes a machine learning approach to directly train a prediction system for whether a boolean sentence is satisfiable.  The strengths of the paper seem to be largely in proposing an architecture for SAT problems and the analysis of the generalization performance of the resulting classifier on classes of problems not directly seen during training.  Although the resulting system cannot be claimed to be a state of the art system, and it does not have a correctness guarantee like DPLL based approaches, the paper is a nice re introduction of SAT in a machine learning context using deep networks.  It may be nice to mention e.g. (W. Ruml. Adaptive Tree Search. PhD thesis, Harvard University, 2002) which applied reinforcement learning techniques to SAT problems.  The empirical validation on variable sized problems, etc. is a nice contribution showing interesting generalization properties of the proposed approach.  The reviewers were unanimous in their recommendation that the paper be accepted, and the review process attracted a number of additional comments showing the broader interest of the setting.
The authors propose to tackle the problem of catastrophic forgetting in continual learning by adopting the generative replay strategy with the generator network as an extendable memory module.   While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues: (1) poor presentation clarity of the manuscript and incremental technical contribution in light of prior work by Serra et al. (2018); (2) rigorous experiments and in depth analysis of the baseline models in terms of accuracy, number of parameters, memory demand and model complexity would significantly strengthen the evaluation – see R1’s and R3’s suggestions how to improve; (3) simple strategies such as storing a number of examples and memory replay should not be neglected and evaluated to assess the scope of the contribution.  Additionally R1 raised a concern that preventing the generator from forgetting should be supported by an ablation study on both, the discriminator and the generator, abilities to remember and to forget.  R1 and R3 provided very detailed and constructive reviews, as acknowledged by the authors. R2 expressed similar concerns about time/memory comparison of different methods, but his/her brief review did not have a substantial impact on the decision.  AC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
This paper tackles a very valuable problem of learning object detection and object dynamics from video sequences, and builds upon the method of Zhu et al. 2018. The reviewers point out that there is a lot of engineering steps in the object proposal stage, which takes into account background subtraction to propose objects. In its current form, the writing of the paper is not clear enough on the object instantiation part, which is also the novel part over Zhu et al., potentially due to the complexity of using motion to guide object proposals. A limitation of the proposed formulation is that it works for moving cameras but only in 2d environments. Experiments on 3D environments would make this paper a much stronger submission. 
This paper describes a new batching strategy for more efficient training of deep neural nets. The idea stems from the observation that some operations can only be batched more efficiently in the backward, suggesting that batching should be different between forward and backward. The results show that the proposed method improves upon existing batch strategies across three tasks. The reviewers find the work novel, but note that it does not properly address the trade offs made by the technique   such as memory consumption. They also argue that the writing should be improved before acceptance at ICLR.
The paper studies how to construct infinitely deep infinite width networks from a theoretical point of view, and uses the results of its theoretical analysis to design a weight initialization scheme for finite width networks. While the idea is interesting and the paper may contain novel theoretical contributions, the experimental results are weak, as pointed out by all three reviewers from several different perspectives. In particular, it seems that the presented theoretical analysis is useful mainly for weight initialization and hence has limited potential impacts. In addition, the authors have responded to neither the AC s question, nor a detailed anonymous comment that challenges the value of Proposition 1 given the previous work by Aronszajn.
 Pros:   This is an interesting and relevant topic   It is well motivated and mostly clear  Cons:   The motivation, large amounts of data such as occur in lifelong learning, is not well examined in the evaluation which focuses on quite small problems.  For an example of work which addresses the lifelong memory management issue (though does not learn a memory management policy) see [1].   In general the evaluation is not adequate to the claims.   Reviewer 2 is concerned with the use of a bi directional RNN for the comparison of memory entries since it may overfit to order.   Reviewer 1 is somewhat concerned with novelty over other memory management schemes.  [1] Scalable Recollections for Continual Lifelong Learning. https://arxiv.org/pdf/1711.06761.pdf
Reviewers are in a consensus and recommended to reject after engaging with the authors. Further, many additional questions raised in the discussion should be addressed in the submission to improve clarity. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
This paper proposes an approach to pruning units in a deep neural network while training is in progress. The idea is to (1) use a specific "scoring function" (the absolute valued Taylor expansion of the loss) to identify the best units to prune, (2) computing the mean activations of the units to be pruned on a small sample of training data, (3) adding the mean activations multiplied by the outgoing weights into the biases of the next layer s units, and (4) removing the pruned units from the network. Extensive experiments show that this approach to pruning does less immediate damage than the more common zero replacement approach, that this advantage remains (but is much smaller) after fine tuning, and that the importance of units tends not to change much during training. The reviewers liked the quality of the writing and the extensive experimentation, but even after discussion and revision had concerns about the limited novelty of the approach, the fact that the proposed approach is incompatible with batch normalization (which severely limits the range of architectures to which the method may be applied), and were concerned that the proposed method has limited impact after fine tuning.
The paper presents a novel view on adversarial examples, where models using ReLU are inherently sensitive to adversarial examples because ReLU activations yield a polytope of examples with exactly the same activation. Reviewers found the finding interesting and novel but argue it is limited in impact. I also found the idea interesting but the paper could probably be improved as all reviewers have remarked. Overall, I found it borderline but probably not enough for acceptance.
The reviewers raised a number of major concerns including the incremental novelty of the proposed (if any), a poor readability of the presented materials, and, most importantly, insufficient and unconvincing experimental evaluation presented. The authors did not provide any rebuttal. Hence, I cannot suggest this paper for presentation at ICLR.
The paper proposes a new fine tuning method for improving the performance of existing anomaly detectors.  The reviewers and AC note the limitation of novelty beyond existing literature.  This is quite a borader line paper, but AC decided to recommend acceptance as comprehensive experimental results (still based on empirical observation though) are interesting.
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The paper is clearly written and well motivated, but there are remaining concerns on contributions and comparisons.  The paper received mixed initial reviews. After extensive discussions, while the authors successfully clarified several important issues (such as computation efficiency w.r.t splitting) pointed out by Reviewer 4 (an expert in the field), they were not able to convince him/her about the significance of the proposed network compression method.   Reviewer 4 has the following remaining concerns:  1) This is a typical paper showing only FLOPs reduction but with an intent of real time acceleration. However, wall clock speedup is different from FLOPs reduction. It may not be beneficial to change the current computing flow optimized in modern software/hardware. This is one of major reasons why the reported wall clock time even slows down. The problem may be alleviated with optimization efforts on software or hardware, then it is unclear how good/worse will it be when compared with fine grain pruning solutions (Han et al. 2015b, Han et al. 2016 & Han et al. 2017), which achieved a higher FLOP reduction and a great wall clock speedup with hardware optimized (using ASIC and FPGA);  2) If it is OK to target on FLOPs reduction (without comparison with fine grain pruning solutions),    2.1) In LSTM experiments, the major producer of FLOPs   the output layer, is excluded and this exclusion was hidden in the first version. Although the author(s) claimed that an output layer could be compressed, it is not shown in the paper. Compressing output layer will reduce model capacity, making other layers more difficult being compressed.    2.2) In CNN experiments, the improvements of CIFAR 10 is within a random range and not statistically significant. In table 2, "Regular low rank MobileNet" improves the original MobileNet, showing that the original MobileNet (an arXiv paper) is not well designed. "Adaptive Low rank MobileNet" improves accuracy upon "Regular low rank MobileNet", but using 0.3M more parameters. The trade off is unclear.  In addition to these remaining concerns of Reviewer 4, the AC feels that the paper essentially modifies the original network structure in a very specific way: adding a particular nonlinear layer between two adjacent layers. Thus it seems a little bit unfair to mainly use low rank factorization (which can be considered as a compression technique that barely changes the network architecture) for comparison. Adding comparisons with fine grain pruning solutions (Han et al. 2015b, Han et al. 2016 & Han et al. 2017) and a large number of more recent related references inspired by the low rank baseline (M. Jaderberg et al 2014) , as listed by Reviewer 4, will make the proposed method much more convincing. 
On the positive side, this is among the first papers to exploit non Euclidean geometry, specifically curvature for adversarial learning. However, reviewers are largely in agreement that the technical correctness of this paper is unconvincing despite substantial technical exchanges with the authors.  
AR1 is concerned about the poor organisation of this paper.  AR2 is concerned about the similarity between TRL and TR. The authors show some empirical results to support their intuition, however, no theoretical guarantees are provided regarding TRL superiority.  Moreover,  experiments for the Taskonomy dataset as well as on RNN have not been demonstrated, thus AR2 did not increase his/her score.  AR3 is the most critical and finds the clarity and explanations not ready for publication.  AC agrees with the reviewers in that the proposed idea has some merits, e.g. the reduction in the number of parameters seem a good point of this idea. However, reviewer urges the authors to seek non trivial theoretical analysis for this method. Otherwise, it indeed is just an intelligent application paper and, as such, it cannot be accepted to ICLR.
This paper shows how to obtain more homogeneous activation of atoms in a dictionary. As reviewers point out, the paper is well written and indeed shows that the propose scheme results in a more uniform activation. However, the value of this contribution rests on making a case that uniformity is indeed a desirable outcome per se. As two reviewers explain, this crucial point is left unaddressed, which makes the paper too weak for ICLR.
As far as I know, this is the first paper to combine transductive learning with few shot classification. The proposed algorithm, TPN, combines label propagation with episodic training, as well as learning an adaptive kernel bandwidth in order to determine the label propagation graph. The reviewers liked the idea, however there were concerns of novelty and clarity. I think the contributions of the paper and the strong empirical results are sufficient to merit acceptance, however the paper has not undergone a revision since September. It is therefore recommended that the authors improve the clarity based on the reviewer feedback. In particular, clarifying the details around learning \sigma_i and graph construction. It would also be useful to include the discussion of timing complexity in the final draft.
The paper presents an interesting idea for increasing the robustness of adversarial defenses by combining with existing domain adaptation approaches. All reviewers agree that the paper is well written and clearly articulates the approach and contribution.  The main areas of weakness is that the experiments focus on small datasets, namely CiFAR and MNIST.  That being said, the algorithm is reasonably ablated on the data explored and the authors provided valuable new experimental evidence during the rebuttal phase and in response to the public comment. 
This work proposes to reduce memory use in network training by quantizing the activations during backprop. It shows that this leads to only small drops in accuracy for resnets on CIFAR 10 and Imagenet for factors up to 8. The reviewers raised concerns about comparison to other approaches such as checkpointing, and questioned the technical novelty of the approach.  The authors were able to properly address the concerns around comparisons, but the issue around novelty remained. This could be compensated by strengthening the experimental results and leveraging the memory saving for instance to train larger networks. Resubmission is encouraged.
The paper presents the use of information bottlenecks as a way to identify key "decision states" in exploration, in a goal conditioned model. The concept of "decision states" is actually common in RL, states where exploring can lead to very diverse/new states. The implementation of the "information bottleneck" is done by adding a regularizing term, the conditional mutual information I(A;G|S).  The main weaknesses of the paper were its lack of clarity and the experimental section. It seems to me that the rebuttals, and the additional experiments and details, made the paper worthy of publication. The authors cleared enough of the gray areas and showcased the relative merits of the methods.
The paper analyses GRUs using dynamic systems theory.  The paper is well written and the theory seems to be solid.  But there is agreement amongst the reviewers that the application of the method might not scale well beyond rather simple 1  or 2 D GRUs (i.e., with one or two GRUs).  This limitation, which is an increasingly serious problem in machine learning papers, should be solved before the paper should be published.  A very recent extension of the simulations to 16 GRUs improves this, but a rigorous analysis of higher dimensional systems is pending and poses a considerable block for acceptance.
The paper presents a new approach to learn separate class invariant and class equivariant latent representations, by training on labeled (and optional additional unlabelled) multi class data. Empirical results on MNIST and SVHN show that the method works well. Reviewers initially highlighted the following weaknesses of the paper: insufficient references and contrasting with related work (given that this problem space has been much explored before),  limited novelty of the approach, limited experiments (MNIST only). One reviewer also mentioned a sometimes vague, overly hyperbolic, and meandering writeup.  Authors did a commendable effort to improve the paper based on the reviews, adding new references, removing and rewriting parts of the paper to make it more focused, and providing experimental results on an additional dataset (SVHN). The paper did improve as a result. But while attenuated, the initial criticisms remain valid: the literature review and discussion remains short and too superficial. The peculiarities of the approach which grant it (modest) originality are insufficiently (theoretically and empirically) justified and not clearly enough put in context of the whole body of prior work. Consequently the proposed approach feels very ad hoc. Finally the additional experiments are a step in the right direction, but experiments on only MNIST and SVHN are hardly enough in 2018 to convince the reader that a method has a universal potential and is more generally useful. Given the limited novelty, and in the absence of theoretical justification, experiments should be much more extensive, both in diversity of data/problems, and in the range of alternative approaches compared to, to build a convincing case. 
This paper proposes a combination of three techniques to improve the learning performance of Atari games. Good performance was shown in the paper with all three techniques together applied to DQN. However, it is hard to justify the integration of these techniques. It is also not clear why the specific decisions were made when combining them. More comprehensive experiments, such as a more systematic ablation study, are required to convince the benefits of individual components. Furthermore, it seems very hard to tell whether the improvement of existing approaches, such as Ape X DQN, was from using the proposed techniques or a deeper architecture (Tables 1&2&4&5). Overall, this paper is not ready for publication. 
The paper proposes a progressive pruning technique that achieves high pruning ratio. Reviewers have a consensus on rejection. Reviewer 1 pointed out that the experimental results are weak. Reviewer 2 is also concerned about the proposed method and experiments. Reviewer 3 is is concerned that this paper is incremental work. Overall, this paper does not meet the standard of ICLR. Recommend for rejection. 
This paper presents CMOW—an unsupervised sentence representation learning method that treats sentences as the product of their word matrices. This method is not entirely novel, as the authors acknowledge, but it has not been successfully applied to downstream tasks before. This paper presents methods for successfully training it, and shows results on the SentEval benchmark suite for sentence representations and an associated set of analysis tasks.  All three reviewers agree that the results are unimpressive: CMOW is no better than the faster CBOW baseline on most tasks, and the combination of the two is only marginally better than CBOW. However, CMOW does show some real advantages on the analysis tasks. No reviewer has any major correctness concerns that I can see.  As I see it, this paper is borderline, but narrowly worth accepting: As a methods paper, it presents weak results, and it s not likely that many practitioners will leap to use the method. However, the method is so appealingly simple and well known that there is some value in seeing this as an analysis paper that thoroughly evaluates it. Because it is so simple, it will likely be of interest to researchers beyond just the NLP domain in which it is tested (as CBOW style models have been), so ICLR seems like an appropriate venue. It seems like it s in the community s best interest to see a method like this be evaluated, and since this paper appears to offer a thorough and sound evaluation, I recommend acceptance.
This paper proposes and end to end trainable architecture for data augmentation, by defining a parametric model for data augmentation (using spatial transformers and GANs) and optimizing validation classification error through the notion of influence functions. Experiments are reported on MNIST and CIfar 10.   This is a borderline submission. Reviewers found the theoretical framework and problem setup to be solid and promising, but were also concerned about the experimental setup and the lack of clarity in the manuscript. In particular, one would like to evaluate this model against similar baselines (e.g. Ratner et al) on a large scale classification problem. The AC, after taking these comments into account and making his/her own assessment, recommends rejection at this time, encouraging the authors to address the above comments and resubmit this promising work in the next conference cycle. 
This paper applies Dirichlet distribution to the latent variables of a VAE in order to address the component collapsing issues for categorical probabilities. The method is clearly presented, and extensive experiments are carried out to prove the advantage against VAEs with other prior distributions.  The main concern of the paper is the limited novelty. The main methodology contribution of this paper is to combine the decomposition a Dirichlet distribution as Gamma distributions, and approximating Gamma component with inverse Gamma CDF, but both components are common practices.   R3 also points out that the paper is distracted by two different messages the authors try to convey. The presentation and experiments are not designed to provide a cohesive message. The concern is not solved in the authors  feedback.  Based on the current reviews, this paper does not meet the standard for ICLR publication. Despite the limited novelty in the proposed model, if the paper could be revised to show that a simple modification is good for solve one problem with general applications, it would make a good publication in a future venue.
This paper studies memorization properties of convnets by testing their ability to determine if an image/set of images was used during training or not. The experiments are reported on large scale datasets using high capacity networks.   While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues: (1) more formal justifications are required to assess the scope and significance of this work contributions   see very detailed comments by R2 about measuring networks capacity to memorize and the role of network weights and depth as studied in MacKay,2002. In their response the authors acknowledged they didn’t take into account network weights and depth but strived at an empirical evaluation scenario.  (2) writing and presentation clarity of the paper could be substantially improved – see very detailed comments by R3 and also R2;  (3) empirical evaluations and effect of the negative set used for training are not well explained and analysed (R2, R3).  AC can confirm that all three reviewers have read the author responses and have contributed to the final discussion. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
This paper heavily modifies standard time series VAE models to improve their representation learning abilities.  However, the resulting model seems like an ad hoc combination of tricks that lose most of the nice properties of VAEs.  The resulting method does not appear to be useful enough to justify itself, and it s not clear that the same ends couldn t be pursued using simpler, more general, and computationally cheaper approaches.
All the reviewers and AC agrees that the main strength of the paper that it studies a rather important question of the validity of using linear interpolation in evaluating GANs. The paper gives concrete examples and theoretical and empirical analysis that shows linear interpolation is not a great idea. The potential weakness is that the paper doesn t provide a very convincing new evaluation to replace the linear interpolation. However, given that it s largely unclear what are the right evaluations for GANs, the AC thinks the "negative result" about linear interpolation already deserves an ICLR paper. 
This paper analyzes random auto encoders in the infinite dimension limit with an assumption that the weights are tied in the encoder and decoder. In the limit the paper is able to show the random auto encoder transformation  as doing an approximate inference on data. The paper is able to obtain principled initialization strategies for training deep autoencoders using this analysis, showing the usefulness of their analysis. Even though there are limitations of paper such as studying only random models, and characterizing them only in the limit, all the reviewers agree that the analysis is novel and gives insights on an interesting problem. 
The paper presents a novel approach to exploration in long horizon / sparse reward RL settings. The approach is based on the notion of abstract states, a space that is lower dimensional than the original state space, and in which transition dynamics can be learned and exploration is planned. A distributed algorithm is proposed for managing exploration in the abstract space (done by the manager), and learning to navigate between abstract states (workers). Empirical results show strong performance on hard exploration Atari games.  The paper addresses a key challenge in reinforcement learning   learning and planning in long horizon MDPs. It presents an original approach to this problem, and demonstrates that it can be leveraged to achieve strong empirical results.   At the same time, the reviewers and AC note several potential weaknesses, the focus here is on the subset that substantially affected the final acceptance decision. First, the paper deviates from the majority of current state of the art deep RL approaches by leveraging prior knowledge in the form of the RAM state. The cause for concern is not so much the use of the RAM information, but the comparison to other prior approaches using "comparable amounts of prior knowledge"   an argument that was considered misleading by the reviewers and AC. The reviewers make detailed suggestions on how to address these concerns in a future revision. Despite initially diverging assessments, the final consensus between the reviewers and AC was that the stated concerns would require a thorough revision of the paper and that it should not be accepted in its current stage.  On a separate note, a lot of the discussion between R1 and the authors centered on whether more comparisons / a larger number of seeds should be run. The authors argued that the requested comparisons would be too costly. A suggestion for a future revision of the paper would be to only run a large number (e.g., 10) of seeds for the first 150M steps of each experiment, and presenting these results separately from the long running experiments. This should be a cost efficient way to shed light on a particularly important range, and would help validate claims about sample efficiency.
The reviewers raised a number of major concerns including lack of explanations, lack of baseline comparisons, and lack of discussion on pros and cons of  the main contribution of this work    the presented Temporal Gaussian Mixture (TGM) layer. The authors’ rebuttal addressed some of the reviewers’ comments but failed to address all concerns (especially when it comes to the success of TGMs; it remains unclear whether this could be attributed solely to the way TGMs are applied rather than to their fundamental methodological advantage). Having said that, I cannot suggest this paper for presentation at ICLR.
Interesting paper applying memory networks that encode external knowledge (represented in the form of triples) and conversation context for task oriented dialogues. Experiments demonstrate improvements over the state of the art on two public datasets.  Notation and presentation in the first version of the paper were not very clear, hence many question and answers were exchanged during the reviews.  
The paper describes a WaveNet like model for MIDI conditional music audio generation. As noted by all reviewers, the major limitation of the paper is that the method is evaluated on a synthetic dataset. The rebuttal and post rebuttal discussion didn t change the reviewers  opinion.
This manuscript proposes a new algorithm for learning from positive and unlabeled data. The motivation for this work includes cases of selection bias, where the positive label is correlated with observation. The resulting procedure is shown to learn a scoring function that preserves the class posterior ordering, and can thus be thresholded to obtain a classifier.  The problem addressed is interesting, and the approach sounds reasonable. The writing seems to be well done, particularly after the rebuttal when the work was better placed in context.  The reviewers and AC note issues with the evaluation of the proposed method. In particular, the authors do not provide a sufficiently convincing empirical evaluation on real data. 
This paper presents a reinforcement learning approach to hierarchical text classification.  Pros: A potentially interesting idea to drive the search process over a hierachical set of labels using reinforcement learning.  Cons: The major concensus among all reviewers was that there were various concerns about experimental results, e.g., apple to apple comparisons against prior art (R1), proper tuning of hyper parameters (R1, R2), the label space is too small (539) to have practical significance compared to tens of thousands of labels that have been used in other related work (R3), and other missing baselines (R3). In addition, even after the rebuttal, some of the technical clarity issues have not been fully resolved, e.g., what the proposed method is actually doing (optimizing F1 metric vs the ability to fix inconsistent labeling problem).  Verdict:  Reject. While authors came back with many detailed responses, they were not enough to address the major concerns reviewers had about the empirical significance of this work.
Granger Causality is a beautiful operational definition of causality, that reduces causal modeling to the past to future predictive strength. The combination of classical granger causality with deep learning is very well motivated as a research problem. As such the continuation of the effort in this paper is strongly encouraged. However, the review process did uncover possible flaws in some of the main, original results of this paper. The reviewers also expressed concerns that the experiments were unconvincing due to very small data sizes. The paper will benefit from a revision and resubmission to another venue, and is not ready for acceptance at ICLR 2019.
This paper proposes a reinforcement learning approach that better handles sparse reward environments, by using previously experienced roll outs that achieve high reward. The approach is intuitive, and the results in the paper are convincing. The authors addressed nearly all of the reviewer s concerns. The reviewers all agree that the paper should be accepted.
This method proposes a criterion (SNIP) to prune neural networks before training.  The pro is that SNIP can find the architecturally important parameters in the network without full training. The con is that SNIP only evaluated on small datasets (mnist, cifar, tiny imagenet) and it s uncertain if the same heuristic works on large scale dataset. Small datasets can always achieve high pruning ratio, so evaluation on ImageNet is quite important for pruning work. The reviewers have consensus on accept. The authors are recommended to compare with previous work [1][2] to make the paper more convincing.   [1] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. NIPS, 2015.  [2] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. NIPS, 2016.
This paper introduces Mahe, a model agnostic hierarchical explanation technique, that constructs a hierarchy of explanations, from local, context dependent ones (like LIME) to global, context free ones. The reviewers found the proposed work to be a quite interesting application of the neural interaction detection (NID) framework, and overall found the results to be quite extensive and promising.  The reviewers and the AC note the following as the primary concerns of the paper: (1) a crucial concern with the proposed work is the clarity of writing in the paper, and (2) the proposed work is quite expensive, computationally, as the exhaustive search is needed over local interactions.  The reviewers appreciated the detailed comments and the revision, and felt the revised the manuscript was much improved by the additional editing, details in the papers, and the additional experiments. However, both reviewer 1 and 3 have strong reservations about the computational complexity of the approach, and the additional experiments did not alleviate it. Further, reviewer 1 is still concerned about the clarity of the work, finding much of the proposed work to be unclear, and recommends further revisions.  Given these considerations, everyone felt that the idea is strong and most of the experiments are quite promising. However, without further editing and some efficiency strategies, it barely misses the bar of acceptance.  
This work presents a method to model embeddings as distributions, instead of points, to better quantify uncertainty. Evaluations are carried out on a new dataset created from mixtures of MNIST digits, including noise (certain probability of occlusions), that introduce ambiguity, using a small "toy" neural network that is incapable of perfectly fitting the data, because authors mention that performance difference lessens when the network is complex enough to almost perfectly fit the data.   Reviewer assessment is unanimously accept, with the following points:  Pros: + "The topic of injecting uncertainty in neural networks should be of broad interest to the ICLR community." + "The paper is generally clear." + "The qualitative evaluation provides intuitive results."  Cons:   Requirement of drawing samples may add complexity. Authors reply that alternatives should be studied in future work.   No comparison to other uncertainty methods, such as dropout. Authors reply that dropout represents model uncertainty and not data uncertainty, but do not carry out an experiment to compare (i.e. sample from model leaving dropout activated during evaluation).   No evaluation in larger scale/dimensionality datasets. Authors mention method scales linearly, but how practical or effective this method is to use on, say, face recognition datasets, is unclear.    As the general reviewer consensus is accept, Area Chair is recommending Accept; However, Area Chair has strong reservations because the method is evaluated on a very limited dataset, with a toy model designed to exaggerate differences between techniques. Essentially, the toy evaluation was designed to get the results the authors were looking for. A more thorough investigation would use more realistic sized network models on true datasets.  
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The problem is well motivated and related work is thoroughly discussed   The evaluation is compelling and extensive.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    Very dense. Clarity could be improved in some sections.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
The paper gives a theoretical analysis highlighting the role of codimension on the pervasiveness of adversarial examples. The paper demonstrates that a single decision boundary cannot be robust in different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set.   The main concern with the paper is that most of the theoretical results might have a very restrictive scope and the writing is difficult to follow.   The authors expressed concerns about a review not being very constructive. In a nutshell, the review in question points out that the theory might be too restrictive, that the experimental section is not very strong, that there are other works on related topics, and that the writing of the paper could be improved. While I understand the disappointing of the authors, the main points here appear to be consistent with the other reviews, which also mention that the theoretical results in this paper are not very general, that the writing is a bit complicated or heavy in mathematics, and not easy to follow, or that it is not clear if the bounds can be useful or easily applied in other work.   One reviewer rates the paper marginally above the acceptance threshold, while two other reviewers rate the paper below the acceptance threshold. 
This clearly written paper develops a novel, sound and comprehensive mathematical framework for computing low variance gradients of expectation based objectives. The approach generalizes and encompasses several previous approaches for continuous random variables (reparametrization trick, Implicit Rep, pathwise gradients), and conveys novel insights.  Importantly, and originally, it extends to discrete random variables, and to chains of continuous random variables with optionally discrete terminal variables. These contributions are well exposed, and supported by convincing experiments. Questions from reviewers were well addressed in the rebuttal and helped significantly clarify and improve the paper, in particular for delineating the novel contribution against prior related work. 
This paper presents a dataset for measuring disentanglement in learned representations. It consists of MNIST digits, sometimes transformed in various ways, and labeled with a variety of attributes. This dataset is used to measure statistics of various learned models.  Measuring disentanglement is certainly an important problem in our field. This dataset seems to be well designed, and I would recommend its use for papers studying disentanglement. The experiments are well designed. While the reviewers seem bothered by the fact that it s limited to MNIST, this doesn t strike me as a problem. We continue to learn a lot from MNIST, even today.  But producing a useful dataset isn t by itself a significant enough research contribution for an ICLR paper. I d recommend publication if (a) it were very different from currently existing datasets, (b) constructing it required overcoming significant technical obstacles, or (c) the dataset led to particularly interesting findings.  Regarding (a), there are already datasets of similar complexity which have ground truth attributes useful for measuring disentanglement, such as dSprites and 3D Faces. Regarding (b), the construction seems technically straightforward. Regarding (c), the experimental findings are plausible and consistent with past findings (which is a good validation of the dataset) but not obviously interesting in their own right.  So overall, this seems like a useful dataset, but I cannot recommend publication at ICLR. 
This work proposes a modification of gradient based saliency map methods that measure the importance of all nodes at each layer. The reviewers found the novelty is rather marginal and that the evaluation is not up to par (since it s mostly qualitative). The reviewers are in strong agreement that this work does not pass the bar for acceptance.
The paper proposes a Bayesian extension to existing knowledge base embedding methods (like DistMult and ComplEx), which is applied for for hyperparameter learning. While using Bayesian inference for for hyperparameter tuning for embedding methods is not generally novel, it has not been used in the context of knowledge graph modelling before. The paper could be strengthened by comparing the method to other strategies of hyperparameter selection to prove the significance of the advantage brought by the method.
The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss. The method are simple and effective. The paper is clear and easy to follow. However, the real speedup on CPU/GPU is not demonstrated beyond the theoretical FLOPs reduction. Reviewers are also concerned that the idea of dynamic channel pruning is not novel. The evaluation is on fairly old networks.
There is a clear consensus among the reviews to accept this submission thus I am recommending acceptance. The paper makes a clear, if modest, contribution to language modeling that is likely to be valuable to many other researchers.
The authors give a characterization of stochastic mirror descent (SMD) as a conservation law (17) in terms of the Bregman divergence of the loss. The identity allows the authors to show that SMD converges to the optimal solution of a particular minimax filtering problem. In the special overparametrized linear case, when SMD is simply SGD, the result recovers a recent theorem due to Gunasekar et al. (2018). The consequences for the overparametrized nonlinear case are more speculative.  The main criticisms are around impact, however, I m inclined to think that any new insight on this problem, especially one that imports results from other areas like control, are useful to incorporate into the literature.   I will comment that the discussion of previous work is wholly inadequate. The authors essentially do not engage with previous work, and mostly make throwaway citations. This is a real pity.  I would be nice to see better scholarship.
 The paper presents a sensible algorithm for knowledge distillation (KD) from a larger teacher network to a smaller student network by minimizing the Maximum Mean Discrepancy (MMD) between the distributions over students and teachers network activations. As rightly acknowledged by the R3, the benefits of the proposed approach are encouraging in the object detection task, and are less obvious in classification (R1 and R2).   The reviewers and AC note the following potential weaknesses: (1) low technical novelty in light of prior works “Demystifying Neural Style Transfer” by Li et al 2017 and “Deep Transfer Learning with Joint Adaptation Networks” by Long et al 2017   See R2’s detailed explanations; (2) lack of empirical evidence that the proposed method is better than the seminal work on KD by Hinton et al, 2014; (3) important practical issues are not justified (e.g. kernel specifications as requested by R3 and R2; accuracy efficiency trade off as suggested by R1); (4) presentation clarity.   R3 has raised questions regarding deploying the proposed student models on mobile devices without a proper comparison with the MobileNet and ShuffleNet light architectures. This can be seen as a suggestion for future revisions.   There is reviewer disagreement on this paper and no author rebuttal. The reviewer with a positive view on the manuscript (R3) was reluctant to champion the paper as the authors did not respond to the concerns of the reviewers.  AC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
This manuscript proposes spread divergences as a technique for extending f divergences to distributions with different supports. This is achieved by convolving with a noise distribution. This is an important topic worth further study in the community, particularly as it related to training generative models.  The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work, or expressing issues about the clarity of the presentation. Further improvement of the clarity, combined with additional convincing experiments would significantly strengthen this submission.
The reviewers in general found the paper approachable, well written and clear.  They noted that the empirical observation of mode collapse in active learning was an interesting insight.  However, all the reviewers had concerns with novelty, particularly in light of Lakshminarayanan et al. who also train ensembles to get a measure of uncertainty.  An interesting addition to the paper might be some theoretical insight about what the model corresponds to when one ensembles multiple models from MC Dropout.  One reviewer noted that it s not clear that the ensemble is capturing the desired posterior.  As a note, I don t believe there is agreement in the community that MC dropout is state of the art in terms of capturing uncertainty for deep neural networks, as argued in the author response (and the abstract).  To the contrary, I believe a variety of papers have improved over the results from that work (e.g. see experiments in Multiplicative Normalizing Flows from over a year ago).
The paper presents a novel strategy for statistically motivated feature selection i.e. aimed at controlling the false discovery rate. This is achieved by extending knockoffs to complex predictive models and complex distributions via (multiple) generative adversarial networks.   The reviewers and ACs noted weakness in the original submission which seems to have been fixed after the rebuttal period   primary related to missing experimental details. There was also some concern (as is common with inferential papers) that the claims are difficult to evaluate on real data, as the ground truth is unknown. To this end, the authors provide empirical results with simulated data that address this issue. There is also some concern that more complex predictive models are not evaluated.  Overall the reviewers and AC have a positive opinion of this paper and recommend acceptance.
This paper offers a novel perspective for learning latent multimodal representations. The idea of segmenting the information into multimodal discriminative and modality specific generating factors is found to be intriguing by all reviewers and the AC. The technical derivations allow for an efficient implementation of this idea.  There have been some concerns regarding the experimental section, but they have all been addressed adequately during the rebuttal period. Therefore the AC suggests this paper for acceptance. It is an overall nice and well thought work.  
This paper studies RL with perturbed rewards, where a technical challenge is to revert the perturbation process so that the right policy is learned.  Some experiments are used to support the algorithm, which involves learning the reward perturbation process (the confusion matrix) using existing techniques from the supervised learning (and crowdsourcing) literature.  Reviewers found the problem setting new and worth investigating, but had concerns over the scope/significance of this work, mostly about how the confusion matrix is learned.  If this matrix is known, correcting reward perturbation is easy, and standard RL can be applied to the corrected rewards.  Specifically, the work seems to be limited in two substantial ways, both related to how the confusion matrix is learned.   * The reward function needs to be deterministic.   * Majority voting requires the number of states to be finite. The significance of this work is therefore mostly limited to finite state problems with deterministic reward, which is quite restricted.  As the authors pointed out, the paper uses discretization to turn a continuous state space into a finite one, which is how the experiment was done.  But discretization is likely not robust or efficient in many high dimensional problems.  It should be noted that the setting studied here, together with a thorough treatment of an (even restricted) case, could make an interesting paper that inspires future work.  However, the exact problem setting is not completely clear in the paper, and the limitations of the technical contributions is also somewhat unclear.  The authors are strongly advised to revise the paper accordingly to make their contributions clearer.  Minor questions:     In lemma 2, what if C is not invertible.     The sampling oracle assumed in def 1 is not very practical, as opposed to what the paper claims.     There are more recent work at NIPS and STOC on attacking RL (including bandits) algorithms by manipulating the reward signals.  The authors may want to cite and discuss.
The paper explores the effect of normalization and initialization in residual networks, motivated by the need to avoid exploding and vanishing activations and gradients. Based on some theoretical analysis of stepsizes in SGD, the authors propose a sensible but effective way of initializing a network that greatly increases training stability. In a nutshell, the method comes down to initializing the residual layers such that a single step of SGD results in a change in activations that is invariant to the depth of the network. The experiments in the paper provide supporting evidence for the benefits; the authors were able to train networks of up to 10,000 layers deep. The experiments have sufficient depth to support the claims. Overall, the method seems to be a simple but effective technique for learning very deep residual networks.   While some aspects of the network have been used in earlier work, such as initializing residual branches to output zeros, these earlier methods lacked the rescaling aspect, which seems crucial to the performance of this network.  The reviewers agree that the papers provides interesting ideas and significant theoretical and empirical contributions. The main concerns by the reviewers were addressed by the author responses. The AC finds that the remaining concerns raised by the reviewers are minor and insufficient for rejection of the paper. 
All reviewers recommended rejecting this submission so I will as well. However, I do not believe it is fundamentally misguided or anything of that nature.  Unfortunately, reviewers did not participate as much in discussions with the authors as I believe they should. However, this paper concerns a relatively niche problem of modest interest to the ICLR community. I believe a stronger version of this work would be a more application focused paper that delved into practical details about a specific case study where this work provides a clear benefit.
  pros:   good, sensible idea   good evaluations on the domains considered   good analysis   cons:   novelty, broader evaluation  I think this is a good and interesting paper and I appreciate the authors  engagment with the reviewers.  I agree with the authors that it is not fair to compare their work to a blog post which hasn t been published and I have taken this into account.  However, there is still concern among the reviewers about the strength of the technical contribution and the decision was made not to accept for ICLR this year. 
although some may find the proposed approach as incremental over e.g. gu et al. (2018) and kiela et al. (2018), i believe the authors  clear motivation, formulation, experimentation and analysis are solid enough to warrant the presentation at the conference. the relative simplicity and successful empirical result show that the proposed approach could be one of the standard toolkits in deep learning for multilingual processing.   J Gu, H Hassan, J Devlin, VOK Li. Universal Neural Machine Translation for Extremely Low Resource Languages. NAACL 2018. D Kiela, C Wang, K Cho. Context Attentive Embeddings for Improved Sentence Representations. EMNLP 2018.
The authors propose an approach for continual learning of a sequence of tasks which augments the network with task specific neurons which encode  adversarial subspaces  and prevent interference and forgetting when new tasks are being learnt. The approach is novel and seems to work relatively well on a simple sequence of MNIST or CIFAR10 classes, and has certain advantages, such as not requiring any stored data. However, the reviewers agreed that the presentation of the method is quite confusing and that the paper does not provide adequate intuition, visualisation, or explanation of the claim that they are preventing forgetting through the intersection of adversarial subspaces. Moreover, there was a concern that the baselines were not strong enough to validate the approach.
The paper proposes a novel differential way to output brush strokes, taking a few ideas from model based learning. The method is efficient in that one can train it in an unsupervised manner and does not require paired data. The strengths of the paper are the qualitative results that demonstrate nice interpolations among other things, on a number of datasets (esp. post rebuttal).  The weaknesses of the paper are the writing (which I think is relatively easy to improve if the authors make an honest effort) and some of the quantitative evaluation. I would encourage the authors to get in touch with the SPIRAL paper authors in order to get access to the SPIRAL generated MNIST test data and then perhaps the classification metric could be updated.  In summary, from the discussion, the major points of contention were the somewhat lacking initial evaluation (which was fixed to a large extent) and the quality of writing (which could be fixed more). I believe the submission is genuinely novel, interesting (esp. the usage of world model like techniques) and valuable for the ICLR audience so I recommend acceptance.
The reviewers reached a consense on that the paper is not quite ready for publication at ICRL. The main potential drawback include a) the exposition of the paper can be improved; b) it s not entirely clear that some of the assumptions (such as the threshold for the first layer, the polynomial approximation of higher layers) are meaningful , and it seems that the proof technique exploits heavily some of these assumptions and some of the key intermediate steps won t hold in practice. (see reviewer 3 s comment for more details.) The authors clarify the writing and intuitions in the response, but overall the AC decided that the paper is not quite ready for publications at the moment.  
This is a solid paper that proposes and analyzes a sound approach to zero order optimization, covering a variants of a simple base algorithm.  After resolving some issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Some concerns regarding the necessity for such algorithms persisted, but the connection to adversarial examples provides an interesting motivation.
This paper conducts a study on provable defenses to spatially transformed adversarial examples. In general, the paper pursues an interesting direction, but reviewers had many concerns regarding the clarity of the presentation and the depth of the experimental results, which the authors did not address in a rebuttal. 
Dear authors,  All reviewers agreed that your work sheds new light on a popular class of algorithms and should thus be presented at ICLR.  Please make sure to implement all their comments in the final version.
This work studies the performance of several end to end CNN architectures for the prediction of biomedical assays in microscopy images. One of the architectures, GAPnet, is a minor modification of existing global average pooling (GAP) networks, involving skip connections and concatenations. The technical novelties are low, as outlined by several reviewers and confirmed by the authors, as most of the value of the work lies in the empirical evaluation of existing methods, or minor variants thereof.   Given the low technical novelty and reviewer consensus, recommend reject, however area chair recognizes that the discovered utility may be of value for the biomedical community. Authors are encouraged to use reviewer feedback to improve the work, and submit to a biomedical imaging venue for dissemination to the appropriate communities.  
The submission hypothesizes that in typical GAN training the discriminator is too strong, too fast, and thus suggests a modification by which they gradually increases the task difficulty of the discriminator. This is done by introducing (effectively) a new random variable   which has an effect on the label   and which prevents the discriminator from solving its task too quickly.   There was a healthy amount of back and forth between the authors and the reviewers which allowed for a number of important clarifications to be made (esp. with regards to proofs, comparison with baselines, etc). My judgment of this paper is that it provides a neat way to overcome a particular difficulty of training GANs, but that there is a lot of confusion about the similarities (of lack thereof) with various potentially simpler alternatives such as input dropout, adding noise to the input etc. I was sometimes confused by the author response as well (they at once suggest that the proposed method reduces overfitting of the discriminator but also state that "We believe our method does not even try to “regularize” the discriminator"). Because of all this, the significance of this work is unclear and thus I do not recommend acceptance.
This was a borderline paper, as reviewers generally agreed that the method was a new method that was appropriately explained and motivated and had reasonable experimental results. The main drawbacks were that the significance of the method was unclear. In particular, the method might be too inflexible due to being based on a hard coded rule, and it is not clear why this is the right approach relative to e.g. GANs with a modified training objective). Reviewers also had difficulty assessing the significance of the results on biological datasets. While such results certainly add to the paper, the paper would be stronger if the argument for significance could be assessed from more standard datasets.  A note on the review process: the reviewers initially scored the paper 6/6/6, but the review text for some of the reviews was more negative than a typical 6 score. To confirm this, I asked if any reviewers wanted to push for acceptance. None of the reviewers did (generally due to feeling the significance of the results was limited) and two of the reviewers decided to lower their scores to account for this.
Strengths: A co evolution of body connectivity and its topology mimicing control policy is presented.  Weaknesses: Reviewers found the paper to be lacking in detail. The importance of message passing in achieving the given results is clear on one example but not some others. Some reviewers had questions regarding the baseline comparisons. The authors provided lengthy details in responses on the discussion board, but reviewers likely had limited time to fully reread the many changes that were listed. AC:  The physics in the motions shown in the video require signficant further explanation. It looks like the ball joints can directly attach themselves to the ground, and make that link stand up. Thus it seems that the robots are not underactuated and can effectively grab arbitrary points in the environment. Also it is strange to see the robot parts dynamically fly together as if attracted by a magnet.  The physics needs significant further explanation.  Points of Contention: The R2 review is positive on the paper (7), with a moderate confidence (3). R1 contributed additional questions during the discussion, but R2 and R3 were silent.  The AC further examined  the submission (paper and video).  The reviewers and the AC are in consensus regarding the many details that are behind the system that are still not understood.  The AC is also skeptical of the non physical nature of the motion, or the unspecified behavior of fully actuated contacts with the ground. 
Pros:    A new framework for learning sentence representations   Solid experiments and analyses   En Zh / XNLI dataset was added, addressing the comment that no distant languages were considered; also ablation tests  Cons:     The considered components are not novel, and their combination is straightforward    The set of downstream tasks is not very diverse (See R2)    Only high resource languages are considered (interesting to see it applied to real low resource languages)  All reviewers agree that there is no modeling contribution.  Overall, it is a solid paper but I do not believe that the contribution is sufficient. 
All reviewers recommend acceptance. The problem is an interesting one. THe method is interesting. Authors were responsive in the reviewing process.  Good work. I recommend acceptance :)
All reviewers appreciate the empirical analysis and insights provided in the paper. The paper also reports impressive results on SSL. It will be a good addition to the ICLR program. 
The paper proposes a novel method that learns decompositions of an image over parts, their hierarchical structure  and their motion dynamics given temporal image pairs. The problem tackled is of great importance for unsupervised learning from videos. One downside of the paper is the simple datasets used to demonstrate the effectiveness of the method.  All reviewers though agree on it being a valuable contribution for ICLR.  In the related work section the paper mentions "...Some systems emphasize learning from pixels but without an explicitly object based representation (Fragkiadaki et al., 2016 ...". The paper you cite in fact emphasized the importance of having object centric predictive models and the generalization that comes from this design choice, thus, it may be potentially not the right citation. 
The paper presents an approach to estimate the "effective path" of examples in a network to reach a decision, and consider this to analyze if examples might be adversarial. Reviewers think the paper lacks some clarity and experiments. They point to a confusion between interpretability and adversarial attacks, they ask questions about computational complexity, and point to some unsubstanciated claims. Authors have not responded to reviewers. Overall, I concur with the reviewers to reject the paper.
The authors propose an approach for a learnt attention mechanism to be used for selecting agents in a multi agent RL setting. The attention mechanism is learnt by a central critic, and it scales linearly with the number of agents rather than quadratically. There is some novelty in the proposed method, and the authors clearly explain and motivate the approach. However the empirical evaluation feels quite limited and does not show conclusively that the method is superior to the others. Moreover, the simple empirical results don t give any evidence how the attention mechanism is working or whether it is truly the attention that is affecting the results. The reviewers were split on their recommendation and did not come to a consensus. The AC feels that the paper is not quite strong enough and encourages the authors to broaden the work with additional experiments and analysis.
This paper provides a mean field theory analysis of batch normalization. First there is a negative result as to the necessity of gradient explosion when using batch normalization in a fully connected network. They then provide further insights as to what can be done about this, along with experiments to confirm their theoretical predictions.  The reviewers (and random commenters) found this paper very interesting. The reviewers were unanimous in their vote to accept.
The main issue with the work in its current form is a lack of motivation and some clarity issues. The paper presents some interesting ideas, and will be much stronger when it incorporates a more clear discussion on motivation, both for the problem setting and the proposed solutions. The writing itself could also be significantly improved. 
The paper compared between different CNNs for UAV trail guidance. The reviewers arrived at a consensus on rejection due to lack of new ideas, and the paper is not well polished. 
This paper demonstrated interesting observations that simple transformations such as a rotation and a translation is enough to fool CNNs. Major concern of the paper is the novelty. Similar ideas have been proposed before by many previous researchers. Other networks trying to address this issue have been proposed. Such as those rotation invariant neural networks. The grid search attack used in the experiments may be not convincing. Overall, this paper is not ready for publication.
Strengths  The paper introduces a promising and novel idea, i.e., regularizing RL via an informationally asymmetric default policy  The paper is well written.  It has solid and extensive experimental results.  Weaknesses   There is a lack of benefit on dense reward problems as a limitation, which the authors further acknowledge as a limitation. There also some similarities to HRL approaches.  A lack of theoretical results is also suggested. To be fair, the paper makes a number of connections with various bits of theory, although it perhaps does not directly result in any new theoretical analysis. A concern of one reviewer is the need for extensive compute, and making comparisons to stronger (maxent) baselines. The authors provide a convincing reply on these issues.  Points of Contention  While the scores are non uniform (7,7,5), the most critical review, R1(5), is in fact quite positive on many aspects of the paper, i.e., "this paper would have good impact in coming up with new  learning algorithms which are inspired from cognitive science literature as well as mathematically grounded." The specific critiques of R1 were covered in detail by the authors.  Overall  The paper presents a novel and fairly intuitive idea, with very solid experimental results.   While the methods has theoretical results, the results themselves are more experimental than theoretic. The reviewers are largely enthused about the paper.  The AC recommends acceptance as a poster. 
This paper presents an approach that relies on DNNs and bags of features that are fed into them, towards object recognition.  The strength of the papers lie in the strong performance of these simple and interpretable models compared to more complex architectures.  The authors stress on the interpretability of the results that is indeed a strength of this paper.  There is plenty of discussion between the first reviewer and the authors regarding the novelty of the work as the former point out to several related papers;  however, the authors provide relatively convincing rebuttal of the concerns.  Overall, after the long discussion, there is enough consensus for this paper to be accepted to the conference.
Reviewers mostly recommended to accept after engaging with the authors. I have decided to reduce the weight of AnonReviewer3 because of the short review. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The problem and approach, steganography via GANs, is interesting.   The results seem promising.   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.  The original submission was imprecise and difficult to follow and, while the AC acknowledges that the authors made significant improvements, the current version still needs some work before it s clear enough to be acceptable for publication.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Concerns varied by reviewer and there was no main point of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers did not reach a consensus. The final decision is aligned with the less positive reviewers, one of whom was very confident in his/her review. The AC agrees that the paper should be made clearer and more precise. 
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The paper presents a generative model of sequences based on the VAE framework, where the generative model is given by CNN with causal and dilated connections.   Novelty of the method is limited; it mainly consists of bringing together the idea of causal and dilated convolutions and the VAE framework. However, knowing how well this performs is valuable the community.  The proposed method appears to have significant benefits, as shown in experiments. The result on MNIST is, however, so strong that it seems incorrect; more digging into this result, or sourcecode, would have been better.
This paper presents an RL agent which progressively synthesis programs according to syntactic constraints, and can learn to solve problems with different DSLs, demonstrating some degree of transfer across program synthesis problems. Reviewers agreed that this was an exciting and important development in program synthesis and meta learning (if that word still has any meaning to it), and were impressed with both the clarity of the paper and its evaluation. There were some concerns about missing baselines and benchmarks, some of which were resolved during the discussion period, although it would still be good to compare to out of the box MCTS.  Overall, everyone agrees this is a strong paper and that it belongs in the conference, so I have no hesitation in recommending it.
The paper proposes a novel variational inference framework for knowledge graphs which is evaluated on link prediction benchmark sets and is competitive to previous generative approaches. While the idea is interstnig and technically correct, the originality of the contribution is limited, and the paper would be clearly improved by providing a clearer motivation for using generative models instead of standard methods and a experimental demonstration of  the benefits of using a generative instead of a discriminative model,  especially since the standard method perform slightly better in the experiments. Overall, the work is slightly under the acceptance threshold. 
Strengths: Strong results on future frame video prediction using a 3D convolutional network. Use of future video prediction to jointly learn auxiliary tasks shown to to increase performance. Good ablation study.  Weaknesses: Comparisons with older action recognition methods. Some concerns about novelty, the main contribution is the E3D LSTM architecture, which R1 characterized as an LSTM with an extra gate and attention mechanism.   Contention: Authors point to novelty in 3D convolutions inside the RNN.  Consensus: All reviewers give a final score of 7  well done experiments helped address concerns around novelty. Easy to recommend acceptance given the agreement. 
The paper presents interesting idea, but the reviewers ask for improving further paper clarity   that includes, but is not limited to, providing in depth explanation of assumptions and also improving the writing that is too heavy and difficult to understand.
This paper presents Universal Transformers that generalizes Transformers with recurrent connections. The goal of Universal Transformers is to combine the strength of feed forward convolutional architectures (parallelizability and global receptive fields) with the strength of recurrent neural networks (sequential inductive bias). In addition, the paper investigates a dynamic halting scheme (by adapting Adaptive Computation Time (ACT) of Graves 2016) to allow each individual subsequence to stop recurrent computation dynamically.  Pros:  The paper presents a new generalized architecture that brings a reasonable novelty over the previous Transformers when combined with the dynamic halting scheme. Empirical results are reasonably comprehensive and the codebase is publicly available.  Cons: Unlike RNNs, the network recurs T times over the entire sequence of length M, thus it is not a literal combination of Transformers with RNNs, but only inspired by RNNs. Thus the proposed architecture does not precisely replicate the sequential inductive bias of RNNs. Furthermore, depending on how one views it, the network architecture is not entirely novel in that it is reminiscent of the previous memory network extensions with multi hop reasoning (  a point raised by R1 and R2). While several datasets are covered in the empirical study, the selected datasets may be biased toward simpler/easier tasks (  R1).   Verdict: While key ideas might not be entirely novel (R1/R2), the novelty comes from the fact that these ideas have not been combined and experimented in this exact form of Universal Transformers (with optional dynamic halting/ACT), and that the empirical results are reasonably broad and strong, while not entirely impressive (R1). Sufficient novelty and substance overall, and no issues that are dealbreakers. 
This paper studies group equivariant neural network representations by building on the work by [Cohen and Welling,  14], which introduced learning of group irreducible representations, and [Kondor 18], who introduced tensor product non linearities operating directly in the group Fourier domain.   Reviewers highlighted the significance of the approach, but were also unanimously concerned by the lack of clarity of the current manuscript, making its widespread impact within ICLR difficult, and the lack of a large scale experiment that corroborates the usefulness of the approach. They were also very positive about the improvements of the paper during the author response phase. The AC completely agrees with this assessment of the paper. Therefore, the paper cannot be accepted at this time, but the AC strongly encourages the authors to resubmit their work in the next conference cycle by addressing the above remarks (improve clarity of presentation and include a large scale experiment). 
This paper proposes a transfer learning approach based on previous works on this area, to build language understanding models for new domains. Experimental results show improved performance in comparison to previous studies in terms of slot and intent accuracies in multiple setups. The work is interesting and useful, but is not novel given the previous work. The paper organization is also not great, for example, the intro should introduce the approach beyond just mentioning transfer learning and meta learning. The improvements over the baselines look good, but the baselines themselves are quite simple. It d be better to include comparisons with other state of the art methods. Also, the improvements over DNN are not consistent, it would be good to analyze and come up with suggestions on when to use which approach. 
This is a difficult decision, as the reviewers are quite polarized on this paper, and did not come to a consensus through discussion. The positive elements of the paper are that the method itself is a novel and interesting approach, and that the performance is clearly state of the art. While impressive, the fact that a relatively simple task module trained on the features from Zhu et al. can match the performance of GAZSL suggests that it is difficult to compare these methods in an apples to apples way without using consistent features. There are two ways to deal with this: train the baseline methods using the features of Zhu, or train correction networks using less powerful features from other baselines.  Reviewer 3 pointed this out, and asked for such a comparison. The defense given by the authors is that they use the same features as the current SOTA baselines, and therefore their comparison is sound. I agree to an extent, however it should be relatively simple to either elevate other baselines, or compare correction networks with different features. Otherwise, most of the rows in Table 1 should be ignored. Running correction networks in different features in an ablation study would also demonstrate that the gains are consistent.  I think the authors should run these experiments, and if the results hold then there will be no doubt in my mind that this will be a worthy contribution. However, in their absence, I can’t say with certainty how effective the proposed method really is. 
The paper presents two new methods for model agnostic interpretation of instance wise feature importance.   Pros: Unlike previous approaches based on the Shapley value, which had an exponential complexity in the number of features, the proposed methods have a linear complexity when the data have a graph structure, which allows approximation based on graph structured factorization. The proposed methods present solid technical novelty to study the important challenge of instance wise, model agnostic, linear complexity interpretation of features.   Cons: All reviewers wanted to see more extensive experimental results. Authors responded with most experiments requested. One issue raised by R3 was the need for comparing the proposed model agnostic methods to existing model specific methods. The proposed linear complexity algorithm relies on the markov assumption, which some reviewers commented to be a potentially invalid assumption to make, but this does not seem to be a deal breaker since it is a relatively common assumption to make when deriving a polynomial complexity approximation algorithm. Overall, the rebuttal addressed the reviewers  concerns well enough, leading to increased scores.  Verdict: Accept. Solid technical novelty with convincing empirical results.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
AR1 finds the paper overly lengthy and ill focused on contributions of this work. Moreover, AR1 would like to see more results for G ZSL. AR2 finds the  paper is lacking in clarity, e.g. Eq. 9, and complete definition of the end to end decision pipeline is missing. AR2 points that the manuscript relies on GZSL and comparisons to it but other more recent methods could be also cited:   Generalized Zero Shot Learning via Synthesized Examples by Verma et al.   Zero Shot Kernel Learning by Zhang et al.   Model Selection for Generalized Zero shot Learning by Zhang et al.   Generalized Zero Shot Learning with Deep Calibration Network by Liu et al.   Multi modal Cycle consistent Generalized Zero Shot Learning by Felix et al.   Open Set Learning with Counterfactual Images   Feature Generating Networks for Zero Shot Learning Though, the authors are welcome to find even more relevant papers in google scholar.  Overall, AC finds the paper interesting and finds the idea has some merits. Nonetheless, two reviewers maintained their scores below borderline due to numerous worries highlighted above. The authors are encouraged to work on presentation of this method and comparisons to more recent papers where possible. AC encourages the authors to re submit their improved manuscript as, at this time, it feels this paper is not ready and cannot be accepted to ICLR. 
The paper addresses the problem of out of distribution detection for helping the segmentation process.  The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of ICLR. AC also thinks the authors should avoid using explicit OOD datasets (e.g., ILVRC) due to the nature of this problem. Otherwise, this is a toy binary classification problem.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
The reviewers raised a number of major concerns including the incremental novelty of the proposed (if any) and insufficient and unconvincing experimental evaluation presented. The authors did not provide any rebuttal. Hence, I cannot suggest this paper for presentation at ICLR.
The paper proposes to take into accunt the label structure for classification tasks, instead of a flat N way softmax. This also lead to a zero shot setting to consider novel classes. Reviewers point to a lack of reference to prior work and comparisons. Authors have tried to justify their choices, but the overall sentiment is that it lacks novelty with respect to previous approaches. All reviewers recommend to reject, and so do I.
The paper uses dynamical systems theory to evaluate feed forward neural networks.  The theory is used to compute the optimal depth of resnets.  An interesting approach, and a good initiative.  At the same time, the approach seems not to be thought through well enough, and the work needs another level of maturation before publication.  The application that is realised is too immature, and the corresponding contributions are not significant in their current form.  All reviewers agree on rejection of the paper.
This paper proposes to use meta learning to design MCMC sampling distributions based on Hamiltonian dynamics, aiming to mix faster on set of problems that are related to the training problems. The reviewers agree that the paper is well written and the ideas are interesting and novel. The main weaknesses of the paper are that (1) there is not a clear case for using this method over SG HMC, and (2) there are many design choices that are not validated. The authors revised the paper to address some aspects of the latter concern, but are encouraged to add additional revisions to clarify the points brought up by the reviewers. Despite the weaknesses, the reviewers all agree that the paper exceeds the bar for acceptance. I also recommend accept.
This paper provides a good finding that maximizing a lower bound of the M H acceptance rate is equivalent to minimizing the symmetric KL divergence between target the proposal. This lower bound is then used to learn sampler for both density and sample based settings. It also nicely connects GAN with MCMC by providing a novel loss function to train the discriminator. Experiment on MNIST dataset in Sec 4.2 shows training the proposal with the symmetric KL is better than variational inference that optimizes KL(q||p).  However, there are a few concerns raised in both the reviews and other comments that should be further clarified. 1. Training an independent proposal may reduce the rate of convergence.  2. In the density based setting experiments, the learnt independent proposal is only used to provide an initial point and a random walk kernel is actually used for sampling. This is different from what is proposed algorithm in Section 3. 3. The proposed algorithm is only compared with VI in density based setting, and there are no comparison with other baselines in the sample based setting, despite the close connections of the proposed method with other models. Stochastic gradient MCMC methods, A NICE MC, GAN will be good baselines for empirical comparisons. Also, the dataset in Sec 4.2 is a subset of the standard MNIST, which makes comparison with other literatures difficult.  For the first concern, the authors provided new experiments for low dimensional synthetic distributions. It is very helpful to show the comparable performance with A NICE MC in this case, but the real challenge in high dimensional distributions remains unexamined. For the second concern, the authors consider the use of random walk as a heuristic that allows to obtain better samples from the posterior, but that significantly changes the proposed transition kernel in Alg. 1.  This paper would be significantly stronger and make a very good contribution to this area by addressing the problems above.
This paper proposes a new kernel learning framework for change point detection by using a generative model. The reviewers agree that the paper is interesting and useful for the community. One of the reviewer had some issues with the paper but those were resolved after the rebuttal. The other two reviewers have short reviews and somewhat low confidence, so it is difficult to tell how this paper stands among other that exist in the literature. Overall, given the consistent ratings from all the reviewers, I believe this paper can be accepted. 
This is a well written paper that contributes a clear advance to the understanding of how gradient descent behaves when training deep linear models.  Reviewers were unanimously supportive.
The reviewers agree that the paper is well written, and they all seem to like the general idea. One of the earlier criticisms was that you did not compare against other robust loss functions, but you have partially rectified that by comparing to L1 in the appendix. As per the request of reviewer 2 I would also compare to the Huber loss.  One remaining concern is the lack of theoretical justification, which could help address the comment of reviewer 3 regarding blurry images from location uncertainty. The other concern is that you should compare your method using FID scores from a standard implementation so that your numbers are comparable to other papers. Some of the reviewers were impressed, but confused by your relatively low scores.
The paper introduces a variant of the variational autoencoder (VAE) for probabilistic non negative matrix factorization. The main idea is to use a Weibull distribution in the latent space. There is agreement among the reviewers that the paper is technically sound and well written, but that it lacks in motivation and demonstration of utility of the proposed method. All the reviewers think the approach is not particularly novel and somewhat incremental. The main issue is that the empirical evaluation of the algorithm is also quite limited. Specifically, it should have been compared with Bayesian NMF. Many papers have addressed Bayesian NMF with variational inference (Cemgil; Fevotte & Dikmen; Hoffman, Blei & Cook) like in VAE. Experimentally, Bayesian NMF and the proposed PAE NMF could easily be quantitatively compared on matrix completion tasks. Overall, there was consensus among the reviewers that the paper is not ready for publication. 
This paper provides interesting results on convergence and stability in general differentiable games. The theory appears to be correct, and the paper reasonably well written. The main concern is in connections to an area of related work that has been omitted, with overly strong statements in the paper that there has been little work for general game dynamics. This is a serious omission, since it calls into question some of the novelty of the results because they have not been adequately placed relative to this work. The authors should incorporate a thorough discussion on relations to this work, and adjust claims about novelty (and potentially even results) based on that literature.
Although all the reviewers find the problem and the approach of using hierarchical models important and interesting, how it has been executed in this submission has not been found favourable by the reviewers.
The paper presents a family of models for relational reasoning over structured representations. The experiments show good results in learning efficiency and generalization, in Box World (grid world) and StarCraft 2 mini games, trained through reinforcement (IMPALA/off policy A2C).  The final version would benefit from more qualitative and/or quantitative details in the experimental section, as noted by all reviewers.   The reviewers all agreed that this is worthy of publication at ICLR 2019. E.g. "The paper clearly demonstrates the utility of relational inductive biases in reinforcement learning." (R3)
The paper benchmarks three strategies to adapt an existing TTS system (based on WaveNet) to new speakers.  The paper is clearly written. The models and adaptation strategies are not very novel, but still a scientific contribution. Overall, the experimental results are detailed and convincing. The rebuttals addressed some of the concerns.  This is a welcomed contribution to ICLR 2019.
The paper is well written and easy to follow. The experiments are adequate to justify the usefulness of an identity for improving existing multi Monte Carlo sample based gradient estimators for deep generative models. The originality and significance are acceptable, as discussed below.  The proposed doubly reparameterized gradient estimators are built on an important identity shown in Equation (5). This identity appears straightforward to derive by applying both score function gradient and reparameterization gradient to the same objective function, which is expressed as an expectation. The AC suspects that this identity might have already appeared in previous publications / implementations, though not being claimed as an important contribution / being explicitly discussed. While that identity may not be claimed as the original contribution of the paper if that suspicion is true, the paper makes another useful contribution in applying that identity to the right problem: improving three distinct training algorithms for deep generative models. The doubly reparameterized versions of IWAE and reweighted wake sleep (RWS) further show how IWAE and RWS are related to each other and how they can be combined for potentially further improved performance.   The AC believes that the paper makes enough contributions by well presenting the identity in (5) and applying it to the right problems. 
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciate the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
While the reformulation of RNNs is not practical as it is missing sigmoids and tanhs that are common in LSTMs it does provide an interesting analysis of traditional RNNs and a technique that s novel for many in the ICLR community. 
The paper presents Dopamine, an open source implementation of plenty of DRL methods. It presents a case study of DQN and experiments on Atari. The paper is clear and easy to follow.  While I believe Dopamine is a very welcomed contribution to the DRL software landscape, it seems there is not enough scientific content in this paper to warrant publication at ICLR. Regarding specifically the ELF and RLlib papers, I think that the ELF paper had a novelty component, and presented RL baselines to a new environment (miniRTS), while the RLlib paper had a stronger "systems research" contribution. This says nothing about the future impact of Dopamine, ELF, and RLlib – the respective software.
This paper investigates a data selection framework for domain adaptation based on reinforcement learning.  Pros: The paper presents an approach that can dynamically adjust the data selection strategy via reinforcement learning. More specifically, the RL agent gets reward by selecting a new sample that makes the source training data distribution closer to the target distribution, where the distribution comparison is based on the feature representations that will be used by the prediction classifier. While the use of RL for data selection is not entirely new, the specific method proposed by the paper is reasonably novel and interesting.  Cons: The use of RL is not clearly motivated and justified (R1,R3) and the method presented in this paper is rather hard to follow might be overly complex (R1). One fair point R1 raised is more clean cut empirical evaluation that demonstrates how RL performs clearly better than greedy optimization. The authors came back with additional analysis in Section 4.2 to address this question, but R1 feels the new analysis (e.g., Fig 3) is not clear how to interpret. A more thorough ablation study of the proposed model might have addressed the reviewer s question more clearly. In addition, all reviewers felt that baselines are not convincingly strong enough, though each reviewer pointed out somewhat different aspects of baselines. R3 is most concerned about baselines being not state of the art, and the rebuttal did not address R3 s concern well enough.  Verdict: Reject. A potentially interesting idea but 2/3 reviewers share strong concerns about the empirical results and overall clarity of the paper.
This paper proposes a novel coding scheme for compressing neural network weights using Shannon style coding and a variational distribution over weights.  This approach is shown to improve over existing schemes for LeNet 5 on MNIST and VGG 16 on CIFAR 10, strictly dominating them in terms of compression/error rate tradeoffs. Comparing to more baselines would have been helpful. Theoretical analysis based on non trivial extensions of prior work by Harsha et al. (2010) and  Chatterjee & Diaconis (2018) is also presented. Overall, there was consensus among the reviewers that the paper makes a solid contribution and should be published. 
The paper advocates neuroscience based V1 models to adapt CNNs.  The results of the simulations are convincing from a neuroscience perspective.  The reviewers equivocally recommend publication.
Pros:   simple, sensible subgoal discovery method   strong inuitions, visualizations   detailed rebuttal, 15 appendix sections  Cons:   moderate novelty   lack of ablations   assessments don t back up all claims   ill justified/mismatching design decisions   inefficiency due to relying on a random policy in the first phase  There is consensus among the reviewers that the paper is not quite good enough, and should be (borderline) rejected.
The reviewers liked the paper in general but the empirical evaluation lacks studies on a wider range of different data sets.
This paper investigates a new approach to machine reading for procedural text, where the task of reading comprehension is formulated as dynamic construction of a procedural knowledge graph. The proposed model constructs a recurrent knowledge graph (as a bipartite graph between entities and location nodes) and tracks the entity states for two domains: scientific processes and recipes.  Pros: The idea of formulating reading comprehension as dynamic construction of a knowledge graph is novel and interesting. The proposed model is tested on two different domains: scientific processes (ProPara) and cooking recipes.  Cons: The initial submission didn t have the experimental results on the full recipe dataset and also had several clarity issues, all of which have been resolved through the rebuttal.   Verdict: Accept. An interesting task & models with solid empirical results. 
This paper proposes a “guided” evolution strategy method where the past surrogate gradients are used to construct a covariance matrix from which future perturbations are sampled. The bias variance tradeoff is analyzed and the method is applied to real world examples.  The method is not entirely new, and discussion of related work as well as comparison with them is missing. The main contribution is in the analysis and application to real world examples, and the paper should be rewritten focusing on these contributions, while discussing existing work on this topic thoroughly.  Due to these issue, I recommend to reject this paper. 
This paper studies the behavior of gradient descent on deep neural network architectures with spatial locality, under generic input data distributions, using a planted or "teacher student" model.   Whereas R1 was supportive of this work, R2 and R3 could not verify the main statements and the proofs due to a severe lack of clarity and mathematical rigor. The AC strongly aligns with the latter, and therefore recommends rejection at this time, encouraging the authors to address clarity and rigor issues and resubmit their work again.   
This paper proposes a new permutation invariant loss (where the order doesn t matter), motivated by set autoencoding settings. This is an important problem, and the authors  solution is interesting.  The reviewers, however, found the exposition to be unclear, in particular the explanation on how the loss function is derived was confusing for two of the reviewers. Reviewers also found the experimental results to be not convincing, even after the revision. This is a borderline paper: the idea is valuable and I d encourage the authors to develop it further, improving exposition and including additional experiments as suggested by the reviewers. 
The reviewers found the work interesting and sensible.  The application of latent space constrained autoencoders to wireless positioning certainly seems novel.  Applications can certainly be exciting additions to the conference program.  However, the reviewers weren t convinced that the technical content of the paper was sufficiently novel to be interesting to the ICLR community.  In particular, the reviewers seem concerned that there are no comparisons to more recent methods for dimensionality reduction and learning latent embeddings, such as variational auto encoders.  Certainly a comparison to more recent work constraining latent representations seems warranted to justify this particular approach.
This paper introduces a newly collected dataset of natural language interactions between a tourist and a guide for localization and navigation.  The paper also includes baseline experiments with a reasonably novel approach.  The task is well motivated (although an open question remains due to GPS, comment by reviewer 1), but the description of the dataset and collection, approach and experiments were not ideal in the first version of the paper. Much of the information was pushed to the appendix and it was hard to follow the paper without going back and forth, and even then some points were missing. Authors rewrote parts of the paper to address these concerns, but there are still some open questions. For example, is it possible to have sub tasks, given the task is complex and may not be easy to accomplish as a whole? Or could simple LSTM be another baseline (the final review of the third reviewer)?   
The paper presents a strategy for randomizing the underlying physical hyper parameters of RL environments to improve policy s robustness. The paper has a simple and effective idea, however, the machine learning content is minimal. I agree with the reviewers that in order for the paper to pass the bar at ICLR, either the proposed ideas need to be extended theoretically or it should be backed with much more convincing results. Please take the reviewers  feedback into account and improve the paper.
The authors propose a scheme to learn a mapping between the discrete space of network architectures into a continuous embedding, and from the continuous embedding back into the space of network architectures. During the training phase, the models regress the number of parameters, and expected accuracy given the continuous embedding. Once trained, the model can be used for compression by first embedding the network structure and then performing gradient descent to maximize accuracy by minimizing the number of parameters. The optimized representation can then be mapped back into the discrete architecture space. Overall, the main idea of this work is very interesting, and the experiments show that the method has some promise. However, as was noted by the reviewers, the paper could be significantly strengthened by performing additional experiments and analyses. As such, the AC agrees with the reviewers that the paper in its present form is not suitable for acceptance, but the authors are encouraged to revise and resubmit this work to a future venue. 
The overall consensus after an extended discussion of the paper is that this work should be accepted to ICLR. The back and forth between reviewers and authors was very productive, and resulted in substantial clarification of the work, and modification (trending positive) of the reviewer scores.
The authors consider the problem of active plasticity in the mammalian brain, seen as being a means to enable lifelong learning. Building on the recent paper on differentiable plasticity, the authors propose a learnt, neuro modulated differentiable plasticity that can be trained with gradient descent but is more flexible than fixed plasticity. The paper is clearly motivated and written, and the tasks are constructed to validate the method by demonstrating clear cases where non modulated plasticity fails completely but where the proposed approach succeeds. On a large, general language modeling task (PTB) there is a small but consistent improvement over LSTMS. The reviewers were very split on this submission, with two reviewers focusing on the lack of large improvements on large benchmarks, and the other reviewer focusing on the novelty and success of the method on simple tasks. The AC tends to side with the positive review because of the following observations: the method is novel and potentially will have long term impact on the field, the language modeling task seems like a poor fit to demonstrate the advantages of the dynamic plasticity, so focusing on that benchmark overly much is misleading, and the paper is high quality and interesting to the community. 
I enjoyed reading the paper myself and agree with some of the criticisms raised by the reviewers, but not all of them. In particular, I don t think it s a major issues that this work studies an explicit regularization scheme BECAUSE the state of our understanding of generalization in deep learning is so embarrassingly poor!!   Unlike a lot of work, this work is engaging with the *approximation* error and developing risk bounds (called "generalization error" here ... not my favorite term for the risk!) rather than just controlling the generalization gap. The simple proof in the bounded noiseless case was nice to see.  On the other hand, not being familiar with the work of Klusowski and Barron (2016), I m not willing to overrule the reviewers on judgments that this work is not novel enough. I would suggest the authors take control of this and paint a more detailed picture of how these two bodies of work relate, including how the proof techniques and arguments overlap.  Some other comments:   1. your theorem requires \lambda > 4, but then you re using \lambda   0.1. this seems problematic to me.  2. your "nonvacuous upper bound" is path norm/sqrt(n) ... but do the numbers in the table include the constants? looking at the constants that are likely to show up, (4Bn sqrt(2 log 2d), they are easily contributing a factor greater than 10 which would make these bounds vacuous as well.  you need to explain how you are calculating these numbers more carefully.  3. several times Arora et al and Neyshabur et al are cited when reference is being made to numerical experiments to show that existing bounds are vacuously large. But Dziugaite and Roy, who you cite for the term "nonvacuous", made an earlier analysis of path norm bounds in their appendix and point out that they are vacuous.   4. the paper does not really engage with the fact that you are unlikely to be exactly minimizing the functional J. any hope of bridging this gap?  5. the experiments in general are a bit too vaguely described. also, you control squared error but then only report classification error. would be interested to see both.
This paper proposes model poisoning (poisoned parameter updates in a federated setting) in contrast to data poisoning (poisoned training data). It proposes an attack method and compares to baselines that are also proposed in the paper (there are no external baselines). While model poisoning is indeed an interesting direction to consider, I agree with reviewer concerns that the relation to data poisoning is not clearly addressed. In particular, any data poisoning attack could be used as a model poisoning attack (just provide whatever updates would be induced by the poisoned data), so there is no good excuse to not compare to the existing strong data poisoning attacks. One reviewer raised concerns about lack of theoretical guarantees but I do not agree with these concerns (the authors correctly point out in the rebuttal that this is not necessary for an attack focused paper). I do feel there is room to improve the overall clarity/motivation (for instance, equation (1) is presented without any explanation and it is still not clear to me why this is the right formulation).
This paper seeks to shed light on why seq2seq models favor generic replies. The problem is an important one, unfortunately the responses proposed in the paper are not satisfactory. Most reviewers note problems and general lack of rigorousness in the assumptions used to produce the theoretical part of the paper (e.g., strong assumption of independence of generated words). The experiments themselves are not convincing enough to warrant acceptance by themselves.
The paper investigates a novel formulation of a stochastic, quasi Newton optimization strategy based on the natural idea of relaxing the secant conditions.  This is an interesting and promising idea, but unfortunately none of the reviewers recommended acceptance.  The reviewers unanimously fixated on weaknesses in the paper s technical presentation.  In particular, the reviewers expressed some dissatisfaction with many aspects, including:    Key details of the experimental evaluation were omitted (particularly concerning configuration of the baseline competitors), which is an essential aspect of reproducibility.  One consequence is that the reviewers were not confident in the veracity of the experimental comparison.    The reviewers struggled with a lack of clarity and accurate rendering of some key technical details.  An example is dissatisfaction with the non symmetry of the inverse Hessian approximation, which was not fully alleviated by the author responses.    The proposed approach does not appear to possess any intrinsic advantage over standard methods from a computational complexity perspective.  I think this is promising work, but a careful revision that strengthened the underlying technical claims appears necessary to make this a solid contribution.
AR1 finds that extension of the previously presented ICLR 18 paper are interesting and sufficient due to the provided analysis of universality and depth efficiency. AR2 is concerned with the lack of any concrete toy example between the proposed architecture and RNNs. Kindly make an effort to add such a basic step by step illustration for a simple chosen architecture e.g. in the supplementary material. AR3 is the most critical (the analysis TT RNN based on the product non linearity done before, particular case of rectifier non linearity is used, etc.)  Despite the authors cannot guarantee the existence of corresponding weight tensor W in less trivial cases, the overall analysis is very interesting and it is the starting point for further modeling. Thus, AC advocates acceptance of this paper. The review scores do not indicate this can be an oral paper, e.g. it currently is unlikely to be in top few percent of accepted papers. Nonetheless, this is a valuable and solid work.  Moreover, for the camera ready paper, kindly refresh your list of citations as a mere 1 page of citations feels rather too conservative. This makes the background of the paper and related work obscure to average reader unfamiliar with this topic, tensors, tensor outer products etc. There are numerous works on tensor decompositions that can be acknowledged:   Multilinear Analysis of Image Ensembles: TensorFaces by Vasilescou et al.   Multilinear Projection for Face Recognition via Canonical Decomposition by Vasilescou et al.   Tensor decompositions for learning latent variable models by Anandkumar et al.   Fast and guaranteed tensor decomposition via sketching by Anandkumar et al.  One good example of the use of the outer product (sums over rank one outer products of higher order) is paper from 2013. They perform higher order pooling on encoded feature vectors (although this seems to be the shallow setting) similar to Eq. 2 and 3 (this submission):   Higher order occurrence pooling on mid and low level features: Visual concept detection by Koniusz et al. (e.g. equations equations 49 and 50 or 1, 16 and 17 realize Eq. 3 and 13 in this submission)   Higher Order Occurrence Pooling for Bags of Words: Visual Concept Detection (similar follow up work)  Other related papers include:   Long term Forecasting using Tensor Train RNNs by Anandkumar et al.   Tensor Regression Networks with various Low Rank Tensor Approximations by Cao et al.  Of course, the authors are encouraged to cite even more related works.
The reviewers have agreed this work is not ready for publication at ICLR.
The paper studies a convolutional LSTM (ConvLSTM) based model (DRC: Deep Repeated ConvLSTM) trained through reinforcement, and shows that it performs better than other model free approaches, in particular in term of generalization. The ability to generalize is attributed to being able to plan. This last part is not completely convincing.  The paper is clearly written, the experiments are in 4 limited domains: Sokoban, Boxworld, MiniPacman, Gridworld. While diverse, tasks are still all similarly navigation in top down (2D) grid worlds. It is unclear what are the limits of the reach of this study. The experimental evidence presented here could also be interpreted as: local best response recognition of shapes (Conv) and memory of such patterns and associated actions (LSTM) are sufficient for all those environments.  Overall, this is an interesting direction, but it falls slightly short of being acceptable for publication at ICLR.
This paper is about unsupervised learning for ASR, by matching the acoustic distribution, learned unsupervisedly, with a prior phone lm distribution. Overall, the results look good on TIMIT. Reviewers agree that this is a well written paper and that it has interesting results.  Strengths   Novel formulation for unsupervised ASR, and a non trivial extension to previously proposed unsupervised classification to segmental level.   Well written, with strong results. Improved results and analysis based on review feedback.  Weaknesses   Results are on TIMIT   a small phone recognition task.   Unclear how it extends to large vocabulary ASR tasks, and tasks that have large scale training data, and RNNs that may learn implicit LMs. The authors propose to deal with this in future work.  Overall, the reviewers agree that this is an excellent contribution with strong results. Therefore, it is recommended that the paper be accepted.
This was a borderline paper and a very difficult decision to make.  The paper addresses a potentially interesting problem in approximate POMDP planning, based on simplifying assumptions that perception can be decoupled from action and that a set of sensors exhibits certain conditional independence structure.  As a result, a simple approach can be devised that incorporates a simple greedy perception method within a point based value iteration scheme.  Unfortunately, the assumptions the paper makes are so strong and seemingly artificial to the extent that they appear reverse engineered to the use of a simple perception heuristic.  In principle, such a simplification might not be a problem if the resulting formulation captured practically important scenarios, but that was not convincingly achieved in this paper indeed, another major limitation of the paper is its weak motivation.  In more detail, the proposed approach relies on decoupling of perception and action, which is a restrictive assumption that bypasses the core issue of exploration versus exploitation in POMDPS.  As model of active perception, the proposal is simplistic and somewhat artificial; the motivation for the particular cost model (cardinality of the sensor set) is particularly weak a point that was not convincingly defended in the discussion.  Perhaps the biggest underlying weakness is the experimental evaluation, which is inadequate to support a claim that the proposed methods show meaningful advantages over state of the art approaches in important scenarios.  A reviewer also raised legitimate questions about the strength of the theoretical analysis.  In the end, the reviewers did not disagree on any substantive technical matter, but nevertheless did disagree in their assessments of the significance of the contribution.  This is clearly a borderline paper, which on the positive side, was competently executed, but on the negative side, is pursuing an artificial scenario that enables a particularly simple algorithmic approach.  Despite the lack of consensus, a difficult decision has to be made nonetheless.  In the end, my judgement is that the paper is not yet strong enough for publication.  I would recommend the authors significantly strengthen the experimental evaluation to cover off at least two of the major shortcomings of the current paper: (1) The true utility of the proposed method needs to be better established against stronger baselines in more realistic scenarios.  (2) The relevance of the restrictive assumptions needs to be more convincingly established by providing concrete, realistic and more challenging case studies where the proposed techniques are still applicable.  The paper would also be improved if the theoretical analysis could be strengthened to better address the criticisms of Reviewer 4.
There has been a recent focus on proving the convergence of Bayesian fully connected networks to GPs. This work takes these ideas one step further, by proving the equivalence in the convolutional case.  All reviewers and the AC are in agreement that this is interesting and impactful work. The nature of the topic is such that experimental evaluations and theoretical proofs are difficult to carry out in a convincing manner, however the authors have done a good job at it, especially after carefully taking into account the reviewers’ comments.  
This paper makes the intriguing observation that a density model trained on CIFAR10 has higher likelihood on SVHN than CIFAR10, i.e., it assigns higher probability to inputs that are out of the training distribution. This phenomenon is also shown to occur for several other dataset pairs. This finding is surprising and interesting, and the exposition is generally clear. The authors provide empirical and theoretical analysis, although based on rather strong assumptions. Overall, there s consensus among the reviewers that the paper would make a valuable contribution to the proceedings, and should therefore be accepted for publication.
The paper proposes a novel way to ensemble multi class or multi label models based on a Wasserstein barycenter approach. The approach is theoretically justified and obtains good results. Reviewers were concerned with time complexity, and authors provided a clear breakdown of the complexity. Overall, all reviewers were positives in their scores, and I recommend accepting the paper.
This paper is extending the meta learning MAML method to the mixture case. Specifically, the global parameters of the method are now modeled as a mixture. The authors also derive the elaborate associated inference for this approach.  The paper is well written although Rev2 raises some presentation issues that can surely improve the quality of the paper, if addressed in depth.   The results do not convince any of the three reviewers. Rev3 asks for a clearer exposition of the results to increase convincingness. Rev2 and Rev1 also make similar comments.    Rev1 also questions the motivation of the approach, although the other two reviewers seem to find the approach well motivated. Although it certainly helps to prove the motivation within a very tailored to the method application, the AC weighted the opinion of all reviewers and did not consider the paper to lack in the motivation aspect.   The reviewers were overall not very impressed with this paper and that does not seem to stem from lack of novelty or technical correctness. Instead, it seems that this work is rather inconclusive (or at least it is presented in an inconclusive manner): Rev1 says that the important questions (like trade offs and other practical issues) are not answered, Rev2 suggests that maybe this paper is trying to address too much, and all three reviewers are not convinced by the experiments and derived insights.   Finally, Rev2 points out some inherent caveats of the method; although they do not seem to be severe enough to undermine the overall quality of the approach, it would be instructive to have them investigated more thoroughly (even if not completely solving them).
This paper presents a novel family of probabilistic approaches to computing the similarities between two sentences using bag of embeddings representations, and presents evaluations on a standard benchmark to demonstrate the effectiveness of the approach. While there seem to be no substantial disputes about the soundness of the paper in its current form, the reviewers were not convinced by the broad motivation for the approach, and did not find the empirical results compelling enough to serve as a motivation on its own. Given that, no reviewer was willing to argue that this paper makes an important enough contribution to be accepted.  It is unfortunate that one of the assigned reviewers—by their own admission—was not well qualified to review it and that a second reviewer did not submit a review at all, necessitating a late fill in review (thank you, anonymous emergency reviewer!). However, the paper was considered seriously: I can attest that both of the two higher confidence reviewers are well qualified to review work on problems and methods like these.
This paper presents an interesting strategy of curriculum learning for training neural networks, where mini batches of samples are formed with a gradually increasing level of difficulty.   While reviewers acknowledge the importance of studying the curriculum learning and the potential usefulness of the proposed approach for training neural networks, they raised several important concerns that place this paper bellow the acceptance bar: (1) empirical results are not convincing (R2, R3); comparisons on other datasets (large scale) and with state of the art methods would substantially strengthen the evaluation (R3); see also R2’s concerns regarding the comprehensive study; (2) important references and baseline methods are missing – see R2’s suggestions how to improve; (3) limited technical novelty   R1 has provided a very detailed review questioning novelty of the proposed approach w.r.t. Weinshall et al, 2018.   Another suggestions to further strengthen and extend the manuscript is to consider curriculum and anti curriculum learning for increasing performance (R1).  The authors provided additional experiment on a subset of 7 classes from the  ImageNet dataset, but this does not show the advantage of the proposed model in a large scale learning setting.  The AC decided that addressing (1) (3) is indeed important for understanding the contribution in this work, and it is difficult to assess the scope of the contribution without addressing them.  
This paper provides interesting discussions on the trade off between model accuracy and robustness to adversarial examples. All reviewers found that both empirical studies and theoretical results are solid. The paper is very well written. The visualization results are very intuitive. I recommend acceptance. 
This paper proposed a method that creates neural networks that can run under different resource constraints. The reviewers have consensus on accept. The pro is that the paper is novel and provides a practical approach to adjust model for different computation resource, and achieved performance improvement on object detection. One concern from reviewer2 and another public reviewer is the inconsistent performance impact on classification/detection (performance improvement on detection, but performance degradation on classification). Besides, the numbers reported in Table 1 should be confirmed: MobileNet v1 on Google Pixel 1 should have less than 120ms latency [1], not 296 ms.    [1] Table 4 of https://arxiv.org/pdf/1801.04381.pdf
This paper provides a novel and non trivial method for approximating the eigenvectors of the Laplacian, in large or continuous state environments. Eigenvectors of the Laplacian have been used for proto value functions and eigenoptions, but it has remained an open problem to extend their use to the non tabular case. This paper makes an important advance towards this goal, and will be of interest to many that would like to learn state representations based on the geometric information given by the Laplacian.   The paper could be made stronger by including a short discussion on why the limitations of this approach. Its an important new direction, but there must still be open questions (e.g., issues with the approach used to approximate the orthogonality constraint). It will be beneficial to readers to understand these issues.
No reviewer has made a strong case for accepting this paper or championed it so I am recommending rejecting it. The unfavorable reviewers, although they mention real issues, have not highlighted some of the most important barriers to accepting this work.  One major, but not necessarily dispositive, concern is that the paper only presents results on MNIST. However, even if we put aside this concern, there are several issues with the motivation and approach of this paper. If this technique is actually good at improving the model outside the clean image distribution, then the paper should show that and not just L2 worst case perturbations. To quote the intro of the paper: "How can deep learning systems successfully generalise and at the same time be extremely vulnerable to minute changes in the input?" The answer is: they don t generalize and this work does not show us improved generalization. Even a small amount of test error in the data distribution suggests that the closest test error to a given point will often be quite close to the starting point, although this is easier to see with linear models. The best way to fix this work would be to study (average case) error on noisy distributions (as in the concurrent submission https://openreview.net/forum?id S1xoy3CcYX ).
The reviewers raised a number of concerns including low readability/ clarity of the presented work and methodology, insufficient and at times unconvincing experimental evaluation of the proposed, and lack of discussion on pros and cons of the presented. The authors’ rebuttal addressed some of the reviewers’ comments but failed to address all concerns and reconfirmed that relatively large changes are still needed for the paper to be useful to the readers. Hence, although I believe this could be a very interesting paper, I cannot suggest it at this stage for presentation at ICLR.
Pros:   interesting novel formulation of policy learning in homogeneous swarms   multi stage learning process that trades off diversity and consistency (fig 1)  Cons:   implausible mechanisms like averaging weights of multiple networks   minor novelty   missing ablations of which aspect is crucial    dubious baseline results   no rebuttal  One reviewer out of three would have accepted the paper, the other two have major concerns. Unfortunately the authors did not revise the paper or engage with the reviewers to clear up these points, so as it stand the paper should be rejected.
This paper received high quality reviews, which highlighted numerous issues with the paper.  A common criticism was that the results in the paper seemed disconnected.  Numerous technical concerns were raised. Reading the responses, it seems that some of these issues are nonissues, but it seems also that the writing was not sufficiently up to the standard required of this type of technical work. I suggest the authors produce a rewrite and resubmit to the next ML conference, taking the criticisms they ve received here very seriously.
This paper suggests a problem with the standard ELBO for the multi modal case, and proposes a new objective to address this problem.  However, I (and some of the reviewers) disagree with the motivation.  First of all, there s no reason one can t train a separate encoder for every combination of modalities available, at least when there are only 2 or 3.  Failing that, one could simple optimize per example approximate posteriors without using an encoder.  Second, once you stop optimizing the ELBO, you ve lost the motivating principle for training VAEs, and must justify your new objective empirically.  Almost all of the results are (in my opinion) ambiguous plots of latent encodings.  Finally, a point made throughout the paper and discussions was that different modalities should give the same encodings, which is plainly false.  One of the reviewers made this point: "The fact that z_a !  z_b !  z_{a,b} should be expected if a and b provide different information. I don t see the problem with this.", which you dismiss.  Additionally, the encoder s job is to approximate the true posterior.  The true posteriors will in general be different for different modalities.  I would recommend focusing on ways to train the original ELBO in the presence of different modalities, instead of modifying it based on these intuitions. 
 pros:   The paper is clear and easy to read   Both Reviewer 1 and Reviewer 2 found the empirical evaluation to be good  cons:   Some of the reviewers felt that the proposed approach lacked novelty (e.g. with respect to Nogueira and Cho)   Some of the architecture choices seem complicated and it was not fully clear to the reviewers (even after the rebuttal) how and why things were working better in this approach than in other similar ones.  I think this is a good paper but it doesn t quite meet the bar for acceptance at this time.   
All reviewers agree that the presented audio data augmentation is very interesting, well presented, and clearly advancing the state of the art in the field. The authors’ rebuttal clarified the remaining questions by the reviewers. All reviewers recommend strong acceptance (oral presentation) at ICLR. I would like to recommend this paper for oral presentation due to a number of reasons including the importance of the problem addressed (data augmentation is the only way forward in cases where we do not have enough of training data), the novelty and innovativeness of the model, and the clarity of the paper. The work will be of interest to the widest audience beyond ICLR.
Improving the staleness of asynchronous SGD is an important topic. This paper proposed an algorithm to restrict the staleness and provided theoretical analysis. However, the reviewers did not consider the proposed algorithm a significant contribution. The paper still did not solve the staleness problem, and it was lack of discussion or experimental comparison with the state of the art ASGD algorithms. Reviewer 3 also found the explanation of the algorithm hard to follow.
The Reviewers noticed that the paper undergone many editions and raise concern about the content. They encourage improving experimental section further and strengthening the message of the paper. 
The paper presents a novel gradient estimator for optimizing VAEs with discrete latents, that is based on using a Direct Loss Minimization approach (as initially developed for structured prediction) on top of the Gumble max trick. This is an interesting and original alternative to the use of REINFORCE or Gumble Softmax. The approach is mathematically well detailed, but exposition could be easier to follow if it used a more standard notation. After clarifications by the authors, reviewers agreed that the main theorerm is correct. The proposed method is shown empirically to converge faster than Gumbel softmax, REBAR, and RELAX baselines in number of epochs. However, as questioned by one reviewer, the proposed method appears to require many more forward passes (evaluations) of the decoder for each example. Authors replied by highlighting that an argmax can be more computationally efficient than softmax (in cases when the discrete latent space is structured), and also clarified in the paper their use of an essential computational approximation they make for discrete product spaces. These are important aspects that affect computational complexity. But they do not address the question raised about using significantly more decoder evaluations for each example. A fair comparison for sampling based gradient estimation methods should rest on actual number of decoder evaluations and on resulting timing. The paper currently does not sufficiently discuss the computational complexity of the proposed estimator against alternatives, nor take this essential aspect into account in the empirical comparisons it reports.  We encourage the authors to refocus the paper and fully develop and showcase a use case where the approach could yield a clear a computational advantage, like the structured encoder setting they mentioned in the rebuttal. 
Strong paper on hierarchical RL with very strong reviews from people expert in this subarea that I know well. 
This paper proposes a new solution for tackling domain adaptation across disjoint label spaces. Two of the reviewers agree that the main technical approach is interesting and novel. The final reviewer asked for clarification of the problem setting which the authors have provided in their rebuttal. We encourage the authors to include this in the final version. However, there is also a consensus that more experimental evaluation would improve the manuscript and complete experimental details are needed for reliable reproduction.
This paper presents a new multi task training and evaluation set up called the Natural Language Decathlon, and evaluates models on it. While this AC is sympathetic to any work which introduces new datasets and evaluation tasks, the reviewers agreed amongst themselves that the paper is not quite ready for publication. The main concern is that multi task learning should show benefits of transferring representations or other model components between tasks, demonstrating better generalisation and less task specific overfitting, but that the results in the paper do not properly show this effect. A more thorough study of which tasks "interact constructively" and what model changes can properly exploit this needs to be done. With this further work, the AC has no doubt that this dataset and task suite, and associated models, will be very valuable to the NLP community.  I should note that there were some issues during the review period which lead to AC confidential communication between AC and authors, and AC and reviewers, to be leaked to the reviewers. It was due to an OpenReview bug, and no party is at fault. Through private discussion with the interested parties, we were able to resolve this matter, and through careful examination of the discussion, I am satisfied that the reviews and final recommendations of the reviewers were properly argued for and presented in good faith.
This paper explores an interpretation of generative models in terms of interventions on their latent variables.  The overall set of ideas seems novel and potentially useful, but the presentation is unclear, the goal of the method seems poorly defined, and the qualitative results (including the videos) are unconvincing.  I recommend you put work into factoring the ideas in this paper into smaller ones.  For instance, definition 1 is a mess.  I would also recommend the use of algorithm boxes.
Strengths: This paper is "thorough and well written", exploring the timbre transfer problem in a novel way. There is a video accompanying the work and some reviewers assessed the quality of the results as being good relative to other approaches. Two of the reviewers were quite positive about the work.  Weaknesses: Reviewer 2 (the lowest scoring reviewer) felt that the paper was a little too far from solving the problem to be of high significance and that there was:    too much focus on STFT vs. CQT    too little focus on getting WaveNet synthesis right    too limited experimental validation (too restricted choice of instruments)    poor resulting audio quality    feels too much of combining black boxes  AMT listening tests were performed, but better baselines could have been used. The author response addressed some of these points.  Contention:  An anonymous commenter noted that the revised manuscript added some names in the acknowledgements, thereby violating double blind review guidelines. However, the aggregated initial scores for this work were past the threshold for acceptance. Reviewer 2 was the most critical of the work but did not engage in dialog or comment on the author response.   Consensus: The two positive reviewers felt that this work is worth of presentation at ICLR. The AC recommends accept as poster unless the PC feel the issue of names in the Acknowledgements in an updated draft is too serious of an issue.  
 The authors presents a technique for training neural networks, through dynamic sparse reparameterization. The work builds on previous work notably SET (Mocanu et al., 18), but the authors propose to use an adaptive threshold for and a heuristic for determining how to reparameterize weights across layers.  The reviewers raised a number of concerns on the original manuscript, most notably 1) that the work lacked comparisons against existing dynamic reparameterization schemes, 2) an analysis of the computational complexity of the proposed method relative to other works, and that 3) the work is an incremental improvement over SET. In the revised version, the authors revised the paper to address the various concerns raised by the reviewers. To address weakness 1) the authors ran experiments comparing the proposed approach to SET and DeepR, and demonstrated that the proposed method performs at least as well, or is better than either approach. While the new draft is in the ACs view a significant improvement over the initial version, the reviewers still had concerns about the fact that the work appears to be incremental relative to SET, and that the differences in performance between the two models were not very large (although the author’s note that the differences are statistically significant). The reviewers were not entirely unanimous in their decision, which meant that the scores that this work received placed it at the borderline for acceptance. As such, the AC ultimately decide to recommend rejection, though the authors are encouraged to resubmit the revised version of the paper to a future venue. 
the proposed approach of predicting k nearest neighbouring examples as an auxiliary task is an interesting idea. however, the submission should have studied further on how those examples are predicted (e.g., sequence prediction is one, but you could try set prediction, or so on) rather than how sequential prediction of nearest neighbours is done together with different types of classifiers (many of which are arguably not necessarily suitable for classification,) which was a sentiment shared by all the reviewers.   more careful investigation of different ways in which nearest neighbour prediction could be incorporated and more careful/thorough analysis on how the incorporation of this auxiliary task changes the behaviours or properties of the representation would make it a much better paper (also with clearer writing.)
The paper proposes a novel network architecture for sequential learning, called trellis networks, which generalizes truncated RNNs and also links them to temporal convnets. The advantages of both types of nets are used to design trellis networks which appear to outperform state of art on several datasets.  The paper is well written and the results are convincing.
This paper proposes novel recurrent models for polyphonic music composition and demonstrates the approach with qualitative and quantitative evaluations as well as samples. The technical parts in the original write up were not very clear, as noted by multiple reviewers. During the review period, the presentation was improved. Unfortunately the reviewer scores are mixed, and are on the lower side, mainly because of the lack of clarity and quality of the results.
The paper proposes a framework for continual/lifelong learning that has potential to overcome the problems of catastrophic forgetting and data privacy.  R1, R2 and AC agree that the proposed method is not suitable for lifelong learning in its current state as it linearly increases memory and computational cost over time (for storing features of all points in the past and increasing model capacity with new tasks) without account for budget constraints.  The authors responded in their rebuttal that the data is not stored in the original form, but using feature representation (which is important for privacy issues). The main concern, however, was about the fact that one has to store information about all previous data points which is not feasible in lifelong learning. In the revision the authors have tried to address some of the R1’s and R2’s suggestion about taking into account the budget constraints. However more in depth analysis is required to assess feasibility and advantage of the proposed approach.  The authors motivate some of the key elements in their model as to protect privacy. However no actual study was conducted to show that this has been achieved.  The comments from R3 were too brief and did not have a substantial impact on the decision.  In conclusion, AC suggests that the authors prepare a major revision addressing suitability of the proposed approach for continual learning under budget constraints and for privacy preservation and resubmit for another round of reviews.   
Pros:    The paper is well written  Cons:    Not very novel    Evaluation only on sentiment classification, whereas approaches applicable in broader context exists     There are question re baselines (R3)  Neither reviewer was particularly enthusiastic about the paper, I believe, mostly because of the limited score and novelty.   
It is a simple but good idea to consider the choice of mini batch size as a multi armed bandit problem. Experiments also show a slight improvement compared to the best fixed batch size.  The main concerns from the reviewers are that (1) treating the choice of hyper parameters as a bandit problem is known and has been exploited in different context, and this paper is limited to the choice of the mini batch size, (2) the improvement in the test error is not significant. The authors  feedback did not solve the concerns raised by R2.  This paper conveys a nice idea but as the current form it falls slightly below the standard of the ICLR publications. One direction for improvement, as suggested by the reviewer, would be extending the idea for a wider hyper parameter selection problems.
Alternating minimization is surprisingly effective for low rank matrix factorization and dictionary learning problems. Better theoretical characterization of these methods is well motivated. This paper fills up a gap by providing simultaneous guarantees for support recovery as well as coefficient estimates for  linearly convergence to the true factors, in the online learning setting. The reviewers are largely in agreement that the paper is well written and makes a valuable contribution.  The authors are advised to address some of the review comments around relationship to prior work highlighting novelties.
The paper studies an narrowly focused but interesting problem   if the Visual Question answering model “FILM” from Perez et al (2018) is able to decide if “most” of the objects have a certain attribute or color. While the work itself is appreciate by the reviewers, concerns remain about the conclusion being limited in scope due to the synthetic nature of the data, and the analysis fairly narrow (a single model with a single very specific task). We encourage the authors to use reviewer feedback to make the manuscript stronger for a future deadline. 
This manuscript proposes a new algorithm for instance wise feature selection. To this end, the selection is achieved by combining three neural networks trained via an actor critic methodology. The manuscript highlight that beyond prior work, this strategy enables the selection of a different number of features for each example. Encouraging results are provided on simulated data in comparison to related work, and on real data.  The reviewers and AC note issues with the evaluation of the proposed method. In particular, the evaluation of computer vision and natural language processing datasets may have further highlighted the performance of the proposed method. Further, while technically innovative, the approach is closely related to prior work (L2X)   limiting the novelty.   The paper presents a promising new algorithm for training generative adversarial networks. The mathematical foundation for the method is novel and thoroughly motivated, the theoretical results are non trivial and correct, and the experimental evaluation shows a substantial improvement over the state of the art.
The paper extends an existing approach to imitation learning, GAIL (Generative Adversarial Imitation Learning, based on an adversarial approach where a policy learner competes with a discriminator) in several ways and demonstrates that the resulting approach can learn in settings with high dimensional observation spaces, even with a very low dimensional discriminator. Empirical results show promising performance on a (simulated) robotics block stacking task, as well as a standard benchmark   Walker2D (DeepMind control suite).  The reviewers and the AC note several potential weaknesses. Most importantly, the contributions of the paper are "muddled" (R2). The authors introduce several modifications to their baseline, GAIL, and show empirical improvements over the baseline. However, the presented experiments do systematically identify which modifications have what impact on the empirical results. For example, R2 mentions this for figure 4, where it appears on first look that the proposed approach is compared to the vanilla GAIL baseline   however, there appear to be differences from vanilla GAIL, e.g., in terms of reward structure (and possibly other modeling choices   how close is the GAIL implementation used to the original method, e.g., in terms of the policy learner and discriminator)? There is also confusion on which setting is addressed in which part of the paper, given that there is both a "RL+IL" and an "imitation only" component.  In their rebuttal, the authors respond to, and clarify some of the questions raised by the reviewers, but the AC and corresponding reviewers consider many issues to remain unclear. Overall, the presentation could be much improved by indicating, for each set of experiments, what research question or hypothesis it is designed to address, and to clearly indicate conclusions on each question once the results have been discussed. In its current state, the paper reads as a list of interesting and potentially highly valuable ideas, together with a list of empirical results. The real value of the paper should come in when these are synthesized into lessons learned, e.g., why specific results are observed and what novel insights they afford the reader. Overall, the paper will benefit from a thorough revision and is not considered ready for publication at ICLR at this stage.  The AC notes that they placed less weight on R3 s assessment, due to their relatively low confidence, because they appear not to be familiar with key related work (GAIL), and did not respond to further requests for comments in the discussion phase.  The AC also notes a potential weakness that was not brought up by the reviewers, and which they therefore did not weigh into their assessment of the paper, but nevertheless want to share to hopefully help improve a future version of the paper. Figure 6(b) should be interpreted with caution given that performance with a greater number of demonstrations (120 vs 60) showed lower performance. The authors note in the caption that one of the "120 demos" runs "failed to take of". This suggests that variance for all these runs may be underestimated with the currently used number of seeds. It is not clear what the shaded region indicates (another drawback) but if I interpret these as standard errors then this plot would suggest lower performance for higher numbers of demonstrations with some confidence   clearly that conclusion is unlikely to be correct.
Dear authors,  All reviewers liked your work. However, they also noted that the paper was hard to read, whether because of the notation or the lack of visualization.  I strongly encourage you to spend the extra effort making your work more accessible for the final version.
The authors present a method for fine grained entity tagging, which could be useful in certain practical scenarios.  I found the labeling of the CoNLL data with the fine grained entities a bit confusing.  The authors did not talk about the details of how the coarse grained labels were changed to fine grained ones.  This detail is important and is missing from the paper.  Moreover, there are concerns about the novelty of the work, both in terms of the task definition and the model (see the review of Reviewer 1, e.g.).  There is consensus amongst the reviewers, in that, their feedback is lukewarm about the paper.  
AR1 is is concerned that the only contribution of this work is  combining second order pooling with with a codebook style assignments. After discussions, AR1 still maintains that that the proposed factorization is a marginal contribution. AR2 feels that the proposed paper is highly related to numerous current works (e.g. mostly a mixture of existing contributions) and that evaluations have not been improved. AR3 also points that this paper lacks important comparisons for fairly evaluating the effectiveness of the proposed formulation and it lacks detailed description and discussion for the methods.  AC has also pointed several works to the authors which are highly related (but by no means this is not an exhaustive list and authors need to explore google scholar to retrieve more relevant papers than the listed ones):  [1] MoNet: Moments Embedding Network by Gou et al. (e.g. Stanford Cars via Tensor Sketching: 90.8 vs. 90.4 in this submission, Airplane: 88.1 vs. 87.3% in this submission, 85.7 vs. 84.3% in this submission) [2] Second order Democratic Aggregation by Lin et al. (e.g. Stanford Cars: 90.8 vs. 90.4 in this submission) [3] Statistically motivated Second order Pooling by Yu et al (CUB: 85%) [4] DeepKSPD: Learning Kernel matrix based SPD Representation for Fine grained Image Recognition by Engin et al. [5] Global Gated Mixture of Second order Pooling for Improving Deep Convolutional Neural Networks by Q. Wang et al. (512D representations) [6] Low rank Bilinear Pooling for Fine Grained Classification  by S. Kong et al. (CVPR I believe). They get some reduction of size of 10x less than tensor sketch, higher results than here by some >2% (CUB), and all this obtained in somewhat more sophisticated way.  The authors brushed under the carpet some comparisons. Some methods above are simply better performing even if cited, e.g. MoNet [1] uses sketching and seems a better performer on several datasets, see [2] that uses sketching (Section 4.4), see [5] which also generates compact representation (8K). [4] may be not compact but the whole point is to compare compact methods with non compact second order ones too (e.g. small performance loss for compact methods is OK but big loss warrants a question whether they are still useful). Approach [6] seems to also obtain better results on some sets (common testbed comparisons are essentially encouraged).   At this point, AC will also point authors to sparse coding methods on matrices (bilinear) and tensors (higher order) from years 2013 2018 (TPAMI, CVPR, ECCV, ICCV, etc.). These all methods can produce compact representations (512 to 10K or so) of bilinear or higher order descriptors for classification. This manuscript fails to mention this family of methods.  For a paper to be improved for the future, the authors should consider the following:   make a thorough comparison with existing second order/bilinear methods in the common testbed (most of the codes are out there on line)   the authors should vary the size of representation (from 512 to 8K or more) and plot this against accuracy   the authors should provide theoretical discussion and guarantees on the quality of their low rank approximations (e.g. sketching has clear bounds on its approximation quality, rates, computational cost). The authors should provide some bounds on the loss of information in the proposed method.   authors should discuss the theoretical complexity of proposed method (and other methods in the literature)  Additionally, the authors should improve their references and the story line. Citing  (Lin et al. (2015)) in Eq. 1 and 2 as if they are the father of bilinear pooling is misleading. Citing (Gao et al. (2016)) in the context of polynomial kernel approximation in Eq. 3 to obtain bilinear pooling should be preceded with earlier works that expand polynomial kernel to obtain bilieanr pooling. AC can think of at least two papers from 2012/2013 which do derive bilinear pooling and could be cited here instead. AC encourages the authors to revise their references and story behind bilinear pooling to give unsuspected readers a full/honest story of bilinear representations and compact methods (whether they are branded as compact or just use sketching etc., whether they use dictionaries or low rank representations).  In conclusion, it feels this manuscript is not ready for publication with ICLR and requires a major revision. However, there is some merit in the proposed direction and authors are encouraged to explore further.
This paper proposes model based reinforcement learning algorithms that have theoretical guarantees. These methods are shown to good results on Mujuco benchmark tasks. All of the reviewers have given a reasonable score to the paper, and the paper can be accepted.
The paper studies the mismatch between value estimation in RL from finite vs. infinite trajectories. This is an interesting problem, but the reviewers raised concerns regarding (1) the consistency and coherence of the story (2) the significance of theoretical analysis and (3) significance of the results. I appreciate that the authors made significant changes to the paper to address the comments. However, given the extent of changes, I think another review cycle is needed to check the details of the paper again.
The authors identify a source of bias that occurs when a model overestimates the importance of weak features in the regime where sufficient training data is not available. The bias is characterized theoretically, and demonstrated on synthetic and real datasets. The authors then present two algorithms to mitigate this bias, and demonstrate that they are effective in experimental evaluations.   As noted by the reviewers, the work is well motivated and clearly presented. Given the generally positive reviews, the AC recommends that the work be accepted. The authors should consider adding additional text describing the details concerning Figure 3 in the appendix. 
The authors provide a new analysis of generalization in deep linear networks, provide new insight through the role of "task structure". Empirical findings are used to cast light on the general case. This work seems interesting and worthy of publication.
AR1 asks for a clear experimental evaluation showing that capsules and dynamic routing help in the GCN setting. After rebuttal, AR1 seems satisfied that routing in CapsGNN might help generate  more representative graph embeddings from different aspects . AC strongly encourages the authors to improve the discussion on these  different aspects  as currently it feels vague. AR2 is initially concerned about experimental evaluations and whether the attention mechanism works as expected, though, he/she is happy with the revised experiments. AR3 would like to see all biological datasets included in experiments. He/she is also concerned about the lack of ability to preserve fine structures by CapsGNN. The authors leave this aspect of their approach for the future work.  On balance, all reviewers felt this paper is a borderline paper. After going through all questions and responses, AC sees that many requests about aspects of the proposed method have not been clarified by the authors. However, reviewers note that the authors provided more evaluations/visualisations etc. The reviewers expressed hope (numerous times) that this initial attempt to introduce capsules into GCN will result in future developments and  improvements. While AC thinks this is an overoptimistic view, AC will give the authors the benefit of doubt and will advocate a weak accept.  The authors are asked to incorporate all modifications requested by the reviewers. Moreover,  Graph capsule convolutional neural networks  is not a mere ArXiV work. It is an ICML workshop paper. Kindly check all ArXiV references and update with the actual conference venues.
The paper presents an interesting empirical analysis showing that increasing the batch size beyond a certain point yields no decrease in time to convergence. This is an interesting finding, since it indicates that parallelisation approaches might have their limits. On the other hand, the study does not allow the practitioners to tune their hyperparamters since the optimal batch size is dependent on the model architecture and the dataset. Furthermore, as also pointed out in an anonymous comment, the batch size is VERY large compared to the size of the benchmark sets. Therefore, it would be nice to see if the observation carries over to large scale data sets, where the number of samples in the mini batch is still small compared to the total number of samples. 
The paper proposes an interesting neural architecture for traffic flow forecasting, which is tested on a number of datasets. Unfortunately, the lack of clarity as well as  precision  in writing appears to be a big issue for this paper, which prevents it from being accepted for publication in its current form. However, the reviewers did provide valuable feedback regarding writing, explanation, presentation and structure,  that the paper would benefit from. 
This paper generated a lot of discussion (not all of it visible to the authors or the public).   R1 initially requested reasonable comparisons, but after the authors provided a response (and new results), R1 continued to recommend rejecting the paper simply because they personally did not find the manuscript insightful. Despite several requests for clarification, we could not converge on a specific problem with the manuscript. Ungrounded gut feelings are not grounds for rejection.   After an extensive discussion, R2 and R3 both recommend accepting the paper and the AC agrees. Paper makes interesting contributions and will be a welcome addition to the literature. 
As the reviewers point out, the paper is below the acceptance standard of ICLR due to low novelty, unclear presentation, and lack of experimental comparison against the state of the art baselines.
Strengths: This paper provides a useful review of some of the recent work on gradient estimators for discrete variables, and proposes both a computationally more efficient variant of one, and a new estimator based on piecewise linear functions.  Weaknesses:  Many new ideas are scattered throughout the paper.  The notation is a bit dense.  Comparisons to RELAX, which had better results than REBAR, are missing.  Finally, it seems that REBAR was trained with a fixed temperature, instead of optimizing it during training, which is one of the main benefits of the method.  Points of contention: Only R1 mentioned the omission of REBAR and RELAX.  A discussion and a few comparisons to REBAR were added to the paper, but only in a few experiments.  Consensus:  This paper is borderline.  I agree with R1: quality 6, clarity 8, originality 6, significance 4.  All reviewers agreed that this was a decent paper but I think that R2 and R3 were relatively unfamiliar with the existing literature.  Update for clarification:    This section has been added to clarify the reasons for rejection.  The abstract of the paper states:  "We show that the commonly used Gumbel Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better performance in variational inference and on binary optimization tasks."  The fact that Gumbel Softmax is biased is well known.  Reducing its bias was the motivation for developing the _exactly_ unbiased REBAR method, which already has similar asymptotic complexity.  A major side benefit of using an exactly unbiased estimator is that the estimator s hyperparameters can be automatically tuned to reduce variance, as in REBAR and RELAX.  This paper focuses on methods for reducing bias and variance, but hardly discusses related methods that already achieved its stated aims. This a major weakness of the paper.  The experiments only compared with REBAR, and did not even tune the temperature to reduce variance (removing one of its major advantages).  This reject decision is not made mainly on lack of experiments or state of the art results.  It s because the idea of reducing the bias of continuous relaxation based gradient estimators has already been fruitfully explored, and zero bias CR estimators have been developed, but this work mostly ignores them.  However, thorough experiments are always going to be necessary for a paper proposing biased estimators, because there are already many such estimators, and little theory to say which ones will work well in which situations.  Suggestions to improve the paper:  Run experiments on all methods that directly measure bias and variance.  Incorporate discussion of REBAR throughout, not just in an appendix.  Run comparisons against REBAR and RELAX without crippling their ability to reduce variance.   Do more to characterize when different estimators will be expected to be effective.
The reviewers agree that this is a novel paper with a convincing evaluation.
All three reviewers raised the issues that (a) the problem tackled in the paper was insufficiently motivated, (b) the solution strategy was also not sufficiently motivated and (c) the experiments had serious methodological issues.
The paper presents a variational inequality perspective on the optimization problem arising in GANs. Convergence of stochastic gradient descent methods (averaging and extragradient variants) is given under monotonicity (or convex) assumptions. In particular, binlinear saddle point problem is carefully studied with batch and stochastic algorithms. Experiments on CIFAR10 with WGAN etc. show that the proposed averaging and extrapolation techniques improve the GAN training in such a nonconvex optimization practices.  General convergence results in the context of general non monotone VIPs is still an open problem for future exploration. The questions raised by the reviewers are well answered. The reviewers unanimously accept the paper for ICLR publication.
The paper proposes a new framework for out of distribution detection, based on variational inference and a prior Dirichlet distribution.  The reviewers and AC note the following potential weaknesses: (1) arguable and not well justified choices of parameters and (2) the performance degradation under many classes (e.g., CIFAR 100).  For (2), the authors mentioned that this is because "there are more than 20% of misclassified test images". But, AC rather views it as a limitation of the proposed approach. The out of detection detection problem is a one or two classification task, independent of how many classes exist in the neural classifier.  In overall, the proposed idea is interesting and makes sense but AC decided that the authors need more significant works to publish the work.
This paper studies the relationship between flatness in parameter space and generalization. They show through visualization experiments on MNIST and CIFAR 10 that there is no obvious relationship between the two. However, the reviewers found the motivation for the visualization approach unconvincing and further found significant overlap between the proposed method and that of Ross & Doshi. Thus the paper should improve its framing, experimental insights and relation to prior work before being ready for publication.
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciate the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
This paper does two things. First, it proposes an approach to estimating the mutual information between the input, X, or target label, Y, and an internal representation in a deep neural network, L, using MINE (for I(Y;L)) or a variation on MINE (for I(X;L)) and noise regularization (estimating I(X;L+ε), where ε is isotropic Gaussian white noise) to avoid the problem that I(X;L) is infinite for deterministic networks and continuous X. Second, it attempts to validate the information bottleneck theory of deep learning (Tishby and Zaslavsky, 2015) by exploring an approach to training DNNs that optimizes the information bottleneck Lagrangian, I(Y;L) − βI(X;L+ε), layerwise instead of using cross entropy and backpropagation. Experiments on MNIST and CIFAR 10 show improvements for the layerwise training over cross entropy training. The penalty on I(X;L+ε) is described as being analogous to weight decay. The reviewers raised a number of concerns about the paper, the most serious of which is that the claim that the layerwise training results validate the information bottleneck theory of deep learning is too strong. In the AC s opinion, R1 s critique that "[i]f the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?" is critical, and the authors  reply that "this quantity is in fact a more appropriate measure for “compactness” or “complexity” than the mutual information itself" undermines their claim that they are validating the information bottleneck theory of deep nets because the information bottleneck theory claims to be using mutual information. The AC also suggests that if the authors wish to continue this work and submit it to another venue, they (1) discuss the fact that MINE estimates only a lower bound that may be quite loose in practice and (2) say in their experimental section whether or not the variance of the regularizing noise was tuned as a hyperparameter, and if so, how results varied with different amounts of noise. Finally, the AC regrets that only one reviewer participated in the discussion (in a very minimal way), despite the reviewers  receiving several reminders that the discussion is a defining feature of the ICLR review process.
The paper proposes an approach to define an "interpretable representation", in particular for the case of patient condition monitoring. Reviewers point to several concerns, including even the definition of explainability and limited significance. The authors tried to address the concerns but reviewers think the paper is not ready for acceptance. I concur with them in rejecting it.
Reviewers mostly recommended to reject after engaging with the authors, however since not all author answers have been acknowledged by reviewers, I am not sure if there are any remaining issues with the submission. I thus lean to recommend to reject and resubmit. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
The paper describes the use of tactile sensors for exploration.  An important topic which has been addressed in various previous publications, but is unsolved to date.  The research and the paper are unfortunately in a raw state.  Rejected unanimously by the reviewers, without rebuttal chances used by the authors.
This work presents a reconstruction GAN with an additional classification task in the objective loss function. Evaluations are carried out on medical and non medical datasets.   Reviewers raise multiple concerns around the following:    Novelty (all reviewers)   Inadequate comparison baselines (all reviewers)   Inadequate citations. (R2 & R3)  Authors have not offered a rebuttal. Recommendation is reject. Work may be more suitable as an application paper for a medical conference or journal. 
This paper investigates copying mechanisms and reward functions in sequence to sequence models for question generation. The key findings are threefold: (1) when the alignments between input and output are weak, it is better to use latent copying mechanism to soften the model bias toward copying, (2) while policy gradient methods might be able to improve automatic scores, their results poorly align with human evaludation, and (3) the use of adversarial objective also does not lead to useful training signals.       Pros: The task is well motivated and the paper presents potentially useful negative results on policy gradient and adversarial training.  Cons: All reviewers found the clarity and organization of the paper requires improvements. Also, the proposed methods are reletively incremental and the empirical results are not strong. While the rebuttal answered some of the clarification questions, it does not address major concerns about the novelty and contributions.  Verdict: Reject due to relatively weak contributions and novelty.
Since the reviewers unanimously recommended rejecting this paper, I am also recommending against publication. The paper considers an interesting problem and expresses some interesting modeling ideas. However, I concur with the reviewers that a more extensive and convincing set of experiments would be important to add. Especially important would be more experiments with simple extensions of previous approaches and much simpler models designed to solve one of the tasks directly, even if it is in an ad hoc way. If we assume that we only care about results, we should first make sure these particular benchmarks are difficult (this should not be too hard to establish more convincingly if it is true) and that obvious things to try do not work well.
The paper proposes a technique for defending against adversarial examples that relies on averaging pixels that are close to each other both in position and value. This approach seems to be an interesting preprocessing technique in the robust training pipeline. However, the actual claims made are not well supported and, in fact, seem somewhat implausible. 
This paper addresses the issue of numerical rounding off errors that can arise when using latent variable models for data compression,  e.g., because of differences in floating point arithmetic across different platforms (sender and receiver). The authors propose using neural networks that perform integer arithmetic (integer networks) to mitigate this issue. The problem statement is well described, and the presentation is generally OK, although it could be improved in certain aspects as pointed out by the reviewers. The experiments are properly carried out, and the experimental results are good. Thank you for addressing the questions raised by the reviewers. After taking into account the author s responds, there is consensus that the paper is worthy of publication. I therefore recommend acceptance. 
The main goal of the submission is to figure out a way to produce less "noisy" saliency maps. The RectGrad method uses some thresholding during backprop, like Guided Backprop. The visuals of the proposed method are good, but the reviewers rightfully point out that evaluating whether the proposed method is any good is not obvious. The ROAR/KAR results are perhaps not telling the whole story (and the authors claim that RectGrad is not expected to get a high ROAR score, but I would like to see this developed more in a further version of this work).  Generally, I feel like there was a healthy back and forth between authors and R3 on the main concerns of this work. I agree that the mathematical justification for RectGrad seems not fully developed. Given all of these concerns, at this point I cannot support acceptance of this work at ICLR.
The paper describes  a method for the link prediction problem in both directed and undirected hypergraphs.  While the problem discussed in the paper is clearly importnant and interesting, all reviewers agree that the novelty of the proposed approach is somewhat limited given the prior art.
The reviewers agree  that the paper is worthy of publication at ICLR, hence I recommend accept.  Regarding section 4.3 of the submission and the claim that this paper presents the first insight for existing work from a divergence minimization perspective, as pointed out by R2, I went and checked the details of RAML and they have similar insights in their equations (5) and (8). Please make this clearer in the paper. Regarding evaluation using greedy search instead of beam search, please consider using beam search for reporting test performance as this is the standard setup in sequence prediction. Please take my comments and the reviews into account an prepare the final version.
The submission proposes a hierarchical clustering approach (nested means clustering) to determine good quantization intervals for non uniform quantization.  An empirical validation shows improvement over a very closely related approach (Zhu et al, 2016).  There was an overall consensus that the literature review was insufficient in its initial form.  The authors have proposed to extend it somewhat.  Other concerns are related to the novelty of the technique (R4 was particularly concerned about novelty over Zhu et al, 2016).  Two reviewers were against acceptance, and one was positive about the paper.  Due to the overall concerns about the novelty of the approach, and that these concerns were confirmed in discussion after the rebuttal, this paper is unlikely to meet the threshold for acceptance to ICLR.
The paper proposes an original idea for training a generative model based on an objective inspired by a VAE like evidence lower bound (ELBO), reformulated as two KL terms, which are then approximately optimized by two GANs. They thus use implicit distributions for both the posterior and the conditional likelihood. The idea is original and intriguing. But reviewers and AC found that the paper currently suffered from the following weaknesses: a) The presentation of the approach is unclear, due primarily to the fact that it doesn t throughout unambiguously enough separate the VAE like ELBO *inspiration*, from what happens when replacing the two KL terms by GANs, i.e. the actual algorithm used. This is a big conceptual jump that would deserve being discussed and analyzed more carefully and thoroughly. b) Reviewers agreed that the paper does not sufficiently evaluate the approach in comparative experiments with alternatives, in particular its generative capabilities, in addition to the provided evaluations of the learned representation on downstream tasks. Reviewers did not reach a clear consensus on this paper, although discussion led two of them to revise their assessment score slightly towards each other s. One reviewer judged the paper currently too confusing (point a) putting more weight on this aspect than the other reviewers.  Based on the paper and the review discussion thread, the AC judges that while it is an original, interesting and potentially promising approach, its presentation can and should be much clarified and improved. 
The paper studies safer policy improvement based on non expert demonstrations.  The paper contains some interesting ideas, and is supported by reasonable empirical evidence.  Overall, the work has a good potential.  The author response was also helpful.  That said, after considering the paper and rebuttal, the reviewers were not convinced the paper is ready for publication, as the significance of this work is limited by a rather strong assumption (see reviews for details).  Furthermore, the presentation of the paper also requires some work to improve (see reviews for detailed comments).
The authors consider the interesting and important problem of how to train a robust driving policy without allowing unsafe exploration, an important challenge for real world training scenarios. They suggest that both good and intentionally bad human demonstrations could be used, with the intuition being that humans can readily produce unsafe exploration such as swerving which can then be learnt using both positive and negative regressions. The reviewers all agree that the paper would not appeal to or have relevance for the wider community. The reviewers also agree that the main ideas are not well presented, that some of the claims are confusing, and that the writing is not technical enough. They also question the thoroughness of the empirical validation. 
The paper builds up on the gated fusion network architectures, and adapt those approaches to reach improved results.  In that it is incrementally worthwhile.  All the same, all reviewers agree that the work is not yet up to par.  In particular, the paper is only incremental, and the novelty of it is not clear.  It does not relate well to existing work in this field, and the results are not rigorously evaluated; thus its merit is unclear experimentally.  
The paper addresses the problem semantic segmentation using a sequential patch based model. I agree with the reviewers that the contributions of the paper are not enough for a machine learning venue: (1) there has been prior work on using sequence models for segmentation and (2) the complexity of the proposed approach is not fully justified. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account and improve the paper.
The paper makes two fairly incremental contributions regarding training binarized neural networks: (1) the swish based STE, and (2) a regularization that pushes weights to take on values in { 1, +1}. Reviewer1 and reviewer2 both pointed out concerns about the incremental contribution, the thoroughness of the evaluation, the poor clarity and consistency of the writing. Reviewer3 was muted during the discussion. Given the valid concerns from reviewer1/2, this paper is recommended for rejection. 
This manuscript proposes an implicit generative modeling approach for the non linear CCA problem. One contribution is the proposal of Conditional Mutual Information (CMI) as a criterion to capture nonlinear dependency, resulting in an objective that can be solved using implicit distributions. The work seems to be well motivated and of interest to the community.  The reviewers and AC opinions were mixed, and the rebuttal did not completely address the concerns. In particular, a reviewer pointed out an issue with a derivation in the paper, and the issue was not satisfactorily resolved by the authors. Some additional reading suggests that the misunderstanding may be partially due to incomplete notation and other issues with clarity of writing.
AR1 and AR3 have found this paper interesting in terms of replacing the spectral operations in GCN by wavelet operations. However, AR4 was more critical about the poor complexity of the proposed method compared to approximations in Hammond et al. AR4 was also right to find the proposed work similar to Chebyshev approximations in ChebNet and to highlight that the proposed approach is only marginally better than GCN. On balance, all reviewers find some merit in this work thus AC advocates an accept. The authors are asked to keep the contents of the final draft as agreed with AR4 (and other reviewers) during rebuttal without making any further theoretical changes/brushing over various new claims/ideas unsolicited by the reviewers (otherwise such changes would require passing the draft again through reviewers).
The paper was judged by the reviewers as providing interesting ideas, well written and potentially having impact on future research on NN optimization.  The authors are asked to make sure they addressed reviewers comments clearly in the paper.
This paper proposes a method called approximate empirical Bayes to learn both the weights and hyperparameters. Reviewers have had a mixed feeling about this paper. Reviewers agree that the novelty of this paper is limited since AEB is already a well known method (in fact, iterative conditional modes is a well known algorithm). Unfortunately, the paper completely ignores the huge literature on this topic; the previous reference to use AEB is from McInerney (2017).  Another issue is that the paper seems to be unaware of any issues that this type of approach might have. Here is a reference that discusses some problems with this type of approach:  "Deterministic Latent Variable Models and their Pitfalls" Max Welling∗ Chaitanya Chemudugunta, Nathan Sutter, 2008  The experiments presented in the paper are interesting, but then are not really doing a good job to assess why the method works well here even though in theory it should not be as good as the exact empirical Bayes method.   This paper does not meet the bar for acceptance at ICLR and therefore I recommend a reject for this paper. 
This paper proposes a new method to mine sentence from Wikipedia and use them to train an MT system, and also a topic based loss function. In particular, the first contribution, which is the main aspect of the proposal is effective, outperforming methods for fully unsupervised learning.  The main concern with the proposed method, or at least it s description in the paper, is that it isn t framed appropriately with respect to previous work on mining parallel sentences from comparable corpora such as Wikipedia. Based on interaction in the reviews, I feel that things are now framed a bit better, and there are additional baselines, but still the explanation in the paper isn t framed with respect to this previous work, and also the baselines are not competitive, despite previous work reporting very nice results for these previous methods.  I feel like this could be a very nice paper at some point if it s re written with the appropriate references to previous work, and experimental results where the baselines are done appropriately. Thus at this time I m not recommending that the paper be accepted, but encourage the authors to re submit a revised version in the future.
The submission proposes a method that combines sparsification and low rank projections to compress a neural network.  This is in line with nearly all state of the art methods.  The specific combination proposed in this instance are SVD for low rank and localized group projections (LGP) for sparsity.  The main concern about the paper is the lack of stronger comparison to sota compression techniques.  The authors justify their choice in the rebuttal, but ultimately only compare to relatively straightforward baselines.  The additional comparison with e.g. Table 6 of the appendix does not give sufficient information to replicate or to know how the reduction in parameters was achieved.  The scores for this paper were  borderline, and the reviewers were largely in consensus with their scores and the points raised in the reviews.  Given the highly selective nature of ICLR, the overall evaluations and remaining questions about the paper and comparison to baselines indicates that it does not pass the threshold for acceptance.
While it appears that the authors have done significant amount of work to investigate this topic, there are concerns that the theorems are not rigorously/precisely presented, and it is unclear how they can guide the design and training of neural network models in practice. The response and revision of the authors do not provide sufficient materials to address these concerns. 
This paper proposes a meta learning algorithm that extends MAML, particularly focusing on multimodal task distributions. The paper is generally well written, especially with the latest revisions, and the qualitative experiments show some interesting structure recovered. The primary weakness of the paper is that the experiments are largely on relatively simple benchmarks, such as Omniglot and low dimensional regression problems. Meta learning papers with convincing results have shown results on MiniImagenet, CIFAR, CelebA, and/or other natural image datasets. Hence, the paper would be more compelling with more difficult experimental settings. In the paper s current form, the reviewers and the AC agree that it does not meet the bar for ICLR.
This paper considers "prototypes" in machine learning, in which a small subset of a dataset is selected as representative of the behavior of the models. The authors propose a number of desiderata, and outline the connections to existing approaches. Further, they carry out evaluation with user studies to compare them with human intuition, and empirical experiments to compare them to each other. The reviewers agreed that the search for more concrete definitions of prototypes is a worthy one, and they appreciated the user studies.  The reviewers and AC note the following potential weaknesses: (1) the specific description of prototypes that the authors are using is not provided precisely, (2) the desiderata was found to be informal, leading to considerable confusion regarding the choices that are made and their compatibility with each other, (3) concerns in the evaluation regarding the practicality and the appropriateness of the user study for the goals of the paper.  Although the authors provided detailed responses to these concerns, most of them still remained. Both reviewer 1 and reviewer 2 encourage the authors to define the prototypes defined more precisely, providing motivation for the various choices therein. Even though some of the concerns raised by reviewer 3 were addressed, it still remains to be seen how scalable the approach is for real world applications.  For these reasons, the reviewers and the AC feel that the authors would need to make substantial improvements for the paper to be accepted.
The presented paper introduces a method to represent neural networks as logical rules of varying complexity, and demonstrate a tradeoff between complexity and error. Reviews yield unanimous reject, with insufficient responses by authors.  Pros: + Paper well written  Cons:   R1 states inadequacy of baselines, which authors do not address.   R3&4 raise issues about the novelty of the idea.   R2&4 raise issues on limited scope of evaluation, and asked for additional experiments on at least 2 datasets which authors did not provide.  Area chair notes the similarity of this work to other works on network compression, i.e. compression of bits to represent weights and activations. By converting neurons to logical clauses, this is essentially a similar method. Authors should familiarize themselves with this field and use it as a baseline comparison. i.e.: https://arxiv.org/pdf/1609.07061.pdf 
The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
Dear authors,  The reviewers pointed out a number of concerns about this work. It is thus not ready for publication. Should you decide to resubmit it to another venue, please address these concerns.
This paper is an analysis of the phenomenon of example forgetting in deep neural net training. The empirical study is the first of its kind and features convincing experiments with architectures that achieve near state of the art results. It shows that a portion of the training set can be seen as support examples. The reviewers noted weaknesses such as in the measurement of the forgetting itself and the training regiment. However, they agreed that their concerns we addressed by the rebuttal. They also noted that the paper is not forthcoming with insights, but found enough value in the systematic empirical study it provides.
The paper presents a modification of the convolution layer, where the convolution weights are generated by another convolution operation. While this is an interesting idea, all reviewers felt that the evaluation and results are not particularly convincing, and the paper is not ready for acceptance.
The paper proposes an interesting data dependent regularization method for orthogonal low rank embedding (OLE). Despite the novelty of the method, the reviewers and AC note that it s unclear whether the approach can extend other settings with multi class or continuous labels or other loss functions. 
The paper adds a new level of complexity to neural networks, by modulating activation functions of a layer as a function of the previous layer activations.  The method is evaluated on relatively simple vision and language tasks.  The idea is nice, but seems to be a special case of previously published work; and the results are not convincing.  Four of five reviewers agree that the work would benefit from: improving comparisons with existing approaches, but also improving its theoretical framework, in light of competing approaches.
The paper addresses the problem of learning to (re)rank slates of search results while optimizing some performance metric across the entire list of results (the slate). The work builds on a wealth of prior work on slate optimization from the information retrieval community, and proposes a novel approach to this problem, an extension of pointer networks, previously used in sequence learning tasks.   The paper is motivated by an important real world application, and has potential for significant practical impact. Reviewers noted in particular the valuable evaluation in an A/B test against a strong production system   showing that the work has practical impact. Reviewers positively noted the discussion of practical issues related to applying the work at scale. The paper was found to be clearly written, and demonstrating a thorough understanding of related work.  The authors and AC also note several potential weaknesses. Several of these were addressed by the authors, as follows. R3 asked for more breadth on metrics, and additional clarifications   the authors provided the requested information. Several questions were raised regarding the diverse clicks setting and choice of hyperparameter \eta   both were discussed in the rebuttal. Further analysis / discussion of computational and performance trade offs are requested and discussed.  Overall, the main drawback of the paper, raised by all three reviewers, is the size of the contribution. The paper extends an approach called "pointer networks" to the model application setting considered here. The reviewers and AC agree that, while practically relevant and interesting, the research contribution of the resulting approach limited. As a result, the recommendation is to not accept the paper for publication at ICLR in its current form. 
The paper suggests a new measurement of layer wise margin distributions for generalization ability. Extensive experiments are conducted. Though there lacks a solid theory to explain the phenomenon. The majority of reviewers suggest acceptance (9,6,5). Therefore, it is proposed as probable accept.
The paper proposes a learning by teaching (LBT) framework to train an implicit generative model via an explicit one. It is shown experimentally, that the framework can help to avoid mode collapse. The reviewers commonly raised the question why this is the case, which was answered in the rebuttal by pointing to the differences between the KL  and the JS divergence and by showing a toy problem for which the JS divergence has local minima while the KL divergence has not. However, it still remains unclear why this should be generally and for explicit models with insufficient capacity the case, and if the model will be scalable to larger settings, therefore the paper can not be accepted in the current form.
Dear authors,  All reviewers pointed to severe issues with the analysis, making the paper unsuitable for publication to ICLR. Please take their comments into account should you decide to resubmit this work.
Pros:   novel, general idea for hard exploration domains   multiple additional tricks   ablations, control experiments   well written paper   excellent results on Montezuma  Cons:   low sample efficiency (2B+ frames)   unresolved questions (non episodic intrinsic rewards)   could have done better apples to apples comparisons to baselines  The reviewers did not reach consensus on whether to accept or reject the paper. In particular, after multiple rounds of discussion, reviewer 1 remains adamant that the downsides of the paper outweigh its good points. However, given that the other three reviewers argue strongly and credibly for acceptance, I think the paper should be accepted.
This paper proposes a meta learning algorithm that performs gradient based adaptation (similar to MAML) on a lower dimensional embedding. The paper is generally well written, and the reviewers generally agree that it has nice conceptual properties. The method also draws similarities to LEO. The main weakness of the paper is with regard to the strength of the experimental results. In a future version of the paper, we encourage the authors to improve the paper by introducing more complex domains or adding experiments that explicitly take advantage of the accessibility of the task embedding. Without such experiments that are more convincing, I do not think the paper meets the bar for acceptance at ICLR.
The authors provide an interesting method to infuse hierarchical information into existing word vectors. This could help with a variety of tasks that require both knowledge base information and textual co occurrence counts. Despite some of the shortcomings that the reviewers point out, I believe this could be one missing puzzle piece of connecting symbolic information/sets/logic/KBs with neural nets and hence I recommend acceptance of this paper.
The paper proposes an adversarial framework that learns a generative model along with a mask generator to model missing data and by this enables a GAN to learn from incomplete data. The method builds on AmbientGAN but it is a novel and clever adjustment to the specific problem setting of learning from incomplete data, that is of high practical interest.
This paper addresses the problem of learning with outliers, which many reviewers agree is an important direction. However, reviewers point to issues with the experiments (missing baselines, ablations, etc.) and are concerned that the assumptions in the theoretical analysis are too strong. These were potentially addressed in a revised version of the paper, but the revisions are so major that I do not think it is appropriate to consider them in the review process (and it is hard to assess to what extent they address the issues without asking reviewers to do a thorough re appraisal, which goes beyond the scope of their duties). I encourage the authors to take reviewer comments into account and prepare a more polished version of the manuscript for future submission.
The paper proposes to build word representation based on a histogram over context word vectors, allowing them to measure distances between words in terms of optimal transport between these histograms. An empirical analysis shows that the proposed approach is competitive with others on semantic textual similarity and hypernym detection tasks. While the idea is definitely interesting, the paper would be streghten by a more extensive empirical analysis. 
This paper is concerned with combining past approximation methods to obtain a variant of Deep Recurrent GPs. While this variant is new, 2/3 reviewers make very overlapping points about this extension being obtained from a straightforward combination of previous ideas. Furthermore, R3 is not convinced that the approach is well motivated, beyond “filling the gap” in the literature.   All reviewers also pointed out that the paper is very hard to read. The authors have improved the manuscript during the rebuttal, but the AC believes that the paper is still written in an unnecessarily complicated way.   Overall the AC believes that this paper needs some more work, specifically in (a) improving its presentation (b) providing more technical insights about the methods (as suggested by R2 and R3), which could be a means of boosting the novelty. 
This paper proposes a novel method of solving inverse problems that avoids direct inversion by  first reconstructing various piecewise constant projections of the unknown image (using a different CNN to learn each)  and then combining them via optimization to solve the final inversion.  Two of the reviewers requested more intuitions into why this two stage process would  fight the inherent ambiguity.  At the end of the discussion, two of the three reviewers are convinced by the derivations and empirical justification of the paper. The authors also have significantly improved the clarity of the manuscript throughout the discussion period. It would be interesting to see if there are any connections between such inversion via optimization with deep component analysis methods, e.g. “Deep Component Analysis via Alternating Direction Neural Networks ” of Murdock et al. , that train neural architectures to effectively carry out the second step of optimization, as opposed to learning  a feedforward mapping.  
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready.
The paper proposes a quantity to monitor learning on an information plane which is related to the information curves considered in the bottleneck analysis but is more reliable and easier to compute.   The main concern with the paper is the lack of interpretation and elaboration of potential uses. A concern is raised that the proposed method abstracts away way too much detail, so that the shapes of the curves are to be expected and contain little useful information (see AnonReviewer2 comments). The authors agree to some of the main issues, as they pointed out in the discussion, although they maintain that the method could still contain useful information.   The reviewers are not very convinced by this paper, with ratings either marginally above the acceptance threshold, marginally below the acceptance threshold, or strong reject.   
This paper considers an interesting hypothesis that ReLU networks are biased towards learning learn low frequency Fourier components, showing a spectral bias towards low frequency functions. The paper backs the hypothesis with theoretical results computing and bounding the Fourier coefficients of ReLU networks and experiments on synthetic datasets.  All reviewers find the topic to be interesting and important. However they find the results in the paper to be preliminary and not yet ready for publication.   On theoretical front, the paper characterizes the Fourier coefficients for a given piecewise linear region of a ReLU network. However the bounds on Fourier coefficients of the entire network in Theorem 1 seem weak as they depend on number of pieces (N_f) and max Lipschitz constant over all pieces (L_f), quantities that can easily be exponentially big.  Authors in their response have said that their bound on Fourier coefficients is tight. If so then the paper needs to discuss/prove why quantities N_f and L_f are expected to be small. Such a discussion will help reviewers in appreciating the theoretical contributions more.  On experimental front, the paper does not show spectral bias of networks trained over any real datasets. Reviewers are sympathetic to the challenge of evaluating Fourier coefficients of the network trained on real data sets, but the paper does not outline any potential approach to attack this problem.   I strongly suggest authors to address these reviewer concerns before next submission.  
The paper proposes a simple method for detecting out of distribution samples. The authors  major finding is that mean and standard deviation within feature maps can be used as an input for classifying out of distribution (OOD) samples. The proposed method is simple and practical.  The reviewers and AC note the following potential weaknesses: (1) limited novelty and somewhat ad hoc approach, i.e., it is not too surprising to expect that such statistics can be useful for the purpose. Some theoretical justification might help. (2) arguable experimental settings, i.e., the performance highly varies depending on validation (even in the revised draft), and sometimes irrationally good. It also depends on the choice of classifier.  For (2), I think the whole evaluation should be done assuming that we don t know how it looks the OOD set. Under the setting, the authors should compare the proposed method and existing ones for fair comparisons. AC understands the authors follows the same experimental settings of some previous work addressing this problem, but it s time that this is changed. Indeed, a recent paper by Lee at al. 2018 considers such a setting for detecting more general types of abnormal samples including OOD.  In overall, the proposed idea is simple and easy to use. However, AC decided that the authors need more significant works to publish the work. 
A novel  approach for quantized deep neural nets is proposed,  which is more principled than commonly used  straight through gradient method. A theoretical analysis of the algorithm s converegence  is presented, and empirical results show advantages of the proposed approach. 
The paper proposes a meta learning approach to "language guided policy learning" where instructions are provided in the form of natural language instructions, rather than in the form of a reward function or through demonstration. A particularly interesting novel feature of the proposed approach is that it can seamlessly incorporate natural language corrections after an initial attempt to solve the task, opening up the direction towards natural instructions through interactive dialogue. The method is empirically shown to be able to learn to navigate environments and manipulate objects more sample efficiently (on test tasks) than approaches without instructions.   The reviewers noted several potential weaknesses: while the problem setting was considered interesting, the empirical validation was seen to be limited. Reviewers noted that only one (simple) domain was studied, and it was unclear if results would hold up in more complex domains. They also note lack of comparison to baselines based on prior work (e.g., pre training).  The authors provided very detailed replies to the reviewer comments, and added very substantial new experiments, including an entire new domain and newly implemented baselines. Reviewers indicated that they are satisfied with the revisions. The AC reviewed the reviewer suggestions and revisions and notes that the additional experiments significantly improve the contribution of the paper. The resulting consensus is that the paper should be accepted.  The AC would like to note that several figures are very small and unreadable when the paper is printed, e.g., figure 7, and suggests that the authors increase figure size (and font size within figures) to ensure legibility.
The paper proposed a deep, Bayesian optimization approach to RL with model uncertainty (BAMDP).  The algorithm is a variant of policy gradient, which in each iteration uses a Bayes filter on sampled MDPs to update the posterior belief distribution of the parameters.  An extension is also made to POMDPs.  The work is a combination of existing techniques, and the algorithmic novelty is a bit low.  Initial reviews suggested the empirical study could be improved with better baselines, and the main idea of the proposed method could be expended.  The revised version moves towards this direction, and the author responses were helpful.  Overall, the paper is a useful contribution.
The paper proposes a particle based framework for learning object dynamics. A scene is represented by a hierarchical graph over particles, edges between particles are established dynamically based on Euclidean distance. The model is used for model predictive control, and there is also one experiment with a particle graph built from a real scene as opposed to simulation.  All reviewers agree that the architectural changes over previous relational networks  are worthwhile and merit publication. They also suggest to tone down the ``dynamic” part of the graph construction by stating that edges are determined based on a radius. In particular, previous works also consider similar addition of edges during collisions, quoting Mrowca et al. "Collisions between objects are handled by dynamically defining pairwise collision relations ... between leaf particles..." which suggests that comparison against a baseline for Mrowca et al. that uses a static graph is not entirely fair. The authors are encouraged to repeat the experiment without disabling such dynamic addition of edges.   
AR1 seeks the paper to be more standalone and easier to read. As this comment comes from the reviewer who is very experienced in tensor models, it is highly recommended that the authors make further efforts to make the paper easier to follow. AR2 is concerned about  the manually crafted role schemes and alignment discrepancy of results between these schemes and RNNs. To this end, the authors hypothesized further reasons as to why this discrepancy occurs. AC encourages authors to make further efforts to clarify this point without overstating the ability of tensors to model RNNs (it would be interesting to see where these schemes and RNN differ). Lastly, AR3 seeks more clarifications on contributions.  While the paper is not ground breaking, it offers some starting point on relating tensors and RNNs. Thus, AC recommends an accept. Kindly note that tensor outer products have been used heavily in computer vision, i.e.:   Higher Order Occurrence Pooling for Bags of Words: Visual Concept Detection by Koniusz et al. (e.g. section 3 considers bi modal outer tensor product for combining multiple sources: one source can be considered a filter, another as role (similar to Smolensky at al. 1990), e.g. a spatial grid number refining local role of a visual word. This further is extended to multi modal cases (multiple filter or role modes etc.) )   Multilinear image analysis for facial recognition (e.g. so called tensor faces) by Vasilescu et al.   Multilinear independent components analysis by Vasilescu et al.   Tensor decompositions for learning latent variable models by Anandkumar et al.  Kindly  make connections to these works in your final draft (and to more prior works).  
The paper generalizes the concept of "hindsight", i.e. the recycling of data from trajectories in a goal based system based on the goal state actually achieved, to policy gradient methods.  This was an interesting paper in that it scored quite highly despite all three reviewers mentioning incrementality or a relative lack of novelty. Although the authors naturally took some exception to this, AC personally believes that properly executed, contributions that seem quite straightforward in hindsight (pun partly intended) can be valuable in moving the field forward: a clean and didactic presentation of theory backed by well designed and extensive empirical investigation (both of which are adjectives used by reviewers to describe the empirical work in this paper) can be as valuable, or moreso, than a poorly executed but higher novelty works. To quote AnonReviewer3, "HPG is almost certainly going to end up being a widely used addition to the RL toolbox".  Feedback from reviewers prompted extensive discussion and a direct comparison with Hindsight Experience Replay which reviewers agreed added significant value to the manuscript, earning it a post rebuttal unanimous rating of 7. It is therefore my pleasure to recommend acceptance.
The paper presents an interesting idea, but there are significant concerns about the presentation issues and experimental results (e.g., comparisons with baselines). Overall, it is not ready for publication. 
The area chair agrees with the authors and the reviewers that the topic of this work is relevant and important. The area chair however shares the concerns of the reviewers about the setup and the empirical evaluation:   Having one model that can be pruned to varying sizes at run time is convenient, but in practice it is likely to be OK to do the pruning at training time. In light of this, the empirical results are not so impressive.   Without quantization, distillation and fused ops, the value of the empirical results seems questionable as these are important and well known techniques that are often used in practice. A more thorough evaluation that includes these techniques would make the paper much stronger.
The paper studies whether the best strategy for transfer learning in RL is to transfer value estimates or policy probabilities. The paper also presents a model based value centric (MVC) framework for continuous RL. The reviewers raised concerns regarding (1) the coherence of the story, (2) the novelty and importance of the MVC framework and (3) the significance of the experiments. I encourage the authors to either focus on the algorithmic aspect or the transfer learning aspect and expand on the experimental results to make  them more convincing. I appreciate the changes made to improve the paper, but in its current form the paper is still below the acceptance threshold at ICLR.  PS: in my view one can think of value as (shifted and scaled) log of policy. Hence, it is a bit ambiguous to ask whether to transfer value or policy.
This paper presents a promising model to avoid catastrophic forgetting in continual learning. The model consists of a) a data generator to be used at training time to replay past examples (and removes the need for storage of data or labels), b) a dynamic parameter generator that given a test input produces the parameters of a classification model, and c) a solver (the actual classifier). The advantages of such combination is that no parameter increase or network expansion is needed to learn a new task, and no previous data needs to be stored for memory replay.  There is reviewer disagreement on this paper. AC can confirm that all three reviewers have read the author responses and have significantly contributed to the revision of the manuscript.  All three reviewers and AC note the following potential weaknesses: (1) presentation clarity needed substantial improvement. Notably, the authors revised the paper several times while incorporating the reviewers suggestions regarding presentation clarity. R2 has raised the final rating from 4 to 5 while retaining doubts about clarity.  (2) weak empirical evidence: evaluation with more than three tasks and using more recent/stronger baseline methods would substantially strengthen the evaluation (R2, R3). AC would like to report the authors added an experiment with five tasks and provided a verbal comparison with "Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence", ECCV 2018 by reporting the authors results on the MNIST dataset.  (3) as noted by R2, an ablation study of different model components could strengthen the evaluation. The authors included such ablation study in Table 4 of the revised paper.  (4) reproducibility of the model could be difficult (R1). In their response, the authors promised to make the code publicly available.  AC can confirm that all three reviewers have contributed to the final discussion. Given the effort of the reviewers and authors in revising this work and its potential novelty, the AC decided that the paper could be accepted, but the authors are strongly urged to further improve presentation clarity in the final revision if possible. 
 This paper proposes an adversarial learning framework for dialogue generation. The generator is based on previously proposed hierarchical recurrent encoder decoder network (HRED) by Serban et al., and the discriminator is a bidirectional RNN. Noise is introduced in generator for response generation. The approach is evaluated on two commonly used corpora, movie data and ubuntu corpus.  In the original version of the paper, human evaluation was missing, an issue raised by all reviewers, however, this has been added in the revisions. These supplement the previous automated measures in demonstrating the benefits and significant gains from the proposed approach.  All reviewers raise the issue of the work being incremental and not novel enough given the previous work in HRED/VHRED and use of hierarchical approaches to model dialogue context. Furthermore, noise generation seems new, but is not well motivated, justified and analyzed.  
 pros:   the paper is well written and presents a nice framing of the composition problem   good comparison to prior work   very important research direction  cons:   from an architectural standpoint the paper is somewhat incremental over Routing Networks [Rosenbaum et al]   as Reviewers 2 and 3 point out, the experiments are a bit weak, relying on heuristics such as a window over 3 symbols in the multi lingual arithmetic case, and a pre determined set of operations (scaling, translation, rotation, identity) in the MNIST case.  As the authors state, there are three core ideas in this paper (my paraphrase):  (1) training on a set of compositional problems (with the right architecture/training procedure) can encourage the model to learn modules which can be composed to solve new problems, enabling better generalization.  (2) treating the problem of selecting functions for composition as a sequential decision making problem in an MDP (3) jointly learning the parameters of the functions and the (meta level) composition policy.  As discussed during the review period, these three ideas are already present in the Routing Networks (RN) architecture of Rosenbaum et al.  However CRL offers insights and improvements over RN algorithmically in a several ways:  (1) CRL uses a curriculum learning strategy.  This seems to be key in achieving good results and makes a lot of sense for naturally compositional problems. (2) The focus in RN was on using the architecture to solve multi task problems in object recognition. The solutions learned in image domains while "compositional" are less clearly interpretable.  In this paper (CRL) the focus is more squarely on interpretable compositional tasks like arithmetic and explores extrapolation. (3) The RN architecture does support recursion (and there are some experiments in this mode) but it was not the main focus.  In this paper (CRL) recursion is given a clear, prominent role.  I appreciate that the authors  engagement in the discussion period. My feeling is that  the paper offers nice improvements, a useful framing of the problem, a clear recursive formulation, and a more central focus on naturally compositional problems.  I am recommending the paper for acceptance but suggest that the authors remove or revise their contributions (3) and (4) on pg. 2 in light of the discussion on routing nets.  Routing Networks, Adaptive Selection of Non Linear Functions for Multi task Learning, ICLR 2018
The area chair agrees with reviewer 1 and 2 that this paper does not have sufficient machine learning novelty for ICLR. This is competent work and the problem is interesting, but ICLR is not the right venue since the main contributions are on defining the task. All the models that are then applied are standard.
This paper introduces a new graph neural network architecture designed to learn to solve Circuit SAT problems, a fundamental problem in computer science. The key innovation is the ability to to use the DAG structure as an input, as opposed to typical undirected (factor graph style) representations of SAT problems. The reviewers appreciated the novelty of the approach as well as the empirical results provided that demonstrate the effectiveness of the approach.  Writing is clear. While the comparison with NeuroSAT is interesting and useful, there is no comparison with existing SAT solvers which are not based on learning methods. So it is not clear how big the gap with state of the art is. Overall, I recommend acceptance, as the results are promising and this could inspire other researchers working on neural symbolic approaches to search and optimization problems.
Strengths   The paper presents a method of training two level hierarchies that is based on relatively intuitive ideas and that performs well. The challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the separation of internal state from external state is a clean principle that can potentially be broadly employed.  The method does well in outperforming the alternative baselines.  Weaknesses  There is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses  a policy ensemble;  phase info is used in DeepLoco/DeepMimic; methods such as "Virtual Windup Toys for Animation"  exploited periodicity (25y ago);  More comparisons with prior work such as Florensa et al. would help.  The separation of internal and external state is an assumption that may not hold in many cases. The results are locomotion focussed. There are only two timescales.  Decision  The reviewers are largely in agreement to accept the paper.  There are fairly simple but useful lessons to be found in the paper for those working on HRL problems, particularly those for movement and locomotion.  The AC sees the novely with respect to different pieces of related work is the weakest point of the paper.   The reviews contain good suggestions for revisions and improvements;  the latest version may take care of these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution to ICLR 2019. 
The paper evaluates several off the shelf algorithms for predicting the return on real estate property investment. The problem is interesting, but there is a consensus that the paper contains little technical novelty, and the empirical study on a fairly small dataset is also not convincing. 
All reviewers recommend accept.  Discussion can be consulted below.
Four reviewers have evaluated this paper. The reviewers have raised concerns about the specific formulation used for adversarial example generation which requires further clarity in motivation and interpretation. The reviewers have also made the point that the experimental evaluation is against previous work which tried to solve a different problem (black box based attack) and hence the conclusions are unconvincing.
In this work, the authors explore using genetic programming to search over network architectures. The reviewers noted that the proposed approach is simple and fast. However, the reviewers expressed concerns about the experimental validation (e.g., experiments were conducted on small tasks; issues with comparisons (cf. feedback from Reviewer2)), and the fact that the method were not compared against various baseline methods related to architecture search. 
All reviewers recommend acceptance, with two reviewers in agreement that the results represent a significant advance for autoregressive generative models. The AC concurs. 
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.     The approach is novel   The experimental results are convincing.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The authors didn t show results with non Gaussian noise   Some details that could help the understanding of the method are missing.   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
The reviewers agree the paper is not ready for publication. 
The paper presents an original approach to replace inefficient discrete autoregressive posterior sampling by a parallel sampling procedure based on fixed point iterations reminiscent of normalizing flow, but for discrete variables. All reviewers liked the idea, and found that it was an original and promising approach. But all agreed the paper was poorly written and very unclear.  All also found the experimental section lacking, in clarity and scope.  Authors did not provide a rebuttal.  Overall a potentially really promising idea, but the paper is not yet ripe. 
The topic of universal adversarial perturbation is quite intriguing and fairly poorly studied and the paper provides a mix of new insights, both theoretical and empirical in nature. However, the significant presentation issues make it hard to properly understand and evaluate them. In particular, the theoretical part feels rushed and not sufficiently rigorous, and it is unclear why focusing on the case of equivariant network is crucial. Also, it would be useful if the authors put more effort in explaining how their contributions fit into the context of prior work in the area.  Overall, this paper has a potential of becoming a solid contribution, once the above shortcomings are addressed.
This paper interprets batch norm in terms of normalizing the backpropagated gradients. All of the reviewers believe this interpretation is novel and potentially interesting, but that the paper doesn t make the case that this helps explain batch norm, or provide useful insights into how to improve it. The authors have responded to the original set of reviews by toning down some of the claims in the original paper, but haven t addressed the reviewers  more substantive concerns. There may potentially be interesting ideas here, but I don t think it s ready for publication at ICLR.  
The paper studies the problem of reinforcement learning under certain constraints on action sequences. The reviewers raised important concerns regarding (1) the general motivation, (2) the particular formulation of constraints in terms of action sequences and (3) the relevance and significance of experimental results. The authors did not submit a rebuttal. Given the concerns raised by the reviewers, I encourage the authors to improve the paper to possibly resubmit to another venue.
The work presents a method of imposing harmonic structural regularizations to layers of a neural network. While the idea is interesting, the reviewers point out multiple issues.  Pros: + Interesting method + Hidden layer coherence tends to improve  Cons:   Deficient comparisons to baselines or context with other works.   Insufficient assessment of impact to model performance.   Lack of strategy to select regularizers   Lack of evaluation on more realistic datasets
All reviewers agree in their assessment that this paper is not ready for acceptance into ICLR.
This paper argues that each layer of a network may have some channels useful for and some not useful for transfer learning. The main contribution is an approach which identifies the useful channels through an attention based mechanism. The reviewers agree that this work offers a valuable new approach that offers modest improvements over prior work.   The authors should take care to refine their definition of behavior regularization, including/expanding on the discussion from the rebuttal phase. The authors are also encouraged to experiment with other architecture backbones and report both overall performance as well as run time for learning with the larger models.  
The use of SARAH for Policy optimization in RL is novel, with some theoretical analysis to demonstrate convergence of this approach. However, concerns were raised in terms of clarity of the paper, empirical results and in placement of this theory relative to a previous variance reduction algorithm called SVRPG. The author response similarly did not explain the novelty of the theory beyond the convergence results of what was given by the paper on SVRPG.  By incorporating some of the reviewer comments, this paper could be a meaningful and useful contribution.
All three reviewers expressed concerns about the assumptions made for the local stability analysis. The AC thus recommends "revise and resubmit".
All the reviewers agree that the paper has an interesting idea on regularizing the spectral norm of the weight matrices in GANs, and a generalization bound has been shown. The empirical result shows that indeed regularization improves the performance of the GANs. Based on these the AC suggested acceptance. 
This paper proposed an unsupervised learning algorithm for predictive modeling. The key idea of using NCE/CPC for predictive modeling is interesting. However, major concerns were raised by reviewers on the experimental design/empirical comparisons and paper writing.  Overall, this paper cannot be published in its current form, but I think it may be dramatically improved for a future publication. 
The paper presents a GAN for learning a target distribution that is defined as the difference between two other distributions.  The reviewers and AC note the critical limitation of novelty and appealing results of this paper to meet the high standard of ICLR.   AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
This paper provides the first convergence analysis for convex model distributed training with quantized weights and gradients. It is well written and organized. Extensive experiments are carried out beyond the assumption of convex models in the theoretical study.  Analysis with weight and gradient quantization has been separately studied, and this paper provides a combined analysis, which renders the contribution incremental.   As pointed out by R2 and R3, it is somewhat unclear under which problem setting, the proposed quantized training would help improve the convergence. The authors provide clarification in the feedback. It is important to include those, together with other explanations in the feedback, in the future revision.  Another limitation pointed out by R3 is that the theoretical analysis applies to convex models only. Nevertheless, it is nice to show in experiments that deep networks training is benefitted from the gradient quantization empirically.
This paper addresses a clear open problem in representation learning for language: the learning of language agnostic representations for zero shot cross lingual transfer. All three reviewers agree that it makes some progress on that problem, and my understanding is that a straightforward presentation of these would likely have been accepted to this conference. However, there were serious issues with the framing and presentation of the paper.  One reviewer expressed serious concerns about clarity and detail, and two others expressed serious concerns about the paper s framing. I m more worried about the framing issue: The paper opens with a sweeping discussion about the nature of language and universal grammar and, in the original version, also claims (in vague terms) to have made substantial progress on understanding the nature of language. The most problematic claims have since been removed, but the sweeping introduction remains, and it serves as the only introduction to the paper, leaving little discussion of the substantial points that the paper is trying to make.  I reluctantly have to recommend rejection. These problems should be fixable with a substantial re write of the paper, but the reviewers were not satisfied with the progress made in that direction so far.
This work is effectively an extension of progressive nets, where the task ID is not given at test time. There were several concerns about novelty of this work and the evaluation being insufficient. There was a reasonable back and forth between the reviewers and authors, and the reviewers are all aligned with the idea that this work would need a substantial rewrite in order to be accepted at ICLR.
The paper provides an interesting combination of existing techniques (such as GCN and and the Bernoulli Poisson link) to address the problem of overlapping community detection. However, there were concerns about lack of novelty, evaluation metrics, and missing comparisons with previous work. The authors did not provide a response to address these concerns.
This paper has two main contributions. The first is that it proposes a specific framework for measuring catastrophic forgetting in deep neural networks that incorporates three application oriented constraints: (1) a low memory footprint, which implies that data from prior tasks cannot be retained; (2) causality, meaning that data from future tasks cannot be used in any way, including hyperparameter optimization and model selection; and (3) update complexity for new tasks that is moderate and also independent of the number of previously learned tasks, which precludes replay strategies. The second contribution is an extensive study of catastrophic forgetting, using different sequential learning tasks derived from 9 different datasets and examining 7 different models. The key conclusions from the study are that (1) permutation based tasks are comparatively easy and should not be relied on to measure catastrophic forgetting; (2) with the application oriented contraints in effect, all of the examined models suffer from catastrophic forgetting (a result that is contrary to a number of other recent papers); (3) elastic weight consolidation provides some protection against catastrophic forgetting for simple sequential learning tasks, but fails for more complex tasks; and (4) IMM is effective, but only if causality is violated in the selection of the IMM balancing parameter. The reviewer scores place this paper close to the decision boundary. The most negative reviewer (R2) had concerns about the novelty of the framework and its application oriented constraints. The authors contend that recent papers on catastrophic forgetting fail to apply these quite natural constraints, leading to the deceptive conclusion that catastrophic forgetting may not be as big of a problem as it once was. The AC read a number of the papers mentioned by the authors and agrees with them: these constraints have been, at least at times, ignored in the literature, and they shouldn t be ignored. The other two reviewers appreciated the scope and rigor of the empirical study. On the balance, the AC thinks this is an important contribution and that it should appear at ICLR.
All three reviewers found that the motivation for the proposed method was lacking and recommend rejection. The AC thus recommends the authors to take these comments in consideration when revising their manuscript.
The paper introduces a benchmark suite providing a series of synthetic distributions and metrics for the evaluation of generative models. While providing such a tool kit is interesting and helpful and it extends existing approaches for evaluating generative models on simple distributions, it seems not to allow for very different additional conclusions or insights.This limits the paper s significance. Adding more problems and metrics to the benchmark suite would make it more convincing.
The paper addresses the problem of interpreting recurrent neural networks by quantizing their states an mapping them onto a Moore Machine. The paper presents some interesting results on reinforcement learning and other tasks. I believe the experiments could have been more informative if the proposed technique was compared against a simple quantization baseline (e.g. based on k means) so that one can get a better understanding of the difficulty of these task.  This paper is clearly above the acceptance threshold at ICLR. 
The paper proposes an evolutionary architecture search method which uses weight inheritance through network morphism to avoid training candidate models from scratch.  The method can optimise multiple objectives (e.g. accuracy and inference time), which is relevant for practical applications, and the results are promising and competitive with the state of the art. All reviewers are generally positive about the paper. Reviewers’ feedback on improving presentation and adding experiments with a larger number of objectives has been addressed in the new revision.   I strongly encourage the authors to add experiments on the full ImageNet dataset (not just 64x64) and/or language modelling   the two benchmarks widely used in neural architecture search field.
The reviewers raise a number of concerns including limited methodological novelty, limited experimental evaluation (comparisons), and poor readability. Although the authors did address some of the concerns, the paper as is needs a lot of polishing and rewriting. Hence, I cannot recommend this work for presentation at ICLR.
The paper aims to study what is learned in the word representations by comparing SkipGram embeddings trained from a text corpus and CNNs trained from ImageNet.  Pros: The paper tries to be comprehensive, including analysis of text representations and image representations, and the cases of misclassification and adversarial examples.   Cons: The clarity of the paper is a major concern, as noted by all reviwers, and the authors did not come back with rebuttal to address reviewers  quetions. Also, as R1 and R2 pointed out the novelty over recent relevant papers such as (Dharmaretnam & Fyshe, 2018) is not clear.  Verdict: Reject due to weak novelty and major clarity issues.
The paper describes knowledge distillation methods. As noted by all reviewers, the methods are very similar to the prior art, so there is not enough novelty for the paper to be accepted. The reviewers  opinion didn t change after the rebuttal.
The paper needs more revisions and better presentation of empirical study.
This paper extends the transformer model of Vashwani et al. by replacing the sine/cosine positional encodings with information reflecting the tree stucture of appropriately parsed data. According to the reviews, the paper, while interesting, does not make the cut. My concern here is that the quality of the reviews, in particular those of reviewers 2 and 3, is very sub par. They lack detail (or, in the case of R2, did so until 05 Dec(!!)), and the reviewers did not engage much (or at all) in the subsequent discussion period despite repeated reminders. Infuriatingly, this puts a lot of work squarely in the lap of the AC: if the review process fails the authors, I cannot make a decision on the basis of shoddy reviews and inexistent discussion! Clearly, as this is not the fault of the authors, the best I can offer is to properly read through the paper and reviews, and attempt to make a fair assessment.  Having done so, I conclude that while interesting, I agree with the sentiment expressed in the reviews that the paper is very incremental. In particular, the points of comparison are quite limited and it would have been good to see a more thorough comparison across a wider range of tasks with some more contemporary baselines. Papers like Melis et al. 2017 have shown us that an endemic issue throughout language modelling (and certainly also other evaluation areas) is that complex model improvements are offered without comparison against properly tuned baselines and benchmarks, failing to offer assurances that the baselines would not match performance of the proposed model with proper regularisation. As some of the reviewers, the scope of comparison to prior art in this paper is extremely limited, as is the bibliography, which opens up this concern I ve just outlined that it s difficult to take the results with the confidence they require. In short, my assessment, on the basis of reading the paper and reviews, is that the main failing of this paper is the lack of breadth and depth of evaluation, not that it is incremental (as many good ideas are). I m afraid this paper is not ready for publication at this time, and am sorry the authors will have had a sub par review process, but I believe it s in the best interest of this work to encourage the authors to further evaluate their approach before publishing it in conference proceedings.
Reviewers are in a consensus and recommended to reject. However, the reviewers did not engage at all with the authors, and did not acknowledge whether their concerns have been answered. I therefore lean to reject, and would recommend the authors to resubmit. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit.  
This paper provides a simple and intuitive method for learning multilingual word embeddings that makes it possible to softly encourage the model to align the spaces of non English language pairs. The results are better than learning just pairwise embeddings with English.  The main remaining concern (in my mind) after the author response is that the method is less accurate empirically than Chen and Cardie (2018). I think however that given that these two works are largely contemporaneous, the methods are appreciably different, and the proposed method also has advantages with respect to speed, that the paper here is still a reasonably candidate for acceptance at ICLR.  However, I would like to request that in the final version the authors feature Chen and Cardie (2018) more prominently in the introduction and discuss the theoretical and empirical differences between the two methods. This will make sure that readers get the full picture of the two works and understand their relative differences and advantages/disadvantages.
The conditional network embedding approach proposed in the paper seems nice and novel, and consistently outperforms state of art on variety of datasets; scalability demonstration was added during rebuttals, as well as multiple other improvements; although  the reviewers did not respond by changing the scores, this paper with augmentations provided during the rebuttal appears to be a useful contribution  worthy of publishing at ICLR. 
The paper introduces an interesting idea of using different rates of learning for low level vs high level computation for meta learning. However, the experiments lack the thoroughness needed to justify the basic intuition of the approach and design choices like which layers to learn fast or slow need to be further ablated.
This paper proposes a new unsupervised learning approach based on maximizing the mutual information between the input and the representation. The results are strong across several image datasets. Essentially all of the reviewer s concerns were directly addressed in revisions of the paper, including additional experiments. The only weakness is that only image datasets were experimented with; however, the image based experiments and comparisons are extensive. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.
This paper proposes a meta learning algorithm for reinforcement learning that incorporates expert demonstrations. The objective is to improve sample efficiency, which is an important problem.   The referees find the approach well motivated and pertinent, but the theoretical and practical contributions of the paper too slim. A concern was also raised in regard to reproducibility of the results, missing details about the implementation and comparisons with previous results.   The authors did not respond to the reviews.   The four referees are not convinced by this paper, with ratings from strong reject to ok, but not good enough. 
This paper proposes a GAN based method to recover images from a noisy version of it. The paper builds upon existing works on AmbientGAN and CS GAN. By combining the two approaches, the work finds a new method that performs better than existing approaches.  The paper clearly has new interesting ideas which have been executed well. Two of the reviewers have voted in favour of acceptance, with one of the reviewer providing an extensive and detailed review. The third reviewer however has some doubts which were not resolved completely after the rebuttal.  Upon reading the work myself, I am convinced that this will be interesting to the community. However, I will recommend the authors to take the comments of Reviewer 2 into account and do whatever it takes to resolve issues pointed by the reviewer.  During the review process, another related work was found to be very similar to the approach discussed in this work. This work should be cited in the paper, as a prior work that the authors were unaware of.  https://arxiv.org/abs/1812.04744 Please also discuss any new insights this work offers on top of this existing work.  Given that the above suggestions are taken into account, I recommend to accept this paper. 
The presented method proposes to use saliency maps as a component for an additional metric of forgetting in continual learning, and as a tool as additional information to improve learning on new tasks.   Pros:  + R2 & R3: Clearly written and easy to follow.  + R3: New metric to compare saliency masks + R3: Interesting idea to utilize previously learned saliency masks to augment learning new tasks.  + R1: Performance improvements observed.  Cons:   R1 & R2: Novelty is limited in the context of prior works in this field. Unanswered by authors.   R2: Concerns around method s ability to use salient but disconnected components. Unanswered by authors.   R2: Experiments needed on more realistic datasets, such as ImageNet. Unanswered by authors.    R3: Performance gains are small.    R1 & R2: Literature review is insufficient.   Reviewers are leaning reject, and R2 s concerns have not been answered by the authors at all. Idea seems interesting, authors are encouraged to take into careful consideration the feedback from authors and continue their research.
Strengths:  The paper introduces a novel constrained optimization method for RL problems. A lower bound constraint can be imposed on the return (cumulative reward),  while optimizing one or more other costs, such as control effort.  The method learns multiple  The paper is clearly written.  Results are shown on the cart and pole, a humanoid, and a realistic Minitaur  quadruped model.  AC: Being able to learn conditional constraints is an interesting direction.  Weaknesses:  There are often simpler ways to solve the problem of high amplitude, high frequency  controls in the setting of robotics.   The paper removes one hyperparameter (lambda) but then introduces another (beta), although beta is likely easier to tune. The ideas have some strong connections to existing work in  safe reinforcement learning. AC: Video results for the humanoid and cart and pole examples would have been useful to see.  Summary:   The paper makes progress on ideas that are fairly involved to explore and use  (perhaps limiting their use in the short term), but that have potential,  i.e., learning state dependent Lagrange multipliers for constrained RL. The paper is perfectly fine technically, and does break some new ground in putting a particular set of pieces together.  As articulated by two of the reviewers, from a pragmatic perspective, the results are not  yet entirely compelling. I do believe that a better understanding of working with constrained RL, in ways that are somewhat different than those used in Safe RL work.    Given the remaining muted enthusiasm of two of the reviewers, and in the absence of further calibration, the AC leans marginally towards a reject. Current scores: 5,6,7. Again, the paper does have novelty, although it s a pretty intricate setup. The AC would be happy to revisit upon global recalibration. 
The paper presents a conformal prediction approach to supervised classification, with the goal of reducing the overconfidence of standard soft max learning techniques. The proposal is based on previously published methods, which are extended for use with deep learning predictors. Empirical evaluation suggests the proposal results in competitive performance. This work seems to be timely, and the topic is of interest to the community.  The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the strength of the empirical evidence supporting the claims. Additional experiments would significantly strengthen this submission.
The paper presents a graph neural network that represents the movements of electrons during chemical reactions, trained from a dataset to predict reactions outcomes.  The paper is clearly written, the comparisons are sensical. There are some concerns by reviewer 3 about the experimental results: in particular the lack of a simpler baseline, and the experimental variance. I think the some of the important concerns from reviewer 3 were addressed in the rebuttal, and I hope the authors will update the manuscript accordingly.  Overall, this is fitting for publication at ICLR 2019.
The paper presents a topological complexity measure of neural networks based on persistence 0 homology of the weights in each layer. Some lower and upper bounds of the p norm persistence diagram are derived that leads to normalized persistence metric. The main discovery of such a topological complexity measure is that it leads to a stability based early stopping criterion without a statistical cross validation, as well as distinct characterizations on random initialization, batch normalization and drop out. Experiments are conducted with simple networks and MNIST, Fashion MNIST, CIFAR10, IMDB datasets.   The main concerns from the reviewers are that experimental studies are still preliminary and the understanding on the observed interesting phenomenon is premature. The authors make comprehensive responses to the raised questions with new experiments and some reviewers raise the rating.   The reviewers all agree that the paper presents a novel study on neural network from an algebraic topology perspective with interesting results that has not been seen before. The paper is thus suggested to be borderline lean accept.  
This paper studies the compression aspect of the information bottleneck. It seeks to clarify a debate about the evolution of mutual information between inputs and representations during training in neural networks. The paper discusses numerous ideas and techniques and arrives at valuable conclusions.   A concern is that parts of the paper (theoretical parts) are intended for a separate paper, and are included in the paper only for reference. This means that the actual contribution of the present paper is mostly on the experimental part. Nonetheless, the discussion derived from the theory and experiments seem valuable in the ongoing discussion of this topic. In any case, I encourage the authors to make efforts to obtain a transparent separation of the different pieces of work.   A concern was raised that the current paper mainly addresses a discussion that originated in a paper that has not passed peer review. On the other hand, this discussion does occupy many researchers and justifies the analysis, even if the originating paper has not been published in a peer reviewed format.    All reviewers are confident in their assessment. Two of them regard the paper positively and one of them regards the paper as ok, but not good enough, with main criticism in relation to the points discussed above.   Although the paper is in any case very good, unfortunately it does not reach the very high bar for acceptance at this ICLR. 
The paper suggests using meta learning to tune the optimization schedule of alternative optimization problems. All of the reviewers agree that the paper is worthy of publication at ICLR. The authors have engaged with the reviewers and improved the paper since the submission. I asked the authors to address the rest of the comments in the camera ready version.
The paper proposes a method for saving computation in surveillance videos (videos without camera motion) by re using features from parts of the image that do not change. The results show that this significantly saves computation time, which is a big benefit, given also the amount of surveillance video input available for processing nowadays. Reviewers request comparisons to obvious baselines, e.g., selecting a subset of frames for processing or performing a low level pixel matching to select the pixels to compute new features on. Such experiments would make this paper much stronger. There is no rebuttal  and thus no ground for discussion or acceptance. 
This paper presents a new defense against adversarial examples using random permutations and a Fourier transform. The technique is clearly novel, and the paper is clearly written.   However, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable. This degradation is due to the random permutation of the images, which effectively disallows the use of convolutions.   Furthermore, Reviewer 1 points out that the baselines are insufficient, as the authors do not explore (a) learning the transformation, or (b) using expectation over transformation to attack the model.   This concern is further validated by the fact that Black box attacks are often the best performing, which is a sign of gradient masking. The authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack. However, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step.   The paper thus requires significantly stronger baselines and attacks.
The reviewers acknowledge the value of the careful analysis of Gaussian encoder/decoder VAE presented in the paper. The proposed algorithm shows impressive FID scores that are comparable to those obtained by state of the art GANs. The paper will be a valuable addition to the ICLR program.   
The reviewers agree that providing more insights on why batch normalization work is an important topic of investigation, but they all raised several problems with the current submission which need to be addressed before publication. The AC thus proposes "revise and sesubmit".
This manuscript presents a reinterpretation of hindsight experience replay which aims to avoid recomputing the reward function, and investigates Floyd Warshall RL in the function approximation setting.  The paper was judged as relatively clear. The authors report a slight improvement in computational cost, which some reviewers called into question. However, all of the reviewers pointed out that the experimental evidence for the method s superiority is weak. Two reviewers additionally raised that this wasn t significantly different than the standard formulation of Hindsight Experience Replay, which doesn t require the computation of rewards for relabeled goals.  Ultimately, reviewers were in agreement that the novelty of the method and quality of the obtained results rendered the work insufficient for publication. The Area Chair concurs, and urges the authors to consider the reviewers  pointers to the existing literature in order to clarify their contribution for subsequent submission.
This paper proposes a GAN based framework for image compression.  The reviewers and AC note a critical limitation on novelty of the paper i.e., such a conditional GAN framework is now standard. The authors mentioned that they apply GAN for extreme compression for the first time in the literature, but this is not enough to justify the novelty issue.  AC thinks the proposed method has potential and is interesting, but decided that the authors need new ideas to publish the work. 
The paper proposes an interesting framework for visualizing and understanding GANs, that will be of clear help for understanding existing models and might provide insights for developing new ones. 
The paper describes an architecture search method which optimises multiple objectives using a genetic algorithm. All reviewers agree on rejection due to limited novelty compared to the prior art; while the results are solid, they are not ground breaking to justify acceptance based on results alone. 
This paper proposes an algorithm for end to end image compression outperforming previously proposed ANN based techniques and typical image compression standards like JPEG.  Strengths   All reviewers agreed that this a well written paper, with careful analysis and results.  Weaknesses   One of the points raised during the review process was that 2 very recent publications propose very similar algorithms. Since these works appeared very close to ICLR paper submission deadline (within 30 days), the program committee decided to treat this as concurrent work.  The authors also clarified the differences and similarities with prior work, and included additional experiments to clarify some of the concerns raised during the review process. Overall the paper is a solid contribution towards improving image compression, and is therefore recommended to be accepted. 
The paper introduces the use of J S shrinkage estimator in policy optimization, which is new and promising.  The results also show the potential.  That said, reviewers are not fully convinced that in its current stage the paper is ready for publication.  The approach taken here is essentially a combination existing techniques.  While it is useful, more work is probably needed to strengthen the contribution.  A few directions have been suggested by reviewers, including theoretical guarantees and stronger empirical support.
The paper presents an approach to address the open set recognition task based on inter and intra class distances. All reviewers are concerned with novelty and more experimental comparisons. Authors have added some results, but reviewers did not think these were enough to make the paper convincing enough. Overall I agree with reviewers and recommend to reject the paper.
This paper provides an interesting benchmark for multitask learning in NLP. I wish the dataset included language generation tasks instead of just classification but it s still a step in the right direction. 
This paper builds on a promising line of literature developing connections between Gaussian processes and deep neural networks.  Viewing one model under the lens of (the infinite limit of) another can lead to neat new insights and algorithms.  In this case the authors develop a connection between convolutional networks and Gaussian processes with a particular kind of kernel.  The reviews were quite mixed with one champion and two just below borderline.  The reviewers all believed the paper had contributions which would be interesting to the community (such as R1: "the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own" and R2: "I really like the idea of authors that kernels based on convolutional networks might be more practical compared to the ones based on fully connected networks").  All the reviewers found the contribution of the covariance function to be novel and exciting.  Some cited weaknesses of the paper were that the authors didn t analyze the uncertainty from the model (arguably the reasoning for adopting a Bayesian treatment), novelty in appealing to the central limit theorem to arrive at the connection, and scalability of the model.  In the review process it also became apparent that there was another paper with a substantially similar contribution.  The decision for this paper was calibrated accordingly with that work.  Weighing the strengths and weaknesses of the paper and taking into account a reviewer willing to champion the work it seems there is enough novel contribution and interest in the work to justify acceptance.  The authors provided responses to the reviewer concerns including calibration plots and timing experiments in the discussion period and it would be appreciated if these can be incorporated into the camera ready version.
The reviewers and this AC agree that the paper is not of acceptable form due to several issues: (1) limited novelty, (2) limited/unclear experimental validation, and (3) presentation issues.
The extension of convnets to non Euclidean data is a major theme of research in computer vision and signal processing.  This paper is concerned with Graph structured datasets. The main idea seems to be interesting: to improve graph neural nets by first embedding the graph in a Euclidean space reducing it to a point cloud, and then exploiting the induced topological structure implicit in the point cloud.   However, all reviewers found this paper hard to read and improperly motivated due to poor writing quality. The experimental results are somewhat promising but not completely convincing, and the proposed framework lacks a solid theoretical footing. Hence, the AC cannot recommend acceptance at ICLR 2019.  
The paper presents a CNN that is trained from human games to predict which actions to take for China Competitive Poker (Dou dizhu).  The paper is poorly written, not because of the English, but because it is hard to understand the details of the proposed solution: it is not straight forward to reimplement a solution from the presentation in the paper. It lacks explanations for several design decisions. This is unfortunate, as the authors point out in the rebuttal that they actually did way more experiments that are presented in the paper. Moreover, the experimental results lack comparisons to baselines, ablations, so that the proposed solution could be evaluated fairly.  In its current state, this paper can not be accepted for presentation at ICLR 2019.
The paper proposes a quantization framework that learns a different bit width per layer.  It is based on a differentiable objective where the Gumbel softmax approach is used with an annealing procedure.  The objective trades off accuracy and model size.  The reviewers generally thought the idea has merit.  Quoting from discussion comments (R4): "The paper cited by AnonReviewer 3 is indeed close to the current submission, but in my opinion the strongest contribution of this paper is the formulation from architecture search perspective." The approach is general, and seems to be reasonably efficient (ResNet 18 took "less than 5 hours")  The main negatives are the comparison to other methods.  In the rebuttal, the authors suggested in multiple places that they would update the submission with additional experiments in response to reviewer comments.  As of the decision deadline, these experiments do not appear to have been added to the document. In the discussion: R4: "This paper seems novel enough to me, but I agree that the prior work should at least be cited and compared to. This is a general weakness in the paper, the comparison to relevant prior works is not sufficient." R3: "Not only novel, but more general han the prior work mentioned, but the discussion / experiments do not seem to capture this."  With a range of scores around the borderline threshold for acceptance at ICLR, this is a difficult case.  On the balance, it appears that shortcomings in the experimental results are not resolved in time for ICLR 2019.  The missing results include ablation studies (promised to R4) and a comparison to DARTS (promised to R3): "We plan to perform the suggested experiments of comparing with exhaustive search and DARTS. The results will be hopefully updated before the revision deadline and the camera ready if the paper is accepted." These results are not present and could not be evaluated during the review/discussion phase.
This paper studies change point detection in time series using a multiscale neural network architecture which contains recurrent connections across different time scales.   Reviewers were mixed in this submission. They found the paper generally clear and well written, and the idea of adding a multiscale component to the model interesting. However, they also pointed out weaknesses in the related work section and found the experimental setup somewhat limited. In particular, the paper provides little to no analysis of the learnt features. Taking these assessments into consideration, the AC concludes this submission cannot be accepted at this time. 
This paper proposes an interesting approach to leveraging crowd sourced labels, along with an ML model learned from the data itself.   The reviewers were unanimous in their vote to accept.
The paper proposes to use importance resampling (IR) as an alternative to the more popular importance sampling (IS) approach to off policy RL.  The hope is to reduce variance, as shown in experiments.  However, there is no analysis why/when IR will be better than IS for variance reduction, and a few baselines were suggested by reviewers.  While the authors rebuttal was helpful in clarifying several issues, the overall contribution does not seem strong enough for ICLR, on both theoretical and empirical sides.  The high variance of IS is known, and the following work may be referenced for better 1st order updates when IS weights are used: Karampatziakis & Langford (UAI 11).  In section 3, the paper says that most off policy work uses d_mu, instead of d_pi, to weigh states.  This is true, but in the current context (infinite horizon RL), there are more recent works that should probably be referenced:   http://proceedings.mlr.press/v70/hallak17a.html   https://papers.nips.cc/paper/7781 breaking the curse of horizon infinite horizon off policy estimation
The paper presents "recall traces", a model based approach designed to improve reinforcement learning in sparse reward settings. The approach learns a generative model of trajectories leading to high reward states, and is subsequently used to augment the real experience collected by the agent. This novel take on combining model based and model free learning is conceptually well motivated and is empirically shown to improve sample efficiency on several benchmark tasks.  The reviewers noted the following potential weaknesses in their initial reviews: the paper could provide a clearer motivation of why the proposed approach is expected to lead to performance improvements, and how it relates to learning (and uses of) a forward model. Details of the method, e.g., model parameterization is unclear, and the effect of hyperparameter choices is not fully evaluated.  The authors provided detailed replies to all reviewer suggestions, and ran extensive new experiments, including experiments to address questions about hyperparameter settings, and an entirely new use of the proposed model in a learning from demonstration setting. The authors also clarified the paper as requested by the reviewers. The reviewers have not responded to the rebuttal, but in the AC s assessment their concerns have been adequately addressed. The reviewers have updated their scores in response to the rebuttal, and the consensus is to accept the paper.  The AC notes that the authors seem unaware of related work by Oh et al. "Self Imitation Learning" which was published at ICML 2018. The paper is based on a similar conceptual motivation but imitates high value traces directly, instead of using a generative model. The authors should include a discussion of how their paper relates to this earlier work in their camera ready version.
The reviewers raised a number of major concerns including the incremental novelty of the proposed (WGANs are applied to a new domain), and, most importantly, insufficient and unconvincing experimental evaluation presented (including the lack of comparative studies). The authors’ rebuttal failed to fully alleviate reviewers’ concerns. Hence, I cannot suggest this paper for presentation at ICLR.
The paper studies difficulties in training deep and narrow networks. It shows that there is high probability that deep and narrow ReLU networks will converge to an erroneous state, depending on the type of training that is employed. The results add to our current understanding of the limitations of these architectures.   The main criticism is that the analysis might be very limited, being restricted to very narrow networks (of width about 10 or less) which are not very common in practice, and that the observed collapse phenomenon can be easily addressed by non symmetric initialization.   There were some issues with the proofs that were covered in the discussed between authors and reviewers. The revision is relatively extensive.   This is a borderline case. The paper receives one good rating, one negative rating, and a borderline accept rating. Although the paper contributes interesting insights to a relevant problem that clearly needs contributions in this direction, the analysis presented in the paper and its applicability in practice seems to be very restrictive at this point.  
Reviewer ratings varied radically (from a 3 to an 8). However, the reviewer rating the paper as 8 provided extremely little justification for their rating. The reviewers providing lower ratings gave more detailed reviews, and also engaged in  discussion with the authors. Ultimately neither decided to champion the paper, and therefore, I cannot recommend acceptance.
The paper studies RL from a rate distortion (RD) theory perspective.  A new actor critic algorithm is developed and evaluated on a series of 2D grid worlds.  The paper has some novel idea, and the connection of RL to RD is quite new.  This seems like an interesting direction that is worth further investigation.  On the other hand, all reviewers agreed there is a severe flaw in this work, casting a doubt where RD can be directly applied to an RL setting because the distribution is not fixed (unlike in standard RD).  This issue could have been addressed empirically, by running controlled experiments, something the the paper might include in a future version.
This paper proposes an efficient method to compute the singular values of the linear map represented by a convolutional layer. It makes uses of the special block matrix form of convolutional layers to construct their more efficient method. Furthermore, it shows that this method can be used to devise new regularization schemes for DNNs. The reviewers did note that the diversity of the experiments could be improved, and R2 raised concerns that the wrong singular values were being computed. The authors should add a section clarifying why the singular values of a convolutional linear map are not found directly by performing SVD on the reshaped kernel   indeed the number of singular values would be wrong. A contrast with the singular values obtained by simple reshaping of the kernel would also be helpful.
This paper introduces an autoencoder architecture that can handle sequences of data, and attempts to automatically disentangle multiple static and dynamic factors.  Quality:  The main idea is relatively well motivated.  However the motivation for the particular technical choices made seems a little lacking, and the complexity of the proposed model put a lot of strain on the experiments.  A lot of important updates were made by the authors in the rebuttal period, however I feel the number of changes are a lot to ask the reviewers to re evaluate.  Clarity:  The English of the paper isn t great, including the title (should be "Using an ..." or "Using the ...").  The intro is clear enough, but belabors a relatively simple point about how an image model can t model factors in video.  There were some concerning parts where major issues seemed to be glossed over.  E.g. "FHVAE model uses label information to disentangle time series data, which is different setup with our FAVAE model."  As far as I understand, they both are trained from unsupervised data.   Originality:  This paper does a good job of citing related work, but seems incremental in relation to the FHVAE.  But the main problem is that the proposed method makes a lot of changes from a standard time series VAE, and the limited number of experiments means it s hard to say what the important factor in this model s performance is.  Significance:  Ultimately it s hard to say what the takeaway from this paper is.  The authors motivated and evaluated a new model, but the work wasn t done in a systematic enough way to make an strong conclusions.  What conclusion were asserted seem specious and overly general, e.g. " Since dynamic factors have the same time dependency, these models cannot disentangle dynamic factors.".  Why not?  Why can t a dynamic model learn the time scales of each of its factors automatically? 
The authors have proposed an approach for directly learning a spatial exploration policy which is effective in unseen environments. Rather than use external task rewards, the proposed approach uses an internally computed coverage reward derived from on board sensors. The authors use imitation learning to bootstrap the training and then fine tune using the intrinsic coverage reward. Multiple experiments and ablations are given to support and understand the approach. The paper is well written and interesting. The experiments are appropriate, although further evaluations in real world settings really ought to be done to fully explore the significance of the approach. The reviewers were divided, with one reviewer finding fault with the paper in terms of the claims made, the positioning against prior art, and the chosen baselines. The other two reviewers supported publication even after considering the opposition of R1, noting that they believe that the baselines are sufficient, and the contribution is novel. After reviewing the long exchange and discussion, the AC sides with accepting the paper. Although R1 raises some valid concerns, the authors defend themselves convincingly and the arguments do not, in any case, detract substantially from what is a solid submission.
The authors present a method for training a policy for a self driving car. The inputs to the policy are map based perceptual features and the outputs are waypoints on a trajectory, and the method is an augmented imitation learning framework that uses perturbations and additional losses to make the policy more robust and effective in rare events. The paper is clear and well written and the authors do demonstrate that it can be used to control a real vehicle. However, the reviewers all had concerns about the oracle feature representation which is the input and also concerns about the lack of baselines such as optimization based methods. They also felt that the approach was limited to self driving cars and thus would have limited interest for the community.
I m quite concerned by the conversation with Anonymous, entitled "Why is the dependence...". My issues concern the empirical Rademacher complexity (ERC) and in particular the choice of the loss class for which the ERC is being computed. This  class is obviously data dependent, but the Reviewers concerns centers on the nature of its data dependence. It is not valid to define the classes by the Jacobian s norm on the input data, as this _structure_ over the space of classes is data dependent, which is not kosher. The reviewer was gently pushing the authors towards a very strong assumption... i m guessing that the jacobian norm over all data sets was bounded by a particular constant. This seems like a whopping assumption. The fact that I can so easily read this concern off of the reviewer s comments and the authors seem to not be able to understand what the reviewer is getting at, concerns me.  Besides this concern, it seems that this paper has undergone a rather significant revision. I m not convinced the new version has been properly reviewed. For a theory paper, I m concerned about letting work through that s not properly vetted, and I m really not certain this has been. I suggest the authors consider sending it to COLT.
The paper contributes to the theoretical understanding of finite width ReLU networks. It contributes new ideas and constructions to investigate the representational power of such networks. In particular, the analysis works without skip connections. Referees found the paper refreshingly well written and pleasant to read.   There is a concern that the paper may be overstating the novelty and innovation of the results, as some of them are easy implications, and there are other previous works that have obtained results on finite width networks (see AnonReviewer4 s comments).  On the other hand, the authors were careful to cite when they reuse proof techniques from these and other works (AnonReviewer2). Another concern is that the considered target function space might be too narrow (see AnonReviewer2 s comments). The authors clarify that the choice was because the considered classes are known to be hard to approximate and there are no known classical methods that would yield exponential approximation accuracy. Another concern is that the results might not be suitable to ICLR, having an emphasis on approximation theory and less on learning (see AnonReviewer3 s comments).   The reviewers consistently rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below acceptance threshold ratings.   While this appears to be a well written paper with valuable new ideas in regard to the approximation properties of networks, the contributions were not convincing enough. I would suggest that developing a clearer connecting to learning and broader classes of target functions could increase the appeal of the paper. 
The authors discuss an improved distillation scheme for parallel WaveNet using a Gaussian inverse autoregressive flow, which can be computed in closed form, thus simplifying training. The work received favorable comments from the reviewers, along with a number of suggestions for improvement which have improved the draft considerably. The AC agrees with the reviewers that the work is a valuable contribution, particularly in the context of end to end neural text to speech systems. 
The paper presents generative models to produce multi agent trajectories. The approach of  using a simple heuristic labeling function that labels variables that would otherwise be latent in training data is novel and and results in higher quality than the previously proposed baselines. In response to reviewer suggestions, authors included further results with models that share parameters across agents as well as agent specific parameters and further clarifications were made for other main comments (i.e., baselines that train the hierarchical model by maximizing an ELBO on the marginal likelihood?).
This paper introduces set transformer for set inputs. The idea is built upon the transformer and introduces the attention mechanism. Major concerns on novelty were raised by the reviewers. 
Pros:   novel idea of endowing RL agents with recursive reasoning   clear, well presented paper   thorough rebuttal and revision with new results  Cons:   small scale experiments  The reviewers agree that the paper should be accepted.
The reviewers have all recommended accepting this paper thus I am as well. Based on the reviews and the selectivity of the single track for oral presentations, I am only recommending acceptance as a poster.
The reviewers agree that the idea for dataset distillation is novel, however it is unclear how practical it can be. The paper has been significantly improved through the addition of new baselines, however ultimately the performance is not quite good enough for the reviewers to advocate strongly on its behalf. Perhaps the paper would be better motivated by finding a realistic scenario in which it would make sense for someone to use this approach over reasonable alternatives.
Strengths: Well written paper on a new kind of spherical convolution for use in spherical CNNs. Evaluated on rigid and non rigid 3D shape recognition and retrieval problems. Paper provides solid strategy for efficient GPU implementation.  Weaknesses: There was some misunderstanding about the properties of the alt az convolution detected by one of the reviewers along with some points needing clarifications. However, discussion of these issues appears to have led to a resolution of the issues.  Contention: The weaknesses above were discussed in some detail, but the procedure was not particularly contentious and the discussion unfolded well.  All reviewers rate the paper as accept, the paper clearly provides value to the community and therefore should be accepted. 
The paper proposes a novel approach to neural net construction using dynamical systems approach,  such as higher order Runge Kutta method; this approach also allows a dynamical systems interpretation of DenseNets and CliqueNets. While all reviewers agree that this is an intersting a novel approach, along the lines of recent developments in the field on dynamical systems approaches to deep nets, they also suggest to further improve the writing/clarity of the paper and also strengthen  the empirical results (currently, the method only provided advantage on CIFAR 10, while being somewhat suboptimal on other datasets, and more evidence for empirical advantages of the proposed approach would be great). Overall, this is a very interesting and promising work, and with a few more empirical demonstrations of the method s superiority as well as more polished wiriting the paper would make a nice contribution to ML community.
The paper shows how techniques introduced in the context of unsupervised machine translation can be used to build a style transfer methods.  Pros:     The approach is simple and questions assumptions made by previous style transfer methods (specifically, they show that we do not need to specifically enforce disentanglement).       The evaluation is thorough and shows benefits of the proposed method     Multi attribute style transfer is introduced and benchmarks are created     Given the success of unsupervised NMT, it makes a lot of sense to see if it can be applied to the style transfer problem  Cons:    Technical novelty is limited     Some findings may be somewhat trivial (e.g., we already know that offline classifiers are stronger than the adversarials, e.g., see Elazar and Goldberg, EMNLP 2018).     
as r1 and r2 have pointed out, this work presents an interesting and potentially more generalizable extension of the earlier work on introducing noise as regularization in autoregressive language modelling. although it would have been better with more extensive evaluation that goes beyond unsupervised language modelling and toward conditional language modelling, but i believe this is all fine for this further work to be left as follow up.  r3 s concern is definitely valid, but i believe the existing evaluation set as well as exposition merit presentation and discussion at the conference, which was shared by the other reviewers as well as a programme chair.
This paper proposes a unified approach for performing state estimation and future forecasting for agents interacting within a multi agent system. The method relies on a graph structured recurrent neural network trained on temporal and visual (pixel) information.   The paper is well written, with a convincing motivation and a set of novel ideas.   The reviewers pointed to a few caveats in the methodology, such as quality of trajectories (AnonReviewer2) and expensive learning of states (AnonReviewer3). However, these issues do not discount much of the papers  quality. Besides, the authors have rebutted satisfactorily some of those comments.  More importantly, all three reviewers were not convinced by the experimental evaluation. AnonReviewer1 believes that the idea has a lot of potential, but is hindered by the insufficient exposition of the experiments. AnonReviewer3 similarly asks for more consistency in the experiments.  Overall, all reviewers agree on a score "marginally above the threshold". While this is not a particularly strong score, the AC weighted all opinions that, despite some caveats, indicate that the developed model and considered application fit nicely in a coherent and convincing story. The authors are strongly advised to work further on the experimental section (which they already started doing as is evident from the rebuttal) to further improve their paper.
The paper proposes an improved method for uncertainty estimation in deep neural networks.  Reviewer 2 and AC note that the paper is a bit isolated in terms of comparing the literature.  However, as all of reviewers and AC found, the paper is well written and the proposed idea is clearly new/interesting.
This paper proposes adaptive neural trees (ANT), a combination of deep networks and decision tress. Reviewers 1 leans toward reject the paper, pointing out several flaws. Reviewer 3 also raises concerns, despite later increasing rating to marginally above threshold.  Of particular note is the weak experimental validation.  The paper reports results only on MNIST and CIFAR 10. MNIST performance is too easily saturated to be meaningful. The CIFAR 10 results show ANT models to have far greater error than the state of the art deep neural network models.  As Reviewer 1 states, "performance of the proposed method is also not the best on either of the tested datasets. Please clearly elaborate on why and how to address this issue. It would be more interesting and meaningful to work with a more recent large datasets, such as ImageNet or MS COCO."  The rebuttal fails to offer the type of additional results that would remedy this situation. Without a convincing experimental story, it is not possible to recommend acceptance of this paper.
The reviewers agree the paper brings a novel perspective by controlling the conditioning of the model when performing quantization.  The experiments are convincing experiments. We encourage the authors to incorporate additional references suggested in the reviews. We recommend acceptance. 
This paper addresses a well motivated problem and provides new insight on the theoretical analysis of representational power in quantized networks. The results contribute towards a better understanding of quantized networks in a way that has not been treated in the past.   The most moderate rating (marginally above acceptance threshold) explains that while the paper is technically quite simple, it gives an interesting study and blends well into recent literature on an important topic.   A criticism is that the approach uses modules to approximate the basic operations of non quantized networks. As such it not compatible with quantizing the weights of a given network structure, but rather with choosing the network structure under a given level of quantization. However, reviewers consider that this issue is discussed directly and clearly in the paper.   The reviewers report to be only fairly confident about their assessment, but they all give a positive or very positive evaluation of the paper. 
The reviewers conclude the paper does not bring an important contribution compared to existing work. The experimental study can also be improved. 
This paper presents a differentiable simulator for protein structure prediction that can be trained end to end. It makes several contributions to this research area. Particularly training a differentiable sampling simulator could be of interest to a wider community.  The main criticism comes from the clarity for the machine learning community and empirical comparison with the state of the art methods. The authors  feedback addressed a few  confusions in the description, and I recommend the authors to further polish the text for better readability. R4 argues that a good comparison with the state of the art method in this field would be difficult and the comparison with an RNN baseline is rigorously carried out.  After discussion, all reviewers agree that this paper deserves a publication at ICLR.
This paper claims to demonstrate that CNNs, unlike human vision, do not have a bias towards reliance on shape for object recognition. Both AnonReviewer1 and AnonReviewer2 point to fundamental flaws in the paper s argument, which the rebuttal fails to resolve. (AnonReviewer1 s criticisms are unfortunately conflated with AnonReviewer1 s reluctance to view neuroscience or biological vision as an appropriate topic for ICLR; nonetheless AnonReviewer1 s technical criticism stands).  These observations are:  AnonReviewer2:  "Authors have carefully designed a set of experiments which shows CNNs will [overfit] to non shape features that they added to training images. However, this outcome is not surprising."  AnonReviewer1:  "The experiments don t seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias"  "The best way to demonstrate this would have been to subject a trained image categorization CNN to test data with object shapes in a way that the appearance information couldn’t be used to predict the object label. The paper doesn’t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented."  The AC agrees with both of these observations. CNN behavior is partially a product of the training regime. To examine the scientific question of whether CNNs have similar biases as human vision, the training regimes should be similar. Conversely, if human vision evolved in an environment in which shortcut recognition cues were available via indicator pixels, perhaps it would not have a shape bias.  This paper appears fundamentally flawed in its approach. The results are not informative about differences between human vision and CNNs, nor are they surprising to machine learning practitioners.
This paper presents a biologically plausible architecture and learning algorithm for deep neural networks.  The authors then go on to show that the proposed approach achieves competitive results on the MNIST dataset.  In general, the reviewers found that the paper was well written and the motivation compelling.  However, they were not convinced by the experiments, analysis or comparison to existing literature.  In particular, they did not find MNIST to be a particularly interesting problem and had questions about the novelty of this approach over past literature.  Perhaps the paper would be more impactful and convincing if the authors demonstrated competitive performance on a more challenging problem (e.g. machine translation, speech recognition or imagenet) using a biologically plausible approach. 
The reviewers in general like the paper but has serous reservations regarding relation to other work (novelty) and clarity of presentation. Given non linear state space models is a crowded field it is perhaps better that these points are dealt with first and then submitted elsewhere.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The problem is interesting and challenging   The proposed approach is novel and performs well.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The clarity could be improved  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Many concerns were clarified during the discussion period. One major concern had been the experimental evaluation. In particular, some reviewers felt that experiments on real images (rather than in simulation) was needed. To strengthen this aspect, the authors added new qualitative and quantitative results on a real world experiment with a robot arm, under 10 different scenarios, showing good performance on this challenging task. Still, one reviewer was left unconvinced that the experimental evaluation was sufficient.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  Consensus was not reached. The final decision is aligned with the positive reviews as the AC believes that the evaluation was adequate. 
This paper proposes a method to resolve "language drift," where a pre trained X >language model trained in an X >language >Y pipeline drifts away from being natural language. In particular, it proposes to add an auxiliary training objective that performs grounding with multimodal input to fix this problem. Results are good on a task where translation is done between two languages.  The main concern that was raised with this paper by most of the reviewers is the validity of the proposed task itself. Even after extensive discussion with the authors, it is not clear that there is a very convincing scenario where we both have a pre trained X >language, care about the intermediate results, and have some sort of grounded input to fix this drift. While I do understand the MT task is supposed to be a testbed for the true objective, it feel it is necessary to additionally have one convincing use case where this is a real problem and not just the artificially contrived. This use case could either be of practical use (e.g. potentially useful in an application), or of interest from the point of view of cognitive plausibility (e.g. similar to how children actually learn, and inspired by cognitive science literature).  A concern that offshoots from this is that because the underlying idea is compelling (some sort of grounding to inform language learning), a paper at a high profile conference such as ICLR may help re popularize this line of research, which has been a niche for a while. Normally I would say this is definitely a good thing; I think considering grounding in language learning is definitely an important research direction, and have been a fan of this line of work since reading Roy s seminal work on it from 15 years ago. However, if the task used in this paper, which is of questionable value and reality, becomes the benchmark for this line of work I think this might lead other follow up work in the wrong direction.  I feel that this is a critical issue, and the paper will be much stronger after a more realistic task setting is added.  Thus, I am not recommending acceptance at this time, but would definitely like the authors to think hard and carefully about a good and realistic benchmark for the task, and follow up with a revised version of the paper in the future.
The paper needs work to improve clarity and strengthen the technical message. Also, the authors broke the policy of anonymous submission which disqualifies the paper.
This paper improves upon the PATE GAN framework for differentially private synthetic data generation. They eliminate the need for public data samples for training the GAN, by providing a distribution which can be sampled from instead.  The authors were unanimous in their vote to accept.
The reviewers and AC note that the strength of the paper includes a) an interesting compression algorithm of neural networks with provable guarantees (under some assumptions), b) solid experimental comparison with the existing *matrix sparsification* algorithms. The AC s main concern of the experimental part of the paper is that it doesn t outperform or match the performance of the "vanilla" neural network compression algorithms such as Han et al 15. The AC decided to suggest acceptance for the paper but also strongly encourage the paper to clarify the algorithms in comparison don t include state of the art compression algorithms. 
 Pros:   A useful and well structured dataset which will be of use to the community   Well written and clear (though see Reviewer 2 s comment concerning the clarity of the model description section)   Good methodology  Cons:   There is a question about why a new dataset is needed rather than a combination of previous datasets and also why these datasets couldn t be harvested from school texts directly.  Presumably it would ve been a lot more work but please address the issue in your rebuttal.   Evaluation: Reviewer 3 is concerned that the evaluation should perhaps have included more mathematics specific models (a couple of which are mentioned in the text).  On the other hand, Reviewer 2 is concerned that the specific choices (e.g. "thinking steps") made for the general models are non standard in seq 2 seq models.  I haven t heard about the thinking step approach but perhaps it s out there somewhere. It would be helpful generally to have more discussion about the reasoning involved in these decisions.  I think this is a useful contribution to the community, well written and thoughtfully constructed.  I am tentatively accepting this paper with the understanding that you will engage directly with the reviewers to address their concerns about the evaluation section.  Please in particular use the rebuttal period to focus on the clarity of the model description and the motivation for the particular models chosen.  Also consider adding additional experiments to allay the concerns of the reviewers.
All reviewers agree that the paper is not quite ready for publication. 
The paper proposes a method to escape saddle points by adding and removing units during training. The method does so by preserving the function when the unit is added while increasing the gradient norm to move away from the critical point. The experimental evaluation shows that the proposed method does escape when positioned at a saddle point   as found by the Newton method. The reviewers find the theoretical ideas interesting and novel, but they raised concerns about the method s applicability for typical initializations, the experimental setup, as well as the terminology used in the paper. The title and terminology were improved with the revision, but the other issues were not sufficiently addressed.
This paper proposes a novel approach for network pruning in both training and inference. This paper received a consensus of acceptance. Compared with previous work that focus and model compression on training, this paper saves memory and accelerates both training and inference. It is activation, rather than weight that dominates the training memory. Reviewer1 posed a valid concern about the efficient implementation on GPUs, and authors agreed that practical speedup on GPU is difficult. It ll be great if the authors can give practical insights on how to achieve real speedup in the final draft. 
Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. A significant concern is that the definition of privacy used here is not adequately justified. This opens up issues of: 1) possible attacks, 2) privacy guarantees that are not worst case, among others. 
Strengths:  This paper introduces a clever construction to build a more principled disentanglement objective for GANs than the InfoGAN.  The paper is relatively clearly written.  This method provides the possibility of combining the merits of GANs with the useful information theoretic quantities that can be used to regularize VAEs.  Weaknesses:  The quantitative experiments are based entirely around the toy dSprites dataset, on which they perform comparably to other methods.  Additionally, the qualitative results look pretty bad (in my subjective opinion).  They may still be better than a naive VAE, but the authors could have demonstrated the ability of their model by comparing their models against other models both qualitatively and quantitatively on problems hard enough to make the VAEs fail.  Points of contention:  The quantitative baselines are taken from another paper which did zero hyperparameter search.  However the authors provided an updated results table based on numbers from other papers in a comment.  Consensus:  Everyone agreed that the idea was good and the experiments were lacking.  Some of the comments about experiments were addressed in the updated version but not all.
This paper generated a lot of discussion. Paper presents an empirical evaluation of generalization in models for visual reasoning. All reviewers generally agree that it presents a thorough evaluation with a good set of questions. The only remaining concerns of R3 (the sole negative vote) were lack of surprise in findings and lingering questions of whether these results generalize to realistic settings. The former suffers from hindsight bias and tends to be an unreliable indicator of the impact of a paper. The latter is an open question and should be worked on, but in the opinion of the AC, does not preclude publication of this manuscript. These experiments are well done and deserve to be published. If the findings don t generalize to more complex settings, we will let the noisy process of science correct our understanding in the future. 
In considering the reviews and the author response, I would summarize the evaluation of the paper as following: The main idea in the paper   to combine goal conditioning with successor features   is an interesting direction for research, but is somewhat incremental in light of the prior work in the area. Most of the reviewers generally agreed on this point. While a relatively incremental technical contribution could still result in a successful paper with a thorough empirical analysis and compelling results, the evaluation in the paper is unfortunately not very extensive: the provided tasks are very simple, and the difference from prior methods is not very large. All of the tasks are equivalent to either grid worlds or reaching, which are very simple. Without a deeper technical contribution or a more extensive empirical evaluation, I do not think the paper is ready for publication in ICLR.
This paper proposes the use of holographic reduced representations in language modeling, which allows for a cleaner decomposition of various linguistic traits in the representation. Results show improvements over baseline language models, and analysis shows that the representations are indeed decomposing as expected.  The main reviewer concern was the lack of strength of the baseline, although the authors stress that they were using the default baseline from TensorFlow, which seems like it will be reasonable to me. Another concern is that there is other work on using HRR to disentangle syntax and semantics in representations for language (e.g. "Distributed Tree Kernels" ICML 2012, but also others), that has not been considered.   Based on this, this seems like a very borderline case. Given that no reviewer is pushing strongly for the paper I m leaning towards not recommending acceptance, but I could very easily see the paper being accepted as well.
The reviewers agreed that there are some promising ideas in this work, and useful empirical analysis to motivate the approach. The main concern is in the soundness of the approach (for example, comments about cumulative learning and negative samples). The authors provided some justification about using previous networks as initialization, but this is an insufficient discussion to understand the soundness of the strategy. The paper should better discuss this more, even if it is not possible to provide theory. The paper could also be improved with the addition of a baseline (though not necessarily something like DeepStack, which is not publicly available and potentially onerous to reimplement).  
This paper is borderline for publication for the following reasons: 1) the title is misleading. The majority of the ICLR audience understands by "spatial structure" the structure of the external 3D world, as opposed to the position of the sensors in the internal coordinate system of the agent. Though the authors argue that knowing the positions of the sensors eventually leads to learning the 3D world structure, this appears like a leap in the argument.  2) The equation s \phi(m) described a mapping from robot postures to sensory states. This means the agent should remain within the same scene. The description of this equation in the manuscript as "The mapping  can be seen as describing how “the world” transforms changes in motor states into  changes in sensory states ..." makes this equation appear more general than what it is. s  \psi(s,m) would be better described by such sentence.   
This paper presents new generalized methods for representing sentences and measuring their similarities based on word vectors. More specifically, the paper presents Fuzzy Bag of Words (FBoW), a generalized approach to composing sentence embeddings by combining word embeddings with different degrees of membership, which generalize more commonly used average or max pooled vector representations. In addition, the paper presents DynaMax, an unsupervised and non parametric similarity measure that can dynamically extract and max pool features from a sentence pair.   Pros: The proposed methods are natural generalization of exiting average and max pooled vectors. The proposed methods are elegant, simple, easy to implement, and demonstrate strong performance on STS tasks.  Cons: The paper is solid, no significant con other than that the proposed methods are not groundbreaking innovations per say.   Verdict: The simplicity is what makes the proposed methods elegant. The empirical results are strong. The paper is worthy of acceptance.
The paper presents an interesting treatment of transforming a block sparse fully connected neural networks to a ResNet type Convolutional Network. Equipped with recent development on approximations of function classes (Barron, Holder) via block sparse fully connected networks in the optimal rates, this enables us to show the equivalent power of ResNet Convolutional Nets.   The major weakness in this treatment lies in that the ResNet architecture for realizing the block sparse fully connected nets is unrealistic. It originates from the recent developments in approximation theory that transforming a fully connected net into a convolutional net via Toeplitz matrix (operator) factorizations. However the convolutional nets or ResNets obtained in this way is different to what have been used successfully in applications. Some special properties associated with convolutions, e.g. translation invariance and local deformation stability, are not natural in original fully connected nets and might be indirect after such a treatment.    The presentation of the paper is better polished further. Based on ratings of reviewers, the current version of the paper is on borderline lean reject.
The paper explores unsupervised domain adaptation when the output is structured. Here they focus experimentally on semantic segmentation in driving scenes and use the spatial structure of the scene to produce two losses for adaptation: one global and one patch based. The method tackles an important problem and proposes a first attempt at a new solution. While the the experiments are missing ablations and some comparisons to prior work as noted by the reviewers, the authors have provided comments in their rebuttal explaining the relation to the prior work and promising to include more in the revised manuscript.   The paper is borderline, but falls short on the necessary updates requested by reviewers.  The use of the structured output which is available in semantic segmentation of driving scenes is a useful direction. The paper is missing enough key results and analysis in it s current form to be accepted. 
This paper studies the  behavior of training of over parametrized models. All the reviewers agree that the questions studied in this paper are important. However the experiments in the paper are fairly preliminary and the paper does not offer any answers to the questions it studies.  Further the writing is very loose and the paper is not ready for publication. I advise authors to take the reviews seriously into account before submitting the paper again. 
This paper proposes a combination of SVGD and SLGD and analyzes its non asymptotic properties based on gradient flow. This is an interesting direction to explore. Unfortunately, two major concerns have been raised regarding this paper:  1) the reviewers identified multiple technical flaws. Authors provided rebuttal and addressed some of the problems. But the reviewers think it requires significantly more improvement and clarification to fully address the issues. 2) the motivation of the combination of SVGD and SLGD, despite of being very interesting, is not very clearly motivated; by combining SVGD and SLGD, one get convergence rate for free from the SLGD part, but not much insight is shed on the SVGD part (meaning if the contribution of SLGD is zero, then the bound because vacuum). This could be misleading given that one of the claimed contribution is non asymptotic theory of  SVGD style algorithms" (rather than SLGD style..). We encourage the authors to addresses the technical questions and clarify the contribution and motivation of the paper in revision for future submissions.   
This paper introduces an unsupervised algorithm to learn a goal conditioned policy and the reward function by formulating a mutual information maximization problem. The idea is interesting, but the experimental studies seem not rigorous enough. In the final version, I would like to see some more detailed analysis of the results obtained by the baselines (pixel approaches), as well as careful discussion on the relationship with other related work, such as Variational Intrinsic Control.
This paper presents an interesting approach to image compression, as recognized by all reviewers. However, important concerns about evaluating the contribution remains: as noted by reviewers, evaluating the contribution requires disentangling what part of the improvement is due to the proposed approach and what part is due to the loss chosen and evaluation methods. While authors have done a valuable effort adding experiments to incorporate reviewers suggestions with ablation studies, it does not convincingly show that the proposed approach truly improves over existing ones like Balle et al. Authors are encouraged to strengthen their work for future submission by putting particular emphasis on those questions.
The paper proposes the idea of using optimal transport to evaluate the semantic correspondence between two sets of words predicted by the model and ground truth sequences. Strong empirical results are presented which support the use of optimal transport in conjunction with log likelihood for training sequence models. I appreciate the improvements to the manuscript during the review process, and I encourage the authors to address the rest of the comments in the final version.
The reviewers highlighted aspects of the work that were interesting, particularly on the chosen topic of multi label output of graph neural networks. However, no reviewer was willing to champion the paper, and in aggregate all reviewers trend towards rejection.
The authors have presented a simple yet elegant model to learn grid like responses to encode spatial position, relying only on relative Euclidean distances to train the model, and achieving a good path integration accuracy. The model is simpler than recent related work and uses a structure of  disentangled blocks  to achieve multi scale grids rather than requiring dropout or injected noise. The paper is clearly written and it is intriguing to get down to the fundamentals of the grid code. On the negative side, the section on planning does not hold up as well and makes unverifiable claims, and one reviewer suggests that this section be replaced altogether by additional analysis of the grid model. Another reviewer points out that the authors have missed an opportunity to give a theoretical perspective on their model. Although there are aspects of the work which could be improved, the AC and all reviewers are in favor of acceptance of this paper.
Although one reviewer recommended accepting this paper, they were not willing to champion it during the discussion phase and did not seem to truly believe it is currently ready for publication. Thus I am recommending rejecting this submission.
This paper proposed an interesting approach to weight sharing among CNN layers via shared weight templates to save parameters. It s well written with convincing results. Reviewers have a consensus on accept.
With an average review score of 4.67 and a short review for the one positive review it is just not possible to accept the paper.
This paper presents a large scale annotation of human derived attention maps for ImageNet dataset. This annotation can be used for training more accurate and more interpretable attention models (deep neural networks) for object recognition. All reviewers and AC agree that this work is clearly of interest to ICLR and that extensive empirical evaluations show clear advantages of the proposed approach  in terms of improved classification accuracy. In the initial review, R3 put this paper below the acceptance bar requesting major revision of the manuscript and addressing three important weaknesses: (1) no analysis on interpretability; (2) no details about statistical analysis; (3) design choices of the experiments are not motivated. Pleased to report that based on the author respond, the reviewer was convinced that the most crucial concerns have been addressed in the revision. R3 subsequently increased assigned score to 6. As a result, the paper is not in the borderline bucket anymore. The specific recommendation for the authors is therefore to further revise the paper taking into account a better split of the material in the main paper and its appendix. The additional experiments conducted during rebuttal (on interpretability) would be better to include in the main text, as well as explanation regarding statistical analysis.  
The paper proposes an extension to reinforcement learning with self imitation (SIL)[Oh et al. 2018]. It is based on the idea of leveraging previously encountered high reward trajectories for reward shaping. This shaping is learned automatically using an adversarial setup, similar to GAIL [Ho & Ermon, 2016]. The paper clearly presents the proposed approach and relation to previous work. Empirical evaluation shows strong performance on a 2D point mass problem designed to examine the algorithms behavior. Of particular note are the insightful visualizations in Figure 2 and 3 which shed light on the algorithm s learning behavior. Empirical results on the Mujoco domain show that the proposed approach is particularly strong under delayed reward (20 steps) and noisy observation settings.  The reviewers and AC note the following potential weaknesses: The paper presents an empirical validation showing improvements over PPO, in particular in Mujoco tasks with delayed rewards and with noisy observations. However, given the close relation to SIL, a direct comparison with the latter algorithm seems more appropriate. Reviewers 2 and 3 pointed out that the empirical validation of SIL was more extensive, including results on a wide range of Atari games. The authors provided results on several hard exploration Atari games in the rebuttal period, but the results of the comparison to SIL were inconclusive. Given that the main contribution of the paper is empirical, the reviewers and the AC consider the contribution incremental.  The reviewers noted that the proposed method was presented with little theoretical justification, which limits the contribution of the paper. During the rebuttal phase, the authors sketched a theoretical argument in their rebuttal, but noted that they are not able to provide a guarantee that trajectories in the replay buffer constitute an unbiased sample from the optimal policy, and that policy gradient methods in general are not guaranteed to converge to a globally optimal policy. The AC notes that conceptual insights can also be provided by motivating algorithmic or modeling choices, or through detailed analysis of the obtained results with the goal to further understanding of the observed behavior. Any such form of developing further insights would strengthen the contribution of the submission.
Interesting approach aiming to leverage cross domain schemas and generic semantic parsing (based on meaning representation language, MRL) for language understanding. Experiments have been performed on the recently released SNIPS corpus and comparisons have been made with multiple recent multi task learning approaches. Unfortunately, the proposed approach falls short in comparison to the slot gated attention work by Goo et al.  The motivation and description of the cross domain schemas can be improved in the paper, and for replication of experiments it would be useful to include how the annotations are extended for this purpose.  Experimental results could be extended to the other available corpora mentioned in the paper (ATIS and GEO). 
 * Strengths  This paper applies deep learning to the domain of cybersecurity, which is non traditional relative to more common domains such as vision and speech. I see this as a strength. Additionally, the paper curates a dataset that may be of broader interest.  * Weaknesses  While the empirical results are good, there appears to be limited conceptual novelty. However, this is fine for a paper that is providing a new task in an interesting application domain.  * Discussion  Some reviewers were concerned about whether the dataset is a substantial contribution, as it is created based on existing publicly available data. However, these concerns were addressed by the author responses and all reviewers now agree with accepting the paper.
This paper introduces unsupervised meta learning algorithms for RL. Major concerns of the paper include: 1. Lack of clarity. The presentation of the method can be improved. 2. The motivation and justification of applying unsupervised meta learning needs to be strengthened. More discussions and better motivating examples may be useful. 3. Experimental details are not sufficient and comparisons may not be sufficient to support the aim. Overall, this paper cannot be accepted yet. 
The manuscript centers on a critique of IRGAN, a recently proposed extension of GANs to the information retrieval setting, and introduces a competing procedure.   Reviewers found the findings and the proposed alternative to be interesting and in one case described the findings as "illuminating", but were overall unsatisfied with the depth of the analysis, and in more than one case complained that too much of the manuscript is spent reviewing IRGAN, with not enough emphasis and detailed investigation of the paper s own contribution. Notational issues, certain gaps in the related work and experiments were addressed in a revision but the paper still reads as spending a bit too much time on background relative to the contributions. Two reviewers seemed to agree that IRGAN s significance made at least some of the focus on it justifiable, but one remarked that SIGIR may be a better venue for this line of work (the AC doesn t necessarily agree).  Given the nature of the changes and the status of the manuscript following revision, it does seem like a more comprehensive rewrite and reframing would be necessary to truly satisfy all reviewer concerns. I therefore recommend against acceptance at this point in time.
All in all, while the reviewers found that the problem at hand is interesting to study, the submission s contributions in terms of significance/novelty did not rise to the standards for acceptance. The reasoning is most succinctly discussed by R3 who argues that IFS and EFS are basically feature selection and applying them to feature attribution is not particularly novel from a methodological point of view. 
The presented approach demonstrates an invertible architecture for auto encoding, which demonstrates improvements in performance relative to VAE and WAE s on MNIST.   Pros: + R3: The idea of pseudo inversion is interesting. + R3: Manuscript is clear.   Cons:   R1,2,3: Additional experiments needed on CIFAR, ImageNet, others.   R1: Presentation unclear. Authors have not made any apparent attempt to improve the clarity of the manuscript, though they make their point that the method allows dimensionality reduction in their response.   R1, R2: Main advantages not clear.     R3: Text could be compressed further to allow room for additional experiments.   Reviewers lean reject, and authors have not updated experiments. Authors are encouraged to continue to improve the work.
The paper proposes an approach to remedying mode collapse problem in GANs. This approach relies on using multiple discriminators and assigning a different portion of each minibatch to each discriminator.   + preventing mode collapse in GAN training is an important problem    the exact motivation for the proposed techniques is not fully fleshed out    the evaluation and baselines used are lacking
Lean in favor  Strengths:  The paper tackles the difficult problem of automatic robot design. The approach uses graph neural networks to parameterize the control policies, which allows for weight sharing / transfer to new policies even as the topology changes.  Understanding how to efficiently explore through non differentiable changes to the body is an important problem (AC). The authors will release the code and environments, which will be useful in an area where there are  currently no good baselines (AC).   Weaknesses: There are concerns (particularly R2, R1) over the lack of a strong baseline, and with the results  being demonstrated on a limited number of environments (R1)  (fish, 2D walker). In response, the authors clarified the nomenclature and description of a number of the baselines, and added others. AC: there is no submitted video (searches for "video" on the PDF text produces no hits); this is seen by the AC as being a real limitation from the perspective of evaluation.  AC agrees with some of the reviewer remarks that some of the original stated claims are too strong.   AC: the simplified fluid model of Mujoco (http://mujoco.org/book/computation.html#gePassive) is unable to model the fluid state, in particular the induced fluid vortices that are responsible for a good portion of fish locomotion, i.e., "Passive and active flow control by swimming fishes and mammals" and other papers. Acknowledging this kind of limitation will make the paper stronger, not weaker; the ML community can learn from much existing work at the interface of biology and fluid mechancis.  There remain points of contention, i.e., the sufficiency of the baselines. However, the reviewers R2 and R3 have not responded to the detailed replies from the authors, including additional baselines (totaling 5 at present)  and pointing out that baselines such as CMA ES (R2) in a continuous space and therefore do not translate in any obvious way to the given problem at hand.    On balance, with the additional baselines and related clarifications, the AC feels that this paper makes a  useful and valid contribution to the field, and will help establish a benchmark in an important area. The authors are strongly encouraged to further state caveats and limitations, and to emphasize why some candidate baseline methods are not readily applicable. 
This paper presents a method for measuring the degree to which some representation for a composed object effectively represents the pieces from which it is composed. All three authors found this to be an important topic for study, and found the paper to be a limited but original and important step toward studying this topic. However, two reviewers expressed serious concerns about clarity, and were not fully satisfied with the revisions made so far. I m recommending acceptance, but I ask the authors to further revise the paper (especially the introduction) to make sure it includes a blunt and straightforward presentation of the problem under study and the way TRE addresses it.  I m also somewhat concerned at R2 s mention of a potential confound in one experiment. The paper has been updated with what appears to be a fix, though, and R2 has not yet responded, so I m presuming that this issue has been resolved.  I also ask the authors to release code shortly upon de anonymization, as promised.
Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. The greatest concern was that the novelty beyond past work has not been sufficiently demonstrated.
All authors agree that the relativistic discriminator is an interesting idea, and a useful proposal to improve the stability and sample quality of GANs. In earlier drafts there were some clarity issues and missing details, but those have been fixed to the satisfaction of the reviewers. Both R1 and R3 expressed a desire for a more theoretical justification of why the relativistic discriminator should work better, but the empirical results are strong enough that this can be left for future work.
The paper describes a method which, given a music waveform, generates another recording of the same music which should sound as if it was performed by different instruments. The model is an auto encoder with a WaveNet like domain specific decoder and a shared encoder, trained with an adversarial "domain confusion loss". Even though the method is constructed mostly from existing components, the reviewers found the results interesting and convincing, and recommended the paper for acceptance.
This paper introduces a few training methods to fit the dynamics of a PDE based on observations.  Quality:  Not great.  The authors seem unaware of much related work both in the numerics and deep learning communities.  The experiments aren t very illuminating, and the connections between the different methods are never clearly and explicitly laid out in one place. Clarity:  Poor.  The intro is long and rambly, and the main contributions aren t clearly motivated.  A lot of time is spent mentioning things that could be done, without saying when this would be important or useful to do.  An algorithm box or two would be a big improvement over the many long english explanations of the methods, and the diagrams with cycles in them. Originality:  Not great.  There has been a lot of work on fitting dynamics models using NNs, and also attempting to optimize PDE solvers, which is hardly engaged with. Significance:  This work fails to make its own significance clear, by not exploring or explaining the scope and limitations of their proposed approach, or comparing against more baselines from the large set of related literature.
This paper contributes a novel approach to evaluating the robustness of DNN based on structured sparsity to exploit the underlying structure of the image and introduces a method to solve it. The proposed approach is well evaluated and the authors answered the main concerns of the reviewers. 
This paper introduces a distillation approach for black box classifiers that trains generalized additive models (GAM), an additive model over feature shapes, thus providing global explanations for the model. Given the importance of interpretability, the reviewers appreciated the focus of this work. The reviewers also found the experiments, both on real and synthetic datasets, extremely thorough and were impressed by the results. Finally, they also mentioned that the paper was clearly well written.  The reviewers and AC note the following potential weaknesses:  (1) The primary concern, raised by all of the reviewers, is the lack of novelty;the proposed approach is a straightforward application of GAMs to model distillation, where black box output is the training data of the GAM, (2) The reviewers are also concern that the proposed approach is limited in scope to tabular datasets, and would not work for more interesting, complex domains like text or images, and (3) The reviewers are concerned that the interpretability of GAMS is assumed, without describing the limitations, for example, if there are correlated features, the shapes would affect each other in uninterpretable ways. Amongst other concerns, the reviewers were concerned about the formatting of the plots and tables in the paper, which made it difficult to read them, and the lack of a user study to verify the interpretability claims.  In response to these criticisms, the authors provided comments and a substantial revision to the papers, heavily restructuring the paper to fit extra experiments (comparison to other global explanation techniques, including a user study) and make the figures and tables readable. While the paper was much improved by these changes, and two of the reviewers increased their scores accordingly, concerns about the limited novelty and scope still remained.  Ultimately, the reviewers did not reach a conclusion, but the concerns of novelty and scope overwhelmed the clear benefits of the approach and the strong results. This paper was very close to getting accepted, and we strongly urge the authors to submit it to other premier ML conferences.
This paper proposes new heuristics to prune and compress neural networks. The paper is well organized. However, reviewers are concerned that the novelty is relatively limited. The advantage of the proposed method is marginal on ImageNet. What is effective is not very clear. Therefore, recommend for rejection. 
This paper proposes search guided training for structured prediction energy networks (SPENs).  The reviewers found some interest in this approach, though were somewhat underwhelmed by the experimental comparison and the details provided about the method.  R1 was positive and recommends acceptance; R2 and R3 thought the paper was on the incremental side and recommend rejection. Given the space restriction to this year s conference, we have to reject some borderline papers. The AC thus recommends the authors to take the reviewers comments in consideration for a "revise and resubmit".
This paper presents a new unsupervised training objective for sentence to vector encoding, and shows that it produces representations that often work slightly better than those produced by some prominent earlier work.  The reviewers have some concerns about presentation, but the main issue—which all three reviewers pointed to—was the lack of strong recent baselines. Sentence to vector representation learning is a fairly active field with an accepted approach to evaluation, and this paper seems to omit conspicuous promising baselines. This includes labeled data pretraining methods which are known to work well for English (including results from the cited Conneau paper)—while these may be difficult to generalize beyond English, this paper does not attempt such a generalization. This also includes more recent unlabeled data methods like ULMFiT or Radford et al. s Transformer which could be easily trained on the same sources of data used here. The authors argue in the comments that these language models tend to use more parameters, but these additional parameters are only used during pretraining, so I don t find this objection compelling enough to warrant leaving out baselines of this kind. Baselines of both kinds have been known for at least a year and come with distributed models and code for close comparison.
Quality: The overall quality of the work is high.  The main idea and technical choices are well motivated, and the method is about as simple as it could be while achieving its stated objectives.  Clarity:  The writing is clear, with the exception of using alternative scripts for some letters in definitions.  Originality:  The biggest weakness of this work is originality, in that there is a lot of closely related work, and similar ideas without convergence guarantees have begun to be explored.  For example, the (very natural) U net architecture was explored in previous work.  Significance:  This seems like an example of work that will be of interest both to the machine learning community, and also the numerics community, because it also achieves the properties that the numerics community has historically cared about.  It is significant on its own as an improved method, but also as a demonstration that using deep learning doesn t require scrapping existing frameworks but can instead augment them.
This paper proposes a method for tracing activations in a capsule based network in order to obtain semantic segmentation from classification predictions.  Reviewers 1 and 2 rate the paper as marginally above threshold, while Reviewer 3 rates it as marginally below. Reviewer 3 particularly points to experimental validation as a major weakness, stating: "not sure if the method will generalize well beyond MNIST", "I’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only."  The AC shares these concerns and does not believe the current experimental validation is sufficient. MNIST is a toy dataset, and may have been appropriate for introducing capsules as a new concept, but it is simply not difficult enough to serve as a quantitative benchmark to distinguish capsule performance from U Net. U Net and Tr CapsNet appear to have similar performance on both MNIST and the hippocampus dataset; the relatively small advantage to Tr CapsNet is not convincing.  Furthermore, as Reviewer 1 suggests, it would seem appropriate to include experimental comparison to other capsule based segmentation approaches (e.g. LaLonde and Bagci, Capsules for Object Segmentation, 2018). This related work is mentioned, but not used as an experimental baseline. 
this submission follows on a line of work on online learning of a recurrent net, which is an important problem both in theory and in practice. it would have been better to see even more realistic experiments, but already with the set of experiments the authors have conducted the merit of the proposed approach shines. 
This paper propose to obtain high pruning ratio by adding constraints to obtain small weights. Reviewers have a consensus on rejection due to not convincing experiments and lack of novelty.
The paper proposes a regularization method that introduces an information bottleneck between parameters and predictions.  The reviewers agree that the paper proposes some interesting ideas, but those idea need to be clarified. The paper lacks in clarity. The reviewers also doubt whether the paper is expected to have significant impact in the field.
This paper was reviewed by three experts (I assure the authors R3 is indeed familiar with RL and this area). Initially, the reviews were mixed with several concerns raised. After the author response, R2 and R3 recommend rejecting the paper, and R1 is unwilling to defend/champion/support it (not visible to the authors). The AC agrees with the concerns raised (in particular by R2) and finds no basis for overruling this recommendation. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue. 
This paper introduced an adaptive importance sampling strategy to select mini batches to speed up the convergence of network training. The method is well motivated and easy to follow.  The main concerns raised by the reviewers are limited novelty of the proposed simple idea compared to related recent work, and moderate empirical performance.  The authors argue that the particular choice of the adaptive sampling method comes after trying various methods. I believe providing more detailed discussion and comparison with different methods together with the "active bias" paper would help the readers appreciate the insights conveyed in this paper.  The authors provide some additional experiments in the revision. It would make the whole experiment section a lot stronger and convincing if the authors could run more thorough experiments on extra challenging datasets and include all the results int the main text.   Additional experiment to clarify the merit of the proposed method on either faster convergence or lower asymptotic error would also improve the contribution of this paper.
All reviewers rate the paper as below threshold. While the authors responded to an earlier request for clarification, there is no rebuttal to the actual reviews. Thus, there is no basis by which the paper can be accepted.
Evaluating this paper is somewhat awkward because it has already been through multiple reviewing cycles, and in the meantime, the trick has already become widely adopted and inspired interesting follow up work. Much of the paper is devoted to reviewing this follow up work. I think it s clearly time for this to be made part of the published literature, so I recommend acceptance. (And all reviewers are in agreement that the paper ought to be accepted.)  The paper proposes, in the context of Adam, to apply literal weight decay in place of L2 regularization. An impressively thorough set of experiments are given to demonstrate the improved generalization performance, as well as a decoupling of the hyperparameters.   Previous versions of the paper suffered from a lack of theoretical justification for the proposed method. Ordinarily, in such cases, one would worry that the improved results could be due to some sort of experimental confound. But AdamW has been validated by so many other groups on a range of domains that the improvement is well established. And other researchers have offered possible explanations for the improvement. 
The reviewers liked the clarity of the material and agreed the experimental study is convincing. Accept.
The paper presents a careful analysis of SGD by characterizing the stochastic gradient via von Mises Fisher distributions. While the paper has good quality and clarity, and the authors  detailed response has further clarified several raised issues, some important concerns remain: Reviewer 1 would like to see careful discussions on related observations by other work in the literature, such as low rank Hessians in the over parameterized regime, Reviewer 2 is concerned about the significance of the presented analysis and observations, and Reviewers 2 and 4 both would like to see how the presented theoretical analysis could be used to design improved algorithms. In the AC s opinion, while solid theoretical analysis of SGD is definitely valuable, it is highly desirable to demonstrate its practical value (considering that it does not provide clearly new insights about the learning dynamics of SGD).
The reviewers overall agree that excitation dropout is a novel idea that seems to produce good empirical performance. However, they remain optimistic, but unconvinced by the experiments in their current form. The authors have done an admiral job of addressing this through more experiments, including providing error bars, however it seems as though the reviewers still require more. I would recommend creating tables of architecture x dropout technique, where dropout technique includes information dropout, adaptive dropout, curriculum dropout, and standard dropout, across several standard datasets. Alternatively, the authors could try to be more ambitious and classify Imagenet. Essentially, it seems as though the current small scale datasets have become somewhat saturated, and therefore the bar for gauging a new method on them is higher in terms of experimental rigor. This means the best strategy is to either try more difficult benchmarks, or be extremely thorough and complete in your experiments.  Regarding the wide resnet result, while I can appreciate that the original version published with higher errors, the later draft should still be taken into account as it has a) been out for a while now and b) can been reproduced in open source implementations (e.g., https://github.com/szagoruyko/wide residual networks).
The paper addresses the challenging and important problem of exploration in sparse rewards settings. The authors propose a novel use of contingency awareness, i.e., the agent s understanding of the environment features that are under its direct control, in combination with a count based approach to exploration. The model is trained using an inverse dynamics model and attention mechanism and is shown to be able to identify the controllable character. The resulting exploration approach achieves strong empirical results compared to alternative count based exploration techniques. The reviewers note that the novel approach has potential for opening up potential fruitful directions for follow up research. The obtained strong empirical results are another strong indication of the value of the proposed idea.   The reviewers mention several potential weaknesses. First, while the proposed idea is general, the specific implementation seems targetted specifically towards Atari games. While Atari is a popular benchmark domain, this raises questions as to whether insights can be more generally applied. Second, several questions were raised regarding the motivation for some of the presented modeling choices (e.g., loss terms) as well as their impact on the empirical results. Ablation studies were recommended as a step to resolving these questions Reviewer 3 questioned whether the learned state representation could be directly used as an additional input to the agent, and if it would improve performance. Finally, several related works were suggested that should be included in the discussion of related work.  The authors carefully addressed the issues raised by the reviewers, running additional comparisons and adding to the original empirical insights. Several issues of clarity were resolved in the paper and in the discussion. Reviewer 3 engaged with the authors and confirmed that they are satisfied with the resulting submission. The AC judges that the suggestions of reviewer 1 have been addressed to a satisfactory level. A remaining issue regarding results reporting was raised anonymously towards the end of the review period, and the AC encourages the authors to address this issue in their camera ready version.
This work propose a method for learning a Kolmogorov model,  which is a binary random variable model that is very similar (or identical) to a matrix factorization model. The work proposes an alternative optimization approach that is again similar to matrix factorization approaches.  Unfortunately, no discussion or experiments are made to compare the  proposed problem and method with standard matrix factorization; without such comparison, it is unclear if the proposed is substantially new, or a reformation of a standard problem. The authors are encouraged to improve the draft to clarify the connection matrix factorization and standard factor models. 
Although the paper considers a somewhat limited problem of learning a neural network with a single hidden layer, it achieves a surprisingly strong result that such a network can be learned exactly (or well approximated under sampling) under weaker assumptions than recent work.  The reviewers unanimously recommended the paper be accepted.  The paper would be more impactful if the authors could clarify the barriers to extending the technique of pure neuron detection to deeper networks, as well as the barriers to incorporating bias to eliminate the symmetry assumption.
This paper proposes a new method for graph representation in sequence to sequence models and validates its results on several tasks. The overall results are relatively strong.  Overall, the reviewers thought this was a reasonable contribution if somewhat incremental. In addition, while the experimental comparison has greatly improved from the original version, there are still a couple of less satisfying points: notably the size of the training data is somewhat small. In addition, as far as I can tell all comparisons with other graph based baselines actually aren t implemented in the same toolkit with the same hyperparameters, so it s a bit difficult to tell whether the gains are coming from the proposed method itself or from other auxiliary differences.  I think this paper is very reasonable, and definitely on the borderline for acceptance, but given the limited number of slots available at ICLR this year I am leaning in favor of the other very good papers in my area.
This paper proposes to use constrained Bayesian optimization to improve the chemical compound generation. Unfortunately, the reviewers raises a range of critical issues which are not responded by authors  rebuttal. 
The paper presents an explicit memory that directly contributes to more efficient exploration. It stores trajectories to novel states, that serve as training data to learn to reach those states again (through iterative sub goals).   The description of the method is quite clear, the method is not completely novel but has some merit. Most weaknesses of the paper come from the experimental section: too specific environments/solutions, lack of points of comparisons, lacking some details.  We strongly encourage the authors to add additional experimental evidence, and details. In its current form, the paper is not sufficient for publication at ICLR 2019.  Reviewers wanted to note that the blog post from Uber ("Go Explore") did _not_ affect their evaluation of this paper.
The paper introduces a new variance reduced policy gradient method, for directional and clipped action spaces, with provable guarantees that the gradient is lower variance. The paper is clearly written and the theory an important contribution. The experiments provide some preliminary insights that the algorithm could be beneficial in practice. 
This paper presents an algorithm for combining various feature types when training recurrent networks. The features are handled by modifying the update rules and cell states based on the features  type   dense, sparse, static, w/ decay, etc.  Strengths   The model handles each feature according to its type and handles cell state and transitions appropriately.    Extends earlier work to handle more feature types, like sparse features.  Weaknesses   Limited novelty. Models similar to various aspects of the proposed system have been presented in prior works. For example: TLSTM, which the authors use as a baseline. Although some components are novel, like the treatment of sparse features, contributions, in my opinion, are not sufficient to be accepted at ICLR.   Presentation: Confusing and not enough information for reproducing results; multiple reviewers raised concerns about presentation of the feature types and experimental results. There were suggestions to improve, which the authors did consider during revision, but some concerns still remain.  In the end, the reviewers agreed about the limited novelty of this work, given existing literature. The recommendation, therefore, is to reject the paper.  
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
This paper presents an interesting method for code generation using a graph based generative approach.  Empirical evaluation shows that the method outperforms relevant baselines (PHOG).  There is consensus among reviewers that the methods are novel and is worth acceptance to ICLR.
This paper proposes to combine rewards obtained through IRL from rewards coming from the environment, and evaluate the algorithm on grid world environments. The problem setting is important and of interest to the ICLR community. While the revised paper addresses the concerns about the lack of a stochastic environment problem, the reviewers still have major concerns regarding the novelty and significance of the algorithmic contribution, as well as the limited complexity of the experimental domains. As such, the paper does not meet the bar for publication at ICLR.
The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
The paper investigates an incremental form of Sliced Inverse Regression (SIR) for supervised dimensionality reduction.  Unfortunately, the experimental evaluation is insufficient as a serious evaluation of the proposed techniques.  More importantly, the paper does not appear to contribute a significant advance over the extensive literature on fast generalized eigenvalue decompositions in machine learning.  No responses were offered to counter such an opinion.
The authors present a study characterizing adversarial examples in the audio domain. They highlight the importance of temporal dependency when defining defense against adversarial attacks.  Strengths   The work presents an interesting analysis of properties of audio adversarial examples, and contrasts it with those in vision literature.   Proposes a novel defense mechanism that is based on the idea of temporal dependency.  Weaknesses   The technique identifies adversarial examples but is not able to make the correct prediction.   The reviewers raised issue around clarity, but the authors took the effort to improve the section during the revision process.   The reviewers agree that the contribution is significant and useful for the community. There are still some concerns about clarity, which the authors should consider improving in the final version. Overall, the paper received positive reviews and therefore, is recommended to be accepted to the conference.
The paper proposes GAN regularized by Determinantal Point Process to learn diverse data samples.  The reviewers and AC commonly note the critical limitation of novelty of this paper.  The authors pointed out  "To the best of our knowledge, we are the first to introduce modeling data diversity using a Point process kernel that we embed within a generative model. "  AC does not think this is convincing enough to meet the high standard of ICLR.  AC decided the paper might not be ready to publish in the current form.
 The paper proposes an method for investigating robustness of graph neural nets for node classification problem; training time attacks for perturbing graph structure are generated using  meta learning approach. Reviewers agree that the contribution is novel and empirical results support the validity of the approach.  
This is a well written paper that shows how to use optimal transport to perform smooth interpolation, between two random vectors sampled from the prior distribution of the latent space of a deep generative model. By encouraging the marginal of the interpolated vector to match the prior distribution, these interpolated distribution preserving random vectors in the latent space are shown to result in better image interpolation quality for GANs. The problem is of interest to the community and the resulted solutions are simple to implement.   As pointed out by Reviewer 1, the paper could be made clearly more convincing by showing that these distribution preservation operations also help perform interpolation in the latent space of VAEs, and the AC strongly encourages the authors to add these results if possible.   The AC appreciates that the authors have added experiments to satisfactorily address his/her concern:  "Suppose z_1,z_2 are independent, and drawn from N(\mu,\Sigma), then t z_1 + (1 t)z_2 ~ N(\mu, (t^2+(1 t)^2)\Sigma). If one lets y | z_1, z_2 ~ N(t z_1 + (1 t)z_2, (1 t^2 (1 t)^2)\Sigma) as the latent space interpolation, then marginally we have y ~ N(\mu, \Sigma). This is an extremely simple and fast procedure to make sure that the latent space interpolation y is highly related to the linear interpolation t z_1 + (1 t)z_2 but also satisfies  y ~ N(\mu, \Sigma)."  The AC strongly encourages the authors to add these new results into their revision, and highlight "smooth interpolation" as an important characteristic in addition to "distribution preserving." A potential suggestion is changing "Distribution Preserving Operations" in the title to "Distribution Preserving Smooth Operations." 
The authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias.  To achieve this, the authors learn the planners and the reward functions from demonstrations. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments. The problem considered is important and challenging. One issue is that in order to make progress the authors need to make strong and restrictive assumptions (e.g., assumption 3, the well suited inductive bias). It is not clear if the assumptions made are reasonable. Experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be. Overall, there is consensus among the reviewers that the paper is interesting but not ready for publication. 
This paper proposes a criterion for representation learning, minimum necessary information, which states that for a task defined by some joint probability distribution P(X,Y) and the goal of (for example) predicting Y from X, a learned representation of X, denoted Z, should satisfy the equality I(X;Y)   I(X;Z)   I(Y;Z). The authors then propose an objective function, the conditional entropy bottleneck (CEB), to ensure that a learned representation satisfies the minimum necessary information criterion, and a variational approximation to the conditional entropy bottleneck that can be parameterized using deep networks and optimized with standard methods such as stochastic gradient descent. The authors also relate the conditional entropy bottleneck to the information bottleneck Lagrangian proposed by Tishby, showing that the CEB corresponds to the information bottleneck with β   0.5. An important contribution of this work is that it gives a theoretical justification for selecting a specific value of β rather than testing multiple values. Experiments on Fashion MNIST show that, in comparison to a deterministic classifier and to variational information bottleneck models with β in {0.01, 0.1, 0.5}, the CEB model achieves good accuracy and calibration, is competitive at detecting out of distribution inputs, and is more resistant to white box adversarial attacks. Another experiment demonstrates that a model trained with the CEB criterion is *unable* to memorize a randomly labeled version of Fashion MNIST. There was a strong difference of opinion between the reviewers on this paper. One reviewer (R1) dismissed the work as trivial. The authors rebutted this claim in their response and revision, and R1 failed to participate in the discussion, so the AC strongly discounted this review. The other two reviewers had some concerns about the paper, most of which were addressed by the revision. But, crucially, some concerns still remain. R4 would like more theoretical rigor in the paper, while R2 would like a direct comparison against MINE and CPC. In the end, the AC thinks that this paper needs just a bit more work to address these concerns. The authors are encouraged to revise this work and submit it to another machine learning venue.
Reviewer scores straddle the decision boundary but overall this does work does not meet the bar yet. Even after discussion with the authors, the reviewers reconfirmed there  reject  recommendation and the area chair agrees with that assessment.
While the paper contains interesting ideas, the reviewers agree the experimental study can be improved. 
In this work, the authors conduct experiments using variants of RNNs and Gated CNNs on a speech recognition task, motivated by the goal of reducing the computational requirements when deploying these models on mobile devices. While this is an important concern for practical deployment of ASR systems, the main concerns expressed by the reviewers is that the work lacks novelty. Further, the authors choice to investigate CTC based systems which predict characters. These models are not state of the art for ASR, and as such it is hard to judge the impact of this work on a state of the art embedded ASR system. Finally, it would be beneficial to replicate results on a much larger corpus such as Librispeech or Switchboard. Based on the unanimous decision from the reviewers, the AC agrees that the work, in the present form, should be rejected. 
The reviewers vary in their scores but overall there is agreement that this paper is not ready for acceptance. 
The reviewers found the paper to be well written, the work novel and they appreciated the breadth of the empirical evaluation.  However, they did not seem entirely convinced that the improvements over the baseline are statistically significant.  Reviewer 1 has lingering concerns about the experimental conditions and whether propensity score matching within a minibatch would provide a substantial improvement over propensity score matching across the dataset.  Overall the reviewers found this to be a good paper and noted that the discussion was illuminating and demonstrated the merits of this work and interest to the community.  However, no reviewers were prepared to champion the paper and thus it falls just below borderline for acceptance.
This paper proposes the NonLinearity Coefficient (NLC), a metric which aims to predicts test time performance of neural networks at initialization. The idea is interesting and novel, and has clear practical implications. Reviewers unanimously agreed that the direction is a worthwhile one to pursue. However, several reviewers also raised concerns about how well justified the method is: in particular, Reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad hoc. Reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity.   These concerns make it clear that the paper needs more work before it can be published. And, in particular, addressing the reviewers  concerns and providing proper comparison to related works will go a long way in that direction.
The paper presents a novel unit making the networks intrinsically more robust to gradient based adversarial attacks. The authors have addressed some concerns of the reviewers (e.g. regarding pseudo gradient attacks) but experimental section could benefit from a larger scale evaluation (e.g. Imagenet).
This paper proposes a variant of GAIL that can learn from both expert and non expert demonstrations. The paper is generally well written, and the general topic is of interest to the ICLR community. Further, the empirical comparisons provide some interesting insights. However, the reviewers are concerned that the conceptual contribution is quite small, and that the relatively small conceptual contribution also does not lead to large empirical gains. As such, the paper does not meet the bar for publication at ICLR.
This paper presents a substantially new way of introducing a syntax oriented inductive bias into sentence level models for NLP without explicitly injecting linguistic knowledge. This is a major topic of research in representation learning for NLP, so to see something genuinely original work well is significant. All three reviewers were impressed by the breadth of the experiments and by the results, and this will clearly be among the more ambitious papers presented at this conference.  In preparing a final version of this paper, though, I d urge the authors to put serious further effort into the writing and presentation. All three reviewers had concerns about confusing or misleading passages, including the title and the discussion of the performance of tree structured models so far.
Based on the majority of reviewers with reject (ratings: 4,6,3), the current version of paper is proposed as reject. 
Dear authors,  The reviewers all appreciated the treatment of the topic and the quality of the writing. It is rare for all reviewers to agree on this, congratulations.  However, all reviewers also felt that the paper could have gone further in its analysis. In particular, they noticed that quite a few points were either mentioned in recent papers or lacked an experimental validation.  Given the reviews, I strongly encourage the authors to expand on their findings and submit the improved work to a future conference.
The paper makes progress on a problem that is still largely unexplored, presents promising results, and builds bridges with  prior work on optimal control.  It designs input convex recurrent neural networks to capture temporal behavior of  dynamical systems; this then allows optimal controllers to be computed by solving a convex model predictive control problem.  There were initial critiques regarding some of the claims. These have now been clarified. Also, there is in the end a compromise between the (necessary) approximations of the input convex model and the true dynamics, and being able to compute an optimal result.   Overall, all reviewers and the AC are in agreement to see this paper accepted. There was extensive and productive interaction between the reviewers and authors. It makes contributions that will be of interest to many, and builds interesting bridges with known control methods.
Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
The reviewers raise a number of concerns including no methodological novelty, limited experimental evaluation, and relatively uninteresting application with very limited real world application. This set of facts has been assessed differently by the three reviewers, and the scores range from probable rejection to probable acceptance. I believe that the work as is would not result in a wide interest by the ICLR attendees, mainly because of no methodological novelty and relatively simplistic application. The authors’ rebuttal failed to address these issues and I cannot recommend this work for presentation at ICLR.
This paper provides a theoretical analysis of the Turing completeness of popular neural network architectures, specifically Neural Transformers and the Neural GPU. The reviewers agreed that this paper provides a meaningful theoretical contribution and should be accepted to the conference. Work of a theoretical nature is, amongst other types of work, called for by the ICLR CFP, but is not a very popular category for submissions, nor is it an easy one. As such, I am happy to follow the reviewers  recommendation and support this paper.
The reviewers all like the idea, and though the performance is a little better when compared to prototypical networks, the reviewers felt that the contribution over and above prototypical networks was marginal and none of them was willing to champion the paper. There is merit in that there is increased robustness to outliers, and future iterations of the paper should work to strengthen this aspect.  As a quick nitpick: based on my reading, and on Figure 3, it looks like there might be a typo in the definition of X_k (bottom of page 4). Right now it is defined in terms of the original data space x, when I think it should be defined in terms of the embedding space f(x). Overall this paper is a good contribution to the few shot learning area.
Strengths:  This paper gives a detailed treatment of the connections between rate distortion theory and variational lower bounds, culminating in a practical diagnostic tool.  The paper is well written.  Weaknesses:  Many of the theoretical results existed in older work.  Points of contention:  Most of the discussion was about the novelty of the lower bound.  Consensus:  R3 and R2 both appear to recommend acceptance (R2 in a comment), and have both clearly given the paper detailed thought.
This paper proposes methods to improve the performance of the low precision neural networks. The reviewers raised concern about lack of novelty. Due to insufficient technical contribution, recommend for rejection. 
This paper presents a probabilistic programming language where models are constructed out of building blocks which specify both the distribution and an inference procedure. As a demonstration, they show how a GP LVM can be implemented.  The paper spends a lot of space arguing for the benefits of modularity. Modularity is of course hard to argue with, and the benefits are already understood in the PPL community. But, as the reviewers point out, various other PPLs have already adopted various strategies to enable modular definition of models, and (in cases like Venture) special purpose higher level inference algorithms. This paper contains little discussion of other PPLs and how the specific design decisions relate to theirs, so it s hard to judge whether this paper really covers new ground. Such discussion wasn t added to the revised paper, even though multiple reviewers asked for it. I can t recommend acceptance. 
The proposed method is compressing video sequences with an end to end approach, by extending a variational approach from images to videos. The problem setting is interesting and somewhat novel. The main limitation, as exposed by the reviewers, is that evaluation was done on very limited and small domains. It is not at all clear that this method scales well to non toy domains or that it is possible in fact to get good results with an extension of this method beyond small scale content. There were some concerns about unfair comparisons to classical codes that were optimized for longer sequences (and I share those concerns, though they are somewhat alleviated in the rebuttal).  While the paper presents an interesting line of work, the reviewers did present a number of issues that make it hard to recommend it for acceptance. However, as R1 points out, most of the problems are fixable and I would advise the authors to  take the suggested improvements (especially anything related to modeling longer sequences) and once they are incorporated this will be a much stronger submission.
This paper studies a variational formulation of the loss minimization to study the solution that generalizes the most. An expectation of the loss wrt a Gaussian distribution is minimized to find the mean and variance of the Gaussian distribution. As the variance goes to zero, we recover the original loss, but for a higher value of variance, the loss may be convex. This is used to study the generalizability of the landscape.  Both objective and solutions of the paper are unclear and not communicated well. There is not enough citation to previous work (e.g., Gaussian homotopy exactly considers this problem, and there are papers that study the convexity of the expectation of the loss function). There are no experimental results either to confirm the theoretical finding.  All the reviewers struggle to understand both the problem and solutions discussed in this paper. I believe that the paper could become useful if reviewers  feedback is taken seriously to improve the paper.
This paper introduced a concept called ReLU stability to motivate regularization and enable fast verification. Most of the analysis was presented empirically on two simple datasets and with low performing models. I feel theoretical analysis and more comprehensive and realistic empirical studies would make the paper stronger. In general, the contribution of this paper is original and interesting.  
Pros:   rather novel approach to using optimistic MCTS for exploration with deterministic models   positive rewards on Pitfall  Cons:   lost of domain specific knowledge   deteministic models   lacking clarity   lacking ablations   no rebuttal  I agree with both reviewers that the paper is not good enough to be accepted.
Strengths: The paper presentation was assessed as being of high quality. Experiments were diverse in terms of datasets and tasks.  Weaknesses: Multiple reviewers commented that the paper does not present substantial novelty compared to previous work.  Contention: One reviewer holding out on giving a stronger rating to the paper due to the issue of novelty.   Consensus: Final scores were two 6s one 3.   This work has merit, but the degree of concern over the level of novelty leads to an aggregate rating that is too low to justify acceptance. Authors are encourage to re submit to another venue. 
The paper addresses an interesting problem (learning in the presence of noisy labels) and provides extensive experiments. However, while the experiments in some sense cover a good deal of ground, reviewers raised issues with their quality, especially concerning baselines and depth (in terms of realism of the data). The authors provided many additional experiments during the rebuttal, but the reviewers did not find them sufficiently convincing.
With scores of 5, 4 and 3 the paper is just too far away from the threshold for acceptance.
This paper proposes to estimate the predecessor state dynamics for more sample efficient imitation learning. While backward models have been used in the past in reinforcement learning, the application to imitation learning has not been previously studied. The paper is well written and the results are good, showing clear improvements over GAIL in the presented experiments. The primary weakness of the paper is the lack of comparisons to the baselines suggested by reviewer 1 (a jumpy forward model and a single step predecessor model) to fully evaluate the contribution, and to SAIL and AIRL. Despite these weaknesses, the paper slightly exceeds the bar for acceptance at ICLR. The authors are strongly encouraged to include these comparisons in the final version.
The paper points out a statistical properties of GAN samples which allows their identification as synthetic.  The paper was praised by one reviewer as well written, easy to follow, and addressing an interesting topic. Another added that the authors did an excellent job of "probing into different statistical perspectives", and the fact that they did not confine their investigation to images.  Two reviewers leveraged the criticism that various properties discovered are not surprising given the loss functions and associated metrics as well as the inductive biases of continuous valued generator networks. Tests employed were criticized as ad hoc, and reviewers felt that their generality was limited given their reduced sensitivity on certain modalities. (While Figure 10b is raised by the authors several times in the discussion, and the test statistics of samples are noted to be closer to the test data than to the random baseline, the test falsely rejects the null [p value ~  0.0] for non synthetic test data.)  I would encourage the authors to continue this line of inquiry as it is overall agreed to be an interesting topic of relevance and increasing importance, however based on the criticisms of reviewers and the content of the ensuing discussion I do not recommend acceptance at this time.
This paper presents an interesting and theoretically motivated approach to imposing Lipschitz constraints on functions learned by neural networks. R2 and R3 found the idea interesting, but R1 and R2 both point out several issues with the submitted version, including some problems with the proof probably fixable as well as a number of writing issues. The authors submitted a cleaned up revised version, but upon checking revisions it appears the paper was almost completely re written after the deadline. I do not think reviewers should be expected to comment a second time on such large changes, so I am okay with R1 s decision to not review the updated version. Future reviewers of a more polished version of the paper will be in a better position to assess its merits in detail.
This paper proposes a method for incentivizing exploration in self supervised learning using an inverse model, and then uses the learned inverse model for imitating an expert demonstration. The approach of incentivizing the agent to visit transitions where a learned model performs poorly. This relates to prior work (e.g. [1]), but using an inverse model instead of a forward model. The results are promising on challenging problem domains, and the method is simple. The authors have addressed several of the reviewer concerns throughout the discussion period. However, three primary concerns remain: (A) First and foremost: There has been confusion about the problem setting and the comparisons. I think these confusions have stemmed from the writing in the paper not being sufficiently clear. First, it should be made clear in the plots that the "Demos" comparison is akin to an oracle. Second, the difference between self supervised imitation learning (IL) and traditional IL needs to be spelled out more clearly in the paper. Given that self supervised imitation learning is not a previously established term, the problem statement needs to be clearly and formally described (and without relying heavily on prior papers). Further, the term self supervised imitation learning does not seem to be an appropriate term, since imitation learning from an expert is, by definition, not self supervised, as it involves supervisory information from an expert. Changing this term and clearly defining the problem would likely lead to less confusion about the method and the relevant comparisons. (B) The "Demos" comparison is meant as an upper bound on the performance of this particular approach. However, it is also important to understand what the upper bound is on these problems in general, irrespective of whether or not an inverse model is used. Training a policy with behavior cloning on demonstrations with many (s,a) pairs would be able to better provide such a comparison. (C) Inverse models inherently model the part of the environment that is directly controllable (e.g. the robot arm), and often do not effectively model other aspects of the environment that are only indirectly controllable (e.g. the objects). If the method overcomes this issue, then that should be discussed in the paper. Otherwise, the limitation should be outlined and discussed in more detail, including text that outlines which forms of problems and environments this approach is expected to be able to handle, and which of those it cannot handle.  Generally, this paper is quite borderline, as indicated by the reviewer s scores. After going through the reviews and parts of the paper in detail, I am inclined to recommend reject as I think the above concerns do not outweigh the pros.  One more minor comment is that the paper should consider mentioning the related work by Torabi et al. [2], which considers a similar approach in a slightly different problem setting.  [1] Stadie et al. https://arxiv.org/abs/1507.00814 [2] Torabi et al. IJCAI  18 (https://arxiv.org/abs/1805.01954)
The manuscript describes a novel technique predicting metal fatigue based on EEG measurements. The work is motivated by an application to driving safety. Reviewers and the AC agreed that the main motivation for the proposed work, and perhaps the results, are likely to be of interest to the applied BCI community.   The reviewers and ACs noted weakness in the original submission related to the clarity of the presentation and breadth of empirical evaluation. In particular, only a few baselines were considered. As a result, for the non expert, it is also unclear if the proposed methods are compared against the state of the art. There was also a particular concern that this work may not be a good fit for an ICLR audience. 
This paper presents a dataset and method for training a model to infer, from a visual scene, the program that would generate/describe it. In doing so, it produces abstract disentangled representations of the scene which could be used by agents, models, and other ML methods to reason about the scene.  This is yet another paper where the reviewers disappointingly did not interact. The first round of reviews were mediocre to acceptable. The authors, I think, did a good job of responding to the concerns raised by the reviewers and edited their paper accordingly. Unfortunately, not one of the reviewers took the time to consider author responses.  In light of my reading of the responses and the revisions in the paper, I am leaning towards treating this as a paper where the review process has failed the authors, and recommending acceptance. The paper presents a novel method and dataset, and the experiments are reasonably convincing. The paper has flaws and the authors are advised to carefully take into account the concerns flagged by reviewers—many of which they have responded to—in producing their final manuscript.
This paper proposes a VAE based model which is able to perform musical timbre transfer.   The reviewers generally find the approach well motivated. The idea to perform many to many transfer within a single architecture is found to be promising. However, there have been some unaddressed concerns, as detailed below.   R3 has some methodological concerns  regarding negative transfer and asks for more extended experimental section.  R1 and R2 ask for more interpretable results and, ultimately, a more conclusive study. R2 specifically finds the results to be insufficient.  The authors have agreed with some of the reviewers  feedback but have left most of it unaddressed in a new revision. That could be because some of the recommendations require significant extra work.  Given the above, it seems that this paper needs more work before being accepted in ICLR. 
This paper proposed Residual Gradient Compression as a promising approach to reduce the synchronization cost of gradients in a distributed settings. It provides a useful approach that works for a number of models. The reviewers have a consensus that the quality is below acceptance standard due to practicality of experiments and lack of contribution.
This paper attempts to answer its suggestive title by arguing that this generic lack of invariance in large CNN architectures is due to aliasing introduced during the downsampling stages.  This paper received mixed reviews. Positive aspects include the clarity and exhaustive empirical setups, whereas negative aspects focused on the lack of substance behind some of the claims. Ultimately, the AC took these considerations into account and made his/her own assessment, summarized here.  The main claim of this paper implies the following: modern CNNs are unable to build invariance to small shifts, but somehow are able to learn far more complex invariances involving lighting, pose, texture, etc. This must be empirically verified beyond reasonable doubt, and the AC thinks that the current experimental setup does not achieve this threshold. As mentioned by reviewers and by public comments, the preprocessing pipeline is a key factor that may be confounding the analysis, and this should be better analysed. For example, as mentioned in the reviews below, the shift in the image can be either done by inpainting, cropping, or using a fixed background. The authors claim that there are no qualitative differences between those preprocessing choices, but by inspecting Figures 2B and 8C, the AC notices a severe change in  jaggedness ; in other words, the choice of preprocessing *does* affect the quantitative measures of (un)stability, even though the qualitative assessment (unstable in all setups) is the same. In particular, using non centered crops should be the default setup, since it requires no preprocessing. It is confusing that it appears in the appendix instead of the inpainting version of figure 2b. This is important, since it implies that the analysis is mixing two perturbations: the actual action of the translation group and the choice of preprocessing, and that the latter is by no means negligible. I would suggest the authors to perform the following experiment to disentangle the effect of translation by the effect of preprocessing. Since the translation forms a group, for any shift applied to the image, one can  undo  it by applying the inverse shift. Say one applies a shift to image x of d pixels and obtains x  T(x,+d) as a result (by using whatever border handling procedure). If border effects were negligible, then x  T(x , d) should give us back x, so a good measure of how unstable the network is is to measure the difference in prediction between x,x  and x . If predicting x  is as unstable as predicting x , it follows that the network is actually unstable to the border effect introduced by T.   Given this, the AC recommends rejection at this time, and encourages the authors to resubmit their work by addressing the above point. 
The paper presents a novel perspective on optimizing lists of documents ("slates") in a recommendation setting. The proposed approach builds on progress in variational auto encoders, and proposes an approach that generates slates of the desired quality, conditioned on user responses.   The paper presents an interesting and promising novel idea that is expected to motivate follow up work. Conceptually, the proposed model can learn complex relationships between documents and account for these when generating slates. The paper is clearly written. The empirical results show clear improvements over competitive baselines in synthetic and semi synthetic experiments (real users and clicks, learned user model).  The reviewers and AC also note several potential shortcomings. The reviewers asked for additional baselines that reflect current state of the art approaches, and for comparisons in terms of prediction times. There are also concerns about the model s ability to generalize to (responses on) slates unseen during training, as well as concerns about the realism of the simulated user model in the evaluation. There were questions regarding the presentation, including model details / formalism.  In the rebuttal phase, the authors addressed the above as follows. They added new baselines that reflect sequential document selection (auto regressive MLP and LSTM) and demonstrate that these perform on par with greedy approaches. They provide details on an experiment to test generalization, showing both when the model succeeds and where it fails   which is valuable for understanding the advantages and limitations of the proposed approach. The authors clarified modeling and evaluation choices.   Through the rebuttal and discussion phase, the reviewers reached consensus on a borderline / lean to accept decision. The AC suggests accepting the paper, based on the innovative approach and potential directions for follow up work.  
This paper proposes an approach to regularizing classifiers based on invertible networks using concepts from the information bottleneck theory. Because mutual information is invariant under invertible maps, the regularizer only considers the latent representation produced by the last hidden layer in the network and the network parameters that transform that representation into a classification decision. This leads to a combined ℓ1 regularization on the final weights, W, and ℓ2 regularization on W^{T} F(x), where F(x) is the latent representation produced by the last hidden layer. Experiments on CIFAR 100 image classification show that the proposed regularization can improve test performance. The reviewers liked the theoretical analysis, especially proposition 2.1 and its proof, but even after discussion and revision wanted a more careful empirical comparison to established forms of regularization to establish that the proposed approach has practical merit. The authors are encouraged to continue this line of research, building on the fruitful discussions they had with the reviewers.
The reviewers raised a number of concerns including insufficiently demonstrated benefits of the proposed methodology, lack of explanations, and the lack of thorough and convincing experimental evaluation. The authors’ rebuttal failed to alleviate these concerns fully. I agree with the main concerns raised and, although I also believe that the work can result eventually in a very interesting paper, I cannot suggest it at this stage for presentation at ICLR.
This paper presents a taxonomic study of neural network architectures, focussing on those which seek to map onto different part of the hierarchy of models of computation (DFAs, PDAs, etc). The paper splits between defining the taxonomy and comparing its elements on synthetic and "NLP" tasks (in fact, babi, which is also synthetic). I m a fairly biased assessor of this sort of paper, as I generally like this topical area and think there is a need for more work of this nature in our field. I welcome, and believe the CFP calls for, papers like this ("learning representations of outputs or [structured] states", "theoretical issues in deep learning")). However, despite my personal enthusiasm, the reviews tell a different story.  The scores for this paper are all over the place, and that s after some attempt at harmonisation! I am satisfied that the authors have had a fair shot at defending their paper and that the reviewers have engaged with the discussion process. I m afraid the emerging consensus still seems to be in favour of rejection. Despite my own views, I m not comfortable bumping it up into acceptance territory on the basis of this assessment. Reviewer 1 is the only enthusiastic proponent of the paper, but their statement of support for the paper has done little to sway the others. The arguments by reviewer 3 specifically are quite salient: it is important to seek informative and useful taxonomies of the sort presented in this work, but they must have practical utility. From reading the paper, I share some of this reviewer s concerns: while it is clear to me what use there is the production of studies of the sort presented in this paper, it is not immediately clear what the utility of *this* study is. Would I, practically speaking, be able to make an informed choice as to what model class to attempt for a problem that wouldn t be indistinguishable from common approaches (e.g. "start simple, add complexity"). I am afraid I agree with this reviewer that I would not.  My conclusion is that there is not a strong consensus for accepting the paper. While I wouldn t mind seeing this work presented at the conference, but due to the competitive nature of the paper selection process, I m afraid the line must be drawn somewhere. I do look forward to re reading this paper after the authors have had a chance to improve and expand upon it.
This paper proposes the use of recently propose neural ODEs in a flow based generative model.   As the paper shows, a big advantage of a neural ODE in a generative flow is that an unbiased estimator of the log determinant of the mapping is straightforward to construct. Another advantage, compared to earlier published flows, is that all variables can be updated in parallel, as the method does not require "chopping up" the variables into blocks.  The paper shows significant improvements on several benchmarks, and seems to be a promising venue for further research.  A disadvantage of the method is that the authors were unable to show that the method could produce results that were similar (of better than) the SOTA on the more challenging benchmark of CIFAR 10. Another downside is its computational cost. Since neural ODEs are relatively new, however, these problems might resolved with further refinements to the method. 
The paper combines PAC Bayes bound with network compression to derive a generalization bound for large scale neural nets such as ImageNet. The approach is novel and interesting and  the paper is well written. The authors provided detailed replies and improvements in response to reviewers questions, and all reviewers agree this is a very nice contribution.
All reviewers recommend reject and there is no rebuttal. There is no basis on which to accept the paper.
All three reviewers found this to be an interesting exploration of a reasonable topic—the use of ontologies in word representations—but all three also expressed serious concerns about clarity and none could identify a concrete, sound result that the paper contributes to the field.
The manuscript proposes a novel estimation technique for generative models based on fast nearest neighbors and inspired by maximum likelihood estimation. Overall, reviewers and AC agree that the general problem statement is timely and interesting, and the subject is of interest to the ICLR community  The reviewers and ACs note weakness in the evaluation of the proposed method. In particular, reviewers note that the Parzen based log likelihood estimate is known to be unreliable in high dimensions. This makes a quantitative evaluation of the results challenging, thus other metrics should be evaluated. Reviewers also expressed concerns about the strengths of the baselines compared. Additional concerns are raised with regards to scalability which the authors address in the rebuttal. 
Strengths:  One shot physics based imitation at a scale and with efficiency not seen before. Clear video, paper, and related work.  Weaknesses described include:  the description of a secondary contribution (LFPC)  takes up too much space (R1,4); results are not compelling (R1,4); prior art in graphics and robotics (R2,6); concerns about the potential limitations of the linearization used by LFPC.  The original reviews are negative overall (6,3,4). The authors have posted detailed replies. R1 has posted a followup, standing by their score. We have not heard more from R2 and R3.  The AC has read the paper, watched the video, and read all the reviews. Based on expertise in this area, the AC endorses the author s responses to R1 and R2.  Being able to compare LFPC to more standard behavior cloning is a valuable data point for the community;  there is value in testing simple and efficient models first. The AC identifies the following recent (Nov 2018) paper as being the closest work, which is not identified by the authors or the reviewers. The approach being proposed in the submitted paper demonstrates equal or better scalability, learning efficiency, and motion quality, and includes examples of learned high level behaviors. An elaboration on HL/LL control: the DeepLoco work also learns mocap based LL control with learned HL behaviors.        although with a more dedicated structure.        Physics based motion capture imitation with deep reinforcement learning        https://dl.acm.org/citation.cfm?id 3274506  Overall, the AC recommends this paper to be accepted as a paper of interest to ICLR.  This does partially discount R3 and R1, who may not have worked as directly on these specific problems before.  The AC requests is rating the confidence as "not sure" to flag this for the program committee chairs, in light of the fact that this discounts the R1 and R3 reviews. The AC is quite certain in terms of the technical contributions of the paper. 
This paper adapts language models (LMs), recurrent models trained on large corpus to produce the next word in English, to two commonsense reasoning tasks: the Winograd schema challenge and commonsense knowledge extraction. For the former, the language model score itself is used to obtain substantial gains over existing approaches for this challenging task, while a slightly more involved training procedure adapts the LMs to commonsense extraction. The reviewers appreciated the simplicity of the changes to existing LMs and the impressive results (especially on the WSC).   The reviewers point out the following potential weaknesses: (1) clarity issues in the writing and the presentation, (2) a lack of novelty in the proposed approach, given a number of recent work has shown the ability of language models to perform commonsense reasoning, and (3) critical methodological issues in the evaluation that raise questions about the significance of the results. A lack of response from the authors meant that there was no further discussion needed, and the reviewers encourage the authors to take the feedback to improve further versions of the paper.
The paper proposes a principled modeling framework to train a stochastic auto encoder that is regularized with mutual information maximization. For unsupervised learning, this auto encoder produces a hybrid continuous discrete latent representation. While the authors  response and revision have partially addressed some of the raised concerns on the technical analyses, the experimental evaluations presented in the paper do not appear adequate to justify the advantages of the proposed method over previously proposed ones, and the clarity (in particular, notation) needs further improvement. The proposed framework and techniques are potentially of interest to the machine learning community, but the paper of its current form fells below the acceptance bar. The authors are encouraged to improve the clarify of the paper and provide more convincing experiments (e.g., on high dimensional datasets beyond MNIST).
The reviewers felt that the method was natural and the writing was mostly clear (although could be improved by providing better signposting and fixing typos). However, there was also general agreement that comparison to other methods was weak; one reviewer also points out that the way that the reported numbers compare the methods on different sets of data, which might be an inaccurate measure of performance (this is more minor than the overall issue of lack of comparisons). While the authors provided more comparison experiments during the author response, it is in general the responsibility of authors to have a close to final work at the time of submission.
This paper analyzes existing approaches to program induction from I/O pairs, and demonstrates that naively generating  I/O pairs results in a non uniform sampling of salient variables, leading to poor performance. The paper convincingly shows, via strong evaluation, that uniform sampling of these variables can much result in much better models, both for explicit DSL and implicit, neural models. The reviewers feel the observation is an important one, and the paper does a good job providing sufficiently convincing evidence for it.  The reviewers and AC note the following potential weaknesses: (1) the paper does not propose a new model, but instead a different data generation strategy, somewhat limiting the novelty, (2) Salient variables that need to be uniformly sampled are still user specified, (3) there were a number of notation and clarity issues that make it difficult to understand the details of the approach, and finally, (4) there are concerns with the use of rejection sampling.  The authors provided major revisions that address the clarity issues, including an addition of new proofs, cleaner notation, and removal of unnecessary text. The authors also included additional results, such as KL divergence evaluation to show how uniform the distribution is. The authors also described the need for rejection sampling, especially for Karel dataset, and clarified why the Calculator domain, even though is not "program synthesis", still faces similar challenges. The reviewers agreed that not having a new model is not a chief concern, and that using rejection sampling is a reasonable first step, with more efficient techniques left for others for future work.  Overall, the reviewers agreed that the paper should be accepted. As reviewer 1 said it best, this paper "is a timely contribution and I think it is important for future program synthesis papers to take the results and message here to heart".
This paper proposes an approach for imitation learning from video data. The problem is important and the contribution is timely. The reviewers brought up several concerns regarding the clarity of the paper and the lack of sufficient comparisons. The authors have improved the paper significantly, adding several new comparisons and improving the presentation. However, concerns still remain regarding the description of the method and the presentation of the results. Hence, the reviewers agree that the paper does not meet the bar for publication. 
The paper proposes an algorithm for semi supervised learning, which incorporate biased negative data into the existing PU learning framework.  The reviewers and AC commonly note the critical limitation of practical value of the paper and results are rather straightforward.  AC decided the paper might not be ready to publish as other contributions are not enough to compensate the issue.
The paper suggests a new way to learn a physics prior, in an action free way from raw frames. The idea is to "learn the common rules of physics" in some sense (from purely visual observations) and use that as pre training. The authors made a number of experiments in response to the reviewer concerns, but the submission still fell short of their expectations. In the post rebuttal discussion, the reviewers mentioned that it s not clear how SpatialNet is different from a ConvLSTM, mentioned the writing quality and the fact that the "physics prior" is really quite close to what others call video prediction in other baselines.  
The paper investigates problems that can arise for a certain version of the dual form of the Wasserstein distance, which is proved in Appendix I. While the theoretical analysis seems correct, the significance of the distribution is limited by the fact, that the specific dual form analysed is not commonly used in other works. Furthermore, the assumption that the optimal function is differentiable is often not fulfilled neither. The paper would herefore be significantly strengthen  by making more clear to which methods used in practice the insights carry over.  
The paper presents a new neural program synthesis architecture, SAPS, which seems to produce accuracy improvements in some synthesis tasks. The reviewer consensus, even after discussion with the authors, was that the paper is not acceptable at the conference. Two concerns emerge during discussion, even considering the authors efforts to improve the paper. First, the system seems to have many "moving parts", but there is a lack of rigorous ablation studies to demonstrate which components of the system (or combination thereof) make significant contributions to the results. I agree with this assessment: it is not sufficient to demonstrate increased scores, even if the experimental protocol and clear and sound (more on this later), but there must be some evidence as to why this increase happens, both in the discussion and in the empirical segment of the paper, by conducting a thorough ablation study. Second, all reviewers had issues with proper and fair comparison with prior work, with the consensus being that the model is not adequately compared to convincing benchmarks in the paper.  The results of the paper sound like there is something promising going on, but the need for a clear presentation of what is the driving factor behind any improvement is not only a superficial stylistic requirement, but a key tenet of proper scholarship. This is one front on which the paper fails to make a successful case for the work and methods it describes, and unfortunately is not ready for publication at this time (despite having a cool title).
This paper introduces a method for unsupervised abstractive summarization of reviews.  Strengths:  (1) The direction (developing unsupervised multi document summarization systems) is exciting  (2) There are interesting aspects to the model  Weaknesses:  (1)  The authors are clearly undecided how to position this work: either as introducing a generic document summarization framework or as an approach specific to summarization of reviews. If this is the former, the underlying assumptions, e.g., that the summary looks like a single document in a group is problematic. If this is the latter, then comparison to some more specialized methods are lacking (see comments of R1).  (2) Evaluation, though improved since the first submitted version (when human evaluation was added), is still not great (see R1 / R3). The automatic metrics are not very convincing and do not seem to be very consistent with the results of human eval. I believe that instead or along with human eval, the authors should create human written summaries and evaluate against them. It has been done for extractive multi document summarization and can be done here. Without this, it would be impossible to compare to this submission in the future work.    (3) It is not very clear that generating abstractive summaries of the form proposed in the paper is an effective way to summarize documents.  Basically, a good summary should reflect diversity of the opinions rather than reflect an average / most frequent opinion from tin the review collection.  By generating the summary from a review LM, the authors make sure that there is no redundancy (e.g., alternative views) or contradictions. That s not really what one would want from a summary  (See R3 and also non public discussion with R1)  Overall, I d definitely like to see this work published but my take is that it is not ready yet.  R1 and R2 are relatively negative and generally in agreement.  R3 is very positive. I share excitement about the research direction with R3 but I believe that concerns of R1 and R2 are valid and need to be addressed before the paper gets published.       
The paper investigates an interesting question and points at a promising research direction in relation to whether adversarial examples are distinguishable from natural examples.   A concern raised in the reviews is that the technical contribution of the paper is weak. A main concern with the paper is that the experiments have been conducted only on one simple data set. The authors proposed to add more experiments and improve other points, but a revision didn t follow.   The reviewers consistently rate the paper as ok, but not good enough.   I would encourage the authors to conduct the improvements proposed by the reviewers and the authors themselves. 
The paper proposes an attention mechanism to focus on robust features in the context of adversarial attacks. Reviewers asked for more intuition, more results, and more experiments with different attack/defense models. Authors have added experimental results and provided some intuition of their proposed approach. Overall, reviewers still think the novelty is too thin and recommend rejection. I concur with them.
The paper is extremely difficult to read, even given that both reviewers have very strong math / theoretical background. Although it may potentially include interesting ideas, nothing in the work could not be understood by the ICLR audience.  
AR1 is concerned about whether higher order interactions are modeled explicitly and if pi SGD convergence conditions can be easily satisfied. AR2 is concerned that basic JP has been conceptually discussed in the literature and \pi SGD is not novel because it was realized by Hamilton et al. (2017) and Moore & Neville (2017). However, the authors provide some theoretical analysis for this setting in contrast to prior works. AR1 is also concerned that the effect of higher order information has not been  disentangled  experimentally from order invariance. AR4 is concerned about  poor performance of higher order Janossy pooling compared to k  1 case and asks about the number of hyper parameters. The authors showed a harder task of computing the variance of a sequence of numbers in response.  On balance, despite justified concerns of AR2 about novelty and AR1 about experimental verification, the work appears to tackle an interesting topic.  Reviewers find the problem interesting and see some hope in the proposed solutions. On balance, AC recommends this paper to be accepted at ICLR. The authors are asked to update manuscript to reflect honestly weaknesses as expressed by reviewers, e.g. issue with effects of  higher order information  and  disentangled  from order invariance.
Reviewers mostly recommended to reject. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
While there was some support for the ideas presented, the majority of reviewers did not think this paper was ready for publication at ICLR. In particular the experiments need more work, including the protocol for validation, and attention to overfitting.
The paper introduces a GAN architecture for generating small patches of an image and subsequently combining them. Following the rebuttal and discussion, reviewers still rate the paper as marginally above or below the acceptance threshold.  In response to updates, AnonReviewer3 comments that "ablation experiments do make the paper stronger" but it "still lacks convincing experiments for its main motivating use case: generating outputs at a resolution that won t fit in memory within a single forward pass".  AnonReviewer2 points to the major shortcoming that "throughout the exposition it is never really clear why COCO GAN is a good idea beyond the fact that it somehow works. I was missing a concrete use case where COCO GAN performs much better."  Though authors provide additional experiments and reference high resolution output during the discussion phase, they caution that these results are preliminary and could likely benefit from more time/work devoted to training.  On balance, the AC agrees with the reviewers that the paper contains some interesting ideas, but also believes that experimental validation simply needs more work, and as a result the paper does not meet the bar for acceptance. 
This work provides two contributions: 1) Brain Score, that quantifies how a given network s responses compare to responses from natural systems; 2) CORnet S, an architecture trained to optimize Brain Score, that performs well on Imagenet. As noted by all reviewers, this work is interesting and shows a promising approach to quantifying how brain like an architecture is, with the limitations inherent to the fact that there is a lot about natural visual processing that we don t fully understand. However, the work here starts from the premise that being more similar to current metrics of brain processes is by itself a good thing   without a better understanding of what features of brain processing are responsible for good performance and which are mere by products, this premise is not one that would appeal to most of ICLR audience. In fact, the best performing architectures on imagenet are not the best scoring for Brain Score. Overall, this work is quite intriguing and well presented, but as pointed out by some reviewers, requires a "leap of faith" in matching signatures of brain processes that most of the ICLR audience is unlikely to be willing to take.
The reviewers agree this paper is not good enough for ICLR.
This paper proposes a simple modification of the Adam optimizer, introducing a hyper parameter  p  (with value in the range [0,1/2]) parameterizing the parameter update: theta_new   theta_old + m/v^p where p 1/2 falls back to the standard Adam/Amsgrad optimizer, and p 0 falls back to a variant of SGD with momentum.   The authors motivate the method by pointing out that:    Through the value of  p , one can interpolate between SGD with momentum and Adam/Amsgrad. By choosing a value of  p  smaller than 0.5, one can therefore use perform optimization that is  partially adaptive .    The method shows good empirical performance.   The paper contains an inaccuracy, which we hope will be solved before the final version. The authors argue that the 1/sqrt(v) term in Adam results in a lower learning rate, and the authors argue that the effective learning rate "easily explodes" (section 3) because of this term, and that a "more aggressive" learning rate is more appropriate. This last point is false; the value of 1/sqrt(v) can be smaller or larger than 1 depending on the value of  v , and that a decrease in value of  p  can result in either an increase or decrease in effective learning rate, depending on the value of v. The value of  v  is a function of the scale of loss function, which can really be arbitrary. (In case of very high dimensional predictions, for example, the scale of the loss function is often proportional with the dimensionality of variable to be modeled, which can be arbitrarily large, e.g. in image or video modeling the loss function tends to be of a much larger scale than with classification.)  The authors promise to include a comparison to AdamW [Loshchilov, 2017] that includes tuning of the weight decay parameter. The lack of this experiments makes it more difficult to make a conclusion regarding the performance relative to AdamW. However, the methods offer potentially orthogonal (and combinable) advantages.  [Loshchilov, 2017] https://arxiv.org/pdf/1711.05101.pdf 
The paper introduces a W2GAN method for training GAN by minimizing 2 Wasserstein distance using  by computing an optimal transport (OT) map between distributions. However, the difference of previous works  is not significant or clearly clarified as pointed out some of the reviewers. The advantage of W2GAN over standard WGAN is also superficially explained, and did not supported by strong empirical evidence. 
This paper proposes a general purpose continuous relaxation of the output of the sorting operator. This enables end to end training to enable more efficient stochastic optimization over the combinatorially large space of permutations.  In the submitted versions, two of the reviewers had difficulty in understanding the writing. After the rebuttal and the revised version, one of the reviewers is satisfied. I personally went through the paper and found that it could be tricky to read certain parts of the paper. For example, I am personally very familiar with the Placket Luce model but the writing in Section 2.1 does not do a good job in explaining the model (particularly Eq 1 is not very easy to read, same with Eq. 3 for the key identity used in the paper).   I encourage authors to improve writing and make it a bit more intuitive to read.  Overall, this is a good paper and I recommend to accept it. 
The paper proposes a decision forest based method for outlier detection.  The reviewers and AC note the improvement over the existing method is incremental.  Although  the problem is of significant practical importance, AC decided that the authors should do more works to attract the attention of a broader range of ICLR audience.
This paper introduces fairly complex methods for dealing with OOV words in graphs representing source code, and aims to show that these improve over existing methods. The chief and valid concern raised by the reviewers was that the experiments had been changed so as to not allow proper comparison to prior work, or where comparison can be made. It is essential that a new method such as this be properly evaluated against existing benchmarks, under the same experimental conditions as presented in related literature. It seems that while the method is interesting, the empirical section of this paper needs reworking in order to be suitable for publication.
The reviewers agree the paper is not ready for publication. 
The paper aims to clean data samples with label noise in the training procedure.   The reviewers and AC note the following potential weaknesses: (1) the assumption of uniform noise, which is not the case in practice, (2) marginal gains under real world datasets and (3) highly empirical and ad hoc approach.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more significant works to publish the work.  
This paper proposes an optimization algorithm based on  weak contraction mapping . The paper is written poorly without clear definitions and mathematical rigor. Reviewers doubt both the correctness and the usefulness of the proposed method.  I strongly suggest authors to rewrite the paper addressing all the reviews before submitting to a different venue. 
The reviewers are in general impressed by the results and like the idea but they also express some uncertainty about how the proposed actually is set up. The authors have made a good attempt to address the reviewers  concerns. 
Strengths: Paper uses an efficient inference procedure cutting inference time on intermediate frames by 53%, & yields better accuracy and IOU compared to the one recent closely related work.  The ablation study seems sufficient and well designed. The paper presents two feature propagation strategies and three feature fusion methods. The experiments compare these different settings, and show that interpolation BMV is indeed a better feature propagation.  Weaknesses: Reviewers believed the work to be of limited novelty. The algorithm is close to the optical flow based models Shelhamer et al. (2016) and Zhu et al. (2017). Reviewer asserts that the main difference is that the optical flow is replaced with BMV, which is a byproduct of modern cameras.  R3 felt that there was Insufficient experimental comparison with other baselines and that technical details were not clear enough.  Contention: Authors assert that Shelhamer et al. (2016) does not use optical flow, and instead simply copies features from frame to frame (and schedules this copying). Zhu et al. (2017) then proposes an improvement to this scheme, forward feature warping with optical flow. In general, both these techniques fail to achieve speedups beyond small multiples of the baseline (< 3x), without impacting accuracy.  Consensus: It was disappointing that some of the reviewers did not engage after the author review (perhaps initial impressions were just too low). However, after the author rebuttal R1 did respond and held to the position that the work should not be accepted, justified by the assertion that other modern architectures that are lighter weight and are able to produce fast predictions.  
 The paper investigates mixed integer linear programming methods for neural net robustness verification in presence of adversarial attckas. The paper addresses and important problem, is well written, presents a novel approach and demonstrates empirical improvements; all reviewers agree that this is a solid contribution to the field.
The reviewers raised a number of major concerns including the incremental novelty of the proposed and a poor readability of the presented materials (lack of sufficient explanations and discussions). The authors decided to withdraw the paper.
The paper combines the ideas of VAT and Bad GAN, replacing the fake samples in Bad GAN objective with VAT generated samples. The motivation behind using the K+1 SSL framework with VAT examples remains unclear, particularly in the light of Prop. 2 which shows smoothness of classifier around the unlabeled examples is enough (which VAT already encourages). R2 and R3 have raised the point of limited insight and lack of motivation behind combining VAT and Bad GAN objectives in this way. R2 and R3 are also concerned about the empirical results which show only marginal improvements over VAT/BadGAN in most settings.   AC feels that the idea of the paper is interesting but agrees with R2/R3 that the proposed objective is not motivated well enough (what is the precise advantage of using K+1 SSL formulation with VAT examples?). The paper really falls on the borderline and could be improved if this point is addressed convincingly. 
A focused contribution that is clearly presented. That being said, the task of low resource named entity recognition is fairly narrow and it is hard to tell how significant the empirical results are. The paper could be much stronger if it evaluated on a second task (and third task). Right now it is unclear whether the technique would generalize to other tasks.
The paper proposes a method to perform manifold regularization for semi supervised learning using GANs. Although the SSL results in the paper are competitive with existing methods, R1 and R3 are concerned about the novelty of the work in the light of recent manifold regularization SSL papers with GANs, a point that the AC agrees with. Given the borderline reviews and limited novelty of the core method, the paper just falls short of the acceptance threshold for ICLR. 
The paper propose a new metric for the evaluation of generative models, which they call CrossLID and which assesses the local intrinsic dimensionality (LID) of input data with respect to neighborhoods within generated samples, i.e. which is based on nearest neighbor distances between samples from the real data distribution and the generator. The paper is clearly written and provides an extensive experimental analysis, that shows that LID is an interesting metric to use in addition to exciting metrics as FID, at least for the case of not to complex image distributions The paper would  be streghten by showing that the metric can also be applied in those more complex settings.  
The paper proposes to replace dynamic routing in Capsule networks with a trainable layer that produces routing coefficients. The goal is to improve their scalability. This is promising as a research direction but reviewers have raised several concerns about unclear contributions and lack of a thorough evaluation of the approach. There is also a recent relevant work pointed out by Reviewer 1 that should be discussed. Given these concerns, the paper is not suitable for publication in its current form, however I encourage the authors to use reviewers  comments for improving the paper and resubmit in next venues.
The paper argues for a GAN evaluation metric that needs sufficiently large number of generated samples to evaluate. Authors propose a metric based on existing set of divergences computed with neural net representations. R2 and R3 appreciate the motivation behind the proposed method and the discussion in the paper to that end. The proposed NND based metric has some limitations as pointed out by R2/R3 and also acknowledged by the authors   being biased towards GANs learned with the same NND metric; challenge in choosing the capacity of the metric neural network; being computationally expensive, etc. However, these points are discussed well in the paper, and R2 and R3 are in favor of accepting the paper (with R3 bumping their score up after the author response).  R1 s main concern is the lack of rigorous theoretical analysis of the proposed metric, which the AC agrees with, but is willing to overlook, given that it is nontrivial and most existing evaluation metrics in the literature also lack this.  Overall, this is a borderline paper but falling on the accept side according to the AC. 
This work proposes a "teaching to teach" (T2T) method to incorporate structured prior knowledge into the teaching model for machine learning tasks. This is an interesting and timely topic. Unfortunately, among many other issues, this paper is fairly poorly writing and can benefit from a significant rewriting. The authors did not provide a rebuttal and hence we recommend a rejection.    
This paper combines two different types of existing optimization methods, CEM/CMA ES and DDPG/TD3, for policy optimization. The approach resembles ERL but demonstrates good better performance on a variety of continuous control benchmarks.  Although I feel the novelty of the paper is limited, the provided promising results may justify the acceptance of the paper.
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The paper presents an adversarial learning framework for active visual tracking, a tracking setup where the tracker has camera control in order to follow a target object. The paper builds upon Luo et al. 2018 and proposes jointly learning  tracker and target policies (as opposed to tracker policy alone). This automatically creates a curriculum of target trajectory difficulty, as opposed to the engineer designing the target trajectories. The paper further proposes a method for preventing the target to fast outperform the tracker and thus cause his policy to plateau. Experiments presented justify the problem formulation and design choices, and outperform Luo et al. . The task considered is  very important, active surveillance with drones is just one sue case.  A downside of the paper is that certain sentences have English mistakes, such as this one:  "The authors learn a policy that maps raw pixel observation to control signal straightly with a Conv LSTM network. Not only can it save the effort in tuning an extra camera controller, but also does it outperform the..." However, overall the manuscript is well written, well structured, and easy to follow. The authors are encouraged to correct any remaining English mistakes in the manuscript. 
This work examines how to deal with multiple classes. Unfortunately, as reviewers note, it fails to adequately ground its approach in previous work and show how the architecture relates to the considerable research that has examined the question beforehand.
This paper studies an interesting phenomenon related to adversarial training   that adversarial robustness is quite sensitive to semantically lossless shifts in input data distribution.   Strengths   Characterizes a previously unobserved phenomenon in adversarial training, which is quite relevant to ongoing research in the area.   Interesting and novel theoretical analysis that motivates the relationship between adversarial robustness and the shape of input distribution.  Weaknesses   Reviewers pointed out some shortcomings in experiments, and analysis of causes and remedies to adversarial robustness. The authors agree that given the current state of understanding, these are hard questions to pose good answers for. The result and observations by themselves are interesting and useful for the community.  The weakness that the paper does not propose a solution for the observed phenomenon remains, but all reviewers agree that the observation in itself is interesting. Therefore, I recommend that the paper be accepted. 