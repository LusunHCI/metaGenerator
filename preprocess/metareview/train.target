The paper has been actively discussed, both during and after the rebuttal phase. I enjoyed, and I am thankful for, the active communication that took place between the authors and the reviewers.  On the one hand, the reviewers agreed on several pros of the paper, e.g., * Clear, well presented manuscript * The presentation of practically relevant setting * A work that fosters reproducible research (both BO data and algorithms are made available) * Careful experiments  On the other hand, several important weaknesses were also outlined, e.g., * _Novelty_: While the authors claim they “introduce a practically relevant and fundamentally novel research problem”, existing commercial HPO solutions already mention, and propose solutions for, the very same problem, e.g., [AWS](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic model tuning warm start.html) (section “Types of Warm Start Tuning Jobs”) and [Google cloud](https://cloud.google.com/blog/products/gcp/hyperparameter tuning on google cloud platform is now faster and smarter) (section “Learning from previous trials”). The reviewers all agreed on the fact that this down weights the novelty aspect (claimed many times in the rebuttal and the manuscript): The paper formalizes an already existing framework rather than introducing it. * In the light of the weakened "novelty" contribution (see above), the reviewers regretted the absence of a novel transfer method _tailored to HT AA_, which would have certainly strengthened the submission. * _“Dynamic range” of the benchmark_: It is difficult to evaluate the capacity of the benchmark to discriminate between different approaches (e.g., see new Fig. 3 showing the violin plot with all three methods for transfer, as suggested by Reviewer 1: the improvements over "best first" seem marginal at best). To better understand the benchmark, it would be nice to illustrate its “dynamic range” by exhibiting a more powerful method that would substantially improve over “best first”.  As illustrated by its scores, the paper is extremely borderline. Given the mixed perspectives of pros and cons, we decided with the reviewers to recommend the rejection of the paper. 
This paper studies the problem of multi agent meta learning. It can be viewed as extending Al Shedivat et al. (2018) by incorporating the dynamics of other agents. The reviewers praised clear writing and theory. There were two main concerns. The first concern is the novelty when compared to Al Shedivat et al. (2018). The second concern are experiments, which could be more ambitious and are not always clearly described.  The reviews of this paper were borderline and this was not enough to get accepted.
All reviewers recommended rejection after considering the rebuttal from the authors. The main weaknesses of the submission include poorly motivated claims and designs, and insufficient experimental comparisons. The AC did not find sufficient grounds to overturn the reviewers  consensus recommendation.   
This paper presents a benchmarking suite, primarily targeting the domain of evolutionary style optimization algorithms, and an effective heuristic algorithm selection procedure ABBO.  The reviewers seemed quite split in their reviews with significant variance, particularly with one outlier review (9) lifting up the average.  They all felt that there was significant value in the work presented and that the benchmark could be useful for designing and evaluating new methods.  However, there were concerns regarding details about the contributions (e.g. a detailed description of ABBO and which contributions to the suite were novel vs obtained from other benchmarks), the relevance of this work to the ICLR community, and choice of algorithms presented (i.e. not SOTA).    In general, this seems like a useful contribution for the evolutionary algorithm community but this paper seems off topic from the conference.  Certainly optimization is important and of interest to the community.  However, there is no machine learning component to the technical contribution of this paper, and it ignores many of the contributions to black box optimization within this community (see e.g. the citations from AnonReviewer1, and the literature on surrogate based black box optimization   i.e. Bayesian optimization).  The RL optimization problems are somewhat relevant, but AnonReviewer1 raises concerns about the reporting of those results and the representation of the current literature.  There is an algorithm proposed in this work, but it s largely heuristic and no comparison is given to state of the art portfolio optimization algorithms from the machine learning community (e.g. P3BO from Angermueller et al., ICML 2019).  A venue such as GECCO seems much more well suited to this work.
The paper proposes a meta gradient boosting framework to tackle the model agnostic meta learning problem. The idea is to use a base learn that learns shared information across tasks, and gradient boosted modules to capture task specific modules. The experiments show that the proposed meta gradient boosting framework (with 5 gradient boosting modules) achieves better or competitive results compared to the baselines. However, there were several issues that the author feedback did not addressed properly. For instance, R2 were not satisfied by discussing briefly the suggested baselines without adding the comparison, or R1 pointed out that the claim “the learning and updating strategy proposed in the method ensured a weak base learner” because clear separable datasets could convergence quickly and weak is not anymore applicable. Besides these two specific concerns, the reviewers expected a large revision of the paper due to several cons about the paper. All reviewers agreed a mayor revision is needed before acceptance. Therefore I recommend rejection.
The paper studies the robustness of binary neural networks (BNNS), showing how quantized models suffer from gradient vanishing. To solve this issue, the authors propose temperature scaling approaches that can overcome this masking, achieving near perfect perfect success in crafting adversarial inputs for these models. The problem is interesting and important. However, the major concerns are that the technical novelty is limited raised by two Reviewers, small improvements for linear loss functions. The most related work is not compared in the experiment.  
The paper proposes a method for the interesting task of dialog summarisation which is slowly getting attention from the research community. In particular, they propose a method which first generates a summary draft and then a final draft.  Pros: 1) The paper is well written 2) Addresses an interesting problem 3) SOTA results  Cons: 1) Lack of novelty 2) No quantitative analysis of the summary draft though it is as an important part of the proposed solution 3) Human evaluations are not adequate (the authors have said they will expand on this but clear details are not provided) 4) The BART model seems to have some advantage as it is pre trained on XSUM data whereas some of the other models are not (the authors haven t clarified this sufficiently in the rebuttal)  Overall, the reviewers were not completely happy with the work and there was not clear champion. 
The covered topic is timely and of potential impact for many application domains, such as drug design. The paper is well written and presentation is clear. The proposed approach seems to have some degree of originality. Experimental results seem to be generally good, and in the rebuttal the authors have provided further experimental support to their main claim.  There are however some issues that have not been solved by the author’s rebuttal. I think two of them are the most important and related:   i) significance of contribution: although the authors have tried in the rebuttal to explain how the proposed approach differs from related papers, it seems that there are still doubts about the amount of innovation introduced by the paper. This issue could have been mitigated by SOTA experimental results in presence of a proper model selection, that, however does not seem to be the case here (see next point);  ii) model selection: the authors did not clearly explain the model selection procedure in the rebuttal. This is an important issue since it is often easy to get good results by picking the best run a posteriori. Unfortunately in the literature there are highly cited papers where model selection is not performed in a proper way and reviewers very often reject papers just looking at numbers without looking at how the numbers were obtained. So, I believe it is important to accept only papers where model selection is properly done and properly explained, so to allow for reproducibility of experiments. 
This paper introduces an important and interesting problem and a potentially interesting approach. Unfortunately, the reviewers agree that the current version isn t appropriate for ICLR in its current form. However, hopefully the feedback will be useful for the authors in revising and resubmitting this paper to another venue.
This paper proposes a framework to train a discriminative model robust against (i) label noise, (ii) out of distribution input, and (iii) input corruption. To tackle these problems, a complex model is proposed that combines several existing models including InfoNCE style contrastive learning, prototypical contrastive loss, Mixup, and reconstruction loss. Noisy training labels are cleaned using a temporally consistent label smoothing mechanism, combined with a curriculum learning algorithm.   Originally, the reviewers raised concerns regarding the limited ablation experiments and the lack of studies on real world noisy labels. The additional experiments in the revised version addressed some of these concerns. Thus, the reviewers increased their rating slightly.  However, the reviewers in the discussion phase agree that the proposed method has a limited novelty, is complex, and involves many moving parts that require a careful design and hyperparameter tuning, and they do not recommend accepting the submission. I agree with the reviewers and recommend rejection. 
This paper explores the effects of padding in convnets used for various visual recognition tasks (classification, segmentation). This is an important and relevant design choice that is often overlooked, as noted by reviewers. However, I share the concerns of AR2 & AR4 with the evaluation. The design of the ResNet variant used for the "No Pad" baseline seems potentially fatally flawed: the bilinear upsampling used to match the feature map sizes for the residual addition results in a misalignment of the inputs with the outputs, which potentially explains the performance degradations seen throughout most experiments, as opposed to (or perhaps in addition to) the lack of positional information and border effects resulting from the NoPad scheme that is claimed as the reason for the performance drop. It is true that how to do this is an open question, as the authors argue in their added Appendix A.1, but I nonetheless share the reviewers  skepticism that the chosen approach will result in a meaningful comparison of the effect of padding and border effects. In fact, the results in Table 5 on texture recognition seem to suggest that "No Pad" approach may indeed be flawed, given that "No Pad" performs the worst, while "Reflect" padding performs best, even though both methods share the property that the network should have difficulty inferring positional information. Given the reliance on this dubious baseline throughout the results, I can t recommend the submission for acceptance in its current form. However, I still appreciate the direction of this work and hope the authors will consider resubmitting it after revising it in order to make the evaluation more convincing based on the reviewers  feedback.
The paper tackles the problem of mitigating the effect of model discrepancies between the learning and deployment environments. In particular, the author focus on the worst case possible performance. The paper has both an empirical and theoretical flavor. The algorithm they derived is backed by theoretical guarantees. There exists a gap between the theory presented and the final practical algorithm, which generated some elements of concern from the reviewers. Some of these issues (choice and sensitivity of the Lipschitz constant, in what cases can we make that assumption, choice of p_w, discrepancy between the theoretical proposal and the practical algorithm) are well addressed in the rebuttal. However, after careful examination of the reviews, the meta reviewer is still not convinced that the paper meets the minimum requirements for acceptance, as many of the reviewers  initial concerns still remain.
Despite the fact that some of the reviewers found the idea interesting, none of them believe that the paper is ready to be published at this stage. For example, better comparison with existing/similar work, and more solid argument on why the idea is better than alternatives are mentioned. All considered, unfortunately I cannot recommend acceptance of this paper in its current form. I encourage the authors to consider these comments and revise their paper accordingly.
This paper addresses the problem of estimating a “birds eyed view” overhead semantic layout estimate of a scene given an input pair of stereo images of the scene. The authors present an end to end trainable deep network that fuses features derived from the stereo images and projects these features into an overhead coordinate frame which is passed through a U Net style model to generate the final top view semantic segmentation map. The model is trained in a fully supervised manner. Experiments are performed on the CARLA and KITTI datasets.   While R2 was positive, they still had some concerns after reading the rebuttal and the other reviews. Specifically, they were not convinced about the value of the IPM module. This concern was also shared by R4, especially in light of the relationship to Roddick et al. BMVC 2019. R1 had concerns about the experiments, specifically the quantitative comparisons to MonoLayout. The authors addressed these comments, but it is still not clear if the differences can be attributed to the number of classes, how they are weighted, or the training split used? R3 had questions about the utility of BEV predictions in general. However, as stated by R2, there is a lot of value in approaching the problem in this way.   In conclusion, while there were some positive comments from the reviewers, there were also several significant concerns. With no reviewer willing to champion the paper, there is not enough support to justify accepting the paper in its current form.  
In this paper, the authors draw connections between probabilistic graphical models (specifically LWF chain graphs) and neural network models. There was general agreement amongst the reviewers that this is an interesting topic that merits further study, and would be of interest to the ICLR audience. At the same time, all of the reviewers have read the author response and there is a consensus that the novelty and significance of this work is limited. The connections between CGs and NNs are somewhat standard and well known, and the significance of the results has not been convincingly demonstrated. 
The authors study "robustness curves" which are plots of the robust error versus the radius used in the corresponding l_p ball threat model.  Pro: I completely agree with the authors that the current evaluation purely based on evaluation for a single radius is insufficient and one should report the complete curve.   Con: The authors are overclaiming that they have come up with robustness curves. Very early papers e.g. even in the adversarial training paper of Madry there are plots of robust accuracy versus chosen threshold. Moreover, I agree with one of the reviewers that using PGD for the purpose of a robustness curve is inaccurate and in particular inefficient as several attacks for different radii have to be done. There have been several attacks developed which aim to find the adversarial sample with minimum norm and thus compute the robustness curve in one run.  The additional insights e.g. intersection of robustness curves are partially to be expected and I don t find them sufficient to move the paper over the bar for ICLR.  As these insights are additionally  only shown for relatively small models which seem far away from the state of the art, it is unclear if they generalize. However, I encourage to follow some of the reviewer s suggestions to improve the paper.
The authors provide four rigorous upper bounds on the operator norm of the linear transformation associated with a 2D convolutional layer of a neural network.  One of these is a heuristic proposed in earlier work by Miyato et al, and widely used, so, among other things, their result provides theoretical context for that method which will be of broad interest.  All four of their bounds can be efficiently computed and have easily computed gradients, so they propose using the minimum of the four bounds for various purposes.  Since, for standard architectures, the Lipschitz constant of a network can be bounded above by the product of the operator norms of its layers, there are a variety of applications of differentiable bounds on these operator norms.  They show that their new bound is sometimes much tighter than the bound of Miyato et al, and can be computed much more efficiently than two known methods for exact computation.  The paper is written well, which will facilitate future work building on this work.  The analysis builds on earlier work, but insight was required to obtain the new results;  the fundamental novelty of the mathematical development was confirmed by an expert reviewer.  While they experimentally compared the accuracy of their approximations to those of the method of Miyato, et al, the case for the practical utility of their method would have been stronger if they had shown that their regularizer led to better results for some tasks.  However, I believe that the paper should be accepted purely on the basis of its theoretical contribution, which enhances our understanding of this important topic, and, even if it cannot be directly applied, seems like to inspire practically useful methods in the future. 
There are some interesting ideas in this paper, but I agree with reviewers that without a comparison to existing work, it is hard to place this work in its proper context. The authors make several arguments in dismissing the need for side by side comparisons, but I do not find these arguments convincing.  * First, the authors argue that there are no suitable benchmarks for them to compare, and that in particular SyGuS benchmarks would not be suitable because they are dealing with a different problem. I disagree. There are 2 tracks in SyGuS specifically for programming by example problems, one for string manipulations and one for bit vector programs. I think the string manipulation problems would be a good match for this technique.  * The authors also argue that their technique is so much more general than prior techniques that a side by side comparison would be unfair. However, their most complex benchmark, sorting, has been somewhat of a standard benchmark in the program synthesis community for about a decade now. And while a lot of recent synthesis work has focused on domain specific languages, many systems starting with Sketch and continuing with Myrth and Synquid were turning complete. Turing completeness can make a big difference if you are trying to synthesize verified code, but in the context of programming by example, turning completeness does not really present any fundamental challenges.  I am willing to believe that this technique is more scalable than existing techniques, so that while existing techniques may do better than this technique when synthesizing for small languages, this technique would surpass them when applied to a bigger language. But if that s the argument that the authors want to make I would like to see some evidence, and ideally some quantitative data as to how big a language would have to be before this technique wins out.
The authors propose an approach to mitigate mode collapse phenomena in GANs. Motivated by the intuition that mode collapse stems from catastrophic forgetting of the discriminator, the authors propose a solution inspired by recent research in continual learning and dynamically add new discriminators during training. The authors empirically demonstrate that combining the proposed scheme with existing GANs leads to improvements in terms of Inception Score and FID.   This paper is trying to address a significant problem for the generative modeling community. The reviewers appreciated the clarity of writing, the empirical results, and the idea of using normalising flows for an elegant visualisation. However, the reviewers have pointed out several major issues which were not adequately addressed by the authors. The first one is the clear failure to position the work with respect to related work. In fact, the main idea related to catastrophic forgetting was already established in [1,2] and subsequent works. Secondly, the improvements over the baseline come at a significant computational overhead which is extremely challenging and impractical. Finally, given the trend that large scale models achieve significantly better results in practice, the proposed approach is not only impractical, but potentially extremely wasteful. Given very limited novelty, failure to position the work, and impracticality of the proposed solution, I will recommend rejection.  [1] https://arxiv.org/abs/1810.11598  [2] https://arxiv.org/abs/1911.06997
Thanks for your submission to ICLR.  This paper presents an extension to Deep Hashing Networks that utilizes angular similarity, and show improved results using the proposed method.  The reviewers were somewhat mixed on this paper, with two of three reviewers on the negative side.  Some reviewers appreciated that the paper was easy to follow and well written, though one reviewer felt that the paper s writing and presentation could improve.  A big concern about the paper expressed by multiple reviewers was that the paper was incremental, in that the main architectural difference seemed to be a change in loss function over existing work.  Unfortunately, the reviewers were fairly unresponsive to attempts to get them to respond to the rebuttals offered by the authors.    Ultimately, I took a look at the paper and found it to be borderline.  I do think the contribution is a bit limited, particularly as it is in an area which has seen many papers over the years (and thus has a high bar for new work).  However, with some additional work this paper could definitely be acceptable.  I think it could use an additional round of editing and review, and I d encourage the authors to submit this paper to another venue.
This paper addresses the real world problem of semi supervised learning where the distribution from which the labeled examples are drawn is different from the distribution from which the unlabeled examples are drawn.  The task is motivated by structure activity prediction for drug design (quantitative structure activity prediction, or QSAR).  Examples represent molecules, and we wish to predict a real valued measure of binding affinity.  Exactly the general problem of data skew arose with exactly this task for example in one of the KDD Cup 2001 tasks.  While the authors here mention that labeled data may be focused more on active molecules (those with a high continuous valued response), in the KDD Cup 200`1 data the reverse was true, and the unlabeled test data were skewed to higher activity level.  I say all this to agree with the authors about the real world nature of the problem they address.  Also, some reviewers felt more empirical evaluation was needed, so that may be an additional data set for the authors to consider using.  Reviewer concerns including that the approach was simplistic, the empirical results were insufficient, and the claims were oversold.  The author replies and revisions, and the discussion, moved the reviews to be more favorable but still not strong enough to justify acceptance yet.  Nevertheless, the consensus is that the paper addresses an important problem and the revisions are headed in the right direction to make a strong future paper, and that the authors should be encouraged to continue this work.
This promising work proves that the proposed contrastive learning approach to representation learning can recover the underlying topic posterior information given standard topic modelling assumptions. The work provides detailed proof and detailed experiments. The analysis is interesting and yields interesting insights. However, the experimental results are somewhat weak by lacking comparison with more recent document representation work.  Pros:   Good detailed proofs and experiments.   Interesting idea of using topic modelling to understand representation learning.  Cons:   The description of DirectNCE is somewhat hidden and could be better introduced in the paper.   Experimental baselines are weak lacking a comparison to recent document representation work such as Arora et al. 2019.   Stronger classification baselines could be incorporated.
This paper proposed a conditional graph generative model closely following the unconditional generative model NetGAN and extending it by adding conditioning on extra information available for graph generation (“shadow” node attributes as the authors call it).  Overall this is an extension over NetGAN and gives this class of models the ability to utilize extra information when generating graphs.  However, all reviewers lean toward the reject side and the concerns raised by the reviewers range from limited novelty to the could be improved writing.  I want to highlight two issues.  The first is, is the GAN formulation really necessary?  Could you directly learn using a likelihood objective, e.g. treat the sampled random walks as sequence data, and learn an autoregressive sequence generative model on these data (as a language model for example)?  You could then also generate sample walks from your model.  The benefit is training would be much simpler and more stable.  I believe this could also be related to R2’s concern that the model might be learning the distribution from s alone, as that is entirely possible and potentially easier (not with the GAN formulation proposed here, though).  The second issue is that I encourage the authors to dig deeper into explainability.  The notion of explainability is obviously important but it is not very well defined.  Does controllability equal explainability?  Is a visualization sufficient to demonstrate explainability (and further, is this sufficient to show this method offers more explainability than alternatives)?  Also intuitively random walk models are not very explainable due to the large amount of randomness in the generation process, compared to e.g. autoregressive graph generative models that generate graphs part by part (related to R1’s concern).  If explainability is the main motivation, maybe other alternatives can be more competitive.  Overall I would recommend a reject at this time but encourage the authors to improve the paper further.
The idea presented in the paper is interesting and has caught the attention of the reviewers. However there seem to be only a tepid support for acceptance with a reviewer championing rejection.  There is little novelty in the approach but empirical validation shows results that consistently improve over selected baselines. I am afraid that more evaluations would be needed at this stage to consider this work for acceptance.
The paper proposes an interesting step in the direction of neuro symbolic reasoning. While there is no consensus among reviewers about the key novelty of the method, all acknowledge the interest of the direction. All of them also recognize that the submission improved greatly during the discussion phase: clarification of motivations, of experimental settings and results, of discussion with previous work.  However, despite those improvements, the submission is not yet ready for publication at ICLR. We encourage the authors to use the very detailed reviews and comments to improve the work. In particular, we encourage them to pay attention at three aspects:  1/ Comparison with large language models: the discussion wrt T5 is important. A key motivation for the proposed model is that it is bringing information and elements for QA (or other reasoning tasks) that purely scaling up language models can not bring. Or maybe they can bring the same kind of improvement but at a much lower computational cost. In any case, this is a very important point to justify the interest of such approach, and neuro symbolic reasoning overall, empirically.  2/ Using GPT2 (or equivalent): the discussion on using GPT 2 for generating new facts is key too. It is essential to bring this description from appendix to the core of the paper. But more discussion are expected.  For instance, what if GPT 2 generates facts that are false and lead to answering and justifying a wrong answer? In other words, how does it impact the integrity of the contextualized KG? This is an essential point that needs to be worked on more thoroughly.   3/ Overall there have been a lot of discussion to improve the motivations and the contributions. But they are not reflected in the paper necessarily. Following R2, we encourage the authors to "refocus the existing version (e.g., from vague discussion about neural symbolic models towards establishing solid comparison to the most related previous work in various sections of the submission)"    
The paper proposes a new adaptive optimization algorithm which is claimed to have better convergence properties and lower susceptibility to gradient variance. Reviewers found the idea of normalizing on the fly to be interesting, but raised some important concerns. Although similar to AdaGrad, Expectigrad has a very important differentiation due to division by $n_t$. Assuming $\beta 0$ in my opinion is also ok and many papers assume this for analysis. Even after accounting for these two facts, during discussions the reviewers considered the work to be incremental and a more thorough evaluation is needed to determine the benefits of algorithm. Specifically, please compare to important and relevant baselines (like AdamNC and Yogi), because sometimes it felt like baselines were picked and dropped randomly. The empirical improvement provided by Expectigrad compared to SOTA is not clear (both on synthetic problems from Reddi et al and real problems). Thus, unfortunately, I cannot recommend an acceptance of the paper in the current form. However, I would strongly encourage authors to resubmit after improving according to reviewer suggestions.   Some other minor points that came up during discussion are:  1. choice of hyperparameters was not clear to reviewers, e.g. different optimizer may behave very differently for same set of hyperparameters, so it would not be fair to compare them as is. 2. gradients would never be exactly zero in deep networks, so is current definition of $n_t$ good enough?
I thank the authors and reviewers for the lively discussions about the paper. All reviewers indicated that the work has merits and novelty however there were concerns about showing the benefits of the proposed method experimentally specially on malware applications. Given all, I think the paper needs a bit more work to be accepted.     AC
This paper introduced a new ODE integration scheme that allows constant memory gradient computation.  I was concerned that the low order of convergence of this method would make it impractical, but the authors performed extensive experiments and got impressive results.  Overall the paper addresses one of the main practical difficulties with large neural ODE models.  The authors satisfactorily addressed the reviewers  concerns in the discussion.
Meta learning for offline RL is an understudied topic with lots of potential impact in the research community. This paper takes the first stab against that challenging problem by proposing a solution similar based on PEARL and distance metric learning. The results look good and it seems like the authors have addressed some of the concerns raised by the reviewers. As a result, I suggest to accept this paper.  However, this paper still has some shortcomings as reviewers suggested more baselines with more experiments on standardized benchmarks such as D4RL or RL Unplugged could make the paper stronger.
This paper discusses how one can equip reinforcement learning agents with an intrinsic reward function that helps identifying factors of variation within a family of MDPs, effectively allowing agents to do experiments in the environment. This is interpreted as causal factors that control important aspects of the environment dynamics.  Although this is a very relevant topic and there was extensive discussion during the discussion phase, with reviewers acknowledging that the final version of the submitted manuscript substantially improved over the original submission, most reviewers still recommend the rejection of the paper. This is mainly due to the assessment that there are still several unclear technical aspects related to the paper. Shortly, the reviewers felt that the paper had important clarity issues, that the claims being made were imprecise, and that there was a dearth of details about the empirical results, making them not fully convincing.  I strongly recommend the authors to take the reviewers suggestions into consideration to have a much stronger submission to future venues. 
The authors present a study where they investigate whether meta learning techniques leverage the underlying task distribution. To do so, the authors come up with two conditions, in the first they generate tasks using a grammar and in the second condition, which is the null condition essentially, the tasks have the same statistical properties as the compositional task but they are not derived from a simple grammar. The authors find that while humans are better in the compositional condition, models are better in the null condition.  All reviewers have been positive with this work, but some concerns were raised regarding clarity around the use of some terms, such as compositionality. The rebuttal period has been very productive and the reviewers have acknowledged the improvements on the manuscript.   All in all, I think this is a good study to appear on ICLR and I believe researchers would benefit from the design of the study that will perhaps open new opportunities around careful evaluation of meta learning agents.
The paper presents a decomposition of the value function in the context of CCDA.  Most reviewers find this paper clear and well written, although one reviewer suggests to change the paper structure.  The method presented in this paper is simple and well justified by a theoretical section. Experiments on several domains, including Starcraft 2 micro management tasks, are supporting the claims of that section. After some reviewers pointed out that the tabular setup is not useful in practice, the authors have extended the empirical and theoretical results to a more general setup.  Some reviewers point out that some theoretical results may not be directly related to the experimental findings. In particular, reviewer 3 does not support a central claim of the paper, and find that CDM is misleading and not provably representing the core problem. In general, reviewer 3 does not support acceptance of this paper, but I still believe this paper should be accepted based on the other reviews (clearly in favour of acceptance). I hope that the authors and reviewer 3 will be able to further discuss and reach understanding, which hopefully should lead to fruitful results.
The paper presents a generic way to add group sparsity based regularizers to a family of adaptive optimizers leading to generalizations of many popular optimizers ADAM, ADAGRAD etc  to their group versions. Overall the reviewers appreciated the algorithmic contribution and its genericness in terms of application to most known adaptive optimizers. While the paper s revision during the rebuttal phase satisfied some reviewer concerns regarding the experimental baselines and the precise experimental methodology, reviewers continued to have concerns regarding the experiments performed   the potential lack of fine tuning post pruning, the use of s_t tilde as opposed to s_t in the practical algorithms amongst others listed in the review. Overall, the reviewers deemed the theoretical contribution of the paper not significant enough in terms of novelty and the decision hinged on the efficacy of the experimental evaluation   the lingering concerns for which led to the decision.  
This paper analyzes the universal approximation power of deep residual neural networks based on nonlinear control theory. Some concerns regarding the clarify, significance and connection to practice were raised, and partially addressed by the rebuttal. After discussions, all the reviewers feel positive about the contribution of the paper.
This paper presents the method of using Fisher information matrix values to identify examples near the decision boundary for a model, and proposes to preferentially use these examples in evaluation.   Pros:   Reviewers found this use of FIM values to be novel and interesting.   The paper presents fairly extensive results demonstrating the properties of examples selected and perturbed under the proposed methods.   Cons:    One of the main aims of this paper is to promote the use of examples selected in this way as evaluation sets. The method relies on values estimated from a specific model to select difficult examples, so this raises a fairly serious objection: If our goal in evaluation is to produce a fair comparison of two models, how do we choose which model we should use to select the test examples?  Reviewers were fairly unanimous in the objection that they raised, and while there was substantial discussion both with the authors and among reviewers, no reviewer was satisfied that the authors took this concern seriously or offered a clear resolution.
The paper proposes a new approach to knowledge distillation by searching for a family of student models instead of a specific model. The key idea is that given an optimal family of student models, any model sampled from this family is expected to perform well when trained using knowledge distillation. Overall this is an interesting idea and an important direction of research. However, the reviewers raised several concerns regarding novelty and experimental evaluation. There was a clear consensus among the reviewers that the paper is not yet ready for publication. The specific reasons for rejection include the following: (i) the proposed method is somewhat incremental, and the paper s contributions should be adjusted accordingly; (ii) the experimental results in the paper do not provide a clear/fair comparison with existing approaches, and additional baselines should be considered. The reviewers have provided detailed feedback in their reviews, and we hope that the authors can incorporate this feedback when preparing future revisions of the paper.
This clearly written paper has been constructively evaluated by three expert reviewers who provided at least two very detailed and informative summaries. The authors have addressed the inquiries raised by the reviewers in a comprehensive fashion, and at least one reviewer has updated their score as a result of those detailed rebuttals. In spite of some outstanding limitations, including a somewhat limited view of the relation of the proposed approach to existing alternatives, the reviewers are consistent in suggesting that this work is sufficiently mature to be considered for the inclusion in the program of ICLR 2021. I concur with that and recommend accepting this paper.
Four reviewers rate this article borderline. R3 finds the paper clearly presented and the method effective, but misses quantitative analysis of the dynamic range problem as well as novelty. Following the discussion and revision, she/he considers the paper improved and updated the score to 5, still being concerned about the novelty. R1 considers the paper makes an important observation but has concerns about experiments, rating it 6. R2 considers that the paper contributes a clear idea, but indicates that more analysis and supporting results are needed. She/he indicated a number of shortcomings in the initial review, and found the update good, hence tending to rate the paper higher after the responses, 6. R4 considers the paper well motivated and the method valid. However, he/she found the writing poor and over claiming results, and that more rigorous mathematical notation would help. After the discussion and revision, he/she found the paper better and increased the score to 5, but still found issues preventing the paper from being accepted. In summary, the reviewers agree that the paper contains an interesting and well motivated method, but they also point at a number of shortcomings. The revision improved several of them but others persisted. Although the ratings improved after the discussion, the overall rating is borderline. This is a very competitive call, and hence I have to recommend reject at this time. 
Dear authors,  Thank you for your submission. The reviewers all appreciated the direction of research and the message that GN can be a bad measure of generalization. That said, they all shared concerns regarding the strength of the conclusions that can be drawn from your work.  I encourage you to address their comments and submit a revised version to a later conference.    Reviewer 1 wanted to update their review but couldn t so here is the update:   Some more details on my original concerns  Thank you for your detailed responses. I wanted to add more details to the ones not discussed by other reviewers.    Regarding the speed of computing the gradient norm, I still don t agree that the computation cost is high. Figure 6 in the Backpack paper shows the cost of computing individual gradients at most 4x the cost of a single backprop not 100 1000x. In reference [2] that I gave, there is also a cheaper approximation discussed with computational costs detailed in Appendix B. As long as the computation of gradient norm is comparable with the cost of a single back prop it should be cheap enough to run all your experiments.    Regarding the conclusions in the paper. Thank you for giving more details. Adding those explanations to the paper would help. I personally missed some of those takeaway messages.  Overall, I strongly recommend either strengthening the link between GN and AGN or using better approximations. As well as better discussing the conclusions. Of course in addition to the suggestions by other reviewers.
This paper proposes a method (via a novel objective) for feature alignment between source and target tasks in an unsupervised domain adaptation scenario.  Pro   the proposed approach is sensible in many realistic scenarios of distribution shift   the submission provides an extensive empirical evaluation establishing state of the art results on several benchmark tasks  Con   there is no thorough discussion of the the underlying assumptions (when should we expect them to hold? for shich types of tasks? which types of shifts? can they generally be reliably tested? from which type of data? unlabeled?)   one reviewer raised concerns over novelty, which should be more clearly addressed before publication   two reviewers raised concerns over use of target data for hyper parameter selection, which seem valid; these should be fixed or clearly explained (and implications of this be discussed) in subsequent versions of this work  I agree with concerns of the reviewers (the last two points), and would therefore not recommend this work for publication in the current state.
This paper proposes a method of risk sensitive multi agent reinforcement learning in cooperative settings.  The proposed method introduces several new ideas, but they are not theoretically well founded, which has caused many confusions among the reviewers.  Although some of the confusions are resolved through discussion, there remain major concerns about the validity of the method.  
After the rebuttal phase, all reviewers give borderline scores (leaning slightly positive, one of these noted in the comment rather than final review). While the reviewers recognize the potential merit of the contribution (efficiency while preserving effectiveness), support for acceptance is not sufficient. The major concerns include novelty (shared by multiple reviewers) and the limited experimental settings.
This work proposes to train networks with mixed image sizes to allow for faster inference and also for robustness. The reviewers found the paper was well written and appreciated that the code was available for reproducibility. However, the paper does not sufficiently compare to related methods. The authors should resubmit once the comparisons suggested by the reviewers have been added to the paper.
The paper is concerned with learning representations for time varying graphs which is an important problem that is relevant to the ICLR community. For this purpose the authors propose a new method to extend skip gram with negative sampling to higher order tensors with the goal to perform an implicit tensor factorization of time varying graphs.The proposed approach shows promising experimental improvements compared to previous methods. Reviewers highlighted also the tasks considered in the paper as well as the theoretical and qualitative analysis as further positive aspects.  However, there exist still concerns regarding the current version of the manuscript. In particular, reviewers raised concerns regarding the novelty of the approach (SGNS, its extension to higher order tensors, as well as the connection to PMI have been studied in the literature). As such the new technical contributions are limited. Reviewers raised also concerns regarding the scalability of the method and its applicability to large graphs. The revised version addresses this concern to some extent by showing experiments on mid sized graphs with 2000/5000 nodes. While this clearly improves the paper, I agree with the majority of the reviewers that the manuscript requires an additional revision to iron out the points raised in this round of reviews. However, the presented results are indeed promising and I d encourage the authors to revise and resubmit their work considering the reviewers  feedback.
All the reviewers and I agree that the proposed approach is interesting and the paper is overall well written. However, I agree with R3 that the paper  need further re working the theoretical part (see the post rebuttal comments of R4). Thus, I would encourage the authors to carefully address the comments of the reviewers in the revised version of the paper, which would ultimately improve the quality of the paper. 
Dropout s Dream Land: Generalization from Learned Simulators to Reality  This work explores the use of dropout inside a learned dynamics model, so that when an agent is trained inside an environment generated by this model (rather than the actual environment), the policy learned would do better in the actual environment. In a sense, this is a form of domain randomization in a learned simulator. They show that their approach has better transfer capabilities compared to the baseline World Models method, where an entropy injection via temperature adjustment is used to make transfer more effective, and this is particularly evident in the CarRacing "learn in latent simulation" experiment.  While this work is interesting to me, and I believe it has something to offer to the ICLR community, after reading the reviews and also the detailed author / reviewer discussion (and after understanding and clarifying all of the nuanced points in the experiments), the reviewers and myself believe that this paper needs more work before meeting the bar of ICLR conference acceptance. I believe the author clarified many issues and misunderstandings with the reviewers, and we have made sure the reviewers took that into consideration. Based on the reviews, I have summarized recommendations below to help the authors improve this work. There are 2 dimensions of the work that can be improved:  1) Novelty and Connections with prior works that used dropout in RL  While this work is not using MC dropout (it applies dropout inside the LSTM M), there has been sufficient work (as listed by R2) using MC dropout with RL, and the reviewers  impression that while they may not be exactly the same, it does bring to question of the novelty in this work. It is recommended to discuss not only that the approach is not MC dropout, but also connections with previous work that used MC dropout (or even other forms of dropout) dynamics models to prevent overfitting, and also discuss why this particular approach is needed (or is better, via experiments), over MC dropout, if that can also be used to generate novel dream environments. As R3 mentioned, the idea of applying dropout (of whichever form) on a learned dynamics model *is* new, and this point should be emphasized very clearly to the reader, so I recommend the authors improve the writing to incorporate discussions and relations to previous work (R2 provides a good list), and emphasize what is considered novel in this particular work.  2) Experimental design  The authors show that the proposed method offers a clear advantage over the baseline WM approach for CarRacing, where they show that DDL can do the "train in dream / deploy in real env" transfer much better than the original baseline can. For the Doom experiment, I m less concerned about the exact score compared to the baseline (as noted by R1), given the high variance of these results, and also the high randomness in the process used to collect data via a random policy, and see that their result is within the margin of statistical error. Just noting the replication effort that went in as a footnote and citing existing results, noting the high variance, should suffice.  What would really improve the work, IMO, is to compare to GameGAN on PacMan environment. Would DDL offer improvements vs GameGAN on PacMan? This will be a strong data point for the proposed method s effectiveness, compared to more trivial tasks such as DoomTakeCover. R3 also brings out a good point that the paper offers better sim to sim transfer, rather than sim to real transfer. That is another avenue to explore, if this work is to be improved.  The authors have cited PlaNet / Dreamer / SimPle papers, but mentioned that these works don t deal with the issue of reality gap, but I would argue that the iterative learning (data gathering / retraining) aspect of these algorithms is actually one method to address the gap. The tasks studied in these more recent papers, such as DM Control from Pixels, or Atari, have more datapoints, or "leaderboard" participants, using this paper s terminology, so they can also be considered if the authors wish to try DDL to see if this method can help improve the performance of these newer approaches which are based on world models with iterative training and data collection. One can explore whether this approach can lead to better sample efficiency gains, in addition to absolute performance after training, when combined with iterative training in PlaNet / Dreamer type approaches.  Overall, the work in its current form would make a good workshop paper, but I look forward to seeing more work done in the experiments to see better convincing results, in addition to clarifying the writing on related approaches and making contributions / novelty more clear, which I believe will really improve the work for a future submission.
Summary: This paper provides an approach for causal inference in observational survival dataset in which the outcome is of time to event type with right censored samples.  To this end, the paper adapts the balanced representation learning approach proposed in (Shalit et al, 2017) to the context of survival analysis. The paper adapts an approach that uses flexible models to learn nuisance models, common in machine learning.  The authors validated their approach via simulation study and a set of application datasets: a EHR based cohort study of cardio vascular health, an RCT dataset of HIV patients, and a semi synthetic dataset.  The main concerns of reviewers were due to perceived lack of originality relative to the original proposal in (Shalit et al, 2017) 
This paper studies the number of linear regions of a multi layer ReLU network and gives a new upper bound. Reviewers concern about the writing and the results are incremental compared with previous results.
The paper considers generalization analysis of SGD using stability analysis. The authors argued the use of normalized version of the loss function, and angle wise stability. However, the reviewers pointed out that both motivation and novelty of the current work are not strong enough for it to be accepted by ICLR.
The reviewers raised a number of concerns, but  the authors provided no rebuttal to the reviewers  comments.  One reviewer felt the experimental fitting was not thorough enough. Suppose one used layers of oriented bandpass filters, separated by non linearities, would that perform well on the task convnets are trained on?  The AC doesn t agree with the arguments of R3.  I hope the comments of the reviewers, particularly the many specific comments of reviewers R1 and R2, will be helpful to you as you revise the manuscript.  The AC feels a more thorough experimental evaluation, and following up on many of the suggestions of the reviewers will lead to a strong paper.  As it stands, however, with 3 recommendations for rejection (1 weak), and only 1 weak recommendation for acceptance, we need to reject. 
The reviews were largely split in the beginning. Some of the concerns are firmly addressed, e.g. new results evaluating the actual latency in real hardware, and one reviewer raised the score from 5 to 6. However, another reviewer is not fully convinced by the response and decided to keep the original score of  "3: Clear rejection".    There are mainly two issues here. One is about the novelty of the method. The reviewer asked about the novelty and difference from CondConv/DynamicConv, however the authors emphasized the techniques to successfully train conditional computation models in general. As the focus of the paper is the proposal of the new (claimed as better) method, I have to say it is missing the points.  (The authors could have organized the storyline of the paper as "best practices for training conditional computation models" or something like that, if that is the true contribution the paper.) Another issue is about the advantage over the CondConv method. In the newly added results, BasisNet does not show clear advantage in terms of accuracy speed trade off without early exiting. Although the authors simply state that it is "infeasible" to do early termination on CondConv, the reason is not clear. Indeed, one can easily try layer level early exiting as done in BranchNet for example, if not the model level early exiting assumed for the BasisNet.   Base on the discussion above, I conclude that the paper has to clarify many issues before publication and thus recommend rejection.
The paper presents a new method for generation of backdoor attacks against deep networks. The new method uses global warping instead of noise patches which makes the attack much more stealthy than previous approaches. The attack effectiveness is demonstrated on 3 benchmark datasets. A small user study is carried out to demonstrate that the attack is stealthier than conventional backdoor attacks.   The new attack is a novel and original contribution which is likely to advance the understanding of backdoor attacks. There were some issues with respect to clarity in the original manuscript but the authors adequately addressed the critical remarks raised by the reviewers. 
While it’s commonly acknowledged that the paper is well written, the reviews are a bit split: R3 and R1 are mildly positive/negative, respectively, R2 and R4 both voted for reject. R2 asked many questions regarding experiments, which were addressed in the details in the rebuttal. R4 raised 6 questions regarding the bound, and the authors only answered some of them in the rebuttal. R4 felt “the method is lacking in a theoretic proof of a strict bound, which is the primary contribution of the paper”. Both R1 and R4 pointed out the proposed algorithm is not practical as expected, especially the results on larger scale such as ImageNet are missing.   The AC cannot agree with the authors’ argument that the contribution of the paper is “a conceptual framework that it is possible to certify a watermark for neural networks” in responding to such criticisms. It’s indeed very important for this conceptual framework to be proven valuable through thorough experiments and solid comparisons. 
This paper provides a novel generalization bound for neural networks using knowledge distillation. In particular, they argue that  "test error <  training error + distillation error + distillation complexity" where the distillation complexity is typically much smaller than the original complexity of the neural network. This is motivated by the empirical findings that neural networks can typically be significantly compressed in practice using KD without losing too much accuracy.    I found this result novel and the direction is very promising. This is a clear accept for ICLR. 
This paper proposes a novel and interesting approach called co distillation for distributed training. The main idea is to add a regularizer in order to encourage local models to be consistent with the global objective. Although the idea is a promising alternative to local update SGD, the approach is mostly empirical. The claim that co distillation helps reduce overfitting could be better justified by theoretical analysis, in addition to the experimental results. I hope that the reviewers  constructive comments will help improve the paper for future re submission.
This work investigates the recently proposed hypothesis that enhanced shape bias improves neural network robustness to common corruptions. Several interesting experiments are performed to better understand the contributing factors that lead to improved robustness of models trained with texture randomization. Of particular note, the authors design a data augmentation strategy that verifiably increases the shape bias of model, but for which corruption robustness is not improved. Reviewers agreed that this is an interesting counter example to the shape bias hypothesis and improves our understanding of why stylization improves robustness. Given the carefully designed experiments investigating an important topic I recommend accept.
In line with recent work in the NAS literature, the authors consider a weak NAS performance strategy to filter out bad architectures and narrow down the exploration to the most promising region of the search space. The authors propose to estimate weak predictors progressively by learning a series of weak predictors that can connect towards the best architectures. The authors provided a number of additional experiments during rebuttal, addressing most of the reviewers  comments convincingly and further showing the strong performance of their method. However, the authors should relate their work to Bayesian optimization, which comes in many flavors, and black box optimization techniques in general as their work shows a number of similarities, but is less principled.
This paper proposes a new and unusual way of training hard attention mechanisms in vision models. Instead of training with reinforcement learning (as is typical), the authors develop a procedure for generating "glimpse sequences" that can be effectively used as supervision. Models trained in this way produce qualitatively "better" glimpse sequences, higher accuracy, and converge in fewer steps. There was some disagreement and discussion about the merits of the paper. Overall, there were some major concerns:   The process for obtaining glimpse sequences is very computationally expensive. The authors argue that this cost can be amortized because the same glimpse sequences can be computed once for a given dataset and reused. However, there was limited real world motivation for this setup, apart from mentioning neural architecture search (a niche method that is not widely used, and probably has never been used to develop hard attention models).   The method relies on a "convincing" generative model for a given dataset. This limits experiments to simple datasets with unrealistically constrained visual structure. The authors point out that as generative models get better, their method could be applied to more realistic datasets, but as it stands the experimental validation is correspondingly weak.   The improvement in performance is not huge   the "WSRAM" baseline outperforms the use of "near optimal" glimpse sequences. While it is certainly true that the proposed method converges faster, the fact that the proposed method requires such an expensive preprocessing step downgrades this benefit significantly   While the authors provided insightful distillation based baselines in the rebuttal, it remains to be seen whether simple distillation from stronger RAM models (e.g. WSRAM which outperforms the proposed method) could be made to work better/more efficiently. These factors lead to a reject decision overall.
 The paper considers exploiting low rank structure in Q function and the Hamiltonian Monte Carlo (HMC) to approximate the expectation in Q learning to reduce the stochastic approxiamtion error, and thus, achieves "efficient RL". The authors tested the algorithm empirically within some simple environments.   As reviewers (R1, R3, R4) mentioned, the major bottleneck of this algorithm is the assumption that the dynamics is known up to a constant, which is extremly strong, and thus, limits the application of the algorithm. I suggest the authors to consider the common RL setting, without any knowledge about the transition models, and make fair empirical comparison with baselines in the same setting.  
The paper proposes a method that combines imitation learning and meta learning, which aims to be able to explore beyond the provided demonstrations.   While the paper addresses an important topic, and the authors are commended on a productive conversations, there is a consensus among the reviews that the work is not yet ready for publication. The future manuscript should address: reexamine the assumptions and improve presentation.
This paper proposes a new method for post training quantization, achieving very good results. After the author s response, all the reviewers were positive. There were some issues regarding clarity, and about explaining why the methods work better than just optimizing the loss, but I think the reviewers were eventually satisfied.  Following some info after the author s response phase, I ll just ask the authors to verify their published code works with publicly available PyTorch packages, so their method could be easily used.
The authors demonstrate that complete neural network verification methods that use limited precision arithmetic can fail to detect the possibility of attacks that exploit numerical roundoff errors. They develop techniques to insert a backdoor into networks enabling such exploitation, that remains undetected by neural network verifiers and a simple defence against this particular backdoor insertion.   The paper demonstrates an important and often ignored shortcoming of neural network verification methods, getting around which remains a significant challenge. Particularly in adversarial situations, this is a significant risk and needs to be studied carefully in further work.  All reviewers were in agreement on acceptance and concerns raised were adequately addressed in the rebuttal phase, hence I recommend acceptance. However, a few clarifications raised by the official reviewers and public comments should be addressed in the final revision: 1) Acknowledging that incomplete verification methods that rely on sound overapproximation do not suffer from this shortcoming. 2) Concerns around reproducibility of MIPVerify related experiments brought up in public comments.
Please clarify as early as the abstract that you refine the analysis of the algorithm proposed by Shalev Shwartz et al (which is a great contribution given the importance of the problem).
The reviewers agree that the paper, in its current form, is not strong enough to allow for publication.  There are specific weaknesses that need to be tackled: a better correlation study; a clearer relationship to existing literature (and improvement on the novelty); clearer, more precise use of descriptions.  The authors are encouraged to continue with their work and submit a more mature manuscript.
This paper proposes an approach to probabilistic time series forecasting based on combining autoregressive deep learning models with normalizing flows. In terms of strengths, time series forecasting is a fundamental problem. The proposed approach is a reasonable combination of existing model components that provides a flexible, end to end trainable framework for multivariate probabilistic forecasting. The experiments are well conducted and the results outperform recently published methods. While the reviewers raised a number of questions, all of the reviewers agree that their questions have be answered satisfactorily by the authors during the discussion and the paper should be accepted. The authors should be sure to incorporate the reviewer suggestions and author responses into the final paper. 
All three reviewers initially recommended reject.  The main concerns were: 1) weak technical contribution and insight [R1, R2, R3, R4]; 2) incremental novelty (another variation of SiamFC) [R1, R2, R3]; 3) unconvincing experiment results against missing SOTA [R1, R2, R3];  The author s response did not assuage these concerns.
This paper would greatly benefit from some reorganization/rewriting since, as pointed out by some of the reviewers, it’s hard to follow in its current form. While a biologically inspired NAS algorithm could be an interesting direction to explore, the current paper falls short in providing evidence that the approach is well motivated or empirically strong.  In terms of empirics, too many details are missing on the search space/architecture, ablations and comparison with existing methods. For future submissions, it would be particularly useful for the authors to explicitly discuss why they don’t find competing methods applicable to their setting.
This paper establishes the currently sharpest regret bounds for reinforcement learning in episodic factored MDP. The result improve the result by Osband and Van Roy 2014. The proposed FMDP BF is a model based algorithm that construct confidence sets of the transition distributions using Bernstein  and adapt policies by optimistic planning. The regret bounds holds with high probability. They also provide a lower bound for this class of problems. Reviewers all see merit in the theoretical results of the paper and reach a consensus that this is a good paper. We d still like to request that the authors make all corrections and clarifications following the reviewers s suggestions, especially to improve the clarity of the formulation and proof sketches.  A separate suggestion: Model based RL is a long existing approaches. For MDP belongs to a specific family, there exist regret bounds that depend on Eluder dimension of the the MDP family, see eg. https://arxiv.org/abs/1406.1853 and https://arxiv.org/abs/2006.01107. Can these results be applied to the factored MDP family and yield similar regret bounds? It would be necessary to add discussions about these papers, and explain why or why not these general regret bounds can apply to analyze HMDP.
The manuscript describes a method for identifying and correcting classifier performance when labels are assigned incorrectly. The identification is based on clustering classification failure regions in a VAE latent space and the correction phase is based on fine tuning the classifier with additional synthetic samples from the VAE.  Reviewers agreed that the manuscript is not ready for publication. The main issue is that the suggested training method is similar to adversarial training methods used to gain adversarial robustness. The method does not help in debugging and fixing failures in general. 
This paper presents a model for spatiotemporal point processes using neural ODEs. Some technical innovations are introduced to allow the conditional intensity to change discontinuously in response to new events. Likewise, the spatial intensity is expanded upon that proposed in prior work on neural SDEs. Reviewers were generally positive about the contributions and the empirical assessments, and the authors made substantial improvements during the discussion phase.
In this paper, the authors propose a new max sliced Wasserstein distance. Specifically, the proposed method is a multiple sliced variants of the existing max sliced Wasserstein distance. Compared to the subspace Robust Wasserstein distance, the proposed method can be efficiently computed.  Overall, the proposed method is a good extension of the max sliced Wasserstein and can be used in various applications. All authors agree to accept the paper, so, I also vote for acceptance.
The paper proposes an interesting approach that leverages shared dynamics across causal systems for improved joint causal discovery.  The reviewers and AC all agree that the approach is interesting, promising and that the paper is well written.   While theoretical validation would be an exciting thing to have, it is perfectly acceptable for the paper to focus on an empirical study. But in this case, it is very important to provide convincing evaluation experiments.  As several reviewers have pointed out, in order to convincingly demonstrate the value of the approach, it would be very important for the experiments to go beyond noiseless systems.  We strongly encourage the authors to address this point as this will significantly strengthen the significance of their contributions.
The authors study the expressive power of Graph Neural Network architectures for the link prediction problem and provides theoretical justification for the strong performance of SEAL on link prediction benchmarks. However, The reviewers think the paper needs to improve in several aspects before it can be published: 1. More clearly explain the theoretical analysis and contribution. 2. Extensive and in depth discussion of the similarities with and difference to the work of Li et al. to show the novelty of current work. 
The reviewers and authors have had a significant and healthy discussion around this manuscript. The reviewers remain concerned about the some of the central claims in this manuscript. While they have appreciated the clear communication and willingness of the authors to clarify most of their concerns, this central issue unites the reviewers in maintaining their desire to see a more significant revision of this work before publication. I recommend that the authors take the reviewers  recommendations in improving the presentation and comparison of their ideas.
All Reviewers point out that the paper, although having some strong points, does not meet the bar for a highly selective machine learning conference like ICLR. Hence, my recommendation is to REJECT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Well written paper.   Ambitious task.   Code will be released.  Cons:   Unclear task terminology (music production; misleading title).   Mixed results.   Experimental design could be improved.   Exposition could be improved (technical details missing).   Lack of comparison (for instance with other CycleGAN variants; more experimental setups).   Lack of discussion on the use of a source algorithm for pre processing data.
The paper proposes to do unsupervised discovery of 3D physical objects. The core idea is to decompose the scene into primitives that contain: (a) a segment; (b) 3D position and dynamics; and (c) appearance. These are combined with a physics model and renderer to discover objects/primitives by watching videos; the core supervisory signal used is that one should be able to reconstruct future scenes and that objects/primitives ought to be physically consistent. The system is tested on synthetic data as well as real videos of blocks.   The reviewers were positive about many aspects but, at the time of submission had a number of concerns. These were, in view of of many of the four reviewers, largely addressed. These are as follows:   One overarching concern (R3, R4) was the experiments that the paper’s title and motivation focused heavily on 3D but the experiments lacked a 3D experiment of any variety. The authors addressed this by adding 3D IOU and recall. While numbers are low for IOU, this is a challenging area and the AC appreciates this as did R3 and R4.   Another concern is the data itself (R4,R1). R4 in particular cites the synthetic nature of it as a stumbling block; R1 is similarly concerned about the difficulty of the backgrounds (and the rigidity of the objects). The AC thinks that the data is sufficient for this paper given the overall paper focus, methodological contributions, and particular set of claims. However, the AC is highly sympathetic to R4’s arguments and thinks more realistic real data (beyond the additional data of towers of blocks in front of a white sheet) would substantially improve the impact of the paper and the direction of research.   The last content focused concern was disagreement that the system is unsupervised (R2,R4). The authors have addressed this with experiments using a hard coded system that uses a heuristic based on the bottom coordinate, which obtains good results as well. All reviewers with this concern seem satisfied although the AC would note this assumes a single ground plane, which ties into concerns about the data (although this is a small nitpick).   R2 had substantial concerns about the legibility and reproducibility of the paper. These have been largely addressed in the revision, as far as the AC can tell.  The paper is an good contribution on a challenging and important problem. While the AC shares some of R4’s concerns about the data (and indeed how data difficulty and method interact), the AC finds the revised paper compelling and recommends acceptance.
 The reviewers have  different views on the papers but agreed that the paper can be accepted. However, they suggested some points of improvements including the writing (clarity and style) and experiments showing strong improvements compared to WGAN.
This paper introduces a clever new problem that may prove useful in the advancement of Automatic Theorem Proving   finding intermediate steps in a proof. A non synthetic benchmark is created based on a large human created dataset of proofs. Neural models were shown to have non trivial performance. Reviewers were convinced that this is ultimately a useful benchmark.
The authors propose a method for attacking neural NLP models based on individual word importance ("WordsWorth" scores).  This is an interesting, timely topic and there may be some interesting ideas here, but at present the paper suffers from poor presentation which makes it difficult to discern the contribution. Presentation issues aside, it seems that the experimental setup is missing key baselines (an issue not sufficiently addressed by the author response). 
This paper propose a method to explain the contextualization of BERT by identifying a set of influence paths from the input to the output.  Although all reviewers give overall score 6, their comments are pointing to the negative direction.  The following excerpts summarize the general sentiment of the reviews: R2: Overall, I incline toward rejecting. This paper provides an instrument to explain BERT, but I have a hard time understanding the result itself (influence paths or patterns). I also have a major concern with the final analysis and its findings.   R4: I think the paper has improved substantially. The direction is exciting, but even with the updates I believe this paper needs quite some work to improve the presentation and clarity, which is why I have not updated my score. R1: Overall, I think there are certainly interesting analyses that could come out of this work, but the current paper does not provide a clear enough contribution to be ready for publication. R3: It looks like the proposed method is largely based on (Lu et al, 2020); the major difference is the introduction of Multi partite patterns, which basically expands the objects of influence analysis from paths to patterns (partial paths). This doesn t look like a significant novelty. 
Considering reviewers  comments and comparing with similar papers recently published or submitted, this is a good paper but hasn t reached the bar of ICLR.  We believe that the paper is not ready for publication yet, and strongly encourage the authors to use the reviewers  feedback to improve the work and resubmit to one of the upcoming conferences. 
The paper proposes a new computational block called "StarSaber" which is a self attention based block derived from RNN fixed point approximations.  All the 4 reviewers and the authors agree that the work is not ready for publication. While the motivation of the authors is interesting, some reviewers have raised concerns about the validity of the derivation. All the reviewers agree that authors need to clarify the difference between StarSaber and Transformer, compare with Transformer in large scale pre training experiments, compare the compute speed, do a lot of ablations to validate the design choices.  I recommend the authors to incorporate all the reviewers  comments and make a stronger submission to a future conference!
The paper presents a Bayesian approach for classification able  to  adapt  to  novel  classes  given  only  a  few  labeled  examples. The models combines a one vs each approximation of the likelihood combined with a Gaussian process. This allows to resort to a data augmentation scheme based on Polya gamma random variables.  The paper is clearly written and combines existing techniques in a convincing manner; the experiments demonstrate better accuracy and uncertainty quantification on benchmark datasets.   I recommend acceptance.
All three reviewers expressed consistent concerns on this submission in their reviews. In addition, none of them enthusiastically supported this work during discussion. It is clear this submission does not make the bar of ICLR. Thus a reject is recommended.
This paper proposes a deep reinforcement learning approach for solving minimax multiple TSP problem. Their main algorithmic contribution is to propose a specialized graph neural network to parameterize the policy and used a clipped idea to stabilize the training. Unfortunately, the reviewers remain to be unconvinced by the experiments after the rebuttal and the writing need to be significantly improved. Also, it would be worthwhile to study how the proposed method can generalize to other problems.  
It is common in imitation learning to measure and minimize the differences between the agent’s and expert’s visitation distributions. This paper proposes using Wasserstein distance for this, named PWIL, by considering the upper bound of its primal form and taking it as the optimization objective. The effectiveness of the approach is demonstrated by an extensive set of experiments.   Overall, reviewers reached general agreement that this paper makes a good contribution to the conference, and given the overall positive reviews, I also recommend accepting the paper. 
The paper received mixed reviews, with one review voting for acceptance, one strongly opposed, and two borderline ones. The discussion essentially involved R1 and R2, who gave the most informative reviews. After discussion, they did not update their score, even though they appreciated the work and effort done by the authors during the rebuttal.   In short, the paper has some merit, but several concerns were raised, which the area chair agrees with, leading to a rejection recommendation. The innovation was found to be limited and the discussion between practice and theory (meaning assumptions made in this work) are not discussed in a convincing manner, and these concerns remained after the rebuttal. The experiments were also subject to improvements.  It is however likely that with a major revision, this work may become publishable to a another venue.
This paper investigate an interesting problem of multi agent RL with self play. We agree with the reviewers that the paper requires more work before it can be presented at a top conference.  We would  encourage the authors to use the reviewers  feedback to improve the paper and resubmit to one of the upcoming conferences. 
This paper proposes a potentially very interesting and original approach to handle label noise.  The numerical experiments suggest that the method works very well. But the paper  itself has been deemed very hard and demanding to read and understand for a general machine learning crowd and even by experts in the fields of optimal transport and  Markov theory.   Note that due to the low confidence in several review an additional emergency review by an expert was asked and it confirmed the global opinion from  other reviewers that the paper is interesting but needs a major rewriting before acceptance in a ML conference. The AC strongly suggest that the authors work on a more pedagogical introduction and explanation of the method before resubmitting.  
The paper proposes a variant of Kanerva Machine Wu et al. (2018) by introducing a spatial transformer to index the memory storage and Temporal Shift Module Lin et al., (2019). The KM++ model learns to encode an exchangeable sequence locally via the spatial transformer. The proposed method is evaluated on conditional image generation tasks. The empirical results demonstrated the nearby keys in the memory encoded related and similar images. Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in a way that satisfied the reviewers. The basic ideas in the paper are interesting to both the machine learning and the wider cognitive science communities. However, additional experiments should be included in Table 1 to complete the "DKM w/TSM (our impl)" row on Fashion MNIST, CIFAR 10, and DMLab in the final revision for completeness. 
This article proposes latent variable augmentation scheme for inference in nonlinear multivariate Hawkes processes. It combines existing approaches (Polya gamma and sparsity inducing variables) in a sensible way and is clearly written. Concerns were raised with respect to the comparison to alternative baselines, and answered by the authors. As a result, some reviewers have increased their score, and I recommend acceptance. 
  The paper aims at controllable generation by introducing an additional "content conditioner" block in the Transformer models. The paper further provides 4 different variants of a pre training task to train the content conditioner model.   While the proposed approach seems an incremental contribution over CTRL and PPLM, certain reviews praised the approach being novel while keeping the architecture changes minimal. Overall, reviews indicate that the overall proposed method of fine grained controlled generation with self supervision is valuable, and empirical results support its effectiveness.   All reviewers initially raised concerns regarding clarity and lack of human evaluation. However, clarity issues seem to be resolved through author/reviewer discussions and the updated revision.  R3 had important concerns regarding topic and sentiment relevance evaluations.   While the reviewer remains unconvinced after discussions with authors, after carefully reading the revised paper and discussions, I feel that the authors tried to address this point fairly  through their additional experiments and also edited their contribution statement accordingly.  Overall, at least two reviewers sounded very excited about this work and other than R3 s concerns, the general sentiment about this work was positive. Therefore, I recommend weak accept.    There are still some writing issues that I strongly encourage authors to carefully address in the future versions. Quoting from reviewer discussions:  > Differentiability of the adversarial loss. Authors just added one statement saying " Through continuous approximation.." without any more details are given, which continuous approx was used (Gumbel softmax?) and how they overcame the problem of its training instability.   > Table 6, can be misleading, authors bold the results when cocon+ is performing better than baselines (mostly in content similarity) but not the other way around topic/sentiment accuracy. The latter is arguably more important.
The paper proposes a new solution for cross domain correspondence in control, which combines GANs and cycle consistency, and separates shifts in observation space and in action space. The paper targets unpaired data / simulations, and discovers alignment of state by enforcing that domains are mappable.  The paper was received well by reviewers, who pointed out several strengths: a strong contribution on a fundamental problem, and an interesting formulation; a well written and well positioned paper; This compensates minor weaknesses, in particular the fact that transfer has been tested between two different simulated environments.   The reviewers unanimously suggested acceptance, the AC concurs.
All reviewers agree the paper does not meet the acceptance bar, and an authors  rebuttal is not available. Therefore, I recommend rejection.
The focus of the submission is kernel ridge regression in the distributed setting. Particularly, the authors present optimal learning rates under this assumption both in expectation and in probability, while they relax previous restrictions on the number of partitions taken. The effectiveness of the approach is demonstrated in synthetic and real world settings.  As summarized by the reviewers, the submission is well organized and clearly written, the authors focus on an important problem, they present a fundamental theoretical contribution which also has clear practical impact. As such the submission could be of interest to the ICLR and ML community.
The authors consider view consistency when learning graph neural networks. However, as mentioned by the reviewers, the novelty of the proposed method is limited and the rationality of the implementation is not convincing. More deep discussions about related papers and analytic experiments are required to support this work. Additionally, I have concerns about the scalability of the method   whether it can deal with more than two views and how it will perform are not studied in this work. I tend to reject it based on its current status. 
This paper introduces a new multilingual parallel Bible dataset for African languages, a new method for determining similarities between languages, and a collection of experiments to evaluate methods for choosing an additional language based on (a) similarity and (b) language history to include in a multilingual MT system. Results show that strategic inclusion of an additional language can substantially improve BLEU. Reviewers universally agree that progress on MT for African languages is a very important goal. However, reviewers pointed to several major concerns with the current draft: (1) lack of sufficient detail for replicating experiments, (2) missing analysis to interpret why experimental gains are so large, and (3) missing discussion and comparison with already existing methods in multilingual MT (e.g. multilingual training for low resource languages). I agree with reviewers that the paper is not ready for acceptance in its current form, but encourage re submission, possibly at an NLP conference.  
This paper proposes fine grained layer attention to evaluate the contribution of individual encoder layers. This departs from the standard transformer architecture where the decoder uses only the final encoder layer. This paper investigates how encoder layer fusion works, where the decoder layers have access to information for various encoder layers. The main finding of the paper is that the encoder embedding layer is particularly important. They propose SurfaceFusion, which only connects the encoder embedding layer to the softmax layer of decoders, leading to accuracy gains.  There was some disagreement among reviewers about this paper. Overall, I found this a simple but effective contribution with interesting findings that can help future research in seq2seq models. Some of the weaknesses (discussing other relevant works, discussing other variants of FGLA, adding new experimental results) have been addressed in the updated version of the paper. One of the reviewers suggested running additional experiments on GLUE style tasks (with a masked language model) to be really sure if the technique is convincing, and particularly try it with larger models (T5 was suggested). While adding those experiments would be a plus, I disagree that this is crucial   this paper is focusing on seq2seq tasks and is already considering several tasks: summarization, MT, and grammar correction. The results found by this paper are interesting and can foster future research extending this beyond these 3 tasks. Even if larger models can make the improvements smaller, there are many inconveniences in just increasing scale (memory consumption, energy consumption, etc.) It is my opinion that the community should value research that tries to understand the weaknesses of smaller models, rather than relying on large scale models to solve all problems.
The reviewers appreciate the importance of enforcing safety in RL, and the technical directions considered in the paper related to incorporating cost in advantage estimation.  However, they express several concerns about the formulation of the problem considered and the consistency of the approach, as well as the somewhat incremental contribution w.r.t. CPO.  Three reviewers recommend rejection.
The paper proposes a method to grow deep network architectures over the course of training. The work has been extremely well received and has clear novelty and solid experiment validation.
The paper studies three kinds of memory augmented Transformers, focusing on one (the MemTransformer, which adds [MEM] tokens to a document.)  This is a nice clean extension of Transformers and a topic well worth investigating.  Unfortunately, the experimental results were considered unconvincing:     The baselines were relatively weak    The experimental setting was unusual (eg only 10 epochs)    The experiments did not show consistent improvement  Overall the paper was considered below acceptable quality for ICLR. 
This paper proposes techniques to lower the barrier to run large scale simulations under resource (compute) constraints. The key idea is to do batch simulation and policy learning on 1 or more GPUs without sacrificing the fps rate for rendering (~20k fps on 1GPU). The proposed setup and methods are evaluated on the point navgiation tasks on two environments namely Gibson and Matterport3D. One of the key ideas for rendering is to render a big tile of images for separate instantiations of the environment in parallel. This gives big speeds up to rendering and policy optimization.   ${\bf Pros}$: 1. Large number of FPS with smaller compute budget. Large scale Deep  RL research has been difficult to democratize due to the need for big compute budgets. Although this paper is more heavy on the engineering side, I think it can greatly accelerate research and therefore seems like a good fit for the ICLR community to consider.   2. The paper and proposed steps are clearly written and justified  ${\bf Cons}$: 1. This method is limited to environments where the observation space follows a particular structure. This is perhaps the biggest limitation of this approach but the underlying assumptions are reasonable and quite a few realistic environments will fall into this category.      During the rebuttal and discussion period, R2 raised concerns about ablations but was satisfied with author s response. R5 raised concerns about other prior work   CuLE (Dalton et al, NeurIPS 2020). However, this paper is concurrent work and does not tackle 3D simulation rendering as done in this paper. For these reasons I believe the paper does not have any big red flags or pending concerns. 
All the reviewers rate the paper above the bar. They like the experiment results and think the proposed latent space editing approach makes intuitive sense. While several weakness points were raised, including a lack of continuous editing comparison and sometimes vague descriptions, they were not considered major to reject the paper. After consolidating the reviews and rebuttal, the AC agrees with the reviewer assessment and recommends accepting the paper.
The paper proposes to minimize the loss while regularizing its sharpness: so that the minimum will lie in a region with uniformly low loss. The reviewers uniformly appreciated the paper. They have made a number of suggestion for improving the paper, which the authors should consider incorporating in their final version.  
The reviews are concerned about the novelty/incremental nature of the paper and partially also about the  conclusions drawn from the experiments. The authors did not take the chance to write a response.
Being able to give confidence intervals or have a robust measure of uncertainty is very important for offline RL methods. In this work, they proposed a dropout based method to have a measure of uncertainty. The authors provide an significant empirical improvements over other baselines. Nevertheless, as it stands right now and as AnonReviewer5 have pointed out, this paper has some important shortcomings. I have noticed that the authors have updated the paper, but still the some of the important points made by AnonReviewer5 are unaddressed as it stands right now. Thus, I am suggesting to reject this paper hoping that the authors will address those issues and resubmit to a different venue.  Firstly, I agree with AnonReviewer5, it is not clear if the dropout and the variance trick used in this paper actually represents epistemic uncertainty that we would like to have for an offline RL algorithm, because the variance do not necessarily need to shrink as you train it with more data, and as opposed to supervised learning setting it is not clear what type of uncertainty the proposed dropout method will induce in the offline RL setting. It would have been nice to have some results showing how calibrated the uncertainty estimates coming from the dropout is... I would recommend the authors not to include any claims regarding the epistemic uncertainty in the camera ready version of the paper.  Also as AnonReviewer5 pointed out, having distributional baselines and/or ensemble methods like REM or bootstrapped DQN would be a more fair comparison. So, it would be nice to see some of those baselines in a future version of this paper.
Although this paper tackles an important problem, all reviewers agree that it requires further work before it can be published. First, the paper would need to be polished in order to be easier to read. Stronger experiments would also be needed in order to support the claims of the paper, e.g. by considering additional datasets and proper baselines. Finally, an important concern about this paper is novelty and originality. It is not clear at this point that the contribution is substantial enough for a conference like ICLR. Addressing these points would significantly improve the paper.
This is a clear reject. None of the reviewers supports publication of this work. The concerns of the reviewers are largely valid.
This paper presents an approach to use spatio temporal self similarity (STSS) as a feature for a convolutional neural network for video understanding. The proposed approach extracts STSS as a descriptor capturing similarities between local spatio temporal regions, and adds conventional layers such as soft argmax, fully connected layers, and conv. layers on top of it.  On one hand, all of the reviewers agree that the novelty of the paper is limited. On the other hand, most of the reviewers (except R1) appreciated thoroughness of the experiments and ablations. In the end, the reviewers gave 3 marginally above the acceptance threshold ratings and 1 marginally below the threshold rating.  The AC views this paper as a borderline paper. None of the reviewers are excited about the paper, and it is a typical "Nice experiments with limited novelty" (by R1) paper. The concept of the STSS itself was already proposed in prior studies as mentioned in the paper and by the reviewers, and this paper  engineers  a new way to take advantage of STSS without further theoratical or conceptual justifications on why it should work. The newly added Kinetics and HMDB results in the rebuttal are nice, but the impact of STSS seems to be minimal in these results.  Overall, the AC find the paper slighly lacking to be considered for ICLR.
In this paper, a method to solve the segmentation problem by continuous optimization is proposed by using a soft differentiable warping function. The proposed method is theoretically sound, and interesting experiments such as the data analysis of covid19 are also presented. This is a good paper in terms of both theory and application.
Dear Authors,  Thanks for your detailed feedback to and even communications with the reviewers. Your additional input certainly clarified some of the concerns raised by the reviewers and also improved their understanding of your work.  However, we still think that the notion of sequential bias is unclear, and the authors overclaim what they have done. For these reasons, this paper cannot be recommended for acceptance. I hope that the detailed feedback from the reviewers will help improve this work for future publication. 
All four reviewers recommend rejecting the paper. However there is agreement that this is an interesting line of research, and the AC agrees. Reviewers provided extensive and well educated feedback. The authors did not respond to the raised concerns.
The paper attempts to improve retrieval in open domain question answering systems, which is a very important problem. In this regards, the authors propose to utilize cross attention scores from a seq2seq reader models as signal for training retrieval systems. This approach overcomes typical low amount of labelled data available for retriever model. The reviewers reached a consensus that the proposed approach are interesting and novel. The proposed approach establish new state of the art performance on three QA datasets, although the improvements over previous methods are marginal. Overall, reviewers agree that the paper will be beneficial to the community and thus I recommend an acceptance to ICLR. 
During the discussion phase, although the reviewers acknowledge the effectiveness of the proposed approach, they raised the concern about the novelty of the paper.  In my opinion, I also agree that the novelty is not well justified in this paper. In the related work section, although the authors put an effort to review the existing studies of subspace learning and feature selection, their relationship (similarity and/or difference) to the proposed method is not discussed. Since the idea of using subspace learning and feature selection in clustering is standard, the novelty of this work should be introduction of the integration step into neural networks, which is not significant enough in its current state. The paper becomes more significant if, for example, theoretically discuss the unique characteristics of the integration into NNs which does not appear in the usual setting.   In addition, the motivation of face clustering is not convincing. I recommend either (1) use and discuss the domain specific property of the problem of face clustering in the proposed method, or (2) construct a general clustering method. Since the authors present additional experiments in the author response, I guess (2) fits. Then, however, the paper should be re organized.  The readability can be improved. For example, Algorithm 1 receives training data {X, A}, but I cannot find the definition of A. Also, please italicize mathematical symbols.  Overall, the paper is still not ready for publication, I will therefore reject the paper. 
The authors consider the problem of causal inference from multiple conditionally ignorable models that yield different observed data distributions.  This problem is distinct from transportability (which assumes some types of causal invariance across domains, and aims to move causal conclusioned learned in one context to another using this invariance).  The authors adapt a machine learning approach from (Shalit et al, 2017).  Because the authors describe an algorithm rather than a model, it was a bit difficult to understand what assumptions tie the different observed data distributions together (I am guessing there is a way to formulate a  global model  tying all datasets together in terms of the algorithm hyperparameters but the authors do not discuss this).  The authors evaluate their method via a simulation study.  Moreover, in response to reviewer criticism, the authors uploaded additional results from semi synthetic data.  Some of the concerns of reviewers were about novelty and scope of evaluation (in addition, some complained about writing and notation). 
The authors’ feedback has not fully addressed the reviewers’ concerns and the reviewers think that the paper is not ready for the publication. The authors should consider the following issues for the future submission:  1) The concern from Reviewer 1: if a local device receives very little data but its data come from a mixture component with large weight, its gradient will likely be biased (due to the lack of data) but will still dominate others (due to its large mixture weight).  2) Numerical experiments are not consistent with theoretical results. The theory is for convex but experiments are with non convex loss. The response from authors does not resolve this issue.   3) Notation is confusing and changing throughout. We strongly suggest the authors revise carefully this and make it clear.   Although the experimental results are potential, we would like the authors to revise it carefully by addressing the reviewers’ concerns and further improve it by considering theoretical results for non convex in order to submit to the next venues.  
The paper describes a new technique to generalize across different environments.  More precisely a new state similarity metric is defined with a contrastive learning embedding technique.  Unlike previous works that extend supervised learning techniques such as data augmentation and regularization to RL, the proposed approach takes into account the sequential nature of RL.    The reviewers unanimously praised the work in terms of theory, algorithm and empirical evaluation.  This is a novel and technically deep contribution that advances the state of the art for RL generalization.
This paper presents an empirical study focusing on Bayesian inference on NNGP   a Gaussian process where the kernel is defined by taking the width of a Bayesian neural network (BNN) to the infinity limit. The baselines include a finite width BNN with the same architecture, and a proposed GP BNN hybrid (NNGP LL) which is similar to GPDNN and deep kernel learning except that the last layer GP has its kernel defined by the width limit kernel. Experiments are performed on both regression and classification tasks, with a focus on OOD data. Results show that NNGP can obtain competitive results comparing to their BNN counterpart, and results on the proposed  NNGP LL approach provides promising supports on the hybrid design as to combine the best from both GP and deep learning fields.  Although the proposed approach is a natural extension of the recent line of work on GP BNN correspondence, reviewers agreed that the paper presented a good set of empirical studies, and the NNGP LL approach, evaluated in section 5 with SOTA deep learning architectures, provides a promising direction of future for scalable uncertainty estimation. This is the main reason that leads to my decision on acceptance.  Concerns on section 3 s results on under performing CNN & NNGP results on CIFAR 10 has been raised, which hinders the significance of the results there (since they are way too far from expected CNN accuracy). The compromise for model architecture in order to enable NNGP posterior sampling is understandable, although this does raise questions about the robustness of posterior inference for NNGP in large architectures.
All reviewers agree that the paper is well written and some of the experiments are interesting. However, the paper did not clearly highlight how this work fits in with prior research, neither did it show what the advantages of the presented homogeneous network are. The authors addressed some of these concerns in the rebuttal, but not enough to sway the reviewers. In the end all reviewers recommend rejection, and the AC sees no evidence to overturn this recommendation.
The paper proposed a two stage method to select instances from a set, involving candidate selection (learning a function  to determine a Bernoulli probability for each input) and AutoRegressive subset selection (learning a function  to generate probabilities for sampling elements from a reduced set); both stages use the Concrete distribution to ensure differentiability. The experiments show the performance of the proposed method on several use cases, including reconstruction of an image from a subset of its pixels, selecting sparse features for a classification task, and dataset distillation for few shot classification. I read the paper and I agree with the reviewers that in its current format the paper is hard to follow. I strongly encourage the authors to add more discussion and intuition on the proposed method and extend the experiments with more baseline comparison and ablation studies in the revised version. 
The paper proposes to bring together a GAN, a differentiable renderer, and an inverse graphics model. This combined model learns 3D aware image analysis and synthesis with very limited annotation effort (order of minutes). The results look impressive, even compared to training on a labeled dataset annotation of which took several orders of magnitude more time.  The reviewers point out the novelty of the proposed system and the very high quality of the results. On the downside, R2 mentions that the model appears over engineered and some important experimental results are missing. The authors’ response addresses these concerns quite well.  Overall, this is a really strong work with compelling results, taking an important step towards employing generative models and neural renderers “in the wild”. I think it can make for a good oral.  
The paper proposes a simple modification to Laplace approximation  to improve the quality of uncertainty estimates in neural networks.  The key idea is to add “uncertainty units” which do not affect the predictions but change the Hessian of the loss landscape, thereby improving the quality of uncertainty estimates. The “uncertainty units” are themselves trained by minimizing a non Bayesian objective that minimizes variance on in distribution data and maximizes variance on known out of distribution data. Unlike previous work on outlier exposure and prior networks, the known out of distribution data is used only post hoc.  While the idea is interesting and intriguing, the reviewers felt that the current version of the paper falls a bit short of the acceptance threshold (see detailed comments by R3 and R2’s concerns about Bayesian justification for this idea). I encourage the authors to revise and resubmit to a different venue. 
The paper develops a methodology for using graph neural networks for mesh based physics simulation. This extends recent work that focused on grids or particles to mesh based domains, which are challenging due to irregular (and possibly changing) connectivity. The reviewers had some concerns but recognized that this is an important work that will be of broad interest and may have significant impact.
The authors propose self predictive representations (predicting the agents own future latents of a forward model with data augmentation) as a means of improving the data efficiency of agents. The reviewers found the paper to be compelling (after the authors made adjustments) and pointed out that the method is likely generic and might be widely applicable. Experimental improvements in the work are significant, and the method is well explored.
This paper studies how (two layer) neural nets extrapolates. The paper is beautifully written and the authors very successfully answered all the questions. They managed to update the paper, clarify the assumptions and add additional experiments. 
The paper discusses a new threat model for multi exit DNNs: attacks against efficiency of inference. The proposed attack increases the inference time of such networks by the factor of 1.5 5, while at the same reducing the accuracy of attacked networks. Unlike classical adversarial examples, the new type of attack cannot be thwarted by adversarial training.  Overall, the paper exhibits a novel contribution, is well written and methodically sound. Its practical motivation is somewhat weak, as it is currently unclear for which applications such attacks may be feasible. However, the novelty of the threat model addressed by this paper makes it an interesting methodical contribution. 
This paper derives CLT type results for the minimum $\ell_2$ norm least squares estimator allowing both n and p to grow.  Pros: As one reviewer puts it: Asymptotic confidence intervals for different prediction risks are derived. These results seem new.   Cons: It s not clear what has been gained by having these results, other than having them.  Reasoning: Staring at Figure 1 for a while, what jumps out is how little the CI matters. Unless $p\approx n$, the band is essentially uniform around the first order result derived elsewhere. The claim the authors seem to make at the bottom of page 1 is that, "supposing I have 90 observations and 100 predictors, it may not be so bad to collect 8 more observations. Even though on average I m worse off, perhaps not for my data?" The flip side of this argument is "why am I using min norm OLS"? I think that the authors are making the wrong argument in this paper. The point of analyzing this problem is not to understand what happens when $p\approx n$ but to understand why $p \gg n$ is good, and thereby try to justify parameter explosion in deep learning. I should be looking at the left side of Figure 1, not the center. Even the language "more data hurt" is the wrong statement. The point isn t to show that collecting data is bad but to justify adding parameters. We should say "more parameters help". If the authors  proof technique added to the understanding in that case, then this paper would be more convincing. As is, I find it hard to overrule with the reviewers who appear to be mainly on the fence with little enthusiasm. 
The paper has good contributions to a challenging problem, leveraging a Faster RCNN framework with a novel self supervised learning loss. However reviewer 4 and other chairs (in calibration) considered that the paper does not meet the bar for acceptance. The other reviewers did not champion the paper either, hence i am proposing rejection.   Pros:   R1 and R3 agree that the proposed model improves over related models such as MONET.   The value of the proposed self supervised loss connecting bounding boxes and segmentations is well validated in experiments.  Cons:   R4 gives good suggestions that may be useful to reach a broader readership, namely introducing more of the concepts used in the paper., e.g. "stick breaking, spatial broadcast decoder, multi otsu thresholding" so it becomes more self contained. R4 also suggests improving the writing more generally.   R4 still finds the proposed "method quite complex yet derivative" after the rebuttal.   All reviewers complain about lack of experiments in real data, but the authors did revise their paper and add some coco results in the appendix. These could be part of the main paper in a future version.
The paper proposes formulating safety constraints as formal language constrains, as a step toward bridging the gap between ML and software engineering, and enabling safe exploration in RL.  The authors responded and improved the paper significantly during the rebuttal period. Despite that, the reviewers raise the question, and I agree, that the significance of the paper, especially the novelty of the method, do not meet ICLR standard. The future version of the paper should be developed more in terms of the novelty, evaluations, and related works.  
I thank the authors for their submission and very active participation in the author response period. World state tracking is an important problem that encompasses existing problems like coreference resolution. I agree with R2 and R3 that proposing a novel environment in which we can investigate to what extend Transformers can tackle world state tracking should be interesting to the community. The majority of the reviewers agree that this paper presents an interesting benchmark [R2,R3,R4] with good thorough experimental work [R1,R2,R4]. However, R1 is confused about the positioning of the work and R4 finds the work narrow. R2, despite positive review, agrees with this assessment. I agree with this assessment as well and, after discussion with the program chairs, came to the decision that this paper is not ready for publication in its current state. I strongly encourage the authors to incorporate R1 s and R4 s feedback, in particular with respect to positioning this environment in comparison to TextWorld, and resubmit to the next venue.
Most reviewers agree that the paper makes valuable contribution in analyzing single timescale actor critic algorithms. There were some doubts on the theoretical advantage over two timescale algorithms and the realizability assumptions, but the authors made satisfactory clarifications.   Therefore, acceptance is recommended, though I strongly suggest the authors to explicitly state key assumptions required to ensure global optimality in the abstract and introduction to avoid confusion.  
This paper compresses neural networks via so called Sparse Binary Neural Network designs. All reviewers agree that the paper has limited novelty. Experiments are only performed on small datasets with simple neural networks. However, even with toy experiments, results are very weak. There is no comparison with the SOTA. Numerous related works are missed by the authors. Besides, the paper is poorly written, and there are misleading notations.
This paper studies the patch based convolutional kernels for image classification, and finds that making the kernel dependent on data is necessary for designing competitive kernels for image classification. The proposed simple method shows comparable results to those end to end deeper architectures on CIFAR 10 and ImageNet datasets.   All reviewers feel that the paper is interesting, important, and the performance is impressive. During the rebuttal, the authors have addressed most of the questions and concerns raised by the reviewers. In particular, authors have clarified the motivation, discussed the model size of the proposed method (requested by R1), added precise details about the spectrum definition and intrinsic dimension (requested by R4), and taken the suggestions from all reviewers to improve their paper.   After rebuttal, all reviewers agree on accepting the paper. After checking the discussions between the authors and reviewers, I am convinced that the original concerns of the reviewers are addressed. Hence, I recommend that this paper be accepted. 
The paper introduces a new locality aware importance weighted sampling procedure for distributed training of GNNs. While the paper is interesting, the reviewers raised some fundamental concerns about it.  The focus on the paper is on scalable methods and the experiments or only run on medium size datasets(<2m nodes). For such a paper larger scale experiments are expected.  Furthermore, the novelty of the paper is limited.  Overall, the paper is below the high acceptance bar of ICLR.
This paper proposes a differentiable trust region based on closed form projects for deep reinforcement learning. The update is derived for three types of trust regions: KL divergence, Wasserstein L2 distance, and Frobenius norm, applied to PPO and PAPI, and shown to perform comparably to the original algorithms.  While empirically the proposed solutions does not bring clear benefits in terms of performance, as correctly acknowledged by the authors, it is rigorously derived and carefully described, bringing valuable insights and new tools to the deep RL toolbox. The authors improved the initial submission substantially based on the reviews during the discussion period, and the reviewers generally agree that the work is of sufficient quality that merits publication. To improve the paper and its impact, I would recommend applying the method to also off policy algorithms for completeness. Overall, I recommend accepting this submission.
The majority of reviewers recommend acceptance for this paper, and the average score is in the acceptance rate. Only one reviewer (reviewer 2) recommend rejection, and from the reading the review, the authors answer, and the paper, I think it is possible that the reviewer missed the motivation behind this architecture, which is partly reason for rejection. Unfortunately the reviewer did not answer the authors so I cannot be sure if the reviewer is aware of that. Therefore, I am confident in my recommendation to follow the majority of the reviewers.  The reviewers generally believe this paper is well written. The paper has a good structure and although quite technical, is still easy to follow.  Concerns were raised about the scale of the experiments and motivations for some experimental choices.  I appreciate that the authors have extended the set of evaluations on the Starcraft task, and added ablations studies, which partly address some of these concerns. 
Thank you for your submission to ICLR.  This paper had somewhat dissenting reviews, but three of four reviewers felt negatively about the paper.  On the positive side, the reviewers noted good motivation for the problem, a good ablation study and, in some cases, good performance over standard cross entropy.  On the negative side, the reviewers noted limited novelty, missing discussion or comparison to prior work, and in some cases only marginal improvement over existing methods.  The positive reviewer still remained positive after discussion, but noted that their confidence was low on the paper and that they would defer to others  opinions.  The other reviewers still had several concerns after the discussion phase.  Ultimately, it seems that the paper could use some additional work before it is ready for publication.  I would strongly encourage the authors to keep the reviewer comments in mind when preparing a future version of the manuscript.
Nice ideas with practical advantages.  
The authors have made significant efforts to thoroughly address all the concerns. Due to the amount of discussions, I had to go through the paper myself and agree with the authors on many of the points. In my opinion, this is a solid theoretical work on the pitfalls of IRM. 
This paper proposed an extension of the SIGN model as an efficient and scalable solution to handle prediction problems on heterogeneous graphs with multiple edge types.  The approach is quite simple: (1) sample subsets of edge types, then construct graphs with these subsets of edge types and (2) compute node features on each such graph as if they have only a single edge type, (3) then aggregate the representations from multiple graphs into one using an attention mechanism, and (4) train MLPs on node representations as in SIGN.  Results show that such a simple method can produce quite good results, and is very efficient and scalable.  The reviewers of this paper put it on the borderline, with 3 out of 4 leaning toward rejection.  The most common criticism is the lack of novelty.  Indeed this paper is an extension of prior work SIGN, and the proposed approach is simple.  However, I personally think the simplicity and the great empirical results is rather the strength of this paper.  The authors also did a good job addressing reviewers’ comments and concerns in the discussions, but a few reviewers unfortunately didn’t actively engage in the process.  I d really encourage the authors to improve and highlight the strength of this paper more and submit to the next venue.
This paper is a study in optimizing the Donsker Varadhan lower bound on mutual information focusing on a "drift" problem.  The bound is a difference between terms which appears to have an extra degree of freedom where the two terms increase or decrease together.  They propose a fix for this problem. The authors state that the DV bound is of practical value but in most cases it is replaced by discriminative lower bounds as in contrastive predictive coding (CPC) which are biased but have lower variance. The paper does not address the variance (convergence) issues with the DV bound.  We have a well informed reviewer who feels that the paper is not sufficiently novel and has other issues supporting rejection.  Other reviews are not very enthusiastic.  I will side with rejection.
This paper proposes a k NN smoothing procedure for dealing with the problem of churn prediction. The idea is interesting and is based on theoretical foundations. The reviews have raised some limitations in the significance and in the experiments. The rebuttal provided by the authors have addressed some concerns. However, the new experimental evaluation have raised new concerns about the results, in particular with respect to results given in Table 3. Some typo may exist, but even some doubts remain on the experimental evaluation and results and thus on the effectiveness of the results. Authors  rebuttal was too late to allow another round of discussion. Considering the current concerns and uncertainties on the paper, I have to recommend rejection.
The paper motivates the need for robustness, citing a paper on adversarial attacks. The type of perturbations are quite different (and of greater concern) than those originally included in the work, namely additive Gaussian or Laplace noise. This was raised by reviewers 1, 2 and 3.  The authors provided a detailed rebuttal in which they addressed many concerns of the reviewers. In particular, they included experiments with adversarial attacks. While these initial results seem interesting, the AC believes that they are too preliminary and it is not possible to evaluate their significance. The work should incorporate stronger baselines (other than plain networks) and consider more challenging forms of attacks (only FGSM attacks were studying). In particular, the paper should discuss where these ideas sit in the context of the literature and compare against conceptually related works that have been published in the area, such as  Xie, Cihang, et al. "Feature denoising for improving adversarial robustness." CVPR. 2019.  Without the adversarial perturbations, the impact of the work reduces significantly, as highlighted by reviewer 1, 2 and 3. In such case, it would be important to include stronger baselines in that setting as well. Please see the suggestions made by reviewer 1.  The authors satisfactorily addressed the problems in the derivation of the updates raised by Reviewers 1 and 4. However, new questions arise that would require careful consideration, as mention by reviewer 4 (which the authors could not answer as they were posted after the discussion period ended). This alone would not imply rejection, but suggests that the paper would be stronger after incorporating further feedback.  While this did not play a role in the decision, the AC suggests to also look at the literature around LISTA, in particular the work: Liu, Jialin, and Xiaohan Chen. "ALISTA: Analytic weights are as good as learned weights in LISTA." ICLR. 2019.  All four reviewers recommend rejecting the paper and did not change their position after reading the author s response. The AC agrees with this assessment. 
This paper proposes the c score, which is the aggregation of a "consistency profile" that measures per instance generalization.  Naive computation of the c score is expensive and thus requires an approximation.  The paper then uses the c score to analyze several image benchmarks and their learning dynamics.    While the reviewers found the experiments to be well done, their primary concern was over the novelty and ultimate usefulness of the c score.  As R1 and R4 point out, the c score correlates with other known measures such as accuracy and training speed.  The authors claim this is a contribution.  In turn, it is hard to tell if the c score is a true metric of interest or a recapitulation of what is already known.  No reviewer was in favor of acceptance.
The paper presents a significant body of seemingly solid work, but its contribution nevertheless feels limited: It evaluates a single MLM on a single dataset, and results are largely unsurprising. Note: The authors added experiments on other LMs in the rebuttal. The idea of using perturbations is related in spirit to many interpretability methods and adversarial techniques, and using higher order correlations for interpreting neural networks is, for example, at the heart of relational similarity analysis. A few suggestions to make the work more relevant to a wider audience: Compare with several probing techniques   e.g., in a tree decoding set up   or contrast results across domains (using OntoNotes), or across languages (using OntoNotes and other PTB style treebanks). Also: While results were added for multiple LMs, differences were not analysed in detail. 
This paper explores meta learning of local plasticity rules for ANNs. The authors demonstrate that they can meta learn purely local learning rules that can generalize from one dataset to another (though with fairly low performance, it should be noted), and they provide some data suggesting that these rules lead to more robustness to adversarial images. The reviews were mixed, but some of the reviewers were very positive about it. Specifically, there are the following nice aspects of this work:  A) The meta learning scheme has interesting potential for capturing/learning biological plasticity rules, since it operates on binary sequences, which appears to be a novel approach that could help to explain things like STDP rules.  B) It is encouraging to see that the learning rules can generalise to new tasks, even if the performance isn t great.  C) The authors provide some interesting analytical results on convergence of the rules for the output layer.  However, the paper suffers from some significant issues:  1) The authors do not adequately evaluate the learned rules. Specifically:    The comparison to GD in Fig. 2 is not providing an accurate reflection of GD learning capabilities, since a simple delta rule applied directly to pixels can achieve better than 90% accuracy on MNIST. Thus, the claim that the learned rules are "competitive with GD" is clearly false.    The authors do not compare to any unsupervised learning rules, despite the fact that the recurrent rules are not receiving information about the labels, and are thus really a form of unsupervised learning.    There are almost no results regarding the nature of the recurrent rules that are learned, either experimental or analytical. Given positive point (A) above, this is particularly unfortunate and misses a potential key insight for the paper.  2) The authors do not situate their work adequately within the meta learning for biologically plausible rules field. There are no experimental comparisons to any other meta learning approaches herein. Moreover, they do not compare to any known biological rules, nor papers that attempt to meta learn them. Specifically, several papers have come out in recent years that should be compared to here:  https://proceedings.neurips.cc/paper/2020/file/f291e10ec3263bd7724556d62e70e25d Paper.pdf https://www.biorxiv.org/content/10.1101/2019.12.30.891184v1.full.pdf https://proceedings.neurips.cc/paper/2020/file/bdbd5ebfde4934142c8a88e7a3796cd5 Paper.pdf https://openreview.net/pdf?id HJlKNmFIUB https://proceedings.neurips.cc/paper/2020/file/ee23e7ad9b473ad072d57aaa9b2a5222 Paper.pdf  And, the authors should consider examining the rules that are learned and how they compare to biological rules (e.g. forms of STDP), if indeed biological insights are the primary goal.  3) The paper needs to provide better motivation and analyses for the robustness results. Why explore robustness? What is the hypothesis about why these meta learned rules may provide better robustness? There is little motivation provided. Also, the authors provide very little insight into why you achieved better robustness and insufficient experimental details for readers to even infer this. This section requires far more work to provide any kind of meaningful insight to a reader. What was the nature of the representations learned? How are they different from GD learned representations? Was it related to the ideas in Theorem 4? Note: Theorem 4 is interesting, but only applies to a specific form of output rule.  4) In general, the motivations and clarity of the paper need a lot of work. What are the authors hoping to achieve? Biological insights? Then do some analyses and comparisons to biology. More robust and generalisable ML? Then do more rigorous evaluations of performance and comparisons to other ML techniques. Some combination of both? Then make the mixed target much clearer.  5) The authors need to tidy up the paper substantially, and do better at connecting the theorems to the rest of the paper, particularly for the last 2 theorems in the appendix. Also, note, Theorems 2 & 4 appear to have no proofs.  Given the above considerations, the AC does not feel that this paper is ready for publication. This decision was reached after some discussion with the reviewers. But, the AC and the reviewers want to encourage the authors to take these comments on board to improve their paper for future submissions, as the paper is not without merit.
This paper presents a variant of MAML or Reptile, where the meta update along the long trajectory of the inner loop optimization is bypassed to reduce the computational overhead appeared in MAML. The main idea is to use the look ahed optimizer with careful tuning of relevant hyperparameters, which is done by a teacher student scheme. Lazy MAML/Reptile are presented and experiments  demonstrated their validity. While the paper contains interesting ideas, most of reviewers have a few concerns which are not even resolved even after the author responses. First of all, ResNet analogy with respect to teacher update was claimed but it was never clearly shown in the paper. The method needs careful tuning of hyperparameters in the inner loop, but  the study about the computation requirements is not convincing yet.  Long inner loops are computationally feasible for both fomaml and reptile so its not clear in which way the proposed method is improving lengthy exploration in the inner loop other than the performance being better in the experiments. Improving the paper, taking these comments into account, will lead to a good work in near future. 
The authors propose an RL based approach, “Rewriting by Generating (RBG)”, to solve large scale capacitated vehicle routing problems (CVRPs): such problems are NP hard in general and are ubiquitous. The RL agent consists of a "Generator" and "Rewriter". In generation, the graph is sub divided into several regions and in each region, an RL algorithm runs to get the best (or near optimal) route. The rewriter then patches these near optimal sub solutions together using “hierarchical RL”.  The paper is generally well written.   One main concern is related to generalizability: the authors respond that their approach can work for other NP hard combinatorial optimization problems such as knapsack. The authors are encouraged to do a systematic study of several such (related) problems where their approach can work. It was also a concern that the overall approach of partitioning the input instance and rewriting the CVRP solution by merging regions and recomputing routes, is also employed by commercial OR solvers. The authors are encouraged to do a careful comparison (and perhaps melding) with such available solvers, to get a hybrid “OR + ML” improvement. It is also suggested that the authors include several different constraints from real world VRP (e.g., heterogeneous vehicle costs, costs of missed shipment, route limits, upper bounded number of vehicles etc.).  
This paper proposes an interesting method for combining retrieval based models and graph neural networks for source code summarization. Finding new ways of bringing in additional context for graph based models is an important research direction in this space, and the paper presents a novel and effective approach. The initial submission was missing experiments on existing benchmarks, but new experiments presented in the discussion phase are enough to resolve that concern. Reviewers are unanimously in support of acceptance. 
This paper received borderline scores but overall lean positive.   The reviewers point out that the paper presents interesting new ideas and an effective solution to the problem of automatically searching for loss functions. The empirical results are convincing, although the baselines are not the strongest possible in terms of absolute performance. Overall, the ACs find that the paper has sufficient novelty and technical contribution to be accepted. 
The reviewers had some initial concerns about this submission. While the authors  rebuttal does a good job to address these concerns, the reviewers still have some doubts about the contribution of this paper and potential impact. In particular, it is not clear whether the performance improvements observed with the proposed algorithms is due to the ability to correct for noisy rewards or whether there are multiple other explanations for the improvement in performance. This makes it hard to predict whether the proposed algorithms will actually be useful in settings where noisy rewards or demonstration data are present.
This paper proposes information theoretic quantification of epistemic uncertainty in autoregressive models.   This is a difficult problem that receives much less attention than the unstructured case. The paper is well written, contributes novel and tractable to estimate measures which are analysed formally and empirically with convincing experiments on ASR and NMT.   The reviewers and myself are overall pleased by this submission. The discussion phase went well and most concerns have been resolved.  
**Problem Significance**  This paper introduces an interesting taxonomy of OODs and proposed an integrated approach to detect different types of OODs. The AC agrees on the importance of a fine grained characterization of outliers given the large OOD uncertainty space.   **Technical contribution** The key idea of the paper is to combine the predictions from multiple existing OOD detection methods. While the AC recognizes the effort made by the authors to address the review comments, reviewers have several major standing concerns regarding limited contributions, insufficient analysis, and lack of clarity. The AC agrees with reviewers that the paper is not ready yet for ICLR publication, and can be further strengthened by:    (R1) reporting the computational cost for the integrated approach. The inference time for approaches such as Mahalanobis is typically a few times more expensive than the MSP baseline. The cumulative time for calculating all four scores may be non negligible. Authors are encouraged to analyze the performance tradeoff in a future revision.    (R2 & R3) discussing the effect of hyper parameters tuning, which be overly complicated in practice as it involves combinations of multiple methods that each have multiple parameters to tune.    (R3) comparing with more recent development on OOD detection and move the new results to the main paper. The AC also thinks it s worth discussing the connection and comparison to methods on quantifying uncertainty via Bayesian probabilistic approaches.   (R2 & R4) more rigorous analysis of the benefits of the proposed integrated approach, both empirically and theoretically. Based on Table 7, the performance of using Mahalanobis alone is almost competitive as the proposed approach (except for the CIFAR10 CIFAR100 pair). This may deem further careful examination to understand what value other components are adding, and in what circumstance.    (R2, R3 & R4) More discussion on the implication of the taxonomy and algorithm in the high dimensional space would be valuable. The 2D toy dataset might be too simple to reflect the decision boundary as well as uncertainty space learned by NNs. Moreover, it s important to justify further how aleatoric and epistemic uncertainty manifests in the current experiments using NNs. For example, epistemic uncertainty can arise due to the use of limited samples or due to the model uncertainty associated with the model regularization.   Recent work by Hsu et al. [2] also attempt to define a taxonomy of OOD inputs (based on semantic shift and domain shift), which can be relevant for the authors.   **Recommendation** Three knowledgeable reviewers have indicated rejection. The AC discounted R4 s rating due to the less familiarity in this area and lack of participation in the post rebuttal discussion.   [1] Richard Harang, Ethan M. Rudd. Towards Principled Uncertainty Estimation for Deep Neural Networks [2] Hsu et al. Generalized ODIN: Detecting Out of distribution Image without Learning from Out of distribution Data 
+ Interesting method for binaural synthesis from moving mono audio + Nice insight into why l2 isn t the best loss for binaural reconstructions.  + Interesting architectural choice with nice results. + Nicely motivated and clearly presented idea   especially after addressing the reviewers comments.  I agree with the idea of a title change. While I think its implied that the source is probably single source, making it explicit would make it clearer for those not working in a closely related topic. Hence, "Neural Synthesis of Binaural Speech from Mono Audio" as suggested in the review process sounds quite reasonable. 
In this paper, a defense method against test time adversarial ML attacks is proposed. Unfortunately, it is not clear whether the proposed method is practically useful or not, because the types of attacks assumed in this paper are too simple and heuristic. Also, the position of the proposed method in the vast amount of existing research on adversarial attacks is not clear. Although the proposed method is conceptually interesting, no evidence is provided that the proposed method is significantly superior to existing approaches.
This paper evaluates the extent to which disentangled representations can be recovered from pre trained GANs with style based generators by finding an orthogonal basis in the space of style vectors, and then training an encoder to map images to coordinates in the resulting latent space. To construct the orthogonal basis, the authors consider 3 recently proposed methods for controllable generation, along with a newly developed generalization of one of these methods. The authors evaluate metrics for disentanglement for 4 datasets, consider an abstract visual reasoning task, and compute unfairness scores.  Reviewers expressed diverging opinions on this paper. R2 is in support of acceptance,  R3 finds the paper borderline but is leaning towards acceptance, whereas R4 is critical. R2 and R4 engaged in a relatively detailed discussion, but maintained their scores.   Having read the paper, the metareviewer feels this submission indeed has strengths and weaknesses. On the one hand, the main results are notable; it is worth reporting that disentangled representations can be recovered from pretrained GANs is a relatively straightforward manner. In this context, the metareviewer feels that some comments by R4 are more critical than is warranted. The authors do not necessarily have to show that GAN based methods uniformly improve upon VAE based methods, either in terms of disentanglement metrics or in terms of sensitivity to hyperparameters. The main claim in this submission is that GAN based methods are mostly comparable to VAE based methods, and this claim is both sufficiently notable and sufficiently supported by experimental results.   At the same time, this submission is not without flaws.  The writing is on the rough side, and as R4 notes the authors have removed all white space between paragraphs. The metareviewer also feels it is not satisfactory to show a box plot for GAN based methods in Figure 2 and ask the reader to compare these plots to the violin plots for VAE based methods in the Locatello paper. The authors need to find a way to make a more direct comparison here. R4 s comments about the comparison in the abstract reasoning setting are also well taken –– here the baseline employs standard (entangled) models, so it is unclear what conclusions we should draw from this experiment. Similarly the unfairness results once again appeal to an indirect comparison to results in the  Locatello paper on this topic.  On balance, the metareviewer is inclined to say that this submission, in its current form, falls just below the threshold for acceptance. These results are clearly of note to the community and worth reporting, but the presentation has enough flaws that another round of reviews is warranted based on a revised manuscript. The metareviewer hopes to see this paper appear a conference in the (near) future. 
The paper presents a novel method for learning with noisy labels based on an interesting insight into the learning dynamics of deep neural networks.   Reviewers unanimously vote for acceptance. I agree with their assessment, and it is my pleasure to recommend the paper for acceptance.   If I can draw attention to one comment, I strongly agree with R1 that the criterion in Eq. (3) is somewhat poorly motivated. I believe the paper would benefit from a clearer exposition of this part.   Please make sure to address all reviewers  remarks in the camera ready version. Thank you for submitting your work to ICLR.
The paper studies offline meta reinforcement learning. Overall the scope of this contribution seems limited. Reviewers have raised concerns about the significance of the presented results given the assumptions, and that the experimental environments are not extensive and do not fully support the claimed advances. 
This paper presents a deep RL algorithm to handle tasks where rewards can differ greatly in magnitude.  The proposed solution decomposes the reward into a set of exponentially sized bins with a thermometer encoding, and computes a weighted sum of the value functions learned for each bin.  The approach addresses the common tactic of reward clipping and value rescaling in deep RL algorithms.  The experiments demonstrate the potential utility of this approach on artificially constructed Atari games, and the experiments also show the approach remains competitive on six standard Atari games.    The reviewers found both strengths and weaknesses in the paper.  The overall approach was viewed as a clear and sensible (R1, R2, R4) approach to handling widely varying reward scales in a domain.  It may be a useful contribution in a manner similar to other methods that make deep RL algorithms more robust to scaling issues encountered in practice (R4).  The main concerns were whether this was solving a real problem or not (R2, R3), and the lack of a theoretical development for the multiple heuristics (R2,R3).    The author response then simplified the algorithm, which also served to clarify which aspects of the algorithm were relevant to the performance improvements. The response removed some of the heuristics (mixing in Monte Carlo returns) and changed other choices to be more principled ($1/\sigma^2$). The author response also described how the proposed algorithm addressed different scaling concerns from those handled by earlier methods. The author response also provided clarifications to many minor questions raised by the reviewers.  In the ensuing discussion, the reviewers were happy with the revised paper.  Though some minor theoretical reservations remained, the reviewers agreed this paper was a useful contribution.  The reviewers indicate to accept the paper as a useful contribution in deep RL to address certain reward scaling issues.  The paper is therefore accepted. 
There are many recent methods for the formal verification of neural networks. However, most of these methods do not soundly model the floating point representation of real numbers. This paper shows that this unsoundness can be exploited to construct adversarial examples for supposedly verified networks. The takeaway is that future approaches to neural network verification should take into account floating point semantics.  This was a borderline paper. On the other hand, to anyone well versed in formal methods, it is not surprising that unsound verification leaves the door open for exploits. Also, there is prior work (Singh et al., NeurIPS 2018) on verification of neural networks that explicitly aims for soundness w.r.t. floating point arithmetic. On the other hand, it is true that many adversarial learning researchers do not appreciate the value of this kind of soundness. In the end, the decision came down to the significance of the result. Here I have to side with Reviewer 1: the impact of this problem is limited in the first place, and also, the issue of floating point soundness has come up in prior work on neural network verification. For these reasons, the paper cannot be accepted this time around.
After a bit of discussion, all reviewers are for accepting the paper.  Strengths: + Clarity (agreed by R4, R2, R1). The paper is easy to read and follow the core contributions. R3 had a concern about the correctness of a derivation, which was resolved in the discussion. + The work solves a core problem of generative models over multimodal applications, building on prior work with mixture and product of expert models. As R1 notes: "By combining MVAE and MMVAE under one framework, this may provide insights to researchers in this area." + On the datasets studied, the details for reproducibility are transparent, and multiple metrics and uncertainty over the metric results are reported.  Weaknesses: + Multi modality of the benchmarks. The experiments evaluate on MNIST SVHN, "PolyMNIST", and CelebA. The latter two benchmarks are fairly arguable in whether they re really multimodal as R4 notes: e.g., CelebA has two "modalities" of image and attribute pairs. It seems arguable whether you even need multimodal approaches. + Scale of the benchmarks. Language models (especially with Transformer architectures) have been studied quite a bit over multiple modalities, and these works scale significantly better applying simple strategies. It remains to be seen empirically what the utility of multimodal latent variable models really are.
This paper received mixed reviews, 3 positives (7, 6, 6) and 2 negatives (4, 4). Due to the divergence of the reviews, I carefully read the paper and made my best efforts to understand the paper and the review comments. This paper proposes to learn a quantization network using a small calibration set given a network trained with the full precision. The combination of AdaQuant, integer programming, and batch norm tuning makes sense although they do not have substantial novelty. The three components are reasonably tightly coupled and comprise a complete algorithm. However, the sequential AdaQuant distracts the main claim of this work significantly. This is probably added during the review process but looks ad hoc to me. Sequential AdaQuant seems to be effective to improve accuracy, but cannot be applied before the bit allocation was set, which makes it require integer programming no more. Because of this issue, the overall presentation becomes confusing and the argument sometimes sounds unfair (please refer to the last posting by R5.).   In addition, the presentation of this paper could be improved, especially for the details of the integer programming formulation. It is not clear how to define some variables mathematically. The discussion about the size of the calibration set together with the overfitting issue is lacking, and rigorous discussion and analysis would make the paper much stronger. The reviewers are not convinced of the novelty of this paper, and they rather believe that this is an engineering oriented work. Considering this fact,  the evaluation of this paper is not very comprehensive. The ablation study with respect to the size of the calibration set should be conducted more intensively. The experiment fails to show the benefit of mixed precision quantization effectively and it is limited to presenting the compression ratio in Figure 3. The authors used a small calibration set taken from the training dataset, which looks weird because they claim that the post training quantization requires only a small "unlabeled" calibration set at the beginning of the abstract; it is more desirable to use arbitrary examples in the same domain.  Despite the interesting aspects, I believe that this paper needs a focus and substantial improvement for publication, and, consequently, recommend rejection.
As the title states (and reads somewhat like an openreview review title), the authors apply the options framework from the RL community to perform hierarchical RL where the option is the dialogue act and the subproblem is the NLG component in task oriented dialogue (TOD) policy learning. The two technical contributions (beyond the conceptual connection above) is showing that asynchronous updates between the hierarchy levels guarantees convergence and language model based discriminator to densify the reward structure. Empirical results are solid improvements over recent SoTA findings.     Pros   + This is a conceptually appealing application of RL to TOD and they authors had to make additional modifications to get it to work — which will help other researchers in this space. + There are both theoretical and empirical contributions. The theoretical contributions are also insightful and not superfluous to the problem being studied. + Using a language model based discriminator for reward shaping isn’t completely new (although I haven’t seen in this setting and stated exactly the same), but is interesting and effective.    Cons    + The writing could use significant work; while the reviewers/rebuttal cleared up many issues, I actually didn’t appreciate the value of this paper in my first read due to the writing (even if the motivation, etc. is sufficiently clear). + Human evaluation is treated as somewhat of an afterthought and there isn’t a deep dive into error analysis of the results. The visualization is a good first step, but there isn’t really a when/why this method works better than others, which is important for a problem where evaluation isn’t conclusive in the best cases. This is also significant since the authors claim ‘comprehensibility’.  Evaluating along the requested dimensions:    Quality: The conceptual and theoretical contributions are both of high quality. This is a promising approach to TOD and the authors additions (e.g., async optimization, LM reward shaping) are good examples of applied research. The empirical results are sufficient to above average, but not as strong (although this is partially an artifact of TOD evaluation)    Clarity: The motivation is good, but the paper could use some work in writing. Some examples include (1) stating precisely how the option choices are derived (latent variables), (2) mapping out notation in something like a preliminaries section, (3) sketch of proofs in the main body for continuity. If the reader is familiar with the closest cited work, it is a bit easier, but I think some effort in making the paper more self contained would increase its impact.   Originality: Options in HRL is widely known, but applying it to TOD is novel to the best of my (and the reviewers) knowledge. I think many could have come up with the basic idea, but it took some effort to get it to work.   Significance: This is a widely studied problem and the approach is fairly convincing. I don’t think it will be ‘disruptive’ or cross pollinate to other application areas, but will almost certainly be cited within the conversational agent community.  In summary, the reviewers like this paper a bit more than myself personally — I think it is borderline with a preference to accept while the reviews are a more confident accept. However, the reviewers are also experienced experts in this area. I also do think that the authors handled concerns well in the rebuttal stages and addressed my more pressing concerns. I would encourage the authors to improve the writing if accepted, but I would prefer to accept this if possible. 
Four reviewers have reviewed this paper and after rebuttal, they were overall positive about the proposed idea. We congratulate authors on the paper.
The paper offers novel insights about memorization, the process by which deep neural networks are able to learn examples with incorrect labels. The core insight is that late layers are responsible for memorization. The paper presents a thorough examination of this claim from different angles. The experiments involving rewinding late layers are especially innovative.  The reviewers found the insights valuable and voted unanimously for accepting the paper. The sentiment is well summarized by R2: "The findings of the paper are interesting. It shows the heterogeneity in layers and training stage of the neural net".  I would like to bring to your attention the Coherent Gradients paper (see also R1 comment). This and other related papers already discusses the effect of label permutation on the gradient norm. Please make sure you discuss this related work. As a minor comment, please improve the resolution of all figures in the paper.   In summary, it is my pleasure to recommend the acceptance of the paper. Thank you for submitting your work to ICLR, and please make sure you address all remarks of the reviewers in the camera ready version. 
This paper analyses linear regions in ReLU networks using a new algorithm for extracting linear terms based on the data. The reviewers found the paper to be well written with sound results. While the paper itself provides only modest evidence of the algorithm’s utility (mainly in terms of highlighting some distinctions between fully connected and convolutional networks), the algorithm and the corresponding new paradigm of exploring linear terms rather than counting regions may prove useful in future analyses. Altogether, I think this paper will interest theorists focusing on ReLU networks and I recommend acceptance.
The authors propose training free neural architecture search using two theoretically inspired heuristics: the condition number of the Neural Tangent Kernel (to measure "trainability" of the architecture), and the number of linear regions in the input space (to measure "expressivity"). These two heuristics are negatively and positively correlated with test accuracy, respectively, allowing for fast, training free Neural Architecture Search. It is certainly not the first training free NAS proposal, but achieves competitive results with much more expensive NAS methods.  A few reviewers mentioned limited novelty of the method, a claim with which I agree. The contribution of the paper, however, is something different than how it was presented. The core message seems to be that the two proposed heuristics can greatly speed up NAS, and should be a baseline method against which more expensive methods should test.  I feel like this is a borderline paper, but may be of interest to researchers in the field.
This paper studies synthetic data generation for graphs under the constraint of edge differential privacy. There were a number of concerns/topics of discussions, which we consider separately: 1. Theoretical contributions. There are not that many theoretical contributions in this paper. I think this is OK, if the other components are compelling enough. On the theory, the authors mention that accounting for the constants is important in the analysis of DPSGD. On the contrary, I would say that these constants are not very important: if one requires specific constants, numerical procedures can determine values, otherwise for the sake of theory, no one generally needs these constant factors.  2. Empirical/experimental contributions. This was the primary axis for evaluation for this paper. None of the authors were especially compelled by the results. The methods are essentially combinations of known tools from the literature, and it is not clear why these are the right ones to solve this problem in particular. If the results were very exciting, that might be sufficient to warrant acceptance, but it is still not clear how significant the cost of privacy is in this setting. The experiments are not thorough enough to give serious insight here. It is a significant oversight to not provide results on DPGGAN without the privacy constraint, as this is the best performing model with privacy. The omission of something as important as this (and lack of inclusion in the response, with only a promise to include later) is indication that the experiments are not sufficiently mature to warrant publication at this time. The decision of rejection is primarily based on concerns related to the empirical and experimental contributions.  3. Privacy versus link reconstruction. Reviewer 4 had concerns about the notion of privacy, claiming that it does not correspond to the probability of a link being irrecoverable. This is differential privacy "working as intended", which is not intended to make each link be irrecoverable: it is simply to make sure the answer would be similar whether or not the edge were actually present, so it may be possible to predict the presence of an edge even if we are differentially private with respect to it (e.g., the presence of many other short paths between two nodes are likely to imply presence of an edge). Some discussion of this apparent contradiction might be warranted, as this might mislead reader who are specifically trying to prevent edge recovery. It might also be worthwhile to have discussion of node DP in the final paper. The authors comment "we focus on edge privacy because it is essential for the protection of object interactions unique for network data compared with other types of data"   the stronger notion of node differential privacy might also be applicable here. It would indeed be interesting to know whether it can preserve the relevant statistics (some of which seem more "global" and thus preservable via node DP).  
This paper presents an approach for mitigating subgroup performance gap in images in cases when a classifier relies on subgroup specific features. The authors propose a data augmentation approach, where synthetically produced examples (by GANs) act as instantiations of the real samples in all possible subgroups. By matching the predictions of original and augmented examples, the prediction model is forced to ignore subgroup differences encouraging invariance. The proposed method of ‘controlled data augmentations’ (as precisely called by R4) is relevant and well motivated, the theoretical justifications support the main claims, and the experimental results are diverse and demonstrate merits of the proposed approach. As rightly pointed out by R3, ‘The appendices are also very thorough, and the code is organized well’.  In the initial evaluation, the reviewers have raised (in unison) concerns regarding overlapping subgroups per class, and an imbalance problem in the subgroups when training GANs. There were also questions reg. theoretical justifications, and empirical evaluations of the baseline methods. The authors have addressed all major concerns in the rebuttal. Pleased to report that based on the author respond with extra experiments and explanations, R2 has raised the score from 6 to 7. In conclusion, all four reviewers were convinced by the author’s rebuttal, and AC recommends acceptance of this paper – congratulations to the authors!  There is a colossal effort in the community addressing a goal similar to this work – learning invariant representations w.r.t. sensitive features by means of algorithmic fairness methods. (R1 and R3 relate to it). When preparing the final version, the authors are encouraged to elaborate more on the discussion/comparison to fairness based methods, ideally including empirical evidence where possible (where subgroups overlap, e.g. CelebA). The AC believes this will strengthen the final revision and will have an even broader impact in the community. 
 The paper considers the risk sensitive RL by exploiting entropic risk. The major contribution of this paper is providing the theoretical guarantees for the proposed risk senstive value iteration with function approximation.   The major concern of this paper is the similarity to the existing work in (Fei et al., 2020). I encourage the authors to reorganize the paper and emphasize the differences to highlight the major contribution. 
The paper proposed a meta learning method for tuning the learning rate. In the discussion, reviewers agreed that the key issue is that the empirical evaluation is not yet sufficient to demonstrate the efficacy of the method. In particular, this is an especially pressing issue given that there are now many meta learning methods for tuning the learning rate (none popular in practice though), and the paper does not compare to any of them. Relatedly, most reviewers found that the novelty of the method is not clearly established and discussed in the paper.   Based on the above, I have to recommend rejecting the paper. I would like to thank the authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work. 
 In this paper, the authors proposed the expected quadratic utility maximization (EQUM) to implement the risk aware decision making for mean variance RL. The EQUM framework directly optimizes the weighted sum of first and second order moments, while ignores the square of first moments, and thus, largely reduces the difficulty in optimization. The authors tested the proposed policy gradient based algorithm empirically.   However, the connection to classic mean variance is not clearly by simply ignoring the square of first moments in the objective theoretically (R2, R3, R4). Meanwhile, the effect of tunable weights (\psi) is not clear and consistent empirically (R2, R3, R4).    As all reviewers agree this paper is interesting and promising, I encourage the authors to address these issues and consider to submit to next venue.  
The paper presents a scalable data poisoning algorithm for targeted attacks, using the idea of designing poisoning patterns which "align" the gradients of the real objective and the adversarial objective. This intuition is supported by theoretical results, and the paper presents convincing experimental results about the effectiveness of the model.  The reviewers overall liked the paper. However, they requested a number of clarifications and some additional work, which should be incorporated in the final version (however, the authors are not required to use the wording as poison integrity/ poison availability). In particular, it would be great to see the experiment the authors suggested in their response to Reviewer 2 about the effectiveness of their method for multiple targets (this is important to better understand the limitations of the proposed approach).
The authors provide a new analysis of learning of two layer linear networks with gradient flow, leading to some novel optimization and generalization guarantees incorporating a notion of the imbalance in the weights.  While there was some diversity of opinion, the prevailing view was that the results were not sufficiently significant for publication in ICLR.
The reviewers appreciated that the paper was clear and well written. They also appreciated that the paper has been largely improved during the discussions. The results seem to support the claim and the experiments on Minecraft are convincing.   Yet, the reviewers had some important concerns. First the focus on RUDDER seems too strong and the method doesn t seem to be that much related to RUDDER. Presenting the work as a trajectory matching method seems more appropriate. In addition, the authors support their choice of referring to RUDDER because it comes with theoretical guarantees. But RUDDER s guarantees come from the usage of a modified LSTM while Align RUDDER doesn t use an LSTM.   The hierarchical approach was also questioned as the way to switch between different sub policies is not very well explained in the paper. Baselines wrt to the switching method could not be provided. Similarly, the structure of the Minecraft task seems to be used heavily to define the hierarchy and meta planning, so more baselines (with less structured tasks) were requested.   The method also suffers from scalability issues as the authors acknowledge that if the number of events grows, they would need to downsample the events so as to apply their method. 
The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. The resulting idea is simple but also surprising that it works so well. All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. In addition, they identified the following strengths of the paper:   The experiments are exhaustive, identifying many domains where the approach can be applied   The presented results are compelling   The paper is well written   The paper introduces a new problem setup that has not been studied before  I agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.  
This paper describes a new and experimentally useful way to propose masked spans for MLM pretraining, by masking spans of text that co occur more often than would be expected given their components   ie that are statistically likely to be non compositional phrases.  The authors should make some attempt to connect their PMI heuristic with prior methods for statistical phrase finding and term recognition, eg https://www.aaai.org/Papers/IJCAI/2007/IJCAI07 439.pdf or https://link.springer.com/chapter/10.1007/978 3 540 85287 2_24 in the final paper.
The authors propose a "jumpy RNN" to adaptively change the step size of an RNN to match the time scales of the system dynamics. Reviewers found merit in the simple and intuitive idea, but were less enthusiastic about the experimental results and the comparison to existing work. (Adaptive step size methods have been a subject of recent work in RNNs, not to mention in numerical methods for ODE solvers.) Overall, I think the additions the authors made in the discussion phase did strengthen the paper, but further work is necessary before publication. 
The authors address the problem of learning environment invariant representations in the case where environments are observed sequentially. This is done by using a variational Bayesian and bilevel framework.  The paper is borderline, with two reviewers (R2 and R3) favoring slightly acceptance and two reviewrs (R4 and R1) favoring rejection.  R4 points out that the current experiments do not do a good job of reflecting a continual learning setup and that simple modifications on existing IRM based methods could outperform the method proposed by the authors. The authors are encouraged to take into account the reviewer s suggestions to improve the paper.  R1 argued initially that the proposed solution is not learning at all since it has errors very close to random guessing. While the authors have improved their method in the revision, the results are still close to random guessing, which questions the practical usefulness of the proposed approach. Also, in the revision, the authors managed to obtain better results when their method is combined with Environment Inference for Invariant Learning (EIIL), but these results are secondary and not the main part of the paper.  The authors should improve the work taking into account the reviewrs  comments.
This paper sits right at the borderline: the reviewers agree that it is interesting and addresses a relevant problem. On the negative side, the presentation could be improved (including some incorrect claims), and the experiments could be strengthened (both in terms of baselines and datasets used). Ultimately, the paper will probably require another round of reviews before it is ready for publication.
The paper proposes a new multimodal neuro symbolic technique for synthesizing programs. The specification is given in natural language (soft constraints) and input output examples (hard constraints). The multimodal program synthesis is formulated as a constrained maximization problem where the goal is to find a program maximizing the conditional probability w.r.t. the natural language specification while satisfying the input output examples. The proposed technique is evaluated on a multimodal synthesis dataset of regular expressions, and significant performance gains are shown w.r.t. the state of the art synthesis methods. Overall this is an important direction of research, and the paper presents significant results in the space of multimodel program synthesis.  I want to thank the authors for actively engaging with the reviewers during the discussion phase. The reviewers generally appreciated the paper s ideas; however, there was quite a bit of spread in the reviewers  assessment of the paper (scores: 4, 5, 6, 7). In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should more clearly describe the paper s primary contributions, compare their technique with related work that combines neural generation approaches with deductive methods, and simplify the presentation of technical sections. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers  feedback when preparing future revisions of the paper.
This paper proposes a method for bilingual lexicon induction. The proposed method is efficient, it optimizes a reconstruction and transfer loss. Extensive experiments are reported, and the methods provides improvements over prior work. Overall, the paper brings together prior ideas in a useful way.
The paper proposes an upsampling layer design for converting layouts to images. Three reviewers rate the paper below the bar, while one reviewer rates the paper marginally above the bar. The main concern that several reviewers raise is the novelty. Particularly, R1 and R3 point out that the proposed method shares great similarity to CARAFE [Wang et al. ICCV 2019]. The AC agrees with the reviewers. 
While this paper would be significantly improved with experiments on real data, the reviewers all agreed that there is value in the ideas and simple experiments in this paper and all voted for acceptance after the discussion period.  We encourage the authors to consider adding an experimental evaluation in more realistic settings (e.g. with real data) in the final version of the paper.
In this paper, the authors propose a theoretically principled neural network that inherently resists ℓ∞ perturbations without the help of adversarial training. Although the authors insist to focus on the novel design with comprehensive theoretical supports, the reviewers still concern the insufficient empirical evaluations despite the novel idea and theoretical analysis.
The paper presents an evolutionary optimization framework for training discrete VAEs, which is different to the standard way of training VAEs. One of the main criticism of the paper was the choice of experiments, but the authors addressed this point by adding an inpainting benchmark.  Unfortunately, the reviewers  scores are borderline, and one of the reviewers pointed out the lack of scalability (more precisely, linear scalability with the number of observations) and cannot recommend acceptance based on the limited application impact. Given the large number of ICLR submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue.
The paper aims to provide a framework for learning non linear feature mappings such that are invariant to environments. The critical concern raised by the reviewers is their assumption: that causal features of the label are conditionally independent given the label. But in any DAG, conditioning on a common child (here, the label) renders the parents dependent. Their assumption thus is not going to hold other than on a measure zero set of parameters.
The paper addresses the problem of learning and exploiting common (latent) task structure in multi task reinforcement learning settings. The authors introduce a new formalism for capturing this type of structure and derive a gradient based learning algorithm. They provide novel theoretical insights and strong empirical results.  Reviewers initially raised several concerns, regarding assumptions and especially accessibility of the paper (and in particular theoretical discussions). The majority of these concerns have been addressed in the detailed rebuttal. The resulting consensus is to accept the paper. Authors are encouraged to continue to improve accessibility of the paper for the camera ready submission.
 The question the authors address is relevant and interesting mostly in the UDA setting. However, there exists several recent works that have  highlighted the importance of label distribution ratio in DA (Wu et al., Combes et al. etc.), hence the main contribution of the paper is to propose a novel analysis and results in the multi source setting. That said, the paper has mixed reviews and after going through the paper, the reviews and the discussion, I tend to agree with some of the reviewers that while  the idea is interesting, the paper lacks in several points that makes it unsuitable to publication, for now.  Here are the main points leading to the decision.   A) UDA is usually the most frequent situation that occurs in domain adaptation and the most difficult to handle.  The theoretical novelty of the bound comes only from the multi source aspect that seems to be original  B) there is a strong contradiction in the paper. In the intro, they state that the paper addresses situations where conditional distributions differ. However in 4.2 they assume that they are finally equal.  In section 4.1, the authors show that for optimizing their problem, they need to have labels, mostly for estimating the class conditional distributions. When these labels are available in the target domain, the problem is pretty simple and there exists many simple baselines that can handle this problem. However, in a UDA setting, they do not have label and proposes a method for estimation label proportion by assuming S_t(z|y)   T(z|y), which is in contradiction with their initial hypotheses S_t(z|y) !  T(z|y). Hence under their assumption, the left hand side of Lemma 1 is zero and the equality is useless. Hence, I would suggest the authors to avoid such a contradiction.  Under equality of S_t(z|y)   T(z|y), the approach proposed by the authors bears strong similarity with the work of Redko et al 2019 (cited in their paper). So I would highly to recommend them to compare with that algorithm. .   C) the authors use a lot a trick related to filtering, moving average.... I guess those parts is important for making the approach works and they are not properly analyzed.  D) The paper is  confusing in its writing and somehow this confusion makes the theoretical details hard to understand. For instance, in section 3 the loss function is defined as having two variables but used one line after with only 1. In the theorem, it is not clear whether the true labelling function intervenes or how the y in h(x,y) is related to the true labels. I guess a clarification is needed here for making the soundness of the theoretical results.
This paper presents a method named LowKey, which is designed to protect user privacy. This is done by taking advantage of adversarial attacks to pre process facial images against the black box facial recognition system in social media, yet the processed facial images remain visually acceptable. The paper experimentally illustrates that it is effective against two existing commercial facial recognition APIs.   The reviewers unanimously agree that this is an interesting and important problem, and recommend the paper for acceptance. The ACs agree.
Reviewers agree that this work is promising. The paper is well grounded in the literature and different aspects of the considered methods are investigated through a variety of experiments. Unfortunately, this paper does not provide sufficient details to allow the reader to understand what has been done nor how to adequately build from it. For example, details in the Appendix lack sufficient formalization of the equations or concepts used to train the preference based agents. The paper would benefit from clarifications of the method, procedures, and equations used. Beyond that, a major concern lied within the evaluation of the simulated patients across different initializations. Provided that one of the proposed contributions of this paper is a robust simulation platform for RL research within healthcare, it would be important to report convincing results on the patient physiologies admitted by the simulator and characterizing the behaviors of policies learned using this simulator. Finally, issues regarding the structure of the paper, including the split between the main paper and the Appendix, should be resolved before this paper can be published. Notably, the authors should consider elevating important material from the Appendix into the main paper.
This paper proposes a method for regularizing the pre training of an embedding function for relation extraction from text that encourages well formed clusters among the relation types. Experiments on FewRel, SemEval 2010 Task 8, and a proposed FuzzyRed dataset show that the proposed prototype method generally outperforms prior state of the art, including MTB (Soares et al., 2019), which was the strongest. The key, novel idea is to model prototype representations for target relations as part of the learning process. A contribution of the work is to show that learning prototype representations are useful in supervised deep learning architectures even beyond few shot learning. This additional learning objective is useful as an inductive bias, and is perhaps of interest even beyond relation extraction research.  Reviewers generally found the proposed method sound and intuitive, and the original set of experiments promising. Some of the reviewers raised concerns about the setup of the experiments, including the relationship between the pre training and target tasks, and the need for several additional baselines. The authors were able to address these concerns, and the reviewers did not raise any follow up concerns.
The paper studies a high order discretization of the ODE corresponding to Nesterov s accelerated method, as introduced by Su Boyd Candes. The main claim of the paper is that the more complex discretization scheme leads to a method that is more stable and faster. However, the theoretical claims do not seem sufficiently supported, and the experimental results are insufficient to judge the usefulness of the proposed approach. Thus, the reviews could not recommend acceptance, and I concur. The authors are advised to revise the paper to provide more theoretical and experimental evidence for usefulness/competitiveness of the proposed approach, and resubmit to a different venue. 
This paper focuses on disentangled representation learning from multi view data, which is an interesting and hot topic. However, there are several papers published in the last couple of years (especially in NeurIPS2020 and ECCV2020) solving very similar problems with closely related contributions to this paper. The contributions of this paper compared to all recent works in this space is unclear. Contributions and benefits of individual components in the method are not investigated. Although the method is designed for multi view settings, the authors run experiments on simple settings with only two views. The experiments seem quite limited and do not show the method s capabilities. The rebuttal does not properly address the reviewers  concerns either.   The paper received four reviews with three recommending below acceptance threshold (rejection) and one above the acceptance threshold (although this one was the least confident scoring). Given all the above shortcomings and reviewer recommendations I do not recommend acceptance of the paper.   
This paper received overall positive scores. All the reviewers agree that the approach presented in the paper is simple yet effective and the results are very impressive with >95% parameter reduction while maintaining the accuracy. The authors promptly revised the paper based on initial reviews. Therefore, I recommend accept and hope the authors incorporate the additional comments from Reviewer3 after the discussion. 
The paper has been actively discussed in the light of the authors’ response. Following a strong consensus across the reviewers, the paper is recommended for rejection. Even though the paper was, overall, found quite clear, theoretically sound and tackling a relevant problem for the ICLR community, they listed several concerns that remained unclarified after the rebuttal, e.g.,  * Important baselines missing (e.g., high dimensional BO baselines), a concern unanimously shared across the reviewers. As an example, the fact that using trust regions could benefit HOZOG should be demonstrated empirically. The same goes for the fact that HOZOG is a better strategy compared to TurBO. Such a statement warrants an empirical validation * Further discussion about the non convexity and local optima concerns (raised by reviewer 4) * Limitation to continuous hyperparameters.  This list, together with the detailed comments of the reviewers, highlight opportunities to improve the manuscript for a future resubmission.
This paper proposes a unified model based framework for high level skill learning and composition through hierarchical RL. The proposed approach combines high level planning in a low dimensional space with low level skill learning, where each low level skill is a policy conditioned on the high level task. The low level policies are learned by using a mutual information objective. The proposed approach is evaluated on locomotion tasks, and is shown to be overall more data efficient than alternative baselines. The reviewers agree that this work is original and sufficiently empirically motivated for acceptance. Two reviewers were concerned by the experimental setup and the transfer setting that are somehow too simple, but the authors fixed these issues in the improved version based on the feedback.
This paper focuses on the adversarial robustness of deep neural networks against multiple and unforeseen threat models, which proposes a threat model called Neural Perceptual Threat Model (NPTM). The philosophy behind sounds quite interesting to me, namely, approximating human perception with a neural neural "neural perceptual distance". This philosophy leads to a novel algorithm design I have never seen, i.e., Perceptual Adversarial Training (PAT) which achieves good robustness against various types of adversarial attacks and even could generalize well to unforeseen perturbation types.   The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all  comments in the final version.
This paper proposes a semi supervised setting to reduce memory budget in replay based continual learning. It uses unlabeled data in the environment for replaying which requires no storage, and generates pseudo labels where unlabeled data is connected to labeled one. The method was validated on the proposed tasks.  Pros:   The semi supervised continual learning setting is novel and interesting.   The proposed approach is memory efficient, since it does not need exemplars to replay past tasks.  Cons:   The scale of experiment is small. It lacks evaluation in real world environment.   The novelty is limited, because it is a combination of existing technologies: pseudo labeling, consistency regularization, Out of Distribution (OoD) detection, and knowledge distillation.   The comparison might not be fair due to different settings.  The authors addressed the fairness and scalability with additional experiments and leave some suggestions of reviewers for future work. R3 had a concern on the error propagation of pseudo labels which I also share. The authors agreed that this is a challenge for all CL methods.  In summary, the reviews are mixed. All reviewers agree that the semi supervised continual learning setting is novel and interesting, and some have concerns on scalability and novelty of the method which I also share. So at present time I believe there is much room for the authors to improve their method and experiments before publication. 
This is a borderline case. The paper seems solid although some of the numbers are likely incorrect because in some results tables in the appendix the error taken over all attacks is higher than for the best individual attack (which should never happen).  The main contribution of this paper is to augment a standard adversarial loss (against attacks from different norms) with a “consistency” term (consistency between clean, adversarial and noise augmented samples). The relatively large jump in robustness compared to existing schemes that do adversarial training against multiple norms is a bit surprising. A possible explanation could be that the additional consistency term smoothes the landscape around the clean samples a little bit, which could help to find better adversarial examples. The latter would be very similar to a paper by Pushmeet and colleagues (https://arxiv.org/pdf/1907.02610.pdf) which is not cited, but definitely should. It might also be worthwhile to compare to this paper.   Taken together, this work is interesting but not sufficiently convincing yet to belong to the top papers to be selected for publication at ICLR.  
The authors have done a very thorough job of responding to the comments from reviewers. The paper has a clear contribution, namely that attention maps predict contacts as well as existing unsupervised pipelines. This paper deserves to be published.  In the final version, the authors should discuss briefly "BERTology Meets Biology: Interpreting Attention in Protein Language Models"(https://openreview.net/forum?id YWtLZvLmud7) and "Improving Generalizability of Protein Sequence Models via Data Augmentations" (https://openreview.net/forum?id Kkw3shxszSd). However, the authors should also make sure that the final version respects the ICLR length limits.  I am recommending poster acceptance because the results are anticlimactic given the recent success of Deepmind at CASP 2020.  
The authors develop a novel robustness certificate based on randomized smoothing that accounts for second order smoothness of functions smoothed with Gaussian noise. They develop a variant of Gaussian smoothing based on these insights that improves sample efficiency of randomized smoothing using gradient information.  While the ideas presented were interesting, reviewers were concerned about the quality of presentation of the paper (confused positioning of results relative to prior work) as well as the lack of significant improvements upon existing methods in the experimental section. Overall, the paper is borderline based on the reviewers  comments and ratings   however, there is not sufficient evidence to justify acceptance.  I would encourage the authors to consider a significant revision to improve the clarity of contributions made and strengthen experimental results to demonstrate significant improvements, which would validate the power of the theoretical ideas presented.
The paper studies personalized federated learning, mixing a global model with locally trained models.  Reviewers agreed on the relevance of the problem and that the work contains valuable contributions, such as the generalization bounds. After discussion, unfortunately consensus remained that the paper remains narrowly below the bar in the current form.  Concerns remained on novelty over the Mapper optimization algorithm which also has adaptivity to the local/global combination of models, the dependence of the generalization bound on the mixing parameter as it converges to the global model,  as well as on the strength of the experimental findings compared to well known FedAvg and related method in a realistic benchmark environment (such as e.g. Leaf), since the dataset choice (and even more its partition among clients) is a crucial aspect for measuring personalization in a fair way. We hope the feedback helps to strengthen the paper for a future occasion.
The paper presents an interesting perspective on improving offline RL within BRAC framework.  Given the improvements over BRAC, the paper is well organized and easy to understand.   The overall results pique interest in comparison with more recent Offline/Batch RL papers: BRAC, BEAR, CQL.  The results in this paper bring BRAC family of methods closer to CQL with a number of practical improvements, and could have impact in practice.   However, the reviewers have slight split over the marginal value of additional machinery. There do remain some concerns:   KL divergence is not the best metric to capture OOD issues between policies.    The additional machinery in comparison to CQL may be unnecessary, at least in terms of results.    The method requires many task specific key hyper parameters, which limits the generality of the approach.  I would recommend rejection as it stands. The paper needs more careful empirical analysis that explains what methodical improvements are actually required and which ones only provide marginal bumps.  With multiple task specific hyper params, it may be tricky for these ideas to realize their potential if not clearly understood.  Further release of sufficiently documented and easy to use implementation, will probably be required for acceptance since the main argument in the paper are number of technical improvements in BRAC.
Reviewers raised various concerns about the motivation, unclear justification of the idea and claim, insufficient comparison with related work, and weak experimental results. While authors had made efforts to improve some of these issues in the rebuttal, the revision was not satisfied for publication quality. Overall, the paper has some interesting idea, but is not ready for publication.  
Although originally all reviewers were leaning towards rejection, the authors have done a very good job at addressing their concerns, significantly strenghtening the paper. There is now a consensus towards weak acceptance, with the exception of R3. However, I have decided to ignore R3 s review for the following reasons:   The original review was way too short and uninformative   R3 did not reply to the authors  request for more constructive feedback   R3 did not reply to my own request (private email)  That being said, even if other reviewers decided to increase their score after the rebuttal and discussion period, none of them was particularly enthusiastic about it: this remains a borderline paper combining ideas that, although promising, are not particularly original. At this time it falls slightly short off meeting the bar for an ICLR publication. I do believe that combining ideas from the RL and evolutionary research communities is a promising research direction, and I encourage the authors to take into account the reviewers  remaining comments to polish their paper (in particular, adding even stronger empirical results, and ensuring the key take aways are clearly communicated).
Motivated by (1) the problem of scaling up optimal transport to high dimensional problems and (2) being able to tolerate noisy features, this paper introduces a new optimization problem that they call feature robust optimal transport where they find a transport plan with discriminative features. They show that the min max optimization problem admits a convex formulation and solve it using a Frank Wolfe method. Finally they apply it to the layer selection problem and show that it achieves state of the art performance for semantic correspondence datasets.   The reviews were mixed for this paper. The main negative, which was brought up in all the reviews, is the lack of novelty compared to earlier methods like SRW which already combine dimensionality reduction and optimal transport. The new method in this paper still does have value since it can scale up to larger dimensional problems. It would have been nice to have a wider range of experiments, which would present a more compelling case for its applicability. Another reviewer brought up a correctness issue, however it is not clear if this is actually a bug or merely a misunderstanding about how the pieces in the overall proof fit together. In any case, the reviewers pointed out various places where the writing could be improved. 
The paper clearly has merits, presenting a reasonable approach to zero shot cross lingual learning with good results, but with limited novelty, perhaps. I am sympathetic to the departure from XTREME on NER, agreeing with the authors that using CoNLL data is more interesting than WikiANN.   The post rebuttal discussion centered on novelty and baselining   and specifically, whether other approaches to unsupervised data augmentation exist that should be used to baseline the proposed work. The authors argued that most of the approaches mentioned by the reviewers were in some way supervised. I personally think the confusion is a result of the paper being somewhat poorly framed:  Reviewer 2, for example, suggests a bunch of baselines. Some of these require gold labels for supervised fine tuning to condition the MLM, but this seems like a trivial difference, which is orthogonal to using the augmentation strategies as baselines? Also, other papers have been presented that do not require gold labels, e.g. https://www.aclweb.org/anthology/D18 1100.pdf  Also, on the discussion of Täckström et al. (2012): Older approaches relying on distributional clusters *are* in fact data augmentation methods. Training on augmented data with words replaced is, in the limit, equivalent to training with clusters, when replacement words are sampled from clusters. Others have in the past proposed to use FSAs or clusters induced from static embeddings.  What the authors suggest is a form of co training procedure, so similarly, semi supervised algorithms   e.g., tri training   could have been used as baselines.    In sum, I think the sentiment shared across the reviewers is that the results are largely unsurprising, and could likely be obtained in different ways, including jointly training with a target language modeling objective, tri training, etc. Finally, I agree with Reviewer 2 that a “detailed comparison and discussion of the trade off” between the different approaches to data augmentation, even beyond what’s apples to apples, would benefit the paper. Maybe there s other advantages to the proposed approach over other baselines (effectiveness, robustness)?  
This is a creative piece of work wherein learning of what is normally family specific Potts models is turned into an amortized optimization problem across different families of proteins. The Potts models are learned with a pseudolikelihood approach, and the evaluation of the model against baselines is performed only on a contact prediction problem. This last point is problematic, because on the one hand, the authors use this "as a proxy for the underlying accuracy of the Potts model learned", and on the other hand, claim that "we do not want to claim our method is state of the art for contact prediction, it is certainly not".  Overall, the paper is promising, but is too preliminary on the empirics to warrant publication at this time.
The paper s main message is that some existing NLP techniques that claim to improve performance by the use of a knowledge graph may not achieve this improved performance because of the knowledge graph or at least the explanation given may be questionable.  This is thought provoking and it will incite the community to think more carefully about the real factors of improved performance.  The initial version of the paper was not well written, but the authors improved the writing significantly.  The paper includes a thorough empirical evaluation to support the main message.  I have read the paper and I believe that this work will be of interest to a diverse audience.
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees that this paper addresses an important topic. However, as the reviewers pointed out. The paper mainly builds the technique on simulated setting, and it is unclear how the method will translate to real world speedups. Past work(e.g. [1]) has also shown that many cases there could be a huge gap when the solution is not built carefully.  The paper would benefit from a prototype to demonstrate the applicability of the approach. This paper is therefore rejected.  Thank you for submitting the paper to ICLR.   [1] Riptide: Fast End to End Binarized Neural Networks 
The paper has been actively discussed, both during and after the rebuttal phase. I am thankful for the active communication that took place between the authors and some of the reviewers.  The paper was, overall, found quite clear, with an interesting methodology (especially the introduction of a forecasting step) and a solid large scale experimental evaluation. As a result, it is recommended for acceptance.  However, several concerns remained after the rebuttal phase and we strongly encourage the authors to try to improve the following aspects of their submission: * Clarify as much as possible (notably in the light of the ablation studies further added in the paper) the importance & impact of the BO component (which cast some doubts among the some reviewers on its necessity to get good performance) * Transparently discuss the choice of, _and the robustness with respect to_, the “hyper hyperparameters” of the proposed method (e.g., k, tau, tau’, kappa, tau_max, mini batch size of validation set,...). Such an in depth discussion is essential to fully demonstrate the practical value of the method.  
This paper provides a new perspective on deep networks by showing that NPK is composed of base kernels and their dependence on the architecture is explicitized. It is further shown that learning the gates can perform better than random gates.  While the paper provides interesting understanding neural networks, it is unclear what practical benefit can be drawn from it. On the architectures considered such as FC, ResNet and CNN (btw, it seems restricted to 1 D), it will be important to show that such insights lead to new models or learning algorithms that improve upon the standard practice in deep learning (or get very close to).  It is debatable whether drawing such a nontrivial insight alone warrants publication at ICLR, while "nontrivial" itself is a subjective judgement.  I understand people differ in their opinions, and the NTK paper has been impactful.  Unfortunately since there are quite a few other papers that are stronger, I have to recommend not accepting this paper to ICLR this time.
This work presents an algorithm   graph structured reinforcement learning (GSRL)  to address the problem of exploration in sparse reward settings. The core elements of this work are 1) to build a state transition graph from experienced trajectories in the replay buffer; 2) learn an attention module that chooses a goal from a subset of nodes in the graph and 3) policy learning via DDPG using "related trajectories", where trajectories that are related to the generated goal are sampled from the replay buffer.  Pros:   all reviewers agree that the idea/work is interesting and valuable to the community   reviewers appreciate the theoretical graph based foundation/motivations  Cons:   clarity: the manuscript still remains hard to follow. Many critical components for understanding are in the appendix.    One of the key steps in this work is the discretization of the state/action space for graph construction. However, this is not mentioned very clearly, which creates a lot of confusion given that you re considering continuous control domains.    Furthermore, the group selection part and training the attention module is expressed in an overly complex manner. Without the reviewers inquiries it would have been impossible to decode the technical details of this key contribution, and unfortunately it remains hard to read/follow.    while the ablation experiments (impact of discretization, group selection ..) are appreciated, but it is not clear on which environment they were generated (average across all? or only one of them?).   do you use DQN and DDPG? There are some conflicting statements in your paper, namely first you say "We use off policy algorithms named DQN (Mnih et al., 2013) for discrete action space and DDPG (Lillicrap et al., 2015) for continuous action space", then in the experiments you say "to demonstrate the real performance gain of our GSRL we set the policy network with DDPG for GSRL and all baselines".     I agree with the reviewers that it s not clear why the chosen baselines are very relevant   there seem to be other more relevant baselines.    the significance of the attention module is not very clear, and is not analysed properly. What does it really learn? some form of deeper analysis would be useful here. How would a version that simply picks the most uncertain state in the graph? The ablation graph presented is not very convincing.   Overall, I believe that this work will make a valuable contribution in the future, with an iteration to improve clarity and better show case the significance of the attention module.
This paper proposes an attention mechanism that works at the phrase level for semantic parsing. Reviewrs agree that the idea has been previously explored outside semantic parsing, that the gains should be shown on less saturated datasets, and that there are issues in the experimental design (observing test set results for many experiments). Thus, at this point I recommend that the paper is rejected.
This paper proposes a method to cope with large vocabulary sizes. The idea is to find a small number of anchor words and to express every other word as a sparse nonnegative linear combination of them. They give an end to end method for training, and give a statistical interpretation of their algorithm as a Bayesian nonparametric prior (in particular an Indian restaurant process). They give extensions that allow them to deduce the optimal number of anchors which allows them to avoid needing to tune this hyperparameter. Finally they give a variety of experiments, particularly in language and recommendation tasks. The results on language are particularly impressive, and in the author response period, at the behest of a reviewer, they were able to extend the experiments to the Amazon Review dataset which contains 233M reviews on 43.5 M items by 15.2 M users.   This paper is a nice combination of a simple but powerful idea, and a range of experiments demonstrating its utility. Other papers have proposed related ideas, but here the main novelty is in (1) using a small number of anchors that can incorporate domain knowledge and (2) using a sparse linear transformation to express other words in this basis. One reviewer did not find the Bayesian nonparametric interpretation to be fruitful, since it does not lead to techniques for handling growing datasets (e.g. if the ideal number of anchors changes over time). 
This paper studies inverse reinforcement learning through the prism of regularized Markov decision processes, by generalizing MaxEntIRL from the negative entropy to any strongly convex regularizer (as a side note, strict convexity might be enough for many results). The reviewers appreciated the clarity, the mathematical rigor and the empirical evaluation of this paper. They asked some questions and raised some concerns, that were mostly addressed in the rebuttal and the revision provided by the authors. This is a strong paper, for which the AC recommends acceptance. 
 This paper tackles the task of translating informal LaTeX math into a formal representation annotated with abstract concepts (sTeX / SMGloM).  The authors build a synthetic training data generation mechanism, and construct an evaluation dataset by hand. The problem is tackled as machine translation, and vanilla systems fail, while GPT 2 pretrained on LaTeX documents performs well. The reviewers recognize the importance of this work, in an area where data is not plentiful and benchmarking is difficult.  The authors do a good job in presenting a difficult topic rather clearly, but I would encourage the authors to continue improving the presentation, possibly with clearer examples or figures.  The particular "copying bias" useful in this task, pointed out by a reviewer, is indeed interesting and I encourage the authors to consider that discussion and the thoughtful reviews deeply. Overall, this is a significant contribution to the field and I recommend acceptance.
The paper proposes to address the out of distribution generalization problem by means of conditional computation in form of a feature modulating module. While the approach is interesting and brings a new take on how to perform feature modulation (although initially felt too similar to Conditional Batch Normalization) some major concerns about the experiments and validation of the approach are raised by all reviewers. Some of the hypothesis made are also challenged due to lack of proper validation. Although the discussion clarified some points I am afraid many open questions are left unanswered and would require a more work to be fully addressed before acceptance.
The reviewers agree in their positive evaluation of the paper. A weakness of the paper pointed out by several reviewers was its presentation, which has hovewer improved. Thus, I m glad to recommend acceptance.
Reviewers raised concerns about the paper s clarity (interchangeable use of subtly different terms, notation, typos), and how realistic/practical certain assumptions are. The authors are encouraged to incorporate the reviewers  detailed comments for a future submission.
This paper tests out some straightforward data augmentation strategies on the protein inputs to the transformer used in the TAPE paper.  Overall, there is insufficient intellectual merit to warrant publication at ICLR. As a side note, the quality of the manuscript in terms of scholarliness of presentation was overall lacking.
The reviewers agreed that the paper can be improved in several aspects such a motivation, novelty and framing of the contribution. The reviewers also liked the clarity of the presentation and some of the ideas. The reviewers constructive input may be used to improve this work. 
While the paper contains some interesting ideas, the reviewers felt that overall the paper is not theoretical well supported, and likewise the experiments are not fully convincing. Even after the rebuttal, these concerns still persist.
The paper proposes sample robustness (a data dependent measure) which is essentially a point wise Lipschitz constant of the label map. The measure is used to choose a subset of training data for training and it measures how small of a perturbation is required to cause a label change w.r.t. label map. initially, the paper lacked theoretical motivation and backing and the empirical studies were limited to be convincing enough. The authors added additional theoretical explanations. There were some mathematical mistakes that were fixed in the revision.  However, that is not enough to justify the proposal fully. Therefore, I suggest the authors improve the theoretical explanation. The paper would also benefit from more empirical analysis and discussion. As is, the paper has limited significance to the community since the conclusion is not convincing enough.   The paper writing quality although improved from the original version, still has room for improvement.   The proposed measure is simple, which can be a plus. but that means that we are also missing on some relationships and interactions between samples impact on training. Therefore, The paper will benefit from clearly discussing pros and cons of the proposed method. Moreover, discussing how this definition works in choosing the best subset of samples will improve the paper.   i thank the authors for their effort and improving the paper in response to the reviews. However, given that myself and reviewers find the modifications enough for addressing all the concerns, I vote for rejecting the paper. Please improve the paper and resubmit to a future venue.
This paper proposes to use context based metric learning, where an attention/Transformer based mechanism is used to incorporate neighborhood information for deep learning based metric learning. This was initially demonstrated on two simpler datasets, although larger ones were added during the rebuttal. On the whole, reviewers appreciated the simplicity and intuition behind the idea, but the consensus among all of the reviewers found several aspects lacking, including: 1) clarity of the descriptions in the paper, 2) novelty compared to existing work, especially that of Set Transformer for clustering, 3) lack of convincing results compared to baselines, or at least analysis/justification for negative results. While the reviewers appreciated the authors  rebuttal and experiments, it did not address many of these concerns. The idea is interesting and seems to hold some promise, so the authors are encouraged to refine these aspects in order to fully explore this idea and submit to a future venue. 
This work proposes algorithms for solving ERM with continuous losses satisfying the PL condition. The first algorithm achieves that by using a chainging noise variance and thus the paper frames the contribution in terms of the advantages of non constant noise rate.  The problem is a well studied one and the result is a nice if relatively modest improvement over Wang et al. However, as pointed out in reviews, in the context of convex optimization the same rate has already been established (Feldman,Koren,Talwar STOC 2020). This work is cited and briefly discussed but the discussion only includes one of the algorithms in the paper (that does have an additional log N factor). The overall assumptions in this paper are not comparable (weaker in some ways and stronger since they only require PL instead of strong convexity) but still the overall the contribution appears to be incremental.
Thanks for your submission to ICLR.  This paper proposes a subspace indexing model for low dimensional embedding.  The reviewers were all generally in agreement that the paper is not ready for publication.  In particular, they felt that the paper had several key weaknesses:   Relevant literature is not discussed  Relevant methods are not evaluated against in the experiments  Experiments on the whole were limited and not sufficiently convincing  The novelty of the paper is not very high  Please consider the reviewer comments carefully when preparing a future version of your paper.
Summary of discussions: R1 was positive on the paper in their initial evaluation, and although dissatisfied with the author s feedback, continued to support the paper. I agree with R1 s assessment that other reviewers  call for more theory is somewhat unfair, considering the fact that very similar papers don t usually include theoretical justification beyond intuitive motivation.  By contrast, R3 is the most negative on the paper, leaning towards rejection. The main concern is that open questions remain as to whether the reported performance can be attributed to the architecture, or the loss function proposed. This is an important point to clarify, and further ablation studies would make the paper stronger.  After considering the strengths and weaknesses of this work, the final decision was to reject. Authors are encouraged to improve this promising work and resubmit to a future venue.
I agree with the reviewers  positive comments about the paper. The BREEDS approach to generating benchmarks seems to be a useful one and addresses an important problem in the space. This approach could be the start of a nice direction of inquiry that will give us new insights into subpopulation shift. And most of the reviewers  negative concerns were addressed by the revision.
This paper received three recommendations of accept and one recommendation of reject.   The paper is mixed.  The results presented are both compelling and will have impact on the community.  The AC does not agree with R2 s views that the paper requires proposal of a novel method for acceptance.  At the same time, the AC also does not agree with the views of the other reviewers that the current experiments alone are enough to carry the paper without more conclusive statements.  As hinted by R3, simply pointing out the problems is not enough without proposing how to adjust our models and experimentation protocols in the future is insufficient.    In its current state, the paper would make for a good workshop submission.  Alternatively, the AC suggests to the authors to expand on the SimpleView baseline and or propose alternative solutions or protocols.
This work proposes capsule networks with deformable capsules for tackling object detection. All reviewers agreed that object detection is an important problem that is interesting to the ICLR community. Reviewers also agree that the proposed approach is novel and interesting, and in particular they mention that proposing an efficient capsule network for detection is non trivial. On the less positive side, during the discussion phase all reviewers had concerns about the weak experimental validation, particularly missing ablation studies to analyse the effectiveness of their contributions. At the end, all reviewers felt that, while this is a promising direction of research for object detection, the experimentation should be improved.
This paper proposes to pre train contextual semantic parsing models on synthesized data (using a small amount of additional supervised training data and grammar based generalizations therefrom) with two new training objectives: Column Contextual Semantics (CCS), mapping text to database columns, and Turn Contextual Switch (TCS), to deal with the update semantics between turns.   I thank the reviewers for their detailed engagement with this paper, and thanks the authors for their responsiveness in doing extra experiments and rewriting that made this paper better and the decision clearer.  Pros  The authors did such a great job of summarizing the pros, that I think I can just copy their summary: "We are glad that the reviewers appreciate the novelty and the effectiveness of our proposed approach (R5), find our experiments to be comprehensive and convincing by achieving SOTA on 3 out of 4 different tasks (R1, R2, R3, R4), ablation studies and analysis to be informative and well done (R2, R4), and think our paper is clearly written and easy to follow (R1, R2, R3, R4)."  Cons    A somewhat specific and ad hoc data synthesis solution   Stronger pre trained contextual language models might beat assumed baselines or methods shown here (R4, R5)   The story is weak and should be better motivated through discussion of contextualization of interpretation  In general the reviewers recommend accepting the paper, and I agree. However, it is perhaps not of the novelty, clarity, or impact size to qualify for more than a Poster. R5 has a good point about how strong pre trained LMs are a general tool and should be preferred to the extent they work in 2020, but I think they are too opinionated to suggest this is a reason for rejection. Along with the other reviewers and the authors, I think it is most reasonable to accept work showing good progress using "medium sized" pre trained LMs   really we thought BERT was big a couple of years ago!   and this work has comprehensive experiments with good results. I would encourage the authors:    To say more about the alternative strategy of instead using a bigger pre trained LM, as has come out in the discussion on OpenReview, and the pros and cons of this approach (though maybe the results with BART are the only fairly comparable data point)   To strengthen the presentation by orienting the paper more around the importance of contextualization in interpreting dialog turns in conversational semantic parsing (as opposed to the "one turn" nature of the original famous semantic parsing datasets).  p.s. One typo I noticed in the revised paper while reading: fours  > four 
We have a very well informed reviewer who strongly feels that this paper is insufficiently novel and significant further discussion on how the paper might be raised to a publishable level with more empirical results.  I will have to side with the more engaged reviewers who feel that the paper should be rejected.
The main concern is that the results in this paper are based on strong asymptotic assumptions. (At least) more empirical results are needed. 
This paper introduces an alternative to Langevin sampling and also the idea of adversarial score sampling.  The reviewers are generally supportive of the paper.  Pros:   The idea behind improving Langevin sampling is theoretically justified and leads to a simple algorithm.    The idea behind adversarial score matching is also shown to be effective    Improvement over baseline  Cons:    Two ideas packed  into one paper, which is reflected by the title as well.     From the narrative it could be thought that using EDS on the last step of CAS is the contribution of the paper. 
This work proposes an EM type of approach for domain adaptation under covariate shift. The approach well motivated and developed and experimentally evaluated on synthetic data.  Pro:   The EM type of framework is simple and natural and  promising direction for DA, which should be explored and analyzed further.  Con:   The presentation is highly overselling the results. Both in terms of the generality of the findings and in terms references to privacy preserving properties. Both would need a solid formal analysis which this submission does not provide.   Several reviewers have stated that, while the authors promised updates to their manuscript during the author response phase, no such updated submission has been made.   The work bases their approach by referring to a well known theoretical DA bound by Ben David et al (2010). The theorem is not stated correctly. The most important component in that work is to restrict the models to a class of bounded capacity.   The claim of the authors of "solving the problem" under covariate shift are overstated. It is reasonable to expect that the authors provide a more thorough analysis of the limitations or their approach, that is, clearly state the conditions under which it would succeed and fail. Below are some references on lower bounds of DA under covariate shift.   Given that the theoretical analysis is limited, a more thorough experimental exploration would be expected.  Refs on difficulty of DA learning under covariate shift and bounded d_H distance:  Shai Ben David, Ruth Urner: On the Hardness of Domain Adaptation and the Utility of Unlabeled Target Samples. ALT 2012: 139 153  Shai Ben David, Tyler Lu, Teresa Luu, Dávid Pál: Impossibility Theorems for Domain Adaptation. AISTATS 2010: 129 136 
Even though the authors revised the problem formulation, the paper seems not ready for publication. The assumptions are still too strong (The learning algorithm assumes knowledge of the sparsity mask). The proof technique also heavily relies on Zhong et al 17 without properly highlighting the difference. 
In order to learn good exploratory behaviors in settings where agents encounter diverse environments, the authors propose an approach which involves learning from episodes that exhibit good episode level exploratory behaviors.  The innovation is in the scoring and learning from these episode level behaviors rather than trying to come up with shorter timescale proxies of exploration.  In making this concrete, the authors propose to score trajectories based effectively on state coverage within an episode (i.e. good exploration corresponds to good state coverage) as well as by scoring episodes relative to one another and giving preference to episodes that explore less often encountered states.  To learn, the core algorithm interleaves standard RL updates with behavioral cloning updates using the best episodes of data, thereby training the policy to both solve the task and explore well at the episode level.  A weakness is that the paper uses low level state in grid worlds and there is some ambiguity in applying this to settings with continuous states.  The authors discuss general strategies for dealing with these limitations as potential future work.  The reviewers were positive about the clarity of the text and felt the core idea that was proposed was simple and effective.  The authors put in solid effort to address reviewer concerns.  The most salient remaining concern, which I share, is that there will be challenges in scaling this approach to more complex environments with continuous state/observation spaces.  Overall, this paper had a consensus "accept" rating (7,7,7,6), and I endorse this as my decision.
This paper proposes constraints to be applied to the weights of a deep neural model during training. These constraints, motivated by an analysis of Rademacher complexity, are compared with other constraints and penalty approaches in transfer learning. The authors were able to build on the reviewers feedback to improve their paper on several points during the discussion phase, leading to a consensus for acceptance among reviewers. They also agreed to conduct experiments targeting stronger experimental results to compare all methods in the situation where they provide state of the art results. This will make a useful contribution to the ICRL audience, and I recommend acceptance. 
This paper introduces supervised contrastive learning loss on top of typical cross entropy loss for fine tuning language model for downstream tasks. While the idea is simple and has been used in vision literature (as pointed out by R1 & R4), its application LM is first introduced in this paper. The experimental gain is small in the regular setting but clearer gains in a few shot learning setting and noisy training dataset (through back translation) setting. Overall the paper is clearly written and experiments are carefully studied. During the discussion phase, the authors provided results on the full GLUE dataset as well as other ablation studies (e.g., CE+CE recommended by R2), improving the paper.  
The paper shows that a form of Fictitious Self Play converges to the Nash equilibria in Markov games. Understanding the theoretical properties of Fictitious Self Play is important, however the paper in its current form is not ready for publication. The paper needs a more thorough discussion on related works, the assumptions made, and as pointed out by Reviewer3, the convergence argument needs to be expanded and explained in more detail. Further, I encourage authors to add experiments and compare their algorithm with other methods. 
All the reviewers agree that the paper presents an interesting idea, and the main concern raised by the reviewers was the clarity of the paper. I believe that the authors have improved the presentation of the paper after rebuttal, however, I still believe that the paper woudl require another round of reviews before being ready for publication, in order to properly assess its contributions. 
All reviewers agreed on the major shortcomings of this submission, the most important of which is that the contributions are insufficiently evaluated. There was no author response. 
The reviewers generally found the idea interesting and the contribution of the paper significant. I agree, I think this is quite a neat idea to investigate, and the paper is written well and is engaging to read.  I would encourage the authors to take into account all of the reviewer suggestions when preparing the camera ready version. Of particular importance is the name: I think it s bad form to appropriate a name already used in other prior work (proto value functions, which are very well known in the RL community), so I think it is very important for the final to change the name to something that does not conflict with an existing technique. Obviously this does not affect my evaluation of the paper, but I trust that the authors will address this feedback (I will check the camera ready).
This paper presents a new inference mechanism for latent variable models, by taking the derivative of log likelihood with respect to a zero valued vector. Initially, the reviewers raised concerns mostly regarding the limited experimentation and missing baselines. However, in the revised version, the authors addressed most of these concerns.   Given that most reviewers are positive after the revision and since the proposed method is simple and interesting, I recommend accepting this paper.
The paper proposes a novel loss for segmentation tasks, which incorporates reasoning about topological accuracy of predicted segmentations vs. ground truth. All reviewers, after the rebuttal period, recommend acceptance, and I agree   it s an interesting paper, offering a potentially useful and clearly novel building block for training segmentation models.
The paper introduces a method for differentially private deep learning, which the authors term Gradient Embedding Perturbation. This is similar to several (roughly) concurrent works, which project gradients to a subspace based on some auxiliary public data. However, a crucial difference involves the use of the residual gradients, which allows the method to achieve the first significant accuracy gains using subspace projection. The reviewers believe this method will be important for the practice of DP deep learning.
As the reviewer confidence of the reviews of 2.8 or lower, I made a full and detailed pass of submission as it was requested by the PCs.   The paper studies how to parallelization of MCTS affects its performance and provides the analysis of the excess regret   how much  "error" we incur by  parallelizing as opposed to single threaded execution.  The submission however does not give convincing arguments on why existing parallel methods perform empirically under their sequential counterparts. This would be particularly useful for the settings described in the submission, which are only resembling the actual parallel solutions.   Furthermore, the paper analysis of a "cumulative regret" in which it counts the errors made for the exploration   but the setting here is a MCTS (search, planning), when we access an oracle (a model of the environment) and we should be rather interested in the sample complexity, studied for example in https://www.cis.upenn.edu/~mkearns/papers/sparsesampling journal.pdf that gave early results and many follow ups until OR  we should study how good the final policy it   and in your parallel setting (which is indeed very timely), how small the excess error of this final policy is. [Or you only focus on a single action recommended by MCTS and you study some pure exploration measure: best arm identification, simple regret, \eps BAI, ... ]  From the theoretical side modeling practice,  I don t see the theoretical framework provided to be suited with practical considerations that an MCTS is facing.  Specifically the need of condition 1 for the consequence of Theorem 1 to hold is very worrying.  Briefly it states that if  the value and the counts are not the same as reference method, then the results obtained from the that parallel executions would not match the results reference method.  While this true, it simply tells us that it would be good if the parallel threads came up with the same value as the reference method   the main issue that this does not tell the practitioner how to assure the condition states in Eq 8. It would be much more interesting to characterize that, i.e., under which conditions (in particular, that the practitioner can influence OR  at least verify prior to the execution) is the property stated in the Eq. 8 satisfied.   Theorem 2 bring additional concerns. A book of Munos on MCTS (https://hal.archives ouvertes.fr/hal 00747575v4/document) has Section 2.3 devoted why vanilla UCT has poor performance. There is work by  Kocsis and Szepesvári studying under which conditions UCT can have favorable regret    there is at least a discussion missing why Theorem 2 would be able to bypass known hardness for UCT for higher depths, and if not I question its utility. Finally, from finite time result excess I would expect more finite time lessons learned beyond "regret term that converges to zero as n increases".  
This paper presents an approach to reward shaping in RL centred on the question of how to select between different shaping signals. As such this is an interesting research direction that could make important contributions in the area. Generally the reviewers felt that the paper is too preliminary in its current form. There were several questions raised around problems with the technical formulation. It was also felt that the experiments could be more rigourous to fully validate the claims of the paper.
This work demonstrates that autoregressive (AR) models for machine translation can can be competitive with their non autoregressive (NAR) counterparts in terms of practicality. This is a timely observation, given the flurry of recent work on NAR models, whose primary benefit is often cited to be fast inference.  It was argued that the results are not surprising   if this is the case, I still think this work merits acceptance because its thesis runs counter to the direction the field as a whole seems to be moving in, and the results are convincing. That said, I agree with the authors that the observation that some encoder and decoder layers are interchangeable, is not self evident (i.e. it _is_ surprising). This is of course subjective to some degree, so I am making a judgement call here. The work also has value in that it draws attention to some practices regarding evaluation in NAR machine translation literature that could be improved and made more fair (specifically regarding comparison with AR models).  There were some concerns about whether these models should be evaluated in the small batch or large batch setting. The authors have updated their manuscript in response, and it now explicitly discusses both settings. The authors have also run more experiments and added several additional results requested by reviewers to the manuscript.  All things considered, I am inclined to follow the majority and recommend acceptance.
This paper presents a simple yet effective approach to improve self supervised contrastive approaches like MoCo. There are concerns with respect to novelty/simplicity and low improvements over MoCov2. AC believes that simplicity is good and while gains might not be as huge, they still show usefulness of new loss. It might also provide insights for future papers on self supervised learning. Overall, the sentiment is that paper is above the bar.
This paper proposes a general framework to study the limit behavior of neural models with respect to the scaling of hyperparameters in terms of network width, which covers existing mean field (MF) and neural tangent kernel (NTK) limits, as well as other new limit models that were not discovered before. While the reviewers agree that the study of limiting behavior of neural network models is of great importance and could be a good addition to the current understanding of NTK and MF, there is a technical flaw in the proof (regarding Condition 1) pointed by the reviewer. After reviewer discussion, all reviewers agree that this is a serious issue, and needs to be addressed before publication. I believe it could be a strong paper if the technical flaw can be fixed. I encourage the authors to revise the paper and resubmit it to the next conference. 
The work tackles the task to convert an artificial neural networks (ANN) to a spiking neural network (SNN). The topic is potentially important for energy efficient hardware implementations of neural networks. There is already quite some literature available on this topic.  Compared to these, the manuscript exhibits a number of strong contributions: It presents a theoretical analysis of the conversion error and consequently arrives at a principled way to reduce the conversion error. The authors test the performance of the conversion on a number of challenging data sets. Their method achieves excellent performances with reduced simulation time / latency (usually, in order to achieve comparable performance to ANNs, one needs to run the SNN for many simulated time steps  this simulation time is reduced by their model).  One reviewer criticized that the article was hard to read, but this opinion was not shared by other reviewers and the authors have improved the readability in a revision.  In summary, I believe that this manuscript presents a very good contribution to the field.
Summary:  The authors propose to predict a neural network classifier s generalization performance by measuring the proportion of parameters that can be pruned to produce an equivalent network (in terms of training error). Experimental and theoretical evaluation are provided.   Discussion: The overall opinion in reviews was that the idea is potentially interesting, but needs to be pursued further before publication, and that the empirical evaluation in particular was lacking. That was followed by a detailed discussion, in which authors were able to address a number of concerns, and have provided helpful additional experiments.  Recommendation: This is a potentially interesting paper that is not quite there yet. Although reviewers have raised scores in discussion, the case for acceptance would still be hard to make. I recommend to reject.  It looks like a reasonable  amount of additional work will turn this from what is now on the weak end of borderline into a potentially strong submission, especially given the thoughtful and thorough feedback from reviewers. The next top tier conference deadline is not far away, and I encourage the authors to incorporate the feedback fully and resubmit soon. That being said, I agree with reviewers that the theory provided is, at present, not strong. Also, a point that still seems to require work is the relation between prunability and the use of dropout.  Note to authors and chairs: AnonReviewer3 explicitly stated in discussion that they would raise their score from 5 to 6, but the change was not recorded in the system. My recommendation assumes their score is 6.
This paper treats the problem of running gradient descent ascent (GDA) in min max games with a different step size for the two players. Earlier work by Jin et al. has shown that, when the ratio of the step sizes is large enough, the stable fixed points of GDA coincide with the game s strict local min max equilibria. The main contribution of this paper is an explicit characterization of a threshold value $\tau^*$ of this ratio as the maximum eigenvalue of a specific matrix that involves the second derivatives of the game s min max objective at each (strict local) equilibrium.  This paper generated a fairly intense discussion, and the reviewers showed extraordinary diligence in assessing the authors  work. Specifically, the reviewers raised a fair number of concerns concerning the initial write up of the paper, but these concerns were mostly addressed by the authors in their revision and replies. As a result, all reviewers are now in favor of acceptance.  After my own reading of both versions of the paper and the corresponding discussion, I concur with the reviewers  view and I am recommending acceptance subject to the following revisions for the final version of the paper: 1. Follow the explicit recommendations of AnonReviewer3 regarding the numerical simulations (or, failing that, remove them altogether). [The authors  phrase that "The theory we provide also does not strictly apply to using RMSprop" does not suffice in this regard] 2. Avoid vague statements like $\tau \to \infty$ in the introduction regarding the work of Jin et al. and state precisely their contributions in this context. In the current version of the paper, a version of this is done in page 4, but the introduction is painting a different picture, so this discussion should be transferred there. 3. A persisting concern is that the authors  characterization of $\tau^*$ cannot inform a practical choice of step size scaling (because the value of $\tau^*$ derived by the authors depends on quantities that cannot be known to the optimizer). Neither the reviewers nor myself were particularly convinced by the authors  reply on this point. However, this can also be seen as an "equilibrium refinement" result, i.e., for a given value of $\tau$ only certain equilibria can be stable. I believe this can be of interest to the community, even though the authors  characterization cannot directly inform the choice of $\gamma_1$ and $\gamma_2$ (or their ratio).  Modulo the above remarks (which the authors should incorporate in their paper), I am recommending acceptance.
This paper was reviewed by four experts in the field. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references.
The authors clarified many of R1 and R4 s concerns, but there were important remaining concerns regarding the presentation.  On the bright side, the approach is novel and the experimental results are solid.   However, the main point raised by R1 is the mismatch between the narrative and the theory and the actual algorithm and results. Some exemples of this mismatch include:   Proposition 1 is proved when lambda goes to 0, which is never mentioned in the main paper. One has to look into the appendices to have a discussion of lambda and of the algorithm, while the authors could clearly explain that when discussing the theoretical result. The added discussion on tuning of lambda based on Appendix E does not help because the optimization problem is written as "under the constraint that [...] is maximized", which is not particularly clear.    More generally, the theoretical result (including e.g., the assumption of ergodicity) could be discussed more precisely in terms of what is actually done in the algorithm and the experiments.   as stated by R3, many important aspects of the methods and the experimental setup are only available in appendices, which makes it difficult to understand the similarities and differences in experimental protocol between the curent paper and RL^2, PEARL and IMPORT.   it is unclear what part of DREAM is critical for performance. There is no thorough ablation study nor discussion of the importance of the information bottleneck term, and the only signal given in Figure 5 is that it is critical. The authors could clarify the two aspects (decoupling and information bottleneck).    There was some discussion about this paper, but even under the assumption that the authors answered most R3 s concerns (R3 didn t engage in discussions), the paper is still borderline. In the end there was little support for acceptance because of the presentation issues above.
In this paper, the authors designed a disentanglement mechanism for global and local information of graphs and proposed a graph representation method based on it. I agree with the authors that 1) considering the global and local information of graphs jointly is reasonable and helpful (as shown in the experiments) and 2) disentanglement is different from independence.   However, the concerns of the reviewers are reasonable   Eq. (2) and the paragraph before it indeed show that the authors treat the global and the local information independently. Moreover, the disentanglement of the global information (the whole graph) and the local information (the patch/sub graph) is not well defined. In my opinion, for the MNIST digits, the angle and the thickness (or something else) of strokes can be disentangled (not independent) factors that have influences on different properties of the data. In this work, if my understanding is correct, the global and the local factors just provide different views to analyze the same graphs and the proposed method actually designs a new way to leverage multi view information. It is not sure whether the views are disentangled and whether the improvements are from "disentanglement".   If the authors can provide an example to explain their "disentanglement" simply as the MNIST case does, this work will be more convincing. Otherwise, this work suffers from the risk of overclaiming.
In the context of constructing negotiation dialogue strategies/policies, the authors explore the use of graph attention networks (GATs) for determining the sequence of negotiation dialogue acts   specifically leading to a (1) hierarchical dialogue encoder via pooled BERT + GRU encoding  > (2) GAT over dialogue strategies/acts (many technical details around graph usage)  > (3) GRU decoder. While a relatively straightforward replacement relative to similar architectures with other  structural  encoders, they provide a sound end to end training strategy that is shown to perform well on the buyer seller negotiation task via CraigslistBargain dataset where they demonstrate SoTA performance.    Pros   + Studying the pragmatics component of negotiation dialogue strategies has received recent interest and this seems a good milepost that demonstrates mainstream methodological approaches for this task (i.e., this is a good baseline for future innovations) + The paper is well written in that it is easy to understand intuitively while having sufficient detail to understand the details. + The empirical results appear promising and meet the standard within this sub community   showing improvements with automatic and human evaluation.    Cons      This builds on existing datasets, which are known to have undesirable properties (e.g., automatic evaluation, small number of dialogue datasets, use of explicit dialogues acts, etc.) While it still meets the standards of this sub community, it still isn t a completely convincing task.   While the use of GATs is novel in this setting and they get it to work within the overall architecture, this is something that many people are likely trying at this time   so there isn t an exciting  disruptive  step here.   The empirical results, while satisfactory from a quantitative perspective, even in reading the Appendices, it isn t clear that these are significantly better from a planning perspective or if it is just  pattern recognition  gains.  Evaluating along the requested dimensions:   Quality: The underlying method is fairly straightforward and the authors incorporate up to date GAT related methods to get this to work in this setting. The empirical results are sound if predicated on the general quality in this sub community where you have the standard machine translation evaluation problem for meaning vs. lexical closeness. To mitigate, they use BERTScore and human evaluation   which is at the higher end of what can be reasonably expected.   Clarity: The paper is written clearly overall, especially if considering the appendices where there is significant detail. Related to empirical evaluation, it isn t easy to intuitively interpret the results, but this is again par for the course. Additionally, I believe the authors did a good job responding to reviewer concerns.   Originality: While all of the reviewers agreed that the approach was novel in this setting, one of the reviewers explicitly pointed out that using GATs in negotiation dialogues isn t that exciting   and I mostly agree. I view this as something that somebody would have done and will serve as a good baseline; although I think this sub field is going to need more datasets to continue progressing.   Significance: As stated above, it is a good baseline that I think many are likely thinking of (as the TOD community has been doing this for a bit now). However, it is done well.  Honestly, I agree with the reviewers that this is a somewhat borderline paper   mostly due to it being a fairly  obvious  idea and the nature of the subfield making it not entirely clear if the improvements are due to knowing the target performance while training or due to the methodological advance. Personally, I am convinced, but it isn t totally clear. That being said, it is a well written paper and I think the reviewer issues were sufficiently addressed. Thus, I would prefer to see it accepted as I think it will be a strong methodological baseline for this problem (which hopefully will accumulate more convincing datasets and standard evaluation). 
The three reviewers seem to reach a consensus that the assumptions made in the paper are too strong and hard to interpret. In particular, R1&R2 made the comments that the generative model for the data by itself uses attention, which seems to be make the comparison unfair. The authors seem to argue that the attention model is still expressive enough, which in the AC s opinion could be true but does not justify the use of the generative model when comparing the sample complexity of the two methods. The reviewers also pointed out a few other limitations of the paper. The AC mostly agrees with the reviewers  points (though perhaps with one or two exceptions.) In summary, I think the assumption of using an attention model itself seems to be a big enough issue that makes the paper not read for publication at ICLR. 
The paper had four borderline reviews with none enthusiastic about championing the merits of the paper. While it was felt that the extension of an existing technique to deep learning via amortization is a useful procedure, it is also not very novel and the experiments didn t demonstrate a significant leap in performance.
Most of the reviewers agree that this paper presents an interesting idea. Practically implementing a BNN that gains real world speedup is challenging, and as past work [1] showed, the bottleneck could shift into other layers(besides the accumulation). The paper would benefit from a thorough discussion about the practical impact in implementing the proposed method and relation to past work.   The meta reviewer decided to accept the paper given the positive aspects, and encourages the author to further improve the paper per review comments.  Thank you for submitting the paper to ICLR.   [1] Riptide: Fast End to End Binarized Neural Networks 
All four referees have indicated reject. Severe points of criticism have been raised, concerning the lacking novelty, the  experimental setup and the significance and interpretation of results. I fully agree with the reviewers in all important points, so I recommend rejection.
This paper discusses the likelihood ratio estimation using the Bregman divergence.  The authors consider the  train loss hacking , which is an overfitting issue causing minus infinity for the divergence.   They introduce non negative correction for the divergence under the assumption that we have knowledge on the upper bound of the density ratio.  Some theoretical results on the convergence rate are given.  The proposed method shows favorable performance on outlier detection and covariate shift adaptation.  The proposed non negative modification of Bregman divergence is a reasonalbe solution to the important problem of density ratio estimation.  The experimental results as well as theoretical justfication make this work solid.  However, there are some concerns also.  The paper assumes knowledge on the upper bound of density ratio and uses a related parameter essentially in the method.  Assuming the upper bound  is a long standing problem in estimating density ratio, and it is in practice not necesssarily easy to obtain.  Also, there is a room on improvements on readability.   Although this paper has some signicance on the topic, it does not reach the acceptance threshold unfortunately because of the high competition in this years  ICLR.  
The reviewers agree that this paper has some interesting ideas. However, they believe it needs more work before it is ready for publication, especially so with regards to presentation (SDEs as GANs) and the experiments (backpropagating through the solver rather than using the adjoint dynamics). These would significantly strengthen the paper, but would probably require another round of reviews.
The paper proposes intra class clustering as an indicator of generalization performance and validates this by extensive empirical evaluation. All reviewers have found this connection highly interesting. The author response has also duly addressed most of the reviewers  concerns. Given the importance of studying generalization performance of overparameterized deep models, the paper will potentially generate interesting discussion at the conference. 
This paper presents a variational learning framework for inferring relations between data points. The authors further introduce novel regularizers to avoid unfavorable solutions to their relational learning problem. Qualitative results are provided on rotated versions of MNIST. Additional qualitative results on the Yale face dataset are provided in the appendix.  The reviewers agree that the idea overall is interesting, and the chosen experiments certainly provide some insight into the idea behind the method and demonstrate that the method is learning reasonable representations of relations between data points. I share the sentiment of the reviewers, however, that this paper is not yet ready for publication. The paper in its current form lacks clear positioning against related problems and related research in this community, and the experiments are all qualitative in nature without the attempt to rigorously compare the proposed method against established techniques. The argument brought forward by the authors that the proposed problem is completely novel and therefore no baselines exist is unconvincing as pointed out by the reviewers, as there is related research on e.g. spatial transformer networks [1], neural relational inference [2], and discovery of causal mechanisms [3], which similarly address the problem of discovering relations, interactions, or transformations. Even if these methods don t exactly fit the problem setup presented in this paper, an attempt should be made to design an evaluation that allows one to compare against some of these approaches, especially given that the paper claims to address the general problem of inferring relations between pairs of data points. Overall, I am confident that this would make the paper stronger and more relevant to this community.  [1] Jaderberg et al., Spatial Transformer Networks (NeurIPS 2015) [2] Kipf et al., Neural Relational Inference for Interacting Systems (ICML 2018) [3] Parascandolo et al., Learning Independent Causal Mechanisms (ICML 2018) 
Three of four reviewers are in favour of accepting the paper. Some reviewers raised valid criticism regarding the derivations, interpretation of the mathematical analysis and experimental results. So clearly some aspects of the paper could and should be clarified in accordance with the points raised by the reviewers. However, all in all the paper contains enough contributions to warrant publication.   
In this paper, the authors propose a MCM aware twin least squares GAN (MTGAN) model for hyperspectral anomaly detection. The proposed method is somewhat novel, and the efficacy of the proposed method is validated through experiments. However, the clarity of the paper is low, and the explanation of some formulas is not clear enough. Therefore, the quality of the current version is below the acceptance threshold.  I encourage authors to update the paper based on the reviewer s comments and resubmit it to a future venue.
The paper proposes "HyperGrid Transformers" a modified transformer architecture for learning a single model for multiple tasks in NLP.  The proposed method was evaluated on popular GLUE/SuperGLUE tasks and reported competitive results with the baselines (the improvements are somewhat marginal). The paper contains some interesting idea of using a decomposable hypernetwork to learn grid wise projections for different tasks, which may not be particularly novel in machine learning context but seems new for multitask NLP. Reviewers generally agree the paper is above acceptance bar, however some concerns were raised about clarity of baselines and fairness of experimental comparison as well as stronger baselines. Authors improved some of them in the rebuttal, but there is still some room to further improve the quality of presentation and writing in the final version. 
This paper makes an interesting connection between the density matching in imitation learning and reaching the goal state in goal oriented reinforcement learning. Reviewers generally expressed that the paper proposes an interesting approach, but some aspects of the paper need room for improvement. By reinforcing the experiments that address the reviewers’ various concerns, this paper will make a good contribution towards reinforcement learning research.
This paper proposes a simple generalization to epsilon greedy exploration that induces temporally extended probes and can leverage options.  The idea and analysis are trivial.  Computational results demonstrate when this sort of exploration is helpful.  The paper is well written and the authors offer a fair assessment of when these ideas do or do not address challenging exploration tasks.  A range of computational results support and offer insight into the concepts. 
This paper investigates methods for gradient based tuning of optimization hyperparameters.  This is an interesting area, and the paper isn t bad.  The examination of hypervariance seems relatively novel and useful.  I also appreciate the point about Bayesopt sometimes working well simply due to small ranges.  However, I agree with the criticisms of the reviewers.  Overall this paper isn t quite clear, thorough and impactful enough to make it in this round, but I think with more attention to baselines and scope this paper could be acceptable.  Some minor comments:  1) The signed based optimizer, while simple and sensible (which is good), seem kind of ad hoc. 2) The authors don t seem to have properly scoped the problem and method, since greediness is only a major concern for inner optimization hyperparameters specifically.  It s not clear that for regularization parameters that this problem exists or that your method would apply.  A small nit:  Is hypervariance the right thing to look at, since the problem can exist even in deterministic settings?  Perhaps some sort of sensitivity analysis would be more appropriate.  Also you should reference Barack Pearlmutter s thesis which first explores these issues.   I would also mention that the hypervariance is generally tiny for smaller than optimal learning rates, and massive for larger than optimal learning rates, (the chaotic regime).
All the reviewers agree that the paper studies an important and interesting problem. However the reviewers felt the paper is still in preliminary stages, with incorrect derivations, missing comparisons/references,  and writing. While the authors updated the paper during the discussion stage addressing some of the concerns, the paper still needs more work in adding appropriate comparisons and in presenting the concepts more clearly. Hence I believe the paper is not yet ready for publication and encourage authors to continue their work.
This paper proposes a new metric, AUL, for classification problems with unbalanced data. The paper proves that it is unbiased and it does not need to estimate the mixture proportions, a traditional approach. Empirical results show improvement. While the results are novel, the referees pointed out several concerns 1. Positioning of the results in the context of existing work. It would be helpful if the paper can establish theoretical links to existing metrics such as  AUC, make a very precise statement about what is lacking in those approaches 2. Some of the results, may have bugs in the proof. 3. More ablation studies are needed   In summary the paper has good ideas but maybe premature for publication. 
There was quite some variance in opinion on this paper, with some reviewers commenting on problems with clarity and experimental evaluation. The authors rebuttals improved the reviewer opinions slightly. The rebuttal and accompanying revisions are convincing, and the new experimental results are convincing and also very much appreciated. This is one of the first papers taking a comprehensive look at incremental, few shot classification AND regression. Despite some problems with clarity (which were well addressed in rebuttal and revisions), the paper is original and presents novel ideas about incremental few shot learning.  Pros: consideration of both few shot classification and regression, ablation study well executed and convincing.  (remaining) Cons: some minor problems with clarity   please take reviewer comments on board when preparing the camera ready version.
This paper proposes a Conditional Masked Language Modeling (CMLM) method to enhance the MLM by conditioning on the contextual information.   All of the reviewers think the results are good. However, the reviewers also think the intuition and experiments are not so convincing. The responses and revisions still not satisfy all the reviewers  major concern.
Reviewers have commented on the lack of novelty of the paper as it reads only as applying the variational inference framework of Blundell et al. (2015) to deep metric learning (R2 and R4). Furthermore, the paper has not properly positioned itself when compared to previous works on "Deep variational metric learning" and "Deep adversarial metric learning" (R1) and other previous literature that have studied robustness for metric learning. The argument on robustness to noisy labels needs to be expanded and better fleshed out in a future version of the paper. 
This paper proposes an OOD evaluation framework under three categories: irrelevant input detection, novel class detection, and domain shift detection. As with several reviewers, the AC recognizes the importance and effort to distinguish between different cases of OOD detection, as well as the amount of experimental comparison across several prominent methods in literature (MSP, MC dropout, cosine similarity, ODIN, Mahalanobis).    Despite being well motivated, three knowledgeable reviewers find the paper not ready yet for publication at ICLR. The AC recommends a rejection, given the standing major concerns from the reviewers. The AC is hopeful that the paper can be significantly improved by     sufficiently discussing and highlighting the novel insights of the results.    a more rigorous definition of  "novel" vs. "irrelevant" inputs. There seem to be overlapping definitions between what Hsu et al. considered vs. this paper. In particular,  Hsu et al distinguish i) samples of a novel class but in a known domain, called semantic shift (S), and ii) samples of known class in a novel domain, called non semantic shift (NS), both of which are reconsidered in this paper. Therefore, the novelty of this submission is more precisely to distinguish within the category of semantic shift. The AC agrees that this might deem some more rigorous measurement and definition of "semantic closeness".    The AC also finds the evaluation of domain shift in Section 3.3.2 may be potentially misleading the community, as it falls out of the standard OOD scope. The notion of common corruption is closer to the robustness problem (which is how ML model predictions changes w.r.t some delta changes in the input space). The changes may not be substantial enough to be "out of distribution".  
The paper proposes LORAS (low rank adaptive label smoothing) for training with soft targets with the goal of improving performance and calibration for neural networks. The authors derive PAC Bayesian generalization bounds for label smoothing and show that the generalization error depends on choice of the noise (smoothing) distribution. Empirical results demonstrate the effectiveness of the approach. All reviewers recommend acceptance.
The paper studies the problem of leveraging Positive Unlabeled~(PU) classification and conditional generation with extra unlabeled data simultaneously in one learning framework. Some major review concerns on the weaknesses include limited novel technical contributions, poor presentation and weak experimental results (e.g., experiments were mostly conducted on small toy datasets). Overall, the paper has some interesting idea, but the work is clearly below the ICLR acceptance bar. 
The manuscript proposes a causal interpretation of the self supervised representation learning problem. The data is modeled as being generated from two independent latent factors: style and content, where content captures all information necessary for downstream tasks, and style captures everything that is affected by data augmentations (e.g. rotation, grayscaling, translation, cropping). The main contribution is a specific regularizer for self supervised contrastive learning, motivated by the assumptions about the data generation.   Reviewers agreed that the manuscript is oversold on the causal jargon, as was noted, the manuscript does not perform any causal inference. Nevertheless, they think that there is an interesting interpretation of self supervised learning and the results are noteworthy. 
The authors develop a novel strategy, Deep Partition Aggregation, to train models to be certifiably robust to data poisoning attacks based on flipping labels of a small subset of the training data or introducing poisoned input features. They improve upon existing certified defences against data poisoning and are the first to establish certified guarantees against general poisoning attacks.  Most reviewers were in support of acceptance. Reviewer concerns were raised in the rebuttal phase but were convincingly addressed in the rebuttal phase. One reviewer did raise concerns on the weakness of experimental results on CIFAR 10, but the fact that this method has established the first certified defence in the general poisoning setting and that the results are stronger on other datasets certainly warrant acceptance. I would encourage the authors to clarify this in the final version.  
The paper introduces a game theoretic framework to improve our understanding of dropout. All reviewers appreciated the contribution of the paper. While they had a number of questions/suggestions, almost all of them were adequately addressed. Three reviewers are satisfied and recommend acceptance, while a lone reviewer is on the fence, he/she admits he/she is less knowledgeable about game theory. Overall, I think this paper makes a solid contribution to ICLR.
All reviewers agree that this paper is worth publishing. It investigates a novel idea on how to adaptively prioritise experiences from replay based on relative (within batch) importance. The empirical investigation is thorough, and while the performance improvements are not stunning, the benefit is surprisingly consistent across many environments.
This paper shows that various discrete loss functions can be formulated as an LP. It proposes to relax the constraint Ax   b, x >  0 using a soft constraint and following Mangasarian, proposes to solve the relaxed problem using Newton s method. Backpropagation through these iterations is further proposed. The main motivation is that this results in a GPU friendly implementation.  I think the proposed approach is novel. However, as pointed out by reviewers, the current writing lacks clarity and the experiments are quite weak. There is now a wealth of methods for differentiating through an LP using implicit differentiation, smoothing (which the present paper is a form of, see below) and perturbations. It is important to compare to these methods. The paper also ignores a large literature on convex surrogates for ranking metrics.  I recommend the authors to strengthen the writing and experiments, and to resubmit to a top conference.  Additional comments by the AC    As the sentences "Hence, solving such LPs using off the shelf solvers may slow down the training process" or "Often, this would involve running the solver on the CPU, which introduces overhead" indicate, the authors seem to imply that LPs need to be solved in canonical LP form, min_x <c,x> s.t. Ax <  b, x > 0, using an off the shelf LP solver. This is not how many LPs are solved in practice. For every loss, there will always be an ad hoc solver for the corresponding LP. For instance, the Hungarian algorithm for the Birkhoff polytope.  The paper is missing an important reference: SparseMAP (https://arxiv.org/abs/1802.04223). In this paper, the authors add regularization to the primal LP and use Frank Wolfe or active set methods to solve the problem.  Equation (6) corresponds to relaxing the hard constraint Ax b, x> 0 with a soft one. This approach is in a sense opposite to SparseMAP. Indeed, relaxing the constraints in the primal is equivalent to adding regularization in the dual LP (see, e.g., https://papers.nips.cc/paper/2012/hash/bad5f33780c42f2588878a9d07405083 Abstract.html). Speaking of (6), the authors should clarify that it s a convex objective.  In section 2, the authors review a number of losses which can be written as an LP. It would be better to explicitly state what are A, b and c for each loss (or g, h, E, F, p, B, G, q).  The matrix A could potentially be huge, depending on the LP. Do you need to materialize it in memory in practice? This would limit the approach to relatively small LPs.
The paper considers an alternative to the standard MDP formulation, motived by the novo drug design problem.  The formulation is meant to optimize a notion of expected maximum reward along the trajectory rather than the expected sum of rewards.  The formulation is presented through a variation of the Bellman equation.  Thus mode of presentation does not make it entirely clear what the fundamental problem is and whether it is the right formulation for the application.  The reviewers point out some problems with the analysis.   Experiments compare the proposed max Q algorithm to Q learning and demonstrate that it achieves higher maximum reward.  Experiments involving novo drug design show promise.  This looks like an interesting idea and direction, but the consensus view is that the project deserves further work and polish.
The paper proposes to use the sum of training losses during training, or a variant where the sum of training losses begins to be computed after the first E epochs, to estimate the generalization performance of the corresponding network. Although the results seem promising for query based NAS strategies, the reviewers agree that as the paper proposes something that is fundamentally opposite to the common practice, it requires more careful and thorough analysis. Besides, while the connection made by authors to the Bayesian marginal likelihood is interesting, it s not a rigorous argument that convinces the audience about the applicability of the proposed method. I strongly encourage the authors to add more analysis and discussion to the revised version to strengthen their claim and clarify its scope.  
A novel second order nonlinear oscillator RNN architecture is proposed, analyzed, and evaluated in this paper. The results are solid and impactful. Authors and expert reviewers showed exemplary interactions with each other, improving the manuscript in significant ways. All four reviewers overwhelmingly recommended accept. I recommend that this paper be selected as an oral presentation.
This paper proposes ScaleGrad, a simple technique to encourage generating non repetitive tokens for text generation tasks. The key idea is to modify a language model s token level distributions by rescaling the softmax probability for certain words (in the novel set) by a factor of $\gamma$. Experiments show that ScaleGrad outperforms MLE and Unlikelihood Training (UT).  This paper receives 2 reject and 2 accept recommendations. Most of the reviewers have provided very detailed comments, and the authors have also provided very long and detailed responses. On one hand, all the reviewers agree that the experiments are comprehensive, and the motivation of the proposed method is clear.   On the other hand, several concerns still exist after the rebuttal, namely, hand wavy arguments and inconsistent experimental protocol. (i) The empirical evidence in the experiments is not convincing enough. It makes reviewers more reluctant about the approach after seeing more experimental results during the discussion. That is, for different tasks, different $\gamma$s are used, while the hyper parameters used for other methods seem to be default values. This makes reviewers hesitant that scaling novel tokens and renormalizing the model s output distribution is really significant. (ii) Another minor concern is that the discussion on the potential issue of UL (sec. 5.4) does not look convincing. (iii) After reading the paper, the AC also feels that the novelty of the proposed method might be a little bit limited.   The rebuttal unfortunately did not fully address the reviewers  main concerns. On balance, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The reviewers were clearly excited by the novel application of group theory to the problem of composition, and think the core idea is good.  However, the reviewers also expressed concern about the clarity of the paper, stating that in several places examples might help. Reviewers were also interested in seeing the work tied to real world applications, and how the work expands our existing knowledge about the composition of learned representations.  I hope their suggestions will help the authors to turn this into a stronger, clearer paper.
This work studies an intriguing problem of searching optimal architectures for unsupervised domain adaptation. It is based on a two stage approach: (1) transferable architecture search via DARTS + MK MMD; (2) transferable feature learning via Backbone + MCD.  The reviews for this paper are very insightful, constructive and of high quality. While all reviewers acknowledge the contribution of a new research problem, they unanimously have suggestions for further improving the paper:   The novelty and soundness in the technical method are not fantastic. Authors should consider more elaborated loss designs for the approach, and unify the two stages with the same optimization objective.   The empirical evaluation is by far not extensive and insightful. More stronger NAS approaches and larger scale datasets should be included to give more evidence to support the claims that NAS DA is better.   A featured analysis of how non iid architecture (as searched out by this work) differs from iid architecture would make the paper much more interesting.  Authors did not participate in the rebuttal and discussion phase.  AC scanned through the paper and believes that this paper studies a promising research direction, but the work cannot be accepted before addressing the reviewers  comments. The weaknesses are quite obvious and will have a high probability of being asked by the reviewers of the next conference. So the authors need to make sure that they substantially revise their work before submitting to yet another top venue.
This is a novel, simple, and experimentally well supported new idea for entity linking.  The key insight is to perform entity linking by producing meaningful entity names with seq2seq approaches, and the big surprise is how well this works experimentally (at least for wikipedia style entities).  Very nice paper! 
This paper proposes a graph pooling mechanism based on adaptive edge scores that are then fed into a min cut procedure.  Reviewers acknowledged that this is an important topic of study, but all agreed that the current manuscript does not provide enough evidence about the significance and novelty of their proposed approach.  The AC recommends rejection at this time, and encourages the authors to build from the reviewers feedback to improve upon their current work. 
This paper proposes an approach to data augmentation to train image recognition models called SaliencyMix, which involves pasting salient regions (as judged by some saliency detector) of one image into another, and mixing the two labels accordingly. Most reviewers generally agreed that the proposed approach is simple   it is easy to understand the method and its motivation, especially in the context of related augmentation approaches like CutMix   and has solid experimental results demonstrating its effectiveness.  The main objection the more negative reviewers had to the work is a perceived lack of novelty. In my view it is a new method (even if similar to prior work like CutMix), and as AR5 argues: "this approach probably starts to get to the heart of why these previous strategies work: they are probably less effective ways of doing what this paper suggests." The improvements in Table 1, columns 1 & 3 (CIFAR 10 & CIFAR 100) especially speak to this   these improvements with traditional augmentation disabled are quite substantial, even though the differences become marginal when moving to the "+" augmented versions of the dataset (as well as in ImageNet). So although the method is indeed similar to CutMix, I agree that it offers valuable insight into why these previous methods work. Besides which the results *do* show improvements over similar methods, even if the improvements are marginal.  Overall, I recommend accepting the paper as it provides useful insight into why prior methods work and proposes a new one that in practice works slightly better. Minor comments for the camera ready version:  Please revise the writing based on AR4 s good suggestions.  Highlighting a comment from AR4: > BAsNet for example, was trained on 10k images. Why not simply include these (and their mask) as part of the pretraining when considering some of the baselines ?  I recommend including discussion of this important point in the final version of the paper. The learning based approaches are effectively using additional training data. It s good that a non learning based method happens to perform best so that the results remain comparable with prior work, but this should nonetheless be discussed if the learning based approaches are to be included.  Please remove the blue text coloring (if not already planned   I m not sure if this was done as a "diff" for the response).  > Figure 3(a b) show that Montabone & Soto (2010) performs better on both the datasets and the effects are identical on CIFAR10 and ImageNet datasets  I do not see how Figure 3 shows this. Is "OpenCV Saliency" in the figure using the method from Montabone & Soto (2010)? Please clarify this by making the connection between the bar labels in the figure and the discussion in the text clear for the camera ready version of the paper.
This paper considers the problem of learning models for NLP tasks that are less reliant on artifacts and other dataset specific features that are unlikely to be reliable for new datasets. This is an important problem because these biases limit out of distribution generalization. Prior work has considered models that explicitly factor out known biases. This work proposes using an ensemble of weak learners to implicitly identify some of these biases and train a more robust model. The work shows that weak learners can capture some of the same biases that humans identify, and that the resulting trained model is significantly more robust on adversarially designed challenge tasks while sacrificing little accuracy on the test sets of the original data sets.  The paper s method is useful, straightforward, and intuitively appealing. The experiments are generally well conducted. Some of the reviewers raised questions about evaluating on tasks with unknown biases. The authors addressed these concerns in discussion and we encourage them to include this in the final version of the paper using the additional page.
The reviewers unanimously raised concerns on the lack of insights on why the proposed method works better than Han et al., 2020, and why WTA brings significant gains only to the proposed method and not to Han et al. I think the paper is promising but providing these insights are critical to making the work convincing to the readers. The reviewers have made excellent points to improve the paper; I d recommend the authors to incorporate them in their future submission.
This paper investigates a non attentive architecture of Tacotron 2 for TTS where the attention mechanism is replaced by a duration predictor.  The authors show that this change can significantly improve the robustness. In addition, the authors propose two evaluation metrics for TTS robustness, namely, unaligned duration ratio (UDR) and word deletion rate (WDR), which appear to be novel to the TTS community. The proposed non attentive architecture yields good MOS scores in the experiments.    Overall, the paper is well written but the reviewers commented on the technical novelty of the work as it is essentially an improvement within the Tacotron 2 framework. There is also a lack of comparative study with other existing frameworks with similar techniques.  Although the authors put together a detailed rebuttal to address the comments, in the end the above two major concerns remain. 
The paper presents a mathematical analysis of the discrepancy between GD and GF trajectories. Following the discussion period, a knowledgeable R3 updated his/her initial rating from 4 to 6,  a knowledgeable R4 raised his/her score from 5 to 6. Finally, a very confident R1 considers this a good paper that should be accepted. He/she indicates that this paper provides a unique and very illuminating perspective on gradient descent through an extremely simple idea. The topic is very timely. I agree with R1 that the paper contributes a refreshing perspective with important elements which should be of interest to a good number of researchers. Taking into account the discussion, confidence levels and ratings of the reviewers, I am recommending the paper to be accepted. I would like to encourage the authors to take the reviewers  comments carefully into consideration when preparing the final version of the article. 
Pruning is an important problem in practice. The angle of this study is also interesting. The key concept proposed by this submission is called the "utility imbalance" of the weights.  There are many concerns raised by the reviewers. Let us summarize some of them here: (1) hard to follow even for the domain experts; (2) the definition and motivation on "utility imbalance" are unclear ; (3) loss landscape visualizations are too much simplified to be informative.  There are also lots of concerns on writings. The rebuttal did help clarifying some details. However, most of the concerns still remain. We hope the detailed comments from the reviewers will be useful for the authors to polish this work. 
Based on the observation that the stochastic gradients for deep nets often stay in a low dimensional subspace, this paper proposes projected differential private SGD (DP SGD) that performs noise reduction by projecting the noisy gradients to a low dimensional subspace. Under certain assumptions, the authors provide a theoretical analysis and empirical evaluations to show that the proposed algorithm can substantially improve the accuracy of DP SGD in the high privacy regime. There is unanimous support to accept this paper after the author’s response. Thus, I recommend accept.
This is an interesting paper discussing the impact of classifier abstention on the performance obtained for different groups of data. The reviewers are either very (scores of 8, 7 and 7) or moderately (score of 5) positive about the paper. The main concern is that the paper does not directly propose a solution for the discovered problems. Nevertheless, it can initiate interesting discussions and research around them.
The reviewers all agree that Monet proposed in the paper which optimizes for both local and global memory saving in Deep learning models is theoretically sound and experimentally convincing. Accept!
During the discussion phase, although the reviewers acknowledge superior empirical performance of the proposed method, they shared the two major concerns: 1. Lack of theoretical or empirical justification/proof for the key statement: "the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima". 1. Lack of comparisons with newer methods from e.g. ECCV2020 etc.  In particular, the first point is crucial. As the reviewers pointed out, since it is the main contribution and the key message of this paper, it should be carefully examined theoretically and/or empirically. However, in its current state, there is no theoretical analysis, and empirical evaluation is not convincing.   About the second point, although I think it cannot be a solo reason for rejection, at least it is better to cite and discuss it fo the completeness.  Overall, the contribution of this paper it not significant enough for publication. Hence I will reject the paper.
The 4 reviewers all had a consistent view of this paper:  concern that the scope of the work was overstated (paper claims, without evidence, to apply in more generality than the 1 example scenario shown); concern about the difficulty of implementing this approach (1 TSNN required for each rendered viewpoint); and lack of examples showing how the method performs under more challenging scenarios.  The AC encourages the authors to revise the work in response to the reviews.  That would involve additional experimentation and examples, and some attention to revising the manuscript.   After two of the reviewers complained of lack of clarity in the algorithm description, the authors replied, "We explain our algorithm in the paper; the reader can refer to our code for implementation details."  I hope the authors can be more responsive to the readers  concerns than that in their revisions.
Quality: the paper takes an important question and analyzes it well from a theoretical angle; it also provides empirical evidence to back up its main message in more complex models. The proofs are non trivial. The paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non asymptotic regime.  Clarity: The motivation of studying the double descent phenomenon with optimal regularization is well explained in the introduction. Connections and comparisons with existing related works are discussed clearly. The paper is clearly written, and exposes the results in a clear and accessible fashion.   Originality: The presented theoretical results on the linear regression model are non asymptotic, which is new and different from existing works.   Significance: The proof techniques seem to heavily depend on the specific choice of the loss function and the regularizer, that is, the mean squared loss and the ridge penalty. It is not clear if the techniques can generalize to other settings, which affects its significance.  Main Pros:   the paper takes an important question and analyzes it well from a theoretical angle. The proofs are non trivial; the paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non asymptotic regime.  Main Cons:   Generality of the results. The paper mainly focuses on a simplified linear regression model, where the response variable is linearly generated using some ground truth parameters \beta^*.    The experiments need to be more extensive and better explained, especially for the CIFAR 100 experiments. It is important to discuss this difference clearly at the beginning. 
This paper proposes a regularization term that enforces the orthogonality between (i) a residual between the observed outcome and its estimator and (ii) the treatment and propensity score. The method empirically performs competitively. However, there seems to exist a gap between the proposed method and the assumptions made to provide theoretical guarantees (e.g., R3, R2). R4 was also concerned about the issue and adjusted his/her score accordingly. Even though the authors provide a detailed discussion on most of the reviewers  concerns, some of the problems remain unresolved. Further, unlike other papers submitted to ICLR, the authors did not actually update the paper such that we could check whether the revisions were adequately made. As such, I believe this paper is not quite ready for publication in its current form. 
 The paper developed a method that estimates treatment effect with longitudinal observational data under temporal confounding. It extends the idea of the synthetic control method and offers flexibility and ease of estimation. However, some major concerns remain after the discussion among the reviewers. In particular, the proposed method lacks a clear use case. Moreover, some arguments around ``trustworthiness`` (detecting unreliable ITE estimates) and ``avoid over matching`` need to be refined. The error bound for ``trustworthiness`` can not detect hidden confounding. For overlap issues, the rejection of units with larger error could be overly conservative because the error bound may often be too loose. Regarding ``avoid over matching``, while SyncTwin uses a low dimensional representation as opposed to the whole x vector for matching, it is unclear whether SyncTwin can avoid over match. It is possible that using low dimensional representation makes it easier to find a match in the data and may still over match. Finally the paper would benefit from proper causal identification results.
The reviewers generally appreciated the problem statement and topic of the paper, but raised concerns across the board about the empirical evaluation. Since the paper is largely experimental in nature, a compelling experimental evaluation is important. Reviewer concerns generally centered around: (1) the relative simplicity of the evaluated tasks; (2) somewhat questionable baselines. While the authors provided responses to some of these concerns, after discussion the reviewers generally still felt that these issues were rather severe, and preclude publication at this time. I would encourage the authors to take this feedback into account in improving the empirical evaluation in the future.
The paper presents a "conceptual  advance connecting causality, disentangled representation learning, invariant representations and robust classification". Authors propose to decompose the image generation process to independent mechanism that can be composed (foreground masks (shapes), forground texture, and backgrounds), allowing for a specific image to generate counterfactuals , by changing some variations factors, while keeping other fixed. One can use interventional data to augment classifiers, this can lead in certain cases to improvement in accuracy and in other in improving the robustness.   There was concerns about the clarity of the paper regarding the structured causal model considered and its applicability beyond image generation, experimental protocol for choosing hyperparameters (loss scaling and ratios of real data and interventional samples ) and some missing references. The rebuttal of the authors and their updated paper reflected comprehensively all those concerns and addressed them, highlighting limitations of the method and adding more examples of its failures.   I liked the ideas and concepts in  this paper , and it will be exciting to generalize such generative approach to other domains, this  work is a first step. I think it will be good addition to ICLR program 
The work extends the line of work based on value iteration networks. The main goal is to extend VINS to continuous and partially observable state spaces. The approach combines self supervised contrastive learning and graph representation learning with VINs to address these issues. Reviewers liked the premise of the paper and had several follow up clarifications.  The authors provided the rebuttal and addressed some of the concerns. However, upon post rebuttal discussion, the reviewers decided to maintain their score. While everyone recommended weak acceptance, no one championed the paper. This was primarily due to the concerns in the empirical analysis. It is not clear that XLVINs are clearly outperforming VINs and Graph VINs in all settings. All baselines are not present in all the environments, so it is difficult to draw a consistent conclusion. The paper is in a good state but not fully polished to infer clear conclusions about the effectiveness of the proposed approach. Please refer to the feedback below for more details. We believe strengthening the experimental results section will turn this paper into a very strong submission.
Dear authors,  As you can see, reviewers agree on the importance of the analysis present in the paper but two reviewers feel like it misses important comparisons.  That said, PPO is a popular algorithm and I also welcome any attempt as improving our understanding of its dynamics. With this paper, information about PPO would be more complete but also more spread out across multiple papers.  At the same time, I am sympathetic to the reviewers  arguments and also feel that the paper would have had a much clearer message had some additional ablation studies been performed, for instance on tabular settings where this is easily done.  The overall assessment is that the paper is not yet ready for publication.
This paper studies the problem of how data should be balanced among a set of tasks within meta learning. This problem is interesting, and largely hasn t been studied before. However, the reviewers raised several shortcomings of the current version of the paper, including the significance of the problem setting, the limited experimental study (i.e. the only experiment with real data is CIFAR FS), the depth of the related work section, and the clarity/impreciseness of the writing. Further, the paper has not been revised to address any of these shortcomings. As such, the paper is not ready for publication at ICLR.
The authors propose a dataset and a method for the task of SpokenQA. The dataset is generated by using Google TTS to generate audio segments corresponding to the CoQA dataset and then using an ASR system to generate (noisy) transcripts of these speech segments. The authors then propose a method which uses a combination of various known techniques.   Pros:   A good first attempt at creating an interesting dataset for a useful task  Cons:   Lack of clarity in writing   Use of original clean text as input to the model which beats the purpose (in a natural setting such clean text will not be available)  All reviewers have appreciated the effort and attempt at creating a new dataset for this task. However, they have also pointed out that paper is not very clearly written and some important issues (use of clean text as input to the model) need to be adequately addressed before the paper is ready for acceptance. 
This paper proposes an online distillation method for efficient object recognition. The main idea is to employ a binary student network to learn frequently occurring classes using the pseudo labels generated by a teacher network. In order to identify rare vs frequent classes, an attention triplet loss is used. The proposed scheme is empirically evaluated on CIFAR 100 and tiny imagenet datasets.  The major and common concern from reviewers about this draft is the quality of presentation, which has made it difficult to read and understand the ideas and their underlying motivations. While specific instances of this were mentioned by the reviewers, and responded by the authors, the readability issues go beyond these instances. In fact, I could independently observe presentation issues different from those mentioned by the reviewers. For example, in Eq (1), what is eta (never defined before), what is the loss function here for which the gradient update leads to Eq (1)? The proof also is not clear and seems to have issues. For example, in Eq (13) what is the meaning of multiplying two vectors? If it means dot product, it should be denoted more clearly, e.g. as w^T w^* or <w,w^*>. In Eq (12), can alpha be negative? If not, should be clarified why, and if it can be negative, then (<w^*,w>)^2 >  (alpha n)^2 does not need to hold, but this inequality is used in Eq (14) anyway.  Overall, I think the submission can benefit from an overhaul of the writing. I encourage authors to resubmit after improving on that.
This paper was controversial amongst the reviewers. There is clear utility to the ICLR community: a new model of grid cells based on well known technique (SR) used frequently in ML; good science careful analysis showing the proposed model exhibits key properties and useful in synthetic navigation domains;  such work reminds of the important concerns in natural learning systems which is relevant to those that wish to simulate and build intelligence. Two of the reviewers with subject matter experience in the area advocated for acceptance.  On the other hand, many readers of ICLR may find the paper confusing and unsatisfying as some of the reviewers did. The empirical work was limited to small domains and mostly in the form of demonstrations a typically ICLR reader would expect a performance improvement claim or a scientific hypothesis tested by each experiment. Presented as a new algorithm for ML the paper might appear too limited and simple (e.g., relying on state aggregation). The reviewers with neuro background found the paper clear and well organized, while the ML reviewers found it confusing. The relevance of the work will be limited to a smaller subset of researchers but this is true of many ML works also. Finally, ML readers might be more familar with neuro work which propose computational models and then validate those models against real neural activity data from brains. This is work is not like that, rather using synthetic data to demonstrate important properties and explore empirical conjectures about the model.  In the end the paper is boarder line: the subject matter experts both listed issues that should be addressed (e.g., band cells issue), while the reaction of the ML reviewers suggests the impact of the work might be reduced at ICLR (compared to other venues). Additional text clearly articulating the scope and managing reader expectation could mitigate this concern, but it s not a small task to change the tone and pitch this way. Scientific conferences are about insights and understanding, this paper provides both. Please consider the suggested edits to maximize the impact of your work at ICLR this year.     
The paper proposes a new method to learn representation and community structure of a network jointly. The reviewers agree that the paper contains some interesting ideas but they raise also some important concerns. For example:    even after considering the authors  rebuttal, the paper seems not too novel. In particular the results seem a bit incremental over vGraph.    the notion of community is not formalized by the authors neither in the paper or in the rebuttal. The paper would benefit greatly by having such formal definition   Overall, the paper is interesting but it does not meet the high acceptance bar of ICLR
The paper presents a method to regularize the discriminator in  GAN training with a ranking loss based on the user preference for a desired set within a larger dataset. The tradeoff between GAN loss and preference loss dependence on the distance of the set to the full dataset and the authors consider two regimes : "small and major correction". A major correction is needed when the targeted set is very different from the whole density, authors propose in this scenario to replace samples from the data by samples from the generator. The setting in the paper is interesting and can be useful in practice.   There was a lengthy discussions between the authors and the reviewers, the discussion pinpointed issues , some of them were addressed in the rebuttal . Some issues remain unanswered regarding the clarity and some claims in the paper.  The clarity of the paper needs further improvement and  1)  clarify section 3  the setup and the background section   2)  justify claims about the method, in  the strong correction scenario  when fresh generated samples are introduced how  is this an effective procedure? (conceptually / theoretically).  
The paper proposes to predict sets using conditional density estimates. The conditional densities of the reponse set given the observed features is modeled through an energy based function. The energy function can be specified using tailored neural nets like deep sets and is trained trough approximate negative log likelihoods using sampling.   The paper was nice to read and was liked by all the reviewers. The one thing that stood out to me was the emphasis on multi modality. (multi appears 51 times).  This could be toned down because little is said about the quality relative to the true p(Y | x) and the focus is mainly on the lack of this in existing work.
The paper builds upon hypergraph convolutional networks (HCN), extending them to time varying hypergraphs in dynamical settings.  However, as some of the reviewers pointed out, it would be useful to explore other system variations to better justify the choices in this particular approach; perhaps an evaluation on a wider set of datasets would also strengthen the contribution of the paper,  as well as adding evaluation metrics that can be more appropriate for the application considered (stock market prediction). Also, concerns were raised by several reviewers regarding the somewhat incremental  improvement over the state of art, and the degree of novelty in the proposed approach. Overall, while the problem considered is important and the approach is promising, the paper in its current shape  is somewhat borderline and may require a bit of additional work to be ready for publication. 
The paper aims to do out of distribution (OOD) detection in multi label classification. However,  the challenges of extending energy based  OOD methods in multiclass to multi label setting is not big. This paper just defines the label wise free energy. The key challenging issues in MLC is the label dependency. The paper did not consider modeling the label dependency and devide it to several binary classification issues. And the paper did not provide the theory gurantee. The paper is far below the bar of top conferences.
The paper presents a quantization aware training method for GNNs. The problem is very well motivated, the method is well executed, and experiments are also well designed. The paper does seem relatively low on technical novelty.   All the reviewers are positive about the paper, and the paper has certainly improved significantly over the rebuttal phase.   So, we would like to see the paper accepted at ICLR. 
The paper proposes to augment the original  model to introduce the "luring effect", which can be used for detection and black box defense. Despite being an interesting setup, there are several weaknesses in the threat model (whether it is practical) and the evaluation (lack of adaptive attacks). Those concerns remain after the rebuttal phase.   Threat model: see the concerns raised by Reviewer 1 and the updated comments after the rebuttal phase.   Lack of adaptive attack: the authors assume that the attacker has very limited knowledge to how the system works. This could be viewed as a "black box setting" for adversarial detection evaluation, and actually many other detection works can perform almost perfectly in this setting, so it s not clear how significant are the results. The authors tried to consider some adaptive attacks in their rebuttal but the reviewers are still not fully convinced.  
The authors propose to improve the LMs ability on modelling entities by signalling the existence of entities and also allowing the model to represent entities also as units. The embeddings of the surface form and then entity unit are then added and passed through a layer to predict the next word. The paper evaluates on QA and conducts probing tasks and shows that such an entity modelling results in better performance.  All reviewers have found the idea conceptually simple and novel. At the same time a number of concerns are raised, with the most important being the lack of understanding around which and how hyper parameters matter for this model and, most importantly, the confounder introduced to the model by the much larger size of parameters introduced by the embedding layers. While the authors comment that not all the parameters are used all the time, the size of the embeddings still count at the total size of parameters a model has. Thus, without properly controlling for this (e.g., have an another model where the extra embedding params are given to another part of the model), it is difficult to determine whether adding more parameters was the solution or adding more parameters for modelling the entities. 
There is a lot of agreement on this paper, also reflected in the ratings. There were some technical comments initially, on the approach not being IC and interpretable, missing links to other works and technical descriptions of the network and experiments. The authors cleared up many of these issues though with their responses, providing good arguments in favor of their work. In general, reviewers agree the paper would be interesting to be included in ICLR.
There was a consensus among all the reviewers that the methodological contribution is not significant enough for publication at ICLR. In short, the main contribution of the paper is to include spatial modeling into a deep temporal point process model. However, to do that, they just use a well known method (KDE) on top of a methodology that is very similar to Du et al., KDD 2016.   In addition, in the original submission, the specific functional form for KDE was independent on time, as highlighted by the reviewers, which basically separates spatial and temporal modeling. Unfortunately, further experiments performed by the authors failed to show performance benefits on making it dependent on time.  The authors also claim that an additional contribution is the sampling method, however, this seems to thin of a contribution for a full paper.
The paper looks at the soft constrained RL techniques and proposes a meta gradient approach. One of the biggest problems with the Lagrange Optimization based CMDP algorithms is that the optimization of the Lagrange multiplier is tricky The proposed solution and empirical results have promise. The reviewers broadly agree on their evaluation and the major concerns on comprehension, additional experiments and as well as comparison with baselines have been addressed in the rebuttal.     Convergence rate and quality of fixed point reached. The authors mention convergence to local optima but omit the quality of this solution from perspective of safety. It would be useful to include a discussion on the topic, with potential references to concurrent work.  Other relevant and concurrent papers to potentially take note of:   Risk Averse Offline Reinforcement Learning (https://openreview.net/forum?id TBIzh9b5eaz)   Distributional Reinforcement Learning for Risk Sensitive Policies (https://openreview.net/forum?id 19drPzGV691)    Conservative Safety Critics for Exploration (https://openreview.net/forum?id iaO86DUuKi)  I would recommend acceptance of the paper based on empirical results, conditional on release of sufficiently documented and easy to use implementation.  Given the fact that the main argument is empirical utility of the method, it would be limit the impact of this work if readers cannot readily build on this method. 
The authors propose an approach to learn perturbation sets from data and go beyond the mathematically sound L_p adversarial perturbations towards more realistic real world perturbations. To measure the quality of the learned perturbation set the authors put forward two specific criteria and prove that an approach based on conditional variational autoencoders (cVAE) can satisfy these criteria. In particular, given access to paired data (instance and its perturbation), the authors train a cVAE which can then be used to generate novel perturbations similar to the ones observed during training. Leveraging this generative model the authors train models which are robust to such perturbations while improving the generalisation performance on clean data.  The studied problem is of high significance and the proposed solution is sufficiently novel. The reviewers agree that the paper presents a significant step in the right direction and will be of interest to the ICLR community. The authors addressed all major concerns raised by the reviewers. In my opinion, given the inherent tradeoff between the two terms in Assumption 1, and the approximation gap due to the design choices of the particular cVAE, I feel that a hard problem was reduced to an almost equally hard problem. Nevertheless, the principled approach coupled with promising empirical results are sufficient to recommend acceptance. I strongly advise the authors to incorporate the remaining reviewer feedback and try to tone down claims such as “certifiably robust” given the issues pointed out above.  
This paper tackles a very important topic in deep RL, namely automatic (non differentiable) hyper parameter tuning. It does so by combining ideas from genetic algorithms and neural architecture search with shared experience replay in order to obtain the key property of sample efficiency.  The proposed solution is communicated clearly, and the results are compelling (often 10x improvements), as well as qualitatively interesting.  Unfortunately for the authors, their original submission contained only part of the intended results, hence the borderline scores by some reviewers. In the meanwhile, a second suite of experiments have been added, which I think are compelling enough evidence to validate the paper s approach.
This paper proposes adding noise regularization, iteratively during training to word embeddings. The method is evaluated on CNN based text classification. Overall, there is novelty in the proposed method, however there are concerns about the experiments and analysis of the proposed approach.
The paper presents a spatial temporal prediction framework with causal effects of predictors for better interpretability. The idea is interesting and the touch on modeling causal relations could be useful in practical applications. The paper receives mixed ratings and therefore there has been extensive discussion. We agree that while the paper has some merits, it falls short on the following aspects:   1, One central issue pointed out by all reviewers is the evaluation. For example, the contribution on efficient attention was not compared to any previous work; most of the baselines do no have access to the spatial information, which makes the comparison unfair. The authors did add two more baselines with access to spatial information. However, there are not enough details and discussions to make the results convincing; In addition, other stronger baselines should be added.   2. The notation and technical presentation was extremely lacking in the submitted version, the amount of unintroduced notations. Even in their core contribution equations had major issues with norm and vectors mixed together (see the difference between the corrected equation in the Taylor equation and the one in the original submission)  After the discussion, all reviewers agree that the paper fails to provide a fair and convincing evaluation, and the ratings will be adjusted to reflect the discussion. We hope that the reviews can help the authors improve the draft for a stronger submission in the future. 
This paper investigates the use of class conditional architectures in GANs. It achieves this by employing neural architecture search (NAS) on top of reinforcement learning. Their main contribution is a “flexible and safe” search space; experiments are carried out on CIFAR 10 and  100. Standard performance results are augmented by diagnostic studies.  This paper received a total of five reviews which, remarkably, yielded the same assessment: the paper had merits but was marginally below the acceptance threshold. In general, the reviewers thought the idea was interesting and straightforward, experiments extensive and the paper was clearly written. The primary concerns brought up were novelty (i.e. just cGAN + NAS?, R1 and R2), minimal performance gain (R4, R5), unclear motivation (R2), lack of comparisons   including to other NAS for GAN methods   (R1,R3), limited to low resolution datasets (R2,R3), no reporting of time or space complexity (R1,R3), unclear where improvement comes from   no control for capacity   (R5).   On the point about comparing to NAS + GAN works, the authors responded, stating that the NAS + GAN methods brought up by the reviewer were unconditional GAN methods and pointed out that they made unconditional GAN comparisons in the Appendix.  The authors also emphasized to multiple reviewers that the point of the paper was not to improve NAS. Interestingly, they also made a comment to R5 that the point of the paper was not to improve performance of cGANs, but to improve understanding of them.	  The reviewers are unanimous in that this paper falls just below the bar for the reasons outlined above. Following the discussion phase, I see no reason to overturn their recommendation. I hope that the authors can use the feedback from these reviews to improve this paper and re submit it.
The reviewers agree that this paper overcomes a number of difficult algorithmic and technical challenges in parallelizing the RED method for image reconstruction.  
While the reviewers find the experiments in the paper somewhat interesting, they find that the paper does not sufficiently address whether the limitations shown for models in this paper translate to larger models and other, more realistic, tasks, or an artifact of the setup considered in the paper.  Overall the takeaways seem unclear from the paper and I believe it is not ready for acceptance.  Addressing the issues raised by reviewers and having a more clear discussion on connections to existing results will help the paper.
This paper presents an original perspective on how to learn layouts and modules of neural module networks jointly, in a way that encourages the emergence of compositional solutions. In particular, layouts are treated as messages from an emergent language, and iterated learning is used to encourage the emergence of structure. The paper shows good performance in inducing compositional structure in two datasets.  Summarizing the reviewers  doubts, one is that the idea is tested on relatively toyish data sets, and it is not clear how it would scale up. The second, coming from one reviewer, concerns a lack of originality that, honestly, I do not see. If anything, this is probably the most original paper in my pool.  Concerning the first point, that is a fair objection, but I think that getting good results on program learning on datasets such as CLEVER is more than encouraging for a paper that is introducing quite a novel idea for the first time.  Finally, the authors added new text and new experiments strenghtening their conclusion during the discussion.  I am strongly in favour of accepting this paper. 
The paper studies the effect of explicitly introducing stochastic label noise into SGD updates, showing both theoretically and empirically that this can improve model performance on datasets with "inherent" label noise. The intuition is that this helps the model escape sharp local minima, where predictions may be overconfident.   Reviewers broadly found the work to be conceptually and theoretically interesting, and the empirical results are promising. The paper is thus well posed to be of broad interest to the community.
This paper proposed a new method improving online reinforcement learning using offline datasets. Three reviewers suggested (borderline) acceptance and two did rejection. The main concerns of reviewers are (a) limited/incremental novelty (from all reviewers) and (b) limited experiments (from three reviewers). AC also agrees that the authors  response for novelty beyond the prior works, e.g., AWR (and CRR), is not convincing enough (although their goals/settings are different). AC also thinks that more discussion, analysis and results when offline datasets are poor (e.g., far from experts) are necessary to meet the high standard of ICLR (the authors provided some, but AC thinks it is not convincing enough). Hence, AC recommend rejection. 
This paper provides an approach for weakly supervised learning by label noise correction and OOD sample removal. Overall, all reviewers agree paper is simple and approach makes sense. The experiments are solid with results on Webvision and ImageNet Mini (there were initial concerns but rebuttal handled some of those concerns). AC agrees with reviewers and recommends acceptance. 
Auto Seg Loss uses a differentiable surrogate parameterized loss function that approximates using RL some of the non differentiable metrics for segmentation. Auto Seg Loss outperform cross entropy and other loss functions through a great number of experiments. The main concerns rised by the reviewers (More clarity on the abstract and intro, extending the related work, and performance experiments) has been addressed. Accordingly I recommend the paper to be accepted at ICLR 2021.
This paper proposes to use randomly wired architectures [1] in the context of GNNs and introduces a method for sampling random architectures based on the Erdős–Rényi model. The authors further include a theoretical analysis and two methodological contributions: sequential path embeddings and DropPath, a regularizer. Results are reported on two graph datasets (ZINC and CLUSTER) and on GNN based CIFAR10 image classification.  The reviewers agree that the empirical results presented in the paper are compelling. The value of the contribution largely lies in this aspect, namely the empirical analysis of an existing technique (randomly wired architectures) in the context of GNNs, in addition to several smaller empirical methodological contributions. I agree with the reviewers in that the nature of the contribution and the otherwise limited novelty calls for a more extensive and detailed empirical evaluation (ideally incl. e.g. FLOPS, wall clock time, memory usage) across a wide range of datasets and careful ablation studies, and I encourage the authors to improve on this aspect in a future version of the paper. The theoretical analysis is interesting, but, as pointed out by the reviewers both during the reviews and the later discussion period, does not add sufficient value to the main empirical contribution of the paper to push the paper beyond the acceptance threshold and does not satisfactorily address the question of how the method addresses the oversmoothing problem in GNNs.  [1] Xie et al., Exploring randomly wired neural networks for image recognition (ICCV 2019) 
This paper proposes replacing the softmax of deep NNs with a kernel based Gaussian mixture model, to allow for per class multi modality.  Results show that the method is competitive with other output modifications such as the large margin softmax.    The  two primary concerns of the reviewers were the lack of large scale image classification results and theoretical guarantees.  The authors have added CIFAR 100 results.  Moreover, the authors agree that theoretical results would be nice to have, but such results are non trivial and likely require a PAC Bayes treatment.  I find the method to be well motivated and that the paper demonstrates sufficient experimental rigor.  Given the popularity of the softmax throughout deep learning, this paper will likely be of interest or at least, be of potential use to a large part of the ICLR community.  I encourage the authors to add the ImageNet results to the final version. 
The paper got mixed reviews ranging from 5 to 7. The main concerns of the reviewers were the missing novelty as the paper combines different well known methods for a given problem, so there is no big algorithmic contribution. The presented pipeline for closed loop grasping using imitation learning from a planner, Dagger and subsequent deep RL with TD3 is a straightforward, but sound and intuitive combination of algorithms to address the problem of closed loop grasping.  The presented results and ablation studies also motivate these algorithmic choices. In the rebuttal the authors addressed most concerns regarding the experiments (missing comparisons to open loop grasping and real world experiments), but more real world experiments would be necessary to evaluate the effectiveness of the approach.    This is a borderline paper were  I unfortunately have to recommend rejection due to the missing algorithmic contribution, a major requirement for ICLR. The paper would  however fit very well to a robotics conference and the authors are  encouraged to resubmit the paper the venues such as RSS or CoRL.  
After reading the author’s response, all reviewers recommend accepting the paper. R2 and R3 strongly support the paper while R1 and R4 consider it borderline.  There is agreement that the idea of the work is interesting and novel. The experimental results look solid.   The authors provided an extensive response addressing most of the concerns of the reviewers. In light of this feedback, the reviewers provided some additional comments (which the authors could not address, as the discussion period was over). The AC considers that the authors should incorporate this feedback to the final version of the manuscript. Specifically,  Responding to R1 s first question regarding the noise distribution on the original image being significantly different from Gaussian. The authors provided detailed results, which is highly appreciated. As R1 points out, the authors had to introduce an additional VST for the method. These results should be added to the manuscript is important, to show the limitations of the approach.  R3 asks about the importance of the initialization of the weights of the encode and decoder. This is a natural question as this is a non convex problem. The authors clarify in the manuscript the initialization of x, but do not comment on the weights. It would be good to add a sentence in this regard (as done in the discussion).  R4 mentioned, and the AC agrees, that the authors should try to improve the clarity of the exposition.  The AC considers it important to add in the appendix more visual examples to quantitatively show the performance of the method.
Overall, there were significant concerns about the motivation and experiments in this paper, and these were thought not to merit acceptance on their own. Because of this, the reviewers started discussing the theory to see if that would justify acceptance. The reviewers were not able to find a clear advantage over existing approaches, nor sufficient motivation; also the presentation was found to be largely inaccessible. In the rebuttal there was a brief mentioning of background and possible implications, but they were hard to assess and the paper itself did not have such context nor was updated to have such context. For a future version, one recommendation could be to focus significantly more on context, motivation, and improvements over prior work. Also, making the paper more self contained could help. 
We thank the authors for their detailed responses to reviewers, and for engaging in a constructive discussions.  As explained by the reviewers, the paper is clearly written and the method is novel. However, the novelty is to combine existing ideas and techniques to define an objective function that allows to incorporate cluster assignment constraints, which was considered incremental. Regarding quality, the discussion highlighted some possible improvements that the authors propose to do in a future version of the paper, and we encourage them to follow that direction. Regarding significance, although the experimental results are promising there were some concerns that the improvement over existing techniques is marginal, and that more experiments leading to a clearer message would be useful.  In summary, this is not a bad paper, but it is below the standards of ICLR in its current form.
The submitted paper proposes a novel model/approach for deep clustering which shows good empirical performance on a set of standard benchmark datasets as compared to state of the art baseline algorithms. While I believe that this paper can be turned into a good ICLR paper, it doesn’t meet the standard of ICLR in its current form.  More specifically: 1) The quality of the writeup is poor, containing many typos but more problematically many unclear/confusing statements which are either vague/unclear, not supported by citations and/or substantiated in other parts of the paper (Examples: „This might result in inferior clustering performance, degenerated generative model, and stability issues during training.“ Or “However, this objective seems to miss the clustering target, since the reconstruction term of is not related to the clustering and actual clustering is only associated with the regularization term optimization.“). An important contribution of the paper could be to substantiate these statements and I argue that achieving better performance alone is not sufficient therefore.  2) From a theoretical perspective, it would be interesting whether there is any justification for statements like the ones references above. Furthermore, the authors’ approach involves several approximations whose implications are neither studied nor explained. The authors responded only partially to questions in that regard by reviewers leaving certain concerns unanswered.   3) From an empirical perspective, an extended study of the proposed approach would help t better understand its benefits over existing approaches. Commonly considered settings like mismatch in the number of specified clusters are not studied at all. The proposed approach also seems to be initialized by VaDe (mentioned in Section 4) and it would be interesting to understand to which extend this is necessary and why (and how does performance change/degrade if this is not done). It also makes statements regarding stability unclear as training VaDe itself can be quite challenging. Furthermore, the overall algorithm for training the proposed model should be presented in a compact form in the paper. The paper should also be self contained in the sense of containing information on the important hyper parameters needed for training the proposed model.  In summary, the proposed approach is potentially interesting but the paper should not be accepted in its current form. 
The authors propose two linguistic verifiers for improving extractive question answering when the question is answerable. The first replaces the interrogative in the question with candidate answers and evaluates the result both in isolation and in combination with the answer containing sentence to do answer verification. The second jointly encodes individual sentences and spans with questions in a hierarchical manner to improve use of context in answer prediction performance.  The reviews for this paper are roughly on the cusp: 2 reviewers rate the paper a bit below the acceptance threshold, 1 a bit above, and then 1 now rates the paper as a solid Accept.   Pros    The main strength of the paper, certainly as emphasized by the most positive reviewer is the strong empirical results. Especially on SQuAD v2, the method here seems to roughly equal the current leading system on the leaderboard.   The paper also proposes two methods for improving question answering that make sense, are relatively simple, and work  Cons   The writing and presentation of the paper is not that great. Even at the level of the introduction, the writing just is not very focused: The first page has a lot of background and tutorial information on MRC that just doesn t get to the point of where this paper is situated and what it contributes.   Neither of the proposed systems are that novel (though it is interesting to see that they still have value even in the age of large contextual language models)   The paper lacks ML novelty   The methods appear to be significantly more expensive to run   Some empirical comparisons appear to be lacking  As well as the missing comparisons mentioned by some reviewers, I think that there are a number of other missing relevant datapoints. While not denying that gathering the available results for NewsQA/TriviaQA is much less straightforward than with that nice leaderboard for SQuAD, aren t there are lot of systems with better results on TriviaQA that aren t mentioned in the paper. These include: RoBERTa and SpanBERT (mandarjoshi); BigBird ETC see https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9 Paper.pdf; Longformer; SLQA see https://www.aclweb.org/anthology/P18 1158.pdf .  But, overall, I think the decision on this paper comes down to focus and contributions. Not withstanding the growing size of ICLR, I would like to think that it is not just another ML and ML applications conference, but it is a conference centered on representation learning. The present paper, no matter its quality and strong results, just isn t a contribution to representation learning. It is a much better fit to an NLP conference where it would be a strong contribution to question answering, showing the continuing value of linguistic methods like question rewriting in answer validation. But this just isn t a contribution within the focus of representation learning. Just as R4 does, I encourage the authors to clean up the presentation of the paper a bit and to submit it to an NLP conference, where it would be a strong contribution, for the reasons that R3 emphasizes. 
This paper proposes GAN training of a non autoregressive generator for text. To circumvent the usual problems with non differentiability of text GANs, the authors turn to Gumbel Softmax parameterisation and straight through estimation.   There are a number of aspects to this submission and they are not always clearly positioned. I will concentrate on the two aspects that seem most crucial:  1. The authors position their generator as an implicit generator, but it really isn t. If we take the continuous interpretation of the output distributions: the Gumbel Softmax transformation does correspond to a tractable density, the Concrete density of Maddison et al, with known parameter. If we take the discrete interpretation of the output distribution: Gumbel argmax is just an alternative to sampling from a Categorical distribution with known parameter. In either case, the generator maps the noise source to a collection of conditionally independent distributions each of which has a known parameter and analytical density/mass function. The authors do, however, train the architecture using a GAN type objective *as if* the generator were implicit.  2. In the discussion phase the authors added that GAN training overcomes the independence assumptions made by the generator. Whereas that makes intuitive sense, it suddenly changes the emphasis of the contributions, from proposing an implicit generator (presumably powerful for it being implicit) to proposing a way to circumvent the strong independence assumptions of the generator with a mechanism other than more traditional approximate marginalisation of VAEs. In their rebuttal, the authors commented on the use of non autoregressive VAEs in neural machine translation, and though those observations have indeed been made, they might well be specific to MT. The simplest and more satisfactory response would be to ablate the use of the GAN objective (that is, to train a non autoregressive VAE, also note that, with the same choice of likelihood, posterior collapse is rather unlikely to happen).  Other problems raised by reviewers were addressed in the rebuttal, and I would like to thank the authors for that. For example, ablating the non autoregressive generator and comparing to REINFORCE. I believe these improved the submission.   Still, I cannot recommend this version for publication. I would suggest that the authors consider careful ablations of the components they see as precisely important for the results (that currently seems to be the GAN like objective despite the model not, strictly speaking, requiring it).      
The authors propose to approximate the kernel matrix used in the Sinkhorn algorithm by a combination of sparse + low rank approximation. To do so, the authors propose to compute a low rank approximation of a sparsified (thresholded below a certain value to be 0) kernel matrix using Nyström, and then correct it by adding back the true entries at non sparse entries, after removing those obtained from the approximation. This results in a matrix whose application then results in sparse + low rank.  The first version of the paper contained mostly experimental evidence, which was deemed a bit short by some reviewers.The authors have added theoretical material on the way. Although I believe these are worthy additions, as AC, I do not feel comfortable accepting the paper as of now, because I believe these additions were not properly reviewed. I understand this must be disappointing for the authors, who have sprinted to add new content during the rebuttal phase, but I hope they agree that the rebuttal process is not here to handle entirely new sections, but rather to improve existing parts. In particular, that section should be reviewed by authors knowledgeable on low rank kernel factorization, something I did not see in the pool of reviewers. I also believe the paper still has a few shortcomings. Taken together, I therefore recommend a re submission.  ideas to improve the paper    the authors claim to use Nyström on a sparsified matrix (see eq. 4). The sparsified kernel is no longer positive definite. I would like the authors to comment on this. I understand Nyström could be used naively without any psd ness guarantees, but I think a heads up is needed.There are, furthermore, several local/global factorizations of kernel matrices available out there (e.g. MEKA, https://www.jmlr.org/papers/v18/15 025.html), the main difference here being that the product by such approximation must be guaranteed to be positive for it to work in the Sinkhorn algorithm. I would expect that bounds in expectation to break down sometimes, and therefore result in "catastrophic" failures (i.e. nan s). I think that an algorithm that claims to improve or replace another one, and which has such blind spots, needs such additional experiments (I have read the Limitations section in Appendix B, something more precise would improve the paper). I understand these were not part of the original Nyström paper for Sinkhorn, but since this is an increment over that previous work, therefore lacking a bit its originality, more knowledge needs to be contributed.    For instance, since the authors write an entire paragraph on this (Appendix B), I am surprised that there is not direct mention to the fact that a sparse sinkhorn may simply *not* converge, because it may not satisfy the fully indecomposable property required of matrices for Sinkhorn s algorithm to converge.     i dont think that users have the various identities (14,15) in mind when they think about "backpropagating" through Sinkhorn. What is typically needed is to compute the differentiable properties of the regularized OT matric and/or of the regularized OT cost w.r.t. *point locations* (i.e. x_i). The statement "LCN Sinkhorn works flawlessly with automatic backpropagation" is misleading in the sense that it ignores that problem altogether. Since so many extensions of OT today relay on that differentiability, the section, as it is written now, is problematic.    several methods claim to be faster of more efficient than Sinkhorn to solve OT. Either these methods display faster theoretical convergence (e.g. by using acceleration) or display faster practical convergence (e.g. heavy ball variants) using synthetic, controlled datasets. Using synthetic data helps exhibit highlight relevant regimes for regularization parameters, including those where LSE Sinkhorn may converge but LCN does not work, or vice versa. I understand that the authors  wanted to use real data, but it would be great to clarify whether that setup was used because LCN works better there (in which case this becomes more of a paper at the intersection of OT and word embeddings) or because this happened to be the first and only example the authors thought of.
Thank you for your submission to ICLR.  The reviewers and I are in agreement that the work presents some interesting connections between closed loop control and stabilization of activations to an observed manifold.  Specifically, the idea of using optimal control dynamic programming techniques to compute optimal adjustments to ensure control on this manifold is an interesting one and may have other implications within deep networks.  Although the reviewers were convinced by the experiments on robustness, I remain a bit skeptical here.  The results show that while the method marginally improves robustness to small epsilon perturbations, the models are still quite non robust against the size perturbations frequently used in assessing adversarial robustness (e.g., to eps 8 perturbations on CIFAR10, where the best approach gets ~11% accuracy against PGD attacks).  It doesn t really matter how well a defense works against sub optimal attacks: if PGD is able to decrease its accuracy this much, clearly the model is not very robust (and it seems upon reading that only 20 step PGD, with no restarts, was used as an attack, which is a fairly weak variant of PGD).  Furthermore, the approach didn t improve much upon PGD based adversarial training when combined with it, either, overall suggesting that the impact on robustness is somewhat minor, and needs to be evaluated quite a bit more.  While I don t believe these concerns are substantive enough to override the beliefs of all reviewers, I think that the authors could do a much better job of evaluating the actual robustness of these models (following the advice of https://arxiv.org/abs/1902.06705).  And if the resulting metrics are not as strong as hoped for, then it would be good to evaluate other possible benefits of the approach (perhaps to random distribution shift? it seems a much more likely situation for there to be real gains?).  Thus, while I believe the paper has some interesting ideas, I think the authors should probably tone down some of the current claims of improving adversarial robustness unless they can provide a much more thorough evaluation.
This paper investigates a speech synthesis approach that directly generates raw audios from text or phoneme inputs in an end to end fashion.  The approach first maps the input texts/phonemes into a representation sequence that is aligned with the output at a lower sampling frequency by a differentiable aligner and then upsamples the representation sequence to the full audio frequency by a decoder.  A number of techniques including adversarial training and soft DTW are applied to improve the training.  The experimental results are good. There are raised concerns from the reviewers which are mostly cleared by the rebuttal of the authors.  After the rebuttal and discussion, all reviewers are supportive on accepting the paper. 
This paper proposes an approach to allow a neural network to memorize and reason over a long time horizon. Experiments on synthetic datasets, question answering, and sequence recommendation are presented to evaluate the proposed method.   The paper addresses an important problem of processing long sequences. However,  all reviewers agree that the writing of the paper can be improved (i.e., motivation, details of experiment design/setup, and others below). Importantly, I think the authors need to elaborate the differences of continual memory with existing episodic memory methods. The authors added a paragraph about continual learning during the rebuttal period, and mentioned that their continual memory focuses on remembering infinite information stream without forgetting. Episodic memory models can be applied/adapted for this purpose, so the authors should at least compare with one of them (ideally more).
The authors study the theoretical performance of a meta learning in two settings. In the first one the overall number of possible tasks is limited and tasks are close in KL divergence. The second setting is MAP estimation (in a hierarchical Bayesian framework) for a family of linear regression tasks. Lower and upper bounds are provided on minimax parameter estimation error. This paper has spurred a lot of discussion among reviewers and (competent) external commentators. Most of these criticisms were right on target, but the authors managed to convince the reviewers and myself that there was simply an issue of presentation of the main results. I suggest the authors to take into serious considerations all the aspects raised by the reviewers that has generated misinterpretations of the presented results.
The reviewers found this to be an interesting and clearly written paper, but broadly agreed that it is not yet ready for acceptance. In particular, multiple reviewers felt that the experiments don t show clear benefits of the proposed SVAE approach when compared to the VAEVAE and other baselines; nor do they sufficiently back up the central claim regarding relative benefit of PoE vs MoE for either "AND" or "OR" relations. Hopefully the comments and suggestions from the reviewers, particularly regarding framing and experimental validation, will help in revising the paper.
This paper presents  a new approach to model uncertainty in DNNs, based on deterministic weights and simple stochastic non linearities, where the stochasticity is encoded via a GP prior with a triangular kernel inspired by ReLu. The empirical results are promising. The comments were properly addressed. Overall, a good paper.
The authors propose a technique called Autoencoder Adversarial Interpolation (AEAI). The key idea is to train autoencoder architectures that explicitly "shapes" trajectories in the encoder (latent) space to correspond to smooth geodesics between data points. This is achieved by a combination of several loss terms that are fairly intuitive. The authors empirically justify each term via ablation studies on simple datasets.  Initial review scores had wide variance. The reviewers liked the overall approach as well as the clarity with which the theory and experiments were presented, but raised several concerns. The authors provided succinct responses that seem to have satisfied the reviewers on average.  Unfortunately, after having carefully read this paper (and the authors  responses), I have to go against the wishes of the majority of the reviewers, and recommend a reject. My two main concerns are as follows: a) The synthetic pole dataset, as well as the COIL 100 dataset, are far too simplistic to evaluate performance. It is now standard to report results on considerably more challenging datasets. b) Echoing R2   the authors should articulate why a shaped latent space should actually matter in applications, beyond giving intuitive(I guess?) visualizations and reconstruction error curves. Results on downstream tasks may be one avenue to achieve this.
The paper proposes to create models that address tail classes by computing a linear combination over models (concatenated weight vectors). Reviewers had grave concerns about the technical contribution, including justification of linear averaging of non linear models, and about the experimental results, which improve on tail classes but hurt overall performance. As a result, the paper cannot be accepted to ICLR. 
This paper studies through empirical analysis an interesting problem: distilling the (strong) inductive bias of a teacher model to the student model (of weak inductive bias). The main claim/finding is that not only the "dark knowledge" in the logits can be transferred, but also the inductive bias (e.g. recurrence in RNN and translation invariance in CNN) can be transferred to make the student model stronger. This conclusion looks not very surprising but does contribute some new ideas to the fields of both deep learning and transfer learning.  The paper receives insightful but controversial reviews. Throughout reading the lengthy rebuttal and discussions, the AC, as a neutral referee of both sides, thought that while some expressions in the discussions seem a bit urgent and strained, both reviewers and authors managed to participate in the academic debate with a professional attitude that focus only on the technical issues. These discussions are very extensive and helpful for drawing a thorough understanding of this paper, and of the important research problem.  After the public interactions with the authors, a private discussion was performed between all reviewers and the AC, and among the four reviewers, one argued for rejection, one voted for rejection, one voted for acceptance, and one argued for acceptance. The AC believed that one of the reject votes lacks enough support in the comments and thus discarded it. However, due to the wild disagreement between the reviewers, as well as between the reviewers and the authors, the AC read the paper carefully. The AC s main points are as follows:    The research problem is interesting, and this paper appears to be the first work that studies the inductive bias transfer problem.    The paper has made its endeavor to try to delve into this problem, through providing with extensive empirical results and analyses.    The biggest weakness of this paper is the experimentation approach towards quantitatively studying the inductive bias: comparing the teacher and student models through the relational similarity between the penultimate layer representation is simply not enough to justify that the inductive bias has been distilled/transferred.  Two reasons: + Due to the expressiveness of neural networks, it is not hard for the student to resemble the teacher s representations; in fact, this is a quite common result repeatedly used by researchers and practitioners, even when the student is only a smaller model. While the idea of distilling inductive bias is interesting, it simply cannot be sufficiently justified by the current experimentation design. + Inductive bias is something encoding our prior knowledge about the learning task and is often effective during the whole training procedure, which cannot be refreshed by the training data. However, albeit the distilled student model (transformer or MLP) resembles the representations of the teacher model (RNN or CNN), it is not certain whether the "distilled inductive bias" can linger in the student model if you further fine tune the student model to downstream tasks. That is, it is highly possible that such "distilled inductive bias" of the student model will be refreshed by the future training data. In contrast, if we directly fine tune the teacher model to downstream tasks, their inductive bias (recurrence of RNN or translation invariance of CNN) will be retained successfully in the fine tuned model. Basically, through this thinking, it is not clear whether the inductive bias has be distilled. If the distilled thing is refreshed out, it is probably not the inductive bias. More experimentation or a formal quantification of inductive bias is highly necessary here.  While Reviewer #1 was a bit skeptical in the comments and discussions (regarding which I had a private discussion with him/her), some of his/her comments are reasonable and should be well addressed before this paper could be accepted:   Be rigorous in scientific writing. While the experiments with bias variance tradeoff and calibration are interesting and relevant, the key concepts were used with less care. It is good to expand the authors  understanding of these concepts to make sure what they actually refer to.   Try to provide sufficient elaboration when you try to claim something. It is true that for now, in our field, there are quite a few papers claiming something very big in the title or abstract, but simply cannot fulfill their story through rigorous or sound technical study. I suggest to tone down some of the key claims such that "inductive bias can be transferred" if they are not clearly provable.  Finally, AC believes this paper studies a very interesting problem that may draw wide attention, and the paper is acceptable in a future version if the above comments are well addressed. Since this is already a resubmission (as mentioned by Reviewer #1), I d encourage the authors to focus on the technical parts of the comments and revise the paper substantially before submitting to yet another top venue.
This paper addresses the problem of visual object navigation by defining a novel visual transformer architecture, where an encoder consisting of a pretrained object detector extracts objects (i.e. their visual features, position, semantic label, confidence) that will serve as keys in an attention based retrieval mechanism, and a decoder computes global visual features and positional descriptors as a coarse feature map. The visual transformer is first pretrained (using imitation learning) on simple tasks consisting in moving the state less agent / camera towards the target object. Then an RL agent is defined by adding an LSTM to the VTNet and training it end to end on the single room subset of the AI2 Thor environment where it achieves state of the art performance.  After rebuttal, all four reviewers converged on a score of 6. The reviewers praised the novelty of the method, extensive evaluation with ablation studies, and the SOTA results. Main points of criticism were about clarity of writing and some explanations (which the authors improved), using DETR vs. Faster R CNN, and the relative simplicity of the task (single room and discrete action space). There were also minor questions, a request for more recent transformer based VLN bibliography, and a request for a new evaluation on RoboThor. One area of discussion   where I empathise with the authors   was regarding the difficulty of pure RL training of transformer based agents and the necessity to pre train the representations.  Taking all this into account, I suggest this paper gets accepted. 
This paper is right at the borderline: the reviewers agree it is well written, proposing a simple but interesting idea. However, there was a feeling among the reviewers (especially reviewer 1) that the paper could be strengthened considerably with a better discussion/some theory on the sufficiency of the calibration vectors, as well as experiments on larger datasets. Doing one of these would have substantially strengthened the paper. Due to the remaining shortcomings, the recommendation is not to accept the paper in its present state.
Reviewers split on this paper with one arguing that it is an intriguing and significant paper for both neuroscience and deep learning, whereas others argued that it fails to answer some key questions and stops short of offering testable predictions or novel findings. In particular Reviewer 2 questioned the limited experimental predictions and their experimental backing, as well as the plausibility of gradient calculations.  Reviewer 4 raised more fundamental concerns about the significance of the paper s contributions. All reviewers appreciated the paper s clarity. Overall, though, Reviewers 2 and 4 raised enough significant concerns that I cannot recommend acceptance. 
This work investigates the relationship between adversarial robustness and shape bias of neural networks. Reviewers pointed out that one of the primary questions being investigated "(a) how adversarially robust ImageNet classifiers (R classifiers) generalize to out of distribution examples;" has already been a primary focus of several prior works, and that many of the findings are already well established, or expected given known connections between adversarial robustness and corruption robustness. I recommend the authors rework the paper to focus more on building upon these prior results. As a possible example, the work would be strengthened if the authors compared adversarial training to other data augmentation strategies known to directly improve shape bias and corruption robustness, does adversarial training provide any unique ood robustness properties distinct from these other methods?
The authors present an adaptive model that learns a good policy by adversarial training, focusing on the setting where the query budget is very small. Some experiments are carried out to validate the proposed method. The reviewers  opinions turned out to be split on this paper. On one hand, all reviewers appreciated the idea of the problem and recognized its importance. On the other hand, there are have been multiple concerns regarding readability (but that has improved during the discussion) and about the empirical validation/evaluation. Based on the above, as well as my own reading, I believe this paper contains interesting ideas but, as it currently stands, is not ready for publication.
The paper investigates the interesting question whether increasing the width or the number of parameters is  responsible for improved test accuracy. The paper is very well written and the question is novel and innovative. From a methodological point of view, the experiments are well conducted, too. The theoretical part of the paper is somewhat detached from the experimental part and constitutes more of a heuristic conjecture. In addition, more experiments on a variety of other data sets would have been great. Ideally, the theoretical section would thus be replaced by such additional experiments, but this is of course not  an option for a conference reviewing system. Given the innovative question and well conducted experiments I think that the pros outweighs the cons, and for this reason I recommend to accept the paper. Reviewer concerns have been well addressed by the authors in their rebuttal and updated version of the paper.
This paper proposes a method for differentiable pruning that replaces the hard thresholding of standard pruning, with a soft version that permits taking the gradients of the pruning threshold. The proposed benefits are an accuracy that is better or competitive with alternative methods as well. Moreover, the paper suggests the technique to be efficient.  The pros of this paper are that it is working in an interesting setting of differentiable pruning, with the hope of   in some sense   simplifying the pruning process or at least unifying the process with standard training.  The technique is plausibly justified in its technical development. The paper also follows with a significant number of experiments.   The cons of this paper are that the conceptual framework   beyond the initial idea   is not fully clear. In particular, this paper does not elucidate a clear set of claims and hence, results in the difficulty on the Reviewers part in detangling the claims and identifying the appropriate comparisons.  For example, the paper doesn t take up a simple claim that it is state of the art in accuracy vs parameter measures (and would seem not to given the results of Renda et al. (2020)).  It need not necessarily make this claim, but there are suggestions to such a claim early in the paper. If this is not an intended claim, then the paper can remove any suggestions to such (i.e., the claims around new SoTA for networks not evaluated in prior work).   The paper has a somewhat tentative claim that it is more efficient (in the total number of epochs of training) versus other techniques (Table 3).  However, the presented results are only at a single point versus other methods.  Renda et al. (2020) directly consider accuracy versus retraining cost trade offs. Appendix E of that paper provides one shot pruning results for ResNet 50 showing accuracy on par with that presented here.  The number of retraining epochs is also similar to here. This paper, however, only compares against the most expensive iterative pruning data point in the other paper.  In sum, my recommendation is Reject. This is promising work that needs only (1) to include a few testable claims and (2) to re organize the results (and perhaps run a limited set of new results) to thoroughly explore those claims. For example, if the most important claim is accuracy vs retraining cost, then it needs to show a more complete trade off curve of the two results.  Of course, this, in principle, opens the door to comparisons to many other techniques in the literature. 
The is a borderline paper with the reviewers split in their recommendations.  The decision is therefore not easy.  The work is promising, but a key concern is that the contribution appears incremental: the paper proposes to alternate between kickstarting, which is itself not entirely new as an idea, with a simple instance transfer heuristic.  The resulting method is straightforward, which can be considered a strength, but there is no serious technical justification beyond intuitive motivation.  Rather than present technical analysis, the paper focuses more on intuitively delivering the proposal then evaluating it.  This would be acceptable if the empirical outcomes were undeniably impressive, but the outcomes, though positive, are not overwhelming.  The experimental evaluation is limited in scope, considering only the simplest MuJoCo environments and a benchmark racing simulator.  The authors responded to some of the criticisms forcefully, and were comprehensive in their rebuttal, but if the support for the proposed method is to be entirely intuitive and empirical, one would have expected a more comprehensive evaluation where transfer was used to solve more impressively difficult problems.  Overall, I think this work would be better served by adding a technical analysis that validates the significance of the instance transfer heuristic, combined with a broader empirical study that tackles more challenging domains. 
The paper presents a thorough comparison of different algorithms for multi agent Deep RL methods. The conclusions of the paper is that across a variety of envionment and hyperparameter tuning, multi agent PPO seems to peform well relatively to the competitors.  The reviewers agreed that the paper fills a gap in the literature regarding a fair and thorough comparison of algorithm, and that the paper clearly presents the results. As it stands, the code to reproduce the experiments and the results are a useful contribution to the community. However, the reviewers felt the technical contribution to be below the bar for ICLR, as the paper does not help in understanding the differences between algorithms, or develop insights as to how to further improve algorithms. The large standard deviations of the various algorithms also makes the main experimental insight (MAPPO works consistently well) relatively weak.  
The paper is in general well written and easy to follow, and the considered approach of controlling beta is sensible. However, all reviewers identify shortcomings in the empirical analysis of the proposed method (missing comparison with stronger baselines, convergence issues of the considered baselines, considered datasets, etc.). Furthermore, compared to the ControlVAE the contribution of the paper seems limited; and the empirical evaluation is insufficient to claim superior results in general. The authors did not address most of the concerns raised by the reviewers in their rebuttal. The authors can improve their paper substantially by performing the experimental results proposed by the reviewers and clarifying differences to the ControlVAE—but in its current form the paper does not meet the standard of ICLR. 
Two reviewers recommend rejection, whereas two reviewers slightly lean towards acceptance. All reviewers agree that the paper tackles an important problem, and the proposed direction holds promise and is worth exploring. However, the reviewers raised concerns about the novelty of the proposed approach [R3,R4], the applicability of sparsification to GCN based models [R3,R4], baseline experiments [R1,R3,R4] and the gap between the theoretical aspect of the paper and the implementation of the proposed approach [R2]. The authors engaged with the reviewers during the discussion period and succeeded in motivating the speedup gains of their method, and clarifying some of the reviewer s concerns. However, after discussion, the reviewers still think this is a borderline paper, which could be significantly strengthened by validating the applicability of the proposed sparsification to other GNNs [R1,R2,R3], and in particular, by including the suggested FastGAT sparsified GCN experiment [R1,R3,R4]. The paper could also benefit from improving the presentation of both the analyzed approach and the practical one [R2]. I agree with reviewers  assessment and therefore must reject. However, I acknowledge that the paper does raise notable interest and I encourage the authors to consider the reviewers  suggestions in future iterations of their work.
**Overview**: This paper provides a new clustering based method to predict future probability density of a policy. It provides comparable performance to prior Q learning based methods, but without careful hyper parameter tuning.  **Pro**: The method of using clustering to estimate future density is novel. Both theory and experiments appear solid. In the rebuttal phase, the authors convinced all the reviewers by addressing their concerns. The reviewers unanimous tend to acceptance.  **Con**: The reviewers had many concerns before the rebuttal. But these were addressed by the authors.   **Recommendation**: The C learning method proposed in this paper is novel and can be potentially useful in practice. Both theory and experiments are solid and convincing. Hence the recommendation is accept.     
The paper considers federated learning in the presence of malicious clients and a semi honest centralized server. The authors provide a novel secure aggregation technique (i.e. split the clients into shards, and securely aggregate each shard’s updates, and the estimating things based on the updates from different shards) to protect clients from the server. Furthermore, an important property of the proposed protocol is that the estimation error is (provably) dimension free against Byzantine malicious clients. The paper is well written.   The reviewers had a number of concerns many of which were addressed during the rebuttal phase. There was also another round of discussion after the rebuttal phase. Overall, the reviewers felt that there are still some issues that need to be resolved (see the updated reviews the main issues are: (i) the assumption of non collusion between the server and the clients, (ii) assumptions and analysis of the non iid case, and (iii)  comparing to attacks that are specifically targeted against the baselines). I believe that once these issues are addressed, the paper will provide an important contribution to the area of federated learning.   
All four reviewers raised concerns on the limited technical novelty and insufficient experiments. They unanimously recommended a rejection. I carefully read the authors  rebuttal but did not find strong reasons to go against the reviewers  recommendations. The reviewers made excellent points to further improve the paper. The authors are encouraged to incorporate those for a future submission.
This paper focuses attacks on federated learning. The reviewers had the following concerns:   The assumption of knowledge of batch indices is unrealistic in an HFL setting   The setup only works when doing a single epoch (I believe the authors claim that it is applicable in more general settings, but evidence to that effect has not been provided)   The novelty of the approach is somewhat limited.   The description of the algorithm and comparison to prior work could be clearer.  I raised the question of whether the reviewers would be more positive if there were no claimed results on HFL. They still did not seem positive enough to justify acceptance (due to the other reasons mentioned above).
## Summary  The paper advances the state of the art in training binary neural networks coming out to first place on ImageNet with a controlled computation budget. While any paper making a new record on ImageNet would be a serious candidate for acceptance, it is positive that this one achieves the goal by putting at work the mechanism of conditional computation, innovative for binary networks, and studying in a systematic and clear way how the network width and configuration can be varied while maintaining the computation budged.  ## Review Process and Decision  The paper was thoroughly discussed by reviewers from different aspects. Several weaker spots have been identified (see below and final reviews), but no critical issues that would indicate a necessity of a major revision. In the end, reviewers agreed on acceptance although in some cases they have decided to keep their original < 5 ranking to reflect the scientific value to them from a more global perspective. I think it is an example of well done modeling and experimental work: the work is very clear, uses sound methods, the experimental results are systematic and give interpretable evidence, which is in my experience is rather exceptional for the overall very empirical binary NNs field. I estimate high interest because of the concept of conditional computation put to work here and because of making a new record on ImageNet.  ## Details  * Computation Cost  If such networks are to be deployed in low power devices, the computation cost might need to be estimated more accurately. An example of such estimation is the work by Ding et al. (2019) Regularizing Activation Distribution for Training Binarized Deep Networks, where the energy and area are estimated using information from a semiconductor process design kit.  There is indeed a number of floating point operations around the binary convolutions: first and last layers, experts, skip connections with scale factors and non linearities. The latency and cost of these operations may not be negligible on target devices. In particular Ding et al. (2019)  argue that XNOR Net architecture is 3 times more costly because of floating point scale factors. However the paper does a fair job in comparing in operation counts, which is a good proxy for many devices. The floating point computations needed in various places can be indeed further reduced to lower bit width, the research on quantization techniques shows this is possible and orthogonal to the contribution.  * Novelty of grouped convolutions design and search  The work of Phan et al. (CVPR 2020): Binarizing MobileNet via Evolution based Searching also proposed to search for best grouped convolution under computation budget constraints (evolutionary search method). Strict budget constraint and merging results from different groups are somewhat novel and the prior work can be objectively contemporaneous.  * Clarity  Clarity of the paper has been improved by the revision. One remaining mysticism is still about the gradient estimator for the experts. The paper states: "we wish to back propagate gradients for the non selected experts", "allows meaningful gradients to flow to all experts during training". The problem is that since $\varphi(z)$ is binary one hot on the forward pass, the gradient of the scalar product with $\varphi$ in (2) results in that in the backward pass only the selected expert receives the training signal and by no means all of them. This is regardless of how the gradient is propagated through $\varphi$. Maybe something is missing? I hope the authors can clarify in the final version. I do not consider it as a serious flow since this training scheme is not claimed as a contribution in any case.  One more point on the clarity: The paper claims that using experts increases the network representation power / capacity. While this seems logical, and follows the preceding work in real valued NNs, the paper could provide additional evidence in terms of training performance of these models. Since the teacher with 76% accuracy is used in the distillation, I assume the training never reaches 100% training accuracy in any of the settings. Does the training accuracy improves with experts? This would be a helpful evidence for further work.  * Search method  The paper was further criticized for that the manual search of the architecture is a step back from automated search methods (NAS, BATS). However these methods are themselves a relaxation of discrete choices (experts, if you like), that need to keep all possible configurations at the same time, which may be less stable and too costly for real architectures and datasets. The principles of gradient based architecture search are not entirely clear and the resulting models coming out of these methods typically give no insights regarding good (intelligent) design choices. At present, the systematic exploration with analysis of tradeoffs conducted is seen to have advantages.   
The authors propose a new dataset, namely ImageNet NOC, for evaluating robustness of image classifiers to corruptions. The dataset may be viewed as an alternative to ImageNet C which uses a different set of corruptions. To derive this set of corruptions, the authors first develop a notion of similarity between two corruptions, and then propose an iterative algorithm to build a set of corruptions which, intuitively, is sufficient to cover the larger set of corruptions (i.e., enjoys *high coverage*), and assigns a similar importance to each such corruption (i.e., is *balanced*). Then, the authors argue that ImageNet NOC is superior to ImageNet C as it achieves a higher degree of balance and coverage.  The reviewers found this to be a borderline paper. The reviewers appreciated the introduced metric and agree that there is no point in evaluating on corruptions which are perfectly correlated. In addition, the systematic approach for generating a set of relevant corruptions is seen as a step in the right direction. The reviewers appreciated the author response and were engaged in the discussion. As it currently stands the reviewers are not convinced that the paper is ready for acceptance. To improve the manuscript the authors could extend Tables 3 and 4 with a wider range of models and investigate qualitative differences between models robust on one dataset, but not on the other. Furthermore, there should be a more detailed discussion of stability and computational properties of algorithm 1. In addition, the authors should provide strong arguments as to why is it not sufficient to add additional corruptions to ImageNet C and compute a weighted score instead. The latter suggestion could lead to an iterative improvement of the current set of benchmarks and place more emphasis on the methodology. I suggest the authors to incorporate the reviewer s feedback and place more emphasis on the methodology around algorithm 1, rather then on introducing another dataset which is likely to be superseded as soon as we add a couple more corruptions in the mix. 
The paper introduces a new scheme for compressing gradients in distributed learning which is argued to exploit temporal correlation.  The paper received very detailed reviews and generated a lot of discussions (thank you to the reviewers for the amazing job).  Many reviewers acknowledge that this is interesting work, a simple and potentially useful algorithm but pointed out many problems with discussion, theoretical analysis, and experiments (e.g., it was not clear to R4 and R3 that these are temporal correlations which are beneficial rather  lossiness ). Some of these issues were addressed and the current version is currently much stronger than the initial submission (and stronger than the low average scores suggest). Still, the reviewers do not believe that the paper is ready for publication and I share this sentiment. I would strongly encourage the authors to invest more effort in addressing the reviewers  comments and resubmit the work to one of the upcoming top conferences.
This is a solid paper that proposes a new slicing approach to the fused Gromov Wasserstein distance using projection on directions sampled from  von mises fisher direction, the location parameter of the von mises fisher is choosen to be maximally discriminating between the distribution  $(\max_{\epsilon}\mathbb{E}_{\theta|vMF(\theta|\epsilon,\kappa)}\beta W(\theta\mu,\theta\nu) +(1 \beta)GW(\theta\mu,\theta\nu))$, the new sliced distance is analyzed and extended to mixture of von mises distributions with $k$ locations or directions.  This contribution of the paper is of general interest beyond the application of the paper as mentioned by the reviewers.  Authors applies the new sliced Fused Gromov Wasserstein distance to relational auto encoders and show improvement.  The spherical slicing is original and new and of independent interest and the application is good as it pushes the boundary of relational auto encoders .Reviewers  and AC did not have any concerns with the paper and the rebuttal and revisions addressed all questions raised. Accept  
The paper considers the problem of using sparse coding to create better generalization in neural networks. The new generalization bound of the neural network only depends on the l1 norm of the weight, instead of the original \ell_2 version as in previous papers.    Well this direction is promising, the major concern about this work is that how they compare with existing generalization bounds empirically. There are definitely some hand crafted instances where this bound excel, but the authors did not provide enough evidence that this bound would actually be better than others for neural networks trained in practice: For example, would adding a relatively large \ell_1 regularizer resulted in a drastic decrement in test accuracy? How does the bound compare with compression based approach such as VC dimension + weight pruning (since the weights are somewhat sparse, so the VC dimension is lower)   One might argue that those pruning techniques do not have theoretical guarantees that they can work   Well this technique does not have theoretical guarantees either (whether this objective can be minimized efficiently): The theorem, at least in the current form, seems to only apply to networks that are the "global optimals" of some non convex training objective (MSE loss involving a non linear neural network +  ell_1 regularizer on its weights). It is also unclear whether such global optimals can be found efficiently in practice. At very least, the authors should devote some effort demonstrating the superiority of their bound empirically.   
The paper proposes a model and a training mechanism for multimodal generation. The reviews are generally positive: they praise the generality of the method, the extensive experimental evaluation, and the good empirical results. Overall, no major concerns were raised, and all reviewers recommend acceptance.  A couple of concerns remain, in my view:   The method is generally heuristic, and intuitively rather than theoretically motivated. This is compensated of course by the empirical evaluation, which is thorough.   The paper could be better written. The reviewers suggested some minor improvements which were implemented in the updated version, but I believe there is room for further improvement.  Due to the above concerns, I consider the rating of reviewer #3 (10: Top 5% of accepted papers, seminal paper) to be unjustifiably high. On balance, however, I m happy to recommend acceptance.  Message to the authors:  In the abstract you write: "a simple generic model that can beat highly engineered pipelines". Please be aware that the word "beat" evokes competition, winners and losers, so it s not appropriate in the context of scientific evaluation. Please consider replacing it with something neutral, such as "a simple generic model that can perform better than ...".
The paper proposes to use pre trained 2D (i.e., image) GANs as a mechanism for recovering 3D shape from a single 2D image. The work demonstrates impressive results on not only human and cat faces, but also cars and buildings. The method is demonstrated with qualitative results and quantitative results on multiple datasets and tasks.  The reviewers were persuaded by the novelty and "neatness" of the idea (and the AC is in agreement) as well as the results. At submission time, there were some concerns with experimental details. For instance, there was a question of how carefully the settings have to be tuned (always a concern with unsupervised methods) as well as an overarching concern about the initialization and whether the method will work on less clean data. The reviewers (and the AC) seem to think that these have been sorted out in discussion.   All three reviewers were in favor of acceptance and the area chair is inclined to agree with the reviewers. In particular, the AC finds the work interesting and compelling. While there is an updated version already uploaded during the discussion, the AC encourages the reviewers to double check all the questions from the reviewers and include the answers from the discussion into the camera ready (even these results are in the appendix).
The paper gives a gradient free method for generating adversarial examples for the code2seq model of source code.  While the reviewers found the high level objectives interesting, the experimental evaluation leaves quite a bit to be desired. (Please see the reviews for more details.) As a result, the paper cannot be accepted in the current form. We urge the authors to improve the paper along the lines that the reviews suggest and resubmit to a different venue.
This paper applies spectral initialization and weight decay to neural nets with factorized layers. Although these ideas have been extensively studied in other areas, formalizing and applying them to deep neural nets is of potential interest to the community. The simulation results are nice, especially the experiments on compression methods (comparison to sparse pruning e.g. lottery tickets) and Transformers. I recommend acceptance.  
There is consensus that the submission is not yet ready for publication. The reviews contain multiple comments and suggestions and I hope they can be useful for the authors.
The proposed ConVIRT learns representations of medical data from paired image and text data. While the paper addresses a relevant problem, the reviewers agree that the method has limited novelty. Two reviewers find and that the experiments are not convincing. One reviewer notes that the paper does not compare to the state of the art methods for the tasks. 
The paper extends previous work on intrinsic reward design based on curiosity or surprise toward multiple intrinsic rewards based multiple model predictions and fuse the reward using meta gradient optimization.  While most reviewers find the paper clearly written, several reviewers do bring up the concern on limited contribution of the work on top of existing ones. Reviewers also would like to see experiments conducted in environment with sparse reward rather than the delayed reward setting constructed from dense reward environments. More ablation studies on the different design choices will also be helpful. 
This paper proposes to consider value functions as explicit functions of policies, in order to allow generalization not only on the state(action) space, but also on the policy space. The initial reviews assessed that the paper was dealing with an important RL topic, but also raised many concerns about the position to previous works (PVN and PVF), the theoretical contributions and the experiments. The authors provided a rebuttal and revision that only partly addressed the initial concerns (check also the review of R3, updated to provide additional feedback following the author’s rebuttal). The final discussion led to the assessment that the paper is not ready for publication. Remaining concerns include clarity, claims being not fully supported by the experiments, theoretical aspects and missing baselines. 
All of the reviewers agree that this paper is well written, and provides sound theoretical analyses and comprehensive empirical evaluations. Overall, this paper makes a useful contribution in the direction of individual fairness. The authors have also addressed the concerns raised by the reviewers in their response.
This paper proposes a new network architecture that implements higher order multivariate polynomials (MVP). They show that MVP generalizes well to different types of conditional variables, and can be applied to a broad range of tasks.   However, unifying discrete and continuous conditions and network without activation function are both well studied in literatures. The inappropriate discussions on the prior works and the advantages of the proposed method over prior approaches are not clearly justified.  Although outperforming SOTA is not necessary, the compared methods need to be well chosen which can provide convincing evidence on why MVP is needed under common settings.
This paper shows that L2 self attention is Lipschitz and presents a new method for computing the Lipschitz constant. All reviewers are positive about the technical part of the paper. However, the major concern comes from the significance of the computed Lipschitz constant. The paper only presents some numerical results using simple toy examples, which is insufficient to justify the  importance of the proposed method. The paper would be a much stronger paper if better numerical results could be presented.
The main contribution of the paper is showing that a model based approach can be competitive with (and even outperform) strong model free methods on the 200M Atari benchmark. This is achieved through a set of improvements over the original Dreamer algorithm.  Reviewers have been polarized over this submission (4,5,8,9). After reading the paper, reviews, rebuttals, and engaging with all reviewers in private conversations, I am recommending acceptance as a poster. I agree with R3 and R4 that « this is impressive work », « results are a convincing demonstration of its utility », « it is an important setup from the perspective of model based RL », « the model is elegant », and « the benchmarking discussion is very useful for the community ».  Although it is true, as R1 puts it, that this work can be seen as « an incremental set of tricks over a prior published approach », these tricks are not obvious and lead to very substantial empirical performance gains. Since the authors described them in details and have also committed to sharing their code, I expect them to be quite valuable to other researchers.  Finally, although I respect R2’s choice to stick to their rating of 4, I believe that their main concern, related to not fully understanding why this work improves on the existing SimPLE algorithm, is indeed justified, but is not enough for rejection. DreamerV2 has a lot of differences compared to SimPLE and it would be very costly to investigate in details the impact of each of them. Hopefully, this work will motivate further research in model based RL that will shed more light on such questions. I would encourage the authors, however, to elaborate a bit more on the differences vs. SimPLE in the « Related work » section (or Appendix, if there is not enough room in the main text).
This paper proposes a general framework to use MT to solve structural prediction problems. The method is well developed and be verified in an arrange of tasks including entity recognition, relation classification, event extraction, semantic role labelling, coreference resolution and dialog state tracking and achieves new state of the art in some of these tasks.   Further experiments also suggest the method is especially effect for low resource scenario, if the label semantics can be used appropriately.Further experiments also suggest the method is especially effect for low resource scenario, if the label semantics can be used appropriately. All reviewers agreed to accept the paper and gave very positive comments.  Some reviewers pointed out that the methods do not improve the performance significantly in some of the tasks.  And more analysis is wanted (by reviewer1).
This paper presents work on efficient video analysis.  The reviewers appreciated the clear formulation and effective methodology.  Concerns were raised over empirical validation.  The authors  responses added additional material that assisted in clarifying these points.  After the discussion the reviewers converged on a unanimous accept rating.  The paper contains solid advances in efficient inference for video analysis and is ready for publication.
The paper looks into performance of a single network vs ensemble CNN networks of similar no. of parameters, through lens of accuracy, training time, memory used, inference time.  the authors show that after some threshold, the ensemble model starts to outperform a single model and make better use of its capacity. although this is not the first paper to look into this question and there are two other earlier results from this year, the current paper looks into more measures and not just accuracy. Although initially the paper only looked at over parameterized regime, the authors added experiments on under parametrized case as well. moreover, the authors address the issue of only looking into small and medium sized datasets by adding two more ImageNet experiments.  I thank the authors for engaging with the reviewers, addressing their comments and updating the paper accordingly.  It s of interest for follow up work to consider large data regime and transformer style models as well.
This paper proposes to enhance the robustness of RL and supervised learning algorithms to noise in the observations by dropping input features that are irrelevant for the task. It relies on the information bottleneck framework (well derived in the paper) and learns a parametric compression of the input features that sets them to zero if they are not relevant for the taskn. The method is extensively evaluated on several RL tasks (exploration in VizDoom and DMLab with a noisy “TV” distractor) and supervised tasks (ImageNet or CIFAR 10 classification with noise).  Reviewers have praised the idea, derivation and writing, as well as the extensive experiments on RL and supervised tasks. Critique focused on: * the contrived nature of the TV noise (localised always in the same corner of the image   a standard evaluation according to the authors), * lack of comparison with other feature selection methods, * lack of comparison with Conditional Entropy Bottleneck (done during rebuttal), * more general noise than just specific pixels (clarified by the authors as being the features coming out of a convnet)  Given that the reviewers’ comments were largely addressed by the authors, and given the final scores of the paper, I will recommend acceptance. 
This paper proposes a method to quantify the uncertainty for RNN, which is an important problem in various applications. It provides results in a variety of domains demonstrating that the proposed method outperforms baselines. However, these experiments would benefit greatly from a comparison with SOTA methods for the specific tasks in addition to the considered baselines (e.g. covariance propagation, prior network, and orthonormal certificates). The paper could also be improved by adding a theoretical justification to explain how the Gumbel softmax function is able to capture the underlying data and model uncertainty.
The reviewers generally feel that the phenomenon discovered in this paper is relevant and could be very important when considering interpretability. However, there are still a number of remaining concerns. The reviewers are not convinced by the human study   they feel there is structure in the SIS’s such that a human trained on these images with an abstract category (i.e., without being told their real world label) could potentially successfully learn to classify them. There is also a concern that SIS is model based, that is, the inductive biases of the model (shape, color, etc.) could be leaking information into the SIS image. Finally, there should be some stronger evidence that this represents a serious practical problem for the community. Are there instances where current interpretable approaches break down because of this phenomenon?  One suggestion to potentially strengthen the human experiment: you could try training a denoising autoencoder on the full images, removing 95% of the pixels at random. Then, given an SIS, use the denoising autoencoder to reconstruct the image and then provide that to a human subject. The question is: how much information about the image as a whole is preserved in the SIS (when combined with an appropriate inductive bias)? 
 After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with the reviewers that the paper presented a very interesting idea and empirical studies.  R3 rightfully pointed out the need to clarify relation to related works, as well as the scalability issue.  Notably, because the analysis does not ensure correctness, it has limited applicability in tasks where absolution correctness are required(e.g. Dead code), but can benefit downstream tasks that do not require absolute correctness. A more thorough discussion about this perspective would strengthen the paper.   Right now the paper is borderline, the meta reviewer acknowledges the pros of the paper as mentioned in the reviews, but also thinks the paper can be further improved based on the comment. Therefore the meta reviewer decided to not accept the paper but would encourage the authors to improve the paper per comments for a future submission.  Thank you for submitting the paper to ICLR.  
This paper proposes one vertex attack for GNN, applied to spatiotemporal forecasting. The paper can be improved w.r.t novelty, incorporating graph topology and rigorous analysis. 
The objective of the paper is to develop a framework for solving PDES with reduced model size and for scarce observation settings. It proposes to use functional input dependent convolutions for learning spatio temporal differential operators together with a non linear numerical scheme (Picard solver). Training makes use of an adjoint formulation.  All the reviewers agree that the authors improved the initial version but opt for a reject. In its present form, the technical description is still incomplete with missing explanations. The experiments should be reinforced and the results are partly unexplained.
This paper evaluates several methods for physical prediction on the PHYRE benchmark, finding that while object based methods (e.g. IN, Transformer) perform better in terms of predictive accuracy, pixel based methods (e.g. STN, Deconv) perform better in terms of downstream task performance. The justification is that it is easier for the agent to evaluate good actions using an image based representation rather than an object based representation.  Pros:   Important attempt to catalogue the current state of the field of physical reasoning   Improved baselines on PHYRE  Cons:   As pointed out by R5, there is a failure to evaluate any hybrid pixel relational methods, such as OP3, R NEM, C SWM, etc. Given that the paper s main contribution is its assessment of the current state of the field (in the authors  own words: "providing a realistic picture of the current state of the field"), this seems like a major oversight to me.   As pointed out by several reviewers, the analysis itself is somewhat limited. I don t see it as a problem that the paper does not propose any new methods, but in that case it needs to present a more thorough picture of why certain methods work better in some cases. For example, I share R1 s concern that the Dec model performs worse than the identity function. Can you provide more detailed analysis demonstrating why the latent space is more useful? Can you demonstrate in what cases the object based classifiers struggle, and why? Incorporating more careful hypotheses and ablations I think would help a lot in turning this into a much stronger paper.  I don t think it s a problem that the paper relies solely on 2D, fully observed environments (many other papers on physical reasoning do this, so I think it s a reasonable choice), and I don t think it s a problem that the paper does not propose a new method. But I do find myself agreeing with the reviewers that the evaluations done within this context are insufficient. In the rebuttal, the authors emphasize the various conclusions stemming from the results (regarding the effect of model error, the extent of generalization, what "accuracy" means), but these conclusions are not that surprising (model error is a well known problem in MBRL, deep models are notorious for their failure to achieve strong generalization, and the limitations of pixel accuracy have spawned whole research areas, such as contrastive and adversarial approaches). Again, I don t think the lack of surprising conclusions is itself an issue. But, the fact that the paper does not really make an attempt to explain any nuances or details regarding the conclusions makes it hard to draw a clear contribution from the paper; in that sense, I don t feel the paper really provides "clear guidance" as is argued in the rebuttal.  I do think this paper is very close to being acceptable, and could make a great submission to a future conference if the authors can spend a bit more time on (1) the baselines (i.e., incorporating hybrid models, and ensuring all methods pass basic gut checks) and (2) supporting their conclusions with more detailed analyses.
The authors present a method for self supervised learning of representations of 2D projections of 3D objects. By performing known 3D transformations of an object of interest, a encoder/decoder network is trained to estimate the applied transformation from a series of 2D projections. The proposed method is used as a regularizer and experiments are performed on supervised 3D object classification and retrieval.     After seeing each others’ reviews, one of the main concerns from the reviewers was the relationship between the proposed method and Zhang et al., CVPR 2019 (i.e. AET). The two methods are conceptually very similar, and the consensus from the reviewers is that the authors did not acknowledge the overlap sufficiently and also did not provide a convincing argument as to why they think the approaches are different.   In their rebuttal the authors provided some additional results on real data which is a valuable and welcome addition. However, there were still other concerns that the reviewers had e.g. R2 wanted to know why the model could not be applied directly to 3D shapes instead of 2D projections.   Given the above concerns (specifically the relationship to AET), there is currently not enough support for accepting the paper in its current form. The authors have received detailed feedback and are encouraged to take it onboard when revising the paper in future.   
The paper presents a method to make CNN focus more on structure rather than texture by constraining a random set of neurons per feature map to have constant activation.  The paper has limited novelty and unclear analysis of the experimental results, for instance plots of accuracy vs strength of adversarial perturbation should be produced. Tables are not readable and results tend to be cluttered and confusing.  Some comparisons seem to be cherry picked as pointed out by some reviewers. Although the approach seems to be well received by the reviewers they all shared similar concerns about having a stronger motivation and better validation of the approach (that is not amount of comparisons but the right comparisons that would clear doubts and make the work directly comparable to others). I strongly encourage the authors to perform a deeper analysis and to clearly work on hypothesis and validation of their work. In my opinion, although the reviewers think different, the experiments are not sufficient to validate the strong claim of the paper.
In the discussion, all reviewers acknowledge the novelty of this paper, such as learning from a wide range of AL heuristics, and the ability to transfer the to tasks with arbitrary number of classes. They also think that the additional experiments provided by the authors improve the paper s empirical validity.   However, a major issue raised by the reviewers is that the novelty (especially when compared with Liu et al) may not be enough for ICLR this time. One research direction (implicitly suggested by Reviewer 2) was to learn active learning strategies that go beyond selecting top k scoring examples to explicitly account for batch diversity. We encourage the authors to address this in the next version.
This paper presents a framework for joint differentiable simulation of physics and image formation for inverse problems. It brings together ideas from differentiable physics and differentiable rendering in a compelling framework.
This paper was evaluated by four reviewers. After rebuttal, several concerns remained, e.g. Rev. 1 is interested in more thorough comparisons even if the model is claimed to be backbone agnostic. Rev. 2 is concerned about re print of some theories and authors  response that  contribution is not in theoretical innovation . Rev. 3 is overall not impressed with the clarity of the paper. Finally, Rev. 4 also remains unconvinced after rebuttal due to several somewhat loose explanations provided by authors.  At this point, AC agrees with reviewers that the paper requires more clear cut theoretical contributions, ablations and improvements in writing clarity. While some reviewers might have been more inspired by the aspect of noisy labels, even ignoring this aspect, the overall consensus among all reviewers stands.
This paper studies the role of momentum in temporal difference (TD) learning algorithms, and how this can be systematically exploited to accelerate the TD type algorithms. More specifically, the authors point out that the momentum term could be quite biased, and propose a scheme to remedy this issue. However, the reviewers point out the lack of motivation about bias correction; it is unclear why bias correction is crucial to achieve acceleration. 
This paper has been thoroughly evaluated by four expert reviewers and it had received one public comment. The authors provided extensive explanations and added technical updates to the contents of their submission in response to constructive critiques from the reviewers. Even though some minor issues have not been fully resolved in the discussion between the authors and the reviewers, I consider this paper worthy of inclusion in the program of ICLR 2021 since, albeit marginally, the apparent strengths outweigh its outstanding limitations.  
There is some positive consensus on this paper, which improved somewhat after the very detailed rebuttal comments by the authors. The use of limited amounts of OOD data is interesting and novel. There were some experimental design problems, but these were well addressed in rebuttal.  A reviewer points out that anomaly/outlier detection does not explicitly assume that there is only one class within the normal class (or in distribution data). The one class assumption is mainly made in some popular anomaly detection methods, such as one class classification based approaches for anomaly detection. The authors should take this into careful consideration when preparing a final version of this work. 
As one of the reviewers  comment, the paper presents "a mixed of tricks" for the multilingual speech recognition, which includes 1) the use of a pretrained mBERT, 2) dual adapter and 3) prior adjusting.  First, the relative gains of the pretrained mBERT is marginal (Section 3.3.1). Secondly, using 1) on top of 2) is unnecessary.  These confuses the reader about what the conclusion of the paper is.  It would be better if choosing one aspect of the problem and investigate it deeper.   The decision is mainly because of the lack of novelty and clarity. 
The authors propose techniques to deal with binarization of 3D point clouds and propose EMA and layer wise scale recovery that improve results across the board for PointNet style models. An accept.
This work proposes a non decreasing quantile functional form for distributional RL, and secondly propose using the distributional error as a means of exploration. The experimental results are very exciting. The paper, however, needs further work before acceptance: the reviewers raised concerns about Theorem 1: a full proof is not included (nor written convincingly during discussion), and while several encouraging experiments were added during the discussion to the paper addressing the reviewers concerns, they fell short (understandably, given the time available).  Thus on this basis, I recommend rejection at this time, but think it likely that with these adjustments the paper will be accepted in future.
The paper has been actively discussed in the light of the authors’ response. Even though the paper was, overall, found quite clear with a solid theoretical support, the reviewers listed several concerns that remained unsolved after the rebuttal, e.g.,  * The proposed approach may not be properly scoped/positioned and evaluated as an HPO method, a concern unanimously shared across the reviewers. Although this is not a concern impossible to overcome, the reviewers believed it could not be achieved as part of a simple revision of the paper. * The lack of challenging baselines to fully assess the performance of the proposed method (e.g., see the list suggested by Reviewer 1)  * Along the lines of the previous point, the experiments focus on small scale settings, which does not make it possible to completely assess the performance of the approach * Some discrepancy between the theoretical analysis and the actual experimental settings (e.g., the assumption about bounded losses not valid with the squared loss)   As illustrated by its scores, the paper is extremely borderline. Given the mixed perspectives of pros and cons, the paper is eventually recommended for rejection.  This list, together with the detailed comments of the reviewers, highlight opportunities to improve the manuscript for a future resubmission. 
This paper investigates methods for producing and evaluating interval forecasts rather than point forecasts. The authors focus on spatio temporal forecasts whose interval accuracy is measured with the Mean Interval Score.  Pros:  Uncertainty quantification is an important topic that is often ignored in the ML literature where the focus remains on point predictions. The COVID 19 example the authors use is a clear example of the need for such methods: the CDC sponsored COVID forecast hub mandates interval forecasts.  Cons:  In the words of one reviewer "This paper is almost a review paper", a statement with which I fully concur. Neither the evaluation metric (MIS) nor the methods for generating intervals are novel. There is no particular effort to argue "why MIS and not weighted interval score" or "why these methods". If it were a review paper with more comprehensive coverage of the relevant state of the art (a deficiency mentioned by multiple reviewers), it may make a more positive impression. As is, the story is mainly incomplete. To resubmit at another venue, I would suggest the authors either (a) state clearly what is new here in this paper or (b) make it a review paper that more comprehensively evaluates and discusses current state of the art. The statements in the last paragraph of page 1 read more like the authors are shooting for (b) than (a): "we conduct a systematic study", "we investigate". Finally, the evaluations undertaken here do not really make use of (or correct for) the spatio temporal task. Presumably some locations/time periods are more difficult than others. So simply averaging over everything (as in Table 1) doesn t take the structure into account. It is likely better to examine relative accuracy to strawman forecaster or perhaps use a random effects model.
This paper presents an elegant and effective approach to knowledge transfer in RL by learning a policy prior from expert data. The paper is generally well structured and well written. Generally, all the reviewers were favourable about this paper, with its simple idea and convincing results. It was thought that the paper would benefit from the addition of more discussion around related work, and more experimental results, but it remains a strong paper.
The paper evaluates several different strategies for labeling of missing data, and recommend the best strategy in practice based on the empirical results on six data sets.  The reviewers agree that empirical evaluation is important for providing a good guideline for this practical problem. The concerns of the reviewers include the lack of motivation on the chosen strategies, the lack of novelty (the tools in the strategies are all pretty standard in the literature), the lack of reproducibility (by referring to the authors  own anonymous work for parameters), and the lack of breadth (e.g #data sets) and depth (e.g. metrics explored) in the experiments. 
Information bottleneck is a well known principle that is used for clustering, dimensionality reduction, and recently deep learning. It finds a compressed representation of input X while retaining most information on the response Y. This paper addresses an attempt to interpret the meta learning using the information bottleneck. In addition, a GP based meta learning method is also proposed.  The topic itself is interesting without any doubt. However, most of reviewers have serious concerns about this work, which is summarized below. First of all, two components of this paper (IB and GP based meta learning) do not provide a coherent message.  While the IB interpretation is emphasized in the beginning of this paper, the main point seems to that GP based methods can be more data efficient than gradient based meta learning. There does not much point to GP+MAML or IB interpretation of MAML.  Experiments are not strong enough, although a few ones are added during the author responses. During the discussion with reviewers, no one support this work, so I do not have choice but to suggest rejection. 
This paper studies how to statistically test if a given model violates the constraint of individual fairness. This is an interesting and novel problem, and the paper leverages the technique of gradient flow to identify a "witness" pair for individual fairness violation.  During the rebuttal, the authors have addressed many concerns raised in the reviews. The author should also consider discussing the runtime and improving the exposition to resolve some of the presentation issues raised in the reviews.
This paper addresses an important problem of semi supervised learning of time series data.  Their approach is based on a convolution autoencoder for learning a time series latent space.  To guide learning an appropriate embedding, they explore three alternative internal clustering metrics (prototype loss, Silhouette loss, and DB index loss) coupled with the autoencoder reconstruction loss.   The approach is reasonable and interesting, however as pointed out by the reviewers, the current submitted version needs major revision for it to be accepted.  Key weaknesses are: 1.	It lacks an extensive literature review.  The reviewers have made several suggestions for improving this. 2.	The experiments are weak.  First, state of the art baselines are missing.  Second, additional alternative evaluation metrics will strengthen the evaluation of unsupervised methods (e.g., adding NMI, accuracy, cross validation of internal metrics).  Providing evaluation of when which loss would work better on what types of data would provide insight to the various losses proposed. 
The authors have provided very detailed responses and added additional experimental results, which have helped address some of the referees  concerns. However, since the modification made to a vanilla GAN algorithm is relatively small, the reviewers are hoping to see the experiments on more appropriate real world datasets (not artificially created imbalanced datasets with relatively few classes), more/stronger baselines, and rigorous theoretical/empirical analysis of the method s sensitivity to the quality of the pre trained classifier. The paper is not ready for publication without these improvements. 
All reviewers find the idea of self supervised learning on mathematical reasoning with the proposed skip tree training interesting and gave the firmly positive scores.  The paper is clearly written, and the experiments and the analysis are well organized, particularly the ability of free form conjecturing is quite thought provoking.  Also, the reviewers  initial concerns have been properly addressed during the discussion phrase.   I think this is a good paper from which people can learn a lot, and should be broadly presented at the conference either as an oral or a spotlight presentation.
During the discussion among reviewers, we have shared the concern that this work has a significant overlap with [Liu et al. 2018] and [Liu & Motani 2020]. Although the authors tried to address this concern by the author response, I also think that the difference is not enough. In particular, the reviewers pointed out that Figure 1, Table 1, and Figure 3 are exactly the same with those in [Liu, 2020], and Proposition 2 in [Liu & Motani 2020] is Proposition 1 in this paper. Since these overlaps are not acceptable, I will reject the paper.
This paper is a systematic study of how assumptions that are present recent theoretical meta learning bounds are satisfied in practical methods, and whether promoting these assumptions (by adding appropriate regularization terms) can improve performance of existing methods. The authors review common themes in theoretical frameworks for a meta learning setting that involves a feature learning step, based on which linear predictors for a variety of tasks are trained. Statistical guarantees for such a framework (that is, statistical guarantees for the performance of trained on an additional target task) are based on the assumption that the set of weight vectors of the linear predictors span the space (ie exhibit variety) and that the training tasks all enjoy a similar margin separability (that is, that the representation is not significantly better suited for some of the tasks than others).  The current submission, cleanly reviews the existing literature, distills out these two properties and then proposes a regularization framework (that could be added to various meta learning algorithms) to promote these properties in the learned feature representation.   Finally, the authors experimentally evaluate to what degree the properties are already observed by some meta learning methods, and whether the proposed additions will improve performance. It is established that adding the regularization terms improves performance on most tasks. The authors thus argue that incorporating insights obtained form recent theoretical frameworks of analysis, can lead to improved performance in practice. Naturally, the purpose of the presented results is not to establish a new state of the art on a set of benchmark tasks, but to systematically study and compare the effect of adding regularization terms that will promote the properties that are desirable for a  feature representation based on statistical bounds.  I would argue that the research community should support this type of studies. The work is well presented and conducted. Most importantly, the study has a clear and general message, that will be valuable for researchers and practitioners working in on meta learning.    However, the reviewers did not recommend publishing this type of study for ICLR. The authors are encouraged to resubmit their work to a different venue.
The paper shows that using graph neural networks to address multi task control problems with incompatible environments does not provide benefits to the learning process. The authors instead propose to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems.  The paper is well written and the analysis of the literature has been appreciated.  The contribution is original and relevant to the community.  All the reviewers agree that this paper deserves acceptance. We invite the authors to modify the paper by following the suggestions provided by the reviewers. In particular:   improve the analysis of the empirical results   update the plots   add the suggested references
This paper propses a slice method for approximaing the Kernel Stein Discrepancy, which has been popularly used for learning and inference with unnormalized density models.  The proposed method uses a finite set from the orthogonal bases for the slice to approximate the Stein Discrepancy.  The experimental results show that they outperform exsiting methods in high dimensional cases in the applications of goodness of fit tests and learning of energy based models.    The proposed slice idea is novel and significant.  Especially, unlike sliced Wasserstein, the slices are taken from the limited number of vectors, which should be an advantageous feature of the method.  Experiments demonstrate clear advantages in high dimensional cases, as expected.  The paper is worth accepting in ICLR. 
This paper presents Non Markovian Predictive Coding (NPMC), a method for learning state representations in visual RL domains that can be used for planning. This work builds on recent work on PC3 (Shu et al. 2020) and PlaNet (Hafner et al. 2020). Concretely, NPMC replaces the image reconstruction objective in PlaNet with a noise contrastive estimation (NCE) objective for the latent dynamics model, an NCE objective between the images and representations, and an additional maximum likelihood objective for the latent dynamics.  Reviewers were in agreement that this paper tackles an important problem and appreciated the writing quality, the experiments that demonstrate effectiveness of NPMC in continuous control scenarios, and accompanying theoretical analysis. However, reviewers were on balance in consensus that this paper needs another iteration before it can appear. Aside from discussion of related work, the main weakness noted by reviewers is that the manuscript in its current form makes it clear how NPMC differs from closely related methods from a technical point of view, but does not make it sufficiently clear to what extent non Markovian predictive coding leads to improved planner performance. In particular, the paper lacks detailed comparisons to baselines, and reviewers were not sufficiently convinced by experiments that were added to the appendix after discussion.  The authors indicate that their contribution is that NPMC extends PC3 to RL tasks. The metareviewer appreciates that experimental comparisons can require creative thinking when baselines are not directly applicable to the tasks of interest, but would nonetheless like to encourage the authors to consider how they can improve their experiments.
The paper posits that VAEs, if made sufficiently deep, are able to implement autoregressive models, and could possibly outperform them. Experimentally, the authors attempt make VAEs sufficiently deep so that they are able to outperform autoregressive models on image generation. The authors use a variety of tricks to scale the depth of the model to up to 78 stochastic layers, and achieve SOTA, or near SOTA NLLs on a number of datasets. Furthermore, in comparison to other models (in particular the recently proposed Nouveau VAE), the models achieve these scores using far fewer parameters.  Although the tricks are a bit ad hoc and the novelty is a bit weak, the experimental results are quite strong and would be of interest to anyone working on VAE research. Moreover, one of the weakness of the paper, a lack of ablations, was addressed during the rebuttal. All reviewers believed that the paper should be accepted, and I see nothing in the paper or the reviews to suggest otherwise.
This paper aims to develop a simple yet efficient deep RL algorithm for off policy RL. The proposed method uses advantages to as weight in regression, which is an extension of the known method of reward weighted regression. The paper is in general nicely written, and it comes with a set of theoretical analyses and experiments. While all reviewers admit that the approach is interesting and the work makes an attempt to solve an important yet open problem, there are several aspects of the paper that make it not ready for publication in its current form:    Novelty: As pointed out by reviewers, the proposed method appears to be a minor modification of existing off policy solvers. Although the use of advantages as weights makes intuitive sense, it is unclear why and how the new method significantly differs from and outperforms existing methods. Going forward, it would be helpful if the authors could present more convincing arguments/experiments to demonstrate the power of ARW, relative to similar existing methods.    Experiments provide some insights into the difference between several algorithms, but the results are not strong enough to support the claim of the paper. Please see reviewers  comments for more details. We strongly recommend the authors to take these comments into consideration and develop more rigorous experiments to demonstrate advantages of AWR.    Theoretical analysis is limited. As R#2, R#3 mentioned, the theory analysis in the paper seems to not match the algorithm, and there remain bugs, this it doesn t add to the paper. Although theory might not be the focus of the paper, if the authors decide into include theoretical analysis, the analysis would hopefully provide insights into why and by how much the approach is better.  
The paper studies reinforcement learning in the presence of (adversarial) perturbations in the underlying system dynamics. The main (novel) observation is that  agents trained against a single policy may overfit  to that policy and hence will lack robustness to new/unseen policies. The paper proposes a population based augmentation to the Robust RL formulation in which a population of adversaries are randomly initialized and samples from during training. The authors seek to show that their method generalizes well to unseen policies at test time.  Most reviewers agree that the paper provides a range of solid experimental results (with in distribution and out of distribution tasks) showing robustness and generalization of their methods on several robotics benchmarks while avoiding a ubiquitous domain randomization failure mode. However, all the reviewers (and myself) agree that some of the conceptual claims of the paper may not be precise. For example, some of the reviewers disagree with the authors on finding the (mixed) Nash equilibria. Such general claims are hard to validate (may not even be true) and need theoretical justification. Hence, it is not conceptually clear why using multiple adversaries would not suffer from the same limitations as in the single adversary case.  Also, in the discussion phase, the reviewers agreed that the results/claims of the paper (i.e. overfitting to a single adversary and the need for multiple adversaries) are very interesting, but at the same time need to be confirmed by more extensive experiments.    Indeed, if the above are addressed, the paper would make a strong contribution to the area of RL.   
This work proposes an efficient method for modelling long range connections in point cloud data. Reviewers found the paper to be generally well written. On the less positive side, reviewers felt that the novelty of the work was marginal, and that the experimentation, limited to synthetic data in one domain, was too limited. These concerns remain after the discussion phase. In addition, the authors stated during the discussion that "Our goal is indeed to develop an efficient strategy to model LRIs in real chemical and materials systems”, which conflicts with the presentation of the work as motivated by more general point cloud modelling problems. Given these weaknesses, the final decision was to reject.
The paper first aims to propose a new controllable Pareto multi task learning framework to find pareto optimal solutions. But after the revision according the comments, the paper claims to find finite Pareto stationary solutions. But the paper still can not prove their proposed method can find the Pareto stationary solutions. Even if they can find the  Pareto stationary solutions, they can not guarantee find the pareto front which is conflict with the experiments and claims. There are major flaws in the paper.
This paper was reviewed by 4 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The paper considers generalization in setups in which the training sample  may be generated by a different distribution than the one genertaing the test data. This sounds much like transfer learning, and similarly sounding considerations, of a space of possible generating distributions, ways of measuring the statictical complexity of such spaces and implied error generalization results were analyzed in e.g., Jonathan Baxter s "Theoretical models of learning to learn" 1998 and S Ben David, R Schuller "Exploiting task relatedness for multiple task learning" S Ben David, RS Borbely "A notion of task relatedness yielding provable multiple task learning guarantees" Machine learning 73 (3), 273 287  The current submission does not mention these earlier works.  Furthermore, the paper suffers from mathematical sloppiness. The model uder which the generalization theorems  hold is not clearly defined. For example,  Theorem 2, Theorem 3 and Theorem 4  do not stae what are the probability spaces to which the "probaility p > 1 \delta" quantifications refer.   
Three reviewers are positive, while one is negative. The negative reviewer is well qualified, but the review is not persuasive. Overall, this paper should be published as a wake up call to the research community. Unfortunately, the lesson of this paper is similar to that of several previous papers, in particular  Armstrong, T. G., Moffat, A., Webber, W., & Zobel, J. (2009, November). Improvements that don t add up: ad hoc retrieval results since 1998. In Proceedings of the 18th ACM conference on Information and knowledge management (pp. 601 610).  This submission should be a spotlight, to maximize the chance that future researchers learn its lesson.
This paper presents a way to use GNNs to learn edge weights of a coarsened graph given the node mapping from the original graph to the coarsened graph.  The paper is well written and the approach is well motivated as learning makes it easy to adapt the edge weights to different tasks and objectives, as illustrated in the graph Laplacian and Rayleigh quotient examples.  All the reviewers gage positive reviews for this paper, hence I recommend accepting this paper.  The reason for not promoting this paper further to spotlight or oral is that the paper addressed a relatively small problem, learning the edge weights given the node mapping, and the proposed method is quite simple.  Therefore this paper’s impact could be limited.  One suggestion to the authors is to present more results on downstream tasks, i.e. how does the proposed coarsening algorithm improve downstream task performance, instead of just losses defined without a downstream task in mind.  Example things to consider: does this approach improve graph classification accuracy?  Does this improve downstream GNN model’s efficiency without sacrificing accuracy?
The reviews were a bit mixed: on one hand, by combining and adapting existing techniques the authors obtained some interesting new results that seem to complement existing ones; on the other hand, there is some concern on the novelty and on the interpretation of the obtained results. Upon independent reading, the AC agrees with the reviewers that this paper s presentation can use some polishing. (The revision that the authors prepared has addressed some concerns and improved a lot compared to the original submission.) Overall, the analysis is interesting but the significance and novelty of this work require further elaboration. In the end, the PCs and AC agreed that this work is not ready for publication at ICLR yet. Please do not take this decision as an under appreciation of your work. Rather, please use this opportunity to consider further polishing your draft according to the reviews. It is our belief that with proper revision this work can certainly be a useful addition to the field.   Some of the critical reviews are recalled below to assist the authors  revision:  (a) The result in Theorem 4.1 needs to be contrasted with a single machine setting: do we improve the convergence rate in terms of T here? do we improve the constants in terms of L and M here? What is the advantage one can read off from Theorem 4.1, compared to a single machine implementation? How should we interpret the dependence of (optimal) H on r and lambda_2?   (b) The justification for $T \geq n^4$ is a bit  weak and requires more thoughts: one applies distributed SGD because n is large. What happens if T does not satisfy this condition in practice, as in the experiments?  (c) Extension 1 perhaps should be more detailed as its setting is much more realistic than Theorem 1. One could use Theorem 1 to motivate and explain some high level ideas but the focus should be on Extension 1 3. In extension 2, the final bound seems to be exactly the same as in Theorem 1, except a new condition on T. Any explanations? Why asynchronous updates only require a larger number of interactions but retain the same bound? These explanations would make the obtained theoretical results more accessible and easier to interpret.
The paper introduces variants of RL algorithms that can consume factored state representations. Under the assumption that actions only affect a few factors, these factored RL algorithms can learn more efficiently than their vanilla counterparts. Learning a factored dynamics model (to be used in a model based algorithm) or representing factorized action selection policies (to be optimized by a model free RL algorithm) make intuitive sense in the problem settings that the paper considers. However, the paper should clarify the implicit assumptions being made about how the reward decomposes across factors. For instance, the factored DQN approach seems to require a linear reward decomposition across the factors. The factored DQN approach is also reminiscent of the Hydra algorithm on MsPacMan (https://papers.nips.cc/paper/2017/file/1264a061d82a2edae1574b07249800d6 Paper.pdf Section 4.2) which assigns an RL agent to each factor ("ghost" in MsPacMan) to learn a factor specific Q function. The linear aggregator that they use is identical to the factored DQN in this paper.  The reviewers all rate the paper as borderline. All reviewers suggest that being able to learn the factor graph (or at least parts of it) will greatly widen the scope of applications where the approach can be fruitfully applied   the paper acknowledges this as a compelling line of future work. The biggest weakness is originality   the core message of the paper is just that, where factored representations of state/actions exist RL algorithms must use it. This is not a surprising or novel message. The paper advocates for incorporating the factorization information in the most straightforward way (state masking, followed by action concatenation). Simple in retrospect is usually an excellent feature of an algorithm, not a bug; however, the proposal is literally the first idea a reader will likely think of. It might help to explore other ways of incorporating factorization information (e.g., rather than parameter sharing, have a separate network for each factor; rather than masking, have different width input layers to consume different number of parents in the DAG; etc.) and verifying that they are inferior to factored NN.
This paper presents a hierarchical version of β TCVAE that promotes disentanglement in the latent space and improves the robustness of VAEs over adversarial attacks, without (much) degeneration on the quality of reconstructions. The analysis on the relationship between disentanglement and adversarial robustness is valuable and the method is new. The results look promising. The comments were properly addressed.
The paper touches upon explainable anomaly detection. To that extend, it modified hypersphere classifier towards fully convolutional data description (FCDD). This is, as also pointed out by two of the reviewers a direct application of a fully convolutional network within the hyperspherical classifier. However, the paper  also shows how to then upsample the receptive field using a strided transposed convolution with a fixed Gaussian kernel. Both together with tackling explainable anomaly detection is important. Moreover, the empirical evaluation is quite exhaustive and shows several benefits compared to state of the art. So, yes, incremental, but incremental for a very interesting an important case. 
The paper proposed weighted MOCU, a novel objective oriented data acquisition criterion for active learning. The propositions are well motivated, and all reviewers find the analysis of the drawbacks of several popular myopic strategies (e.g. ELR tends to stuck in local optima; BALD tends to be overly explorative)) interesting and insightful. Reviews also appreciate the novelty of the proposed weighted strategy for addressing the convergence issue of MOCU based approaches. Overall I share the same opinions and believe the paper offers useful insights for the active learning community.  In the meantime, there were shared concerns among several reviewers in the readability (structure and intuition), lack of empirical results on more realistic active learning tasks, and limited discussion on the modeling assumptions. Although the rebuttal revision does improve upon many of these points, the authors are strongly encouraged to take into account the reviews, in particular, to further strengthen the empirical analysis and discussions, when preparing a revision. 
This work studies the question of universal approximation with neural networks under general symmetries. For this purpose, the authors first leverage existing universal approximation results with shallow fully connected networks defined on infinite dimensional input spaces, that are then upgraded to provide Universal Approximation of group equivariant functions using group equivariant  convolutional networks.   Reviewers were all appreciative of the scope of this paper, aimed at unifying different UAT results under the same underlying  master theorem , bringing a much more general perspective on the problem of learning under symmetry. However, reviewers also expressed concerns about the accessibility and readibility of the current manuscript, pointing at the lack of examples and connections with existing models/results. Authors did a commendable job at adding these examples and incorporating reviewers feedback into a much improved revision.   After taking all the feedback into account, this AC has the uncomfortable job of recommending rejection at this time. Ultimately, the reason is that this AC is convinced that this paper can be made even better by doing an extra revision that helps the reader navigate through the levels of generality. As it turns out, this paper was reviewed by three top senior experts at the interface of ML and groups/invariances, who themselves found that the treatment could be made more accessible   thus hinting a difficult read for non experts. In particular, the main result of this work (theorem 9) is based on a rather intuitive idea (that one can leverage UAT for generic neural nets on the generator of an equivariant function), that requires some technical  care  in order to be fleshed out. The essence of the proof can be conveyed in simple terms, after which following through the proof is much easier. Similarly, the paper quickly adopts an abstract (yet precise) formalism in terms of infinite dimensional domains, which again clouds the essential ideas in technical details. While the paper now contains several examples, this AC believes the authors can go to the extra mile of connecting them together, and further discussing the shortcomings of the result   in particular, the remarks on tensor representations and the invariant case are of great importance in practice, and should be discussed more prominently. Finally, while this work is only concerned with universal approximation, an important aspect that is not mentioned here is the quantitative counterpart, ie what are the approximation rates for symmetric functions under the considered models.   
The authors re state Mackay s definition of effective dimensionality and describe its connections to posterior contraction in Bayesian neural networks, model selection, width depth tradeoffs, double descent, and functional diversity in loss surfaces. The authors claim the effective dimensionality leads to a richer understanding of the interplay between parameters and functions in deep neural networks models. In their experiments the authors show that effective dimensionality compares favourably to alternative norm  and flatness  based generalization measures.  Strengths:  1   The authors include a description of how to compute a scalable approximation to the effective dimensionality using the Lanczos algorithm and Hessian vector products.  2   The authors include some novel experimental results showing the effective dimensionality with respect to changes in width and depth. These results are informative in how changes in depth and width affect this metric in a different way. The same for the experiments with the double descent curve.  Weaknesses:  1   For some reason the authors seem to have taken the concept of effective dimensionality from David Mackay s approximation to the model evidence in neural networks and ignored all the extra terms in such approximation. It is currently unclear why there is a need to do this and focus only on the effective dimensionality. Almost all the experiments that the authors describe could have been done using a similar approximation to Mackay s model evidence. It is unclear why is there a need to focus just on a part of Mackay s approximation. The fact that the authors state that the effective dimensionality is only meaningful for models with low train loss seems indicative that David Mackay s approximation to the model evidence would be a better metric.  2   With the exception of the experiments for changes in the effective dimensionality as a function of the depth and width and the double descent curve, all the other experiments and results are expected and not new to anyone familiar with David Mackay s work.  3   The experiments on depth and width are for only one dataset and may not be representative in general. The authors should consider other additional datasets.   The authors should improve the paper, including a justification for using only the effective dimensionality and not David Mackay s approximation to the model evidence. They should also strengthen the experiments by comparing with David Mackay s approximation to the model evidence and should consider additional datasets as mentioned above.
I found the main algorithmic contributions to be interesting and of potential value to practitioners, as highlighted by Reviewer 3. Like several reviewers, I found the causal framing to be confusing, or at least not really to be framed in a framework like Pearl s: the word "confounding" is thrown a few times in the manuscript, but there is no formal sense by which it is linked to what we commonly understand as confounding. We are still talking about what happens inside a predictive model (a deterministic function), not what happens in the real world (the authors are not alone as targets of my observation: my point applies to a lot of the papers in the references, where the causal interpretation is hardly illuminating for those coming from a causal inference background, for instance). The reply to Reviewer 2, for instance, cites [1], which is about Granger causality and has little to do with Pearl s framework. Despite its name, Granger "causality" is a probabilistic concept (or, at best, an idea for identifying non causality) with a very minimal causal basis besides the use of time ordering. A much more rigorous explanation of confounding in this paper s context needs to be provided.   That been said: as helpfully highlighted by Reviewer 3 (and summarized without any need to resort to a causal framing), there are several positive contributions added here, which might be of interest to the ICLR audience. The causal framing unfortunately gets in the way without adding clarity.  In its present state, the paper is not yet ready for publication. We hope that the reviewer comments prove helpful for preparing a strong future submission. 
The paper investigates the average stability of kernel minimal norm interpolating predictors. The main result  establishes an upper bound on a particular notion of average stability for which it is well known that it  can be used to bound the generalization error. This upper bound holds for all interpolating predictors  from the RKHS, but it is minimized by the minimal norm predictor.   While at first glance this result looks highly interesting, a closer look reveals that the significance of the results  crucially depends on the quality of the derived upper bound. Here two reviewers raised concerns, since it is  by no means clear that even the optimized upper bound produces meaningful bounds on the generalization performance. The authors tried to address these concerns in their response and promised to update their  paper accordingly. As a result, they added a paragraph on page 8. Unfortunately, this paragraph remains extremely  vague, in particular if it comes to the more interesting case of non linear kernels. Here, the authors briefly refer to  a paper by El Karoui but no details are given. However, looking at El Karoui s paper it is anything but obvious whether  the results of that paper lead to reasonable upper bounds on the average stability for a sufficiently general class  of distributions.  As a result, I view the paper under review to be premature since it remains unclear if the observed optimality of the minimal norm solution is a real feature or just an artifact due to an upper bound that is simply too loose to make any conclusion.    
This paper studies how layer wise representation and task semantics affect catastrophic forgetting in continual learning. It presents two findings: 1. the higher layers contribute more to forgetting than lower layers, 2. intermediate level similarity between tasks causes the maximal forgetting. It also indicates that existing methods employ either feature reuse or orthogonality to mitigate forgetting.  Pros:   The layer wise analysis of catastrophic forgetting and investigation of different mitigating forgetting methods are important and interesting.   The paper is well motivated and well written.   The results can potentially help to suggest new approaches for developing and measuring mitigation methods.  Cons before rebuttal:   The paper misses discussion on and takeaways from the findings.   How general are the findings? There is a different observation by Kirkpatrick et al. 2017.   Limited diversity of experiments, because the experiments are only done on image classification tasks with CIFAR10 and CIFAR100.  The authors conducted more experiments and updated the paper with added explanations and results. The reviewers found the new evidence and arguments in the rebuttal to be convincing and the authors addressed most concerns.  In summary, the findings from this paper will help researchers better understanding and addressing catastrophic forgetting, and will be of interest to the community. Hence, I recommend acceptance. 
This paper presents a new, large scale, open domain dataset for on screen audio visual separation, and provides an initial solution to this task. As the setting is quite specialized, the authors proposed a neural architecture based on spatial temporal attentions (while using existing learning objective for audio separation). The reviewers were initially concerned that, while reasonably motivated, the architecture seemed some arbitrary. The authors then provided extensive ablation studies to evaluate the significance of each component with existing datasets, and these efforts are appreciated by reviewers. The authors may consider re organizing the paper and moving some ablation studies to the main text. On the other hand, the reviewers believe that the dataset will be very useful for the community due to its diversity in content and label quality.
This paper proposes a method for neural architecture search (NAS) based on adversarial methods. It uses a discriminator trained to distinguish between random vs. good architectures, letting the discriminator s scores serve as a reward signal for an autoregressive generator. I agree with AR1: this is a nice and clever idea. Reviewers generally agreed that the method was interesting, e.g. it s quite flexible in that it s able to incorporate constraints, and that the evaluation is rather extensive and shows that the method performs well across the board. Many minor criticisms were raised and addressed well by the authors in their responses and manuscript updates.  The major criticism shared by most reviewers was the high methodological complexity of the proposed approach, and the proportionally small gains shown over much simpler baselines. This criticism remained despite the authors  responses. The method is indeed complex: the same method without any adversarial component already performs well, and many important details of the model are relegated to Appendix A.2. (I would recommend, for example, moving Fig. 2 to the main text if at all possible. Also, the Appendix can/should be included in the main PDF for ICLR, rather than in supplementary material, as AR1 mentions.) It was not clear to reviewers that the adversarial component of the approach has a significant benefit. The authors respond by pointing to Table 7 showing that the discriminator reduces the number of queries and points out that in reality these queries correspond to expensive evaluations. If this is a major selling point of the method (it sounds like it could be), it should be highlighted and analyzed far more   at least moved to the main text rather than an Appendix   ideally with a real world evaluation showing a practical large improvement in overall wall clock time, rather than a benchmark where these evaluations are free. Perhaps the exclusive reliance on these benchmarks, though undoubtedly useful for quick experimentation, in the end holds back the paper and prevents the method s benefits from becoming apparent to the readers.  As a minor point (also raised by AR1), the paper is formatted incorrectly for ICLR: the font color is off, and more importantly the PDF is unsearchable (text cannot be selected, ctrl F does not work), which makes it very difficult to quickly reference and review. Please try not to stray from the conference provided style file for future submissions.  I appreciate the cleverness of the method, the extent of the evaluation, and the thorough responses to the reviews. However, unfortunately with the current presentation, it is too difficult to discern the benefit of the proposed approach from the manuscript. The approach is nonetheless intuitively appealing and seems quite promising, and I hope the authors will take the reviewers  good feedback into account and resubmit the paper in the future.
The paper primary theoretical contribution claim is to establish the constant size SGD converges linear to the optimal solution in non convex settings. This is shown in the interpolation regime for over parametrized situations when starting from points nearby to the optimum. The paper s empirical claim is to use relatively larger learning rates for SGD in common deep learning settings and claim that they can do well.   My recommendation is based on the overall low scores provided by the reviewers   which did not change post rebuttal. The concerns raised by the reviewers amounting to my decision recommendation is summarized below     Overall the reviewers found the connection between the theoretical results and the overall claims of the paper unconnected. The reviewers found the theoretical contribution of the local convergence weak   particularly in the context of an analysis of constant learning rates and taking into account existing work on the convex case for such results. Furthermore, the experimental contribution of the paper is incremental as the proposed algorithm is standard with just a larger than typical initial learning rates. This factor is usually searched over during Hyper Parameter sweeps in all the large scale learning setups. In this context, SGDL performing favorably, is an interesting observation but not enough of a contribution. Further the reviewers objected to the fact that SGDL does not connect with the theory presented as SGDL in experiments still uses learning rate schedules.  
There were both positive and negative assessments of this paper by the reviewers: It was deemed a well written paper that explores cleanly rederiving the TC VAE in the Wasserstein Autoencoder Framework and that has experiments comparing to competing approaches. However, there are two strong concerns with this paper: First, novelty appears to be strongly limited as it appears a rederivation using known approaches. Second, two reviewers were not convinced by the experimental results and do not agree with the claim that the proposed approach is better than competing methods in providing disentangled representations. I agree with this concern, in particular as assessing unsupervised disentanglement models is known to be very hard and easily leads to non informative results (see e.g. the paper cited by the authors  from Locatello et al., 2019). Overall, I recommend rejecting this paper.
The paper proposes a new approach to continual learning with known task boundaries that is scalable and highly performant, while preserving data privacy.  To mitigate forgetting the proposed approach restricts gradient updates to fall in the orthogonal direction to the gradient space that are important for the past tasks. The main novelty of the approach is to estimate these subspaces by analysing the activations for the inputs linked for each given task.   All reviewers give accepting scores. R2, R3 and R4 strongly recommend accepting the paper, while R1 considers it borderline.  The authors provided an extensive response carefully considering all reviewers  comments. New experiments were introduced (training time analysis and comparisons with expansion based methods), and several clarifications were added.  All reviewers agree that the paper is well written and its literature review adequate.  The main concern of R1 was the similarities with OGD (Farajtabar et al. 2020). R1 considered the authors’ response acceptable. R2, R3 and R4 consider the contribution well motivated and significant and highlight its simplicity. The AC agrees with this assessment.  The empirical evaluation covers most of the typical benchmarks in CL. Very strong results are reported on a variety of tasks both in terms of performance and memory efficiency, as agreed by R2, R3 and R4.  Overall the paper makes a strong contribution to the field of CL. 
This paper is rejected.  I and the reviewers appreciate the changes made by the authors. The paper presents: * An analysis (based on techniques from previous work) of double Q learning which shows that in an analytic model, double Q learning can have multiple sub optimal "approximated" fixed points. * Propose a modification of the update that uses collected trajectories to lower bound the optimal value. * Experiments on several Atari games.  While the theoretical results on double Q learning are interesting, the authors provide little theoretical analysis of their proposed approach. Doing so will significantly strengthen the paper. Additionally, reviewers had concerns about the experiments. R2 questions the parameter setting in the multi step experiments. 
This paper uses an extension of HoloGAN for few shot recognition and novel view synthesis. All but one reviewer gave a final rating of accept. These reviewers were concerned that the submitted version of this work had not adequately placed this work in context with prior art. However, during the discussion these concerns seem to have been addressed sufficiently. The most negative reviewer was not impressed by the quality of the generated images; however these are relatively new methods and the few shot recognition aspect of this work is also part of the contribution. Accounting for all reviews and the discussion the AC recommends accepting this work as a poster. 
The paper shows hardness results for batch reinforcement learning. Authors show that even if all value functions are linear in a given set of features and the exploration data covers all directions, evaluating any policy might require a sample size that is exponentially large in the problem horizon. This is an interesting and somewhat surprising result, and I believe it would be of interest to the wider RL community. I recommend acceptance of this paper.
# Quality: The algorithm is thoroughly evaluated and several interesting experiments are included in the appendix.   # Clarity: The paper is generally well written.  # Originality: The proposed approach is a small but novel improvement over existing algorithms (to the best of the reviewers and my knowledge). The concept of "deployment efficiency" is, in my opinion not novel, since it seems mostly a rebranding of what the MBRL community traditionally refers to as "data efficiency"   although I agree that deployment efficiency is indeed a more accurate term.  # Significance of this work:  The paper deal with a relevant and timely topic. However, the paper does not compare to the larger MBRL literature. Hence, it is difficult to gauge the significance of this work.  # Overall: This manuscript offers a good contribution to the topic of model based reinforcement learning algorithms.  # Minor comments:    I suggest removing the word "impressive" from the abstract. This is a subjective term, which should be avoided.   In my personal opinion, it would be nice to include experiments with more state of the art baselines such as PETS and POPLIN, for which code is available online. It is unclear to me how much the improvement in performance depends on the algorithm itself compared to just having larger batch sizes. From this perspective, Figure 5 in Appendix B is probably the most interesting insight of the manuscript, to me.
This paper receives 3 initial rejection ratings. No rebuttal was submitted by the authors. There is no basis for overturning the reviewers  decisions. This paper should be rejected.
All reviewers find the proposed data augmentation approach simple, interesting and effective. They agree that paper does a good job exploring this idea with number of experiments. However the paper also suffers from some drawbacks, and reviewers raise questions about some of the conclusions of the paper   in particular how to designate an augmentation as either negative or positive is not clear apriori to training. While I agree with this criticism, I believe the paper overall explores an interesting direction and provides a good set of experiments than can be built on in  future works, and I suggest acceptance. I encourage authors to address all the reviewers concerns as per the feedback in the final version.
This paper presents a broad exploratory analysis of the geometry of token representations in large language models, with a focus on isotropy and manifold structure, and reveals some surprising findings that help explain past observations.  Pros:   Clear and surprising analytical findings concerning a broad and widely used family of models.  Cons:   The paper is a fairly broad exploratory analysis, with no single precise claim that ties together every piece of the work.  I thank both the authors and reviewers for an unusually productive discussion.
This paper proposes a method to solve high dimensional, continuous robotic tasks offering a trajectory optimization and a distill policy. The paper is well written and the work is promising. It is very relevant for the robotics and RL communities.
The paper is proposing a domain generalization method based on the intuition that an invariant model would work for any split of train/val. Hence, the method uses adversarial train/val splits during training. The paper is reviewed by three expert reviews and none of them championed the paper to be accepted. I carefully checked the reviews and the authors  response and agree with the reviewers. Specifically:    R#1: Argues that the paper is not ready for publication. Also argues the optimization problem is only a motivation as it is not directly solved. This is an important issue and it needs to be addressed in a conclusive manner.   R#2: Argues empirical studies do not show the value of train/val splitting. I partially disagree with this issue but it is clear that more qualitative and quantitative study is needed to properly justify the proposed method.   R#3: Argues the contribution is not enough for publication. The paper is clearly novel but the contribution and novelty is not presented in a clear manner. Moreover, the empirical study does not complement the novelty. Hence, I disagree with the comment.  Overall, I believe the paper proposes an interesting idea. However, the presentation and empirical studies need to be improved significantly. I recommend authors to address these issues and submit to the next conference.  
The authors propose a network expandable approach to tackle NAS in the continual learning setting. More specifically, they use a RNN controller to decide which neurons to use (for a new task) and the additional capacity required (i.e., number of new neurons to add). This work can be viewed as an extension of RCL and as such suffers from the large runtime. This was a concern for most reviewers. While reviewers highlighted the gains in the experiments conducted, several questions remained regarding the efficiency of the proposed approach and how it compares to other strategies. The practical relevance of the proposed approach was also a concern as its application requires to restrict it to models of modest size. 
The paper shows the success of a relatively simple idea   fine tune a pretrained BERT Model using Variational Information Bottleneck method of Alemi to improve transfer learning in low resource scenarios.  I agree with the reviewers that novelty is low   one would like to use any applicable method for controlling overfitting when doing transfer learning, and of the suite of good candidates, VIB is an obvious one   but at the same time, I m moved by the results because of: the improvements and the success on a wide range of tasks and the surprising success of VIB over other alternatives like dropout etc, and hence I m breaking the tie in the reviews by supporting acceptance.  Its a nice trick that the community could use, if the results of the paper are an indication of its potential.
All three reviewers agree on accepting the paper and think that the proposed approach will be of interest for those working in vdieo prediction.  The authors are asked to include the extra discussion with R3 as part of the paper and include the proposed changes by R2 to provide more thorough experimentation.  The paper is recommended as a poster presentation.
This paper presents an interesting approach for training generative autoencoders with a latent space that lies on a hyperspherical subspace. However, the reviewers have raised concerns regarding the similarity of this work with several prior works and have questioned the experimental setup. Without the authors  response, we cannot situate this paper among the prior work properly. Thus, I recommend Reject at this point. 
This paper proposes a type of Mixup style data augmentation that works at the batch level rather than simply between pairs of examples. Each generated example accumulates salient images from potentially many other examples while ensuring diversity across the generated examples. This is achieved through a 4 part objective with submodular and supermodular components. The paper demonstrates the method using extensive experiments, including generalization performance on CIFAR 100, Tiny ImageNet, ImageNet and GoogleCommands. It also explores weakly supervised object localization, expected calibration error, and robustness to random replacement and Gaussian noise.  Reviewer 1 thought the approach was interesting but raised some concerns with clarity, thoroughness of experiments and whether the approach was computationally prohibitive to be used in practice. I was surprised myself that a discussion on the trade off between computational expense and accuracy gain was not discussed in the submission. The authors responded to the review, adding a comparison to the BP algorithm (Narshiman and Bilmes 2005). The empirical result seems to back up the claim that the proposed algorithm finds a better solution and with less variance. It also appears to run much faster. The authors also responded to minor issues raised with respect to clarity and organization. In their response, the authors provided considerable detail with respect to running time and time complexity, and show that models trained with co mixup are practical, though they do come with a significant added cost. The authors added the requested comparisons to non mixup baselines and enhanced the ablation study. In my opinion, this is a comprehensive and satisfying response, and the paper has improved in many respects since submission.  The review from R2 was largely positive, though limited in its scope. They also expressed concerns with training time (addressed in the response to R1). Clearly the approach extends to an arbitrary (m) number of images; this was explicit in the paper/formulation and clarified by the authors. I have some concern that R2 may have skimmed the paper if they missed this point.  Reviewer 4 thought the paper was interesting and asked several clarifying questions. They expressed concern with the significance of the reported gains. Similar to R1, they asked about non mixup baselines (VAT specifically). This was addressed in the response to R1. The authors responded to the clarifying questions and addressed the issue of significance.  Like the reviewers, I think that this is an intriguing, fresh, and elegant way to perform data augmentation. I appreciate that it has been evaluated just not from the pure generalization setting, but from other angles like robustness and calibration. There are still some outstanding concerns regarding the computational effort required to use Co Mixup, so this would be nice to see in follow up work.
I thank authors and reviewers for discussions. Reviewers found the paper (specially the CAT r method proposed in the rebuttal period) interesting but there are some remaining concerns about the significance of the results and experiments. Given all, I think the paper still needs a bit of more work before being accepted. I encourage authors to address comments raised by the reviewers to improve their paper.    AC
All Reviewers agree that the paper has a clear and solid contribution. Furthermore, all of them highlight that the paper has improved significantly after revision. Hence, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Comparison across network architectures.   Comparison across a broad range of different data sets.   Compactness of the representation (few parameters to learn).   Authors will share code.  Cons:   Role of L2 normalization could be further discussed.
The paper proposes the generalization performance of distillation from random networks as a metric of diversity, named RND. Intuitively, the more diverse the generated datasets, the more difficult it should be for a model to learn a random computation. The reviewers agree that the metric has a novel perspective. Unfortunately, the paper is not sufficiently developed to be accepted at this point. It is currently missing a number of experiments that would demonstrate that this metric is indeed a measure of diversity:  1.) RND shows sensitivity to the truncation trick in GANs (for images), and limiting the size of vocabulary in text, but does not show sensitivity to any other changes in diversity (such as human judgment of diversity) 2.) It does not compare to previous metrics of diversity, of which there are many 3.) How sensitive is RND to architecture choice. 4.) It is non obvious to what extent the metric is sensitive to image/text quality  Strong metrics should demonstrate lack of "failure modes", as the utility of a metric is its inability to be gamed. Currently, the paper does not demonstrate this property, though I imagine that more work will help clear up the strengths and weaknesses of the metric.  As a result, I can only recommend rejection.
The paper demonstrates that Gradient Descents generally operates in a regime where the spectral norm of the Hessian is as large as possible given the learning rate.   The paper presents a very thorough empirical demonstration of the central claim, which was appreciated by the reviewers.  A central issue to me in accepting the work was its novelty. Prior work has shown very closely related effects for SGD. The reviewers appreciated in discussions the novelty of the precise claim about the spectral norm hovering at around $\frac{2}{\eta}$. R4 and R2 also raised the issue that the related work discussion is not sufficient. Please make sure that you discuss very carefully related work in the paper, including a more detailed discussion in the Introduction.  The two key issues raised by R3, who voted for rejection, were that (1) the work studies Gradient Descent (rather than SGD), and (2) lack of theory. I agree with these concerns. Perhaps the Authors should address (1) by citing more carefully prior work that shows that a similar phenomenon does seem to happen in training with SGD. As for (2), I agree here with R1,R2 and R4 that empirical evaluation is a key strength of the paper.   Based on the above, it is my pleasure to recommend the acceptance of the paper. Thank you for submitting your work to ICLR, and please make sure you address all remarks of the reviewers in the camera ready version.
 The paper theoretically investigates two bias correction methods, reweighting and resampling. It considers a very interesting problem and presents illuminating results. The paper could benefit substantially from improving the experiments so that they clearly validate the theoretical results presented.
This paper proposes an extension to the Dreamer agent in which planning (either via MCTS or rollouts) is used to select actions, rather than sampling from the policy prior. The results show small improvements over the baseline Dreamer agent.  Pros:   Important study on incorporating decision time planning into Dyna based agents   Evaluation on many control tasks rather than just a few  Cons:   Lack of ablations and detailed analysis   Claims aren t backed up by quantitative results  The reviewers generally felt that the approach taken in the paper lacked novelty. I agree that the approach is somewhat incremental (in fact I think it is also an instance of [1]). While both incremental changes and reimplementations of older methods with newer techniques can indeed be valuable, the current paper falls short in terms of the evaluation. As pointed out by several reviewers, there is no in depth analysis explaining the design choices in which rollouts or MCTS are most likely to help (e.g. search budget, exploration parameters, etc.). As these parameters can play a large role in performance, I think it is important to characterize their effect on the agent otherwise, I do not think there is a clear learning regarding how to translate these results to other domains and tasks. Additionally, and perhaps even more seriously, there are a number of claims made in the paper about the proposed method being more data efficient or higher performance. But, it is not clear visually that these improvements are statistically significant, and no quantitative tests have been run (and if the authors want to make a claim about data efficiency, I d especially encourage them to report a metric like cumulative regret). Finally, while the incomplete runs are not a reason for rejection on their own, they do add to my overall sense that the paper is incomplete in its current form.  Given the above reasons, I do not feel this paper is ready for publication at ICLR. I d encourage the authors to perform more careful ablations of the effect of incorporating search into the agent, and to back up their claims with more rigorous quantitative results.  One small point: the authors wrote in the rebuttal that "we are not aware of any work which investigates look ahead search based planning for continuous control with learned dynamics". Grill et al. [2] uses MCTS with learned dynamics in a modification of MuZero, though only applies it in one continuous control task (Cheetah Run).  1. Silver, D., Sutton, R. S., & Müller, M. (2008). Sample based learning and search with permanent and transient memories. ICML. 2. Grill, J. B., Altché, F., Tang, Y., Hubert, T., Valko, M., Antonoglou, I., & Munos, R. (2020). Monte Carlo tree search as regularized policy optimization. ICML.
The paper develops a novel provable defense against patch based adversasrial attacks on image classification system, by combining a novel architecture and certification procedure. The theoretical and experimental contributions are convincing and clearly advance the state of the art in provable defenses against adversarial perturbations.  The questions raised by the reviewers were addressed convincingly by the authors during the rebuttal phase, leading to unanimous consensus amongst reviewers towards acceptance. I recommend acceptance.
The reviewers were excited by the paper s theoretical contribution to continual learning, since that aspect of continual learning is underdeveloped.  However, all reviewers (including the most positive reviewer during discussions) expressed that the paper would benefit from revisions to improve the clarity and the thoroughness of comparisons in the paper.  The paper s focus on OGD is not necessarily an issue for it to be of use to the community, as mentioned as a negative point in one review that other reviewers disagreed with. The authors are encouraged to revise this paper incorporating the reviewers  suggestions.
This paper received borderline recommendations (5, 5, 6, 7) but even the two slightly more positive reviewers were lukewarm (R1 and R2). While the reviewers acknowledged the heavy computational requirements to do an apples to apples comparison with existing baselines, they remain underwhelmed with the lack of experiments. I agree with their criticism; even though the proposed idea seems promising, without comprehensive experiments, it is difficult to judge the significance of this work. R1 commented after the discussion period that an earlier version of this paper actually had ImageNet results. R4 made excellent suggestions to improve the paper further. The authors are strongly encouraged to incorporate them into their future submission.  (I am copying R1 s comment below in case it is invisible to the authors after the notification.)  Sorry for the late update   I have read the rebuttal earlier. I would like to keep my acceptance rating but after the rebuttal I am fine either way. The paper first appeared in March on ArXiv, so indeed it is a concurrent work (actually an earlier work compared to BYOL or SwAV). We have actually tried to reproduce the results in the paper a while back but it did not go well (could not reproduce it), but this time the submission also includes the code. While I haven t run it, I trust the results are reproducible (maybe there are some tricks that I am not aware of).  Regarding running experiments on toy examples   I can understand that this research is resource constrained for ImageNet, but the earlier draft actually had some results on ImageNet (60+ top 1 accuracy) (see appendix of https://arxiv.org/pdf/2007.06346v1.pdf), and for some reason this submission removed that. So this is not a positive sign. Overall, my experience for CIFAR vs ImageNet is that it is easier to make things work on CIFAR, while it is much harder to do so on ImageNet. So maybe some trials are indeed done by the authors, but they choose to not report it in the submission for some reason. On the other hand, one can argue that results on toy datasets are good enough contributions for an early develop of something and they are just not ready for larger and more challenging datasets yet.  Therefore, this paper is quite a struggle. I hoped to see a better than this submission as this paper actually had all the time from March to October to improve its quality of experiments (actually even for ImageNet, one can to dozens of cycles on it during this time), but it did not for some reason.
The paper presents a novel procedure to set the steps size for the L BFGS algorithm using a neural network. Overall, the reviewers found the paper interesting and the main idea well thought. However, a baseline that was proposed by one of the reviewers seems to be basically on par with the performance of the proposed algorithm, at least in the experiments of the paper. For this reason, it is difficult to understand if the new procedure has merit or not. Also, the reviewers would have liked to see the same approach applied to different optimization algorithms.  For the reasons, the paper cannot be accepted in the current form. Yet, the idea might have potential, so I encourage the authors to take into account the reviewers  comments and resubmit the paper to another venue.
This paper shows that the double descent phenomenon of ridgeless regression appears under considerably general settings of the input distributions by showing a lower bound of the excess risk. The analysis covers various types of input distributions including deterministic and random feature maps and its asymptotic sharpness is also shown.  One reviewer raised a concern about its novelty compared with existing work, but the authors properly clarified the novelty in the rebuttal and updated version of the manuscript. Although there were some other minor concerns, the reviewers all agree that this paper gives a valuable theoretical result supporting universality of double descent phenomenon. I also concur with this assessment. I think this paper is a solid theoretical paper giving an informative result as a piece of researches in double descent. Thus, I would recommend acceptance of this paper.
We thank the authors for their detailed responses and the revised version, which addresses several of the questions raised by the reviewers.  The paper is correct and clearly written. All reviewers agree that the idea to add structural features in the message passing of graph neural networks is sensible. While different from previous work, the novelty is a bit incremental though, particularly given the previous work on colored graph neural network. The significance of the work is weak, given 1) the need to select "by hand" structural features that are passed as information, 2) the increased time complexity to compute the structural features compared to other GCNN, and 3) the experimental results that suggest that the benefit of the new approach is limited, particularly on challenging task.  To summarize, this is not a bad paper, but we consider it below the standard of ICLR in terms of originality and significance.
This submission proposes an approach for fusing representations at multiple scales to improve object detection systems. Reviewers thought the paper was well written and showed positive results on COCO, a common object detection benchmark. However, reviewers agreed that there was not sufficient methodological novelty or empirical improvement over existing approaches to warrant acceptance at ICLR: several prior works have addressed multiscale fusion and reviewers did not find the evaluation/ablations sufficient to demonstrate the approach yielded substantial improvements over these existing approaches. I hope the authors will consider resubmitting the paper after refining it based on the reviewers  feedback.
This paper proposes enhancing contextualized word embeddings learned by Transformers by modeling long range dependencies via a deep topic model, using a Poisson Gamma Belief Network (PGBN). The experimental results show incorporating topic information can further improve the performance of Transformers. While this is an interesting idea, reviewers pointed out some weaknesses:    GLUE evaluation is not a test of long term dependencies, it remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task.   The improvement over the baseline does not seem to be significant.   The ablation study could be improved and more experiments could be done to understand the effect of hyperparameters choices from the topic model, such as the number of layers of PGBN as well as the topic number of each layer.   A comparison of the model performance for different lengths of input sequences would be helpful.   There are many recent methods for long.range transformer transformer variants, it would be interesting to compare them against the proposed latent topic based method.  Unfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection.
The authors provide a homotopy framework for SGD in order to exploit structures that arise by construction, such as PL. I very much liked the delineated homotopy analysis which is general (i.e., as opposed to simply adding a quadratic, the authors consider a homotopy mapping). While the algorithm should not be considered new, it is still a good proposal to consider in the SGD applications setting. Unfortunately, I cannot recommend acceptance because of several issues that the reviewers raised in detail: Strength of the assumptions, unclear performance improvement in practice, applicability of the locally PL condition, among others. 
Pros:   All reviewers agreed that the idea was particularly interesting/novel. I personally appreciated the perspective of unlearning invariances that prove inconsistent with the training data, rather than learning invariances that are demonstrated by the training data.   The authors significantly improved clarity during the rebuttal period, and two out of three reviewers raised scores or confidence as a result.  Cons:   There were significant concerns raised by reviewers about clarity of presentation, and some concern around whether the specific instantiation of the high level idea was the most sensible. From a *lightweight* reading of the paper on my part, I also feel that the writing style is unnecessarily dense, though I believe the underlying ideas are solid.   One of the reviewers (AnonReviewer4) continues to have serious concerns. I believe the authors and AnonReviewer4 may have both become more entrenched in their positions during the discussion, in a way that wasn t particularly productive.  This paper is borderline score wise. I believe it is particularly important to reward and encourage unusually novel work. Primarily for this reason I bias my decision upwards, and recommend acceptance.  nit: belive  > believe
The paper proposes and studies a new SO(2) equivariant convolution layer for vehicle and pedestrian trajectory prediction. The experiments are detailed and demonstrate the effectiveness of the approach in relation to non equivariant models.
The paper studies the globally optimal solutions to deep network training problems using convex duality. It derives duals of training problems in which we attempt to fit a dataset while regularizing the network weights with weight decay (L2 regularization). The paper uses strong duality to characterize optimal solutions to several instances of this problem. For fitting deep linear networks, it proves that the globally optimal weights “align” across layers. For fitting ReLU networks, it studies two cases: rank one data and “whitened” data, which satisfy $X^T X   I$. It proves that the optimal weights satisfy certain alignment and orthogonality conditions.   Pros and cons:  [+] The paper uses the machinery of convex duality to characterize alignment of weights in optimal solutions to various neural network training problems. The extension of this approach from shallow networks to deep networks is potentially significant.   [+] The paper is well written and technically precise.   [ ] As noted by the reviewers, the assumptions required to analyze deep ReLU networks are somewhat restrictive. In particular, the paper assumes a form of whitening in which the observed data vectors are orthonormal. This is much a much stronger assumption than the whitening usually applied in statistics, in which a linear transformation is applied to ensure that $XX^T   n I$, i.e., the empirical covariance is the identity. While the paper and rebuttal are correct to argue that SGD often uses minibatches of size $n’ \ll d$, the paper’s main claims are about the globally optimal solutions to the overall training problem.   [+/ ] Several reviewers raise concerns about the significance of results on the rank one case for practice. The paper correctly notes that a number of previous works have studied rank one data, and that this paper generalizes those results to deep networks. The paper gives a very clear and explicit recipe for the optimal weights in this restricted setting.  Reviewers are split on the importance of this generalization — in particular, the extent to which results for the rank one case lead to insights that generalize to higher ranks.   [ ] Experiments verify the theory, in the sense that the theoretically derived weights are equal or better than those learned by SGD, in terms of the training loss. However, the learned networks do not seem to generalize (right panels of Figure 4), again raising concerns about the realism of the setting.   Reviewers evaluation of the paper is split, with most reviewers appreciating its technical rigor and clean resolution of the rank 1/linear/whitened cases. While the review generated enthusiasm for the paper and its results, there were also significant concerns about the relatively restricted setting and the strength of the paper’s implications for training realistic networks, some of which remain after the authors response. 
 While reviewers find the ideas in the paper interesting, they also raise several major concerns. In particular, R1 and R4 find the claims of "invertible" and "lossless" to be potentially misleading. The bijective property is achieve on the first stage (L 1 layers) due to a sequence of one to one mappings, as is done in previous work (e.g. i RevNet)  so the novelty is limited.  As stated by R3,  since the paper is a combination of previous methods, the writing should be substantially improved to clarify what the real, new contributions are. The interpretation of the results (e.g. Figure 4) should also be better explained.
This paper received 4 reviews with mixed initial ratings: 4, 8, 5, 7. The main concerns of R1 and R2, who gave unfavorable scores, included limited methodological novelty beyond the data generation and insufficient empirical evaluation of state of the art methods on the proposed dataset. The authors submitted a new revision with a summary of changes and provided detailed responses to each of the reviews separately: it addressed some of the concerns, but did not change the overall position of the reviewers. AC agrees with R3 and R4 that the proposed dataset and the environment may have certain practical impact and enable new research in learning CAD reconstruction. However, the contributions are indeed specific to a narrow CAD community, and R1 felt that the paper needs another round of peer reviews before acceptance, as a significant number of new results have been added during the discussion stage. After discussion with PCs, the final recommendation is to reject.
This paper proposes a method for tool synthesis by jointly training a generative model over meshes and a task success predictor. Gradient based planning is then used to find a latent space tool representation which maximizes task success, given a starting tool and an input scene. The results indicate that this method can successfully generate simple tools, and that it performs better than either a random baseline or a version where the generative model and success predictor are trained independently.  The reviewers unanimously felt that this paper was not quite ready for publication at ICLR. While I m a strong believer that unique and creative papers which tackle understudied problems such as this one ought to be encouraged, and that the authors  rebuttal satisfactorily addressed most of the reviewers  concerns, there was one major point that was not. In particular, all reviewers noted that the paper lacks comparison to convincing baselines and/or sufficiently extensive experiments. While I do not think baselines are necessary per se, especially in such a unconventional setting such as this, I believe what the reviewers are getting at (and I agree) is that the results as presented don t really help the reader understand the contours of the method and/or problem space, and as a result, the contributions of the paper feel thin. For example, here are some questions that the reviewers raised, which I do not feel were adequately addressed:    R3: What are the failure cases of the model?   R2: How important is the particular representation of the task and tool (i.e., visual for the task, meshes for the tool)?   R4: How do the imagined tool trajectories compare between the task aware and task unaware cases?   R4: Is the success classifier trained to the same level of performance in both task aware and task unaware settings? (In general, it would be helpful to include learning curves in the appendix.)   R1: How important is the choice of the particular planning/optimization method (i.e. gradient descent)?   R1: What is the generalization performance of the model along affordance directions (e.g. needing to synthesize longer/shorter tools than seen during training)?  Taken individually, such questions might not be an issue, but together they illustrate a larger concern that the paper has not done a thorough enough job of analyzing and evaluating the proposed method. Therefore, at this stage I recommend rejection. I think that by fleshing the paper out with some answers to the above questions, this could make an excellent submission to a future conference.
This paper presents a continual learning method based on a novelty detection technique. All reviewers are concerned about various issues, especially, motivation, experiment, and presentation. One of the reviewers was initially positive about this paper but downgraded his/her score due to unresolved problems in the proposed method. Considering all the comments and communications with the authors, AC believes that this paper is not ready for publication yet.
This paper proposes a new online contextualized few shot learning setting, with two associated datasets (notably, including one obtained from trajectories within the real world Matterport3D reconstructions). A simple recurrent contextualized extension of Prototypical Networks is also proposed as a stronger baseline, demonstrating the need for incorporating such context. The reviewers all agreed that this is an interesting setting combining continual and few shot learning, offering a more realistic problem that mirrors those that might be encountered by embodied agents. The authors provided very detailed rebuttals, answering some of the questions and concerns raised by the reviewers. In the end, all reviewers agreed that this paper would contribute a significant novel setting, and so I recommend acceptance. I encourage the others to include modifications related to some of the comments, such as strengthening/clarifying the setting including metrics, details of the method, etc.
This paper explores the effect of poorly sampled episodes in few shot learning, and its effect on trained models. The improvements from the additional attention module (CEAM) and regularizer (CECR) are strong, and the ablations are thorough. The reviewers are not fully convinced that poor sampling is indeed the main issue. That is, it could be that CEAM and CECR improve performance for other reasons, but the hypothesis is sensible, and the reviewers believe a more thorough investigation is beyond the scope of this work.  During discussions, one note that came up is whether CEAM works because of cross episode attention, or if the idea of an instance level FEAT is itself a good one. One ablation to sort this out would be to apply FEAT and an instance level FEAT on episodes that are twice as large as those seen by CEAM so that the effective episode size is the same. This would help answer: is it the reduced noise due to effectively larger episodes, a stronger attention mechanism using instance level information, or is the idea of crossover episodes indeed the important factor? The reviewers agree that this baseline, or an analogous baseline, should be included in the final version. 
The paper defines a "local data matrix" (inspired from local Fisher matrix) and uses it to obtain a foliation in the data space. This provides a lens to view the data space from model s perspective. While the idea is interesting, reviewers have two main concerns from the reviewers which are not fully addressed in the author response:  (i) The method works with partially trained model (1 epoch for MNIST) and it s not clear how the observations made in the paper extend to fully trained models,  (ii) The motivation and application of the proposed model centric view of data space needs more work   it will be good to think of some applications where this view can help.   I encourage the authors to consider the suggestions from the reviewers (e.g, R3 suggested label smoothing for (i)), and submit a revised version to a future venue. 
This paper proposes an approach to estimating uncertainty in deep neural network models that avoids the need to make multiple forward passes through a network or through multiple individual models in a posterior ensemble. In terms of strengths, this is an important and timely topic that is of significant interest. The paper is clearly written for the most part. In terms of weaknesses, the significance of the work is low. As the reviewers note, there are multiple questions around the experimental evaluation that remain unresolved following the author feedback and discussion. In particular, the authors do not compare to baseline MCMC methods like HMC/SGHMC that can yield gold standard estimates of posterior predictive uncertainty. While not feasible for large scale models, MCMC methods provides crucial sanity checks for uncertainty estimation on small scale (e.g., MNIST scale) models. Posterior distillation methods like Bayesian Dark Knowledge are also not considered in the evaluation and should be compared to where the distillation computation is feasible. There are also foundational technical correctness issues with respect to uncertainty quantification due to the fact that the paper is approximating the measure of uncertainty produced by MC Dropout, which itself only approximates the true Bayesian posterior predictive distribution under additional assumptions. This makes empirical comparissons to MCMC methods all the more important. Following the discussion, the reviewers agree that the paper is not yet ready for publication.
Although the rebuttal helped clarify the reviewers  confusion on notational confusion and the motivation of problem setup, all reviewers are still in a position of unable to championing the paper:   the technical concerns by Reviewer 4 need to addressed   the paper would have been stronger if baselines such as one class classification / outlier detection have been compared   the algorithm also has at least one short coming over other techniques that it needs to wait some time until it can collect enough test data  We hope the reviews can help the authors strengthen the paper in the next revision. 
The paper proposes to train a rejection sampler in the latent space of a GAN to learn disconnected data manifolds. Reviewers raised concerns about some theoretical aspects of the method as well as about the lack of larger scale datasets (ImageNet) in the experiments. Authors responded to these concerns but some of them still remain (including $\hat{\gamma}(z)$ not guaranteed to be a probability distribution and lack of more convincing experiments). I still think the work is promising, and encourage the authors to revise and resubmit the paper addressing these points highlighted by the reviewers. 
The paper explores the representation power of GNNs, in particular, studying the bottleneck and improving expressiveness with new aggregators, which are analyzed theoretically. This issue was highlighted in previous works, but the merit of this paper is a constructive analysis.   The reviewers were overall not enthusiastic and  raised a few concerns:   Not enough context is provided about related work, in particular, the early work of Corso et. al.    Insufficiently convincing experiments  While the authors provided an elaborate rebuttal and extended the experimental section to address experiment concerns raised by most of the reviewers, the final evaluation was still lukewarm. Given that the conference has a very high bar and there have been many very good submissions on graphs, we find the paper not quite above the bar and hence have no choice but to recommend rejection with a heavy heart. The authors should be commended on their efforts and are encouraged to seek publication elsewhere.   
This paper presents an interesting idea for task free incremental learning on the data stream. The reviewers have extensive discussions after reading all the reviews and the author s rebuttal. There are concerns raised about the presentation of the method and the justification for some parts of the model design choices. The reviewers believe that after addressing these weaknesses the work can be made stronger and may be accepted in a competitive venue.  
The main contribution of this work is introducing large and carefully curated datasets for benchmarking morality judgments of language models. First of all, I d like to thank the reviewers for their detailed and thoughtful reviews and for being engaged in discussions with the authors. We believe that the paper is now much stronger than the initial submission.  The reviewers judged this work as important and largely well executed.  Some of them have initially raised concerns that the claims are too bold but these seem to have been addressed in the revisions and the rebuttal. R4 is still concerned that the ICLR format is not suitable / optimal for presenting a dataset. While we agree that journal format could be more suitable for this work, we do not see that as enough reason to reject the paper, especially given that the author invested much effort in providing extra details about the annotation and the underlying theories.   There are also suggestions to expand error analysis but this also seems to have been mostly addressed.     
The paper proposes a layer wise or block wise distillation scheme, Neighbourhood Distillation, that aims to reduce the training time and to improve parallelism when distilling large teacher networks. By breaking down the end to end distillation objective into blocks, the proposed method enables faster distillation when applied to model compression and block wise architecture search. Several concerns of reviewers were addressed during the rebuttal period by the authors.  However, there are still some concerns among the reviewers after the discussion:  1) Computational cost benefit of the proposed KD method seems marginal in comparison to the baseline, given that we still have to train all the neighborhoods in parallel and potentially to fine tune in the end.   2) It was brought up by a couple of the reviewers that the experiments lack diversity. It would be great to a clearly defined metric applied to a wide variety of the model architectures and datasets. It will also strengthen the paper by providing more details on the experiments.  The basic idea is interesting, but the paper needs further development and modification for publishing.   
The paper explores a solution for mixed precision quantization. The authors view the weights in their binary format, and suggest to prune the bits in a structured way. Namely, all weights in the same layer should have the same precision, and the bits should be pruned from the least significant to most significant. This point of view allows the authors to exploit techniques used for weight pruning, such as L1 and group lasso regularization.  Although the field of quantization and model compression/acceleration is quite mature by now and has a large body of works, this paper is novel in its approach. Although the improvements provided over SoTA results are not very large, I believe that the novelty of the approach would make this paper a welcome addition to ICLR.  There are a few issues to be dealt with pointed out by the reviewers such as confusing terminology or required clarifications, but these are minor revisions that I trust the authors will be able to add to their paper. 
The manuscript presents an approach for identifying sources of uncertainty in object classification tasks by disentangling representations in latent spaces.  Three reviewers agreed that the manuscript is not ready for publication.  Some of the concerns are conceptual flaws, weak evaluation protocol, and an incorrect interpretation of experiment results.  There is no author response.  
After rebuttal the reviewers unanimously agree that this is a strong paper and should be accepted
All reviewers recommend rejection: concerns were raised in terms of technical correctness, quality of presentation and the quality of experiments. There was no rebuttal. The AC agrees with the reviewers and recommends rejection.
The authors have conducted a thorough empirical study on the hyperparameters of representative adversarial training methods. The technical novelty of this paper might be insufficient.  But the empirical findings in this paper explain the strange and inconsistent reported algorithm results in the literature to some extent and remind the necessity and importance of a careful study on hyperparameters. The authors have actively interacted with the reviewers and through the discussions, many unclear issues have been fixed.  
This paper proposes a method to explore neuron interactions within a neural network by deriving rules for the activations of units at different layers. The rules can presumably help interpret the inner workings of the neural network.  The reviewers have very different opinions on the paper and the views did not converge.  However, there is a common concern on the lack of quantitative evaluation on the faithfulness of the rules to the models. I therefore do not recommend accept.   R1[5]: On a related note, I felt the evaluation presented by authors while extensive is rather qualitative in nature. R2[3]: Given that I could provide you with a couple of references that you admit is relevant, and this was just off the top of my head, would you care to comment on a quantitative comparison with the referenced approaches? R3[8]: The examples look very impressive, but my main concern is with whether the examples could have been cherry picked, in the sense that most of the thousands of rules produced may not be useful.   
The reviewers are in consensus that the manuscript is not ready for publication in its current form: more comprehensive evaluation, and careful analysis (either theoretical or empirical) of the simple but effective methodology would improve the quality further. The discussion was constructive and helped the authors to reason about their work better.  The AC recommends Reject and encourages the authors to take the constructive feedback into consideration . 
The problem as formalized in this paper is essentially a domain adaptation problem. There is a training distrinution P and a test distribution P*. the learner gets training data generated by P and aims to minimize the loss of its hypothesis w.r.t. P*. How is it relate dto fairness? The authors add the assumption "we assume the unbiased Bayes decision rule is algorithmically fair in some sense and hope that enforcing the correct notion of fairness allows us to recover h∗ from P". Under such an assumption, almost by definition "enforcing fairness may improve accuracy". By a similar logic, if we assume the unbiased byes decision rule is biased against a certain group, then enforcing bias against that group will imporve accuracy ....
The paper proposed to learn a sequence metric by sharing a memory cell between two LSTMs that run on pairs of sequences. I found the idea quite interesting, but I (and the reviewers) found the inspiration and the analogy from a dynamical systems perspective a unclear, unconvincing and maybe not even necessary to the core essence of the method that was proposed.   The reviewers appreciated the improvement in clarity over the course of the review, but felt that there was still some more distance to cover. In addition, the results were not of a high enough quality to lend support to the success of the method and the experimental section needs more work making it not ready for ICLR acceptance at this time. 
This paper proposes a heuristic for removing privacy sensitive attributes and replacing them with sythetically generated ones. The technique is closely related to an existing work and, as pointed out in the reviews, the experimental evaluation is insufficient for properly evaluating the approach.
This paper studies whether neural networks with different architectures, especially different width and depth, learn similar representations. All reviewers agree that the investigations are thorough and the experimental discoveries are convincing and well explained. Good work. I recommend accept.
The AC and reviewers agree this is an important line of research. However, only one reviewer was initially positive, as the other reviewers raised some issues, and the rebuttal only partially addressed some of them (e.g., the reviewer is now OK with Lemma 1 being correct), but there are typos in the proofs, and there were other issues like the uniform bound that R3 brought up which was retracted in the revision, and such. These issues give us a bit of lack of confidence in the rigor of all the results.  In addition to the lack of carefulness in places, this paper (more than usual for an accepted paper) seemed to miss references in the literature. In addition to all the ones pointed out in the reviews (especially R3 s, which I don t think was fully adequately discussed in the rebuttal), other tight lower bounds on uniform stability have been developed recently (see Thm 4.2 in Bassily et al. https://arxiv.org/pdf/2006.06914.pdf).  From the optimization point of view, it is undesirable to introduce new conditions unless really necessary, and often these new conditions are previously known under a different name; if they really are new, they should be compared to old conditions. In particular, then new "Hessian contractive condition" should be compared to standard non convex conditions like strong growth, error bound, Polyak Lojasiewicz, etc.  Finally, this is based off the Hardt/Recht/Singer 2016 paper, but there is a more recent Hard/Recht work that argues that algorithmic stability is not the right tool, because it cannot explain the fact that training error drops roughly the same with real data or with data with completely random labels   so any generalization theory has to be data dependent. See: "Understanding deep learning requires rethinking generalization" (https://arxiv.org/abs/1611.03530) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals.  So this issue should be addressed as well.  Overall, this could be a promising paper and the AC recommends the reviewers make a substantial revision addressing these concerns.
The paper provides a transfer learning approach to HPO. It builds and improves upon existing methods of zero shot HPO where the high level idea is to use the outcomes of hyper parameters on an offline collection of datasets in order to speed up HPO on a new dataset. On the plus side, the methods provided seem to be novel, and the results seem to be promising. The main issue is the writing and clarity of the paper, making it hard to be certain of the good qualities of the paper. Aggregating the reviews, the details are too spread out between the appendix and main body, the techniques require more motivation behind them, and important details of the experiment are somewhat vague. The authors provided a modified version which is definitely a step in the right direction, however, it does not seem to be enough. I think this is a solid paper based on a promising idea. However, given the almost unanimous agreement about that crucial gap in clarity even after the modified version was uploaded, I recommend rejecting the paper. 
While there was some interest in the analysis, the consensus view was that the original treatment was not sufficiently well motivated, and the revision was too dissimilar from the original submission for it to be evaluated for publication in this year s ICLR.
This paper proposes a new approach to training networks with low precision called Block Minifloat. The reviewers found the paper well written and found that the empirical results were sufficient. In particular, they found the hardware implementation was a strong contribution. Furthermore, the rebuttal properly addressed the comments of the reviewer.
This paper introduces an ensemble method to few shot learning.  Although the introduced method yields competitive results, it is fair to say it is more complicated than much simpler algorithms and does not necessarily perform better. Given that ensembling for few shot learning has been around for a while, it is not clear that this paper will have a significant audience at ICLR.  Sorry about the bad news,   AC.  
I thank the authors and reviewers for the lively discussions. Reviewers found the work to be interesting but some concerns were raised regarding the significance of the results. In particular, two reviewers mentioned that authors did not fully address their concerns in the rebuttal period. Given all, I think the paper still needs a bit of work before being accepted. I recommend authors to address comments raised by the reviewers to improve their work.   AC 
The paper analyses the behaviour of Neural Processes in the frequency domain and, in particular, how it suppresses high frequency components of the input functions. While this is entirely intuitive, the paper adds some theoretical analysis via the Nyquist Shannon theorem. But the analysis remains too generic and it is not clear it will be of broad interest to the community. 
The authors compare different model based R algorithms to see whether observation prediction is important. They show that, as expected, it is. On the other hand, they seem to show that latent space prediction is not very useful. The study is limited to domains with image data: Does this domain have something particularly special? Perhaps experiments with smaller scale POMDP problems might actually have shown something different. It is very difficult to do a study of this type properly, and although the authors have tried, it s hard to see how this paper can be accepted. I agree with some the positive points some reviewers have raised, but I think that, at the end of the day, the paper is trying to draw too general conclusions from a handful of datapoints. Were I writing this paper, I would first try the simplest version of the hypothesis with very basic environments that are, however, more varied than the ones shown here. Would the hypothesis hold, I d scale up to more complex environments and try to also run with more seeds to get a clearer signal. 
The paper presents a variance reduction technique to the Straight Through version of the Gumbel Softmax estimator. The technique is relying on the truncated Gumbel of Maddison et al. I share the excitement of the reviewers about this work and I expect this technique to further influence the field. 
All four knowledgeable referees have indicated reject due to many concerns. In particular, reviewers pointed out that the novelty of this paper is not clear because the difference from related work is very limited (i.e., the difference from Z. Wang and S. Ji is not clear, other than using one additional layer),  and they were concerned that the results of the experiment are not convincing (For example, the results reported in this paper are significantly inferior to those reported in other papers, the GNN architecture used is limited, and the performance difference especially in the additional experiments in the revision, is very marginal). No reviewers were convinced by the authors  claims even through the author s rebuttal and revision.  One important note: Reviewers have stated that they did not explicitly check the identity of the author and did not pose a problem on this, but if we follow the link specified in the original submission, we can see the identity of the author, which may be considered as a violation of the double blind policy. This is a small and regrettable mistake, but it can be a serious problem in the review process. In this review process, reviewers unanimously suggested rejection even ignoring this issue, but it seems that you need to pay attention in your future submissions. 
Unfortunately, the authors did not submit a response during the rebuttal phase.
Thank you for your submission to ICLR.  The reviewers and I unanimously felt, even after some of the clarifications provided, that while there was some interesting element to this work, ultimately there were substantial issues with both the presentation and content of the paper.  Specifically, the reviewers largely felt that the precise problem being solved was somewhat poorly defined, and the benefit of the proposed preimage technique wasn t always clear.  And while the ACAS system was a nice application, it seems to be difficult to quantify the real benefit of the proposed method in this setting (especially given that other techniques can similarly be used to verify NNs for this size problem).  The answer that this paper provides seems to be something along the lines of "ease of visual interpretation" of the pre image conditions, but this needs to be quantified substantially more to be a compelling case.
All the reviewers unanimously agree that the paper should be rejected. The main concern is well summarized by comment by R1 s comment "While the problem is interesting, I found the paper difficult to read as the task is ill defined in section 3 where many notation definitions are missing and some notations are reused in different contexts with different definitions". Also, as R4 mentions the proposed method can be reduced to reward engineering and doesn t provide any scientific or methodological advancement to the problem of testing hypothesis. The authors did not provide any rebuttal. 
This paper proposes a contrastive learning framework that leverages hard negative samples for self supervised training. The proposed framework is theoretically analyzed and its efficacy is examined on several datasets/problems. A group of expert reviewers reviewed the paper and provided positive ratings for this paper. I agree with the reviewers and I recommend accepting this submission.   One of the main discussion points among the reviewers was to what degree Pr1 is "approximately" satisfied in the proposed framework. There are several approximations in this paper that are not fully analyzed.  Some of these approximations could be examined assuming that labeled data is available during training. For example, $p_x^+$ is approximated using a set of semantics preserving transformations. In practice, the distribution induced by augmenting $x$ is very different than the distribution that samples from the instances in the class of $x$. The effect of this approximation could be easily examined by sampling from true class labels. Additionally, it would be very helpful to visualize how $q$ samples from the negative instances and how much it follows Pr1.  I would like to ask the authors to add a small limitations section to the final camera ready version that lists all the assumptions and approximations made in this paper. Please provide a high level analysis on how such assumptions could be validated or such approximations could be measured if labeled data or additional information was provided. This discussion is extremely important for future practitioners to understand the basic assumptions that may not hold in reality and it will enable them to improve upon this work.  
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with the reviewers that the paper touches an interesting topic (reversible computing) but could be improved in the area of presentation and evaluation. Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.  
This paper explores a methodology for learning disentangled representations using a triplet loss to find subnetworks within a transformer.  The authors compare against several other methods and find that their method performs well without needing to train from scratch. The reviewers thought this paper was well written and the authors were very responsive during the review period.  However, there were some questions about the experimental setup and empirical performance of the paper, leaving the reviewers wondering if the performance was convincing.  We agree that there is value in exploring disentangled representations even if they do not necessarily improve performance (as the authors point out), but clearly explaining the reasoning behind all analyses (e.g. specifically choosing domains to introduce a spurious correlation), and justifying differences in performance is particularly important in these cases.
This paper collects a variety of results that cast straight through estimators as arising as principled methods that make a linearization assumption on the loss for functions with binary arguments. R1 & R3 recommended against acceptance, citing clarity concerns and a lack of novelty. R2 & R4 recommended acceptance, but had low confidence. This paper had uncharacteristically low confidence on behalf of the reviewers, and this is my fault. I apologize to the authors for this.   I have read the paper myself. I believe that this paper contains many interesting ideas, but I agree with R1 & R3 that the paper suffers from clarity issues. Unfortunately, these issues persist in the recent revisions, despite having been asked by R1 & R3 to improve the clarity. The authors asked for concrete reference points. Here are some:    "proxy function" is not well defined, despite being critical to the arguments.   deterministic ST is not defined clearly before it is discussed.    The section structure of Sec 2 could be improved. At the moment it seems to flow from the loss function to the standard ST algorithm through to a disjointed list of questions addressed in the paper.   The section titles are not particularly informative.   It is difficult to know which results are known and which results are new.  In general, I believe this work could benefit from a significant restructuring. It would be best to delineate preceding work in its own section, then lay out the new results, making sure that all of the important concepts are clearly defined. I think many of these results are valuable for the community, but the current draft makes it challenging for these great ideas to reach their full potential. 
This paper proposes an interesting approach for learning to decide whether a query graph is isomorphic to a subgraph within the target graph.  The approach has a number of interesting aspects from the machine learning perspective, e.g. the anchored graphs and the order embeddings.  Empirical results show promise in ablation studies and against a few baselines.  However this paper also has a number of issues as pointed out by the reviewers, placing it right on the borderline.  Most notably the clarity of the presentation could be improved as it seems to confuse a few reviewers at various points.  Another thing that I’d like to highlight is that the way to convert the pairwise scores f(z_q, z_u) into the final decision about G_T and G_Q seems worthy of a longer discussion.  Is a simple average across all pairs the best we can do?  I imagine if the query graph is small but the target graph is large then even if the G_Q does match a subgraph of G_T the average score can be quite low.  Overall I do like the ideas proposed in this paper, but also recognize that the paper can benefit from more improvement, so I’d like to recommend rejection but encourage the authors to submit again in the next round.
The paper proposes a competition on generative models on a new dataset to study memorization in generative models and propose a  new metric Memorization Informed Frechet Inception Distance (MiFID).   While this is an important topic, reviewers raised multiple issues and concerns regarding 1)  the metric definition (that it needs to be max and not min, this was acknowledged in the rebuttal but not updated in the paper) , 2)  how this competition is ran in terms of the definition of "cheating",  that the setup is not controlled and only constraining the time of training 3)  the notion of MiFID is depending on the sets of samples considered and the feature extractor used.   Some other reviewers raised concerns that the paper is only concerned by FID and not other metrics , and that it was only verified on GANs.  We hope the authors will address those concerns and submit the paper to an upcoming venue.
This submission got 1 reject and 3 marginally below the threshold. The concerns in the original reviews include (1) lack of theoretical justification. The motivation and claim are from empirical observation; (2) the performance improvement is minor compared with the existing methods; (3) some experiment settings and details are not explained clearly. Though the authors provide some additional experiments to the questions about the experiments, reviewers still keep their ratings. The rebuttal did not address their questions. AC has read the paper and all the reviews/discussions. AC has the same recommendation as the reviewers. The major concerns are (1) the theoretical justification is not clear. The additional explanation given by the authors in their rebuttal, i.e., the prediction becomes sharper and thus the model generalization ability can be improved, is not justified. (2) the experiments are not very convincing and can be further improved in the following two aspects: (1) the motivation experiments should be conducted in a consistent manner, instead of using simplified EL in some cases; (2) the effectiveness of EL should be more significant otherwise it is not clear whether the claim is true or not. At the current status of this submission, AC cannot recommend acceptance for the submission.
The authors propose two algorithms and their theoretical analysis for solving bilevel optimization problems where the inner objective is assumed to be strongly convex. The authors have greatly improved the paper to answer reviewer comments and three out of four reviewers have increased their scores. That said, given the large amount of new material added to this paper during the discussion phase, the program committee believes the paper requires a new round of reviews for a confident assessment. We encourage the authors to resubmit their work to a top conference such as ICML.
This paper proposes an approach to unifying both full context and streaming ASR in a single end to end model.   Techniques such as weight sharing, joint training and teacher student knowledge distillation are used to improve the training.  The so called dual mode ASR is evaluated under the ContextNet and Conformer networks on Librispeech and MultiDomain datasets. The performance is good.  While the technical novelty is not overwhelmingly significant, all reviewers agree that it may have impact to the speech machine learning community as high performance streaming ASR is of great importance in real world deployment of ASR systems.  The authors have meticulously addressed the reviewers  comments and, in particular, changed the title from "universal ASR" to "dual mode ASR" as suggested by some of the reviewers.  After the rebuttal, all reviewers are supportive on accepting the paper.  
This paper proposes a simple method to discover latent manipulations in trained text VAEs. Compared to random and coordinate directions, the authors found that by performing PCA on the latent code to find directions that maximize variance, more interpretable text manipulations can be achieved.   This paper receives 4 reject recommendations with an average score of 3.75. The reviewers have raised many concerns regarding the paper. (i) The idea is straightforward with limited novelty. (ii) There are only mostly qualitative results presented. More in depth analysis and more solid evaluations are needed. (iii) Human evaluation is too small to draw any reliable conclusion. (iv) The proposed method is only tested on one text VAE, how well it can be generalized to other models remains unclear.  The rebuttal unfortunately did not address the reviewers  main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere. 
This paper studies the unlabeled entity problem in NER. Specifically, performance degradation in training of NER models due to unlabeled entities. It analyzes the reason through evaluation on synthetic datasets and finds that it is due to the fact that all the unlabeled entities are treated as negative examples. To cope with the problem, it proposes a negative sampling method which considers the use of only a small subset of unlabeled entities. Experimental results show that the proposed method achieves better performances than the baselines on real world datasets and achieves competitive performances compared with the state of the art methods on well annotated datasets.  Pros •	The paper is clearly written. •	The proposed method appears to be technically sound. •	Experimental results support the main claims. •	The findings in the paper are useful for the field.  Cons •	Novelty of the work might not be enough.  The authors have addressed some clarity and reference issues pointed out by the reviewers in the rebuttal.  Discussions have been made among the reviewers. 
This paper proposes a meta learning method that learns structured features based on constellation modules. Exploiting object parts and their relationships is a promising direction for few shot learning as AnonReviewer3 described. The effectiveness of the proposed method is demonstrated with experiments using standard benchmark, and ablation study.  
All three reviews for this paper were negative, and the authors did not provide rebuttals or comments on the reviews.  The main drawback of this work identified by the reviewers is that the empirical study is not sufficient (e.g., limited comparisons and ablation studies as well as low dimensional examples).
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with reviewers that the paper has limited novelty and could be more clear about mix precision training. Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.  
This paper proposes a pre training technique for semantic parsing with an emphasis on semantic parsing and the technical details required to actually make it work in practice. Overall, all reviewers agree that the results are very good, and you see nice improvement across multiple text to SQL datasets. Some reservations have been raised on (a) the difficulty of creating the SCFG for generating the synthetic data, but this seems to have been properly addressed by the authors and requires a reasonable amount of effort. and (b) how tailored the pre training task is to a particular task (text to SQL) and dataset (Spider). Overall, I tend to agree that the fact that one sees improvement on Spider is slightly less compelling as the grammar is derived from it, but the authors rightfully claim that consistent improvements are also evident in other datasets, even if the gains are somewhat smaller. One can hope the idea can also be generalized to other setups where synthetic data can be generated and the details of how to combine synthetic data with real data should be useful. 
The paper raised a natural question: why good synthetic images can be not so good at training/fine tuning models for downstream tasks (e.g., classification and segmentation)? This problem is named synthetic to real (domain) generalization (where syn/real images are regarded as from the source/target domain), and it is of practical importance when using GAN like methods given limited real images for training. The authors found that the answer to the question is the diversity of the learned feature embeddings, and argued/advocated that we should encourage such diversity when training on syn images in order to better approximate training on real images. To this end, a novel contrastive synthetic to real generalization framework was proposed and shown effective in the well designed experiments.  Overall, the quality is above the bar. While some reviewers had some concerns about the applicability and the motivations for the algorithm design, the authors have done a particularly good job in the rebuttal. After the rebuttal, we all think the paper should be accepted for publication.  I have some comments on the writing. The introduction claiming so many things has only 4 citations, especially the first two paragraphs have no citation. While I do think what claimed there are correct, the authors should include certain supportive evidences after each claim by themselves. Moreover, while I do think the problem hunting part is well motivated, the problem solving part needs its own motivation/justification. When two or more components are combined in a proposal, why this component is chosen and is there other choice that can achieve the same purpose (this concern has also been raised by reviewers)? I believe the components are not randomly chosen among possible candidates (e.g., "we further enhance the CSG framework with attentional pooling"), but for writing a paper, the authors should explain the motivation for the algorithm design because we cannot know the motivation unless they tell us.
This paper considers the problem of identification of causal effects under the unsupervised domain adaptation setting. The authors assume the invariance of the causal structure and use it to regularize the predictor of causal effects. The method is interesting and looks effective, although this assumption may not hold always true (e.g., in some domains, some causal influences may disappear, leading to extra conditional independence relations). Hope the authors will update the paper to address the concerns raised by the reviewers, especially to conduct a sensitivity analysis of the framework to misspecification of the causal structure and make the motivation for the used evaluation metrics clear, and also provide a more thorough review of related work.
The authors propose an intriguing alternative to IFT or unrolled GD as a method for optimizing through arg min layers in a neural net, by using a differentiable sampling based optimization approach. I found the general idea in the paper to be intriguing and thought provoking. The reviewers generally seem to have also appreciated the method, and many of the reviewers  concerns were addressed by the authors during the rebuttal. Although the paper does have a number of flaws   in particular, the evaluation is a bit hard to appreciate, since improvement over prior work is either unclear, or no meaningful comparison is offered,   I think in this case the benefits outweigh the downsides. The work is far from perfect, but the ideas that are presented are interested and valuable to the community, and I think that ICLR attendees will appreciate learning about this work. I would encourage the authors however to improve the paper, and especially the empirical evaluation, as much as possible for the camera ready, and to take reviewer comments into account insofar as feasible. I m also not sure how much I buy the "overfitting to hyperparameters" argument for unrolled GD, and a less charitable interpretation is that the authors present this issue largely to make up for the comparative lack of other benefits. That s not necessarily a bad thing, but I think making such a big deal of it is a bit strange. It s probably fair to say at this stage that the actual benefits of this approach are a bit modest (though improvements in runtime are a good thing...), but the idea is interesting, and may spur future research.
This paper explores the performance of Q learning in the presence of either one sided feedback or full feedback. Such feedbacks play an important role in improving the resulting regret bounds, which are (almost) not affected by the dimension of the state and action space. The motivation of such feedback settings stems from problems like inventory control. However, the assumptions underlying the theory herein are often quite strong, which might limit the applicability of the theory. The dependency on the length per episode H can also be improved.   
This work investigates the choice of a  baseline  for attribution methods. Such a choice is important and can heavily influence the outcome of any analysis that involves attribution methods. The work proposes doing (1) one vs one attribution in a sort of contrastive fashion (2) generating baselines using StarGAN.  The reviewers have brought out a number of valid concerns about this work:  1. One vs one attribution appears to be novel, and distinctive enough from the more prevalent "one vs all" formulations. I am perhaps more optimistic than the reviewers that such a formulation is in fact useful, but I can see where the hesitancy can come from. 2. It s not clear that the evaluation shows that the proposed method is in fact superior to the others. All the reviewers touched upon this one way or another. 3. Somewhat simplistic datasets used for evaluation (noted that there are CIFAR10 results in the rebuttal).  This was more borderline than the scores would indicate. I thank the authors for the extensive replies and extra experiments. I encourage them to incorporate more of the feedback and resubmit to the next suitable conference. I do believe that doing experiments on ImagetNet (like previous work does, such as IG) would be quite worthwhile and convincing. I suspect the computational expense could be mitigated by re using pretrained networks, of which there are many available for ImageNet specifically.
The paper presents hierarchical Bayesian methods for modelling the full covariance structure in cases where noise dimensions cannot be assumed independent.  This is an important problem with potential practical importance. The work is solid.  Conceptual novelty in the work is somewhat limited.  The method is applied in the paper on hierarchical linear regression. It is claimed to be applicable to other methods as well, and the claim is plausible, but to be fully convincing, results and comparisons would need to be shown. The new extended discussion does help somewhat.  There was also discussion about whether ICLR is the best match for this work. This is not a strereotypical ICLR paper though is relevant.  Authors are encouraged to continue this line of work. 
The overall impression on the paper is rather positive, however, even after rebuttal, it still seem that the paper requires further work and definitely a second review round before being ready for publication. Thus, I encourage the authors to continue with the work started during the rebuttal to address the reviewers  comment, which although moved in the right direction would still benefit from further work.  Especially, I believe the experiments could be significantly improved (by for example bringing some results to the main paper). Moreover, a more thorough comparison theoretically and empirically with previous work would increase the impact of the paper. 
The meta reviewer agrees with the reviewers that this is a marginal case. Conditioned on the quality of content and comparisons to other works: Constrained Reinforcement Learning With Learned Constraints (https://openreview.net/forum?id akgiLNAkC7P) Parrot: Data Driven Behavioral Priors for Reinforcement Learning (https://openreview.net/forum?id Ysuv WOFeKR) PERIL: Probabilistic Embeddings for hybrid Meta Reinforcement and Imitation Learning (https://openreview.net/forum?id BIIwfP55pp)  We believe that the paper is not ready for publication yet. We would strongly encourage the authors to use the reviewers  feedback to improve the paper and resubmit to one of the upcoming conferences. 
 This paper analyzes several neighbor embedding methods  t SNE, UMAP, and ForceAtlas2  by considering their objectives as consisting of attractive and repulsive terms. The main hypothesis is that stronger repulsive terms contribute towards learning discrete structures, while stronger attractive terms contribute towards learning continuous/manifold structures. The paper empirically explored the space parameterized by the relative weighting of the attractive and repulsive terms for the t SNE and UMAP algorithms, using several data sets, and qualitatively confirmed their conclusions about the impact of the attractive and repulsion terms as the relative weights vary.   The experimental validation of the paper s main hypothesis is thorough and the use of diverse data sets and neighbor embedding methods is appreciated  as the authors point out, several reviewers missed this contribution. However, several reviewers point out that the insight presented in the paper is already largely present in the literature, and that beyond its analysis the paper does not present new algorithms based on this insight. The authors rebut this claim by arguing that the novelty of the paper lies in it: (1) showing the contrary to the established opinion, UMAP works despite, instead of because, it uses cross entropy loss, and (2) the paper offers for the first time a theoretical understanding of why ForceAtlas2 highlights continuous developmental trajectories, and (3) prior work has not made the connection between UMAP, ForceAtlas2, and t SNE or suggested using exaggeration throughout the optimization process for t SNE rather than simply as a warm up. The paper does indeed present intuitions for (1) (3) based on the attraction repulsion ideas, and makes the connection between these neighbor embedding algorithms by viewing them as variations on the theme of attraction repulsion, but these intuitions are not significant steps forward with respect to what is already known about how neighbor embeddings balance attraction and repulsion. The mathematical analyses consist of stating the gradient for the algorithms and explaining how weighing the attraction and repulsion terms differently lead to different qualitative observations. The use of exaggeration throughout the optimization process is straightforward, and no strong mathematical characterization of the properties of the resulting algorithm is given.  It is recommended that this paper be rejected, as it consists of a thorough empirical validation of an understanding of the trade off between attractive and repulsive forces in neighbor embedding methods that was already present in the literature, along with some straightforward arguments connecting several popular neighbor embedding methods, but does not introduce any significantly new actionable insights or novel algorithms. 
This paper proposes to use RL to learn how to prune attention heads in BERT to achieve regularization for tasks with small dataset size. Specifically, the authors use DQN to learn a policy to prune heads layer by layer.    This paper receives 4 reject recommendations with an average score of 4.5. Though the idea in this paper is interesting, the experiments in the current draft are far from convincing. The reviewers have raised many concerns regarding the paper. (i) Experiments are weak. Only 4 GLUE tasks are considered; it is necessary to also test the proposed methods on other GLUE tasks. (ii) The comparison with other regularization techniques is lacking. (iii) The training overhead of this method needs more careful discussion, as it involves repeated finetuning after each layer is pruned, therefore could be very time consuming. (iv) More comprehensive related work discussion is needed.  The rebuttal unfortunately did not address the reviewers  main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
While the motivation of the paper is interesting the reviewers expressed concerns about the experimental setup, comparison to related work, and paper framing. For experiments, it was unclear why authors compared such disparate methods instead of more fine grained adjustments (e.g., such as corrupting graphs as suggested by R3). For comparison, other methods such as Deep Walk and VGAE (as suggested by R1) seemed missing. I think the biggest issue however was with framing: as the reviewers pointed out, it was not clear enough how looking at downstream performance relates to looking at the manifold. In fact the paper title is much too general and is also well known: manifold learning has been around for 15+ years. I would urge the authors to take the recommendations of reviewers and either design new experiments that explicitly target the manifold or reframe the paper to design new evaluation metrics for latent (possibly structured) generative models.
The paper proposes a user interaction framework where users choose a subset of LFs from a family of LFs generated using some template (e.g. keywords for text classification).   The proposed criteria is not very surprising, but the authors present a practical and useful system that is well demonstrated both in the paper and the very careful author feedback.  These enhancements have also been incorporated in the revised version.    Apart from the literature pointed by the reviewers, here are some more papers that are related to this paper: 1. Gregory Druck, Burr Settles, Andrew McCallum: Active Learning by Labeling Features. EMNLP 2009: 81 90  2. 	Gregory Druck, Gideon S. Mann, Andrew McCallum: Learning from labeled features using generalized expectation criteria. SIGIR 2008: 595 602   3. Data Programming using Continuous and Quality Guided Labeling Functions. In AAAI, 2020.
Fitting a neural net is a stochastic process, with many sources of stochasticity, including initialization, batch presentation, data augmentation, non deterministic low level operations and the non associativity of rounding errors in multi threads systems such as GPUs and TPUs. In this paper, the authors aim to alleviate this randomness by incorporating specific regularizers during learning or by using co distillation.  As the reviewers pointed out, the paper is quite clearly written, but the motivation for this work is not clear. The example of system updates does not correspond to the current study that targets the internal variability of the learning process. Reproducibility is an important issue, but in a statistical context, why would it be relevant to assess reproducibility by the individual decisions made by a single estimate? The usual way of assessing learning algorithms is to look at (a summary of) the distribution of performance for a given learning problem characterized by a data distribution, not to look at individual decisions made by a particular estimate. Furthermore, this study ignores the randomness due to the selection of hyper parameters. Why would the partial reproducibility studied here, for a fixed choice of hyper parameters, be of particular interest? As is, either the work is ill defined and incomplete, or it lacks a clear rationale, and I thus recommend rejection.   I would also like to point a reproducibility issue in the proposed experimental study. The exact meaning of the variability measures reported in the tables is not given, but I assume that it is the standard deviation of the different runs (for example, the 5 replicates in Table 1). These figures are not directly related to the variability of each setup, as they ignore the variability due to the random selections during the ablation study (for example, as I understand it, the last result of Table 1 was obtained for a single arbitrary initialization and a single arbitrary batch order).  
This paper proposes a novel way (Pani) that constructs image patch level graphs and then linearly interpolates the patch level features. The authors show how this can be used in Virtual Adversarial Training (PaniVAT) and Mixup/MixMatch (Pani Mixup). The method is shown to improve classification compared to standard VAT and related techniques on CIFAR 10 (low data setting), as well as outperform Mixup on CIFAR 10/CIFAR 100/TinyImageNet (standard setting, multiple different architectures) with and without data augmentation.  Reviewer 4 liked that the method was simple, but was not convinced of its effectiveness because of the baselines that were chosen. Specifically they thought that FixMatch was a stronger baseline than MixMatch. The authors said that Pani is complementary to FixMatch and similar improvement could be expected when applying Pani to FixMatch instead of MixMatch.  Reviewer 2 appreciated that the work was “important and interesting” and noted that the experiments showed that Pani improved existing algorithms. They were concerned with lack of motivation and lack of theoretical guarantees. The authors clarified motivation in their response to the reviewer but, understandably, were unable to provide any theoretical analysis.  Reviewer 1 expressed disappointment with the writing and understandability of the paper. I read the paper myself and I agree. They posed several clarifying questions to the authors, to which the authors responded. I note that the reviewer could not find the appendix, but it was attached separately as supplementary material.  Reviewer 3 wrote a very short review and stated that they are not familiar with the topic of the paper. With three other full reviews, I have discounted R3’s review because of their extremely low confidence. They also asked a couple of clarifying questions, to which the authors responded. I found the authors’ response satisfying.  Overall, two reviewers are not extremely excited about the paper and one reviewer thinks the work is interesting but has concerns about clarity. I think that overall it is a neat idea, but the paper could use more polishing and clarification. Compared to other borderline papers in my stack, it is not over the bar. It could get there with further work. I hope the authors continue to improve the paper and re submit it in the near future.
This paper proposes an efficient algorithm to obtain a node embedding based on its local PageRank scores. The proposed approach uses a hashing technique and a local partition approach to make the method more efficient and effective. However, the paper has significant drawback and can be further improved in the following aspects:  1. The experimental evaluation is weak and does not allow us to draw meaningful conclusions about the proposed algorithm.  2. The proposed algorithm does not show significant performance improvement on the link prediction task.
This paper presents an approach to tackle visual reasoning by combining MONET and transformers. All reviewers agree that there is some performance improvement shown. But there are several concerns including clarity/writing (multiple reviewers point it), experiments (baselines) and most importantly missing insights from experiments (why it works). While some of the concerns have been handled in rebuttal, the paper still falls short on primary concern of insights/why it works (which reviewers argue is critical for a paper on reasoning). AC agrees that the paper is not yet ready for publication.
the authors propose to use volume coding to enable uniform sampling from an implicit latent space to be used together with a autoregressive language model. all the reviewers find this approach interesting, but all found that the submission would be much stronger with more thorough evaluation. in particular, i noticed that the reviewers wanted to see how the proposed ariel works in comparison to e.g. VAE on a more diverse set of benchmarks, since the choice of two datasets, one synthetic and one small, narrow domain, is somewhat limited largely due to their relative simplicity. furthermore, the reviewers were unsure whether various evaluation metrics the authors have used are exhaustive nor appropriate to demonstrate the efficacy of Ariel or to put the proposed approach correctly in the context of other approaches. i agree with the reviewers on both of these points.  i m thus recommending this manuscript be rejected, and strongly recommend the authors give a bit more thoughts on how to demonstrate the effectiveness of the proposed approach in the context of other approaches and the problem of sentence generation (which is the main problem the authors claim to tackle, as the title directly suggests.) with a better planned experiment and analysis, i believe the authors  efforts will have significant impact.
The paper proposes a method for offline meta RL, where we meta train on pre collected offline data for several RL tasks and adapt to a new task with a small amount of data. The paper assumes that there is no interaction with the environment either during meta train or meta test.  In this setting, motivated by the ide of leveraging offline experience from multiple tasks to enable fast adaptation to new tasks, the paper introduces MACAW, which combines the consistent MAML and the popular offline AWR, improving upon them by adding capacity through parameterization and adding an extra objective in the policy update. As a result, the MACAW proposed for the offline meta RL has the desirable property of being consistent, i.e., converging to a good policy if enough time and data for the meta test task are given, regardless of meta training.    Pros:  + Most of the experiments are well executed, using good baselines. Extensive ablations on the various modifications to MAML+AWR confirmed the utility of the approach for the fully offline meta RL problem. + MACAW is a simple algorithm with theoretical guarantees; the modifications to the policy functions are backed by theory.    Cons:   The reviewers have concerns on the formulation of offline meta RL. One major contribution of the paper is to introduce offline meta RL. However the paper largely borrows the meta RL formulation from the online setting where task MDP. The reviewers think that directly borrowing from regular meta RL as the formulation of offline meta RL might be misleading. The reviewers suggest including behavior policy as part of the task definition for offline meta RL formulation.    Several reviewers raised concerns that the fully offline setting might be unrealistic. Although the author did add a motivation, the reviewers would be interested in seeing MACAW being adapted online at test time on in distribution tasks.     Unfortunately, the authors accidentally revealed their names in one of the modified versions.  
This paper proposed a regularization term to control the bit width and encourage the DNN weights moving to the quantization intervals. The paper is well written and the idea of using the sinusoidal period as a continuous representation is novel. However, the theoretical analysis provided are not consistent with the proposed method.  As for the experimental results, the proposed method incurs significant degradation as compared to the baseline, and comparison with recent quantization methods is lacking.
The work introduces a method that uses the Feature Statistics Alignment paradigm to improve sequence generation with GANs. The contribution is interesting and novel (although marginally), clarity is also good. However the reviewers raised several concerns calling for more comprehensive and thorough evaluation. Experiments show an improvement comparing to selected baselines and the revised paper addressed, at least partially, a serious evaluation concern of one reviewer. Although the excellent revision work some important open questions still seem to remain, in particular the choose of alignment metrics and a thorough evaluation. 
This paper focuses on the limitations of the transformer architecture as an autoregressive model. The paper is relatively easy to follow. Though most reviewers find the paper interesting, the idea is not very novel. The introduction of sequential ness to Transformer is good, though it also slow things down especially as the sequence gets longer.  An extensive set of experiments are performed, though the results are not entirely convincing. The authors are encouraged to add more ablative experiments, efficiency analysis, and large scale results.
The paper studies the problem of certified adversarial robustness when the classifier has a reject option (realized here as an additional class) where the certification is done by adapting IBP techniques to this particular problem setting.  Pro: As previous approaches to adversarial robustness with reject option have often be shown to be non robust as one could circumvent both detector and classifier simultaneously with an adaptive attack, the idea to do this instead with a certified approach is interesting and could potentially initiate more research in this direction.  Con:    The approach seems to some extent trade off robust error with normal error which seems in particular true for MNIST where a stronger loss in normal error is less acceptable.   Comments:   One reviewer mentioned that such a certified approach could be interesting for OOD detection. This has been done recently in: Julian Bitterwolf, Alexander Meinke, Matthias Hein Certifiably Adversarially Robust Detection of Out of Distribution Data, NeurIPS 2020 which should be cited in the present paper.   While the empirical PGD attacks done in this paper are strong, sometimes PGD fails due to gradient obfuscation, I would thus recommend to use additionally a black box attack or the recent AutoAttack. 
This work proposes a stochastic process variant that extends existing work on neural ODEs. The resulting method allows for a fast data adaptive method that can work well fit to sparser time series settings, without retraining. The methodology is backed up empirically, and after the response period, the reviewers  concerns are sufficiently addressed and reviewers are in agreement that the contributions are clear and correct.
Although all reviewers agree that the work is interesting and has potential, several issues in the presentation and the experimental section (especially regarding the ablation) need to be worked on before granting acceptance to the paper. 
The paper presents  a new variant of the Stochastic Heavy Ball method with coordinate wise stepsizes. They prove a regret upper bound in the online convex optimization setting and validate the algorithm on few deep learning tasks.  The reviewers found the paper severely lacking on many aspects. In particular, the formulation appears not motivated at all, the regret upper bounds relies on an unverified assumption of boundedness of the iterates, the momentum parameter must decrease exponentially over time. Note that it is known how to analyze the momentum algorithm under much more general conditions. The empirical evaluation was also judged not sufficient, with only 2 datasets (one of them being MNIST).  Overall, the paper was judged not suited for publication at ICLR.
Overall, this seems like a neat idea and well done work. Main principle is to extract a very sparse net that does a good job at locally "explaining" a given example. The NeuroChains idea does this with a diffentiable sparse objective. I think this work is well positioned and has nice properties: (1) retains a very small percentage of "filters", (2) it appears that all the selected filters are actually needed/useful (3) there are some generalization properties wrt to unseen samples that are close to the sample of interest.  I appreciate that the authors responded with very detailed rebuttals to the concerns of the reviewers. I m still worried, like AnonReviewer4, about the generalization around local regions though the follow up experiments satisfy me for the most part. There is a genuine concern that while this method has the *potential* to produce useful outputs that could be useful for downstream experts to analyze the underlying network, the paper itself doesn t really show this. In other words, while I agree that on the technical side of things, the work passes the bar, it s not clear that the work passes the bar from the impact side of things.  This did make for a genuinely a borderline case in terms of decisions and unfortunately this work landed on the reject side this time around.
Overall, all reviewers generally agree that the idea of using visual similarity to unsupervised alignment of multiple languages is interesting and the proposed method and dataset are well designed, while three of them raised some concerns related to the retrieval nature of the method. In particular,  discussions about its place as a study of machine translation and comparison with other cross lingual retrieval baselines were the main issues. Although authors made great effort to address reviewers  concerns points and did clarify some of them, unfortunately the reviewers were not fully convinced by the response, and one reviewer decided to downgrade the initial score.  After all, three reviewers rate the paper as  below the acceptance threshold . Based on their opinions, I decided to recommend rejection.  I think the entire picture of the work and the logic flow could be much clearer by discussing in a top down manner why this idea should be implemented with a retrieval based approach, rather than superficially adding "using retrieval" to some sentences.  
This paper proposes a method for learning physics combining symbolic computation and learning in an interesting way, targeting sample efficiency. At the initial evaluation, it was on the fence but leaning towards acceptance, with 3 slightly positive and one slightly negative review.   The strengths lie in the combination between symbolic reasoning and statistical ML with a formulation around the classical EM framework. On the other hand, an important issue of the paper is its quite simplistic evaluation on now very easy problems and benchmarks. While benchmarks tend to be simple in the field of learning physics, current work does address more difficult problems than the problems tackled in this paper.  Another issue discussed was the simple trade off in injecting hand crafted inductive bias into a system leading to increased sample efficiency, which was perceived as unsurprising by some reviewers. While this is common in ML, and even strongly more so in learning physics from data synthetically generated with known physical laws, it was perceived to be particularly unsurprising in this paper where the benchmarks are indeed very simple and the laws directly encoded.  The AC discussed this paper with the PCs, and it was judged that the weaknesses in evaluation, in particular the simplicity of the tasks, cannot compensate for the interesting hybrid symbolic/ML formulation, and decided to reject the paper.
The reviewers pointed out several opportunities for improvements and concurred that the paper needs significant work before it is ready for publication.  The authors did not provide a rebuttal. We hope the review process was useful to the authors. 
The reviewers agree that the submitted paper is of high quality and provides a promising approach/framework for Bayesian IRL. Certain concerns regarding details of the implementation and evaluation have already been addressed by the authors during the rebuttal phase, and also the title of the paper was adjusted in line with discussions with the reviewers. For the final paper, the authors should make sure to clearly highlight the advances of inferring a distribution over rewards (this is already partly done by the added grid world experiments) and discuss relations to VAEs as the initially had in mind and even in the paper title. Beyond that, the should of course also address other reviewers’ comments.
The authors present a study on what maintains the stability of emerged communication protocols. To study this question the authors design experiments in bargaining communities of agents in 3 setups,  a) no punishment of restriction of liar agents b) allowing individual agents to refuse bargaining with  liar agents and c) introducing a global punishment system for liar agents.  Overall the reviewers agree that the design of the study is interesting, but also point that motivation and take home messages of this study are unclear. Having read the paper, I share the same opinion. The authors discuss on a very abstract level about the implications of this study for the field of AI, but this study is quite specific and clearly does not capture all the complexities or real societies. From the scale of results and study, I think it would be more valuable to draw some concrete proposals/implications about perhaps multi agent modelling or environment design in general.   All in all, this is an interesting study but some more work needs to be done around research framing. 
The paper is presenting an important empirical finding. When the learning algorithms are initialized with the same point, the continual and multitask solutions are connected by linear and low error paths. Motivated by this finding, the paper proposes a new continual learning algorithm based on path regularization. The paper received unanimously good scores. I agree with the reviews and recommend acceptance. 
This paper proposes a method for offline reinforcement learning methods with model based policy optimization where they first learn a model of the environment to learn the transition dynamics, a critic and the policy in an offline manner. They basically learn the model by training an ensemble of probabilistic dynamics models represented by neural networks that output a diagonal Gaussian distribution over the next state and reward. Then they use the covariance of the probabilistic dynamics model to get an uncertainty measure that they incorporate into the  reward when training it with the AWAC.  There were two main concerns raised by the reviewers:  1) Experiments: As pointed out by the reviewers, the experimental gains don t look very convincing. In particular, the performance of AWAC looks bad, and MB2PO doesn t give much gains on top of it. It is not clear how much better the proposed method is doing on the tasks that it does well, without any confidence intervals or variance measures provided.  2) Novelty:  This is almost a trivial combination of two existing ideas: model based policy optimization and AWAC. It is not clear how useful this particular combination is in practice, and it seems like there is not much insights gained from it.   I think better motivations, further ablations and more empirical analysis to understand the proposed model better. For example, analyzing the types of behaviors learned or how calibrated the uncertainty estimates that is incorporated into the reward is or some hyperparameter sensitivity analysis would make the paper more interesting.  As it stands right now, I am suggesting to reject this paper. I hope the authors will improve the paper for the future...  
This paper studies the role of “noise injection” in GANs with tools from Riemannian geometry, and derives a new noise injection approach that aims to learn a fuzzy coordinate system to model non Euclidean geometry. The new noise injection approach is shown to improve over StyleGANv2 noise injection on lower resolution 128x128 FFHQ, LSUN, and 32x32 CIFAR 10 images.    Some reviewers found the experimental results a “considerable improvement on DCGAN and StyleGANv2” (R3), “extensive and convincing” (R2), while others had concerns around the experimental setup using lower resolution images (R1, R4).  While reviewers were mostly positive about the experimental wins of the paper, there was confusion (R3) and several concerns (R4) around the theory and the relationship between the theory and the practical noise injection algorithm. I additionally had several concerns around the presentation and relation to prior work on generative models. Thus in the current state I cannot recommend this paper for acceptance. Below I highlight concerns that should be addressed in future revisions.   1. My biggest concern is the tremendous gap between the theoretical claims and the practical implementation. When training a GAN with the new form of noise injection, does it learn the skeleton and fuzzy equivalence relationships you claim? This paper is missing any kind of toy experimenting showing that training a GAN with fuzzy reparameterization discovers these relationships or coordinates. Such an experiment would greatly strengthen the paper and help to answer the question of why this new method works (i.e. it’s not just more parameters, a slightly better architecture, or better hyperprameters as mentioned by R3 and R4). There’s also no discussion of what happens theoretically when you have multiple layers of fuzzy reparameterization, and the claims that StyleGAN2’s noise injection limits to Euclidean geometry is false in this case (and thus StyleGAN2’s noise injection can also overcome the “adversarial dimension trap”).   2. Theoretical setting: As mentioned by R4, there is much prior work on the difficulties in fitting a lower dimensional model manifold to a higher dimensional data manifold (e.g. WGAN). Theorem 1 highlights the impossibility of exactly fitting the data manifold with (smooth) neural networks, but the resulting solutions of increasing the dimensionality of the latent space is well known and commonly used (e.g. StyleGAN). This paper also doesn’t discuss the alternative of *approximately* fitting the data manifold with a lower dimensional structure, which is what is often studied in practice.    3. Clarity: The term “noise injection” is overloaded in the literature, and the current presentation of the paper does not sufficiently describe the method. There’s also no discussion of “instance noise” that is another solution to this problem that adds noise to inputs of the discriminator to yield finite f divergences (Sonderby et al., 2016, Roth et al., 2017). The work on instance noise is very related to the approach here, but only adds noise to the output of the generator, not at all levels.  There s also no discussion of how adding noise is just expanding the generative model with additional latent variables, a standard approach that is often discussed in the context of hierarchical generative models. The authors mention the relation to reparameterization trick in VAEs, but argue it is doing something fundamentally different. However, modern VAE architectures (IAF VAE, Very Deep VAE), use a very similar form of modulation at multiple levels in the hierarchy.    4. Experiments: There are no error bars in experimental results, and many results are presented in a new experimental setting defined by the authors (lower resolution than prior work even if using prior code). Rerunning experiments in more standard settings on full resolution images would greatly improve the confidence that the new noise injection strategy is effective. 
This paper represents a practical extension of sound theoretical uncertainty propagation ideas for exploration in deepRL. All the reviewers agreed this was a promising direction and the empirical results strong. It was nice to see additional qualitative analysis of the proposed method, beyond the typical "my number is bigger than yours" type of claims. The discussion was extensive; reviewers with specific subject matter expertise provided high quality and detailed reviews.  Several reviewers were in favour of the paper, but none were willing to champion it as a clear accept.   Indeed the discussion highlighted important concerns with the paper. Several reviewers found the paper to overclaim: most importantly the paper suggests strong theoretical underpinnings of the method without clear evidence. Some found the text very imprecise. The reviewers were torn if such changes represented wording changes or major rewrites. The original submission missed two key pieces of work which were added during the rebuttal phase UBE was added by including the scores from the literature, and the other (Bayesian DQN) was only added to the discussion. Good on the authors for doing so, though ideally both would be implemented again.   The AC s own reading of the paper highlighted a few other concerns. The writing needs work. In addition, the majority of improvement in overall performance appears to be due to very large improvements in a handful of games (e.g. Atlantis, Krull) and significant losses in performance in other games. This was not discussed at all. More surprisingly these games with huge performance gains were not used in the qualitative visualizations of the utility of the bonus found in the paper. The game breakout was used for analysis instead. Oddly the proposed method actually does worse or the same as SOTA methods (e.g., Adaptive EBU, Boot DQN, UBE) in breakout.  This is difficult to get to the bottom of because: (a) the per game score tables in the appendix don t include the scores achieved by important baselines (e.g., Boot DQN, EBU, Adapt EBU), (b) different setups are used across the relevant literature (Boot Q & UBE papers use 200m frames, EBU paper uses 10m frames, and this paper uses 20m), (c) the per game analysis in the appendix focuses on comparing methods proposed in the paper under review. That might all make sense, but it is left to the reader to figure out and I never got to the bottom of it all (table 3 of the Lee et al contains some of the relevant comparison data). Such missing details and lack of analysis are particularly important when the paper boldly claims state of the art performance improvement.  All put together a clear picture emerges: the paper needs polishing, is unclear in places, over claiming, and missing important analysis and explanations in regards to both the theory and the experiments. The reviews are extensive and have provided many insights in how to improve the paper. 
The paper presents a construction for deep learning on point clouds that evolve over time. The key characteristics of the data are irregular sampling in the spatial domain and regular sampling in the temporal domain. The presented construction addresses both these aspects of the data. The review by R3 was negative but was addressed by the authors and R3 did not participate in the discussion. The AC supports acceptance.
Four knowledgeable referees support acceptance for the contributions, and I also recommend acceptance. There is agreement among all reviewers that this paper is about  a highly relevant topic, that the model presented is technically sound and has significantly novel aspects, and that the experimental results are convincing. There were several points of criticism raised by the reviewers, concerning, for instance, further comparison experiments,  the heuristic nature of masking rules, or the treatment of homologous sequences. In my opinion, however, most of these points have been addressed in a rather convincing way during the rebuttal phase.  
This paper studies the low rank properties of DAG models, and illustrates through proof of concept how low rank ness can be exploited in structure learning of DAGs. After a lengthy discussion amongst the reviewers, it became clear that although there are some interesting ideas here, there is not enough enthusiasm for this work in its current form. The results in Section 4 connecting rank to structural properties are interesting, but the reviewers were concerned by the lack of precise statements connecting these results to known ensembles such as scale free graphs (even though the authors discuss some heuristic connections). In the end, despite considerable enthusiasm regarding these ideas and the importance of the problem studied, there remained too many concerns that require a major revision before acceptance.
Three reviewers agree on the value of the contribution and recommend acceptance. A reviewer votes for rejection but the authors have clarified all the major concerns raised by the reviewer. Therefore, I recommend acceptance. 
This paper was reviewed by 5 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The paper proposes a new meta learning algorithm which promises greater robustness to adversarial examples. I will be brief, as the fault with the paper is quite clear: the experimental results are not sufficient. The attack used (FGSM) is particularly dated and weak, and the comparison to existing defences is insufficient. Additionally, prior work (Adversarially Robust Few Shot Learning: A Meta Learning Approach) obtains better results, and is not compared against. The reviewers provided further criticism regarding the motivation for and explanation of the method, but the empirical aspects of the paper are where it primarily falls short of the publishable standard for ICLR.  I recommend rejection, and invite the authors to consider demonstrating robustness to a wider range of attacks (including non gradient based), and a more thorough comparison to defence methods, before resubmitting to another conference.
Well, this paper has achieved something remarkable in this review process:  The initial scores came in at fairly low scores (4, 5, 3, 6).  However, as the discussions / rebuttals went back and forth, the reviewers were able understand and see the merits of the proposed methodology.  Namely, the setting of L2E (Learning to Exploit), which makes use of a novel method called Opponent Strategy Generation, to quickly generate very different types of opponents to play against.  One more pertinent component is the use of MMD (maximum mean discrepancy regularization) which can remove the necessity of dealing with task distributions, and does a better job in creating diverse opponents.  Having understood the technical approach, three of the reviewers decided to substantially increase their scores. R4 increased 4 >6, R5 increased 5 >6, R3 increased 3 >4, while R2 held steady with a score of 6. It was also good to see empirical favorable results compared to other baseline methods: L2E had the best return against unclear opponents, such as Rocks opponent and Nash opponent.  Without any reviewer arguing strongly for acceptance, the program committee decided that the paper in its current form does not quite meet the bar, and also that it would benefit from another revision. 
The paper leverages concepts coming from hindsight relabelling methods to define a novel "iterated" supervised learning procedure to learn policies to reach different goals. The algorithmic solution is well supported in terms of intuition, preliminary theoretical guarantees, as well as strong empirical validation.   There is a general consensus among the reviewers that this is a strong submission and the rebuttal helped in clarifying some aspects of the paper (e.g., the comparison with Go Explore) and reinforced the empirical analysis. This is a clear accept.
The paper provides a method for constructing PAC confidence scores for pre trained deep learning classifiers. The reviewers were all positive about the paper.  Pros:   Has provable guarantees on the reliability of the prediction. Such guarantees are quite desirable in practice.   The problem of neural network uncertainty is important and timely problem, especially in safety critical applications.   The method is simple and well motivated.   Strong empirical performance.   Interesting applications to fast DNN inference and safe planning.  Cons:   Lack of generalization guarantees  the guarantees in the paper only hold on the training set; but in practice, performance in test is what s important.   Only a handful of baselines tested against, most of which (if not all) were naive.
The proposed approach seems to have elements of novelty, it is well presented  and  reasonably motivated by the authors. In addition, empirical results seem to be promising. However, although rebuttal helped to clarify some of the pending issues, there are concerns on the fact that the raised issue about "resolution dilemmas" does not find in the paper a quantitative treatment. Without that, it is difficult to fully understand how to drive the learning of useful structural landmarks. Thus, notwithstanding the paper seems to contribute in a significant way to the advancement of the GNN field, it still needs additional work to better develop  the proposed concepts in a quantitative theory. 
The paper presents a novel framework from transfer learning over GNNs. Experiments ought to better substantiate how structural differences/similarities are measured, as well as relying on prior art to measure transferability success. A plan for incorporating (structural) features would also strengthen the present work.
This paper proposes a framework for artificial life. In the framework, there is no primitive agent construct, but rather a set of basic recurrent network components (such as linear algebra operations). The framework is open ended and objective less. The authors describe the emergence of different organisms out of these building blocks, illustrating the idea with a simplified implementation.  The paper is extremely original, at least in the context of the deep learning/ICLR community. At the same time, together with a majority of reviewers (mostly experts in relevant topics such as neuro evolutionary methods and artificial life), I felt that this work is not ready to be presented at a major conference, for three main reasons. 1) It should make its links to the huge literature on artificial life outside the ICLR community clearer. 2) The empirical work is somewhat limited and 3) this is not counterbalanced by a clear theoretical roadmap.  I thus do not recommend acceptance, although I really hope the authors will present a more thoroughly worked out version of the paper soon. 
This paper presents a theoretical analysis of CNN compression using tensor methods. None of the three reviewers have strong opinion; there scores are 5, 6, and 5.  The attempt to understand the mechanism of how tensor decomposition compresses CNNs is meaningful and interesting. However, the main contribution of this work is not sufficiently distinct compared to the existing approaches and the analysis and proof is conduected only for simplified models as mentioned by reviewers. The practical benefit of this paper is not clear and the experimental validation is weak because only a small number of model architectures were tested on a few small datasets.  This is a borderline paper. However, this paper needs to extend its contribution by performing more comprehensive analysis for general CNNs. 
This paper was reviewed by four experts in the field. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes.  It is also very important to think about how to extend this framework to the more challenging CLEVERER dataset (http://clevrer.csail.mit.edu/). 
The paper argues that a successful backdoor attack on classifiers is connected with further fundamental security issues. In particular they demonstrate and not only an original backdoor trigger but also other triggers can be inserted by anyone with access to the classifiers. Furthermore, the alternative triggers may appear very different from the original triggers, which confirms the claim in the paper s title that such classifiers are "fundamentally broken".  The paper offers an interesting insight into the features of poisoned classifiers. However, such insight is diminished by the fact that the proposed attack requires a substantial manual interaction. The user must manually analyze the adversarial examples generated for robustified classifiers in order to determine the key parameters of alternative triggers. While manual intervention as such does not undermine the main observation of the paper, this makes an automatic exploitation of this idea hardly feasible and hence decreases the significance of the paper s main result. 
This work seeks to describe the heavy tail phenomenon observed for deep networks learned with SGD. The work presents proof of a relationship between curvature, step size, batch size, and a heavy tail weight distribution. The proofs assume a quadratic optimization problem and the authors speculate that the results may also be relevant for non convex deep learning settings. On the positive side the reviewers agreed that this work is one of the first, if not the first, to try to theoretically describe a poorly understood phenomenon in deep learning. On the less positive side, the reviewers believe that the proofs developed in this paper are for an idealized setting that is too different from the settings under which deep models are trained. As such, even though the authors provide some (somewhat mixed) experimental results to support the claim of relevance to deep learning, the reviewers were not convinced. Given that the stated goal of the work is to attempt to explain this phenomenon in deep models, the majority view is that this work, while promising, needs further development to convincingly claim some relevance to the original phenomenon being studied.
Some reviewers expressed concerns on soundness of the theory in the paper. Specifically, theorem 3 does not seem to be correct.  There are other concerns such as the significance of the theoretical contributions, little empirical value and existence of much stronger results. Unfortunately the authors did not provide responses to the concerns raised by the reviewers.   
This paper is not suitable for publication at ICLR. The paper contains a useful message, that neural networks are not a silver bullet, and are especially not well suited to deductive problems. However, as several reviewers pointed out, the claims of the paper are undermined by the fact that it ignores a lot of relevant work on using neural networks in the context of logic reasoning. Reviewer 2 provides a particularly useful list of relevant works on the topic. 
The paper is somewhat borderline, though reviews mostly lean positive. Unfortunately after calibrating compared to other submissions, the work remains somewhat below the bar compared to higher scoring papers.  The reviewers praise the topic, the method, and the experiments (although some of this praise is a little mixed or lukewarm). The most negative review raises several specific concerns about the evaluation methodology, as well as some concerns about data leaks etc. While serious, the authors rebuttal to these claims seems reasonably convincing. While the remaining issues appear not to be dealbreakers, there are nevertheless some lingering concerns which ultimately put the paper slightly below the bar.  The AC notes that their initial inclination was to accept this paper, though it was suggested that the score be lowered after calibration compared to other submissions, mainly due to doubt regarding these lingering issues.
The paper attempts at providing a general benchmark for evaluating/analysis of long range transformer models, consisting of a 6 evaluation tasks. The main goal of the paper is to remove conflating factors such as pretraining from model performance and keeping the benchmark accessible. All reviewers agreed that these are important positive aspects of the paper and the presented analysis/results are useful.    While reviewers generally feel positive about the work, there are some critical concerns on how useful this benchmark is in practice, how generalizable are the results, and whether the benchmark is good at what is intended for. For example, the vanilla Transformer model performs very well on all the proposed tasks, making me question on what we can actually learn about long range dependencies through this benchmark. In addition, most tasks are synthetic and all models fail on 1 of the 6 proposed tasks.   Therefore, I think LRA should be viewed more as a tool for analysis, or as authors nicely put in their response, it should be viewed as a means to "encourage hypothesis driven research instead of hillclimbing or SOTA chasing.".  During discussion period with reviewers, while acknowledging the above mentioned issues, this strength was highlighted as a valuable contribution. Therefore, given the general positive sentiment about the work, I d recommend accept.
This work proposes a method for identifying appropriate graphical models through enumeration, pruning of redundant dependencies, and neural network conditionals. While structure learning is an interesting application and there are some promising results, there were a number of concerns around experimental evaluation, computational complexity of the method, clarity of the presentation, and connections to prior work. In particular, R1 s concerns around the large field of structure learning in Bayesian Networks, and unwillingness to use the established terminology (and comparing to methods there) was not sufficiently addressed in the rebuttal.
The paper addresses a well motivated problem of evaluating the accuracy of a black box classifier A(x) using actively selected set of labeled examples.  They predict two additional classifiers   one to predict if an example will be corrected by A(x) and the second a Bayesian NN to assign a distribution over likely labels of unlabeled examples.  The final accuracy estimate is obtained   from the labeled data and the probabilistic labels on the unlabeled data. Reviewers rightly pointed out that the paper only compares with conventional active learning methods, and skips comparison with methods specifically designed for active evaluation.  A list is attached below.  Also, the overall technical contribution seems limited in terms of both empirical accuracy gains it obtains and novelty of ideas exposed.  Related papers: A lazy man s approach to benchmarking: Semisupervised classifier evaluation and recalibration P Welinder, M Welling, P Perona   Proceedings of the IEEE …, 2013   cv foundation.org  Active evaluation of classifiers on large datasets N Katariya, A Iyer, S Sarawagi   2012  Adaptive Stratified Sampling for Precision Recall Estimation. A Sabharwal, Y Xue   UAI, 2018  Online Active Model Selection for Pre trained Classifiers Mohammad Reza Karimi, Nezihe Merve Gürel, Bojan Karlaš, Johannes Rausch, Ce Zhang, Andreas Krause  Towards Efficient Evaluation of Risk via Herding Z Xu, T Yu, S Sra
The authors design a framework to estimate the uncertainties in the predictions of gradient boosting models, for both classification and regression. The framework contains several methods, some that use sub sampling on data to calculate the estimation, and some that use sub sampling on the trees within one single gradient boosting model (i.e. virtual ensemble) to calculate the estimation. The different methods reveal the trade off between faster calculation and good uncertainty estimation. The authors conduct extensive empirical study to demonstrate the validity of the designed framework.  The reviewers agree that the paper is well written on a very important topic of machine learning in practice. The authors have done a great job addressing the comments from the reviewers, including the comparison to random forest, and adding more motivating examples. The reviewers believe that the work marks a good starting point for addressing this important topic. Nevertheless, the reviewers have some concerns that the results are promising but not impressive yet, and the performance of the virtual ensemble is a bit discouraging.  
Reviewer #2 has written a nice summary of the paper which I quote below.  “The core idea is simple   which is a strength in my view   and does not require retraining the base language model, which could be important as language models become more expensive to train. However, the clarity and experiments in this paper fall short: the experimental setup has issues, the effect on perplexity is quite large but relegated to the Appendix, several claims are speculative and lacking corresponding experimental evidence, and it is unclear how the additional heuristics affect performance.  The method seems promising, but with the current experiments it is difficult to draw conclusions about how the method affects performance and which parts of it are necessary; given that this is an empirical paper, I would therefore not recommend acceptance in its current form.”  Key Strengths + Well motivated problem of considerable interest  + A relatively straightforward Bayesian solution + Proposed solution is computationally efficient compared to other competing approaches.  The paper has been thoroughly reviewed by the reviewers and as a result numerous questions has surfaced. While the authors addressed most of the questions adequately, there are still many unanswered questions. They include:   Readability issues highlighted by Reviewer #1   Reviewer #1: “"how did you measure model confidence about the toxicity label"   Reviewer #4: The perplexity gets much worse as the gedi training is introduced (i.e. Λ decreases), e.g. going from 25 to 45 on IMDb. This result is in the Appendix, and perplexity is never evaluated/reported in the other experiments.   Reviewer #4: Crucially, the GEDI training does not appear to help over just re weighting with the conditional LM ( vs. ). Could the authors comment on this result? How well does domain transfer work for less similar domains? How is perplexity affected for the models reported in Table 2?   Reviewer #4: How small can the conditional LM be? Why was medium used instead of small? What if large was used? Does the conditional LM need to be a large scale pretrained model (it would be nice to see a baseline of a simpler conditional LM)? Several heuristics are used:  weighting, nucleus filtering, keeping tokens over a threshold, repetition penalty, and rescaling the logits to positive (used in only one experiment).   How does each of these affect performance? There are no ablations, and given the small differences in some of the experiments it is unclear whether performance would actually be worse if we changed one of the heuristics. One outcome may be that the method only works for a careful balance of hyperparameters, which could be fine, but we don t have a sense of the variation.   Reviewer #2: The output in Table 6 makes me doubt how the experiments are badly controlled. The outputs from positive and negative sentiment are totally different and almost random text, meaning that the content of the generators is not controlled properly. 
This paper proposes a new RNN architecture called Dynamic RNN which is based on dynamic system identification.  Reviewers questioned the expressivity of the proposed model, practical application/impact of the proposed model, and interpretability of the proposed model. Even though the authors attempted to convince the reviewers, 3 out of 4 reviewers think that this work is not ready for publication.   Specifically, R4 recommends 5 ways to strengthen the paper. I recommend the authors to incorporate this feedback and make a stronger resubmission in the future.
This paper proposes a GNN that uses global attention based on graph wavelet transform for more flexible and data dependent GNN feature aggregation without the assumption of local homophily.  Three reviewers gave conflicting opinions on this paper. The reviewer claiming rejection questioned the novelty of the paper and the complexity of the global attention mentioned in the paper. Even through the authors  responses and subsequent private discussions, concerns about complexity and novelty were not completely resolved.   Considering the authors  claim that the core contribution of this paper is to design fully learnable spectral filters without compromising computational efficiency, it is necessary to consider why it is meaningful to perform global attention based on graph wavelet transform in the first place. In terms of complexity, although the wavelet coefficient can be efficiently calculated using the Chebyshev polynomials mentioned by the authors, in the attention sparsification part, n log n is required **for each node** in sorting, resulting in complexity of n^2 or more. There may still be an advantage of complexity over using global attention in a message passing architecture, but it will be necessary to clarify and verify that, given that the proposed method uses an approximation that limits global attention within K hops.  Also, this paper modifies the graph wavelet transform in graph theory, which requires a deeper discussion. For example, as the authors mentioned, the original wavelet coefficient psi_uv can be interpreted as the amount of energy that node v has received from node u in its local neighborhood. The psi_uv defined by the learnable filter as shown in Equation 3 has a different meaning from the original wavelet coefficient. There is insufficient insight as to whether it is justifiable to use this value as an attention coefficient.  Overall, the paper proposes potentially interesting ideas, but it seems to require further development for publication.
This paper was reviewed by 4 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere. 
This paper has received four positive reviews. The main intellectual contribution of the paper is the introduction of a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning across neurons and even across animals. The reviewers commented on the technical strength of the paper. At the same time, the main contribution remains relatively incremental from a technical standpoint, and while the approach may be of value to future work, the impact of the current study on neuroscience (which is the target here) is quite limited. Nonetheless, there seems to be sufficient enthusiasm from the reviewers to recommend this paper be accepted.
The reviewers’ main concern was a lack of experiments, and additional experiments were provided by the authors.  While the rebuttal was not addressed by the reviewers, the AC feels that the rebuttal did address a number of experimental concerns well enough to justify accepting this paper.
The topic covered by the paper is timely, and the way the authors have addressed the problem seems correct. The provided empirical evidence seems to be sufficient to support the main claim of the paper. Presentation is well structured and clear. Notwithstanding the above merits, the proposed approach seems to confirm other similar proposals presented in the literature, so the contribution of the paper seems to be limited. Although presentation is good, it is not highlighting enough the differences w.r.t. those proposals and the basic approximation result given by Chebyshev polynomials. Especially a better theoretical characterisation w.r.t. to approximation capabilities by Chebyshev polynomials (with no truncation) would have helped to better gain understanding of the merits of the proposed approach. Finally, some of the experimental results do not seem to have a significant  statistical difference w.r.t to the baselines, so it would have helped to have the result of a statistical test.
This paper provides an interesting analysis on the research on Domain Generalization with main principles and limitations. The authors provide a strong rebuttal to address some comments pointed by reviewers. All the reviews are very positive. Hence, I recommend acceptance.
I thank the authors for their submission and very active participation in the author response period. The paper is well motivated, clearly written and demonstrates empirical gains. In discussions, R4 and R5 were championing the paper. R1 stated that the paper improved, but insists that claims of state of the art, given the simplifications induced by a deterministic simulator and hard valid action constraint, are not justified without additional baselines to compare to. The authors have toned down state of the art claims in the revised version of the paper. I agree with R4 that the added requirements in Table 1 sufficiently explain the constraints under which MC LAVE can be applied. Given the strong positive sentiment by R4 and R5, the positive assessment by R3, and the detailed response by the authors, I am recommending acceptance of the paper.
The paper has some interesting points in extending IRM to regret minimization, and extending to structured environments.  I can see the writing has been improved in the revision.  The main criticism arises from the experiment, which can be improved in several aspects.   The reviews have been quite detailed and helpful.
The authors propose a framework which combines pretext tasks and data augmentation schemes with the goal of improving robustness of image representations. The authors show that this approach empirically can lead to increased accuracy on both corrupted and uncorrupted data simultaneously. Furthermore, the authors propose a regularization procedure which can be used to maintain a robust representation during transfer to arbitrary downstream tasks.   The studied problem is significant and highly relevant to the ICLR community. The reviewers agreed that the work is timely, and appreciated the clarity of exposition. At the same time, the reviewers remained in disagreement in terms of novelty and significance of the results   the proposed method is seen as a clear incremental application of existing techniques in the self supervised setting. The authors argue that it was not clear that such augmentations would improve the robustness as well as accuracy, but these methods were developed and optimised to improve robustness. In fact, in [1] the authors conclude that “...today s supervised and self supervised training objectives end up being surprisingly similar” as well as point out that SimCLR is more robust than competing self supervised methods. Hence, establishing that there is indeed some empirical benefit is a step in the right direction, but not sufficient to meet the bar of acceptance. Furthermore, given the recent trend of scaling up existing approaches, in particular in terms of the neural architectures and the batch sizes, the computational costs of the proposed regulariser in Eq (4) coupled with the additional hyperparameter to optimize make the approach less practical and general than claimed. In addition, the reviewers pointed out the lack of comparison to a proper baseline, as well as the issue with hyperparameter selection for the baseline after the author response. Finally, given access to additional data augmentations and one more hyperparameter to tune the results should substantially outperform the baselines.  For the reasons outlined above and the incremental nature of the work, I will recommend rejection. That being said, this was a borderline case, and I urge the authors to carefully revise the manuscript with the received feedback.  [1] https://arxiv.org/abs/2010.08377 
This paper proposes  a general manipulation algorithm for tasks that have sparse rewards. The algorithm uses a Q attention to extract interesting pixel locations with an explicit attention module. A data augmentation method is also proposed to generalize expert demonstrations.  While the proposed method and experiments seem interesting, two out of the three reviewers had several issues regarding the clarity of the paper. The main issue is that a better ablation study needs to be performed to assess the proposed system. For instance, the reviewers question the advantages the Q attention agent brings over a standard attention module. The authors provided detailed explanations and a video showing examples of the expert demonstrations. It is not clear however if the clarity issues are completely resolved. 
The paper proposes how weight encoded neural implicit can be strong 3D shape representations. A neural network is trained such that it overfits over a single shape, and the weights of such network is a great representation for the 3D shape. Results are shown on signed distance field (SDF) generation from meshes.  Strengths:   an interesting idea for generating compact representations of 3D shapes   Will further foster several conversations within the deep learning community  Weaknesses:   Very limited evaluation to support the authors  claims, particularly against other traditional learnable 3D representations
This paper explores the Wasserstein natural gradient in the context of reinforcement learning. R5 rated the paper marginally below the acceptance threshold, but is not very confident about the correctness of his/her assessment. His/her main criticism was the experimental evaluation. This concern was shared by a confident R1. R1 found the paper well structured and that it contains encouraging empirical results, but low technical novelty and (initially) insufficient experiments. His/her initial recommendation was reject, but following an extensive discussion and improvements of the manuscript by the authors, he/she was more convinced about the empirical significance and applicability of the method, and raised his/her score to 6, indicating that the interpretation and presentation improved but that the paper might be interesting only to a moderate number of readers. A confident R2 found this paper very good, although only providing a short review. Two other unfinished or not sufficiently confident reports were not taken into account. Weighing the reports by contents, confidence, and participation in the discussion, the paper scores marginally above the acceptance threshold. In view of the authors  responses, I am discounting R5 s criticism about lack of comparison with the PPO baseline. I personally consider the paper very well written, that it presents a natural and potentially useful application of the Wasserstein natural gradient to the context of reinforcement learning, and enjoyed the discussion of behavioral geometry. I am recommending a borderline accept. However, I also appreciate the concern of the referees about the limited technical innovation and how some of the strengths of the method could be presented more convincingly. Please take these comments carefully into consideration when preparing the final version of the paper. 
This paper was reviewed by four experts in the field. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references.
Firstly, thank you authors for your thought provoking submission and discussion.  The key point of disagreement clearly is the fundamental assumption that "the result of an anomaly detection method should be invariant to any continuous invertible reparametrization f."  All reviewers found this assumption to be too strong, leading all four to recommend rejection.  I also recommend rejection at this time.  To me, it seems reasonable and practical to assume that anomalies are defined based on distance (in a fixed feature space).  So if we are allowed to deform the space, clearly this definition breaks down and the concept of an anomaly becomes empty.  Perhaps I am wrong about this, but nevertheless, the paper could do a much better job of convincing the reader that its fundamental reparametrization assumption is appropriate and of consequence in practice.
This paper proposes Feature Contractive Learning (FCL), a training framework that takes a more nuanced view of robustness, refining it to the sensitivity of the feature.  There are some differing opinions among the reviewers, with some applauding the simplicity of this new take on robustness while others are unsure of its underlying definitions and relationship to adversarial robustness.  The authors claimed to have clarified some of these points in their rebuttal / revision, but unfortunately, there was not much follow up discussion by the reviewers.  Ultimately, there are still enough lingering issues that rejection is warranted.
This paper studies the decision boundaries of shallow ReLU network using the formalism of tropical geometry. Its main takeaway is to provide a new interpretation of the lottery ticket hypothesis in terms of network pruning strategies that preserve certain geometric structure.   Reviewers were appreciative of the clarity of the exposition, and the novel perspective on interesting and elusive phenomena such as the lottery ticket hypothesis. On the other hand, they also expressed some doubts about the significance of some aspects of the theory (such as proposition 1 and corollary 1), as well as the computational considerations required to elevate the analysis to large scale architectures from applications.   Ultimately, and after taking into consideration all the reviewing discussions, the AC believes that this submission is not yet ready for publications, but it is in a trajectory to become an important piece of work. In particular, the AC encourages delving deeper into the tropical network pruning. Additionally, the authors might want to discuss [Breaking the Curse of Dimensionality with Convex Neural Networks, Bach 17] in the related work, since this is the first instance the AC is aware of where the connection between zonotopes and shallow ReLU networks is established. 
Although all reviewers acknowledge that the paper has some merit, the work lacks in novelty. The idea of using normalisation techniques to reduce the domain discrepancy in UDA is well established in the DA and DG community. The theoretical analysis is an interesting first step in providing some insights on this class of approaches, but it still does not support an understanding on why some of these methods work better than others. Given all these considerations, the work is not ready for acceptance at ICLR.
This paper studies ensemble calibration and the relationship between the calibration of individual ensemble member models with the calibration of the resulting ensemble prediction.  The main theoretical result is that individual ensemble members should not be individually calibrated in order to have a well calibrated ensemble prediction.  While other recent work has found this to be the case in empirical results, this paper substantiates the empirical results through theoretical results.   Pros: * Theoretical study of ensemble calibration with meaningful insights  Cons: * Contributions limited to theoretical study of known observation and dynamic temperature scaling. * Dynamic temperature scaling is not shown to outperform baseline methods. * Limited experimental validation: CIFAR 10/CIFAR 100.  The authors engaged in a extensive discussion with reviewers and made changes to their paper, including adding standard deviation results over multiple runs and the SKCE calibration measure.  Overall this is solid work and could be accepted to the conference; however, reviewers agree that parts of the work are lacking, in particular: 1. limited experimental evaluation (one type of task, one/two datasets only), and 2. given known literature the benefit of the derived theoretical results to practioners is not clear.  The discussions have been unable to resolve this disagreement. 
The initial reviews were mixed (2 positive, 2 negative).  The main concerns were about presentation issues: unclear contribution or main point; unclear analysis of figures; missing some motivation of selecting object detectors; etc.).  On the other hand, reviewers appreciated the well formulated paper, analysis and recommendations from the experiments;   The author response addressed the presentation issues and added additional motivations and clarifications. All reviewers in the end recommended accept. 
This paper studies why input gradients can give meaningful feature attributions even though they can be changed arbitrarily without affecting the prediction. The claim in this paper is that "the learned logits in fact represent class conditional probabilities and hence input gradients given meaningful feature attributions". The main concern is that this claim is verified very indirectly, by adding a regularization term that promotes logits learning class conditional probabilities and observing that input gradient quality also improves. Nevertheless, there are interesting insights in the paper and the questions it asks are very timely and important, and overall, it could have a significant impact on further research in this area.
Although the paper is clearly written overall and well motivated, reviewers raised several crucial concerns and, unfortunately, the authors did not respond to reviews.  During the discussion, reviewers agree with that this submission is not ready for publication. In particular, empirical evaluation is not thorough as important baselines are not included and discussion is not convincing.  I will therefore reject the paper. For future submission, I strongly recommend the authors to do author response. There are many cases where the reviewers change their scores based on the interaction between the authors and the reviewers, which is healthy for the review process.
The model presented here may be of use to others in running quantum chemistry simulations, and it may well lead to new advances, but the authors did not sufficiently address the key concerns around the model not being energy conserving and rotation covariant. The approach proposed could be learning such physical rules, and the authors in their general response provide some very preliminary evidence for this, but a much more thorough discussion with a full range of experiments is needed. ICLR is a broad conference where non experts who may have never heard of DFT simulations must parse this work and decide on if/how to follow up. This is a critical missing piece for anyone that wants to do so.  The above is exacerbated by the fact that the work is not well situated against prior work as pointed out by two reviewers. Together these two issues conspire to make understanding this model, the contribution of the work, and what followup is possible, untenable for an ICLR reader. For example, one would not be able to surmise from the manuscript and its brief discussion of rotation covariance that this is likely to result in ForceNet having limited applicability to other DM problems; which one reviewer pointed out and the authors generally agreed with. While the authors respond that perhaps the architecture itself may be useful for other applications, why this would be and what the specific advantage of the current model relative to the state of the art in those fields is unclear. 
This paper introduces a methodology for jointly optimizing neural network architecture, quantization policy, and hardware architecture. There are two key ideas:   Heterogeneous sampling strategy to tackle the dilemma between exploding memory cost and biased search.    Integrates a differentiable hardware search engine to support co search in a differentiable manner.  The paper tackles an important research problem and experimental results are good.  There are two related issues with this paper: 1. Comparison to one shot NAS: one shot NAS methods only need to train the super net once and then can be applied to multiple use cases, while the proposed methodology needs to be executed for each use case. 2. It is not clear whether differentiable search is needed for this joint optimization problem modulo existing tools.  Overall, my assessment is that the paper is somewhat borderline and with some more work will be ready for publication. 
The authors argue that tighter relaxations for certified robustness suffer from a worse loss landscape and thus are outperformed by the much simpler and less tight IBP relaxation and come up with a new relaxation to overcome this problem.   After the rebuttal there still remain doubts about the reasoning regarding the loss landscape (even though I acknowledge that the authors have invested significant amount of work to support their hypothesis). Moreover, the differences to existing certified training methods is small or the proposed method performs worse while being significantly more expensive (in particular if one takes into account the results which are reported on the IBP Crown github page where the reported numbers are significantly lower than reported in the present paper) so that the benefit is unclear.  Thus the majority of the reviewers still suggests rejection and I agree with that even though I think that the paper has its merits and I encourage the authors to continue this line of work. For a next version, the authors should evaluate all the methods ideally with an exact verification method resp. use the best relaxation for all methods. Otherwise the differences can come just from the weaker relaxation but not from a difference in real robustness.  
The paper uses adversarial data to improve generalization in Programming By Example (PBE). The reviews were somewhat mixed with some people finding this useful and interesting while others finding it straightforward and unsurprsing. The reviewers were not convinced of the ultimate usefulness of the approach since it is evaluated on toy or synthetic datasets. The clarity of the presentation could also be improved.
The paper proposes a new game theoretic model, Bayesian Stackelberg Markov Game (BSMG), for designing defense strategies while accounting for the defender s uncertainty over attackers  types. The paper also proposes a learning approach, Bayesian Strong Stackelberg Q learning (BSS Q), to learn the optimal policy for BSMGs. It is shown that BSS Q converges to an equilibrium asymptotically. Experimental results are provided to demonstrate the effectiveness of BSS Q in the context of web application security. Overall this is an interesting approach and an important direction of research. However, the reviewers raised several concerns, and there was a clear consensus that the paper is not yet ready for publication. The specific reasons for rejection include the following: (i) the experimental results are not presented with sufficient clarity, no statistical significance tests are performed, and the choice of baselines is weak; (ii) the contributions are not sufficiently broad, the learning process described in the paper is unclear, and the framework requires a strong assumption of knowing the attackers  distributions. I want to thank the authors for actively engaging with the reviewers during the discussion phase. The reviewers have provided detailed feedback in their reviews, and we hope that the authors can incorporate this feedback when preparing future revisions of the paper. 
Most reviewers found the method proposed to be technically sound, well motivated and particularly interesting due to the interpretability of its results. Indeed, the extraction of interpretable motifs from NAS is a valuable contribution. One of the reviewers was particularly concerned by the lack of guarantees of the proposed method and a perceived failure mode of averaged gradients. We thank both the reviewer and the authors for the detailed discussion on these points. Ultimately, the benefits of the method proposed and the magnitude of the contributions in the paper outweigh these concerns.
This paper proposes to jointly learn a mapper and planner for navigation or manipulation in a 2D space represented by an MxM grid, with the mapper taking raw observations as inputs and producing a 2D MxM occupancy and goal location map, and the planner   pretrained on generic 2D maps of same size MxM   produces an MxM action distance image (the plan). The whole system is trained end to end, with the mapper trained on the specific navigation or manipulation task, and the planner frozen. The Spatial Planning Transformer network can predict the MxM action plans faster than baselines (Value Iteration Networks and Gated Path Planning Networks) because of the attention mechanism in the transformer architecture, as opposed to the local information propagation through the convolutional encoding of the Bellman equation in VIN or GPPN. The differentiable approach is motivated by exploiting regularities in 2D maps and a faster inference time (as opposed to classical Dijkstra planners or VIN / GPPN), and is demonstrated on a simple room navigation task in the Gibson environment.  Reviewers have praised how a simple idea (transformer based planning à la VIN) can be applied as a unifying approach to 2 DOF manipulation and 2D navigation, and the out of sample evaluation. The critique was about: * the lacunary explanation of the mapper (corrected by the authors), * the limits of non recurrent mappers that cannot handle occlusions in the observations when building maps, * confusion about where the full MxM image of action labels (necessary for supervision) can come from (the authors added experiments with sparse labels), * claims about this approach being preferable to A* (countered by the authors, who conducted extensive experiments at the request of R4), * focus on two problems at once (mapping and planning) rather than more ablation analysis of the planner, with a potential comparison to Active Neural SLAM, * missing evaluation using navigation specific metrics, such as SPL.  The authors have a point in their defense of differentiable / learning based methods for planning (including VIN and GPPN) as opposed to classical planning such as A*, and there is value in investigating how an end to end differentiable method for mapping and planning could be designed. At the same time, several reviewers raised concerns about scalability and about the pertinence of combining two learnable modules (mapper and planner) rather than investigating and demonstrating the advantage of transformers for planning.  Given these reviews, rebuttal, and remaining concerns, I am sorry to reject this paper. I hope that with the proposed modifications it will be quickly accepted at another venue.
The paper discusses the problem of how to augment cross modal retrieval for the task of multi modal classification   it uses image caption pairs to improve downstream multimodal learning, and shows improvement in the task of visual question answering. However, the paper has the following weaknesses: (a) lack of novelty, (b) lack of thorough empirical evaluation, (c) the complex model did not give significant gains.
The work proposed to learn causal structure of the environment and use the average causal effect of different categories of the environment, between the current and next state after performing an action as intrinsic reward to assist policy learning. While the reviewers find the ideas presented in the paper interesting and of potential, there are some concerns regarding proper introduction and comparison to related works, and clarity of the algorithm itself. While the two experimental results presented in the paper do show the potential of the work, it is missing an important baseline to disentangle the effectiveness of introducing the causal structure alone vs intrinsic reward. For example, how would A2C with curiosity or surprised based intrinsic reward, which also introduce the surprisingness of the next state as a result of performing an action as additional reward perform on these tasks?  
All reviewers agree that this paper is interesting, but needs improvement in order to be suitable for a highly competitive venue such as ICLR. Reviewer 3 is especially incisive and detailed, but other reviewers make similar points.
This paper addresses an distribution shift and biased Q values that happens when offline agents are finetuned in an online manner. The final revision of the paper is very well written and easy to understand. The proposed method in the paper is interesting, and aiming to address an important issue in RL. The proposed method involves a combination of two well known methods in RL to tackle the distribution shift issue, the paper first suggests to use a balanced replay mechanism a replay for online experiences and another one for the offline. The second improvement is coming from the ensemble distillation.  It seems like in the light of the reviews, the authors have improved manuscript. However, I would like to recommend the paper for rejection. I would like the authors to do further experiments on the individual components of the algorithms, for example what if we run all the experiments only with BR or only using ED how would the performance change. How much improvement is coming from each one of those individual components? As it stands, it is not clear to me right now, and the proposed solution looks a bit complicated and hacky.   The balanced replay mechanism is very similar to the replay approaches that are used for learning from demos methods like R2D3 [1] and DQfD. Also the ensemble distillation approach is very akin to RAND [2] and distillation approaches that are used in lifelong learning algorithms. It is not clear, why it is that important for offline RL. It should potentially improve online RL as well, perhaps some experiments on online RL would be interesting.  Nevertheless, I think the paper is very interesting and attempting to address a very important problem in RL. I would recommend the authors to resubmit the paper to a different venue after doing some small changes on it.  [1] Gulcehre, C., Le Paine, T., Shahriari, B., Denil, M., Hoffman, M., Soyer, H., ... & Barth Maron, G. (2019, September). Making Efficient Use of Demonstrations to Solve Hard Exploration Problems. In International Conference on Learning Representations.  [2] Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.    
This paper proposes a weighted balanced accuracy metric to evaluate the performance of imbalanced multiclass classification. The metric is based on a one versus all decomposition from multi class to binary, and then aggregating the metrics on the binary classification sub problems in a weighted manner. The authors hope to argue that the new metric is more flexible for evaluating classifiers in the imbalanced and importance varying setting.  The reviewers agree that the proposed framework is simple and applicable to an important problem. Somehow the novelty and significance of the work is pretty limited, as many related metrics (e.g. micro/macro averaged metrics) exist in the literature. The authors are encouraged to think about stronger reasoning on how useful the "new" metric is. The experiments are also not convincing nor complete enough to verify the benefits of the proposed metric. 
The paper proposes a personalized federated learning method, which personalizes by computing a weighted combination of neighboring compatible models. Reviewers uniformly liked the quality of writing and level of novelty, and agree on the relevance of the problem and solution. The solution was deemed creative and particularly impactful in the important case of heterogeneous data on each node, and experiments showed convincing improvements. The discussion between reviewers and authors was constructive and has lead to further improvements of the paper. Slight concerns remained on privacy with all models stored on the server, and breath of personalized FL benchmarks used, but reviewers agreed the contributions overall are still significant enough. Future work remains on the theory of the proposed model.
This paper proposes a meta learning based technique to learn how to back translate (generate a synthetic source language translation of an observed target language sentence) for the purpose of better optimising a source to target translation model.   The approach is an interesting novel angle to jointly training the translation model and the back translation component. Compared to techniques like UNMT and DualNMT, the approach offers reduced training time and a simpler formulation with fewer trainable components (and fewer hyperparameters).   During the discussion phase the authors provided additional insight, clarifications, and results that improved our perception of the paper. I would personally appreciate if the authors would update their paper with the clarifications they made to points raised by R2, R3, and R4, especially on the details about meta validation, the discussion about memory footprint, and the additional results on UNMT (and variants). 
This paper studies the convergence of gradient descent ascent (GDA) dynamics in a specific class of non convex non concave zero sum games that the authors call "hidden zero sum games". Unlike general min max games, these games have a well defined notion of a "von Neumann solution". The authors show that if the hidden game is strictly convex concave then vanilla GDA converges not merely to local Nash, but typically to this von Neumann solution.  The paper received four high quality reviews and was discussed extensively during the author rebuttal phase. From an application angle, the authors  replies did not convince the reviewers on the relevance of this paper to GANs, and one of the original "accept" recommendations was downgraded to a "reject" because of this. On the theory side, the novelty over Vlatakis Gkaragkounis et al. (2019) is not clear and the reviewers found the writing often confusing or hard to connect with practice. The reviewer with the most positive recommendation did not champion the paper post rebuttal. In the end, the consensus was that the work shows significant promise, but it requires refocusing before appearing at a top tier conference.
The reviewers have some following concerns:   1) There is lack of experimental result. The experiment on MNIST with small CNN architecture is definitely not sufficient to verify the efficiency of the proposed method. Moreover, the advantage of the proposed method is not very clear due to the choices of the parameters. The choice of the learning rates is quite sensitive.   2) It is not clear why the authors could argue that $ \mathbb{E}(V_T)   \mathcal{O}(T)$ without any theoretical and empirical support. Even if this is correct, this term could dominate the first term unless $ \mathbb{E}(V_T) \leq \mathcal{O}(\sqrt{T})$, which is too strong. If assuming $\mathbb{E}(V_T)   \mathcal{O}(T)$, the convergence results are upper bounded by some constant (note that $\epsilon$ is a constant in this scenario, not arbitrarily small). Hence, the authors failed to show the convergence to a stationary point.   There are some suggestions to improve the paper as follows:   1) Show $\mathbb{E}(V_T)   \mathcal{O}(T)$ and revise the theory properly to make it rigorously by showing upper bounded by some function $R(T) \to 0, T \to \infty$ rather than showing the convergence to some fixed neighborhood. (Note that $\frac{C_4}{\sqrt{N}}$ is a fixed constant).   2) Do more experiments on various datasets and network architectures to verify the efficiency of the proposed method and show the clear advantages compared to others.   3) Provide convergence rate comparisons with other decentralized algorithms (e.g., as a table). It would be nice if the authors also provide the assumptions and the dependent constants so that the readers could really see the differences.   4) Explicitly derive the convergence measure based on the standard one, that is, $\frac{1}{T} \sum_{t 1}^{T} \mathbb{E} [ \| \nabla  f (  X_t )  \|^2 ] $ and add the dependency of $G_{\infty}^2$ to the bound.   5) Revise the paper and implement all necessary comments from the reviewers consistently with the content.  
This paper the flip side of an adversarial "attack" in that data may be perturbed to make it look like a model was performing well rather than the standard notion of adversarial attacks. The reviewers found this notion interesting and potentially worthy of investigation. However as it stands, the proposed applications and methods do not seem developed well enough as would be expected at a conference like this.
This paper presents a series of negative results regarding the convergence of deterministic, "reasonable" algorithms in min max games. The defining characteristic of such algorithms is that (a) the algorithm s fixed points are critical points of the game; and (b) they avoid strict maxima from almost any initialization. The authors then construct a range of simple $2$ dimensional "market games" in which every reasonable algorithm fails to converge, from almost any initialization.  The paper received three positive recommendations and one negative, with all reviewers indicating high confidence. After my own reading of the paper, I concur with the majority view that the paper s message is an interesting one for the community and will likely attract interest in ICLR.  In more detail, I view the authors  result as a cautionary tale, not unlike the NeurIPS 2019 spotlight paper of Vlatakis Gkaragkounis et al, and a concurrent arxiv preprint by Hsieh et al. (2020). In contrast to the type of cycling/recurrence phenomena that are well documented in bilinear games (and which can be resolved through the use of extra gradient methods), the non convergence phenomena described by the authors of this paper appear to be considerably more resilient, as they apply to all "reasonable" algorithms. Determining whether GANs (or other practical applications of min max optimization) can exhibit such phenomena is an important open question, and one which needs to be informed by a deeper understanding of the theory. I find this paper successful in this regard and I am happy to recommend acceptance.
The authors propose a neural module based approach for reasoning about video grounding.  The goal is to provide performance and interpretability.  Unfortunately, the reviewers found the paper opaque, the results confusing, and expressed repeated concerns about the novelty, fairness of comparisons and concerns that the surprising results were not sufficiently well justified by the paper (or the author s response).
The paper proposes a new framework for online hypothesis testing aimed at detecting causal effects (of treatments on outcomes) within subgroups in online settings where treatments are randomized.  Such settings occur in online advertising where different versions of the same website may be presented to a set of otherwise exchangeable users via A/B testing.  Under the standard causal assumptions of SUTVA, and sequential ignorability, in addition to a set of regularity conditions, the authors derive a result (Theorem 1) leading to an online test (Theorem 2).  Since the resulting test s limiting distribution does not have an exact analytic form, the authors instead propose a bootstrap approach to determine a set of parameters to properly control the error rate.  The author validate their approach by a simulation study, as well as via a user click log data from Yahoo!  The reviewer opinion was somewhat split on this paper, in particular some reviewers raised concern about some (conceptually significant) typos, interpretability of assumptions, and the need for parametric assumptions (the dichotomy between linear models and neural networks is surely a false one   the semi parametric literature obtains nice parametric style results, although perhaps not always for tests, without assuming parametric likelihoods all the time). 
All reviewers have carefully reviewed and discussed this paper. They are in consensus that this manuscript merits a strong revision. I encourage the authors to take these experts  thoughts into consideration in revising their manuscript.
This paper proposed an unsupervised domain adaptation method for 3D lidar based object detection. Four reviewers provided detailed reviews: 3 rated “Marginally above acceptance threshold”, and 1 rated “Ok but not good enough   rejection”. The reviewers appreciated simple yet effective idea, the well motivated method, the comprehensiveness of the experiments, and well written paper. However, major concerns are also raised regarding the core technical contributions on the proposed approach. The ACs look at the paper, the review, the rebuttal, and the discussion. Given the concerns on the core technical contributions, the high competitiveness of the ICLR field, and the lack of enthusiastic endorsements from reviewers, the ACs believe this work is not ready to be accepted to ICLR yet and hence a rejection decision is recommended. 
This paper approximates the Whittle index in restless bandits using a neural network. Finding the Whittle index is a difficult problem and all reviewers agreed on this. Nevertheless, the scores of this paper are split between 2x 4 and 2x 7, essentially along the line of whether this paper is too preliminary to be accepted. Therefore, I read the paper and propose a rejection.  The reason is that the paper lacks rigor, which was brought up by the two reviewers who suggested rejections. For instance, in the last line of Algorithm 1, it is not clear what kind of a gradient is computed. The reason is that \bar{G}_b is not a proper baseline, as it depends on the future actions of the bandit policy in any given round. I suggest that the authors look at recent papers on meta learning of bandit policies by policy gradients,  https://papers.nips.cc/paper/2020/hash/171ae1bbb81475eb96287dd78565b38b Abstract.html  https://arxiv.org/abs/2006.16507  This is the level of rigor that I would expect from this paper, to make sure that the gradients are correct.
This paper proposes an autoregressive flow based network, Flowtron, for TTS with style transfer. It integrates the Tacotron architecture with the flow based generative model.  Extensive experiments are carried out in a controlled manner and the results show that the proposed Flowtron framework can achieve comparable MOS scores to the SOTA TTS models and is good at generating speech with different styles. All reviewers consider the work interesting.  There are concerns raised on technical details which mostly have been cleared by the authors  rebuttal. The exposition also has been greatly improved based on the reviewers  suggestions and questions.  Overall, this is an interesting paper and I would recommend acceptance. 
The reviewers highly appreciated the replies and the additional experiments. We also had a private discussion on the paper. To summarize: the replies alleviated quite a few concerns, however the consensus was that the paper still does not meet the bar for a highly competitive conference like ICLR.  The idea of combining MPC (on a  wrong  model)  with a learned cost function is very interesting and a promising direction. On the downside the reviewers are still not entirely convinced about the contribution and believe that the paper requires a significant re write to incorporate the discussed points as well as an additional round of reviews.
There is a pretty good consensus that this paper should not be accepted at ICLR. The reviewers do not seem think that extending MuZero to non deterministic MuZero constitutes a significant advance.  Three reviewers give clear rejects with scores (3, 4, 5) all with good confidence (4).  A fourth reviewer gave a score of 6, i.e., borderline accept.  While the fifth reviewer recommends, he does not seem to be very confident and did not step in to champion the paper.  The program committee decided that the paper in its current form does not meet the acceptance bar.
The authors propose to take a token level generative approach to the task of vision language navigation (R2R/R4R). The reviewers raise a number of concerns which should be noted in the final version of this work.  The primary concern revolves around generality.  How will this approach generalize to more sophisticated generative and discriminative models?  To what extent is the model relying on the short instruction/action sequences to succeed and would not perform well on longer instructions, longer trajectories, or more abstract language.  Finally, the discussion of the uninformed prior is interesting because while "clean", reviewers note there is no realistic grounded language scenario in which an uninformative prior makes sense.
The reviewers agree that this is an interesting and promising paper, although it is on the theoretical side, without even satisfying toy examples to demonstrate its usefulness. This itself is not a fatal problem (ICLR can and should welcome theoretical papers), however including such experiments would significantly strengthen the impact of this paper, and make it more competitive with other ICLR submissions.
The reviewers have ranked this paper as borderline accept. On the negative side, the main claim of the paper (the more categories for training a one shot detector, the better) has already been observed in several works and very intuitive. However, the paper has done significant experimental work to support this claim. The paper is very well written, it carefully explores the existing setups for one shot detection and highlights their weaknesses. The paper also gives advice on how to construct better datasets for one shot detection (the conclusion "add more diverse categories" is somewhat obvious but the paper demonstrates how important that is).
This paper introduces Regioned Episodic Reinforcement Learning (RERL), which partitions the state space by generating a diverse set of goals and then explores the state space by learning policies that reach those goals. This idea is a combination of episodic memory techniques and “goal oriented” reinforcement learning.   After the authors’ responses and the discussion phase, all reviewers converged to recommending the rejection of this paper. The main concerns regarding this paper are:  * Presentation. The proposed approach is not that well justified, some of the claims in the paper are quite imprecise, and there’s relevant related work missing. * Evaluation. The evaluation sometimes feels rushed and is not held in a diverse enough set of tasks, not capturing important properties one would want to capture.  I recommend the authors to pay close attention to presentation, as well as the experiments and analysis in order to make the paper stronger. 
This paper proposes a combined method to address stragglers and adversaries in federated learning. Stragglers are overcome by allowing staleness in model aggregation. Adversaries are handled by using a public dataset to identify poisoned devices and adjusting their weights when doing model aggregation. However, the reviewers raised concerns about: * The correctness of Theorem 1 * The novelty of the paper given that there has been significant previous work on straggler mitigation and robust aggregation in distributed learning.   As a result, I am unable to recommend the acceptance of the paper. However, the idea is certainly promising, and if built upon more rigorously can result in a nice and impactful paper. I hope that the authors can take the reviewers  feedback into account when revising the paper for a future submission!
The paper proposes to combine the span level information into a phrase level representation in the fine tuning phrase for pre trained language models.  The phrases are pre defined in a dictionary.  Experiments show improvements in various downstream tasks in the GLUE benchmark.  It s a borderline paper.  Various concerns were raised by the reviewers, for example, the relation with the SpanBERT method in pre training phrase and the significance of the results.  The authors addressed most of the concerns but the reviewers were not fully convinced.  In general, I think it is an interesting paper with good motivation and results.  Hope it can be improved (e.g. more experiments on SpanBERT) and accepted in another conference.
The paper analyzes the space of mixed sample data augmentation approaches, and proposes a new variant, FMix, based on a new masking strategy. Reviewers point to the fact that FMix is only marginally better than previous approaches, that the experimental setup is unconvincing, and that the proposed analysis might not be grounded. This is a really borderline paper but I see the issues as more important than the benefits, so I recommend rejection.
This paper introduces an interesting application of VAE GAN to the problem of Spectral Synthesis across satellite observations with some additional domain specific changes (new loss, ...). The introduction of a new dataset is also very interesting and can open the door for more methodological development in the community.   While the application is original, the methodological contributions have been judged limited and domain specific by most of the reviewers. The responses from the authors was appreciated by the reviewers (especially the comparison to the UNIT baseline) but did not change their opinion about the limited methodological novelty of the approach. The AC recommends a reject but encourages the authors to resubmit it to a more applied remote sensing venue. 
In this paper, the authors propose to add recurrence to pre trained language models such as GPT 2 or BERT. The idea is similar to the compressive transformer paper: a small module is added to the network, and used to compress the representations from the previous chunk of data from the sequence to a single vector. Then, this vector is added to the keys and values of the self attention module when processing the next chunk. The main contribution of the paper is to show that this technique can be added to pre trained models at fine tuning time.  The main concerns regarding the paper are technical novelty and limited empirical results. The idea of adding recurrence to transformers was previously explored in compressive transformer, and many previous work have considered adding modules with small number of parameters at fine tuning time. Moreover, I do not believe that the empirical section is strong enough to justify the acceptance of the paper, as the method is only evaluated on two language modeling tasks (and one early experiment on HotpotQA). The baselines are weak, and thus, the results are not convincing. For these reasons, I weakly recommend to reject the paper, and encourage the authors to make the empirical section stronger.
The paper examines an idea that knowledge and rewards are stationary and reusable across tasks. An interesting paper that combines number of related topics (meta RL, HRL, time scale in RL, and attention), improving the speed of training.  The authors have addressed the reviewer comments, strengthening the paper. The reviewers  agree, and I concur, that the paper contributes a novel model,  valuable to the ICLR community. It is well thought out, presented, and evaluated. 
The paper proposes to integrate multiple bit configurations (including pruning) into a single architecture, and then automatically select bit resolution through binary gates. The overall approach can be differentiable and optimized with parameters. However, as pointed out by the reviewers, the novelty of this paper can be the big question. Also, it seems to be hard or unpractical to employ different number of bits for layers, given the standard GPU and CPU hardware. 
This paper aims to study the dimension of the Class Manifolds (CM) which are defined as the region classified as certain classes by a neural network. The authors develop a method to measure the dimension of CM by generating random linear subspaces and compute the intersection of the linear subspace with CM. All reviewers agree that this is an interesting problem and worth studying.   However, there are major concerns. One question raised by several reviewers is that the goal of this paper is to analyze the dimension of the region that has the same output for the neural network; while the method and analysis are for a single datum. It is not clear if the obtained result is what the paper really aimed at. Another issue is the experimental results are different from that of local analysis. The dimension estimated by using the method in this paper is much higher.   Based on these, I am not able to recommend acceptance. But the authors are highly encouraged to continue this research. 
The paper analyzes the sample complexity of convolutional architectures, proving a gap between it and that of fully connected (fc) networks. The approach builds on certain invariances of fc nets. The reviewers appreciated the technical content and its contribution to understanding the relative advantages of different architecture, as well as the role of invariance. 
The paper proposes and uses a fairly involved attention based architecture to perform time series forecasting. The idea of transformers is raised, but, given how sequence embedding is often convolutional, and position encoding input is provided to the model (albeit implictly in the form of features having to do with qualitative things such as promotions, etc among other things), I m  of the opinion it is closer to the paper "Convolutional Sequence to Sequence Learning" https://arxiv.org/abs/1705.03122 than it is to transformers per se... Also the connection to sequence to sequence is not clear, since the chain rule of probability isn t really stressed on much.   The paper proposes some interesting ideas, but I feel that it failed to convince the reviewers of the utility and the novelty. Part of it has to do with the clarity of presentation, and part of it, I think has to do with the fact that paper jam packs a bunch of different ideas together, without carefully ablating the influence of their various ideas. For example, transfomers have been applied for time sequences (https://arxiv.org/pdf/2001.08317.pdf), and its not clear in what ways this paper improves on them   is it the complicated attention model scheme ? or is it the multi horizon context schemes ?    That being said, the results shows are relatively decent and the reviewers liked that aspect. Had the paper been easier to follow and the ideas presented with a little more insight it would be been a better fit for ICLR. As it stands, I have to give it a weak reject.
This paper proposes a broad framework for unifying various pruning approaches and performs detailed analyses to make recommendations about the settings in which various approaches may be most useful. Reviewers were generally excited by the framework and analyses, but had some concerns regarding scale and the paper s focus on structured pruning. The authors included new experiments however, which mostly addressed reviewer concerns. Overall, I think is a strong paper which will likely be provide needed grounding for pruning frameworks and recommend acceptance. 
This paper tackles an important problem of generating a synthetic data for stock market agents behavior. In particular, GAN is trained to distinguish stock market time series from synthetic ones. Then market agent parameters can be scored by running time series generated using these parameters through a GAN. This way one can optimize the parameters to find realistic ones.  The reviewers appreciate authors response, however, they found that their concerns has not been fully addressed:  1. Clarity. The reviewers believe that the presentation need to be significantly improved and clarified before the paper is ready for acceptance (see reviewers comments for details). 2. The metrics used to evaluate the performance are not sufficient. While "stylized facts" statistics can be a good sanity check it does not mean that the agents that have been produced are useful in any way. Presumably, the goal of calibration such agents is to use them in simulation to generate synthetic trading for the purpose of building a predictive model. If so then it seems that the best way to judge the success of the calibration by the success of downstream task.  Given this, the paper can not be accepted for the presentation in its current form.
This paper proposes a meta learning algorithm for reinforcement learning. The work is very interesting for the RL community, it is clear and well organized. The work is impressive and it contributes to the state of the art. 
Dear authors,  all reviewers found many interesting contributions in your paper and also pointed out some minor/major issues. In your rebuttal discussions, you addressed most of them to their satisfaction and I hope you will incorporate them carefully also in your final submission.  I hence recommend accepting this paper  
This paper proposes a method to solve regression without correspondence.  The problem is well motivated, and the proposed method is technically sound. The motivation, organization, and presentation of the paper are very clear.  Reviewers’ suggestions to further improve the paper (e.g., clarifications on initialization, comparison and discussion with with EM, AD, etc) were adequately incorporated to the revised manuscript. 
This paper first examines a multi domain separation phenomenon, where different types of adversarial noise lead to different running statistics, and then introduces Gated Batch Normalization (GBN), a building block for deep neural networks that improves robustness against multiple perturbation types. GBN consists of a gated subnetwork and a multi branch BN layer, and each BN branch handles a single type of perturbation. Results were reported on MNIST, CIFAR 10, and Tiny ImageNet. Though the idea and methodology are valid and of interest, some concerns regarding the experimental section remain after rebuttal discussions.
The reviewers generally like the paper, in particular the scalability of the proposed approach. The author response and revised version clarified some questions of the reviewers, however, it didn t fully mitigate their concerns.
This paper presents an approach to hierarchical RL which automatically learns intrinsic task agnostic options. The approach involves a two level hierarchy, with policies learned by lower layer Workers and selected by a higher layer Scheduler. The approach is evaluated on four complex tasks and is shown to outperform existing methods. There were initial concerns with this paper around clarity of a number of points. These included the contributions of this work and questions around the experimental results, such as  discussing the learned options themselves. The authors provided extensive responses to these concerns, and updated the paper accordingly, including addition results and analysis. I believe the paper is now much clearer with interesting contributions.
This paper introduces LambdaNetworks, a new method to capture long range interactions in data (such as global context in images). The method is novel and simple, and the experimental results are strong, especially in terms of speed/accuracy tradeoffs. The paper is well written and easy to follow. For these reasons, I recommend to accept the paper.
# Quality: I personally feel that the comment from Reviewer1 regarding "real world" is a minor but valid point. Even after the rebuttal, the abstract seems to suggest that the proposed algorithm is effective to solve real world challenges. Maybe further rephrasing or explicitly stating that the experiments are in simulation might help to clarify this point.  Reviewer3 also raised valid points regarding the experimental evaluation and the use of just 3 seeds. Given the struggle in reproducibility in RL and the shady experimental practices from even leading AI companies (e.g., cherry picking of seeds), it is paramount that experiments follow strong methodologies and good practices. As such, the experimental results presented in this manuscript should be strengthened.  # Clarity: All the reviewers pointed out that the paper writing should be improved. Although the authors significantly improved the manuscript during the rebuttal period. Several reviewers suggested that the manuscript should be further polished before publication.  # Originality: The two proposed approaches are novel to the best of the reviewers and my knowledge. Two reviewers pointed out that the theoretical results should be explained more thoroughly and to clearly differentiate from prior work.  # Significance of this work:  The paper deal with an important and timely topic. Although the work could be very impactful for real world applications, there is no real world application. Hence it is difficult to gauge the significance of the work.   # Overall: The paper does not feel quite ready for publication yet. A clearer presentation and extended experiments would certainly improve the quality of the manuscript.
The paper looks into theoretical analysis of self training beyond the existing linear case and considers deep networks under additional assumption on data. namely: expansion and minimal overlap in the neighborhood of examples in different classes. The results shed some light on self training algorithms that use input consistency regularizers. Although the assumptions are very hard to check for all input distributions, the authors make an attempt by considering output of BigGAN generator. In summary, the paper is a great first step in understanding self training for deep networks.  The paper is overall clearly written. please add the explanation of  Assumption 4.1 as requested by Reviewer 4.  Pros:   given the extensive use of self training the paper is of great importance to the community  extending the analysis of self training to deep networks  the paper is clearly written and easy to follow  cons:  the assumptions are very hard to validate on all datasets 
The paper is proposing a new framework for understanding generalization in the deep learning. The main idea is considering the difference of stochastic optimization on a population risk and optimization on an empirical risk. The classical theory considers the difference of empirical risk and population risk. This basically translates the practical motivation from finding good function classes to finding good optimizers which can re use the data effectively. Although the paper provides no theoretical result, it provides an interesting empirical study. The paper somewhat demonstrates that SGD on deep networks is somehow good at re using the same data. I believe this angle is very novel and might hope to future theoretical discoveries. The paper is reviewed by four reviewers and two of them argue its acceptance and two of them argue rejection. After discussion, this status remained and I carefully read and reviewed the paper. Here are the major issues raised by the reviewers:    R#1: The paper is missing a theoretical study. The implications on the practical deep learning is not clear.   R#2: Choice of the soft error is particular to the task and how to go beyond soft max is not clear.   R#3: Finds the paper not novel as well as trivial or hard to understand.   R#4: The choice of soft error is ad hoc.  I believe the issues raised by R#3 are not justified. First of all, novelty is very clear and. appreciated by other reviewers. Moreover, the paper is rather easy to understand and the results are very farm from trivial. However, the other issues raised by other reviewers are valid. Specifically, soft error seems to be a limitation of the study. However, the authors respond to this concern and reviewer increases their score. I believe the theory is lacking but the paper is simply showing this novel approach and its empirical validity. A theory to explain this phenomenon would be amazing but not necessary for publication. Similarly, without a theory it is hard to expect any practical implication. Overall, I believe the paper is an interesting and novel one which will likely to lead additional work in the area. Considering we are still far from a satisfying theory of generalization for deep learning and the role of the optimization is clear, this angle worth sharing with the community. Hence, I decide to accept. However, I have some concerns which should be addressed by the camera ready.     Claims should be revised and authors should make sure they have enough evidence for them. For example, authors provide no satisfying evidence for random labels or very limited evidence for pre training. I strongly recommend authors to either remove some of these discussions or present in a fashion which is not a result but part of the discussion for future research.   A section about limitations should be added. Specifically, the soft error choice should be discussed in this limitation section.   Discussion section should be extended with the pointers to the relevant work on bootstrap literature as well as suggestions to the theoreticians. Not providing any theoretical result is always fine but authors should understanding why is it hard to make theoretical statements and where to search them.
## Description   The paper asks the question whether it is possible to accelerate training a binarized neural network from scratch to a given target accuracy [by starting with training a full precision network]. The main claimed contributions are: the idea to use *partially* pretrained networks, experimental evidence regarding the split of the training budget and measuring the speed up.  ## Review Process and Decision  All four reviewers agreed in the low rating of the paper and in the opinion that the paper is not a significant contribution. The area chair supports rejection.  ## Details  It has been already observed that pre training  in some form is needed for achieving the best accuracy:  Rastegari (2016) XNOR Net: ImageNet Classification Using Binary Convolutional Neural Networks Bulat (2019), "Improved training of binary networks for human pose estimation and image recognition" Martinez (2020): "Training Binary Neural Networks with Real to Binary Convolutions"  Alizadeh, (2019 fig. 4) notice that pre training can be viewed as a speed up, but in their setup find that training from scratch gives a better accuracy. Bulat (2019) and Martinez (2020), on the contrary do use pre training to achieve the best accuracy.  It is questionable whether the pre training in these works is partial or not. I believe the result largely depends on the pre training method used, which is not discussed in depth in the submission. More generally, some graduated optimization methods such as graduated smoothing or graduated non convexity are known to help in finding better solutions / lead to faster optimization and in fact Bulat (2019) use pre training with gradual transition from smooth activation to the sign function.  Relative to these points the technical contribution (one paragraph in the paper) is not significant. The empirical part of the contribution shows some effect, but does not indicate a breakthrough on its own. An investigation / design of pre training schemes could make it more substantial.  The empirical analysis proposed does not rule out, and in fact supports, the methodology that for the best final accuracy, the full rather than partial pre training is useful.  The gain of speed up by a factor 1.3 (diminishing to close to 1 if we are interested in the best accuracy), is of little practical interest. In particular, a slight code optimization can give a similar speed up without the complexity and hyperparameters involved in pre training. The authors write  "we are not aware of any effort to exploit binarization during the training phase" There are available public implementations that can optimize the forward pass of binary networks, in particular on GPU, while backward pass can stay in full precision. It could give a similar speed up. In particular Courbariaux (2016) provides a GPU kernel and proposes a variant of BatchNorm with bit shifts rather than multiplications, specifically used at training time. Making the emphasis on a relatively small speed up that can be obtained to train sub optimal models, in my view is not a good strategy to present this work. Rather the phenomenon that (partial) pre training helps with the goal to improve the training methods more substantially I find of higher interest.  Finally, I agree with the reviewers that the lottery ticket hypothesis (Frankle, 2020) work speaks of the speed up only hypothetically and its main (and fairly in depth) contribution is in demonstrating and investigating an interesting phenomenon about training and initialization, which I do not see relevant to this submission. 
Summary: The authors propose to approximate operations on graphs, roughly speaking by approximating the graph locally around a collection of vertices by a collection of trees. The method is presented as a meta algorithm that can be applied to a range of problems in the context of learning graph representations.  Discussion: The reviews are overall positive, though they point out a number of weaknesses. One was unconvincing experimental validation. Another, more conceptual one was that this is a  unifying framework  rather than a novel method. Additionally, there were a number of minor points that were not clear. However, the authors have provided additional experiments that the reviewers consider convincing, and were able to provide sufficient clarification.  Recommendation: The reviewer s verdict post discussion favors publication, and I agree. The authors have convincingly addressed the main concerns in discussion, and novelty is not a necessity: Unifying frameworks often seem an end in themselves, but this one is potentially useful and compellingly simple. 
This paper studies the problem of unsupervised domain translation. Here translation does not refer to language translation. Instead, it refers to the idea of transferring high level semantic features. Specifically, the authors look at digit style transfer (between MNIST/postal address numbers and SVHN/street view house numbers) and Sketches to Reals. The visuals look very convincing and the empirical results are strong, too. There is one weaker review but the authors address the concerns in their response and the reviewer did unfortunately not respond despite promting.
All the reviewers highlight that the paper addresses the important issue of extending deep latent variable models to handle missing non at random data, which are known to be very difficult. The authors suggest modeling the mechanism of missing values and perform inference  using amortized importance weighted variational inference and demonstrate the capacities of their approach on many experiments. The paper highlight the trade off between the complexity of the data model and that of the missing data mechanism. The authors appropriately answer reviewers comments, add new experiments varying the percentage of missing values, and give more details on the methodological part.  I also think that this is a valuable contribution to the community, that the literature is well covered (the historical statistical litterature and the ML one), and that it provides new insights and methods to tackle this difficult problem.  
The scores for this paper have been borderline, however the decision has been greatly facilitated by the participation of the authors and reviewers to the discussion and, more importantly, by active private discussion among reviewers and AC. Specifically, from the private discussion it seems that the reviewers find interesting ideas in this paper, but are overall are not entirely convinced about its significance, at least in the way the paper is currently positioned and motivated.   More specifically, the reviewers found the main idea of using inducing weights interesting, both technically (e.g. associated sampling scheme) but also in terms of application (sparsity). The results are insightful from a theoretical perspective. That is, the inducing weights and their treatment does seem to result in interesting and potentially useful statistical properties for the model. On the other hand, it is important to note that the high level idea of variational inducing weights, with usage of matrix normals in this setting, as well as connection to deep GPs has been studied before, as pointed out by R2 (refs [1,2]). Furthermore, even after discussions the motivation is still not entirely convincing, especially in conjunction with the experiments. Although various interesting ideas exist in the paper, both R2 and R3 in particular remain unconvinced about what is the main benefit (e.g.  pruning or runtime efficiency) stemming out of the proposed idea. Another reviewer agreed with this point in the private discussions.  Apart from overall clearer positioning of the paper, the claimed benefit would need to be supported by experiments tailored to illustrate this main point. The authors argued against some of the suggested comparisons (e.g. past pruning methods), and further discuss that there is no established experimental benchmark for the parameter efficiency of BNNs. I indeed sympathize with both of these arguments; however, I believe that if the reviewers  suggestions for extra experiments are rejected, it remains the responsibility of the authors to find a slightly different way of motivating their work and demonstrating its efficiency in some convincing, meaningful and more well defined setting with the appropriate benchmarks.   
The reviewers agree that this is an interesting and original paper that will be of interest to the ICLR community, and is likely to lead to follow up work.
The paper combines logical reasoning and statistical methods to improve knowledge graph completion. Rules are mined from the KG using AMIE and recursive backward steps are taken, using the mined rules, to determine if a fact is true. The reviewers agree that the paper can be improved by explaining more details of the method to make it more easy to understand.  
There is a clear consensus over all reviewers that this is a very strong empirical analysis, with actionable insights that should prove quite useful both to researchers and practitioners. I have no doubt that many will use it as a reference when implementing and using RL algorithms (especially since the authors said they would release their code).  This is thus a clear accept, that in my opinion would deserve an oral presentation, so as to better disseminate its key findings.
This paper described a system for deriving PDDL (Planning Domain Description Language) operator descriptions from unlabeled visual image pairs.  The goal is to construct STRIPS like descriptions with preconditions, add lists and delete lists for operators that can explain the state transitions seem in the image pairs.  This is combined with a neural form of inductive logic programming (ILP) which historically performs a similar task from logical descriptions rather than images.  While this topic is appropriate for ICLR, the work is fairly incremental and the experiments are limited to the 8 puzzle which, according to a reviewer, is the easiest of the tasks.  In spite of boarder line scores, no rebuttal was provided.  So my recommendation is to not accept the paper.
In the paper, the authors propose a new method for estimating the mutual information based on a neural network classification that is fairly straight forward. The proposed method compares relatively well with known methods for estimating mutual information with a very large number of samples. The main issue of this classifier (a neural network) is that it requires that a classifier that discriminates between x, y pairs coming from p(x,y) and x, y pair coming from p(x)p(y) (this is done via reshuffling). The reviewers point out that the procedure is interesting, but it does not perform significantly better than the other proposed methods.  Also, I want to add that the proposed method is trained by using a given NN trained with 20 epochs and a mini batch of 64. This is a significant issue because if we train the NN to reduce the validation error the posterior probability estimates are typically overconfident a significant work is being done to calibrate them. Why 20? How do we select this number if we cannot use a validation set? With less training example does 20 also work? This is very relevant because in the areas in which p(x,y)/p(x)p(y) is low for very high MI values getting these estimates correctly is critical. The classifier does not need to perform accurately in classification, but an estimation of the posterior probability and NNs will tend to be overconfident here and provide a biased estimate for these values. It will also provide an overestimate probability in the area that both p(x,y) and p(x)p(y) are high.   Finally, the authors reference the paper by Nguyen, Wainwright, and Jordan, but they do not acknowledge how that paper actually estimates log(p(x,y)/p(x)p(y)) similarly. That paper is very general and theoretical, and this paper can only be understood as a particular implementation of their solution. I think the authors missed that point in their paper. Also, I think the authors should acknowledge the papers that have come before using nearest neighbor or histograms for entropy estimation. 
The paper proposes a new measure of difference between two distributions using conditional transport. The paper considers an important problem.  However, some major concerns remain after the discussion among the reviewers. In particular, the paper focuses on the evaluation on a toy dataset. It is unclear whether the claim carries over to large real datasets. The presentation of the paper also needs substantial improvement. 
This paper extends recent work (Whittington & Bogacz, 2017, Neural computation, 29(5), 1229 1262) by showing that predictive coding (Rao & Ballard, 1999, Nature neuroscience 2(1), 79 87) as an implementation of backpropagation can be extended to arbitrary network structures. Specifically, the original paper by Whittington & Bogacz (2017) demonstrated that for MLPs, predictive coding converges to backpropagation using local learning rules. These results were important/interesting as predictive coding has been shown to match a number of experimental results in neuroscience and locality is an important feature of biologically plausible learning algorithms.  The reviews were mixed. Three out of four reviews were above threshold for acceptance, but two of those were just above. Meanwhile, the fourth review gave a score of clear reject. There was general agreement that the paper was interesting and technically valid. But, the central criticisms of the paper were:  1) Lack of biological plausibility The reviewers pointed to a few biologically implausible components to this work. For example, the algorithm uses local learning rules in the same sense that backpropagation does, i.e., if we assume that there exist feedback pathways with symmetric weights to feedforward pathways then the algorithm is local. Similarly, it is assumed that there paired error neurons, which is biologically questionable.  2) Speed of convergence The reviewers noted that this model requires many more iterations to converge on the correct errors, and questioned the utility of a model that involves this much additional computational overhead.  The authors included some new text regarding biological plausibility and speed of convergence. They also included some new results to address some of the other concerns. However, there is still a core concern about the importance of this work relative to the original Whittington & Bogacz (2017) paper. It is nice to see those original results extended to arbitrary graphs, but is that enough of a major contribution for acceptance at ICLR? Given that there are still major issues related to (1) in the model, it is not clear that this extension to arbitrary graphs is a major contribution for neuroscience. And, given the issues related to (2) above, it is not clear that this contribution is important for ML. Altogether, given these considerations, and the high bar for acceptance at ICLR, a "reject" decision was recommended. However, the AC notes that this was a borderline case.
the authors demonstrated that vanilla RNN, GRU and LSTM compute at each timestep a hidden state which is the sum of the current input and the weighted sum of the previous hidden states (weights can be either unit or complicated functions), when sigmoid and tanh functions are replaced by their second order taylor series each. they refer to the first term as token level and the second term as sequence level, and claim that the latter can be thought of as summing n gram features in the case of GRU & LSTM due to the complicated weight matrices used for the weighted sum, largely arising from the gating mechanisms.   the reviewers are largely unsure about the significance of the findings in this paper due to a couple of reasons with which i agree. first, it is unclear whether the proposed approximation scheme is enough to capture much of what happens within either GRU or LSTM. if we consider a single step, it s likely fine to ignore the O(x^3) term arising from either sigmoid or tanh, but when unrolled over time, it s unclear whether these error terms will accumulate or cancel each other. without either empirically or theoretically verifying the sanity of this approximation, it s difficult to judge whether the authors  findings are specific to this approximation scheme or do indeed reflect what happens within GRU/LSTM.   second, because the authors have used relative simple benchmarks to demonstrate their points, it is difficult, if not impossible, to tell whether the authors  findings are about the datasets themselves (which are all well known to be easily solvable or solvable very well with n gram classification models and n gram language models) or about GRU/LSTM, which is related to the first weakness shared by the reviewer. the observations that n gram models and simplified GRU/LSTM models work as well as the original GRU/LSTM models on these datasets might simply imply that these datasets don t require any complicated interaction among the tokens beyond counting n grams, which lead to the original GRU/LSTM trained to be simplified (n gram detectors.)   that said, i still believe this direction is important and is filled with many interesting observations to be made. i suggest the authors (1) verify the efficacy of their approximation scheme (probably empirical validation is enough, and (2) demonstrate their point with more sophisticated problems (carefully designed synthetic datasets are perfectly fine.)   
The paper introduces a new step size rule for the extragradient/mirror prox algorithm, building upon and improving the results of Bach & Levy for the deterministic convex concave setups. The proposed adaptation of EG/Mirror prox   dubbed AdaProx in the submitted paper   has the rate interpolation property, which means that it provides order optimal rates for both smooth and nonsmooth problems, without any knowledge of the problem class or the problem parameters for the input instance. The paper also demonstrates that the same algorithm can handle certain barrier based problems, using regularizers based on the Finsler metric.  The consensus of the reviews was that the theory presented in the paper is solid and interesting. The main concerns shared by a subset of the reviews were regarding the practical usefulness of the proposed method. In particular, the method exhibits large constants in the convergence bounds and cannot handle stochastic setups. Further, the empirical evidence provided in the paper was deemed insufficient to demonstrate the algorithm s competitiveness on learning problems. If possible, the authors are advised to provide more convincing empirical results in a revised version, or, alternatively, to tone down the claims regarding the practical performance of the method.
This paper proposes an approach of generating mathematical expressions with a recurrent neural network, which is trained with risk seeking policy gradient to maximize the quality of best examples rather than average examples.  The proposed approach also enables easily incorporating domain knowledge or constraints to avoid illegal or redundant expressions.  In extensive experiments, the proposed method is shown to significantly outperform strong baselines, including commercial software.  All of the reviewers find the work interesting and relevant, and there are no major concerns or issues after discussion.  The topic is also of interest to a wide range of audience in the ICLR community.
I thank the authors both for going the extra mile in doing further experiments for their response, and making the efforts to synthesize the main comments and concerns of the reviewers.  Overall, I m pretty sympathetic to the idea that syntactic and semantic representations should be very helpful to learning sentence embeddings. They provide a form of scaffolding. But a reviewer notes and I think anyone will admit in 2020 that contextual language models like BERT also provide much of this scaffolding, and it falls to the paper author to provide convincing evidence that using external parsers is valuable and necessary in this quest. In this, the current paper seems to fall somewhat short.  Pros:    Clearly and honestly written paper    Good exploration of value of constituency & dependency parse representations    Exploits recent work in contrastive learning  Cons:    Insufficient novelty    Experimental comparisons not well controlled – too much apples and oranges.     No comparisons of inference speed tradeoffs    Value of exploiting explicit syntax is too much assumed rather than explored    It s not established that use of explicit syntax really delivers versus alternatives such as contextual language models  Several of the reviewers felt that this paper was a fairly limited extension of L & L 2018, without any clearly novel contribution. The issue of comparability in results is complicated. There is a reason to move to a new standard corpus, rather than privileged people passing around archived copies of the old BooksCorpus, and I think your additional experiments show the results are "near enough" but there would still be much more archival value in a new paper having a set of comparable results on a new corpus. The big question of whether to do this or use BERT is better addressed in your additional experiments presenting a random projection of BERT to a comparable higher dimensional space. But unfortunately these results further weaken the clarity of the case for needing to head in the direction of this paper rather than just using a large pre trained contextual LM.
This paper proposes a method for learning non separable Hamiltonian dynamics by including a state of the art symplectic integrator (Tao, 2016) into the model training pipeline. This is a nice improvement on the past work that primarily addressed separable Hamiltonians. The reviewers agree that the paper is well written and that the empirical evaluation is solid. The paper was further improved during the discussion period by incorporating the reviewers  feedback. For this reason I am happy to recommend this paper for acceptance.
This paper introduces the multiple manifold problem   in a simple setting there are two data manifolds representing the positive and negative samples, and the goal is to train a neural network (or any predictor) that separates these two manifolds. The paper showed that this is possible to do with a deep neural network under certain assumptions   notably on the shape of the manifold and also on the ability of the neural network to represent certain functions (which is harder to verify, and only verified for a 1 d case in the paper). The optimization of neural network falls in the NTK regime but requires new techniques. Overall the question seems very natural and the results are reasonable first steps. There are some concerns about clarity that the authors should address in the paper.
This paper focuses on a notion of privacy in learning representations.   One of the primary concerns of the reviewers was clarity of the writing and results. Numerous concerns are mentioned in the reviews, and also more engagement with the fairness literature was desired. One reviewer felt that some of the claims in the paper were unsubstantiated, for example: understanding the sanitization process in a human understandable visual way", "integration of a notion of interpretability". It was felt that the changes required were more than could be expected for a camera ready version. The authors are recommended to revise the paper with a particular eye for clarity to a new reader.  The notion and measurement of privacy was also considered to be somewhat shaky. It is understood that the nature of privacy considered in this paper is different from differential privacy. That said, the latter is a rigorous definition, and the one in this paper seems to be rather empirical in nature. There are no formal guarantees in terms of privacy preservation, and it is not clear whether the representations could leak information when evaluated with a different network. As privacy is a mission critical property, some justification of why the heuristic measurement of privacy is acceptable.  As a side note, the authors should consider using the \citep command for parenthetical citations in the text.
All three reviewers are positive, and the authors have addressed essentially all the questions raised by the reviewers. The main insight of the paper is clear, and the empirical results are good, so a spotlight is deserved.
The paper has two contributions. A novel benchmark for clinical multi modal multi task learning based on the already released MIMIC III and a multi modal multi task machine learning model. While the paper does show value in providing a curated benchmark and combining/unifying existing approaches to a timely problem, the reviewers agree that the paper provides insufficient novelty to warrant publication.
This paper considers the RL problems where actions and observations may be delayed randomly. The proposed solution is based on generating on policy sub trajectories from off policy samples. The benefits of this approach over standard RL algorithms is clearly demonstrated on MuJoCO problems. The paper also provides theoretical guarantees.  This paper is well written overall and technically strong. The majority of the reviewers find that this paper would constitute a valuable contribution to the ICLR program. 
All reviewers explain in detail, why they think the paper should not be accepted. Besides fixing an initially criticized format violation, the authors did not respond to any of the concerns raised the reviewers, and in fact, they partially agree that more work in another direction needs to be done.  
This submission builds on recent work by Bengio et al. (2020) to learn disentangled representations of causal mechanisms. The main innovation in this work is that the authors propose to use compare the generalization gap between a model A >B and its causal inverse B >A to determine the true causal mechanism.   Reviewers  are in consensus that this paper in its current form is not ready for publication. Aside from issues with writing, and comparatively weak experimental results, the reviewers noted significant technical problems in the writing and proofs. As noted by R3, Proposition 1 mentions "unbiased estimator of correct causality direction", which is a nonsensical combination of words in absence of a definition of the causality direction as a random variable. Moreover the proof in the appendix only shows that that 0   KL(P(A|B),Q(A|B)) < KL(P(B|A),Q(B|A)) when P(A|B)   Q(A|B) and P(B) !  Q(B). As R3 also notes, the authors also do not clearly distinguish between the true data distributions (which are not known), datasets that are sampled from these distributions, and approximations to these distributions that are learned from finite sample sets.    The metareviewer would suggest that the authors either improve the level of precision of the formalization of their approach, and/or more explicitly position it as an empirical study, in which case more substantive experimental evaluation would strengthen the contributions.
The paper introduces new methods and building blocks to improve hyperbolic neural networks, including a tighter parameterization of fully connected layers, convolution, and concatenate/split operations to define a version of hyperbolic multi head attention. The paper is well written and relevant to the ICLR community. The proposed methods offer solid improvements over previous approaches in various aspects of constructing hyperbolic neural networks and also extends their applicability. As such, the paper provides valuable contributions to advance research in learning non Euclidean representations and HNNs. All reviewers and the AC support acceptance for the paper s contributions. Please consider revising your paper to take feedback from reviewers after reubttal into account.
This work proposes a method, inspired by Cellular Automata, to generate 3D objects in voxel space. By *only* using local update rule for each location, the method can probabilistic generate high resolution models of everyday objects in the dataset. Due to the ability to incrementally generate details, the quality of the samples are seemingly higher than tradition approaches using Voxel based GANs.  Most reviewers and myself agree this is a strong and interesting paper that will spark good discussion in the ICLR community. It is also well written and ideas are clearly explained. During the review process, the authors improved the work by conducting additional experiments to analyze the sensitivity of hyper parameters and took in and incorporated various suggestions from the reviewers. After the revision, I believe the work to be in good shape to be accepted at ICLR2021, and I will recommend that this paper be accepted (Poster).
# Quality:  The paper makes a good job of presenting the proposed algorithm, which seems interesting and solid. However, the paper fails to place the proposed approach in the larger context of the existing literature. In addition, only qualitative results are presented, without any comparison.  As such, it is impossible to really understand the goodness of the proposed approach.  # Clarity:  The paper is generally well written and clear.  # Originality: The proposed approach is novel to the best of the reviewers and my knowledge.  # Significance of this work:  The paper deal with a very relevant and timely topic. However, as stated by the authors themself the paper is not concerned with high dimensional systems, which is what would really differentiate this work compared to existing literature. In addition, the paper has no quantitative results nor comparisons against previous literature, and does not evaluate any of the standard benchmarks.  # Overall: There is disagreement from the reviewers regarding the acceptance of this paper, and the overall score is very borderline. After thoroughly reading the paper, I agree with the evaluation of Reviewer 2 and 3 regarding the lack of comparisons and thus lean towards rejection.  
This paper proposes a representation learning approach from cardiac signals, which adopts contrastive learning to incorporates knowledge on patient specificity. This problem is highly motivating because of potential application to medicine and healthcare and large amounts of accumulating unlabeled physiological data. The presentation needs to be substantially improved – e.g., lack of clear description of the contribution, the details such as input data and network architecture, a clear description of the downstream tasks, missing explanations of equations, etc – some of which were addressed in the revision. Major concerns include lack of comparison with relevant prior methods developed for cardic signals, and the need for further refinement of domain knowledge based intuitions. 
This submission explores how certain common padding choices can induce spatial biases in convolutional networks. It looks into alternative padding schemes which mitigate these issues and demonstrates significant performance improvements in widely used convnets. Reviewers generally agreed that this is an important point that should be more widely understood in the community, and that the proposed changes are relatively simple to adopt, so this work is likely to be impactful. Most reviewers thought the paper was well written, describing the problem well, and the analysis well executed. Most reviewers acknowledged that most of the weaknesses described in their initial reviews were well addressed by the authors  responses and manuscript updates. Given the strength of the analysis and the impact for many practitioners, I recommend the submission be accepted with a spotlight presentation.
The reviewers are concerned about the novelty of the proposed learning rate schedule, the rigor of the empirical validation, and the relationship between the results and the discussion of sharp vs. local minima. I invite the authors to incorporate reviewers  comments and resubmit to other ML venues.
The rebuttal (revisions, and released code) very successfully addressed all the major concerns the reviewers had.  Pros: The dynamics distance function is a very neat, simple (which is good in this case) idea that is theoretically sound, has proven to perform well in thorough experimental results, and that can be broadly applied.  Cons: None
### Paper summary  This paper investigates theoretically and empirically the effect of increasing the number of parameters ("overparameterization") in GAN training. By analogy to what happens in supervised learning with neural networks, overparameterization does help to stabilize the training dynamics (and improve performance empirically). This paper provides an explicit threshold for the width of a 1 layer ReLU network generator so that gradient ascent training with a linear discriminator yields a linear rate of convergence to the global saddle point (which corresponds to the empirical mean of the generator matching the mean of the data). The authors also provides a more general theorem that generalizes this result to deeper networks.  ### Evaluation The reviewers had several questions and concerns which were well addressed in the rebuttal and following discussion, in particular in terms of clarifying the meaning of "overparameterization". After discussing the paper, R1, R2 and R4 recommend acceptance while R3 recommends rejection. The main concern of R3 is that the GAN formulation analyzed in the paper is mainly doing moment matching between the generator  distribution (produced from a *fixed* set of latent variables z_i) and the empirical mean of the data. R3 argues that this is not sufficient to "understanding the training of GANs". At least two aspects are missing: how the distribution induced by the generator converges according to other notion of divergence (like KL, Wasserstein, etc.); and what about the true generator distribution (not just its empirical version from a fixed finite set of samples z_i)? While agreeing these are problematic, the other reviewers judged that the manuscript was useful first step in understanding the role of overparameterization in GANs and thus still recommend acceptance. And importantly, this paper is the first to study this question theoretically.  I also read the paper in more details. I have a feeling that some aspects of this work were already developed in the supervised learning literature; but the gradient descent ascent dynamic aspect appears novel to me and the important question of the role of overparameterization here is both timely, novel and quite interesting. I side with R1, R2 and R4: this paper is an interesting first step, and thus I recommend acceptance. See below for additional comments to be taken in consideration for the camera ready version.  ### Some detailed comments   Beginning of section 2.3: please be clearer early on that you will keep V fixed to a random initialization rather than learning it. The fact that this is standard in some other papers is not a reason to not be clear about it.   Theorem 2.2: in the closed form of the objective when $d$ is explicitly optimized, we are back to a more standard supervised learning formulation, for example (5) could look like regression. The authors should be more clear about this, and also mention in the main text that the core technical part used to prove Theorem 2.2 is from Oymak & Soltanolkotabi 2020 (which considers supervised learning). This should also a bit more clear in the introduction   it seems to me that the main novelty of the work is to look at the gradient descent dynamic, which is a bit different than the supervised learning setup, even though some parts are quite related (like the full maximization with respect to $d$).   p.6 equation (8): typo   the  $ \mu d_t$ term is redundant and should be removed as already included from $\nabla_d h(d,\theta)$.   p.7 "numerical validations" paragraph: Please describe more clearly what is the meaning of "final MSE". Is this a global saddle point (and thus shows the limit of the generator to match the empirical mean), or is this coming from slowness of convergence of the method (e.g. after a fixed number of iterations, or according to some stopping criterion?). Please clarify. 
This paper introduces C learning, an approach to integrate temporal abstractions to value based methods. Specifically, it uses accessibility functions that estimate horizon aware value functions for goal reaching RL problems. Such an approach allows trading off reliability and speed. After careful consideration I’m recommending the acceptance of this paper. The main weaknesses raised by the reviewers were addressed during the rebuttal, including the improvement of presentation and the introduction of new experiments and baselines. There were not many actionable criticisms left after the discussion and the reviewers acknowledged that the paper has improved since its first version.  For the final version of the manuscript, I recommend the authors to further take R2’s comments/suggestions into consideration. Further incorporating the discussion about TDMs in the main text will improve clarity, better position the paper, and increase its likelihood of having impact. 
This is a solid paper providing the first theoretical convergence result for NAS and showing promising empirical results. The authors  proposed GAEA method can be combined with different types of weight sharing algorithms (DARTS, PC DARTS, etc) and is likely to reduce the impact of the architecture discretization step due to finding sparser solutions. I clearly recommend acceptance and would expect this to make a nice spotlight.
The paper received borderline and negative reviews but has raised many questions and discussions, showing that the paper has some merit. Many concerns were however raised on various aspects of the paper such as mathematical rigor, clarity, and motivation of manifold regularization that is too disconnected from the robustness to local random perturbation which is encouraged by the method. The rebuttal addresses some of these comments and the reviewers have appreciated the detailed answer. Yet, it was not sufficient to change the reviewer s opinions.  In its current form, the paper is not ready for publication and the area chair agrees with most of the reviewer s comments. He recommends a reject, but encourage the authors to take into account the feedback from the reviewer before resubmitting to a future venue.
The paper proposes a self supervised method to predict the gist features of image frames during navigation of an agent supervised by depth and egomotion. The features are retargeted to train navigation policies and outperform previous methods or other pretraining schemes. The idea is related to self supervised by feature prediction but is employed in a zone level as opposed to isolated image level. Though reasonable, in the context of the recent abundance of self supervised prediction papers in various level of spatial visual granularity, the paper may not be of sufficient novelty to present a sizable contribution for ICLR acceptance. 
This paper studies the tensor principal component analysis problem, where we observe a tensor T   \beta v^{\otimes k} + Z where v is a spike and Z is a Gaussian noise tensor. The goal is to recover an accurate estimate to the spike for as small a signal to noise ratio \beta as possible. There has been considerable interest in this problem, mainly coming from the statistics and theoretical computer science communities, and the best known algorithms succeed when \beta \geq n^{k/4} where n is the dimension of v. The main contribution of this paper is to leverage ideas from theoretical physics and build a matrix whose top eigenvector is correlated with v for sufficiently large \beta using trace invariants. On synthetic data, the algorithms achieve better performance than existing methods.  The main negative of this paper is that it is not so clear how tensor PCA is relevant in machine learning applications. The authors gave some references to applications of tensor methods, but I want to point out that all of those works are about using tensor decompositions, which despite the fact that they are both about tensors, are rather different sorts of tools. Many of the reviewers also found the paper difficult to follow. I do think exposition is particularly challenging when making connections between different communities, as this work needs to introduce several notions from theoretical physics. I am also not sure how novel the methods are, since a somewhat recent paper Moitra and Wein, "Spectral Methods from Tensor Networks", STOC 2019 also uses tensor networks to build large matrices whose top eigenvalue is correlated with a planted signal, albeit for a different problem called orbit retrieval. 
This paper received borderline scores, R1, R3, R4 gave a score of 6 and recommended a borderline acceptance. R2 provided by far the most detailed review and recommended a score of 5 (i.e., borderline reject). After the rebuttal, R2 comments, "I believe that the paper is still below the acceptance threshold, although only marginally". Overall, I concur with R2. The reasons are detailed below:   The paper proposes a method for communication between two agents, wherein one agent actuates its joints to communicate intent. Intuitively, this resembles making a gesture. The paper considers the setting of a discrete number of intents. The sender agent is modeled as a neural network that takes as input the intent and outputs a trajectory of joints. The receiver observes a noisy version of the trajectory and outputs the intent. The parameters of the sender policy and receiver discrimination network are optimized to maximize classification accuracy. It is shown that if the intents are sampled from Zipf distribution and trajectories are penalized based on their energy, then a receiver agent initialized from scratch is better at inferring the intent from a pre trained speaker agent, as opposed to when the distribution is uniform or when the energy regularization is not used (Figure 2).  Further, section 5.2 shows that when the listener is provided with the energy of the trajectory then it is better at recognizing the intent as opposed to being provided with the entire trajectory when a number of intents are small. With a larger number of intents (N 10), the performance is at chance accuracy.   The biggest challenge with the paper is that it is very poorly written. Large parts of the method and experimental setup are in the appendix (A.2 / A.3), which makes it hard to understand the paper. Section 4.2 is rather confusing because the ideas introduced are not used for training, but simply for evaluation. Further, the authors point out in the rebuttal that torque curriculum is not required, but it is still there in the paper and makes it more confusing. I recommend the authors to substantially rewrite the paper and focus on relevant parts instead of philosophical arguments. Lastly, I am confused by results in Table 2   the authors mention in the text that with 10 intents, intent identification is at chance (i.e. 34% accuracy), but the table shows 56% accuracy. A clarification would be helpful here.   The problem of communicating intents via gestures, when the agents are unaware of mapping from intents to gestures is an exciting area of research. From the perspective of emergent gestures, this paper has a novel contribution. However, the settings are toy and even in such a setup, the results are underwhelming. The assumptions that make the setup toy are: the listener agent knows about all joint locations of the sender (with some noise) and also has access to the energy exerted by the agent. Without access to energy, the performance is poor. In real world scenarios, these are big assumptions. Furthermore, even when the energy is known For instance, even when the number of intent is small (i.e.,  N 10,) the performance is bad. The authors argue that is due to local minima in the optimization   but that s exactly where the contribution could have been.   I will reiterate, that the authors claim their contribution is in using energy minimization + Zipf intent distribution as a mechanism for communicating intent   which I agree to. However, as pointed out earlier, the paper is not well executed or written and therefore I recommend rejection.   
The paper proposes to maximizing the mutual information to optimize the bin for multiclass calibration. The idea, technique, and presentation are good. The paper solves some multiclass calibration  issues. The author should revise the paper according the reviewer s comments before publish.
The paper proposes a model to defend against multiple lp norm attacks by classifying those attacks. The reviewers raised several concerns about the methodologies. Furthermore, it s not clear how the proposed algorithm can deal with an unseen attack (e.g., only trained on l1, l_infty attacks but encounter l2 attack in the testing phase). The assumption that the attack types are known beforehand is restricted.  
The paper proposes a variant of MAML for meta learning on tasks with a hierarchical tree structure. The proposed algorithm is evaluated on synthetic datasets, and it compares favorably to MAML. The reviewers identified several significant weaknesses, including: (1) the experimental evaluation is limited, and it only includes small synthetic datasets; (2) the proposed algorithm is incremental over MAML. The reviewers agreed that the paper cannot be accepted in its current form. I recommend reject.
The paper presents PIVEN, a deep neural network that produces a prediction interval in addition to a specific point prediction.  PIVEN is distribution free and does not assume symmetric intervals.    All the reviewers agree that the paper investigates an important problem and the paper is well written. The reviewers also identified a couple of weak points, namely:   Novelty: The key idea seems to a combination of prediction loss (common) and prediction interval loss which has been proposed by Pearce et al. 2018.   Claims that PIVEN outperforms existing methods (QD and DE) empirically as some of the improvements seem marginal.   Given these concerns, I think the current version falls a bit short of the acceptance threshold unfortunately. I encourage the authors to revise the draft and resubmit to a different venue. 
  This paper proposes two mechanisms, SelfNorm (used during training and inference) leveraging an attention based recalibration of mean and standard deviation for instance normalization, and CrossNorm which performs cross channel swapping of mean/stdev. Is is shown that the combination (often combined with AugMix) performs well across several datasets in terms of model robustness. Overall the paper has strength in the fact that the method is interesting, simple to implement, and modular. However, reviewers brought up a number of issues including the overstated motivation/writing, lack of clarity, and most importantly need for clear experimental results (comparing to uniform/standard baselines) and identification of the separate mechanisms. It is especially uncertain why it is necessary that they are used *together* (often with AuxMix as well) to obtain the strong performance. As a result, the score for this paper is borderline, tending towards a weak acceptance.   It is appreciated that the authors provided a lengthy rebuttal, including new results in a different domain (NLP); however, the reviewers agreed that not all of the concerns were addressed.  After a lengthy discussion, all of the reviewers agree that while the method is simple, modular, and effective when combined (hence the positive scores from some reviewers), the authors fail to describe the underlying reason for the method s gains, especially with respect to the individual parts (SelfNorm vs. CrossNorm) and why the results only come when these rather independently derived modules are used together. The exposition of the experimental results, with differing baselines/conditions that make it very hard to understand where the effect is coming from, exacerbates this issue.   As a result of these concerns, I recommend rejection of this paper. However, the method is interesting and results promising, so I hope that the authors can clarify the writing and improve the presentation of the results (specifically separating out the effects of SelfNorm and CrossNorm, as well as analyzing how they interact together to improve results) and submit to a future venue. 
This is a well written paper addressing a challenging problem with an original approach.  While one reviewer claims there is not a strong call for calibration of regression tasks, this may well be because methods don t exist.  Certainly, calibration is a critical tool for classification.  The major failing of the paper, however, is the empirical evaluation.  Given that no prior work exists, it is arguably OK to not do this, but one could easily reject the paper on this issue alone, as AnonReviewer4 was inclined to do.  One reviewer, however, thought highly of the paper, which bumped up its average score, more than I think it should have got (due to the poor experimental work).  The abstract could be improved by mentioning the use of kernels, the nature of this solution is a substantial part of the paper.
In this paper, the authors show the effect of RNI on the expressive power of GNN for the first time, where the RNI was initially proposed in Sato et al. 2020. Overall, I like the idea of random node initialization because it is simple, effective, and theoretically well founded. The key concern was that the novelty over the Sato s paper and the reviewers were still not convinced by the response. Therefore, the paper is still below the acceptance threshold.  I strongly encourage authors to revise the paper based on the reviewer s comments and resubmit it to a future venue.  
This paper proposed a way to combine LSTMs with Fast weights for associative inference.  While reviewers had concerns about comparison with Ba et al., and experimental results, the authors addressed all the concerns and convinced the reviewers. The revision strengthened the paper significantly. I recommend an accept.
This paper received 4 reviews with mixed initial ratings: 5, 6, 4, 4. The main concerns of R1, R4 and R2, who gave unfavorable scores, included: insufficient evaluation (lack of experiments on public datasets, small sample size), an ad hoc nature and overall limited novelty of the method, a number of issues with the presentation. In response to that, the authors submitted a new revision and provided detailed answers to each of the reviews separately. After having read the rebuttals, the reviewers (including R3, who initially gave a positive rating) felt that this work overall lacks methodological novelty and does not meet the bar for ICLR. As a result, the final recommendation is to reject.
I thank the authors for their submission and very active participation in the author response period. I want to start by stating that I rank the paper higher as is currently reflected in the average score of the reviewers. The reasons for this are that a) R2 and R3, while responding to the author s rebuttal, do not seem to have updated their score or indicated that they want to keep their initial assessment of the paper   in particular, R2 has acknowledged that additional experiments by the authors were useful and results on KeyCorridorS4/S5R3 are nice, and b) I disagree with R2 s sentiment that MiniGrid is not a suitable testbed   it is by now an established benchmark for evaluating RL exploration and representation learning methods (see list of publications on https://github.com/maximecb/gym minigrid). However, despite my more positive stance on the paper, I fully agree with R1 and R2 that a comparison to EC is needed in order to shed light into which factors of EC SimCLR actually led to improvements in comparison to RIDE. I therefore recommend rejection, but I strongly encourage the authors to take the feedback from the reviewers and work on a revised submission to the next venue.
This paper proposed using the state bisimulation metric to learn invariant representations for reinforcement learning.  The method is generic, effective, and is supported by both theoretical and experimental results.  All reviewers and I think this is a strong contribution to the area.
Knowledge distillation (KD) has been widely used in practice for deployment.  In this paper, a variant of KD is proposed: given a student network, an auxiliary teacher architecture is temporarily generated via dynamic additive convolutions; dense feature connections are introduced to co train the teacher and student models. The proposed method is novel and interesting. Empirical results showed that the proposed method can perform better than several KD variants.  However, it is unclear why the proposed method works, although the authors tried to address this issue in their rebuttal.   Besides this,  a bigger concern on this work is that it missed a comparison with a recent approach in [1] which looks much simpler and performs significantly better on similar experiments.  In [1], their ResNet50 (0.5x) is smaller than the student model in this paper (which used more filters on the top) but showed much stronger performance on both relative and absolute improvements over the same baseline (training from scratch) for the ImageNet classification task. On the technical side, the method in [1] simply uses the original ResNet50 as the teacher model,  and the student model ResNet50 (0.5x) progressively mimics the intermediate outputs of the teacher model from layer to layer. [1] also contains a  theoretic analysis  (mean field analysis based) to support their method. Comparing with the method in [1], the proposed method here is more complicated, less motivated, and less efficient.   [1] D. Zhou, M. Ye, C. Chen, T. Meng, M. Tan, X. Song, Q. Le, Q. Liu and D. Schuurmans. Go Wide, Then Narrow: Efficient Training of Deep Thin Networks. ICML 2020.
Although the connection between randomized smoothing and PDE revealed in this paper is an interesting direction to explore, the method proposed unfortunately is not certified. The method could work as a good empirical defense since the smoothed classifier could be learned more efficiently. 
While this paper was perceived as being fairly well written, the level of novelty and the evaluation were seen as weak by many reviewers. The aggregate opinions across reviewers is just too low to warrant an acceptance rating by the AC. The AC recommends rejection.
The authors introduce vPERL, a model that generates an intrinsic reward for imitation learning. vPERL is trained on demonstrations to minimise a variational objective that matches a posterior formed by "action backtracking" and a forward model, with the intrinsic reward coming from the reward map. The authors might be interested in related work on few shot imitation learning: e.g., "One shot imitation learning", Duan et al, 2017, "Watch, try learn: meta learning from demonstrations and rewards", Zhou et al 2019. As all reviewers pointed out, and I can confirm, the paper is quite tricky to understand in its present form, and would very much benefit the writing being re visited to more clearly express the ideas within (in particular, section 3, which is the core of the contributions).  
The paper considers differentially private federated learning   a well motivated problem. The proposed algorithm is a simple modification to existing methods, e.g., DP FedAvg, but uses a different DP mechanism for noise adding.  The reviewers liked the motivation but criticized the work for its incremental nature and for somewhat overselling the contributions.  Pros:    The paper used advanced Renyi DP accounting to get a stronger privacy utility tradeoff.    The experimental results improve over cp sgd that uses Binomial mechanisms  Cons:     It is a bit incremental in its contribution.  The main contribution is to applying "discrete Gaussian mechanism" to the federated learning problem for the interest of reducing the communication cost.   Discrete Gaussian mechanism and its RDP analysis are both from existing work.    The improvement in privacy utility tradeoff over cp sgd seems to be due to that the discrete Gaussian mechanism has an RDP bound, which plugs right into the subsampling bound and moments accountant.    It is unclear whether the improvement is coming from the different noise or a stronger privacy accounting.  Notice that the privacy accounting of Binomial mechanism in the initial cp sgd paper was rather crude, thus a fair comparison would be to also conduct an RDP analysis for the Binomial mechanism.    Overall, there weren t sufficient support among the reviewers and the experimental results alone are not so groundbreakingly strong to carry the paper single handedly. 
**Overview** The paper provides a simplified offline RL algorithm based on BCQ. It analyzes the algorithms using a sampling based maximization of the Q function over a behavior policy for both Bellman targets and for policy execution   the EMaQ. Based on this, the paper then proposes to use more expressive autoregressive models (MADE) for learning the behavior policy from replay buffer data. The methods work well for harder tasks in the D4RL benchmark.   **Pro**    The method is relatively novel    Algorithms are simple modifications of existing ones   Empirical results are strong, matching or exceeding BEAR on D4RL while at the same time matching SAC for online learning   Work for both and offline   ablation study on the choice of generative model for μ(a|s)  **Con**   The current form of the complexity measure is somewhat not practical.   Theoretical results are not strong enough   Algorithmic contributions appear incremental  **Recommendation** The paper is on the borderline. It contributes simple and nice algorithmic ideas and these ideas work well empirically. These results demonstrate that a good choice of the behavior policy generative model is important for some tasks. At the same time, the reviewers are concerned about the theoretical parts, e.g., issues relates new complexity measure. Overall, the meta reviewer believes that the paper might not be in a status ready for publication.
This work combines deep generative models (variational autoencoders, FragVAE) and multi objective evolutionary computation for molecular design. They use a multilayer perceptron as a predictor for properties. Evolutionary operations are used to explore the latent space of the generative model to produce novel competitive molecules. Experiments are executed to show the effectiveness of the proposed method with respect to Bayesian optimization based methods.  Strengths:  1   Combines multi objective evolutionary computation and deep generative modeling, which is a promising approach to tackle multi objective optimization in structured spaces.  Weaknesses:  All the reviewers agree that the paper is not yet ready for publication. They point out the following areas to improve:  1   The lack of details and clarity in the method.  2   The experimental section needs to be improved. The evaluation metrics and baselines are weak.  3   Describe better and more clearly the novelty of the proposed approach with respect to previous work in the area.
The reviewers had raised a number of concerns which were mostly addressed during the discussion phase thanks to the additional experiments/explanations that the authors provided. However, some of the reviewers are not yet convinced about the main claims of the paper. While the paper provides a number of interesting/important/novel observations, which are obtained using an extensive set of carefully designed experiments, there are still some ambiguities about the main claims that may need further investigation/ justification.   One general issue with the current version is lack of clarity. I recommend that the authors revise the writing of the paper and make an effort to better explain/justify the main claims, ideas, and setups in the paper. Perhaps it would help to add a section early in the paper and define/review the basic concepts/definitions.  I also encourage the authors to reason about their main claims, choice of loss function (and why it is the right choice), and their experimental setups.   As an example, the authors should further investigate the impact of their proposed regularization on semantic representations.  The authors interpret the (distribution of) class information as “semantic” content/features. Indeed, when considering data with a variety of average case perturbations,  one could argue that semantic features like brightness or snow are actually uniform over classes, which might means that class selectivity could not appropriately capture the effect of those features. To evaluate semantic robustness,   it seems necessary to find a way to isolate specific semantic features in the data, i.e. by changing from snow to rain in corresponding images, rather than looking at the performance on the same brightness subset for different levels of their regularization parameter alpha.  Hence there is a need for further investigation on the effectiveness of class selectivity.   Some of the reviewers have indicated that differences in the class selectivity curves in Figures 1,2 appear marginal (e.g. at most 3% difference). Hence, additional experiments (with other data sets) could be beneficial in this regard.      
This paper studies the problem of visual question answering in multi turn dialogues. The proposed method is to identify relevant dialog turns as a path in a semantic graph that connects the dialogue turns. Empirical performance of the proposed method is strong. Reviewers concerns have been compressively addressed. Overall, the paper has novelty, and explores an interesting direction in this line of work.
Reviewers were concerned with the novelty, although appreciated sota results in extensive experiments.
The paper proposes a hyper net method for multi objective optimization, which trains a neural network that maps preference vector to the corresponding Pareto solution. The proposed idea is interesting and useful, although the evaluation of the work is not overwhelming convincing. The writing of the work can be further improved.   Also, the basic idea of the work is the almost the same as a concurrent work "Lin et al 2020. controllable pareto multi task learning" which is also submitted to this conference. The paper cited that paper briefly, "... The proposed method is conceptually similar to our approach...",  which is too vague and brief. We urge the author to provide a through discussion on the detailed difference and similarity of the works, including empirical comparisons when necessary. 
The paper studies the unsupervised RL problem, where the agent is allowed to interact with the environment for a certain amount of time without any extrinsic reward. The main idea is that the initial unsupervised training phase can be used to learn a set of "skills" that could help both in exploration and zero shot transfer for any downstream task.  There is general consensus among the reviewers that the paper is studying an important problem and that the empirical validation is solid. Nonetheless, the technical contribution and the positioning wrt to the relevant literature are relatively weak for proposing acceptance for the paper. Before entering into details, I would like to acknowledge the fact that the rebuttal and the revised version did improve the original submission and clarified some aspects (eg, the structure of the algorithm), yet the contribution does not seems strong enough.  The idea of state space coverage is indeed not novel, either for exploration or for transfer (as properly reviewed in the paper). The authors identified some weaknesses of existing methods (eg, estimating state distributions), but it remains unclear whether the algorithm they propose has any significant technical contribution. In fact, as confirmed by the authors, CPT is rather applying any algorithm that *could* perform a good state space coverage and learn policies at the same time and then use the learned policy during the downstream task combined with a relatively basic "option level" eps greedy strategy. In this sense, it seems like CPT overcomes the limitations of previous algorithms, just by applying another existing algorithm (NGU) that is more scalable. While the evidence that this is "enough" to obtain good results is indeed interesting, it doesn t seems like it is pushing the algorithmic state of the art forward.  A more substantial contribution would be to dive deeper into the state coverage problem and provide an algorithm that is more specifically designed for the transfer setting considered in the paper. In fact, there is no clear evidence that NGU is the *right* approach to perform good coverage and return "useful" skills. Since this is the core concern of the paper, the technical contribution should be more significant on this part. The "meta" algorithm in itself seems rather standard otherwise.  
Though the observation regarding the importance of the low end of the spectrum is interesting in its own right, the paper would be better substantiated by experiments on more datasets and a more thorough characterization of the paper novelty/contrast to state of the art.
This work presents a distributed SVGD (DSVGD) algorithm as a new non parametric Bayesian framework for federated learning. The reviewers concerned with the practical advantages of the proposed method, including the communication cost and the constraint of updating one agent per time. The authors rebuttal helped addressing some of the concerns, including proposing a new Parallel DSVGD algorithm. This is very much appreciated. However, given the significant modification needed over the original version, we think it is better for the authors to further improve the work and submit to the next conference. 
The reviewer concerns generally centered around the novelty of replacing the distance metric for a policy constraint. While the authors clarified many of the reviewer concerns and added some additional comparisons, in the end it was not clear why the proposed approach was interesting: while it is true that this particular distance metric has not been evaluated in prior work, and the result would have been interesting if it resulted in some clear benefits either empirically or theoretically, in the absence of clear and unambiguous benefit, it s not clear how valuable this concept really is. After discussion, the reviewers generally found the paper to not be ready for publication in its present state.
The paper proposes a new distributed training method for graph convolutional networks, using subgraph approximation. The reviewers raised multiple challenges, such as novelty, validity of experiments, and some technical issues. The authors did not respond to the reviewers  comments. The AC agreed with the reviewers that the paper, in the current form, is not ready for publication.
The paper is proposing a multi task learning approach extending existing weighting approaches. An important and novel contribution of the paper is separating the magnitude and direction information in gradient based information. The joint gradient direction is searched by using angle bisectors of task gradients and magnitude is searched by simply finding a scaling which results in uniform loss scales. This approach solves issues like small gradient norm bias of MGDA, etc. The proposed method works well and authors show that it is conceptually relevant to most of the existing algorithms. These conceptual unification is a strong contribution of the paper. The paper is reviewed by three reviewers and received both accept and reject scores. Specifically,    R#2: Championed the paper and argued for its acceptance   R#3: Argues that the novelty is limited and SOTA claim is problematic.   R#4: Argues that the gap between the empirical performance of the proposed method and existing algorithms is small.   Arguments on the empirical performance and the SOTA are irrelevant to the decision since ICLR does not require algorithms to be SOTA or performed significantly better. Hence, the remaining issues are: claim of the SOTA being true or not, and lack of novelty. I read the paper in detail and decided to accept it with the following comments about the reviews:   The paper is clearly novel. Direction and magnitude are first time treated separately. Moreover, resulting unification of the existing approaches and theoretical derivations of the important connections of existing methods are also significant.   The SOTA claim of the paper is technically correct but little misleading. I would recommend authors to simply rephrase it "proposed method outperforms existing methods loss weighting methods under the same experimental settings". The reason for this is the fact that; in principle, "art" includes every possible solution for that problem. Hence, claiming SOTA in a fair and limited evaluation is rather misleading.  In addition to the reviewer comments, here are additional issues which should be addressed by the camera ready deadline:   I think the discussion about MGDA is a bit problematic since removing $\alpha \geq 0$ assumption simply removes the Pareto stationarity guarantee of the method. The resulting direction can increase some loss function and this disagrees with the main point of the Pareto optimality. Hence, I would recommend authors to clarify this while making the connection. Frank Wolfe algorithm is also not really inefficient and unstable since the problem is quadratic with linear constraints and the stability as well as extremely quick convergence can trivially be proved.   In addition to the previous point, the proposed method can actually increase some loss functions as there is no consistency constraint enforced. This is an interesting observation and empirical results suggest that increasing loss of some objectives might actually be valuable. I think this observation deserves some discussion even in the introduction.  
Two reviewers expressed clear concerns about the paper but the authors did not provide any response. 
This paper extends the idea of successor representations. Typically the reward is compute linearly on top of states in this setting but the authors relax it to have a quadratic form.   ${\bf Pros}$: 1. A novel formulation of the successor representation where the reward does not follow the linearity assumption 2. The idea of using a second order term for the reward branch is interesting and could have meaningful implications for learning and exploration.   ${\bf Cons}$: 1. All authors agree that the experimental results do not clearly validate the advantage of this method. More work is needed to establish the effects of using this particular reward structure on a wide variety of tasks  2. Both R2 and R4 were unconvinced of the limitations of the linearity assumptions in the original successor representation formulation   especially in the case when the state is represented by a non linear function approximator.  The ideas presented in this paper are quite interesting and promising. But more experimental work is needed to show the benefits of this approach. 
Solid work on extending AntisymmetricRNN and expanding its expressivity while controlling the global stability of the recurrent dynamics. It contributes to the growing interest in continuous time RNN formulations that can deal with exploding gradient problem, and worthy of ICLR poster presentation. Three reviewers were positive and one was slightly negative. Authors added additional experiments and strengthened the manuscript significantly during the review process.
This paper develops a smoothing procedure to avoid the problem of posterior collapse in VAEs. The method is interesting and novel, the experiments are well executed, and the authors answered satisfactorily to most of the reviewers  concerns. However, there is one remaining issue that would require additional discussion. As identified by Reviewer 1, the analysis in Section 3 is only valid when the number of layers is 2. Above that value, "it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically". Thus, the scope of the analysis in Section 3 deserves further discussion. Given the large number of ICLR submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue.
The paper proposes a novel approach to detect outliers using Optimal transport. the authors prove a very interesting relation between Outlier robust OT and solving OT with a  thresholded loss. Numerical experiments show that the proposed approach indeed work for outlier detection.   The paper had mixed reviews and the comments and changes from the authors were appreciated. The comments about recent (and contemporary) references were not taken into account in the final decision following ICLR guidelines.   One major concern that appeared during discussion was the fact that one important claimed contribution is the ability to perform outlier detection, the proposed method is never evaluated or compared to the numerous existing outlier detection methods. It works on a toy example and seem to provide a robust way to train a robust GAN but the experiments are very limited. Also the claim from the authors that the method scales are not really true. The proposed approach requires solving an exact OT of complexity O(N^3log(N)), while one can use an approximated entropic solver on the thresholded loss it does not solve the ROBOT problem anymore and the relations between the problem does not exist anymore in this case (or are more similar to UOT).  The concerns detailed above and the limited novelty of the contributions (most of the formulations proposed in the paper are already existing in the literature) suggest that the paper in its current iteration  is too borderline for being accepted in a selective venue such as ICLR. The method and the relations uncovered are interesting and the AC encourages the authors to continue work on the proposed method and provide more detailed experiments illustrating and comparing the method to baselines for outlier detection. 
The paper shows (nearly) matching upper and lower bounds on dynamic regret for non stationary finite horizon reinforcement learning problems. The paper studies an important problem and the results are interesting. Some reviewers are concerned that there is not enough algorithmic and theoretical innovations in light of prior results. Authors need to improve the presentation and add a more detailed discussion on related works, the novelty and the originality of the paper, and the new algorithmic and theoretical contributions. Finally, authors can improve the submission by implementing the proposed method and adding experiments.
This paper proposes a method for compressing weight matrices in large scale pre trained NLP encoders (like BERT) through low rank decompositions of both fully connected and self attention layers. The method is used to compress and speedup pre trained models. Experiments measure timing on a single CPU thread and demonstrate speedups with small loss of accuracy. Reviewers noted the that goal of this paper is potentially impactful. Some reviewers viewed the resulting loss in accuracy as marginal, while others viewed it as more substantial   a potential downside. Reviewers also raised concerns about the methodology used to measure inference speedup, a critical measure of success. Specifically, timing experiments were done only on a single CPU thread   while most practical scenarios would almost certainly rely on GPUs   as a result, positive experimental results are less impactful. Authors updated the paper to include GPU timing experiments, which did show speedups   though only marginal speedups over the baseline, TinyBERT. Further, reviewers pointed out that there are several other relevant baselines on compression approaches that are not compared with, and that further analysis should be done on the timing/accruacy tradeoff of baseline methods. Finally, reviewers felt that the contribution of the proposed method relative to other approaches that also attempt to compress transformers is not clearly outlined. Weighing these concerns, I agree with reviewers that the paper is not ready for acceptance in its current form. 
While there are some potentially interesting aspects to this work, it doesn’t acknowledge a significant amount of relevant literature, and there are some unsupported claims. All reviewers believe the paper is not ready for acceptance. Reviewers provided some good thorough reviews and suggestions, but the authors did not choose to respond or engage in discussions to improve the paper. 
The paper describes a cool application of online learning from bandit feedback   creating personalized, adaptive typing interfaces for users with sensorimotor impairments. The problem is well motivated   the interface can observe users  gaze (e.g. via a webcam image), predict a character as an action, and bandit feedback can be collected by observing whether users use the backspace key after the interface s action. Prior work showed that gaze to text can be less burdensome than typing, but this can quickly become untrue the more mistakes the interface makes. So, the goal is to personalize the interaction policy so that it makes fewer mistakes than the default interaction policy trained using a fixed dataset of expert demonstrations.  The high point of the paper is the empirical user study with 12 60 participants   the study convincingly demonstrates that indeed a simple bandit algorithm can improve over the default interface; moreover, users exhibit intriguing co adaptation patterns with the adaptive interfaces. These findings may prove to be an interesting point for future studies in user co adaptation.  The low point of the paper is its algorithmic development. There is a vast literature on bandit/RL algorithms, and incorporating human feedback into their operation (the paper rightly cites TAMER, COACH, etc.) but it is very unclear why any one of these algorithms could not be used for the paper s application. COACH (human feedback gives an explicit view of the action s advantage   which in the contextual bandit setting exactly matches the paper s assumptions) seems particularly appropriate. Although the algorithm proposed in the paper is simple, how applicable is it in any other context? how does it compare to COACH/etc.? when should we prefer this algorithm over others? Furthermore, given that X2T trains a reward model from observed user behavior, a natural baseline would use an epsilon greedy strategy (fraction of the time, pick actions greedily according to the reward model)   this might isolate the benefit of the approximately Boltzmann exploration being conducted on top of the reward estimates in Eqn 2. Finally, since X2T trains a reward model per user it could be particularly informative to visualize what the models have learned to illustrate qualitatively how X2T is personalizing across its user base.  The paper could have a much bigger impact if the authors can figure out some creative way to enable the broader research community to work on this problem domain. A testbed or environment (like RecSim for content recommendation https://github.com/google research/recsim) with configurable but realistic reward models could allow researchers to test several bandit algorithms, MDP vs CB formulations, other ways to interpret user feedback etc. 
The paper introduces a new extrapolation problem for graph representation learning (they refer to it as   counterfactual modeling ). While the problem set up is intriguing and the work likely has merit,  two reviewers (R2 and R4),  found the writing highly problematic and we share their opinion.  Even though some of the concerns they raised, as followed from the rebuttal, were not correct, this confusion, in our view, is largely due to the exposition.  Both these reviewers are experts in geometric deep learning. Their lack of understanding even of relatively central points of the paper, despite clearly investing a large amount of time in reading the paper, indicates that extra work is needed.  The only positive reviewer marked his confidence as very low, provided a rather short review, and did not choose to champion the paper.  While the authors tried to address the reviewers  concerns both in rebuttal and by revising the manuscript, we still feel that much more work is needed before it can be presented at a conference. We understand that this is a challenge to present this work in a conference format; it builds on the diverse background (e.g., in graph representation learning and in causal modeling) and considers a novel setting. However, we still feel that it could have been done much more successfully. In principle, this work may benefit from being presented in a journal paper (e.g., jmlr).
The focus of the submission is the measuring of the discrepancy of two probability distributions. Using the notion of H entropy, the authors propose a new divergence measure the H divergence (Def. 2), as a common generalization of Jensen Shannon divergence and maximum mean discrepancy. They suggest an empirical estimator for H divergence and show that it is consistent (Theorem 2). The efficiency of the technique is illustrated in the context of 2 sample testing, sample quality evaluation and measuring climate change.  Overall, the submission addresses an important problem (defining a class of divergence measure). As pointed out by the reviewers, however fundamental questions on the proposed estimator are not addressed: 1)the motivation of using a set of actions (in the definition of H divergence) and how to design them are unclear,  2)the impact of the loss function l (in the definition of H divergence) is not explored: this can easily lead to instabilities due to the lack of closed form solution and by the arising non convex optimization task. The gain compared to existing solutions have to be understood and explored further.
This work aims at doing Bayesian inference via Langevin dynamics with data subsampling. This builds on previous work with "replica exchange" where parallel chains are run at different temperatures and can be swapped to encourage moving between modes. The main technical novelty here is a scheme to reduce variance. This is done in the style of SGRD by periodically computing the gradient on all data and then using those values as control variates. This is shown to reduce variance.  Reviewers generally felt that this represented a sensible combination of known ideas aimed at an important and timely problem with sufficient empirical evaluation. There was consensus the paper was clearly written. I concur that even if the combination is "expected" to work, the presence of guarantees for performance represent sufficient technical novelty. I particularly applaud the fact that the paper does not over claim and generously gives credit to related work. This is helpful to the reader and encourages the flow of ideas. For these reasons I recommend acceptance of the paper.  In reading the paper, I had a couple questions about the experiments:  1. It s not obvious to me from the experiments how specific the method is to the replica exchange setting. The main control variate idea appears to be applicable without replica exchange. I would very much like to see a "VR SGHMC" row in Table 1 unless there is a good reason that this cannot be done. It would be very beneficial to understand the contributions of these different algorithmic components.  2. The CIFAR experiments directly test variance. That s fine, the paper is aimed at reducing variance, after all. However, I would like to see more tests of the follow on improvements in optimization speed. It has been my experience that improvements in variance sometimes produce surprisingly small improvements in optimization speed. My intuition for this is that reduced variance mostly helps by making it possible to use a larger step size without the same penalty in the stationary dist. In practice, the step size typically ends up being imperfect, meaning that changes in variance have small changes. 
This paper adopts an idea from 1990 for reducing reliance on texture, and shows that this idea improves the quality of visual representations in a variety of tasks. Initially reviewer scores were 7/5/4 but those improved slightly to 7/6/4 (changed in comment, not final review) after the rebuttal stage  thus, one accept, one borderline, and one reject score. Reviewers have concerns about the great simplicity of the approach, where the only contribution is from prior work. Some reviewers request comparisons in a proper domain adaptation setting. While the large number of experimental settings somewhat balance out the concerns, overall, support for acceptance is not strong enough at this stage.
Four knowledgeable referees reviewed this paper; one reviewer (weakly) supports accept and other three indicate reject. Even with the rebuttal, all negative reviewers have concerns on the limited novelty and marginal performance improvement, and agree that the paper is not well qualified for the high standard of ICLR.
I thank the authors for their submission and participation in the author response period. The reviewers unanimously agree that the papers proposes an interesting and original approach to using a costly model on a learner node, while distilling to a cheaper model run on actor nodes to gather experiences in a distributed RL framework. During discussion, R1 and myself emphasized the concern that the experiments in this paper leave open the question whether the approach will work beyond toy environments. However, I side with R2 and R3 in that the paper presents a valuable contribution to the community as it stands, and that the experiments proof the concept to the point that the paper should be accepted. I therefore recommend acceptance.
This paper studies extensions of the Scattering Graph Transform to spatio temporal domains. By exploring several design choices for spatio temporal wavelet filters, the authors provide a solid and broad study of such predefined represenatations, including stability analysis as well as extensive empirical evaluations.  Reviewers were generally favorable, and highlighted the importance of this method as providing a simple yet powerful baseline for spatio temporal graph prediction tasks that requires no training. Despite some concerns about lack of analysis of the empirical results, the AC believes this work will provide a valuable baseline for future research and therefore recommends acceptance as a poster. 
In this paper the authors propose an approach to improving the accuracy of the classification problem based on deep neural networks by detecting the in domain data from background/noise.  The strategy is designed in such a way that the detector and the classifier share the bottom layers of the network.  Theoretical proof is given and experiments are conducted on a variety of datasets.  The novelty of the work is to come up with a better estimate the pdf of the data and use it to help the classification based on the deep neural networks.   There are concerns raised by the reviewers regarding the related work, the exposition and the experimental design.  After the rebuttal from the authors, which is meticulous, some of the issues unfortunately still stand.  The paper needs to make a stronger case in order to be accepted, especially, for instance, the theoretical and empirical comparison with the existing techniques sharing the similar idea. 
The approach proposed here have raised major concerns from multiple reviewers especially concerning the novelty and the experimental validation procedure. Authors did not succeed in convincing reviewers of the value of their work for ML or calcium imaging processing.
We thank the authors for their detailed answers and for providing an updated version of the paper addressing several of the issues raised by the reviewers, including new experimental results.  The paper is technically correct. The comparison with other methods is thorough and includes ablation studies clarifying the contributions of different aspects of the proposed method. One aspect that has been moderately addressed in the new version is the comparison between the "learned lambda" of the paper with a "tuned lambda" suggested by a reviewer. The authors added results where lambda is set to a particular value, however it would be more interesting relevant to consider a real "tuned lambda", i.e., a scalar parameter, shared by all vertices, that is optimized during training; the goal being to clarifying the benefit (if any) of parameterizing lambda as a function of the node, as opposed to a value shared by all nodes.  The paper is clearly written, particularly the revised version.  The novelty is the weakest aspect of the paper. While the specific problem of learning with noisy labels with GCNN may be new, the field of learning with noisy labels in general, and of using label propagation from clean labels to guide the prediction of uncertain labels, has been proposed before, and mentioned in the reviews. The specific instantiation of this idea to the GCNN framework is novel.  The significance of the work is rather positive. The revised version contains results on two real world datasets, where the proposed method outperforms several existing ones. As mentioned by a reviewer, this paper may inspire other researchers to explore in more depth the specific problems of learning with noise on graphs with GCNN, and to exploit the knowledge of a limited set of clean labels which may have practical importance to reduce human annotation efforts.  In summary, the paper proposes a novel model and demonstrates its potential to address a possibly important problem. Although the reviewers did not update their reviews, the authors  responses and updated version correctly addresses several of the initial concerns. The limited conceptual novelty compared to existing work did however not convince us to recommend acceptance, given the high selectivity of the conference.
Reviewers appreciated the care and substantial effort that went into the paper, for instance: AR3) I think it s of good value for the community to see and discuss the paper in the conference. AR4) would be quite valuable for the senior members of the community to read and be familiar with.  The main argument for rejection is the the analysis done in the paper is not typical of ICLR research.  Arguably, the paper could fall under the topic "societal considerations of representation learning including fairness, safety, privacy", but this does not apply because the subject of analysis is the conference ICLR, not representation learning.   I support this argument.  The reviewers posed a good number of questions and issues with the paper, and largely these were addressed well by the authors.  In some cases they addressed the issues properly, and others they argued their case.  For instance AR2 says  "think the ACs decision process is too simplified" and the response summed up as "our ability to do multi factor studies is limited by the size of our dataset".  An important one of these discussions is as follows: AR4)  But since the AC are not identified as biased, and the papers are anonymous, it is not clear what is the mechanism suggested by the authors of how these biases manifest themselves. Authors)  <extensive points>  .... we find the idea that anonymity does not genuinely exist to be entirely plausible. I would argue that neither party can claim to have won this argument, and I am not really sure how it can be resolved.  Fortunately, though, no evidence for gender bias in ACs was found.  In conclusion, the paper is not topical to ICLR material, and the reviewer consensus is Reject.  However, the paper is both valuable and interesting to the community, and it has seen substantial improvement through the review process and a lot of the issues defended well.    The paper should be brought to the attention of the various committees and made available somehow at the conference and acknowledged as a useful publication. 
This paper proposes a suite of benchmark visual model based RL tasks to evaluate causal discovery approaches under systematically varying causal graphs. Despite some disagreement on this point among reviewers, I would come down on the side of saying that a better executed version of this paper would have been a good fit at ICLR. However, its current drawbacks make this a borderline reject. The most important of these drawbacks is: it is unclear to what extent results on these simple environments translate to more realistic complex ones.  Reviewers have also pointed to omitted relevant work that could be discussed in future versions, such as PHYRE. Another relevant benchmark in this vein: https://arxiv.org/abs/1907.09620  
This paper introduces a novel pruning algorithm for neural networks, gently regularizing the weights away (through weight decay) and using Hessian information instead of simple magnitude. All in all an idea that is simple and effective, and could be of interest to a large audience.   AC
While I m sure there are many merits to the underlying work here, the consensus of the reviews is to recommend a rejection as an ICLR paper. That recommendation is based on issues with significance as well as on clarity issues, noted by reviewers even after the revisions.  One pattern I noticed was that it seemed unclear whether the paper was to be regarded primarily as a software paper or as a paper on preprocessing. Most initial reviews evaluated it primarily as a software paper, but some comments from the authors in the discussion period seemed to frame it instead as a paper about research on preprocessing (independent of software). See my other comment for more detail on this question.  Regardless of the intended framing, on significance as an ICLR submission, reviewers did not support its acceptance by either standard: * R1 post response: "Regarding how to frame the paper (either about feature pre processing or the software library), my (favourable) interpretation is to frame it as about the software library. As a paper about feature pre processing it would have even less merit." * R3 post response: "the work has more upside in the software contribution than the feature pre processing research" * R4 post respones: "After reading the revised version and the author response, I am still not convinced that the paper makes a substantial contribution either on the fundamental research angle or the software library angle"  One specific issue was that the experiments did not adequately support the main claims: * R2 post response: "I feel that the experiments in Section 8 are still too limited to demonstrate the value of the techniques and software" * R4 post response: "While the main contributions of the paper are still a little ambiguous, the experiments don t seem to support the claims (ease of use of the library). The results seem to suggest that one of the contributions is the improved accuracy of results, but then the experimentation is far too limited to draw such a conclusion."  On clarity: * R1 post response: "The new section 5 is still very hard to understand and I still couldn t make sense of the family tree primitive mechanism" * R3 post response: "I agree with AnonReviewer1 about the difficulty of understanding the family tree primitives, both before and after the revision" * R2 post response: "I am still unclear on the family tree primitives [...] it s not clear to me what problem the family tree primitives solve"   The authors showed a lot of enthusiasm and good spirits in working to improve the submission. I hope the feedback provided here is useful.  However, based on the consensus of the reviews, I recommend rejection of this submission.
The review team appreciated the new Bayesian perspective offered by the submission, which lends itself well to selection and ranking, though some of them were still not convinced by the motivation (including in the private post rebuttal discussion, R3). The reviewers also identified many points for improvement. The paper was borderline and, given the lack of enthusiastic support from the reviewers, the program committee decided to reject it. We strongly encourage you to address the raised concerns and resubmit to a future venue.
The paper proposes an unsupervised pretraining approach for 3D recognition, which is based on point cloud completion. The initial review receives a mixed rating, with two reviewers rate the paper below the bar and two above the bar. After the rebuttal, R3 changes the opinion from above the bar to a rejection recommendation. While several reviewers recognize the simplicity of the proposed method, R2 and R4 consider the proposed method a straightforward extension of known approaches for NLP and vision tasks. A lack of novelty was also pointed out as a weakness by R3 and R4. After consolidating the reviews and the rebuttal, the AC finds the weakness claims convincing and determines the paper is not ready for publication in the current form. 
Inferring latent trajectory from noisy Ca time series is an important and timely problem and the current study shows some progress in the inference problem. Although the proposed model has some originality, there are remaining issues rendering the manuscript not ready for publication yet. Reviewers raised issues on readability, lack of details, statistics of experiments, Ca time constant estimation, effect size, and lack of comparison. Through an extensive discussion and revisions, I m happy to see the manuscript was greatly improved in readability and additional statistics were provided. However, the Gaussian LFADS  performance at exactly chance raises red flags, effect size is small, and the significance of the scientific findings remain weak. The model is presented as a variational ladder autoencoder system with 2 layers, but the shallow latent representation is tied to the continuous approximation of the point process likelihood. Hence, I view the model as an extension of LFADS rather than a flat hierarchical VAE.  Overall, this paper has a potential of becoming a solid contribution for statistical neuroscience, once the above shortcomings are addressed.
The paper introduces an AutoML method for irregular multivariate time series.   The method automates the selection of the configuration as well as the hyperparameter optimization depending on the task. A Bayesian approach handles the network structure search while VAEs + attention is used to learn  representations from irregularly sampled data. There is an additional contribution: anomaly detection via a sample energy function from a GMM on time windows.  While there is some novelty in the proposed approach, mostly in the way in which existing techniques are combined, the paper also has some limitations:   running the framework over the set of possible models is computationally intensive; in their response, the authors indicate the search space can be constrained, however, doing so would also decrease the performance; in AutoML, added complexity cannot be avoided, but there is no notion of how much longer it takes to find suitable models compared to taking off the shelf methods.   although the paper is geared towards irregularly sampled time series, there are no experiments where the data is naturally irregularly samples; artificially introduced patterns are no substitute for this; (PhysioNet, as suggested by one of the reviewers or MIMIC III both have this type of data and are frequently used in benchmarks)   AutoML is presented as a general framework, but mostly handles clustering and anomaly detection;  unclear of how useful it would be for forecasting or regression; classification realists are shown in Appendix F against simple baselines (GRU D is not considered, for instance) and even so AutoML does not achieve state of the art results in half of the cases
## Description The paper discovers interesting phenomena in training neural networks with binary weights:   Connection between latent weight magnitude and how important its binarized version for the network performance  training dynamics, indicating that large latent weights are identified and stabilize early on   Observation that amongst learned binary kernel, several specific patterns prevail, up to the bits who s reversal has very little effect. This is so regardless of the architecture, the layer considered or the dataset.  The paper further demonstrates how these observations may be used to compress binary neural networks below 1 bit per weight.  ## Review Process and Decision The reviewers welcomed the experimental investigation of new phenomena, but commented the overall technical quality of the work as somewhat substandard. The redundancy of consecutive affine transforms is known and not connected to binary weights investigation. The investigation itself lacks a more in depth analysis. The proposed compression results appeared not convincing to reviewers since a significant drop of accuracy occurs. The AC shares these concerns and supports rejection.  ## General Comments From my perspective, the study undertaken is methodologically „wrong“. An ad hoc training method is investigated, which is not even clearly defined in the paper (there are many „STE“ variants) and for which it is not known what it is doing, what are the real valued weights for and whether they are needed at all (as empirically argued by Helwegen et al. (2019)). As such, the investigation makes impression of poking a black box (the training method in this case). At the same time, there are more clear learning formulations, applicable in the setting of the paper (binary weights), in particular considering the stochastic relaxation: * Shayer et al. (2017): Learning Discrete Weights Using the Local Reparameterization Trick * Roth et al. (2019): Training Discrete Valued Neural Networks with Sign Activations Using Weight Distributions * Peters et al. (2018): Probabilistic binary neural networks  These methods are approximate, but at least the optimization is well posed and it is known what do the real valued weights represent (e.g. logits of binary weight probabilities). From this perspective, it can be seen that latent weights close to 0 correspond to Bernoulli weights that are almost fully random (and thus only contribute noise) and are fragile to gradient steps. Therefore the model can only perform well if it learns to be robust to their state or their state becomes more deterministic (corresponding to large latent weight). So one would actually expect to see in these models phenomena similar to the observed in the paper and not bee too much surprised or astonished by them. Furthermore, there are recent works explaining STE and its latent weights as optimizing the stochastic relaxation: * Meng et al. (2020): Training Binary Neural Networks using the Bayesian Learning Rule * Yanush et al. (2020): Reintroducing Straight Through Estimators as Principled Methods for Stochastic Binary Networks.   The authors are encouraged to make the observed phenomena more explainable by connecting to the mentioned works.  ## Further Details  *  „We show that in the context of using batch normalization after convolutional layers, adapting scaling factors with either hand crafted or learnable methods brings marginal or no accuracy gain to final model.“  From theoretical perspective, this is obvious and known to me. Practically, there could be in principle some difference due to the learning dynamics, and verifying that there is none is a useful but a weak contribution. The section devoted to this issue can be given in the appendix but is not justified in the main paper.  * „change of weight signs is crucial in the training of BWNs“  The sign determines the binary weights, so this is by definition.  * „ Firstly, the training of BWNs demonstrates the process of seeking primary binary sub networks whose weight signs are determined and fixed at the early training stage, which is akin to recent findings on the lottery ticket hypothesis for efficient learning of sparse neural networks“  In the lottery ticket hypothesis paper it is shown explicitly that the identified sparse subnetwork changes during the learning the most rather than retains its initialization state or the state in the beginning of the training. It is therefore could be of a different nature. 
This paper describes a non uniformly weighted version of SGMCMC, combining aspects of SG methods and importance sampling. The idea is interesting and novel, but unfortunately the authors have not made a compelling case for the resulting algorithm being a practical addition to the literature. The experimental analysis is not particularly compelling, and there are key concerns raised about practical implementation, and about the validity of the approximations raised. I hope that the authors will continue along this interesting line of work and add additional explorations of the approximations and improved experimental analysis.
The paper addresses the problem of prior selection in Bayesian neural networks by proposing a meta learning framework based on PAC Bayesian theory. The authors optimize a PAC bound called PACOH in the space of possible posterior distributions of BNN weights. The method does not rely on nested optimization schemes, instead, they directly minimize PAC bound via a variational algorithm called PACOH NN which is based on SVGD and the reparameterization trick.  The method is evaluated on experiments with both synthetic and real world data showing improvements in both predictive accuracy and uncertainty estimates.  Initially many reviewers were positive about the paper. However, it was noticed by one reviewer that the submitted paper presents a very significant overlap with  Jonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes Optimal Meta Learning with PAC Guarantees. arXiv, 2020.  Another reviewer mentioned that they were actually reviewing for AISTATS the above manuscript by Rothfuss et al. The ICLR program chairs were contacted for a possible violation of the dual submission policy for ICLR:   "Submissions that are identical (or substantially similar) to versions that have been previously published, or accepted for publication, or that have been submitted in parallel to this or other conferences or journals, are not allowed and violate our dual submission policy."   The ICLR program chairs decided that the similarities between the two papers are not enough to issue a desk rejection. However, in the discussion period,  three reviewers out of 4 pointed out that, even though the authors did revise Sections 4 and 5 in the current version, these modifications do not seem to be strong enough to make up for the really strong overlaps between the two papers. The reviewers agreed on rejection and stated that this paper should either be merged with the Rothfuss et. al. one (assuming the authors are the same), or its content should be developed to the point of making both of them clearly distinct. 
The authors propose a method for modeling dynamical systems that balances theoretically derived models, which may be grounded in domain knowledge but subject to overly strict assumptions, with neural networks that can pick up the slack. All reviewers were enthusiastic about this work, appreciating its balance of mathematical rigor and experimental assessment. One concern was that this paper follows on decades of related work, which was difficult to adequately summarize. However, changes made throughout discussion phase did address these concerns.
The reviewers were split between accept (7) and borderline reject (two 5 s). All three reviewers acknowledged that the proposed approach is simple and intuitive (but this paper follows, for the most part, the concept of reservoir operation and apply it to transformers). The main criticisms were insufficient experiments (R5) and the lack of a clear conclusion (R2). I found these concerns to be valid and did not find strong reasons to overturn their recommendations. More comprehensive experiments (especially on WMT) and clear conclusions (accuracy or efficiency) would make this paper much stronger.
The paper designs a new way (in some sense a new perspective) on how neural networks can be used to model intervention variables when the goal is to estimate ADRF.  Basically, the idea is to emphasize the importance of the intervention variable by ensuring that it appears not just in every layer but also in every neural of a neural network.    Reviewers mostly agree that this is a good paper with varying degrees, although there are some criticisms on e.g., assuming away the confounders.  However, I believe the authors address the criticisms of R4 satisfactorily.   Overall I find the idea new and interesting and the experimental results strong, hence I happily recommend accepting the paper.  I do have a few quips myself and some comments that may help the authors to further improve the paper.  1. Re: the design that models each parameter as a spline.  This is equivalent to introducing additional parameters (coefficients for spline basis) and adding a fixed linear layer (spline basis themselves) to every layer of the neural networks. t is taken as an input in all layers thus it makes sure that the model prioritizes on learning the impact of t.   2. If you use a B spline basis (that comes with kernels of bounded support), then the proposed method is very similar to stratifying the data according to different bins of t, and then fitting a separate model for each t. The only difference is that the different bins are now smooth kernels and they overlap somewhat. As a side note,  the authors should clearly write out how they are choosing the knots to specify the basis functions. Otherwise the paper will not be reproducible.   3. I am not sure how this method would compare to naive (non deep) baselines. Maybe this was considered in a prior work? If not, then I tend to side with Reviewer 4 that the evaluations are mostly ablation studies and they are not really comparing to representative work in this domain. Given that there is a large body of work on this before deep learning takes over, it is important to somehow compare with the right baselines.  
There were opinions on both sides of this paper from the reviewers.  Reviewers were excited by the novel application of energy based models (EBMs) to continual learning and the resulting performance gains, but were concerned by the more direct application of EBMs (which has been explored in other work, and here adapted to the continual learning setting, so its contribution is marginal) and with the depth of the evaluation, which they thought could be pushed farther. Overall, the reviewers agreed that this paper could benefit from another round of revisions to strengthen its contribution, incorporating many of the excellent points made by the authors in their responses.
The main problem as flagged by reviewers is the lack of formal evidence that the approach is a right one to carry out. Decision tree induction has early been the subject of formal studies in ML, whether in statistics (Friedman et al.) or ML (Kearns et al.). It is a bit sad that a new approach that relies on a much different standpoint on the problem and modelling of tree classification (Section 3, R2), with experimental results recognized by reviewers (R3, R4) is not accompanied by formal analyses on par with SOTA for related approaches (R3, R1). I would strongly suggest the authors fit in a few more Lemmata, either to follow up on specific problems (R1). The paper would tremendously benefit from extensive connections with the existing theory, be it from the generalization and overfitting standpoint (R2, remark #6) or the choice of the appropriate best contender using the boosting literature. Decision was taken not to accept the paper but I would very strongly encourage the authors to revise the draft. 
In this paper, a network architecture search (NAS) problem in a changing environment is studied and an online adaptation (OA) algorithm for the problem is proposed. Many reviewers found that the OA NAS problem discussed in this paper is interesting and practically important. However, many reviewers (including those with high review scores) recognize that the weakness of this paper is the lack of sufficient theoretical verification. Furthermore, although extensive experiments are conducted, it is still not clear whether the experimental setups discussed in the paper are generally applicable to other practical problems. Overall, although this is a nice work in that a new practical problem is considered and a workable algorithm for the problem is demonstrated in an extensive simulation study, I could not recommend the acceptance in its current form because of the lack of theoretical validity and evidence of general applicability.
The paper presents a provable correct framework, namely Universal Aggregation, for training GANs in federated learning scenarios. It aims to address an important problem. The proposed solution is well grounded with theoretical analysis and promising empirical results.   The paper receives mixed ratings and therefore there were extensive discussions. One the positive end, some reviewers think that the authors  feedback provide clarification to confusing part of the paper; on the negative side, the authors feedback also confirms some of the concerns raised in the reviews:   1.  It was confirmed that there is no guarantee that one can find an (nearly) optimal discriminator, which decreases the impact of the work, as in practice we work with non optimal discriminators and hence some of the results couldn t apply.   2. It was confirmed that no privacy guarantees can be given. This is concerning since the complexity of GANs won t prohibit skilled attackers from inferring some information.   While it is true that some of the guarantees would be hard to achieve even for a traditional GAN, the paper sets up a high expectation at the beginning of the paper,  but fails to satisfy the readers with enough evidence.  In addition, the writing can be significantly improved to ensure precise formulations and consistency; the added experiment results are useful, but stronger empirical results could help alleviate the issues in theoretical results.   In summary, the paper has built solid foundations for a good piece of work, but the current version could benefit from one more round of revision to become a strong publication in the future. 
The introduced method is novel and interesting. However, as pointed in the reviews the` paper misses several important references. The authors should extend their discussion on related work by methods from both recommender systems and extreme classification. Besides the papers listed by the reviewers, the introduced method seems also to be related to LTLS (https://arxiv.org/abs/1611.01964) and W LTST (http://papers.neurips.cc/paper/7953 efficient loss based decoding on graphs for extreme classification.pdf), as well as to probabilistic classifier chains (https://icml.cc/Conferences/2010/papers/589.pdf) used for multi label classification (recommendation can be reduced to multi label classification under 0/1 loss by coding each item using a binary code of a fixed length). Nevertheless, the introduced method seems to be novel, nicely reusing and fitting together existing ideas.   Unfortunately, the authors did not submit any rebuttal. Therefore, the paper cannot be accepted to ICLR. We encourage the authors to work further and extend the paper by an exhaustive discussion about related work, a wider experimental study, a more detailed description of all the steps of the method.   
The paper proposes the idea of searching parameterized activation functions, in contrast to the previous handcraft or learnable ones. It may be a counterpart of neural architecture search.  Pros: 1. The idea is very interesting. 2. The paper is well written. 3. The experiments show improvements over baseline activation functions.  Cons: 1. The AC fully agreed with Reviewer #4 that the whole literature of learnable activation function is neglected (Reviewer #2 also alluded to this issue). Although the authors added experiments with learnable baseline activation functionss, the literature review on learnable activation function was not included accordingly. 2. Although the idea of searching activation functions is interesting, the AC doubted the necessity. Since the rich literature of learnable activation functions is already there (note that it is more than introducing parameters to handcrafted ones), can we simply learn piecewise linear activation functions with more pieces so that it can approximate complex enough functions? This can be much more easily implemented (can go along with weight training on the standard deep learning platform) and the computation cost will be much lower. Such a comparison is absolutely necessary. 3. The AC was actually worried about the activation functions founded as they may be too complex, so the generalization issue (even numerical stability issue) may be a concern. More thorough testing is necessary (currently only tested on CIFAR 100 and three CNNs; and Reviewers #3 and #2 also concerned about this issue).  Although Reviewer #2 raised his/her score, the final average score is still below threshold. So the AC decided to reject the paper.
This paper was quite contentious.  While reviewers appreciated the detailed response by the authors, and there is consensus that the paper addresses a relevant problem and contains interesting ideas, in the end there remain several concerns.  The paper provides a complex combination of techniques from active learning, meta learning and symbolic reasoning (via MILPs), and there are concerns about the clarity of the exposition.  For a paper claiming safety properties, there is also a lack of either formal theoretical analysis of well specified safety properties, or a compelling demonstration of its effectiveness on a real system (all experiments are carried out in simulation).
This paper presents a method for relational inference in multi agent/multi object trajectory prediction tasks. Different from the neural relational inference (NRI) model [1], the presented method is able to model time varying relations. Experimental results on physics simulations and sports games (basketball) show benefits over variants of the NRI model.  The reviewers agree that the presented method is mostly solid, that the experiments are insightful, and that this is generally a well written paper. The authors, however, have apparently overlooked recent related work [2] (dNRI) that proposes a very similar model. In the light of dNRI, it is difficult to argue for the novelty of the presented approach, and the paper needs to undergo a revision in order to more clearly differentiate it from the dNRI model, and to resolve the other concerns raised by the reviewers.  [1] Kipf et al., Neural Relational Inference for Interacting Systems (ICML 2018) [2] Graber et al., Dynamic Neural Relational Inference (CVPR 2020)
The paper offers a new take on generalization, motivated by the empirical success of self supervised learning.  Two reviewers found the contribution novel and interesting, and recommend acceptance (with one reviewer championing for it).  Two reviewers remain skeptical about the value of the paper, and the authors are encouraged to add a discussion about the points made in these reviews.  I agree with the positive reviewers and would like to recommend acceptance.
The paper introduces improving passage retrieval for multi hop QA datasets by recursively retrieving passages, adding previously retrieved passages to the input (in addition to a query). This simple method shows gains on multiple QA benchmark datasets, and the evaluation presented in the paper on multiple competitive benchmark datasets (HotpotQA, FEVER) is very thorough (R1, R3, R4).   While the application is pretty narrow, the performance gain (considering both efficiency and accuracy) is fairly significant, and the paper presents a simple model with less assumption (e.g., inter document hyperlinks), that could be useful for future research.   [1] also seems like a relevant line of work.   [1] Generation Augmented Retrieval for Open domain Question Answering https://arxiv.org/pdf/2009.08553.pdf  
The paper proposes a method for training GANs in few shot setting. Two key components of the method are: a skip layer channel wise excitation (SLE) module that encourages gradient flow across resolutions, and a self supervised loss of autoencoding to regularize the discriminator. The results presented in the paper are indeed impressive in the few shot setting. Reviewers had some concerns about training set memorization which have been addressed by the authors with additional evaluations using LPIPS metric. Overall, the paper tackles an important problem of few shot learning of GANs and will be a good addition to the ICLR program. 
The contributions of this paper are twofold: 1) datasets of tasks are provided, and 2) based on the datasets and hyperparameter lists on the datasets, a transfer learning approach for hyperparameter optimization (HPO) is proposed. Many reviewers positively evaluated the idea and approach discussed in this paper. However, the common concern of multiple reviewers and area chair is that it is not clear whether the provided datasets and their hyperparameter lists are generally applicable to other practical problems. Since there is no discussion on how the datasets are constructed, it is not clear whether they have generally or not. In addition, the comparison with existing HPO approaches is not sufficiently made, and it is not clear whether the performance of the proposed method is advantageous over existing methods. Overall, although the idea is very interesting and potentially useful, I cannot recommend the acceptance in its current form due to the lack of evidence on its generality. 
The paper presents a DKL variant with a linear kernel. Representations from several networks is combined through concatenation, making it not quite an ensemble. It s shown that the model is a universal kernel approximator. Experiments are conducted on a large number of UCI datasets.  Following the discussions, the paper still has the following shortcomings:   some lack of clarity in the presentation (for instance, explaining the equivalence between a multi output learner and M different single output learners)   lack of experiments on data where deep learning is typically used (images); the UCI datasets have structured data and other ensembles like XGBoost may outperform the baselines presented in this paper   difference in performance between DKL and DEKL, especially since DKL benefits from a larger model space, theoretically. maybe DEKL has better sample complexity, but does this advantage hold in the case of the large datasets that deep learning is used for?
The authors present a framework for deriving distributional robustness certificates for smoothed classifiers under perturbations of the input distribution bounded under the Wasserstein metric.   Several authors raised concerns regarding the correctness of results presented in the initial version of the paper. While these were addressed during the rebuttal, the reviewers remain concerned about the novelty of the work relative to prior work, in particular the following papers: https://arxiv.org/abs/1908.08729 https://arxiv.org/pdf/2002.04197.pdf https://doi.org/10.1287/moor.2018.0936 and the author responses during the rebuttal did not sufficiently address these concerns.  Hence, I recommend rejection. 
The paper attempts to make transformers more scalable for longer sequences. In this regards, authors propose a clustering based attention mechanism, where only tokens attends to other tokens in the same cluster. This design reduces memory requirements and allows more information mixing than simple local windows. Using the proposed approach, new state of the art performance is obtained on Natural Questions long answer, although marginal. However, reviewers raised numerous concerns. First, the novelty of the paper compared to prior work like reformer or routing transformer which also conceptually does clustering is not resolved. Second, the claim that k means yields a more balanced/stable clustering than LSH is not well established. Finally, why clustering, i.e. attention between similar vectors is better than dissimilar or randomly chosen vectors or does is it even as expressive is not clear. Thus, unfortunately I cannot recommend an acceptance of the paper in its current form to ICLR.
The paper focuses on the task of weakly supervised activity detection (WSAD). The proposed method combines various ideas together: **(i)** a cross attention module for audio visual information fusion and better representation, **(ii)** an open max classifier to treat the background as an open set, and **(iii)** loss terms to encourage temporal continuity of action predictions. The experimental results on well known benchmark datasets are promising as they beat out many other methods in the literature.  Based on the reviewers  comments, it is clear that the reviewers unanimously see value in the proposed methodology and the competitive results. To strengthen the paper, the authors are encouraged to provide a stronger validation of the contributions being claimed for the specific task being addressed. This would more concretely position the claimed contributions in the WSAD literature. 
This paper is attempting to improve the OOD generalization performance of neural networks on relational reasoning tasks. This is an important failure point of general neural network architectures and important research topic. The results of the paper shows impressive improvements on a set of subject.  * The paper is improved during the rebuttal, however, I do agree with the R5 and the paper is still lacking a lot in terms o clarity. The writing of this paper still requires some work.   * As R2 also has written, the proposed idea is not so concrete to apply as practical solutions, and the presentation of the paper still requires some more work.  * R3 pointed out some inaccuracies and it seems like authors have added some ablations in the direction that R3 has suggested.  I am suggesting to reject this paper given that the majority of the reviewers are also leaning towards rejection as well. I would recommend the authors to improve the clarity of the paper, do more ablations for their models and resubmit to a different conference.
The paper presents a new online convex optimization algorithm that uses per coordinate learning rates. The learning rates are changed over time using information coming from the gradients. A regret upper bound is proved and the algorithm is empirically validated on deep learning experiments.  While the analysis is in principle correct, it does not seem to provide any advantage over the guarantees of similar algorithm, for example the mirror descent version AdaGrad with diagonal matrices. Also, despite the intuition of the authors, the reviewers have found that the approach used in the analsysis is fundamentally bounded to give a worse guarantee than AdaGrad. Overall, the theoretical contribution appears to be not sufficient.  On the empirical side, the experiments failed to convince the majority of the reviewers that the algorithm has a significative gain over similar algorithms.  More generally, this paper suffers from the same problem of many other similar papers: There is a complete disconnect from the theory proven under restrictive assumptions (convexity, bounded domains, no stochasticity) and the experiments (non convex functions, no projection on bounded domain, stochastic setting). Unfortunately, the deep learning literature is full of such papers, but the community should strive to do better and substantially raise the quality of field. In this view, I strongly suggest to the authors to try to improve the theoretical contribution, for example, trying to prove a convergence guarantee of the gradients to 0, rather than focusing on regret upper bounds. Such analysis would also suggest better ways to design new optimization algorithms better suited to non convex problems.
This paper studies the robustness of CapsNets under adversarial attacks. It is found that the votes from primary capsules in CapsNets are manipulated by adversarial examples and that the computationally expensive routing mechanism in CapsNets incurs high computational cost. As such, a new adversarial attack is specially designed by attacking the votes of CapsNets without having to involve the routing mechanism, making the method both effective and efficient.  **Strengths:**   * This is the first work which proposes an attack specifically designed for CapsNets by exploiting their special properties.   * The proposed vote attack is more effective and efficient than the other attacks originally proposed for CNNs rather than CapsNets.   * The paper is generally well written.   * The experimental study is quite comprehensive.   * The code will be made available to facilitate reproducibility.  **Weaknesses:**   * The study is mostly for only one type of CapsNets. It is not clear whether the observations in this paper still hold generally for other types of CapsNets even after some additional experiments have been added.   * The presentation of the paper has room for improvement.  The authors are recommended to proofread the references thoroughly to ensure style consistency such as the consistent use of capitalization, e.g.   * “Star caps”  > “STAR Caps”   * “ieee symposium on security and privacy (sp)”  > “IEEE Symposium on Security and Privacy (SP)”  Despite its weaknesses especially those pointed out by Reviewer 2, this paper would be of interest to other researchers as it is the first paper that studies adversarial attacks on CapsNets. 
The paper is written in defense of pseudo labeling. The authors `aim at demonstrating that pseudo labeling based methods can perform on par with consistency regularization methods which have been show to achieve strong performance.  The paper is well written and easy to follow. The reviewers are generally positive about the contribution. It has to be underline, however, that pseudo labeling is still a controversial approach with a very limited theoretical understanding. This paper does not provide any further understanding of it, but proposes several "heuristics", intuitively well motivated, but justified only experimentally. Nevertheless, the results are promising and the paper is an important voice in the general discussion around learning with weak labels and semi supervised learning, two crucial problems in many practical applications. Taking this into account I recommend to accept the paper as a poster.   The reviewers have raised several problems that the authors have been exhaustively discussing in their rebuttals. The one remaining issue is the interaction between calibration and the threshold. This problem has to be clarified in the final version of the paper, as indeed calibration usually does not change the order.
This paper introduces a scalable method for FSP based on FBSDE. The method is theoretically derived then applied on two problems, one simple but with many (1000) agents, and one with only 2 agents but partial observability.  The main strength of this paper lies in the scalability and the time complexity of the proposed method. Computing Nash equilibriums for many agents is a difficult problem and this paper is interesting in this aspect.  However, the reviewers point out several weak points to this paper. The difference with a previous work by Han, Hu and Long needs to be highlighted. Some parts of the paper are not clear, and too much of the important results are pushed into the appendix. Maybe this work is not best fitted to a conference format, and should be submitted to a journal? Another concern raised by the reviewers is that the experimental section does not show significant enough results, and that it is surprising to see a 2 agents problem as an illustration of a method that is aiming at addressing scalability with respect to the number of agents.  Reviewers agree on rejection for this paper, although by a small margin. I therefore recommend rejection. I think that if the authors improve this paper by following the reviewer suggestions, it can be accepted in a future venue.
This paper adapts the semi supervised DP learning methods based on voting to FL. Specifically, PATE and private kNN. The adaptation is fairly straightforward as those methods rely on averaging of votes a primitive that is a standard part of FL. The framework assumes that unlabeled data from the same distribution is available to the server, a very strong assumption. As pointed out in the reviews, the empirical evaluation has a number of major issues. For example, the comparison is with fully supervised SGD based techniques instead of a gradient based semi supervised approach. 
Three reviewers agreed to reject and the other reviewer also suggested it is below the threshold.
Summary: This paper introduces a novel type of reward shaping (not potential based) that can work for MDPs with 0/1 rewards (e.g., systems with goal states). I think the paper contains a nice nice of empirical results, clear explanations/insights, and theoretical contributions. As long as the authors do a good job saying up front what kinds of MDPs the method does/does not address, this would provide an interesting addition to practical methods for solving sparse reward RL tasks. The paper could be strengthened with additional empirical results, but that is almost always true, and I think the number and quality of experiments are well above the bar. The paper could also be strengthened if it better compared with other ways of using demonstrations. This is done in the text, but not empirically, as the authors wanted to focus on reward shaping methods. I think this is a relatively minor point and does not outweigh the many positives.  Discussion: One reviewer remains against accepting this paper. I respectfully disagree with their evaluation and unfortunately they did not update their review after the responses. Hopefully they would agree the final version of this paper is indeed a useful contribution.  Recommendation: I believe this paper should be accepted based on the science. 
Good clarity: a NAS benchmark for ASR and results transferable across datasets. Although this is more specific to speech domain, building such a benchmark for speech is important for general NAS research, especially the papers finds different behaviors compared to image classification benchmarks.   The main factor for the decision is the clarity and importance for NAS in speech domain. 
This paper introduces ICRL, where the RL agent is supposed to maximize the reward under unknown constraints, which should be inferred from the expert demonstration. Reviewers generally agreed that this is an interesting work, and potentially make RL to be applied to more general settings. However, they also would like to see more experimental results with baselines (e.g. agents based on IRL and also related constrained learning approaches) to make the motivation behind the approach more convincing. I hope these concerns are addressed in the future work.
This paper considers meta learning based on MAML.  The authors use Neural Tangent Kernels (NTKs) to develop two meta learning algorithms that avoid the inner loop adaptation, which makes MAML computationally intensive.  Experimental results demonstrate favorable empirical performance over existing methods.   The paper is generally well written and readable.  The proposed methods are well motivated and based on solid theoretical ground.  The emprirical performance shows advantages in efficiency and quality.   This work is worth acceptence in ICLR 2021. 
This work introduces a method for supervised learning that takes a data generating process into account. While the paper proposes an interesting approach to learning a causally invariant model, the reviewers had several concerns about the proposed method. I thank the authors for having the paper revised, addressing the reviewers  comments. However, there are still legitimate issues unresolved about the specific theoretical results and assumptions made throughout the work.  I share similar concerns, and, therefore, recommend rejection. Still, I would like to encourage the authors to address the reviewers  problems in the paper s next iteration.  
This paper presents an approach for modular multi task learning. All the reviewers believe the goals are appealing and the idea is reasonable. However, R2 and R4 raise concerns with respect to novelty. There are also strong concerns regarding experiments. The concerns vary from reproducibility to small improvements and right baselines. The rebuttal fails to provide any new experiments or handle the reviewer concerns. All reviewers and AC agree that paper is not yet ready for publication.
This paper proposed an additional training objective for unsupervised neural machine translation (UNMT). They first train two UNMT models and use these models to generate pseudo parallel corpora.  These parallel corpora are used to optimize the UNMT training objective. The experiments are conducted on several language pairs and they also compared with several alternative works.   All the reviewers admit that the proposed method is straightforward and effective. The authors claim that the new training objective is used to enhance the "data diversification". This point has been questioned by the reviewers. Some reviewers are convinced by the response and some still have different opinions.  From my point of view, the proposed method can also be considered as a kind of combination of  (pseudo) supervised NMT and unsupervised NMT.   The presentation and description of its key contributions seem unclear. However, we encourage the authors to modify their paper and we believe this proposed method can inspire the MT community for further research. At the moment, the paper is seen as not yet ready for publication at this time.
This work brings improvement to contrastive learning method for text data by combining a Wasserstein objective with a "memory bank" strategy for getting (and updating) hard negative samples. The approach leads to small but consistent improvements across a variety of representation learning tasks in both supervised and unsupervised settings. While the paper makes a useful contribution and evaluates with some success on downstream tasks, the reviewers would like to see some intrinsic, qualitative discussion of the representation learning itself, and in comparison to more powerful contrastive learning methods. Overall the work falls below the acceptance threshold in a very competitive venue, so I cannot recommend acceptance.  Even after discarding one uninformative review, the consensus remains borderline. The reviewers are, however, appreciative of the additional clarifying experiments provided by the authors. In the internal discussion, a concern was raised that the BERT baselines may not have a fair chance to compete, as they are not fine tuned on the unsupervised data in the same way that the proposed method is, leading to possibly overestimating the improvement.  I encourage the authors to consider this.  Finally, while some concerns about clarity have been addressed, some remain in the current version. In particular, I spot at least three duplicate and inconsistent entries, for Hjelm et al, Lin et al, and McAllester and Stratos.)
 This paper proposed a new method to prune neural networks using a continuous penalty function. All reviewers suggest acceptance (some are on borderline though) as the authors did a good job in the rebuttal phase. AC also could not find any particular reason to reject the paper (in particular, the overall writing is clear) and thinks that this paper is a meaningful addition to ICLR 2021. 
The paper proposes an unsupervised representation (embedding) learning method for time series. Overall, the paper is well motivated, well written and easy to follow. As agreed by all reviewers, the idea is interesting. To further improve the paper, the authors are encouraged to justify the choice of encoder architectures and window size, and describe more clearly how the statistical test is incorporated.
Four knowledgeable referees have indicated reject mainly because of limited motivation [R1,R3,R4], limited insights on the proposed approach [R1,R2,R3,R4], and inconclusive results [R1,R2,R3,R4]. The claims of the paper could have been strengthened by e.g. discussing currently missing experimental details [R1], performing statistical significance tests [R2,R4], and including comparisons to baselines/previously introduced normalization strategies [R2,R3]. Unfortunately, there was no rebuttal. The paper is therefore rejected.
This paper invariantizes distribution based deep networks by using pairwise embedding of the set’s elements.  The idea is inspired from De Bie et al. (2019), which allows invariance to be incorporated through the interaction functional.  Although the paper is well executed with solid theoretical analysis and solid response to the reviewers  comments, the novelty is limited, and reviewers have concerns with experiments and presentation.  
This paper introduces an approach to model explainability on high dimensional data by: (1) first mapping inputs to a smaller set of intelligible latent features, and then (2) applying the Shapley method to this set of latent features. Several methods are considered for (1), and empirical results are examined across several settings.  Reviewers were mixed in their views   one reviewer was in favor of acceptance, and three were against.   Some concerns were addressed by authors in the discussion period but remaining issues include: concerns over the faithfulness of the approach;  missing comparisons to other related methods such as CaCE; and  a desire for more in depth discussion of the pros and cons of the different methods considered for (1).
All reviewers agree that the paper overclaims its contributions both in the main text and in the title, and given also the limited novelty  and scope it is not suggested for publication.
All reviewers except for AnonReviewer1 were in favour of accept.  AnonReviewer1 was strongly in favour of reject, but AnonReviewer2 argued against some of AnonReviewer1 s opinion.  The authors also gave a coherent, well argued statement of their contribution.  Nevertheless, there are some improvements still needed.  Position:   the scope of the uncertainty estimation to Dirichlet based uncertainty estimation techniques was limited.  Sticking to Dirichlet based uncertainty is limited, although the coverage of methods within the Dirichlet based family is OK but could be improved. Note (from AnonReviewer1 s comments) Joo Chung and Seo, ICML 2020, is one paper that should be included and Chan, Alaa, and van der Schaar, ICML2020 is also relevant.  While its not about adversial attacks it covers a related idea with a good technique.   Finally, these papers cite Ovadia, Fertig Ren etal. NeurIPS 2019, which is an excellent summary of calibration and estimation under shift, not exactly adversarial attacks but surely related.  The big winner is deep ensembles (Lakshminarayanan etal, NeurIPS 2017).  I think using deep ensembles directly would be a good complement to the Dirichlet methods in this paper. Note, also, the authors already included additional works mentioned by AnonReviewer2.  Critique:   The authors proposed a robust training strategy but this didn t lead to uniform improvement.  Position:  The scope of the adversarial attacks is limited.  The attacks covered are a good though basic range.  But because these show problems, the argument is that more sophisticated attacks do not need to be studied.  Position:  The datasets covered is limited.  Certainly, there are problems with extending experiments to text data.  But the argument is that if things don t work well for the smaller datasets given, then that is still a problem, so why bother extending the evaluation to larger datasets.  Arguably, the latter two positions have been addressed by the authors, but not the first two.  This makes the paper marginal. So this is a good publishable paper, but comparatively marginal.
This paper looks at chaos in learning in games, extending a line of work in two players zero sum games (that I found quite restrictive in the past). It somehow reduces the class of more general games to zero sum and cooperative games (this decomposition is already known) so that the techniques can be transposed here.  The paper is interesting, yet sometimes difficult to follow, and I am not certain that it gives many new insights.   Nonetheless, we believe its quality justify acceptance.
The paper was evaluated by 3 knowledgeable reviewers, where 2 reviewers were leaning for acceptance and one reviewer argued for rejection, rendering the paper a borderline paper. The positive negative points that were raised about the paper during the discussion are summarized below:  Strength:   The presented policy optimization method provides strong results   It provided strong insights into the benefits of differentiable simulators for trajectory guided reinforcement learning   The direction of differentiable physics simulators is very promising and the provided benchmarks are interesting  Weak points:   (i) The main contribution of the paper is a novel trajectory optimization method that uses analytical gradients. In a second step, a neural network is fitted to generalize the control from the single trajectories. The given approach is very much related to existing methods such as GPS or IGOR, just that the trajectory optimization is different. A comparison to these methods is needed. For example, how does the algorithm compare to using iLQG as trajectory optimization method ( we could also use analytical gradients for the linierazations used in iLQG)?   (ii) While the presented tasks are very interesting, there is no benchmark on a more well known task. Hence, it is hard to evaluate the performance in comparison to other algorithms.  The paper defintely has interesting contributions in terms of the new trajectory optimization method and I could live with (ii) as the presented experiments are challenging and interesting, the contribution needs to be better evaluated as comparisions to other trajectory optimization methods are missing. I am sure that the paper will be accepted at another conference with this additional experiments, however, without it the paper is incomplete and I can unfortunately not recommend acceptance.  
The reviewers point out several important issues to be addressed, including comparing to other methods that can address the "combinatorial generalization" problems studied (one reviewer points out the crucial difference from "compositional generalization" studied before), addressing the gap between the proposed dataset (simple and has the value of diagnosing/model debug/research algorithm development) and real datasets/problem settings.     As such the AC recommends Reject and encourages the authors to take the constructive feedback to improve. 
This paper proposes that we can understand the evolution of representations in deep neural networks during training using the concept of "usable information". This is effectively an indirect measure of how much information the network maintains about a given categorical variable, Y, and the authors show that it is in fact a variational lower bound on the amount of mutual information that the network s representations have with Y. The authors show that in deep neural networks the usable information that is maintained for different variables during training depends on the task, such that task irrelevant variables (but not task relevant variables) eventually have their usable information reduced, leading to "minimal sufficient representations".   The initial reviews were mixed. A common theme in the critiques was the lack of evidence of the generalization and scalability of these results. The authors addressed these concerns by including new experiments on different architectures and the CIFAR datasets, leading one reviewer to increase their score. The final scores stood at 3, 7 ,7, 7. Given the overall positive reviews, interesting subject matter, and relevance to understanding learned representations in deep networks, this paper seems appropriate for acceptance in the AC s opinion.
The paper introduces the new task of few shot semantic edge detection by adapting existing datasets. It proposes a new method which is compared to a baseline.  Pros:   Clear writing.    Extensive ablation experiments.   Good architectural choices.  Mixed:   The value of the new task raises a mix of opinions. For example R1 sees it as a "relevant problem, and is well suited for few shot tasks", but R2 finds is very similar to few shot segmentation. I think a more interesting version of the problem (that would also create more separation to few shot segmentation) would be to also consider internal edges, not just "semantic boundaries". For example the original BSDS dataset has pure edge annotations.    Besides the task, another novelty of the paper is the proposed multi split matching technique, but while it is well demonstrated empirically (as backed by additional results given by authors in rebuttal), R3 would like to have seen "theoretical or analytical reasoning" and R1 says it is an "ad hoc technique".  Cons:   the PANet+Sobel baseline. All 4 reviewers are unhappy with this baseline: 3 of them find it unfair because of the non standard edge thickening employed and 2 think there would be more recent and better baselines. The authors provided a rebuttal arguing that their GT edges are "not too thick to be unfair" but two of the reviewers mentioned they remained unconvinced   R1 hopes "the authors will work on cleaner evaluation of the baseline" and R4 find the baseline "still unconvincing in the revised version".  Overall the paper would benefit from one more iteration focusing on the evaluation procedure to be convincing and impactful.  
In this paper, the authors propose a new layer by layer training approach for GNN in particular for a large graph. The proposed approach can be easily parallelizable and scale well to a large graph. Reviewers are concerned about the novelty of the approach and the lack of theoretical analysis, and it is not well addressed by the rebuttal. Therefore, this paper is below the acceptance threshold of ICLR. I encourage the authors to revise the paper based on the reviewer s comments and resubmit it to a future venue.
The paper received borderline scores, before and after the rebuttal. Thus, support for paper acceptance isn t sufficiently strong. While the reviewers see merit, concerns which remain after the discussion phase, include how convincing the experimental settings and results are, and uncertainty about the motivation and practical value.
The authors present a hierarchical factorization of the Poisson matrix and explain why sparcity in the encoder is important for interpretability. The reviewers appreciated the contribution of the paper and highlighted the advantage of such an approach for users. The authors have improved their initial version by adding more detail on inferences and experiments.  The decision is to accept the paper.
This paper proposes a new idea for performing knowledge distillation by leveraging teacher’s classifier to train student’s penultimate layer feature via proposing suitable loss functions. Reviewers appreciate the simultaneous simplicity and effectiveness of the method. A comprehensive set of studies are performed to empirically show the effectiveness of the method. Specifically, the proposed distillation method is shown to outperform state of the art across various network architectures, teacher student capacities, datasets, and domains. The paper is well written and is easy to follow. All reviewers rate the paper on the accept side (after the rebuttal) and believe the new perspective this work provides on distillation and its simplicity to implement can lead it to gain high impact. I concur with the reviewers and find this submission a convincing empirical work, and thus recommend for accept. 
This work uses a graph representation of the protein backbone and a GNN for model quality assessment (MQA) and protein design. The proposed GNN has the property that the vector and scalar outputs are equivariant and invariant with respect to composition of 3D rotations and reflections. Overall speaking, the reviewers like this paper very much (especially its technical novelty), and provide quite positive comments. On the other hand, there are also some concerns being mentioned:  1)	The datasets used in the experiments are a little old – experiments on CASP 13 are preferred. 2)	Some technical details are not very clear and the paper writing needs improvements 3)	Experimental comparison with some recent baselines is missing.  The authors did a good job in their rebuttal and paper revision. Most of the above concerns have been addressed. Therefore, we think the current version of the paper is clearly beyond the bar of ICLR. 
This paper introduces Transformer QL, a new variant of transformer networks that can process long sequences more efficiently. This is an important research problem, which has been widely studied recently. Unfortunately, this paper does not compare to such previous works (eg. see "Efficient transformers: A survey"), the only considered baselines being Transformer XL and Compressive transformer. Moreover, the reviewers found the experimental section to be lacking, as the results are weak compared to existing work, and important ablation studies are missing. The authors did not provide a rebuttal. For these reasons, I recommend to reject the paper.
This paper presents a number of techniques to improve the existing non autoregressive end to end TTS model   FastSpeech. These techniques include replacing the teacher forcing with ground truth targets and using a variation adaptor to introduce auxiliary information such as duration, energy and pitch.  The experiments show that the proposed Fastspeech 2 model is faster in training  compared to the existing FastSpeech model and meanwhile can still achieve high quality synthesized speech.  The work reported in the paper is essentially about system improvement over FastSpeech but has it value in the speech community given the current interest in non autoregressive rapid TTS.  On the other hand, concerns are also raised regarding the complexity of the pipeline and the significance of the novelty. The authors  rebuttal is good and has addressed most of the concerns.  Overall, this is an interesting paper and can be accepted.  
This paper studies the problem of learning better video text representation learning with an application to video text retrieval. It proposes a key innovation: it uses a new generative task of cross captioning that addresses issues with contrastive learning by learning to reconstruct a sample’s text representation as a weighted combination of a video support set, using a novel objective function using video set bottlenecks. It uses pre training based on YouTube video ASR pairs, and shows empirical results where the proposed method outperforms multiple SOTA methods.  The authors have addressed the feedback of the reviewers, especially with the following improvements:    Experiments were run on more datasets   Relevant work pointed out by the reviewers were added   Concerns regarding technical details were clarified 
While this paper was received pretty well, especially after the revision, reviewers still find it borderline and request further revisions which we cannot check in this short review cycle. Therefore, we encourage the authors to improve the paper and resubmit to a future venue. In particular, please take into account the reviewers  comment to improve the clarity of the paper. Particularly it is critical to clarify the function class you are working with (essentially polynomials) more clearly than what you currently do (i.e., your current gradient definition). It would be helpful for future work to clearly state that this function class is a shortcoming of your work, and that an interesting direction is to extend this to natural function classes in ML (e.g., logistic loss).
This paper proposes an input dependent dropout strategy, using variational inference to infer the rates.  While the idea is a fairly straightforward variant of recent probabilistic dropout methods, the paper demonstrates consistent improvements across several types of NN layers (dense, convolutional, and attention) in large scale experiments (e.g. ImageNet).  The reviewers unanimously agreed on accepting the paper.
First, I d like to thank both the authors and the reviewers for extensive and constructive discussion. The paper proposes a generalization of SAC, which considers the entropy of both the current policy and the action samples in the replay pool. The method is motivated by better sample complexity, as it avoids retaking actions that already appear in the pool. The paper formulates a theoretical algorithm and proves its convergence, as well as a practical algorithm that is compared to SAC and SAC Div in continuous sparse reward tasks.  Generally, the reviewers found the method interesting. After rounds of discussion and revisions, the reviewers identified two remaining issues. Theoretical analysis still requires improvement and the positioning of the paper is not clear. Particularly, the method is motivated as an exploration method, and it should be evaluated as such, for example, by comparing to a more representative set of baseline methods. Therefore, I m recommending rejection, but encourage the authors to improve the work bases on the reviews, and submit to a future conference.
This paper is not ready for a publication at ICLR, as agreed unanimously by the reviewers.   There are three main reasons for that: 1. Novelty: it is mentioned in the paper that “To the best of our knowledge, a multi span QA architecture has not been proposed", which is certainly incorrect. See the multiple references provided by the reviewers. 2. Evaluation: there is no evaluation in multi span setting on a public dataset. SQuAD being single span, As stated by R1, "Experiment on Amazon internal data is included, however, as the detailed description or the data statistic is missing, it cannot be considered as academic empirical evaluation." Several public datasets could be used like DROP, Quoref, or Natural Questions. 3. Motivation: the reviewers also note that the clarity and motivation behind the work could be improved. Some choices of the architecture or the model should be more clearly justified.  We encourage the authors to look into the multiple comments in the reviews in order to improve the paper and the research project overall.    
This paper proposes  a novel and interesting embedding of graphs emulating the Wasserstein distance. The experiments are good and the authors did a detailed answer taking into account the comments of the reviewer. The responses were appreciated and the AC recommends the paper to be accepted.
After carefully going through the reviews and rebuttal, and looking at the content of the paper as well, I feel there are some issues with the current manuscript. As also pointed out by AnonReviewer5 and AnonReviewer2, the text lacks clarity. From specifically defining what a schema is, to being more explicit about the limitation of the work.  I understand that the authors are interested in a largely unexplored setting, and hence there might not be a lot of prior work to cement the evaluation protocol. Particularly because of this I think such papers need to be upfront and clear not only in what is the setting and what is the evaluation but also what are the limitations and open problems.   I do agree that there is value in this direction of research, and that the idea of re ordering the features using attention (which I have to agree it is reminiscent of Bahdanau et al., ICLR 2015   though the semantics of it and its purpose makes it novel here) might be a way forward. But I do think for the paper to make an impact (and be ICLR ready) it needs more work both in the writing and maybe on the experimental side as well (consider some more complex task, or be more explicit on what is the common aspect between tasks in the distribution that can allow chameleon to work)
This paper uses concepts from physics to make predictions about stochastic gradient descent. The reviews point to two issues. Firstly, the paper was not very accessible to those without a relevant background, and this is reflected in the low confidence rating reviewers gave. More importantly, two of the reviewers consistently pointed out  vague mathematics  and oversimplification in the mathematical arguments.  The authors  feedback did not successfully address the reviewer s concerns, both R3 and R4 indicated there were outstanding concerns.  I should note that despite giving low confidence scores and stating that some concepts from physics are beyond their field of expertise, reviewers gave high quality reviews with detailed comments and questions, and subsequently participated in the discussion revisiting their reviews. This suggests that the low confidence is not a symptom of insufficient reviewer effort, but perhaps a consequence of an inaccessible paper.
The paper presents some interesting insights, but all reviewers have agreed that it does not meet the bar of ICLR. The theoretical results require revision as several issues have been indicated in the reviews. The authors have tried to correct them during the rebuttal, but the reviewers remain unconvinced.  Also the novelty is limited as re ranking is a well known concept and decoupling of head and tail labels is an approach often used in practice across many applications.  The authors should also clarify the way the RankNet method is used and implemented to clarify the issue raised by Reviewer 1. Finally, let me notice that adjusting thresholds for labels has been considered in the XMLC literature, in the context of optimization of the macro F measure (Extreme F measure Maximization using Sparse Probability Estimates, ICML 2016).  
This paper provides theoretical justifications on why the data augmentation technique, Mixup (convex combinations of pairs of data examples) , can help in improving robustness and generalization of GLMs and ReLUs. The authors rewrote a Mixup loss function as the summation of a standard empirical loss and some regularization terms regularizing gradient, Hessian and some higher order terms. Using the quadratic approximation of the Mixup loss (ignoring the higher order terms), the authors proved that the quadratic approximation of the Mixup loss was equivalent to an upper bound of the second order Taylor expansion of an adversarial loss, providing justifications for why Mixup loss training could improve robustness against small attacks. Using the same quadratic approximation of the Mixup loss, the regularization term controlled the hypothesis class to have a smaller Rademacher complexity.  Overall, the paper provides insightful theoretical interpretations for a commonly used data augmentation technique in DL. The paper also supports its claims by numerical experiments. Although there is some minor concerns on using the quadratic approximation of the Mixup loss, as well as R3 term s regularization effect on a broader family of models, the paper provides unique and novel insights on Mixup.; all reviewers acknowledge the authors applying the existing proof techniques to analyze Mixup s effect on robustness and generalization.  Therefore, I recommend accepting this paper.
The paper gives a way of constructing a dataset of programs aligned with invariants that the programs satisfy at runtime, and training a model to predict invariants for a given program.  While the overall idea behind the paper is reasonable, the execution (in particular, the experimental evaluation) is problematic. As a result, the paper cannot be accepted in its present form. Please see the reviews for more details.
This paper improves the wait k based simultaneous NMT by training on an adaptive wait m policy with a controller determining the lag for sentence pair.  The controller is trained with RL to minimize the loss on a validation set. The overall model is reasonable, which is well presented. I however have the following two concerns 1. There is a clear mismatch between training/inference strategies, which raises two problems     1. The motivation:  the authors tried to explain that in discussion,  but it is not convincing enough     2. The title is misleading since there is no future information to use during inference  2. The experiments is not convincing enough in that a) the improvement over baseline is modest, and b) comparison to adaptive wait k and other strong baseline is insufficient   In conclusion I would suggest to reject this paper.  
This work provides interesting insights on the transferability of adversarial perturbations and proposes ways of making it more effective. While several reviewers have found parts of the paper unsatisfactory, there are interesting results to merit acceptance.
This is an empirical paper that proposed a few different settings for applying GNNs on temporal data, including what context window to use, code start vs warm start, incremental training vs static.  This paper also proposed and released a few more temporal graph datasets, which could be useful.  The consensus assessment of the reviewers is that the contributions of this paper are incremental, and the results are expected and not exciting enough.  I want to in particular point out that the results highlighted in the paper, that a GNN with window size 1 is sufficient to recover 90% of the performance of the model on full graph, is probably not the correct message to communicate.  This either indicates that the data and task used in the benchmarks do not require sophisticated long horizon temporal information (which makes the comparison between any methods uninteresting), or it indicates that the metric is not sensitive enough to sufficiently distinguish models trained with different settings.  I would recommend rejection and encourage the authors to improve this paper.
All reviewers recommend that the paper be rejected.  The reviewers appreciate the line of research and is worthwhile, but find that the paper lacks in technical novelty and insight.  The AC is in consensus with their reviews due to the concerns raised regarding novelty and insight and recommends rejection.
This submission develops a novel technique for domain adaptation for the setup where only a trained model (but no data) from the source task is available. The authors propose to fine tune the feature encoder using batch norm statistics of the features extracted. Additionally their criterion also promotes increasing the the mututal information between features and target classification. The developed method is experimentally evaluated on several benchmarks.  Pros:   The problem considered is of practical relevance and general interest in ICLR community   The proposed methodology is well motivated and shows good performance  Cons:   There is no thorough formal analysis of when the method would work and not work; not even on an intuitive level (state conditions under which the proposed method should be expected to work better/worse than other state of the art optimization criteria for the same setup   Alternatively to a sound theoretical analysis, the authors should provide a more extensive set of ablation experiments (this was mentioned by several reviewers)  In the current format, it remains unclear, how the research community would benefit from the study presented.
This submission generated a lot of discussion.  The main strengths of the paper: * It is an interesting application of meta learning (to 3D shape completion), and a novel one. * It appears to work well: it is remarkable that the proposed model can reconstruct shapes as well as it does given a point cloud with only 50 input points.  The main concern (raised by two reviewers) is that while the method is described using the language of meta learning, the proposed architecture ends up looking very similar to a variational autoencoder: a point cloud encoder that outputs some distribution in a latent space, which is then sampled from to produce a code which drives an implicit surface decoder. The only difference appears to be that the proposed method uses a factored distribution in latent space, whereas a traditional VAE uses a non factored one (i.e. a single multivariate Gaussian over all dimensions of the latent code).  One reviewer engaged the authors in a discussion about this point, but the resulting conversation was not satisfactory. One interpretation of the authors  response is that they are simply not aware of how a VAE could be trained using variable sized point clouds as input (which is quite possible using many standard point cloud processing networks). However, at other points, they do seem to grasp this, when they write that "This flexible representation of the uncertainty [i.e. the one proposed by the authors] cannot be attained by the mere average or max pooling aggregation [what a PointNet encoder would do]." They even go on to provide an additional ablation study where they replace their factored probabilistic encoder with a deterministic mean/max pool encoder, and show worse results. Unfortunately, they never compare against *probabilistic* variants of such encoders (i.e. where the mean/max pool output is then used to compute a mean and variance).  Without seeing this comparison, the reviewers believe that this paper cannot be accepted, and I am inclined to agree.  On a related note: one reviewer pointed out an issue with unfair comparisons, in that baselines were trained on high density point clouds and evaluated on low density ones. The reviewer noted that these methods could be (and should have been) trained on point clouds of varying density. Perhaps this relates to my hypothesis that the authors initially did not understand that training such encoders on variable sized point clouds was possible. In any event, in their rebuttals, they have reported some preliminary results from experiments which do this type of training, but these results are not conclusive. Complete, conclusive results from these experiments would also need to be presented before this paper could be accepted.
The paper proposes transfer learning where the target domain data is evolving along time.  They use both labeled and unlabeled data to learn domain and time invariant features based on a discrepancy measure they introduce.  Their proposed algorithm uses VAE to learn such features.  Reviewers have mixed response, although the author feedback did help.   The main limitation with the paper is that it does not seem to be aware of the very extensive literature on continuous domain adaptation.  The related work only discusses papers on transfer learning, multi source domain adaptation, and continuous learning.  But ignores papers on continuous domain adaptation which are much more related to this paper.  The most recent of these that appeared in ICML 2020 also attempts to learn time invariant features using adversarial methods.  Unfortunately, the reviewers seem to be also unaware of this literature:  1.  Continuously Indexed Domain Adaptation,  Hao Wang, Hao He, Dina Katabi, ICML 2020 2.  Active Adversarial Domain Adaptation  3.  Continuous Domain Adaptation using Optimal Transport 4. Learning to Adapt to Evolving Domains   NeurIPS 2020 5.  Judy Hoffman, Trevor Darrell, and Kate Saenko. Continuous manifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 867–874, 2014. 6 Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Adagraph: Unifying predictive and continuous domain adaptation through graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6568–6577, 2019.  7 Atsutoshi Kumagai and Tomoharu Iwata. Learning future classifiers without additional data. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.  
I agree with the majority of reviews that this paper is not sufficiently convincing. 
The paper begins with an observation in standard trained CNNs that the correlations in the output channels are high. Building upon this the paper proposes a new "optimizer" which modifies the gradients to encourage corelations among output channels. They provide a theoretical foundation for the method, by deriving the gradient through placing a riemannian metric on the manifold of parameter tensors which encourages smoothness along the output channel dimension. Two variants (one based on a Sobolev metric) are proposed and are experiments are provided. The underlying idea and the derivation of the gradients were generally appreciated by the reviewers. However some reviewers maintained their concern regarding the effectiveness of the performed experimentation. The gains demonstrated are relatively small over the baselines and more importantly the baselines are quite far off the state of the art baselines for the particular problems. This is the primary reason for my recommendation as experiments are the only source of understanding whether the method is effective (there is little theory   mostly at an intuitive level to justify the form of the optimizer). Overall, I strongly encourage the authors to explore the idea further and strengthen the paper with stronger baselines (perhaps on larger datasets) and resubmit. 
The paper shows that for a simple nonlinear (quadratically parametrized linear) model, stochastic gradient descent (SGD) with a certain label noise and learning rate schedule recovers the data generating model. In contrast, gradient descent with or without Gaussian noise fails. While the results are novel and interesting, they hold for a rather specialized model, which may not reveal anything about deep neural networks, which was the original motivation for this work. Given the narrow focus of the work, unfortunately, I cannot recommend that the paper be accepted. 
The reviewers initially assessed this paper as slightly below the acceptance threshold. The reviewers seem to agree on the novelty and potential impact of this project, but they also highlighted the lack of clarity of the manuscript including lack of clarity in the method used to encode the graph data.   As the authors noted, graph related questions were the focus of most of the comments and questions from the reviewers. This is not because the reviewers did not understand and assess the method from the continual learning side (I am also meta reviewing several continual learning papers and I believe that I can assess the novelty of this work). As I wrote above, reviewers were convinced of the paper s motivation.   The authors provided good responses and discussed with at least one reviewer thoroughly. These interactions seem to have clarified important aspects of your proposed methodology and notably the properties of your graph construction method. I found that your new results on larger datasets also provide an improvement. However, to be properly assessed, this number of clarifications regarding the core method requires a new round of reviews. The discussions have also highlighted some of the limits of your approach which do not seem to be acknowledged in your paper. This includes the discussion with reviewer2 regarding constraints on L & K, node classification (also I find that one to less important), and comparison to GraphSage on the non lifelong learning scenario.  Overall, and while I agree that continual learning from graph data is an important and unexplored problem, I also find that the current manuscript lacks clarity and, even though the ICLR discussion allowed reviewers to discuss these with the authors, there are still significant ways to improve the clarity of the current manuscript. As a result, I do not recommend acceptance of the current manuscript.   I strongly suggest the authors keep on working on their manuscript as their idea seems to have potential and I would imagine that it may become one of the first works in a new interesting line of research.
This paper proposed a new measure of effective gradient flow (EGF), and also compared sparse vs. dense networks on CIFAR 10 and CIFAR 100. The notion of EGF would be interesting, but the paper did not present enough evidence to support this notion.
Dear authors,  I like to topic of your paper very much. Indeed, your work is trying to show that 2nd order methods can be efficiently implemented in a distributed environment and can achieve improvement in training times.  However, having worked on distributed computing for many years, I personally think that reporting running time in your work is not very informative (without mentioning what hardware is used during computation), and one cannot understand the connection or reproduce your results. Also, it was not clear how the baselines were implemented, and how the hyper parameters were tuned. It is also not clear why you haven t picked better benchmarks to compare your work.   I think that addressing both the concerns from reviewers and the one mentioned above would improve the paper significantly. I would really like to see if accepted in the near future after these issues are fully addressed. 
This paper offers a new dataset and accompanying metric to measure the degree to which NLI (textual entailment) systems are aware of gender–occupation associations.  Pros:   The paper deals with an important issue in the context of a visible set of models and datasets.  Cons:   The metric is designed to evaluate bias on models trained for a specific, precisely defined task, but it does not conform to the standard formulation of that task, which makes results on those metric untrustworthy and potentially arbitrary. Reviews had concerns about both the data (the use of references to the form of the premise text) and the metric (the handling of  neutral  predictions).   The proposed definition of bias is not clearly mapped onto a concrete potential harm.   There has been substantial similar prior work on this problem. This doesn t invalidate this work, but it does raise the bar a bit, since arguments of the form  we need to start a conversation about bias in models  are not pursuasive. 
The paper is very interesting and novel, and all reviewers are of the same opinion.  The main concern, however, is on the experimental section that is limited to image classification benchmarks and that some critical comparisons are missing (e.g. clarify factors that play key role in improvement, more computation and therefore more free parameters, how about non discriminative tasks, etc).  The heterogeneity question is in my opinion only partially answered by the authors but I also feel proper handling of this matter would require a proper multi task setup and different target for the work. I also personally find applicability of the approach quite limited, I encourage the authors to further improve their work as I feel that with a proper revision would make a nice contribution for the community.
The paper got mixed ratings. However, keeping in mind the low confidence of some of the reviewers, the paper needed an additional look. The AC himself went over the paper. The paper presents an interesting formalism for private information retrieval. As reviewers have pointed out the formalism is based on several existing ideas on utility privacy tradeoff.  The use of GANs for enforcing privacy is also not new. The rebuttal did not convince some of the reviewers about novelty which seems reasonable given the area and literature in it.   Overall, the paper needs to consolidate all ideas of Adversarial training for privacy and compare and contrast with the proposed approach to make it compelling for publication. 
I think this paper has more positives than the reviews might indicate. And I do not share all the reviewers  concerns about the content of the paper. I think that there are a few concerns, though, that still suggest this paper should not be accepted as it is, when taken in conjunction with the concerns brought up by the reviewers.  On the positive side:     Asynchronous methods often give significant improvements, and the throughput benefits can even be seen here in Table 6.    The experiments are detailed with a lot of results comparing against many alternatives, including for ImageNet.  On the negative side:     The biggest concern I have with this paper is the scale of the experiments. This is supposedly about "distributed" SGD, but the largest scale experiment was run on only two workstations, and many experiments were run in the S1 and S2 settings which don t seem to be distributed at all (run on a single workstation). That is, there s a mismatch between the scale at which these experiments were run and the scale at which people want to run distributed deep learning.    The theory seems to be only an incremental change to the standard local SGD theory. The paper says "At a naive first glance, studying the convergence properties of locally asynchronous SGD would be an incremental to existing analyses for local SGD" but then it does not satisfactorily explain _why_ the approach is _not_ incremental. Not enough is done in the paper to explain why the analysis is not just a trivial combination of the local SGD with the standard approach to make an algorithmic analysis asynchronous. (Or, if the theoretical result _is_ incremental, the paper should make less of a big deal out of it.)    The description of the algorithm in Section 1.2 is confusing. I think it would benefit from being more concrete.  The paper should also compare against the paper "Asynchronous Decentralized Parallel Stochastic Gradient Descent" (Lian et al, 2017). It is actually not clear to me whether the method proposed here is a subset (or superset) of the method described in that paper, but they seem _very_ similar.
# Paper Summary  The goal of this paper is to improve generalization of fairness metrics by borrowing ideas from "mixup", which attempts to improve generalization in the non fairness setting by introducing convex combinations of training examples as virtual examples.  They adapt this idea by interpolating between protected *groups*, and adding a regularizer that forces the classifier to vary smoothly along this interpolation path. To this end, they show that, for a particular interpolation function, the (empirical) disparity in the fairness metric is upper bounded by their proposed regularizer (which depends both on the fairness metric, and the interpolation function). They consider two fairness metrics (disparate impact and equalized odds) and two interpolation functions (convex combinations in the feature space, or in a latent space).  As Reviewer 4 points out, the above is not a complete explanation for why their regularizer works: they ve only really shown that it upper bounds the empirical disparity in the fairness metric (and we could have regularized this empirical disparity directly, and indeed they do so, as a baseline, in their experiments). Presumably the intuition is that their regularizer is improving generalization by (implicitly) depending on virtual examples, but this isn t made explicit.  In a "theoretical analysis" section, they give closed form solutions using classification loss, along with L2 regularization and either (i) a regularizer penalizing the true disparity of impact or (ii) their proposed regularizer (which upper bounds the former). Both reviewer 4 and I seem to doubt if this adds much insight (the other reviewers didn t discuss this section).  They close with experiments on Adult, CelebA, and Jigsaw Toxicity, all of which show dramatic performance gains using their regularizer. However, they only compare to one external baseline (adversarial debiasing).  # Pros  1. Reviewers agreed that the paper was well written 1. The derivation of their regularizer is somewhat complex, but is described step by step, and very clearly 1. Adapting mixup to the problem of improving fairness generalization seems natural and intuitive, but this intuition is maybe given short shrift in the later sections 1. Experiments show impressive results  # Cons  1. Reviewer 1 notes that having the expected value of the classification function be equal for both protected groups does not imply fairness, since the classification function would presumably be thresholded to make hard classification decisions 1. Reviewer 4 points out that they do not actually explain why their regularizer will improve generalization better than the "usual" disparity regularizer. Instead, they only show that it upper bounds the empirical disparity in the fairness metric. Presumably, the intuition is that their mixup regularizer is doing something like adding "virtual samples" 1. I would like to see a more detailed explanation of how their regularizer is implemented, in the main text (they only say that it "can be easily optimized by computing the Jacobian of f on mixup samples") 1. Reviewers 1 and 2 would like more external baselines (there is only one at the moment, "adversarial robustness"), with reviewer 1 suggesting early stopping. The authors added a new early stopping experiment on CelebA to the appendix, but it would be nice to have this baseline included in all experiments in the main text  # Conclusion  Three of the four reviewers recommended acceptance, with the "reject" reviewer scoring it "5: weak reject". This reviewer had three main criticisms: (i) matching expected classification functions is not the same as matching classification *decisions*, (ii) fairness problems might not have a generalization problem to begin with, and (iii) the experiments don t include enough external baselines. I disagree with the second point, but agree with the other two. I think the third is the most critical, since the first could be solved in many cases by e.g. sampling instead of making hard deterministic decisions.  Overall, my opinion is that this is a borderline paper, but that it falls on the "accept" side of the boundary. The idea is intuitive, and exposition is clear, the derivation is quite interesting, and the experimental results are (aside from not having enough baselines) impressive.
The reviewers brought up significant concerns that were not resolved by the authors  responses. The concerns are too significant for the paper to be accepted at this time.
  This paper derives estimators and minimax guarantees for regression under additive Gaussian noise and distributional shift  Despite, some merits raised, limitations on too strong assumptions (knowing the entire marginal distributions, sample complexity bound that could become meaningless if what is assumed known itself is too sample intensive to approximate) were considered important drawbacks by the reviewers. Last but not least, the importance of domain adaptation for fixed design regression was questioned and the answer to that point was not convincing enough. 
This paper introduces neural attention distillation; a new scheme for erasing backdoors in a poisoned neural network. The paper performs an empirical evaluation of their proposed method against  6 state of the art backdoor attacks. The authors show that attention distillation succeeds by using only a small fraction of clean training data without any performance degradation. In addition, the authors have provided ablation studies to clarify the contribution of each component in their proposed approach. Reviewers find the simplicity and effectiveness of the approach an important attribute that may lead this work to have a high impact in the field. The paper is well written, and all reviewers rate it on the accept side. I concur with their opinions and comments and I recommend accept.
The authors carefully study a class of unsupervised learning models called self expressive deep subspace clustering (SEDSC) models,  which involve clustering data arising from mixtures of complex nonlinear manifolds. The main contribution is to show that the SEDSC formulation itself suffers from fundamental degeneracies, and that the experimental gains reported in the literature may be due to ad hoc preprocessing.  The contributions are compelling, and all reviewers appreciated the paper. Despite the paper being of somewhat narrow focus, my belief is that negative results of this nature are useful and timely. I recommend an accept.
This paper proposes benchmark tasks for offline reinforcement learning.  The paper has major strength and weakness, and it has resulted in very active discussion among reviewers, authors, and other participants.  The major strength includes the following:   The proposed benchmark is already heavily used in the community   Offline reinforcement learning is very important to solve reinforcement learning tasks in the real world   The paper covers a range of tasks and provides through evaluation of existing methods to be used as baselines  The major weakness is that it is not sufficiently convincing that the methods that perform well in the proposed benchmark tasks will perform well in the offline reinforcement learning tasks in the real world.    This is partly due to the nature of the benchmark tasks of offline reinforcement learning, which require simulators to evaluate the policies learned with offline reinforcement learning.  This means that one cannot simply collect datasets from real world tasks and provide them as benchmark datasets.    Although one cannot do much about simulators, benchmark tasks for offline reinforcement learning still have many design choices.  In particular, how should the datasets in the benchmark be collected (i.e., behavior policies)?  While the datasets in the proposed benchmark are collected with various behavior policies including humans, it is not necessarily convincing that the resulting benchmark tasks are good for the purpose of evaluating offline reinforcement learning to be used in the real world.  In addition to the suggestions given by the reviewers, a possible direction to improve the paper is to focus on the choice of behavior policies used to generate the datasets in the proposed benchmark.  One might then be able to provide some convincing arguments as to why performing well in the benchmark might imply good performance in the real world by relating it to the choice of behavior policies.
All three reviewers recommend acceptance after the rebuttal stage, and the AC found no reason to disagree with them. The proposed method is simple and effective, and the concerns raised about experimental validation and novelty seem well addressed in the rebuttal. 
The authors propose a Simple Spectral Graph Convolution (S2GC) variant of GCNs, using a modified Markov Diffusion kernel. The approach proposed aims to tackle GCN performance degradation with increased depth (oversmoothing), by combining techniques from earlier works (Simple Graph Convolution and APPNP). This is complemented by a spectral analysis of the properties of the scheme, as well by a comparison to state of the art on several datasets.
Sequence generation models trained via maximum likelihood estimation (or variants of so called  teacher forcing ) condition on *data* samples during training and on *model* samples for predictions. The susceptibility to this potential "mismatch" in input distribution is often referred to as exposure bias (EB).   This paper stresses that most research around EB is focused on addressing it, rather than defining and/or quantifying it. Thus the submission questions the severity of EB and attempts to operationalise a testable definition for it. Myself and all the reviewers strongly support the observations and the agenda, we find the question this paper asks an important one.   Despite our appreciation for this paper s relevance, we have identified a number of problems that prevent me from recommending this paper. I will comment on the two most important points:  1. The  operational definition  of EB in this paper is not sufficiently precise to be testable. It builds on the somewhat commonly accepted view that the effects of EB accumulate as the conditioning context grows longer, and that this causes a model to generate badly distorted sentences. This definition still leaves quite some room for interpretation (without specifying reasonable expectation about how these effects  accumulate  and what/how bad they are, it seems difficult to design tests). We acknowledge that the submission attempts to shed light onto some of these aspects by having some  control groups  using gold data and shuffled strings, but we did not find those sufficient (mostly in light of the next point).  2. MT evaluation metrics (essentially, string similarity metrics), most notably (but not exclusively) BLEU, are used in this work in a setting where we cannot easily grant that they have the discriminating power that the authors expect of them. See this is not a criticism about the imperfections of BLEU (or any other automatic metric), but about the lack of evidence supporting its use against unrelated sentences. We do not find it sufficient that some recent NLG papers have made similar use of it (I, for example, would have criticised those papers on similar grounds).  Overall, we believe this submission asks a relevant question, the insight about dependence on prefix is nice and might lead to a first operational definition of EB (which might be only a few refinements away from the version proposed here). The current evaluation is unconvincing and I believe the authors should be able to find more credible strategies, especially, strategies that have already gone through some scrutiny (for example, in literature around OOD detection and tests for distribution shift).   Though I do not recommend this paper for acceptance, I hope the authors will find valuable feedback in the expert reviews attached.
I am recommending rejection for this paper for the following reasons:  I agree that the main claim is an obvious consequence of the structure of the GAN generator and the prior. I m also not sure why the authors restricted their analysis to GANs, but that s not a super important point to me.  More important is that the experimental validation of the ensembling idea is way below the bar for this conference.  Moreover, I know the authors touched on this a bit, but all of these modern GAN variants that work on imagenet are implicitly ensembling anyway through e.g. the conditioning input and the structure of the special batch norm.  Finally, the authors didn t respond to the reviews. 
Existing works mostly focus on model compression for the classification task. This paper aims for an efficient recommendation system that can well balance the model compression and model accuracy, which therefore brings in new challenges and opportunities. The authors propose to unify the model compression and feature embedding compression and develop an effective and reasonable solution.  The concerns raised by the reviewers have been well fixed and all reviewers agree on the paper s contribution.  The paper is therefore recommended for acceptance. 
While reviewers appreciated the simple approach of this work, the biggest concern reviewers had was with the security guarantee of the method. R4 argued that in a certain case recovering an original image x_1 amounted to guessing 2 coefficients. In the discussion phase the authors argued that security amounts to the adversary guessing 4 floating point numbers, not 2, which requires 100s of millions of years to decode an image correctly. However, R4 is correct that only 2 floating point numbers are necessary. This is because, as described by R4 when one sees outputs x_1 * a_{2,2} and x_2 * a_{2,1}, they can reconstruct x_1 as:  x_1   (x_1 * a_{2,2}   x_2 * a_{2,1}) / (a_{1,1} * a_{2,2}   a_{1,2} * a_{2,1})  Now define:  b_1 :  a_{2,2} / (a_{1,1} * a_{2,2}   a_{1,2} * a_{2,1}) b_2 :  a_{2,1} / ((a_{1,1} * a_{2,2}   a_{1,2} * a_{2,1})  Thus the above equation can be written as:  x_1   x_1 * b_1   x_2*b_2  So an adversary needs to guess 2 floating point numbers. Further, R4 points out that an adversary can obtain x_1 up to a scale factor by simply guessing the relative ratio of the the 2 unknown floating point numbers, i.e., if our guess is c:  x_1/c   x_1 * (b_1/c)   x_2 * (b_2/c)  This is a single floating point number, and not all floating point numbers need to be checked. For many images, information can be leaked even if the true scale of the image is not known.  For this reason I would urge the authors to strengthen the security guarantee of their approach. One way to do this would be to adapt the method so to make the resulting guarantee be a more standard one (e.g., differential privacy, standard cryptographic hardness guarantees). This would eliminate the main reviewer concerns and greatly strengthen the paper.
The reviewers felt that the idea of learning a posterior distribution on optimization algorithms is very novel. However, the negative flip side of this novelty was that it was not clear how the prior and likelihood were defined so that Bayes rule could be approximated. The three reviewers appeared to find the paper somewhat confusing, and while the authors  made significant changes, it would be better to resubmit for a new set of reviews of the revised paper.
The paper proposes a sensitivity based pruning method at initialization. For fully connection and and convolutional neural networks, it shows that the model is trainable only when the initialization satisfies Edge of Chaos (EOC). The paper also provided a rescaling method so that the pruned network is initialized on the EOC. For Resnet, the paper shows that the proposed pruning satisfies the EOC condition by default and further provides re parameterization method to tackle exploding gradients. The experiments show the performance of the proposed method on fully connected and convolution neural network, as well as ResNet. There were some concerns about the contribution of the paper compared to that of [1]. I read the two papers carefully and while both papers aim at addressing a similar problem, i.e., pruning at initialization while avoiding layer collapse, the paper provides a different perspective on the problem, and provides enough theoretical contribution and insights to be found helpful and interesting by the community.  
The paper focuses on the task of finding higher fidelity action proposals for temporal action proposal detection. As the reviewers mentioned, this task is a pre task to temporal activity localization/detection in video, which is the main task to be solved. The paper may be perceived differently if it were presented as a detection method instead. Apart from the scope of the paper, the reviewers also unanimously agree on the limited technical novelty of the proposed methodology in the paper. The proposed method can be seen as an application of self attention and transformer techniques on the problem of activity detection. The goal of these techniques is feature enrichment that serves to incorporate information across long term context, a concept that has appeared previously in other work but not necessarily with the same machinery (e.g. G TAD).   Despite its shortcomings and since it presents promising experimental results on well known proposal/detection benchmarks, the authors can benefit from considering the reviewers  comments and suggestions to produce a stronger and more compelling future submission.
Dear authors,  I took your concerns into account, and I also understand the whole crazy situation around the COVID 19. Many of the reviewers have families (e.g., in US, many kids are now homeschooled, and there are no good daycare solutions as well). I do not plan to list all the good parts of the paper and list weaknesses that are already mentioned and visible to you. Hence, let me focus on my concerns about this paper (and I hope you could find them interesting and they will help you to improve your paper).  + I personally find the use of 2nd order method in DNN a way to improve many inefficiencies of ADAM/SGD, .... and using diagonal scaling is one way to do it.     I personally find some sections not very motivated and explanatory. E.g. Section 3.2 is just telling half of the story and is missing some details to give the reader the full understanding.    The fact that B_t  is not necessary >0, it makes intermediate sense to use some kind of \max\{B_i, \sigma\}  to have the "scaling" to be $\succ 0$. Note that there are also SR1 methods that would guarantee the matrix to be not necessary pd.    Your main motivation was non convex problems, but the only theorem in the main paper was for convex loss only, right? In this case, I guess there is no issue with B_t to have some coordinates <0, right?  Overall, I find the topic interesting and would like to see an updated paper in some of the top ML venues, but right now I cannot recommend it for acceptance!  
This paper was unanimously rated above the acceptance threshold by the reviewers.  While all reviewers agree it is worth accepting, they differed in their enthusiasm.  Most reviewers agree that  major limitations of the paper include that the paper provides no insight into why Dale s principle exists and the actual results are not truly state of the art.  Nevertheless there is agreement that the paper presents results worth publicizing to the ICLR audience.  The comparison of the inhibitory network to normalization schemes is interesting. Also, please reference the Neural Abstraction Pyramid work.  
The paper initially had mixed reviews (4,5,6).  The main issues raised were: 1) limited novelty (re using/integrating components) [R2]; 2) limited generalization ability since the model needs to be retrained on every video [R2, R3]; 3) limited applicability   experiments limited to certain domain of video, while results on videos with large motion are not convincing [R2, R3]; 4) missing ablation studies / experiments [R3, R4].  The author response partially addressed some concerns, but the main points 1 3 are still problematic. In addition, the AC noted that the technical aspect was lacking:    Training with contrastive loss on a single video may likely overfit the embedding to the video, which leads to a meaningless embedding where all non neighboring segments are orthogonal in the embedding space. While changing the softmax temperature can yield higher entropy transition probabilities, the induced probability distribution is probably highly noisy. It would be better to train this on a large video corpus, which will prevent overfitting. Also contrastive loss is typically used to build a discriminative embedding space for classification/recognition, not a smooth embedding space for generation (where distances between embedding vectors are strongly correlated to similarity). Thus some other embedding smoothness terms could be added during contrastive learning.   The learning is only on the transition probabilities, while the video generation is separate. It would have been more convincing to learn the transition probabilities with the video generation process in an end to end manner. Perhaps a discriminator could be placed after the video generator so that the transition probabilities could be learned so as to better mimic real video. Other loss terms based on video temporal smoothness could also be added ensure smoother transitions between clips (e.g., motion consistency).  The negative reviewers remained unconvinced by the author response, and the AC agreed with their concerns. Thus, the paper was recommended for rejection.
The work falls under the setting of learning based sketching/compressive subsampling. It extends the work of Indyk et al 2019 (including sparsity pattern optimization and some theoretical enhancements).  The reviewers agree that while the conceptual novelty including the greedy optimization step is not too much, it is nonetheless interesting and is non trivial. However, given the highly competitive submissions at ICLR, the current scores are not sufficient for acceptance. 
While the author response clarified some concerns, it could not convince the reviewers that the current version of the paper should be accepted for publication at ICLR.  
The paper explores the Birkhoff von Neumann decomposition in order to propagate gradients through a bi partite matching. The task is very relevant to the community but the reviewers raised concerns both about the theory and the practice of the work. Unfortunately the work is not ready for publication at ICLR.  
The reviewers all agreed that the paper is a solid contribution.  Pros:   A simple and reasonable extension to adaptive prediction sets that performs well empirically.   The procedure presented is versatile (i.e. can be applied to general scores or be used to improve base conformal prediction methods).   A very thorough experimental analysis, including large datasets (i.e. Imagenet) and a wide range of model architectures including ResNet 152.   Some formal theoretical guarantees are provided for the procedure, although they appear to be straightforward.  Cons:   Limited technical novelty.  Overall, I recommend a spotlight because the reviewers felt that the topic of predictive uncertainty is of interest to the broader ML and computer vision community, and the paper can have a potentially large impact in popularizing conformal methods as a viable uncertainty estimation method.
The paper was evaluated by 3 knowledgeable reviewers. All reviewers raised concerns about the motivation of the contribution of the paper. It is unclear why the use of an additional discriminator should reduce the variance of the log density ratio estimate. Also, the derivations were found to be not convincing or intuitive. These concerns have also not been alleviated after a rather extensive discussion of the reviewers with the authors. Moreover, the transfer setting to a new environment was unclear as it does not show how the  reward function transfers to new dynamics, so the transfer experiments rather evaluate how well the algorithm can imitate a policy on a different dynamics, but it does not tell that the extracted reward function is valid. While the experimental results seem promising, the authors are encouraged to improve the motivation of contribution, check which of the "incremental" contributions are very necessary and improve their evaluation on the transfer scenario.
This paper introduces two regularizers that are meant to improve out of domain robustness when used in the fine tuning of pretrained transformers like BERT. Results with ANLI and Adversarial SQuAD are encouraging.  Pros:   New method with concrete improvements in several difficult task settings.   New framing of adversarial generalization.  Cons:   The ablations that are highlighted in the main paper body don t do a good job of isolating the specific new contributions. (Though the appendix provides enough detail that I m satisfied that the main empirical contribution is sound.)   Reviewers found the theoretical motivation very difficult to follow in places.
This paper received borderline reviews, but all lean toward acceptance.  The reviews highlighted strengths in the paper, citing that they liked the main idea and its mathematical treatment: * R3: "I liked the abstraction proposed by authors and particularly liked the way authors set up the Definition 1 and analysis afterwards" * R3 post discussion: "I recommend accept because authors have a solid theory which would be useful for the self supervised learning community." * R4: "This work presents a very detailed theoretical analysis for self supervised learning objectives. The idea of inverse predictive learning for filtering task irrelevant information is interesting." * R2: "I like the idea of discarding the redundant task irrelevant information to improve the self supervised learning"  However, there was a consensus among reviewers that the experimental validation was weak, both in terms of not showing enough improvement on enough examples and in terms of studying the effect of certain hyperparameters: * R2: "lack of persuasive experiment results to prove the effectiveness of the proposed method. In fig.3, the improvements on two dataset are marginally, which can not convince me. The \lambda (λ_IP) in proposed objective function seems not robust to different datasets, which makes me doubt about the generalization of this method." * R3: "Ratings can be improved further if authors can relate experimental setup more to the theory which I find slightly disconnected" * R3 post discussion: "All reviewers have concerns about lack of solid experimental evidence [...]  I can not improve my score further because of weak experimental evidence." * R1: "The experiments are conducted in a controlled way [...] Traditional uncontrolled experiments [...] are suggested." * R4: "The variation in the performance shown in Figure 3 is very marginal. [...] Figure 5 a shows some results on Omniglot, but the improvement shown there is very marginal. [...]" * R4: "weights required for inverse predictive learning in the loss formulation is not trivial. [...] Is there a simple way to determine this weights without exhaustive search on target dataset?" * R4: "However, it is not clear from the experimental results if this is really effective."  The authors  revisions aim to improve the discussion of the $\lambda_\text{IP}$ parameter.  Given these experimental limitations, my recommendation is for acceptance but with a low confidence score.
This paper provides a natural combination of conditional neural processes with LieConv models. It is a good step forward for stochastic processes with equivariances. While there is still room to improve the experiments, the authors provided a good response to reviewers, and the paper is a nice contribution.
The paper presents a model for question answering where blocks of text can be skipped and only relevant blocks are further processed for extracting the answer span.    The reviewers mostly praised the general idea.    R3 raised concerns on generalizability of the presented approach.   R4 raised several issues regarding presentation and clarity.   R2 and R4 have concerns regarding execution and find some of the results unconvincing.   While I don t necessarily share R2s concern on small improvements (improvements are still statistically significance), and despite the approach being very interesting, there are several issues that reviewers pointed out and wasn t resolved after discussions.  
This paper proposes Dirichlet Neural Architecture Search (DrNAS), a new NAS algorithm that formulates NAS as a distribution architecture search problem. The paper shows theoretically that DrNAS implicitly regularizes the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search) and presents very strong empirical results on several benchmarks, including the tabular benchmark NAS Bench 201.   The reviews and discussion put this paper very close to the acceptance threshold, so I read it in detail myself to act as a tie breaker. I see a lot of positive aspects of this paper: + it tackles a very important and timely problem + the method implicitly regularizes the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search) + empirical results are very strong + the paper includes insightful ablation studies for the most important parts of the algorithm + the progressive architecture learning approach is a general contribution that reduces the memory complexity and also improves basic DARTS + the paper uses a tabular benchmark to yield results that are directly comparable to those of other papers + the method s hyperparameters are kept fixed across the different benchmarks, which underlines its robustness.  There are also some negative aspects:   The method was not originally derived from a Bayesian point of view. Per request of the reviewers, the relation to variational inference has been added to the appendix, but the difference between the L2 regularization and using the explicit KL regularization that falls out of the Bayesian treatment remains. Nevertheless, the new appendix helps to clear up the relationship.    The paper does not mention availability of the code. This is a must in modern NAS research, as many papers have exposed the poor reproducibility of research in NAS. Fortunately for the authors, I have already seen (independently of this submission) that the code is available on github, but I urge the authors to provide an anonymous repo for review in future submissions, since it was purely by chance that I saw it this time.    Regarding the point of exploration vs. exploitation, the authors emphasize that in contrast to Gumbel softmax based methods, such as GDAS and SNAS, with DrNAS there is no need for a cooling schedule. While it is nice to keep the number of hyperparameters small, this also appears to give up control of when the method switches from exploration to exploitation. In practical applications of AutoML, there will be a time budget, and while a cooling schedule can be adapted to fit this budget, it would be suboptimal if DrNAS is still in the exploration phase by the end of the budget, or has already switched to exploitation after, e.g., 5% of the budget. It would be good if the authors could briefly discuss this issue in their final version (if only by acknowledging that this can be a problem).    Minor negative points   * The paper sometimes uses jargon, and I believe not even always correctly: e.g., even by googling I did not find such a thing as the "iregularized incomplete beta function", it s also not in the original reference by Jankowiak. I only found the "incomplete beta function" (and the regular one).    * The author names for several references are garbled. This is likely due to not replacing a comma between the authors with an "and" in the bibtex file.   * The paper lacks citations for several of the methods it uses, e.g., Adam, cutout, cosine annealing, label smoothing, auxiliary towers, etc. There is no limit on references, and it is standard to cite these concepts to remain more self contained.  * The experimental results of GDAS on NB201 CIFAR 100 do not seem to align with the numbers in the NB201 paper. Did you use the numbers from the paper or rerun this method yourself? Please clarify and check this for the final version. The point of tabular benchmarks is to have comparability and consistency across papers!   * Please have the paper proofread for Grammar again, there are several avoidable errors. E.g., in the first sentence, "lots of attentions"  > "lots of attention". Also things like "alone"  > "along", "down" >"done" etc.  Overall, I think this is a very nice paper, introducing an empirically very strong NAS method that is also theoretically shown to implicitly regularize the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search). I am therefore recommending acceptance. I would like to ask the authors to go through all the reviews again and fix any remaining points in the paper for the final version.
Reviewers all agree on acceptance for this paper. The initial issues with clarity seem to have been addressed by the authors.  The paper introduces a new transformer based architecture for MARL that enables variable input and output sizes, which is used to train the agent in a more general setting and on more diverse tasks for multi task training. The method also produces more interpretable agents. The paper shows results on the Starcraft multi agent challenge (not the full game of Starcraft, but still a recognised and widely used multi agent benchmark). The method produces solid results both in terms of final training performance and zero shot generalisation.  Although reviewers are generally supportive of this paper, they mention that the Starcraft challenge used is somewhat simple (only few units used), and that the transformer based architecture may not be applied to domain which lack the proper structure.   
In this paper, the authors proposed a reinforcement learning based model for aspect based sentiment analysis. As raised by the reviewers, 1) the writing needs to be improved: e.g., presenting the details of the proposed method clearly, citing the references properly, etc. 2) related methods need to be implemented for comparison, 3) the reported results are not SOTA compared with existing methods. Moreover, some technical claims are not convincing, which need to be stated more carefully.  In summary, based on its current shape, this paper is not ready to be published in ICLR.
This paper proposes a measure of task complexity based on a decision DAG like "encoder" where we iteratively branch on some test on the input and the selection of future tests depends on the answer to previous tests until we reach a terminal node in the DAG.  We require that if $x$ and $x $ reach the same terminal node then $P(y|x)   P(y|x )$.  The complexity of the task (the complexity of the distribution $p(x,y)$) is the minimum over all such DAGs of the expected depth of the terminal node for $x$ when drawing $x$ from the marginal $p(x)$.  The reviewers are not enthusiastic and I agree.
The authors study the problem of (insufficient) generalization in gossip type decentralized deep learning. Specifically, they establish an upper bound on the square of the consensus parameter distance, which the authors identify as a key quantity that influences both optimization and generalization. This upper bound (called the critical consensus distance) can be monitored and controlled during the training process via (e.g.) learning rate scheduling and tweaking the amount of gossip. A series of empirical results on decentralized image classification and neural machine translation are presented in support of this observation.  Initial reviews were mixed. While all reviewers liked the approach, concerns were raised about the novelty of the results, the lack of theoretical depth, and the mismatch between theory and experiments. Overall, the idea of tracking consensus distance to control generalization seems to be a practically useful concept.  During the discussion phase the authors were been able to (convincingly, in the area chair s view) respond to a subset of the criticisms.   Unfortunately, concerns remained regarding the mismatch between the theoretical and empirical results, and in the end the paper fell just short of making the cut.   The authors are encouraged to carefully consider the reviewers  concerns while preparing a future revision.
The main contribution of this paper is a nearly linear time algorithm for learning Bayesian networks with a known structure when an epsilon fraction of the samples are contaminated. The model assumes that the directed graph is known and the goal is to estimate a vector of length m that describes the conditional distribution of any node for any configuration if its parents. Let N be the number of samples and let d be the number of nodes. Prior work gave an algorithm that runs in time N d^2 time. This is now improved to roughly Nd time under natural conditions on the "balancedness" and the "minimum parental configuration probability". The algorithm itself is simple, and is a more direct reduction to robust mean estimation.   The reviewers had somewhat differing opinions. The pros are that it s a basic problem, the algorithm is clean and the ingredients in the improved running time could have further applications. The negative is that there are no experiments, even synthetic ones, to demonstrate practicality. Overall it still seems that there is enough excitement about the work to merit acceptance. 
Four reviewers evaluated your work and provided a detailed review with many suggestions. I also think that there is an interesting idea and encouraging results but there is a lack of numerical results and still some parts are still unclear and need to be polished. Consequently in its  current form, the paper can not be accepted for publication. I would advise you to carefully follow the remarks of reviewer 1 to improve  the paper.   
The paper introduces a learning framework for solving incompressible Navier Stokes fluid using a physics informed loss formulation. The PDE is solved on a grid, and the model, implemented via convolutions and a U Net, is trained to minimize the NS residual. The model is trained on a variety of randomized contexts, in a way that allows training to explore a large number of configurations. The paper presents original contributions compared to previous Physics informed framework (discrete formulation, conditioning on the domain conditions, …). All the reviewers agree that the detailed rebuttal provides answers to their questions and that the contribution is significant, they all have a positive assessment of the paper.
The proposed approach for evaluating reward functions is theoretically grounded while having several properties appealing to practical RL tasks. This novel approach fills a gap in the literature. All reviewers agree that this paper has a place at ICLR.
This paper was reviewed by four experts in the field. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references.
Reviewers generally agree that the proposed method UMATO, a two phase optimization dimensionality reduction algorithm based on UMAP, is interesting and has potential, and that the paper is well written. However, there are several concerns with the current paper. In particular, R1 is not convinced by the performance of UMATO on real world datasets compared with previous methods such as t SNE (see the linked papers). Both R1 and R2 are concerned that given the 2 phase approach, UMATO might be much more adapted to clustered data than standard manifold embedding. They pointed out that in the Swiss  roll/S curve examples, UMATO stays very close to PCA, which is used for initialization, instead of globally unfolding the manifold as Isomap. These issues should be clarified/explored further for a better understanding and/or improvement of the current work.
All reviewers feel this paper addresses and important topic, and has many merits. However, it is difficult to recommend publication at this time. The primary concern is that the paper has its theoretical optimality as an important contribution, but the reviewers and myself (in a non public thread) were unable to verify the correctness of the proofs. In part unfortunately this is due to edits to the proofs happening late in the revision period, too late for further discussion with the authors. Some of the particular questions in the proof of theorem 1 (appendix B) include: clarifying the value of $\rho$ which makes the unnumbered equation above equation (6) equivalent to definition 1, and in particular whether the $1/|X_k|$ term should be inside or outside the absolute value; and clarifying various undefined symbols which are introduced in the equation at the top of page 13, but are never defined, including $M$, $b$, and $z_i$. Reviewers also had some concern that the algorithm should be benchmarked against more recent / better performant baselines than Kamiran et al. (2012).
There is value in analyzing pre training for few shot learning, and the observation that improved disentanglement might lead to better initialization schemes for few shot learners is worth exploring. However, in its current state, the reviewers do not think the paper is ready for publication. Specifically, work needs to be done to improve the clarity, comparison to related work, and experimental analysis. 
Investigating using other sensory inputs in our agents, and the impact on exploration is fascinating. We all want to see agents that use more sensory information.  As it stands the paper has several issues that require significant revision, most notably: (1) the polish, quality of the writing and clarity of the text is low, (2) the empirical results are based on 3 runs at this number we might not have enough data to form valid estimates of the std dev the error bars are not defined (see Henderson et al 2018), (3) in the ablation studies the hyper parameters are not tuned (as far as the text suggests) meaning the ablations results might be not representative of the utility of the method, (4) many missing details like hyper parameter tuning, number of runs in some cases, and reasonable descriptions of experiment protocols and baselines, (5) unsupported claims of causality.  Some of the issues were first raised during the discussion period, so another reviewer was brought in and provided a high quality review with many constructive comments. All reviewers reached clear agreement at the end of the discussion period. 
This paper presents a counterfactual approach to interpret aspects within a  sequential decision making setup. The reviewers have reacted to each others  comments as well as the authors  response to their views. I am recommending acceptance of this paper, as it targets an interesting problem and presents an intriguing approach. I think the community would appreciate further discussing this paper at the conference.
Reviews were somewhat mixed here, but the consensus is to reject, with at least one voice (R2) urging rejection. Across reviewers, the recommendation to reject is primarily based on the level of originality with the proposed U Net architecture and on weakness of experiments, especially in comparing to baselines.  Reviewers found strengths in the paper s writing and in its demonstration of generalization to unseen geometries.  However, reviewers noted that the architecture does not win originality/significance points (including R3, the most positive reviewer): * R3: "The weakness of this paper is that it doesn t present any novel techniques. It s an existing architecture (U Net) applied in a new domain (wave simulation)." * R2: "The proposed approach is a straightforward application of U net to predict a spatial field given past few spatial fields (stacked together). However, U Nets, LSTMs, conv LSTMs and other architectures have been tried before. It is unclear what the novel contribution in this paper is [...] and why it would be instrumental in handling unseen geometries over longer periods of time." * R2 post response: "This paper is a clear reject. None of the contributions are novel [...]" * R4: "The paper lacks a novel contribution from the architectural and application side" * R1: "Some previous works also used the U net to predict wave dynamics [...] It is not clear what is the novelty (if any) in the proposed network architecture"  Reviewers also noted weaknesses in the experiments (acknowledged by R3, the most positive reviewer, though that review did not consider them a fatal flaw): * R1: "Not enough Experiments. How does the model generalize with more complicated initial conditions, for example, five or ten droplets? Furthermore, there is no comparison to other existing work." * R2: "There is no evaluation against the state of the art [...]" * R2 post response: "Application of DNNs to this problem, speed ups over numerical solvers, etc. have all been explored by SOTA works which have not been compared against. There is no clear articulation of the claimed novel contributions over the SOTA and empirical validation (or theoretical reasoning) of the same." * R4: "There are no comparisons to other baselines [...] " * R3: "Reviewer 4 brings up some fair points [about experimental issues]. I m not as concerned about the lack of a baseline comparison; that doesn t seem to be the point of this paper [...] there is only so much that can be done in an 8 page conference paper [...] However, given that the other 2 reviewers think the paper could use more work, it would be completely reasonable for the chairs to reject it based on those reviews."  Based on this consensus of reviews, my recommendation is to reject. I hope the feedback from the reviews is helpful to the authors.
The submission proposes instance level and episode level pretext tasks as an unsupervised data augmentation mechanism for few shot learning. Furthermore, transformer are proposed to integrate features from different images and augmentations. The paper received one clear accept, one accept, one borderline accept and two borderline reject recommendations. The main concerns of the R5 and R2 were weak ablation study and the lack of a clear advantage of the method in terms of results compared to the prior state of the art. In the rebuttal, the authors provided more ablation studies. Similarly, the reviewers were concerned about the novelty of the paper being incremental compared to the prior works. Based on the majority vote, the meta reviewer recommends acceptance. 
This paper proposes the Skill Action (SA) architecture, based on the insight that semi MDPs in the option framework can be posed as an equivalent MDP. The paper presents interesting theoretical results and very promising empirical results. We thank the reviewers for their revisions, which provided more insights into the method. Of particular interest was the discussion on the "dominant skill problem". We still feel that the paper would benefit from additional experiments, as discussed in detail in all of the reviews. I believe with this inclusion this will be an impactful paper.
The paper shows a connection between Potts model and Transformers and uses the connection to propose a factored attention energy to use in an MRF. Results are shown, using this energy based on factored attention. Also, pretrained BERT models are used to predict contact maps as a comparison. The reviewers found the paper interesting from a protein structures prediction point of view, but from a machine learning perspective their opinion was that the paper does not offer a coherent, compelling method that is very novel, and the connection between Potts and an energy based attention model is not that overwhelming.  In addition the presentation was somewhat circuitous.    The authors made improvements to the paper over the course of the review, which is appreciated, but the method presented does not match the target for an ICLR paper in terms of methodological contributions.  
Two very confident and fairly confident reviewers rate this paper ok but not good enough, and two other fairly confident reviewers rate the article below the acceptance threshold. Therefore I must reject the article. The reviewers provided encouraging comments and suggestions on how the manuscript could be improved, which I hope the authors will find useful. 
This paper presents a model based posterior sampling algorithm in continuous state action spaces theoretically and empirically. The work is interesting and the authors provide numerical evaluations of the proposed method. But the reviewers find the contribution of the work limited. 
The paper received four negative reviews. The overall idea was found to be interesting, but several concerns were raised. There is a general consensus that the experimental part and the results are not convincing. Several comments have also been made regarding the clarity and motivation, which needs to be strengthened. R4 also mentions references from the sparse estimation literature that would help for positioning the paper. The rebuttal did address some of these points, but it was not sufficient to change their opinion.  Overall, the area chair agrees with the reviewers and follows their recommendation.
This application oriented paper has been carefully evaluated by three expert reviewers. Their assessments all agreed on a quite marginal methodological novelty of the presented work, yet they recognized nicely engineered pipeline of pre existing modules that appears to satisfy an important remote sensing application. I agree with that assessment and concur with the reviewer s opinion that the work as presented, in spite of being of potential interest to healthcare or biomedical communities, will be of little interest to the ICLR audience. Therefore I recommend rejecting it.  
The paper investigates the capacity for neural language models to perform fast mapping word acquisition using a proposed multimodal external memory architecture. Much work exists that shows that neural models are capable of following instructions whose meaning persists across episodes (i.e., slow learning), however much less attention has been paid to instruction following in a one shot learning context. Using a simulated 3D navigation/manipulation domain, the paper shows that the proposed multimodal memory network is capable of both slow and one shot word learning when trained via standard RL.  The submission was reviewed by four knowledgable referees, who read the author feedback and engaged in discussion with the authors. The paper is topical one shot language learning for instruction following using neural models is of significant interest of late. The reviewers agree that the proposed multimodal memory architecture is both interesting and technically solid. The reviewers raised concerns about the experimental evaluation and the role of embodiment. The author feedback together with discussion with reviewers were helpful in resolving some of these issues. However, the authors are encouraged to ensure that the paper clearly motivates the importance of embodiment to slow learning and fast mapping, particularly given the large body of work in language acquisition in robotics, a truly embodied domain, which is notably missing from the related work discussion.
This paper studies the problem of multi domain few shot image classification and proposes a Universal Representation Transformer (URT) layer, which leverages universal features by dynamically re weighting and composing the most appropriate domain specific representations in a meta learning way. The paper extends the prior work of SUR [Dvornik et al 2020] by using meta learning and avoiding additional training during test phase. The experimental results show improvements over SUR in both accuracy (not always significant on some datasets though) and inference efficiency. Overall, the paper is well written with sufficient contributions. After the author s rebuttal and revision, reviewers generally agree the paper can be accepted. I recommend to Accept (Poster). 
This paper analyzes some design choices for neural processes, paying particular attention to their small data performance, uncertainty, and posterior contraction.  This is certainly a worthwhile project, and R3 found the analysis interesting, giving the paper a score of 8.  However, R1, R2, and R4 found the experimental validation to be incomplete and insufficient to support the paper s broader recommendations.  As the paper is investigating the various combinations of implementations, I tend to agree with R1, R2, and R4 that this paper while having some interesting ideas needs a bit more precision and breadth to its experiments.
Summary: This paper provides an interesting and unique challenge problem on human AI collaboration, with sample baselines. I think this is an extremely important topic and the community should embrace such challenge problems.  Discussion: Reviewers agreed this paper should be accepted, particularly after seeing that ICLR has accepted such challenge papers in the past.  Recommendation: I d really like to see this get a spotlight as it would be great to highlight this innovative challenge to the community. 
The paper proposes an algorithm to defend against black box attacks. All the reviewers think the current experiments are not convincing enough, and the method seems to have some issues (e.g., not scalable). 
There are some interesting ideas raised on continuous time models with latent variables in machine learning. However, the reviewers argue, and I agree, that the connection to causal models as typically required in applications about the effects of interventions is not addressed with as much care as it might have been needed.
This paper considers the problem of pruning deep neural networks (DNNs) during training. The key idea is to include DNN elements only if they improve the predictive mean of the saliency (efficiency of the DNN elements in terms of minimizing the loss function). The objective of early pruning is to preserve the sub network that can maximize saliency. This optimization problem is NP hard, and even approximation is very expensive. The paper proves that one can simplify the approximation by ranking the network element by predictive mean of the saliency function.  The proposed approach is novel as most of the prior work on pruning has focused on either (i) pruning on network initialization  or (ii) pruning after the network has been fully trained.   Couple of issues with the paper are: 1. Current approach is somewhat complicated with many hyper parameters 2. Experimental results are not very compelling when compared to pruning on network initialization  Overall, my assessment is that the paper takes a new research direction and has the potential to inspire the community, and followup work may be able to overcome the above two issues in future. However, due to the remaining shortcomings, the paper is not judged ready for publication in its present form. I strongly encourage to resubmit the paper after addressing the above two concerns.
This paper proposes an extension to previous unsupervised feature learning work, with an EM style latent variable model with momentum encoders. The paper is well written and provides a nice read. It has been noted that it is easy to follow and provides good insights. On the experimental side, compared with MoCo, the proposed approach achieve noticeable improvements. One of the reviewers noted the easy reproducibility of the proposed approach.  Some reviewers noted some comparisons were lacking from the original manuscript, but the authors have update the draft to include those. As noted in the reviews, the field of SSL in vision is moving at a very quick pace, making it hard to clearly state what is the SOTA at time t.  Overall, most questions raised by the reviewers were properly addressed during rebuttal   and given the ratings, I suggest acceptance.
 The authors propose a pretraining strategy learning inductive biases in transformers for deduction, induction, and abduction.  Further, the claims and results seem to indicate that such pretraining is more successful in transformers which provide a more malleable architecture for learning inductive (structural) biases.  There are open questions that remain, specifically surrounding disentangling high performance from structural bias learning (i.e. is pretraining doing what we think it is) and whether datasets are the "correct" mechanism for imparting such biases/knowledge.
The paper studies knowledge distillation through the lens of semi parametric inference. There has been a lot of work on knowledge distillation in the past, but, as the paper points out, most of it is heuristic or empirical in nature, and thus theoretical understanding on the subject is lacking. The reviewers generally agree that this paper makes a useful theoretical contribution towards a better understanding of knowledge distillation. However, the reviewers also raised some concerns, especially regarding the clarity of the text, and they felt that the paper might be overstating its contribution. Still, all reviewers recommend acceptance, and on balance the merits of the paper seem to outweigh the weaknesses, so I d be happy to recommend acceptance.
This paper represents the PettingZoo library of multi agent environments, providing a common API and benchmark for multi agent learning. The library has high potential for impact and is likely of interest to a wide range of people in the ICLR community. However, in its current form the paper could be significantly improved by actioning the many pieces of constructive feedback provided by all reviewers.  We have also been made aware of two highly related papers "Multiplayer Support for the Arcade Learning Environment" and "SuperSuit: Simple Microwrappers for Reinforcement Learning Environments." Together all three papers could be one comprehensive manuscript, but appear to have been unnecessarily split into three separate short papers.
This paper conducts a comparison between a small set of models (4 in total) for unsupervised learning. Specifically, the authors focus on comparing Bayesian Confidence Propagating Neural Networks (BCPNN), Restricted Boltzmann Machines (RBM), a recent model by Krotov & Hopfield (2019) (KH), and auto encoders (AE). The authors compare trained weight distributions, receptive field structures, and linear classification on MNIST using the learned representations. The first two comparisons are essentially qualitative comparisons, while on classification accuracy, the authors report similar accuracy levels across the models.  This paper received mixed reviews. Reviewers 4 and 5 felt it did not contribute enough for acceptance, while Reviewers 2 & 3 were more positive. However, as noted by a few of the reviewers, this paper does not appear to achieve much, and provides very limited analysis and experiments on the models. It isn t introducing any new models, nor does it make any clear distinctions between the models examined that would help the field to decide which directions to pursue.  The experiments add little insight into the differences between the models that could be used to inform new work. Thus, the contribution provided here is very limited.   Moreover, the motivations in this paper are confused. In general, it is important for researchers at the intersection of neuroscience and machine learning to decide what their goal is when building and or comparing models. Specifically, is the goal: (1) finding a model that may potentially explain how the brain works, or (2) finding better machine learning tools?  If the goal is (1), the performance on benchmarks is less important. However, clear links to experimental data, such that experimental predictions may be possible, are very important. That s not to say that a model must be perfectly biologically realistic to be worthwhile, but it must have sufficient grounding in biology to be informative for neuroscience. However, in this manuscript, as was noted by Reviewer 4, the links to biology are tenuous. The principal claim for biological relevance for all the models considered seems to be that the update rules are local. But, this is a loose connection at best. There are many more models of unsupervised learning with far more physiological relevance that are not considered here (see e.g. Olshausen & Field, 1996, Nature; Zylberberg et al. 2011, PLoS Computational Biology; George et al., 2020, bioRxiv: https://doi.org/10.1101/2020.09.09.290601). It is true that some of these models use non local information, but given the emerging evidence that locality is not actually even a strict property in real synaptic plasticity (see e.g. Gerstner et al., 2018, Frontiers in Neural Circuits; Williams & Holtmaat, 2018, Neuron; Banerjee et al., 2020, Nature), an obsession with rules that only use pre  and post synaptic activity is not even clearly a desiderata for neuroscience.  If the goal is (2), then performance on benchmarks, and some comparison to the SotA, is absolutely critical. Yet, this paper does none of this. Indeed, the performance achieved with the four models considered here is, as noted by Reviewer 4, very poor. In contrast, there have been numerous advances in unsupervised (or "self supervised") learning in ML in recent years (e.g. Contrastive Predictive Coding, SimCLR, Bootstrap Your Own Latent, etc.), all of which achieve far better results than the four models considered here. Thus, the models being compared here cannot inform machine learning, as they do not appear to provide any technical advances. Of course, some models may combine goals (1) & (2), e.g. seeking increased physiological relevance while also achieving decent benchmark performance (see e.g. Sacramento et al., 2018, NeurIPS), but that is not really the situation faced here, as the models considered have little biological plausibility (as noted above) and achieve poor performance at the same time.  Altogether, given these considerations, although this paper received mixed reviews, it is clearly not appropriate for acceptance at ICLR in the Area Chair s opinion.
This paper presents a probabilistic model for multitask learning with representation learning. The basic idea is to share information across tasks by making the prior over the model parameters of one task conditioned on a convex combination of the variational posteriors of the other tasks.  While some of the reviewers gave high scores and recommended acceptance, one of the reviewers (AnonReviewer3) had some pertinent concerns which lingered even after author response. In particular, AnonReviewer3 mentions that since the prior of one task is conditioned of the variational posteriors of the other tasks, the method is not a proper Bayesian approach. I also read the paper and agree with the assessment. Indeed, the common Bayesian way for multitask learning is to couple the tasks purely based on a prior that encouraging sharing across tasks instead of having task specific prior that depend on the variational posterior of other tasks as is being done in this paper.   I also read the reviews and the author response and have some other concerns as well:    There is a huge amount of prior work on multitask learning, both non Bayesian as well as Bayesian. Although the paper cites several of those it is disappointing that none of the baselines are Bayesian. Even the non Bayesian baselines aren t the state of the art recent methods, which is disappointing given the extensive body of prior work in this area.    The rebuttal wrongly claims MTRL and MRN to be Bayesian methods (included in Table 14 as baselines) whereas they only have a probabilistic formulation and only do point estimation. At a minimum, the paper should show comparison with some Bayesian multitask learning approaches (e.g., shared hierarchical priors, or task clustering, etc). The baselines such as MTRL and MRN aren t among the strongest ones out there.     The paper s title is way too generic. There are several multitask learning papers that use variational inference for a Bayesian model. Moreover, given that the basic formulation itself is a bit problematic to be called Bayesian, the title in some sense is also misleading.  Due to the above issues, I don t think the paper can be accepted in its current form.
While the reviewers seem to like the main idea of the work, they had several concerns, particularly regarding the experiments (both their setup and description) and the overall language of the paper that they found it more suitable for the control community than the ML and representation learning community. The authors provided very long response and tried to address the issues raised by the reviewers during the rebuttals. Fortunately, the response addressed some of the issues they raised and now they all see the paper marginally above the line. However, reading the reviews and response shows that the paper can highly benefit from better writing and describing the experiments. So, I would strongly recommend that the authors include all the information they provided for the reviewers during the rebuttal phase in the paper and improve its quality. 
In this paper, the authors combine ideas from SLAM (using an Extended Kalman Filter and a state with nonlinear transitions and warping) and differentiable memory networks that store a spherical representation of the state (from the ego centric point of view of an RL agent moving in an environment) with depth and visual features stored at each pixel and dynamics transitions corresponding to warping.  The main idea in the paper is very simple and elegant, but I will concur with the reviewers that the writing of the first version of the paper was extremely hard to understand and that the experimental section was too dense. Two subsequent revisions of the paper have dramatically improved the paper.  Given the spread of scores (R1: 6, R2: 7 and R3: 4) and the fact that only R1 and R2 have acknowledged the revisions, I will veer towards acceptance. 
This paper proposed to defend against model stealing attacks by dataset inference. The paper received unanimous rating of "Good paper" and "accept". The reviewers praise this paper insightful and well written. There are active discussion between the reviewers and authors, which further clarify some of the issues. Given the positive review and overall rating, the AC recommends it to be an spotlight paper.
All the reviewers questioned the significance of the result, in the sense that the qualitatively it is not clear how much of an improvement it is to replace "min(S_T,C_T) with Lipschitz assumption" by "min(S_T,C_T,G_T)". The authors  response on this point did not convince the reviewers. If the authors were to resubmit this work to a future conference, we encourage them to significantly expand on this point.
The paper proposes a novelty detection method when training data is itself noisy. A VAE based approach is developed that promotes robustness of the VAE. The paper assumes that the encoder a two component Gaussian mixture distribution, individual components denoting inliers and outliers.  The paper hopes that the posterior of the inliers (normal data points) can be represented by a low rank covariance matrix, while the outliers need a full covariance. Another notable modification is that the Wasserstein 1 regularization is used to replace the KL regularization in the ELBO, which is claimed to be more suitable to the low rank modeling.   While this is a relevant problem, and the idea is perhaps interesting, some concerns have been raised. * The details how to fit the model with the desired mixture posterior in practice is unclear. * The arguments of section 3 to illustrate the superiority of Wasserstein were found unconvincing, with limiting/unclear assumptions * The ultra low latent space dimension (2) is not sufficiently justified  * The experimental section and the selected datasets are small scale, it would be good to include a free larger scale datasets (at least cifar10). * Comparisons to the open set recognition, or out of distribution (OOD) detection would have been a plus.  Overall, this is an OK paper but not yet of sufficient quality.
The paper focuses on the update step in Message Passing Neural Networks, specifically for GNN. A series of sparse variants of the update step, say complete removal and expander graphs with varying density, are compared in empirical studies. The findings are quite useful for practice, and the paper is organized and written well.  As observed by the reviewers, there are several concerns regarding the novelty and contribution of the work. Besides, theoretical analysis of the sparsification approach is lacking. The authors provided a good rebuttal and addressed some concerns, but not to the degree that reviewers think it passes the bar of ICLR. We encourage the authors to further improve the work to address the key concerns. 
This paper proposes a new method to perform uncertainty estimation based on ensembles with diverse network architecture.   The reviewers raised a few concerns:   Although it is ok not to compare with (Tao, 2019), an active analytical comparison with baselines for ensemble diversification should not be overlooked e.g. (Yao et al, 2008), (Olson et al, 2019), (Khurana et al, 2018), etc.   The approach presented in this paper is not novel in the general idea of searching for or diversifying ensembles    The reviewers agree that diversity methods can be implemented on top of NES, but it is unclear whether NES+diversity methods would give more over just diversity methods; so either measuring NES+diversity methods or a direct comparison of NES and diversity methods is important.  We encourage the authors address these issues in the next revision. 
The paper studies mixture of expert policies for reinforcement learning agents, focusing on the problem of policy gradient estimation. The paper proposes a new way to compute the gradient, apply it to two reinforcement learning algorithms, PPO and SAC, and demonstrate it in continuous MuJoCo environments, showing results that are comparable to or slightly exceeds unimodal policies. The main issue raised by multiple reviewers is novelty. Mixture of expert models have been widely studied in the context of reinforcement learning, and while the paper proposes a new method for the gradient computation, a more suitable format, as pointed out by Reviewer 2, could be to ground the paper around the proposed gradient estimator, and compare, both analytically and empirically, it to existing alternatives. Therefore, I recommend rejecting this submission.
 This paper considers the problem of learning a k dimensional latent simplices given perturbations of data points in the simplex: this problem is of wide relevance in machine learning as it encompasses many latent variable models including Latent Dirichlet Allocation and Stochastic Block Models. It presents a modification of the recent algorithm of Bhattacharya & Kannan (SODA, 2020) which takes time O(k * nnz(A)), where A is the matrix of perturbed data points. The modified algorithm works with a low dimensional sketch of the matrix A instead of A, and thereby avoids the dependence on k in the running time of the original algorithm, which used k passes over the data set. The main result of the paper is thus that the latent simplex problem can be solved in O(nnz(A)) time for instances of the problem that satisfy the "Spectrally Bounded Perturbation" property introduced by the authors.  The main questions of the reviewers concerned the question of whether the assumptions needed for the analysis of the novel algorithm to apply hold on real data sets. The authors point out that the assumptions may be stronger than are needed in practice, and suggest that the assumptions could be weakened to assuming that a spectrally accurate sketch could be used  this would increase the run time dependence from just nnz(A), but weakens the assumptions needed. It was also observed that, in addition to a faster runtime, the method outperforms the benchmark method.  This paper should be accepted, due to its theoretical and practical contributions to the problem of latent simplex recovery: it presents an algorithm that provably runs in true input sparsity time given an amenable instance, and practically this algorithms performs well relative to the baseline, verifying the theoretical claims.
Although some reviewers still had concerns about the novelty of the proposed method, most of the other concerns have been addressed in a satisfying manner according to reviewers. They globally have a positive opinion about the paper after revision. 
The paper proposes an intriguing approach for "individual treatment effect" estimation from an observational dataset.  The approach is developed for multiple discrete actions (beyond binary treatments as typically studied in ITE literature) and discrete outcomes (a special case compared to related literature). The idea is to use the "direct method" (i.e. learn a probabilistic classifier using the observed dataset) and sample imputed outcomes for all unobserved action outcomes. Then, learn a probabilistic classifier that fits the observed+imputed dataset well, and iterate the procedure. This intriguing idea seems to converge empirically on a few different problems, and sampling the imputations rather than using deterministic imputations seems to be an important detail. Proof of convergence is however shown for deterministic imputations. The generalization error bound (Theorem 1) also does not show adequate motivation for the proposed method   even with infinite data (n >infinity), the excess risk could scale with the empirical risk of the returned model on the imputed dataset. Without an additional step proving that empirical risk on hat{D} (the imputed dataset) converges to 0 during successive iterations of the procedure, the generalization error bound is incomplete.  Consider the example of Figure 1, but where customer A has arrived to the system twice. So, the dataset contains {x1, $2, 1} and {x1, $3, 0}. When constructing the imputed dataset, the first data point would create 2 regression examples {x1, $1, ..} and {x1, $3, ..} while the second data point would create 2 regression examples {x1, $1, ..} and {x1, $2, ..}. Now, if the two {x1, $1, ..} examples have different imputation labels sampled from the model, this sets up an unrealizable learning problem and the empirical risk on hat{D} cannot be 0 for any predictor. In this toy example, we might know that we should "collapse" the two data points (e.g., de duplicate the dataset to only have unique x s with aggregated action outcomes across all observations) in the original data set and only create one set of imputed labels   but similar unrealizability can happen for x s that are "close" to each other that no model has capacity to label them differently.  The strength of the paper is its intriguing approach to ITE estimation. It is a form of an iterative S learner (vanilla S learners have been widely used in ITE estimation). The low point of the paper is this weakness in theory and analysis   it is unclear if the proposed procedure with sampling imputations (which seems to be important for empirical performance) is even a consistent algorithm. The paper would be much stronger with a more rigorous analysis of when the method will reliably work, and importantly, its limitations   such a study will help practitioners know when to use self training over direct method, targeted max likelihood, S learners, etc. 
This paper generated significant discussion and division amongst the reviewers. On the positive side, some reviewers enjoyed both contributions, feeling the further empirical investigation of existing attacks to be interesting, and the creation of a benchmark to be very useful. On the negative side, no new positive results were proposed, criticism of previous attacks were considered to be unjust, the focus was somewhat narrow, and a benchmark could plausibly be misleading and detrimental.  Given the highly competitive nature of ICLR and the many other excellent submissions, the committee was unable to accept the paper at this time. Below are some suggestions for future submissions.  The content of the paper is generally fine, as long as the caveats and the "tone" are appropriate: we would hope to not mislead potential readers. Here are some (strong) recommendations:   Previous works were proof of concept attacks, and the authors should be careful to not frame them as being "broken"   they perhaps were not meant to be robust to these modifications.   The scope is somewhat narrow. There should be some explicitly statement and justification of the scope, and what in particular is *not* covered by the investigation.   Importantly, a single benchmark can t be a unique gold standard, for many reasons discussed by reviewers. Please state these caveats clearly and prominently in the paper and/or code release, as otherwise the presence of a benchmark could do more harm than good. In particular, Reviewer 4 brought up the following philosophical concern with a benchmark, which I believe is quite reasonable, and I reproduce verbatim. The authors should try to address this in the next version: "This kind of benchmark can push the research to a wrong direction. In my view, the point of attacks are to create an alarm for using machine learning in critical applications. Developing these benchmarks would push the competition in the direction of making existing attacks "better" (whatever "better" means in the benchmark) instead of focusing on designing defense techniques or showing the severity of attacks in other situations. This benchmark could also have a bad effect on future attacks (attacks that want to show a new threat, not the one that try to improve the performance of clean label targeted poisoning attacks on deep neural nets) to gain attention from community as they probably will not pass all the criteria of this benchmark."  As another comment (I believe mentioned by other reviewers), it would be nice if all the terms and settings were defined clearly and precisely. For a benchmarking paper, it is important that the reader can clearly understand the threat model, and what does and does not count as a valid attack.  Finally, many of the reviewers gave detailed comments and concerns. The authors should please note and discuss these concerns in future versions (or at least in a supplement or an arXiv version).
All reviewers express concerns, such as about the presentation, the situation of the paper w.r.t. prior work, the experimental evaluation etc., and recommend rejection.
This paper was quite contentious.  While there is clearly promise in the method and the idea, and reviewers appreciate the importance of encoding non trivial prior knowledge into BO, three reviewers express major concerns regarding the presentation (including worries about over claiming contributions), the specification of the probabilistic model as well as to some extent about the experimental evaluation.  Lastly, the title appears to be a kind of pleonasm   arguably, the key point of using a Bayesian prior is to be able to encode prior knowledge.  The authors consider a different form of prior than perhaps usually meant in BO (in a generative rather than discriminative sense), but the terminology is still confusing.  
Overall, the paper makes some interesting and intuitive observations regarding the autoencoders with a cycle consistency, and aims at achieving controllable synthesis via a disentangled representation. However, the overall consensus was that the manuscript needs further iterations:  In particular: The ideas should be made more precise using mathematical arguments, as it stands some ideas are (e.g. DEAE and UDV) disconnected.  The scope needs to be clarified, e.g. respective contributions of GSL AE and DEAE, use of label information   More numerical/quantitative evaluations, the current experimentation is not convincing enough, needed for better justification (spurious and not convincing experimentations)  The English of the manuscript could be improved as it occasionally hampers the flow. 
This work studies corset based pruning strategies for neural networks, and highlights the looseness of approximation bounds, the difference between approximation error and probability, and the importance of considering post pruning fine tuning. I found the empirical findings and concerns raised around the utility of approximation bounds for pruning guarantees interesting and important, and appreciated the benchmark with varying levels of difficulty. However, the empirical analysis was limited to coreset based methods and a simple LeNet architecture, and could benefit from considering additional non coreset based approaches, architectures, and datasets. While I agree with the authors that new methods are not required for their work to be valuable, I believe that a more thorough empirical analysis is needed to support that their claims that current approximation bounds are not useful across wider experimental settings.
This article provides an analysis of feedforward neural network with iid Gaussian weights and biases in the infinite width limit. The paper  complements earlier work on this topic by taking a function space approach, considering neural networks as infinite dimensional random elements on the input space. This is a well written and rigorous theoretical paper. Although, as noted by a reviewer, there are no direct practical implications, the result is interesting in itself, highly relevant to the ICLR audience, and likely to lead to further exploration of the connections between Gaussian processes and neural networks.   There were a few questions regarding the proofs that have been answered satisfactorily by the authors.   I recommend acceptance. 
There are two main contributions in this paper. First, the use of NN from the same cluster as “views” of the data as understood in classical contrastive learning. Second, the use of additional augmentation techniques, namely cutMix and multi resolution. The reviewers noted that the paper is written well and easy to understand, that the ablation study is conducted well and that the model shows good empirical performance on several tasks.   At the same time, the somewhat limited novelty of the paper was also discussed. As noted by R4, all aspects of the present paper have been discussed in previous work. The difference with previously published clustering based SSL methods was also not very clear. This was discussed in the rebuttal but without strong evidence supporting the claims. Moreover, the ablation study is conducted on models that are trained for 200 epochs. While this is understandable from a pragmatic point of view, the conclusions may be completely different when the model is fully optimised.   Because of all the points raised in the discussions, this paper is a too close to borderline to be accepted. We recommend the authors improve the manuscript given the feedback provided in the reviews and discussion and resubmit to another venue.
The paper proposes a two level hierarchical algorithm for efficient and scalable multi agent learning where the high level policy decides a reduced space for low level to explore in. All the reviewers liked the premise and the experimental evaluation. Reviewers had some clarification questions which were answered in the authors  rebuttal. After discussing the rebuttal, AC as well as reviewers believe that the paper provides insights that will be useful for the multi agent learning community and recommend acceptance.
This paper presents a new method to predict the performance of deep neural networks. It evaluates the method on three different networks: LeNet, AlexNet, and VGG16 under two different frameworks, TensorFlow and TensorRT.  Reviewer 2 thought that the results were promising but comparison with other approaches was weak (PerfNet being the only baseline). They also asked for motivation for the selected architecture as well as raised a number of points for clarification. R2 was also concerned that the single baseline appeared to have not yet been published. The authors clarified this in their response (it was published in ACM RACS, obtaining results directly from those authors).  Reviewer 1 said that the experiments were extensive, but did not find the approach novel (“a normal application of ResNet”). They suggested NAS as a motivating application rather than stopping at predicting execution time. The authors agreed with the importance of predicting execution time in NAS.  Reviewer 3 agreed with Reviewer 1’s assessment of lacking novelty and technical contribution. They also pointed towards NAS, where many methods are already using neural networks to predict execution time. They were also disappointed by the reduced set of architectural elements considered. The authors responded to R3’s comments, but R3 was still not convinced of novelty.  This looks like a fairly straightforward rejection on the basis of not enough technical merit. The authors are encouraged to explore their approach in the context of NAS as per R1’s suggestion.
All reviewers agreed to reject.
This paper investigates the one class classification problem, proposing to learn a self supervised representation and a distribution augmented contrastive learning method; thorough results and analysis show that the method is effective and backs up their claims in terms of the underlying mechanism for why it works. In general, reviewers thought the paper was well written, well motivated/argued, and presents a thorough related work comparison and experimentation, though the novelty was found to be somewhat low. Several reviewers brought up some possible weaknesses in terms of demonstrating uniformity of the representations as well as suggesting additional datasets. Through an interesting discussion, the authors provided additional visualizations and results on the Mvtec dataset. This further bolstered the arguments in the paper.   Overall, this is a strong paper with a clear argument and contribution, and so I recommend acceptance. 
While the reviewers in general liked the ideas proposed in the paper, the experimental evaluation has several issues that need fixing before it can be accepted.
This is a paper that is actively discussed.  The general sentiment is that this paper aims to address an important set of questions. While the technique could be improved with more novelty, the empirical study is extensive. The concerns are about how to interpret the results, or rather whether the empirical evidence fully supports the the claim/hypothesis.  After discussion and rebuttal, the reviewers improved their scores (and one reviewer remained at "weaker marginally above threshold").  The AC read the paper and the discussion. One value the AC sees that the discussion threads between the authors and the reviewers provide a significant amount of scientific value   the questions to be answered are hard and might indeed require further refinements in framing and conceptualization, better techniques,  strong power in experimental designs to rule exclusively various hypotheses.  Thus, the AC recommends acceptance.
This work proposes to train EBMs using multi stage sampling. The EBMs are then used for generating high dimensional images, performing image to image translation, and out of distribution detection. The reviewers are impressed with the results, but indicate that the novelty is limited. While I agree that the work can be seen as a combination of previously proposed techniques, demonstrating that this combination can be made to work well is still a significant contribution to the field. In addition, the paper demonstrates strong results in using Langevin dynamics to translate between images, which I do think is novel. I therefore recommend accepting the paper for a poster presentation.
The paper studied an interesting and important problem in active learning/information acquisition (AFA), and provided an RL based active learning scheme for a broad spectrum of AFA tasks, in both supervised (active classification/regression) and unsupervised (feature completion/recovery) domains. The reviewers generally find the paper well presented, and all appreciate the broad applicability of the proposed approach, which leverages reinforcement learning and a generative surrogate model to learn the acquisition/reward function of AFA. However, there are also shared concerns among several reviewers on the novelty and positioning of the proposed approach, as well as on whether the proposed experiments results well demonstrated the significance of the algorithm. Given that this is a purely empirical paper, both aspects are important to be properly addressed in a revision. 
This paper attends to the problem of how to implement dense associative memories (i.e. modern Hopfield networks) using only two body synapses. This is interesting because modern Hopfield networks have much higher capacity, but at face value, they require synapses with cubic interactions between neurons, which to the best of our knowledge, is not a common feature in neurophysiology (though it should be noted: it is not by any means impossible from a physiological perspective to have cubic interactions at synapses, see e.g. Halassa, M. M., Fellin, T., & Haydon, P. G. (2007). The tripartite synapse: roles for gliotransmission in health and disease. Trends in molecular medicine, 13(2), 54 63.).   The authors show how the use of a layer of hidden neurons, akin to a restricted Boltzmann machine architecture, coupled with the right energy function, can be used to recover dense associative memory models using only two body synapses. They also demonstrate how this connects to recent work on the relationship between attention mechanisms in modern ML models and Hopfield network dynamics.  Overall, the reviewers were positive on this paper. The most common critique related to the question of "biological plausibility". The authors addressed these concerns by adding some more recognition as to the lack of biological plausibility and more discussions of the relevance to neuroscience. To be candid with the authors, if the goal is indeed to make a more biologically plausible model of modern Hopfield networks, than a fair bit more work would be needed to connect the paper to biology well. As it stands, the only connection is the shift to two body synapses by using hidden neurons, but this provides limited insight for most neuroscientists, as noted by Reviewer 2. Also, some of the biological examples provided seem strained (e.g. the colour memory example, where there is no physiological reason to posit that we store colour memories using our retina, or the MNIST example, since there is no reason to suppose that animals can memorise thousands of specific MNIST images). But overall, the critique regarding biological plausibility was attended to. The other concerns raises were also largely addressed.   Given the interesting contributions from this paper, the overall positive reviews, and the decent job at addressing reviewer concerns, the AC believes that this paper should certainly be accepted. A decision of "Accept (Poster)" seems appropriate, though (as opposed to an oral or spotlight), given the lack of biological connections in a paper with a stated goal of achieving a more biologically realistic model.
MAML is a well known gradient based bi level optimization to learn a good initialization over a set of relevant tasks. This paper investigate different variants of MAML, providing empirical analysis of two new algorithms (RDP and MCL). Reviewers agree that it is interesting to see what the change of optimization mechanism on both head and body brings to us in the MAML framework. This is done by only empirical analysis. However, all reviewers have concerns that the current version (or even revised one after the author responses) does not contain substantial contributions over existing work in the sense that: (1) experiments do not support well what s been claimed; (2) writing should be much improved to clearly explain the formulation of RDP and MCL, as well as figures in experiments; (3) the analysis about the importance of multi step adaptation is not clear (Section 4); (4) the proposed method has little improvements over baseline methods.  Without any positive feedback from reviewers, I do not have choice but to suggest rejection.     
All the reviewers are in favor of accepting this paper, which demonstrates both theoretically and empirically the value of reward randomization in solving multi agent reinforcement learning problems. The rebuttal phase was crucial in improving the quality and evaluation of the submission. I am glad to recommend acceptance.
 The paper proposed a new way for training models that stack the same basic block for multiple times   share the weights first and then untie the weights. Ablation study shows that the proposed algorithm has marginal improvement over the baseline. The authors also provide some theoretical justifications to how the proposed idea works. The proposed idea is straightforward and intuitive. Weight sharing has been used in previous works, and what’s new in this paper is to unshare the weights in the middle (with a heuristic rule). The hope is that by doing so, one can achieve a better tradeoff between speedup and accuracy. However the experimental supports are somehow weak and incomplete. For example, in order to show the real speedup, one should provide the full training curve (until convergence) under different settings, instead of just showing one data point (at 500K). It is very common that one can get some speedup at 500K, but the speedup totally disappears after another 500K steps.  Furthermore, the theoretical analysis is conducted in a simplified setting, and it is not very clear whether it can be used to explain what really happened during BERT training. The reviewers conducted some lengthy discussions after the author rebuttal was available. As a final consensus, we think that there are still concerns on the paper, which makes us hesitate to give an ACCEPT recommendation. 
Reviewers liked the concept of the zero day attack and yet raised different concerns about the other parts of the paper. In general, Reviewers wanted to see more thorough experimental evaluations (e.g., against blackbox attack and adaptive attack) and improved clarity of the theoretical analyses. AC encourages authors to incorporate Reviewers  comments when preparing the paper for elsewhere.
The paper addresses regression in a weakly supervised setting where the correct labels are only available for examples whose prediction lie above some threshold. The paper proposes a method using a gradient that is unbiased and consistent.  Pros:   Problem setting is new and this paper is one of the first works exploring it.   The procedure comes with some unbiasedness and consistency guarantees.    Experimental results on a wide variety of datasets and domains.  Cons:   Novelty and technical contribution is limited.   Motivation of the problem setting was found to be unclear.   Some gaps in the experimental section (i.e. needing the use of synthetic data or synthetic modifications of the real data).  Overall, the reviewers felt that as presented, the paper did not convincingly motivate the proposed upper one sided regression problem as important or relevant in practice, which was a key reason for rejection. The paper may contain some nice ideas and I recommend taking the reviewer feedback to improve the presentation. 
The authors propose a new dataset, ChemistryQA which has complex questions requiring scientific and mathematical reasoning. They show that existing SOTA models do not perform well on this dataset thereby establishing the complexity of the dataset.   The reviewers raised several concerns as summarised below:  1) Writing is not very clear 2) The quality of the dataset is hard to judge as some crucial information about the dataset creation process is missing  3) The size of the dataset is small 4) some stronger QA baselines need to be included  Unfortunately the authors did not provide a rebuttal. Hence, its current form this paper cannot be accepted. 
This paper proposes a modular RNN architecture called SCOFF. The work was inspired by cognitive science(object file and schema) and was built upon previous work RIMs. The method is validated on tasks having multiple objects of the same type.  Pros:   It addresses an important problem in DNN   systematic generalization.   The proposal makes sense and is more flexible than RIM.   Experimental results outperform baselines.  Cons before rebuttal:   The presentation of the algorithm is not very clear due to some confusing notations and missing details of algorithm steps.   The comparison with baselines might not be fair due to extra parameters.   The novelty is limited, because the only difference from RIM is weight sharing.  The reviewers raised concerns listed in Cons. The authors successfully addressed concerns: they indicated that the comparison was fair with the same input to both; SCOFF is more flexible than RIM, and there is spatial attention to input. The authors added the missing details in the revised version.  All reviewers agree that the problem is important and the idea is interesting.  Since the authors  rebuttal was very helpful in clarifying the questions raised, I recommend accept. 
This paper proposes to use a single parametric Householder reflection to represent Orthogonal weight matrices. It demonstrates that this is sufficient provided that we make the reflection direction a function of the input vector. It is also demonstrated under which conditions this modified transformation is invertible. The derivations are sound.  This insight allows for cheaper forwarding of the model but it also comes with extra costs: It has an increased computational cost for inversion (e.g. requires optimisation) and, importantly, it does not allow to cache the $O(d)$ matrix so  it is not clear there is an advantage of the method over exp maps when we have parameter sharing (e.g. as in RNNs), since the action of the matrix has to be recomputed every time. The presented experiments are OK, but comparisons to other (potentially more efficient) methods are lacking as pointed out by the reviewers. As it stands it is not clear that this is an idea of broad interest, perhaps more suited to a specialised venue such as a workshop.
Description: The paper presents a weakly supervised model CICGMO for disentangling category, shape and view information from images. Label information is not need as  the weak supervision is done by grouping together different views of the same object. They show that this outperforms other techniques on tasks such as invariant clustering and one shot classification.  Strengths:   Paper is well written   Data category is explicitly modeled    The weakly supervision approach is appealing,  since the grouped data used as supervision information is easy to obtain   Invariant clustering and one shot classification results outperforms other methods significantly, showing CIGMO is doing a decent job at those tasks. This could be explained by CIGMO ability to better  disentangle category shape view.  Weaknesses:   It is unclear how well (quality) the generative model is able to disentangle shape from view   The reconstruction quality is quite low, such that it is difficult often times, in the MULTIPIE example, to clearly identify a face geometry.   Generated results are not evaluated directly, but rather evaluation is done through down stream tasks such as invariant clustering. This makes it difficult to show the quality of shape and view information.
This paper proposed a new semi supervised object detection approach using Unbiased Teacher to jointly address the pseudo labeling bias and overfitting issues. Significant improvements over SOTA were reported on COCO and VOC. Reviewers agree that the proposed method is simple and effective, and the experimental results are solid and convincing.  While the novelty of technical contributions for individual components may not be very significant, the idea is simple and well executed with strong results and good presentation. Overall, the paper is recommended for acceptance (poster). 
Most of the reviewers had serious problems with clarity to start out.  The authors have addressed some, but not all of these problems.   More importantly, there were issues of significance and experimental evaluation. I concur with r4 on the experimental evaluation.  I think if you re going to explicitly specialize toward disentangling affine transform parameters,  that s fine, but then you re in application paper land, and I think there needs to be more of an attempt to show that it will work "in the wild".  For this reason, and for the general reason that reviewers unanimously voted to reject, I am recommending rejection.
The paper combines a few different ideas for representation learning on sequential data and is able to achieve competitive WER on the Librispeech ASR dataset. I appreciate the fact that the authors engaged with reviewers and tried to improve the paper. While I get a sense that the final system has many moving parts, I believe the paper meets the bar for acceptance at ICLR.
This paper provides a novel theoretical analysis of epoch wise double descent for a linear model and a two layer non linear model in the constant NTK regime. Some reviewers noted that these models may be too simple to offer a full explanation for the phenomenon in state of the art practical models, for which the NTK is known to change significantly. While this may be true, I believe that the detailed understanding derived in these simple settings provides an important first step and will surely be of interest to the community. I therefore recommend acceptance.
Although borderline, all reviews are somewhat below the acceptance threshold. The main issue appears to be that the reviewers find the main claims unsupported by the experiments. Other complaints center around the presentation, which could be improved in a revision.
This paper aims to address the robustness issues by considering natural accuracy, sensitivity based robustness and spatial robustness at the same. However, the reviewers pointed out that many things, like the expriment, the presentation, the algorithm, are not clear. In addition, the technique part is weak and below the bar of ICLR.
The reviewers indicated a number of concerns (which I agree with) which have not been addressed by the authors as they have not provided any response.  Indeed, the paper would be significantly improved once these issues are addressed. 
This paper proposes a new implementation of a previously proposed two stage process for video prediction: first predict future segmentation maps, then map them to video frames. Combined with other advances in video prediction and image generation, this simple idea is shown empirically to work very well, producing video predictions up to many hundreds of frames into the future in real stochastic settings with unprecedented quality. Strong ablation studies over the course of the review process further serve to confirm the value of various design choices involved in the implementation. 
This paper proposes a channel pruning method to compress and accelerate pre trained CNNs. The reviewers suggest further analysis of the experimental results to help explain the gains in performance, as well as point out some errors in the formulation. The paper is also found similar to meta pruning method. The authors are encouraged to re submit the paper after adding the analysis and improving the related work section. 
The paper introduces a new idea for multi view classification: using a Dirichlet distribution over the views to model uncertainty.  The paper appears to be clear, well written and sound. Also, the experimental comparison is thorough.  The authors have given pertinent responses to the reviewers  questions, including w.r.t comparing against Bayesian/deep CCA in terms of accuracy.  Overall, this is a good paper. 
The paper is concerned with learning transformation equivariant node representation of graph data in an unsupervised setting. The paper extends prior work in this topic by focusing on equivariance under topology transformations (adding/removing edges) and considering an information theoretic perspective. Reviewers highlighted the promising ideas of the approach, its relevance for the ICLR community, and the promising experimental results (although improvements over prior work are not necessarily significant on all benchmarks).  However, reviewers raised concerns regarding the novelty of the method and the clarity of presentation with respect to key parts of the method. These aspects connect also to further concerns raised, e.g., related to mathematical correctness as well as the significance of the proposed loss function, the benefits of motivating it from MI, and the improvements over GraphTER. The rebuttal didn t fully clarify these points. While the paper is mostly solid, I agree with the reviewers  concerns and   currently   the paper doesn t clear the bar for acceptance; it would require another revision to improve upon these points. However, I d encourage the authors to revise and resubmit their work with considering this feedback.
I found the setup for this paper a bit contrived. The tool is presented as a code translation tool, but it really functions more as a multi language code search tool. The Idea is that one has a program in language A, and a database that contains the same program in language B, so one can translate from A to B simply by searching for the right program in the database.   When evaluated as a language translation tool, it appears to outperform existing language translation schemes, but this is an unfair comparison, because iPTR is being given a database that contains the exact translation of the program in question. The performance is also compared with code search tools, but these are also apples to oranges comparisons, because the tools in question are operating from very high level queries. A much more comparable baseline would be the Yogo tool  recently published in PLDI (https://dl.acm.org/doi/abs/10.1145/3385412.3386001), or for compiled languages you could compare against statistical similarity tools for binaries (https://dl.acm.org/doi/10.1145/2980983.2908126).   The experiment in the appendix A5 is more fair to standard language translation, and it yields results that are much less impressive. I would be much more comfortable with this paper if it were written around this experiment, or alternatively if it were evaluated against a more comparable approach for semantic code search. 
This paper introduces an alternative to self attention, based on matrix factorization, and apply it to computer vision problems such as semantic segmentation. The method is simple and novel and obtains competitive results compared to existing approaches. The reviewers found the paper well written and easy to understand. For these reasons, I recommend to accept the paper.
This paper presents the Order Memory Policy Network (OMPN), an architecture for modelling a hierarchy of sub tasks and discovering task decompositions from demonstration data. Results are presented on a compositional grid world task (Craft) and on a simulated robotics task (Dial).  The reviewers agree that the proposed method is novel and interesting, that the paper addresses an important problem, and that it is well written. One main criticism by the reviewers, the lack of experimental evaluation of different hyperparameter choices, such as the depth of the memory stack and the expected number of subtasks, has to a large part already been addressed in the revision by the authors. The total number of hyperparameters that need to be tuned, however, is quite large and the authors are encouraged to revise their claim "Our central message is that OMPN is a general off the shelf model for task decompositions" in this light. The paper is borderline, and could clearly benefit from a revised, stronger presentation and more extensive experimental evaluation, but I am confident that the authors can use the time until the camera ready version is due to address some of the remaining feedback by the reviewers, and hence I think that this paper can be accepted.  The authors are further encouraged to take the following additional reviewer feedback into account, which was brought up during the internal discussion period: 1) The complexity of the proposed method could be better justified by more thoroughly investigating the effectiveness of using a multi level hierarchy (e.g., by running experiments on more complicated and hierarchical tasks with multiple branches). 2) Further strengthening down stream performance evaluation, such as in imitation learning (in addition to the already presented behavioral cloning results) and/or reinforcement learning, would further strengthen the paper and demonstrate that the discovered decomposition is indeed useful. 
Dear Authors,  Thank you very much for your detailed feedback to the reviewers in the rebuttal phase. The feedback certainly clarified some of the concerns raised by the reviewers and improved their understanding of your work. Indeed, some of the reviewers have increased their scores.  However, overall, we think this paper has rather marginal novelty and there are still several conceptual and technical issues to be further discussed, such as the definition of the grouping concept and the distributional shift assumption.  For these reasons, I suggest rejection of this paper, in comparison with many other strong submissions. I hope that the detailed feedback and additional comments from the reviewers help you improve this work for future publication. 
**Overview** This paper performs detailed ablation studies over different dynamics prediction methods for MBRL. It proposes metrics for models to evaluate how different types of uncertainty impact predictions. The paper also measures control performance with random shooting MPC. The paper further implements a new hyper parameter schedule to achieve new SOTA performance on the acrobat task.  **Pro**    The paper is well written.   The analysis in this paper is very warranted.    The paper provides a very detailed ablation study.   The authors do a great job defining and arguing for evaluation metrics.   The seven properties and metrics are mostly well motivated and well defined.   The authors discussed the results clearly with implications.   The result of the necessity of probabilistic vs. deterministic models in different scenarios is a good contribution to this field.  **Con**   The methodology might be hard generalizable, i.e., there is difficulty in matching the paper to the literature based on its own defined metric.   The scope might be limited.  **Recommendation** The paper provides a significant contribution to MBRL by providing a detailed empirical study. During the rebuttal phase, the authors addresses many reviewers  concerns in a satisfactory way.  The paper is well written and easy to read. The recommendation is an accept. 
This paper addresses the problem of super resolution of coarse physical simulations into fine grained video by satisfying some physical properties. The method uses generative models for sequences of images (conditional GANs with spatial and temporal discriminators), that take into account both the multiplicity of realisations given the same conditioning (via adversarial losses) and the spatial frequencies of the modeled surfaces using an evaluation in the Fourier domain, and that also allow training without paired low  and high resolution samples by relying on a generative losses applied to the downsampled reconstruction. As demonstrated on synthetic 2D data or on 3D frames showing particle simulations, the proposed method can generate images with high frequency content.  Reviewers have praised the intuition of using GANs for modeling high frequency features in physical simulation reconstruction. Weaknesses included: * insufficient analysis and explanation of the Fourier domain supervision * limited novelty w.r.t. TempoGAN (with the exception of frequency based evaluation) and lack of evaluation on more general tasks * unconvincing results with marginal improvement over plain MSE loss * missing ablation studies * toy dataset evaluation only, which makes this work seem preliminary  This paper comes out as 6/7 in my AC stack and I will unfortunately recommend to reject it, hoping that the authors will resubmit an improved version, taking into account the reviewers’ suggestions, for an upcoming conference or for a journal venue. 
This paper tries to address the uncertainty calibration problem in meta learning by weighting the gradient from different tasks according to class wise similarity. There have been many concerns raised by the reviewers and most of them either are still not properly addressed after the rebuttal period.   The main concerns are as follows:   The problem the paper tries to address is not clear. The use of weighting in meta update is motivated from distributional uncertainty, but it is not clear how that will improve the task calibration at meta testing time.   The proposed update runs into the risk of focusing on simple tasks and down weighting hard tasks that could be improved with more learning to get more discriminative features. That might hurt the classification performance even though it gets better calibration quality.   Novelty is limited. The proposed method is a fairly small modification on the original MAML algorithm.   More comprehensive empirical evaluation is required to support the superiority of the proposed method to other baselines.  I suggest the authors take all the reviewers  comments seriously and improve their work for a better revision. 
# Summary The paper was initially well received by reviewers, remarking the new gradient estimator, a new dropbits technique and an interesting observations of better performance when the bitwidth is learned. The experimental results also look promising: showing improved training performance and test performance (including on ImageNet with ResNet 18), properties to reduce quantization error of learned weights, possibility to learn number of bits via learning stochastic bit dropping masks.  A deeper verification of the specific methods proposed however showed principal issues:   The methods proposed in the paper are not sufficiently justified by verifiable formal arguments. The proposed intuitive explanations are entangled and actually lead to wrong conclusions. In particular a main claim of the paper that the proposed estimator reduces bias and variance of Gumbel Softmax estimator was shown wrong and was removed in the revision. The remaining claim that the estimator reduces quantization error is also wrong (see below). With these issues, the gradient part of the paper is largely incorrect, which is in a strong discrepancy with good experimental results.    Other parts of the paper, comprising the remaining technical contributions are not properly positioned with respect to the SOTA and thus are not necessary novel / improving.  The main technical issues were discussed with all reviewers and were either supported or not objected. Therefore, I am confident that the submission has critical problems and must be rejected. I recommend the authors to thoroughly investigate all the raised issues (by all reviewers) before resubmitting to other venues.   # Details  ## Gradient  The overclaim of reducing bias and variance / resolving bias/variance tradeoff has been removed in the revision, but the new gradient estimator remains a central innovation proposed. It is however not justified and cannot indeed be regarded as a good estimator:  * The justification argues about the bias of the Gumbel Softmax sampling distribution, but the proposed estimator does not use a sampling distribution in the forward pass, and thus by design cannot address this problem.  * The backward pass to use gradient in i_max only (Eq. 3) is not based on any justification at all.   * The remaining claimed good property: "to reduce the quantization error" is, according to the definition in sect. 3.4, not a property of a gradient estimator, but of the stochastic relaxation alone. There is an experimental evidence Fig.2 that the estimator _leads_ to lowering the quantization error. This is however in a contradiction with a direct verification of the proposed estimator that was conducted: The verification inspects gradient in a single variable $x$ and a linear loss function of the quantized variable $\hat x$.  It shows that the gradient is zero at grid points and discontinuously reverses the direction at half grid points. Because of such zigzagging, *it does not correspond to minimizing the loss function*, i.e. not a reasonable estimator. The grid points, where the gradient vanishes, may correspond to either local minima or to local maxima of the estimator. Which of the two cases occurs depends exclusively on the sign of the incoming gradient from the loss function. For $L(\hat x)   \hat x$ we observe that the negative gradient points towards nearest grid point, but for $L(\hat x)    \hat x$ it points away from the nearest grid point, i.e. a step would *increase the quantization error*. The implementation of this verification is attached anonymously: https://colab.research.google.com/drive/1PibzRMXQ NVZMUdfgTIK0Q5FxUKyxfqI?usp sharing  * Alternative existing estimators are not sufficiently discussed: e.g. common deterministic STE, as used in quantization papers: to just treat the quantization operation as identity on the backward pass. Estimator used by Shayer et al. (2018),  Ajanthan et al. 2019 “Mirror descent view for neural network quantization”, Unbiased estimators (e.g. Yin et al. 2019 “ARSM: Augment REINFORCE Swap Merge Estimator for Gradient Backpropagation Through Categorical Variables”). While unbiased estimators may still have too high variance and or be too computationally demanding for deep networks, they can be used for verification purposes.   * The claim that it is not possible to apply unbiased estimators, in particular score function estimator, because of dependency on x is incorrect. See e.g. Schulman et al. 2015 “Gradient Estimation Using Stochastic Computation Graphs”. Many works on advanced unbiased estimators also demonstrate experiments with 2 or more layers of hidden discrete stochastic variables. From this and technical discussion with authors, it is seen that the experimental study is Sec 3.4 is very limited and erroneous.   * The rule by which the probability mass of the dropped bits is uniformly spread over the remaining bits is not justified and appears methodologically incorrect. In Fig.4 it is not clear what bits were dropped and why the mass at $ 2\alpha$ has decreased.  ## Gradient and Other Techniques relative to SOTA  * The bias problem of GS estimator, detailed in Fig.1. is not novel to me, it is in fact known that the mean under the concrete distribution (of linear or non linear objective) differs from the mean under the categorical distribution, see e.g.  Lorberbom et al. (2018) Direct Optimization through argmax for Discrete Variational Auto Encoder (Fig.1)  Andriyash et al. (2018) Improved Gradient Based Optimization Over Discrete Distributions  Thus analysis of individual samples in Fig.1 appears unnecessary detailed. The issue that the relaxed distribution of Gumbel Softmax may cause a large estimation error for gradients downstream is already discussed by Louizos et al. (2018) and other works, e.g.   Choi 2017, "Unsupervised Learning of Task Specific Tree Structures with Tree LSTMs" Sec 3.2  and Andriyash (2018). This later problem was previously addressed in many cases by the ST Gumbel Softmax heuristic. This heuristic indeed performs better in CIFAR 10 experiments in the submission / Louizos (2018), which is likely to be a better tuned and more controlled experiment than ImageNet. * More methods should be discussed that reduce the quantization error during learning. E.g.  Cong et al. (2018): “Extremely low bit neural network: Squeeze the last bit out with ADMM”,   who include terms explicitly minimizing the  quantization error. In fact most works quantizing network weights primarily focus on reducing the quantization error, e.g.  Nagel et al. (2019)" Data Free Quantization Through Weight Equalization and Bias Correction  * The prior works on learning bit width should be more extensively discussed / compared to, especially if this part becomes central to the submission. E.g.  Baalen et al. (2020) “Bayesian Bits: Unifying Quantization and Pruning” (or references therein if this is considered contemporaneous).    Courbariaux & David (2015): Training deep neural networks with low precision multiplications  * The new hypothesis for quantization is in fact similar to the effect observed elsewhere that quantizing neural networks progressively leads to better results.  E.g.   Zhou et al. (2017) Incremental Network Quantization  Towards Lossless CNNs with Low Precision Weights.  It is questionable whether the link to the lottery ticket hypothesis is justified, since the latter shows quite the opposite, as reviewers have pointed.
Two referees support accept and two indicate reject. Despite the author s rebuttal, reviewers determined through subsequent private discussions that the paper was insufficient to satisfy the high standards of ICLR due to the lack of diverse evaluations on various models/datasets  and increased computational overhead. Even the two positive reviewers agree on the weakness of the paper in terms of experimental evaluations and do not have a strong opinion on the acceptance. 
This paper extends an earlier work with scalar output to vector output. It establish a relationship of two layer ReLu network and convex program. The result can be used to design training algorithms for ReLu networks with provably computational complexity. Overall, this is an interesting idea, leading to better theoretical insights to computational issues of two layer ReLu networks. 
The reviewers all agreed that the paper represent thorough work but also is closely related to existing literature. (All referees point to other non overlapping literature so it is a crowded field the authors have entered.) The amount of novelty (needed) can always be discussed but given the referees unanimous opinion and knowledgable input it is better for this work to be rejected for this conference. Using this input can make this work a good paper for submission elsewhere. 
The authors introduce an RNN model, ProtoryNet, which uses trajectories of sentence protoypes to illuminate the semantics of text data.  Good points were brought up and addressed in discussion, which have improved the paper   including a helpful suggestion from Rev 3 to fine tune BERT sentence embeddings in ProtoryNet, which led to significant performance gains.  Unfortunately the tone of discussion with one reviewer slipped below the respectful standards to which we aspire, but rest assured that only substantive points on the paper were considered.  Reviewers were split but in discussion converged to leaning against acceptance, allowing the authors to reflect on, and incorporate new results carefully in an updated manuscript.
The paper proposes Fourier temporal state embedding, a new technique to embed dynamic graphs.  However, the paper needs to be improved in writing, computational complexity analysis, and more thorough baseline comparisons.
This paper studies different properties of the top eigenspace of the Hessian of a deep neural network and their overlap. It raised quite a lot of discussion, which finally went in not very constructive way. The reviewers generally agree that the paper has potential, but the actual contribution is limited.  Pros:    The idea that top eigenspaces between different models have high overlap is interesting   The explanation that these structures can be explained by Kronecker product approximation of the Hessian.  Cons:    The connection to PAC Bayes is unclear and seems artificial.   Many of the related work is missing   The models and datasets are too simple, and general conclusions can not be made on such kind of models. Much more testing is needed to verify the claims, including state of the art architectures and datasets.
The paper in its most recent version claims that deep neural networks, when very carefully regularized, outperform methods such as Gradient Boosting Trees on tabular data. This is genuinely surprising to me (in a good way), and I suppose it is as well to the community.  The paper initially received negative reviews with two key remarks that "The results are somewhat expected." (R4, R3, R2). Indeed, the original version mainly stated that very careful regularization helps on tabular data.  Naturally, the reviewers (including myself) seen then as the second key weakness that "All experiments are run on tabular data." (R4, R3).  Based on the reviews, the Authors have clarified and changed their message. I think it is well summarized by R2 "The paper has been significantly refocused and now sells itself as a way of deep neural networks being competitive versus gradient boosting methods, which are dominating the tabular heterogeneous tasks."  As R2 said and was reflected in comments by other reviewers, "[...] convinced by authors response on paper novelty, technical contribution and (after the re focusing) potential usefulness to the community".  Given the new message of the paper, a key new question surfaces. Is this indeed the first convincing demonstration that deep learning can outperform more standard methods on tabular data? R2 pointed out TabNet (see also Google Cloud offering) that already in 2019 claimed "beating GB methods for the tabular data". There is also NeurIPS work "Regularization Learning Networks: Deep Learning for Tabular Datasets"; their abstract opens with "Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs". The latter work did not claim to beat GBT. Regardless, the two works should be carefully discussed and compared empirically to in the new version of the work.   I am also not yet fully convinced by the added comparison to GDBT. Arguably, AutoML from the sklearn package is not the most popular way to use GDBT in practice. How would regularization cocktails compare to GDBT from XGBoost, optimized using either random search or bayesian optimization?  Based on the above, I have to recommend the rejection of the paper. The key reason is: *the new reframing of the paper is exciting but warrants a much more detailed and careful evaluation*.  I really appreciate the work the Authors have put in clarifying and changing the message of the paper. I understand this is disappointing that we won t be able to include the work in ICLR. Nevertheless, I hope that the Authors found the feedback useful, and wanted to thank the Authors for submitting the work for consideration in ICLR.
This work proposes a simple and intuitive way to improve how to learn a communication protocol off policy in the non stationary situation in which messages received in the past do not reflect an agent s current policy. The authors introduce a communication correction that relabels the received message adjusting it to the current policy. The authors show that this method, besides being simple, is effective in a number of experiments. As observed by some reviewers, an issue with the method is that it is not clear how it would scale up to more complex environments than those considered. However, the authors addressed the concerns during the response phase, both adding new experiments, and with a clear statement of what are the outstanding issues. The paper is certainly a clever and solid contribution to the area of multi agent communication learning, and I am strongly in favour of accepting it. 
The authors put a lot of effort in replying to questions and improving the paper (to a point that the reviewers felt overwhelmed).  Pros:   An interesting way of dealing with model bias in MPC   They successfully managed to address the most important concerns of the reviewers, with lots of additional experiments and insights   R3 s concerns have also been successfully addressed by the authors, the review & score were unfortunately not updated  Cons:   The only remaining point is that the simulations seem to be everything but physically realistic (update at end of R1 s review), which is probably a problem of the benchmarks and not the authors faults.
The paper provides a new covariant approach to 3D molecular generation motivated by the desire handle compounds with symmetries. To this end, the method uses equivariant state representations for autoregressive generation, built largely from recently proposed covariant molecular networks (comorant), and integrating such representations within an existing actor critic RL generation framework (Simm et al). The selection of focal atom, element to add, and the distance are realized in an equivariant manner while the compound valuation remains invariant to rotation. The approach is clean and well executed. The authors added additional experiments (e.g., RMSD demonstrating stability of generated compounds) to further reinforce the case for the method.  
This work investigates how importance sampling strategies can improve training with budgeted constraints., with a focus on the benefits from variety provided by data augmentation samples.  Initial clarification issues raised by the reviewers were taken into account such as a new title, clarification of some explanations and corrections of typos.  However, the reviewers still agree that the paper is not ready for publication for several reasons:   the comparison with the literature is still insufficient and should be better organised,   experiments too narrow to conclude general benefits from the paper as it is, since there is a single type of tasks that is studied from the same dataset family. Questions related to very small budgets, below 20% also remain open and would require a new submission.
This work investigates an algorithm to learn representations of Lie groups. It first learns a representation of the Lie algebra by enforcing the Jacobi identity using known structure coefficients. Then obtains the group representation via matrix exponentiation. The paper also proposes a Poincaré equivariant neural network, and applies this model to an object tracking task. The paper is well motivated, the derivations could be more clearly presented but are otherwise sound. The experimental results are promising but rather limited in scope at the time.
This paper proposes O RAAC, an offline RL algorithm that minimizes the Conditional Value at Risk (CVaR) of the learned policy s return given a dataset by a behavior policy. The reviews are generally positive with most agreeing that the paper presents interesting empirical results.   The experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with more clearer safety settings.  Agreeably, asking for comparisons with unpublished results may be unfair, however, it would be recommended to authors to include additional comparisons with latest methods in Offline/Batch RL, including the ones which don t guarantee risk, such as CQL, BRAC, CSC.  Further, The theoretical properties of the proposed algorithm are largely unclear. It would help to analyze the effect of both convergence rates, and fixed points, further what is the effect of addition of risk, does the algorithm converge to a suboptimal solution or get there slower. Finally empirical reporting of cumulative number of failures (discrete count) during training as well as during evaluation would be very useful to practitioners.   Other relevant and concurrent papers to potentially take note of: Distributional Reinforcement Learning for Risk Sensitive Policies (https://openreview.net/forum?id 19drPzGV691) Conservative Safety Critics for Exploration (https://openreview.net/forum?id iaO86DUuKi)  I would recommend acceptance of the paper. I would strong encourage release of sufficiently documented and easy to use implementation.  Given the fact that the main argument is empirical utility of the method, it would be limit the impact of this work if readers cannot readily build on O RAAC.  
Reviewers generally appreciate the theoretical contribution of the paper, namely Accelerated Gradient Descent on the sphere and hyperbolic space with the same convergence rate as the Euclidean counterpart. However, there are several major concerns with the current work. From a theoretical standpoint, the geodesic map, which plays a crucial role in the algorithm and theoretical analysis, exists if and only if the manifold has constant sectional curvature (sphere and hyperbolic space). It is not at all clear how the current approach can be extended beyond this setting.  From an algorithmic viewpoint, the stated algorithm has not been experimentally validated. It is suggested that at least some synthetic experiments, e.g. on the sphere or Poincare disk, be carried out. Finally, the current presentation is quite dense and should be considerably improved.
The paper considers the problem of private data sharing under local differential privacy.   (1) it assumes having access to a public unlabeled dataset for learning a VAE, so it reduces the dimensionality in a more meaningful way than simply running PCA. (2) the LDP guarantee is coming from the standard Laplace mechanism and Randomized Responses. (3) then the authors propose how to learn a model based on the privately released (encoded) data which exploits the knowledge of the noise distribution.  None of these components are new as far as I know, nor were they new in the context of differential privacy. For example, the use of a publicly available data for DP was considered in:     Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 363–378. Springer, 2013.  (they called it Semi Private Learning...)    Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., & Talwar, K. (2017). Semi supervised knowledge transfer for deep learning from private training data. In ICLR 17.  The idea of integrating out the noise by leveraging the known noise structure were considered in:    Williams, O., & McSherry, F. (2010). Probabilistic inference and differential privacy. Advances in Neural Information Processing Systems, 23, 2451 2459.    Balle, B., & Wang, Y. X. (2018). Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising. In International Conference on Machine Learning (pp. 394 403).  And many subsequent work.  The contribution of this work is in combining these known pieces (without citing some of the earlier work) to achieve a reasonably strong set of experimental results (for LDP standard).  I believe this is the first experimental study that uses VAE for the dimension reduction, however, this alone is not sufficient to carry the paper in my opinion; especially since the setting is now much easier, with access to a public dataset.  The reviewers question the experiments are baselines are usually not using a public dataset as well as the practicality of the proposed method.   Also, connections to some of the existing work on private data release (a.k.a., private synthetic data generation) were note clarified. For these reasons, there were not sufficient support among the reviewers to push the paper through.   The authors are encouraged to revise the paper according to the suggestions and resubmit in the next appropriate venue.
This paper received borderline scores, which makes for a difficult recommendation. Unfortunately, two of the reviews were too short and thus were of limited use in forming a recommendation. That includes the high scoring one, which did not adequately substantiate its score.  There is much to admire in this submission. Reviewers appreciated the originality of this research, linking rate reduction optimization to deep network architectures: * R1: "The paper proposes a novel perspective" * R4: "The novelty of the paper is in that formulation of the feature optimisation is baked in into a deep architecture" * R5: " I think the construction seems interesting and the rate reduction metric seems like a reasonable thing to optimize. I found the relationship of coding rate maximization to ReduNet to be quite clever" * R3 (short): "The innovative method allows the inclusion of a new layer structure named ReduNet"  Reviewers also applauded the paper s clarity, including R4 who raised their score to 6 based on satisfying clarity revisions from the authors: * R1: "The writing is good and easy to follow" * R4 post discussion: "Clarity is not an issues anymore   additional explanations provided by the authors and one more careful reading of the paper helped in understanding of all the aspects of the model" * R2 (short): "The paper is well structured."  However, there were some core questions around how well the main significance claims of the paper are supported. The most in depth discussion on these topics is in the detailed thread with R5. In that thread there are many points discussed, but the two issues seem to be: 1. whether the connection between ReduNet and standard neural net architectures is sufficiently substantiated so as to constitute an explanation for behaviors of those standard architectures, like CNNs; and 2. whether the emergence of ReduNet s group invariance/equivariance is surprising or qualitatively new.  The first is much more central. On the first issue, R5 writes in summary: "Fundamentally I think the authors propose a hypothesis: that ReduNets explain DL models. However, the authors do not take meaningful steps towards validating this hypothesis. [...] I would contrast this with, for example, the scattering networks paper (https://arxiv.org/abs/1203.1513) which did an exceptional job of arguing for an ab initio explanation of convolutional networks."  I find R5 s perspective on this point to be compelling, in that the paper currently doesn t do enough to justify these main claims, either through drawing precise nontrivial mathematical connections or through experimental validation. (The thread has a much more detailed and nuanced discussion.)  The second issue is not quite as central to the significance of the paper, but it was noted by multiple reviewers: * R5: "I may be missing something, but given the construction of ReduNet, I feel as though the emergence of a convolutional structure subject to translation invariance is not terribly surprising." * R4: "Finally, I am not sure if the result of obtaining a convnet architecture in ReduNet when translation invariance constraint is added the embedding is all that surprising." * R4 post discussion: "Reading the exchange between the authors and R5 I am still not fully convinced that translation invariance property is all that surprising, but for me that s not a reason to reject."  At the least, the paper as written hasn t yet convinced some readers (myself included) on these claims.  As I mentioned at the start, this paper is borderline, but because I am largely aligned with R5 s perspectives, I think this paper does not quite pass the bar for acceptance. I recommend a rejection, but I look forward to seeing a strengthened version of this work in the future. I hope the feedback here has been useful to bringing about that stronger version.
Reviewers and myself agree that the contribution is clear, significant, and has enough originality. Hence, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Solid technical contribution, specially the use of continuous noise levels.   Clever application of diffusion/score matching models to a new domain and task, with conditioning.   Good empirical results, both objective and subjective.   Listening samples provided.  Cons:   Lack of formal comparison with flow based vocoders.   Potentially limited novelty.   No official code available.  Note: Readers may also be interested in concurrent work https://openreview.net/forum?id a xFK8Ymz5J ("DiffWave: A Versatile Diffusion Model for Audio Synthesis").
This paper considers the problem of searching over the joint space of hardware and neural architectures to trade off accuracy and latency.   Reviewers raised some valid questions about the following aspects: 1. Low technical novelty 2. Prior work on hardware and neural architecture co design, and closely related work are not addressed 3. Lacking details on hardware platform and discussion on physical constraints to determine invalid hardware designs (addressed somewhat, but the response is not satisfactory)  One additional comment: if we care about latency for a particular hardware platform, it is possible to automatically configure adaptive inference techniques to meet the latency constraints.   Overall, my assessment is that the paper requires more work before it is ready for publication.
This paper proposes to learn clinical prototypes via supervised contrastive learning to facilitate the reliable retrieval of clinical information and clustering in large datasets. The presentation of the paper could be substantially improved – e.g., the overview and motivation of the paper, the definition of clinical prototypes, selection of certain evaluation criteria, clarification of terminology in equations, the description of the motivations and settings of the experiments, etc.  In addition to the need to substantial improvement in clarity, major concerns include lack of comparison with more supervised approaches and discussion of relevant literatures raised by reviewers.  
The Authors study the learning dynamics of deep neural networks through the lenses of chaos theory.   The key weakness of the paper boils down to a lack of clarity and precision. Chaos theory seems to be mostly used to computing eigenvalues but is not used to derive meaningful insights about the learning dynamics. R2 noted, "Chaos theory provides a way of computing eigenvalues but does not give much understanding on the neural network optimization.". R4 noted, "The authors use an insight from chaos theory to derive an efficient method of estimating the largest and smallest eigenvalues of the loss Hessian wrt the weight". Hence, statements such as "the rigorous theory developed to study chaotic systems can be useful to understand SGD" seem unsubstantiated.  Reduced to its essence, the key contribution is (1) a method to compute the top and the smallest eigenvalue, (2) the observation that the spectral norm of the Hessian along SGD optimization trajectory is related to the inverse of the learning rate, and (3) a method to automatically tune the learning rate.  Let me discuss these three contributions:  * The significance of the first contribution is unclear, as pointed out by R2. Indeed there are other methods (e.g. power method, Lanczos) for computing these quantities that should achieve either a similar speed or similar stability. Given the rich history of developing estimators of these quantities, a much more detailed evaluation is warranted to substantiate this claim.   * The core insight that the top eigenvalue of the Hessian in SGD is related to the inverse of the learning rate in the training of deep neural networks is nontrivial but is not fully novel. Closely related observations were also shown in the literature. This precise statement however indeed was not stated in the literature. This contribution could be a basis for acceptance, but the paper is not sufficiently focused on it, and the evaluation of this claim is a bit narrow in scope.  * Finally, there is a range array of methods to tune the learning rate. As noted for example by R3, "There are numerous ideas for proposing new optimization and without careful, through comparison to baseline, well known methods", the evaluation is too limited to treat this as a core contribution.  Based on the above, I have to recommend the rejection of the paper. At the same time, I would like to thank the Authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work.
The authors explore modeling the relationship between domain slot pairs in multi domain dialogue state tracking via use of special tokens in pre trained contextualized word embeddings (i.e., one special token for each domain slot pair or special tokens for the domain and the slot that are merged). Beyond this, the basic architecture is very similar to the TRADE architecture (and papers that build on this general slot gate + slot value classifier) for the fixed vocabulary setting. Experiments are conducted on the MultiWOZ 2.1/2.2 datasets, demonstrating impressive improvements over recent results.    Pros   + They demonstrate that domain slot interdependencies can be modeled through special tokens for use with pre trained embeddings. + The top line empirical results are impressive.    Cons     Lack of a deep dive on the empirical analysis to show precisely why/where the proposed method is working better than existing work.   The methodological advance is minimal beyond using better pre trained embeddings.   Only one dataset when others exist and this is largely an empirical paper.   The writing is rushed and reads like a  late breaking  paper.  Evaluating along the specified dimensions: * Quality: The quality of the work was the primary concern of the reviewers. Specifically, this reads like a  late breaking  paper where the table of results is impressive, but there isn t significant examination of the empirical results showing why/when it works relative to competing methods. Focusing just on Tables 2 & 3, much of the improvement is ostensibly really due to the more powerful embeddings. Contextualizing this wrt {SimpleTOD, TRADE, DSTQA, Picklist}, this appears a minor methodological innovation centered around the input embeddings. The empirical results are impressive, but may very well be a result of the more powerful pre trained embeddings   additional empirical analysis and discussion might be able to convince the reader otherwise, but is lacking here. * Clarity: This is a very simple idea, so it should be easily understood by most familiar with the research area. That being said, the paper seems very rushed in general. * Originality: This applies ideas used in many NLP applications to the dialogue state tracking problem. As previously stated, the architecture is similar to several existing DST formulations   where the core idea is to model slot value interdependencies through the contextualized embeddings using special token. While not a trivial idea, it also is something that many could/would have put together. Until it is abundantly clear that this isn t really a study of how to apply larger pre trained embeddings to DST problems, it isn t clear that this is a significant dialogue systems advance beyond the strong performance. * Significance: As stated, this isn t a significant methodological advance. However, the empirical results appear very impressive   although the reviewers expressed some concerns regarding the evaluation. Since this is largely empirical, one of the reviewers pointed out that additional relevant datasets now exist, which would significantly strengthen the case.  In summary, the empirical results appear impressive, ostensibly setting the SoTA. However, there were several concerns regarding the novelty of the approach, if it is actually working better due to the reasons stated, sufficient analysis of the empirical results, amongst other things. Thus, despite the impressive results, the consensus evaluation was that this work is not ready for publication in its current form (even if the top line results should be disseminated). 
Techniques are introduced for improving representation learning capabilities of neural networks, and the result is interpreted in terms of random projections.  In further discussion, even the reviewer with the highest grade said that the paper does not yet have enough clarity to address the reviewers  comments. Particularly important would be to isolate the causal impact of the proposed components in the final result. But also several technical details would need to be clarified including comparing to simple l2 regularization and precise implications of Fig 1.  Positive aspects: The problem of learning representations and decorrelation is of course important. The authors have imagination, and the authors are encouraged to improve the ideas by taking the reviewer feedback into account. 
Dear Authors,  Thank you very much for your detailed feedback to the initial reviews and also for further answering additional questions raised by a reviewer. Your effort has been certainly contributed to clarifying some of the concerns raised by the reviewers and improving their understanding of this paper.  Overall, all the reviewers found a merit in this paper and thus I suggest its acceptance. However, as Reviewer #2 suggested, investigating the convergence in the stochastic case is very important. More discussion on this would be a valuable addition to the paper, which the authors can incorporate in the final version.
This paper presents a differentiable neural architecture search method for GNNs using Gumbel softmax based gating for fast search. It also introduces a transfer technique to search architectures on smaller graphs with similar properties as the target graph dataset. The paper further introduces a search space based on GNNs message aggregators, skip connections, and layer aggregators. Results are presented on several undirected graph datasets without edge features on both node and graph classification.  The reviewers mention that the results are promising, but they unanimously agree that the paper does not meet the bar for acceptance in its current form. I tend to agree with the reviewers in that the effect of the individual contributions (search space vs. method vs. transfer) needs to be better disentangled and studied independently, and that it is unclear why selecting a single aggregation function out of many is important vs. choosing multiple ones at the same time such as in PNA [1] as pointed out by R1. This should be carefully studied going forward. Lastly, all reviewers agreed that the proposed transfer method requires more detailed experimental validation and motivation.  [1] Corso et al.: Principal Neighbourhood Aggregation for Graph Nets (NeurIPS 2020)
The paper proposes an algorithm with sublinear regret for the problem of routing users through a network with unknown congestion functions over an infinite time horizon. The reviewers generally appreciated the main contribution of this work. One of the reviewers also felt that, although it may be possible to obtain the main result using more standard techniques, it is not clear whether doing so is an easy extension of the prior work. Following the discussion, all of the reviewers agreed that the paper missed important related work and it needs a major revision that incorporates the extensive feedback of Reviewer 2. For these reasons, I recommend reject.
The paper presents an interesting idea for making self attention efficient. Several reviewers were not satisfied with the experiments because it did not include runtime and sought after benchmarks.  Rebuttal did a good job of clarifying a few of those with newly added experiments that make the paper stronger. However, the new experiments are in limited settings as well as the real advantage over LSH baselines require more investigation.   This could make a good paper in the future if the experiments are made more rigorous with standard tasks and benchmarks. 
The paper presents an approach to multi agent coordination using goal driven exploration on subspaces of the observation space.  The results of the paper show that the authors  approach performs baselines on grid worlds and two tasks from the StartCraft Multi agent Challenge. While the rebuttal clarified many points raised by the reviewers, there was an agreement that the paper should be more convincing regarding the applicability of the approach. The reviewers were concerned with the scalability of the approach to larger environment, as well as the amount of hand crafting/domain knowledge required to apply the approach. Overall, while the paper contributes interesting results showing that such domain knowledge can help when properly leveraged, it feels like the approach needs be validated on more challenging environments before acceptance. 
 This paper presents approach to improve compute and memory efficiency by freezing layers and storing latent features. The approach is simple and provide efficiency. However, there are concerns as well. One big concern is that the experiments are not on realistic settings for example real world images and the current CNN is too simple. Overall, the reviewers are split. The AC agrees with some of the reviewers that for a paper like this experiments on more realistic setting will make it significantly stronger.  
This paper considers the problem of agents learning to autonomously navigate the web, specifically by focusing on filling out forms. The focus is on using adversarial environment generation to form a curriculum of training tasks. Thank you for the revisions to the manuscript, which have particularly improved readability. The presented problem is really interesting and seems an important real world problem for RL. Despite this, as the paper stands the results are not completely convincing. It seems like there is also scope to rigourously analyse the proposed method on other, better known domains to better quantify its limitations.
The authors present CLIME, a variant of LIME which samples from user defined subspaces specified by Boolean constraints. One motivation is to address the OOD sampling issue in regular LIME. They introduce a metric to quantify the severity of this issue and demonstrate empirically that CLIME helps to address it. In order to stay close to the data distribution, they use constraints based on Hamming distance to data points. They demonstrate that this approach helps to defend against the recent approach of Slack et al. 2020 to fool LIME explanations.  The paper is close to borderline, though concerns remain about experimental validation and the extent of novel contribution, since the original LIME framework is more flexible than described here and allows a custom distance function. Rev 1 believes that the original LIME framework is sufficient to handle Hamming distance constraints though sampling will be less efficient. To their credit, authors engaged in discussion but this should be further elaborated in a revised version.
The paper describes N Bref, a new tool for decompilation of stripped binaries. Compared to previous tools for neural based decompilation, this tool is based on two new ideas: a) to separate the generation of data declarations from the generation of the code itself, and b) the use of more sophisticated network architectures. These network architectures, however, all come from prior work, so the contribution in that regard is only their application to this particular problem.   The authors addressed many of complaints raised by reviewers, particularly with regards to presentation and explanations, but I think the most substantial concerns remain.   The most substantial concern is novelty. The technique is built on a combination of existing models, and its only original idea seems to be to treat the generation of data declarations and the code itself as separate tasks to be handled by independently trained networks.   In terms of results, the paper shows some quantitative improvements over prior work, although it is not so clear that those improvements matter. The quality improvement is measured in terms of AST differences, but it is not clear how often those AST differences translate into semantic differences. More importantly, the tool is restricted to un optimized binaries, which significantly limits its applicability for any real world application. Prior work by Katz et al. is evaluated against optimized binaries, as are other types of lifting such as the Helium project by Mendis et al [1]. Given the prevasiveness of optimization in deployed code, a tool that cannot handle it has virtually no applicability.  I think some significant technical novelty could make up for the lack of evaluation against optimized binaries. Alternatively, strong results on optimized binaries would justify publication even if the technique is built from existing building blocks, but as it stands, I think the paper is too incremental to merit acceptance.   [1]  Charith Mendis, Jeffrey Bosboom, Kevin Wu, Shoaib Kamil, Jonathan Ragan Kelley, Sylvain Paris, Qin Zhao, Saman P. Amarasinghe: Helium: lifting high performance stencil kernels from stripped x86 binaries to halide DSL code. PLDI 2015: 391 402 
This paper explores the brain s activity in response to language, specifically targeting the signatures of syntax in the brain.  The authors specifically investigate the signatures of specific syntactic elements against the "typical" effort based syntax measures from some previous work.    The title and abstract of the paper are clear and compelling, but the text of the paper muddies the message and this was expressed in the reviews.  There may be some debate in the literature as to if syntax and semantics are dissociable, and to what degree we can actually measure syntax in the brain, but I (and your reviewers) have trouble believing that any one actually thinks there is *no* syntax representations in the brain.  Certainly this is not a claim made by either the Federenko or Pylkkanen papers the authors cite. Federenko says "lexico semantic and syntactic processing are deeply inter connected and perhaps not separable" but doesn t claim that the brain doesn t "do" syntax. Pylkkanen says "Syntax in the brain is necessary to explain the fact that humans are exquisitely skilled at judging syntactic well formedness, even for sentences that have no coherent meaning."  I suggest this paper either rephrase the arguments, more clearly articulate the issues they wish to address, or find another venue where the reviewers might be more read to debate *if* syntax is encoded in the brain.  That seems outside of the scope of ICLR.
This paper reveals a novel interpretation of the well established CD for energy based model training as an adversarial game through conditional NCE. The paper could be potential impactful for the community of EBMs.  There are several points should be addressed in final version:  1, Based on such an interpretation, the number of steps becomes a tunable parameters, rather than in vanilla understaning in CD family (the larger, the better in terms of approximation, by with more computation cost).  2, It is okay to stop the gradient when solving an adversarial game as the paper discussed. However, propagating the gradient through the component is also another choice, which leads to the algorithm proposed in [1].  It will be interesting to discuss these in the paper.  [1] Sohl Dickstein, Jascha, Peter Battaglino, and Michael R. DeWeese. "Minimum probability flow learning." arXiv preprint arXiv:0906.4779 (2009).  
This paper proposes a new NAS methods that when doing architecture search, returns flat minima using based on a notion of distance defined for two cells (Eq. (2)). Authors then evaluation the effectiveness of the proposed methods against prior work on several benchmarks.  As authors have discussed in the paper, the idea of using flatness notion in architecture search is not new and has been first proposed by Zela et al 2020. This paper is building on Zela et al 2020 but the proposed algorithm is novel and different than Zela et al 2020. Even though the introduced algorithm is interesting, there are several concerns/areas of improvements:  1  The proposed method s performance is highly dependent to the notion of distance defined in eq. (2). However, the current choice is not well motived and does not seem like a well thought out choice. See for example the issue raised by R1. I think authors need to spend more time on this choice. One other option is to meta learn the vector representation of each operation.  2  All reviewers agree that the improvements marginal and in some cases not statistically significant. Authors have responded by arguing that this is typical for this area of research. I don t find this answer satisfying. For example, consider P DARTS (Chen et al., 2019). P DARTS improves over NA DARTS (the proposed method) on CIFAR 10 and ImageNet and on CIFAR 100 they are on par given the standard deviation of NA DARTS (see Tables 4 and 5). Moreover, the search cost of P DART is 0.27% of NA DARTS (Table 4). So P DARTS has clear advantage over NA DARTS.  Given the above issues, I recommend rejecting the paper. I hope authors would take feedbacks from the reviewing process into account to improve the paper and resubmit. 
All reviewers agree that this paper is not ready for publication. In addition to the technical comments, the authors should pay attention to the comments by Reviewer 3 about the naivete of the motivation provided for the work. Filter bubbles (to the extent that they really exist; there is controversy about this) have multifactorial origins. 
Summary:  The authors propose a Bayesian approach to data cleaning, implemented via a variational auto encoder. They argue that a common problem in this context are posteriors that overfit by concentrating on a low dimensional subset and introduce an optimization target intended to discourage that behavior.  Discussion:  Arguably the main concern brought up in the reviews was how much novelty there is in addressing latent variable posterior collapse, solutions for which have been proposed. The authors were able to clarify that this was due to a misunderstanding (the collapse they address is not in latent space), and the reviewer considers the matter resolved.   Recommendation:  I recommend publication. The reviewers are all positive, agree that the method is interesting, and seems novel. The writing is clear, and remaining doubts have been addressed in the discussion. 
This work addresses the problem of understanding how pre trained language models are encoding semantic information, such as WordNet structure. This is evaluated by recreating the structure of WordNet from embeddings. The study also shows evidence about the limitations of current pre trained language models, demonstrating that all of them have difficulties to encode specific concepts.  pros:   good idea to reveal how well the pre training models encode the underlying knowledge graph   detailed understanding on how language models incorporate semantic knowledge and where this knowledge might be located within the models   experiments show that models coming from the same family are strongly correlated   the paper shows how individual layers of the language models contribute to the underlying knowledge   analysis of the different semantic factors (9 different factors, including number of senses, graph depth etc.)    paper is clearly written and understandable and includes enough details to understand the implementation of the semantic probing classifier.   cons:   weakly connected goals, response from reviewers is string around 3 main topics, which is seen as many for a single scientific paper. It would be easier to focus only on one topic and make a clear conclusion,   single word concepts while CE models are powerful in context,   lack of a profound analysis of the experimental results       hard to understand which semantic category the pre trained methods work well or not well,       clarification about the improvement of the semantic learning abilities based on these results.  Several of the identified issues have been answered in the author s rebuttal, however, the paper would still need more work to be accepted. Note also that the bar a this year ICLR conference is high and we encourage the authors to submit their updated work again at the next conference.
Three reviewers recommended an acceptance (rating 7) while R1 deviated much from them (rating 3). After reading R1 s concerns carefully and the authors  rebuttal, I found some of the criticisms to be invalid. The authors provided a satisfactory response, addressing concerns and clarifying potential misunderstandings. Because R1 did not update the review after the rebuttal period, I am assuming the concerns have been adequately addressed. The three other reviewers all unanimously agreed that this paper tackles a timely topic, proposes a simple and effective approach, and shows convincing empirical results. I concur with the reviewers  recommendations.
This paper proposes an online meta learning algorithm. 3 out of 4 reviews were borderline. The main concern during the discussion was that it is unclear what kind of online learning this paper does. For instance, in theory, the online learner competes with the best solution in hindsight. This is a regret minimizing point of view. The other online learning is streaming. In this case, there is no regret. The goal is a sublinear representation that is competitive with some baseline that uses all space.  After the discussion, I read the paper to understand the points raised by the reviewers. I agree that this paper is not ready to be accepted. My quick review is below:  The authors combine MAML and BOL to have online updates (not all tasks are required beforehand) and handle distribution shift. But the way of combining these is not well justified. In particular,  1) The distribution shift story is not convincing. The reason is that the proposed algorithm is posterior based. By definition, when you use posteriors as in (3) (5), you assume that the datasets are sampled i.i.d. given \theta. This means independently and identically. So no distribution shift. I am familiar with Kalman filtering. For that, you need p(\theta_t | \theta_{t   1}) in (3) (5), which would be sufficient for tracking stochastic distribution shifts.  2) I find the use of BOL unnatural. Since MAML is gradient based, it would be more natural to have a gradient based online learner. Gradient descent has online guarantees and does not require i.i.d. assumptions.  3) The authors should clearly state what the objective of their online algorithm is. In particular, the informal justification of (3) (5) as doing something similar to MAML (the paragraph around (6)) is highly confusing. I could not understand what the authors mean.
The paper proposes an MLP based approach for data without known structure (such as tabular data). At first, the data are partitioned into K blocks in a differentiable way, then the standard MLP is applied to each block. The results are then aggregated recursively to produce the final output.   Pros: 1. Handling less structured data is surely an important problem in machine learning and is much less explored.  2. The paper is well written, easily understandable even with a fast browsing.  3. The experimental results show some improvement.   Cons: 1. The approach is somewhat trivial, and the framework could be improved, see, e.g. Reviewers #3&#4.  2. By the structure of the approach and the type of target data, a more reasonable comparison is with random forest (echoing Reviewer #1), which the authors added during rebuttal, rather than MLP etc. Maybe should even compare with deep random forest. Although the comparison with MLP etc. is quite favorable, the advantage over random forest is somewhat marginal (except on HAPT, which is a imagery data set and random forest may not be good at; also echoing Reviewers #1&#4 s comment on why using imagery data, which do not fit the theme of the paper). Reviewers #3&#4 also had some concerns with the experiments. Reviewer #4 confirmed in the confidential comment that the performance improvement is incremental.  Although the rebuttal seemed to be successful, thus both Reviewers #1 and #4 raised their scores, the average score is still at the borderline. Due to the limited acceptance rate, the area chair has to reject the paper.
Reviewers liked the self supervised learning of compressed videos, noting that it is an "exciting topic" and an "important problem", although they found the proposed methods (PMSP andCTP) less exciting. Reviewers were satisfied with the execution and the extensive experimental studies. AC felt the community may benefit from the paper s intuitive integration of self supervised learning and the compressed video s signals (I and P frames, residuals, motion vectors, etc). 
The reviews were a bit mixed, with a general consensus towards acceptance. The authors were one of the first to extend lookahead to minimax optimization, and demonstrated its potential through thorough experiments. The theoretical results were not as strong or at least not very well presented. Overall, the authors made interesting contributions and this work is of general interest to the ICLR audience. Please consider further polishing the draft according to the reviewers  comments. The AC would also like to draw the authors  attention to the following issues discovered in an independent assessment:  (a) As the reviewers mentioned, how lookahead minimax addresses rotational dynamics is not clearly presented. The current justification is a bit handwaving and speculative.   (b) Please consider rewriting Section 3. If there is some new results on the minimization problem, state the results in a theorem and include all assumptions clearly and precisely. This is also useful for other people to reference your result. As the authors themselves pointed out, this result falls quite short of explaining or motivation lookahead.   (c) Theorem 1, add e.g. in the citation before (Bertsekas, 1999). Theorem 2, in its current form, is quite weak in two aspects: (a) without checking its proof one can already see how to derive it in 1 line or 2. (b) if the base optimizer already converges, what is the point of having lookahead to converge as well? The potentially different convergence rate should be one s target here. It is certainly fine for the authors to not fully justify their proposed algorithm, as long as the authors (hopefully) are at least aware of the issues.  (d) Section 4 is a bit disappointing as one would have expected the authors to derive some qualitative results here (also raised by some reviewers).
The paper proposes an improved method for randomized smoothing, reducing computationally complexity compared with some previous works. The authors propose to learn score functions to denoise the randomized image prior to feeding it to a trained classification model. More specifically,  two image denoising algorithms based on score estimation are proposed to be applied regardless of noise level/type.   Strengths:   The paper shows strong quantitative results. The gap with white box smoothing is small on cifar, outperforming Salman et al. However according to the authors, the performance advantage could be mainly attributed to (1)  the use of better network architecture and (2) the multi scale training, not the major contribution of a score based denoiser.    The denoiser doesn t require access to the pre trained classifiers.   The proposed method only requires training of one score network to handle various types of noise type/level, although reviewers have raised concerns about motivation to having a method that only needs one denoiser for multiple noise levels    the computational bottleneck of randomized smoothing is the prediction time rather than training time and  using the same score function for multiple noise levels could be suboptimal.  Weaknesses:   There are some concerns about the significance of the contribution as well as novelty of the work, as the denoising + pre trained classifier architecture is already proposed. Specifically, the work can be seen as incremental to [1], although the work uses a score based image denoiser whereas [1] uses a CNN based image denoiser and this work is more efficient as it requires only one score network, while [1] trained multiple denoisers with respect to each noise levels.    Reviewers have expressed concerns on the prediction efficiency of score function based generative / denoising models.  The proposed method might exacerbate the weakness of randomized smoothing (i.e., slow prediction), especially in high dimensions.   The reviewers are curious to see the benefit of the proposed denoiser over the state of the art Gaussian denoisers (as used in [1]) under Gaussian noise setting.  Method seems to be effective for low resolution images only. The gap with white box increases on Imagenet.  [1]. Salman, Hadi, et al. "Denoised Smoothing: A Provable Defense for Pretrained Classifiers." Advances in Neural Information Processing Systems 33 (2020). 
This paper studies differentially private, communication efficient training methods for federated learning. While the problem studied in this paper is well motivated and interesting, the reviewers raised several concerns about the paper. Despite the authors  reconstruction protection explanation, the concern over large values of epsilon at the scale of 400 persists. There is not too much technical novelty since the main technique is given by prior work. 
A meta RL algorithms that aims to improve meta policy interpretability by reducing the meta gradient variance and bias estimation. The method is evaluated on an exploration in 2d navigation and meta RL benchmarks.  Despite an important topic of research, the reviewers are unanimous that the paper is an early version and requires further work to be suitable for publishing. Specifically, the future versions of the manuscript should address the novelty by better distinguishing from the prior work, improve the evaluations, presentation of the work. 
This paper received mixed reviews: two positives (6, 6) and two negatives (5, 3). However, the positive reviewers have very low confidence, do not show strong supports for this paper. The reviewers raised various concerns about this paper, and there still exist remaining critical issues although the authors made substantial efforts to answer the questions.  After reading the paper and all the comments by the reviewers, I decided to recommend rejecting this paper mainly due to its weak technical contribution and ignorance of privacy issues. Note that this opinion is shared with two negative reviewers. The proposed model and alternative training scheme are straightforward, and the novelty is not distinct. Also, the authors seem to assume that "the extracted feature vectors and corresponding gradients are not sensitive". This comment is given by R2 but has not been clarified. The proposed method is lacking in this aspect and it is hard to say that it is an FL approach.  
This paper studies gradient descent with weight decay and momentum for scale invariant networks. While this is an interesting research direction, the clarity of the paper suffers from poor writing. In its current form, I doubt this paper would have a large impact in the community. In addition, some reviewers pointed out that the analysis is not rigorous (some steps are missing, the authors re use some unjustified steps from previous papers, ...). The authors claim these steps are obvious, I certainly don t think that s the case and at the very least, one would expect that detailed and rigorous derivations in the appendix if the lack of space is the real issue in the main paper.  I think the feedback provided by the reviewers is valuable and I strongly encourage the authors to take advantage of this feedback to improve their work and submit to another venue.  
The paper studies Knowledge Distillation (KD) to better understand the reasons behind the performance gap between student and teacher models. The analysis is done by conducting exploratory experiments. The paper establishes that the distillation data used for training a student can play a critical role in the performance gap apart from the model capacity. Building on this idea, the authors propose a new approach to distillation, KD+, utilizing out of distribution data when training a student. Extensive experiments are performed to demonstrate the efficiency of KD+. Overall, the paper studies an interesting problem. The results provide a more in depth explanation of how the distillation data and model capacity play a role in the performance gap between student and teacher models in KD.  I want to thank the authors for providing the rebuttal and sharing their concerns about the quality of one of the reviews.  The reviewers appreciated the paper s ideas; however, all the reviewers were on the fence with borderline scores. In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should incorporate the reviewers  feedback to better position the work w.r.t. the existing literature and provide clear reasoning behind the gains for KD+ in experiments. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers  feedback when preparing future revisions of the paper.
The authors propose a novel and elegant way for learning parameterized aggregation functions and show that their approach can achieve good performance on several datasets (in many cases outperforming other state of the art methods). This is also appreciated by most of the reviewers. However, there have been several issues regarding the description of the proposed approach and the conducted experiments. These have been partly resolved in the rebuttal phase but should be more carefully assessed in another iteration of reviews.   More specifically: Experiments regarding learning of a single LAF versus multiple LAF should partly be included in the main paper (e.g. Figure 4 showing the performance for different numbers of LAFs). When constructing deep sets in this setting with a similar number of aggregation function it appears not very sensible to me to incorporate the same aggregation function multiple times but one would rather include a set of different fixed aggregation functions (these could be derived from the proposed LAFs). The experiments would also benefit from including set transformers as baselines (set transformers are discussed in the paper but not considered in the experiments as the authors argue that this is an orthogonal approach; while I agree that the goal of set transformers is different, I think there would be big value in understanding how these approaches compare and/or can be combined).  Beyond that I think 	a brief discussion of the related topic of learning pooling operations (e.g., in CNNs) is warranted.   Some reviewers also find that their concerns are only partially addressed in the rebuttal (e.g., regarding the extension from sets to vectors and applications in which the achieved performance differences are bigger).  One point which didn’t come up in the reviews but I would want to see addressed in a future version of the paper is an extended discussion of Figure 4. While there are cases were LAF clearly performs better, there are also cases, where Deep Sets outperform (this seem to be the cases in which the used aggregation units match the considered task). As LAFs can in theory represent these aggregation function it still seems challenging to learn the correct form of the aggregation function — I would appreciate deeper insights an analysis of this aspect. An immediate heuristic solution for many applications for improving performance thus might be to combine LAFs and standard aggregators.  In summary, the submitted paper has big potential but should be carefully revised and the experiments should be extended before the paper is accepted.
The novelty of the paper are: + introduces a new Hopfield network with continuous states, hence can be learned end to end differentiation and back propagation. + derives efficient update rules + reveals a connection between the update rules and transformers + illustrate how the network can be used as a layer in deep neural network that can perform different functions  The presentation was clear enough for the reviewers to understand and appreciate the novelty, although there were a few points of confusion. I would recommend the authors to address several suggestions that came up in the discussions including:   additional analysis to highlight when and how the networks is able to outperform other competing models   intuitions about the proofs for the theorems (okay to leave the detailed derivation in the appendix)   
This paper considers multi task RL from the perspective of an unsupervised clustering of different tasks with an EM like algorithm. The idea is evaluated on several simple and ATARI domains. We thank the reviewers for their detailed responses and revision. This work still seems a little preliminary in its current form. While the empirical results seem promising, it is generally felt that it would benefit from more extensive experiments, including further comparisons to other approaches and exploring the effects of the hyperparameters on tasks with much larger numbers of clusters. It would also be beneficial to provide some theoretical results, particularly with respect to negative transfer.
The reviewers and AC appreciate the improvements made to the paper and thank the authors for engaging with the reviewer questions. There are now quite a few neuro symbolic approaches, and they are all rather similar. This places a larger burden on the authors to have a thorough and systematic experimental comparison and related work discussion. Reviewers also believe the clarity of the paper should still be improved. The revised paper already made good progress in addressing these concerns, yet the reviewers still believe the paper would strongly benefit from another round of revisions.
The paper considers an extension of randomized smoothing where the smoothing noise may differ for different points. The resulting method shows good performance experimentally. However, the reviewers raised a number of problems which, at the moment, precludes the acceptance of the paper, such as the following:    The paper analyzes the transductive setting, where all the test points are available to fine tune the smoothing parameters of the predictor. It is not clear how this setting corresponds to a real adversarial threat model, and whether the final tuning needs to use the perturbed or unperturbed points. In the first case, the resulting certified radius is different from what is normally used in the literature, while in the latter it is not clear how the method would be useful to mitigate any real adversarial attack.   A related comment is that the paper should explain (and state) properly how the results of Cohen et al.  (2019) are applicable to compute the certified radius, which would also provide a proper explanation why partitioning is used.   The training cost of the procedure seems very high, and this is not discussed.   The clarity of the presentation should be improved.    
The paper has merits on providing a particular way of understanding a prediction model based on auxiliary data (concepts). I have a generally more positive view of it, aligned with the higher scoring reviews. However, I feel a bit uncomfortable of framing it as "causal" in the sense it does not aim to provide any causal predictions, but it is more of a smoothing method for capturing signal contaminated with "uninteresting" latent sources   this is more akin to regression with measurement error (see e.g. Carroll, Ruppert and Stefanski s "Nonlinear regression with measurement error") where, like in this paper, different definitions of "instrumental variables" also exist and are different from the causal inference definition. I can see though why we may want to provide a causal interpretation in order to justify particular assumptions, not unlike interesting lines of work from Scholkopf s take on causality. The paper can be strengthened by some further discussion on the assumptions made about additivity on equations (2) and (3), which feel strong and not particularly welcome in many applications.  The proposed title is still a bit clunky, I feel that the two stage approach is less important than the structural assumptions made, perhaps a title emphasizing the latter rather than the former would be more promising.
The paper considers the use of adversarial self supervised learning to render robust data representations for various tasks, in particular to integrate the Bootstrap Your Own Robust Latents (BYOL) with adversarial training, where a small amount of labeled data is available together with a sizable unlabeled dataset.  Especially the low data regime is of interest.  It extends a previous method with a new adversarial augmentation technique, it is compared against several methods, and the robust representations are shown to be useful more generally.  There were some confusing presentations and questions that were resolved in a detailed discussion with the reviewers.
This paper proposes to (re )examine VAEs with calibrated uncertainties for the likelihood, which is say VAEs in which the variance is learned rather than chosen as a fixed hyperparameter. The authors argue that doing so provides a reasonable means of automatically navigating the tradeoff between minimizing the distortion (the reconstruction loss) and the rate (the KL loss) in the variational objective. In particular, the authors propose to use a diagonal covariance  Σ   σ^2 Ι that is shared across pixels, and note that it is trivial to define  σ(z)   MSE(x, μ(z)) on a per image basis to minimize the reconstruction loss.   This is very much a borderline paper. Reviewers appreciate that the writing is clear, and acknowledge that revisiting the idea of learning calibrated is of interest to the community. At the same time, the reviewers note that the proposed approach has very limited technical novelty, and note problems with the experimental evaluation.   The metareviewer has read the paper, and is critical of the framing of this work. The manuscript in its current form does not do a sufficiently good job of discussing the large and detailed literature that exists on this topic. Learning calibrated decoders is by no means new, which this submission could and should acknowledge much more clearly. The two seminal papers on VAEs both considered learning calibrated decoders. Moreover there is a lack of thoughtful discussion of the reasons why learning a pixel wise σ(z) is not common practice. The authors note that this can lead to problems with training stability, but fail to note that this problem is mathematically ill posed; A well known property of VAEs is that high capacity models will memorize the training data, in the sense that the optimal learned marginal likelihood is equal to the empirical distribution over the training set (i.e. a mixture over delta peaks).   The metareviewer would expect to see a more thoughtful discussion of  the long line of work on navigating the trade off between rate and distortion, as well as the role of model capacity. A good place to start would be a more careful discussion of the autoencoding and autodecoding limits (Alemi et al 2018) and the GECO paper (Rezende et al 2018). More broadly, the metareviewer would expect some discussion of approaches that improve the quality of generation such as [1], and work that considers effect of model capacity on generalization, such as [2].    In terms of experimental evaluation, this paper also somewhat falls short. As R4 notes, some of the results look worryingly bad, which may be due to the fact that the authors train for only 10 epochs (as indicated in  Appendix B). Moreover, what is once again lacking in experiments is a systematic consideration of the role of model capacity. Some comparison to more recent baselines than the β VAE (e.g. GECO) would also be helpful here.   The metareviewer is sympathetic to the basic premise of this paper, which is the claim that learning a σ that is shared across pixels is a pretty good best practice in terms of finding a reasonable balance between rate and distortion. There is certainly room for a paper that communicates this idea. However, such a paper should (a) more explicitly position itself as revisiting this idea rather than introducing this idea, (b) include a more thoughtful discussion of related work, and (c) include a more robust empirical evaluation.   [1] Engel, J., Hoffman, M. & Roberts, A. Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models. arXiv:1711.05772 [cs, stat] (2017).  [2] Shu, R., Bui, H. H., Zhao, S., Kochenderfer, M. J. & Ermon, S. Amortized inference regularization. in Proceedings of the 32nd International Conference on Neural Information Processing Systems 4398–4407 (Curran Associates Inc., 2018).
All four knowledgeable referees have indicated reject mainly because the novelty is limited   they thought (and I also agreed) that  it would be difficult to argue the novelty of the proposed framework simply by considering the more recent compressed network training technique, as the reviewer mentioned through rebuttal. In addition, there were concerns about various terms and basics specialized for hardware that are not kindly explained for more diverse audiences in the machine learning field. It improved a little through revision, but I think it needs a more kind explanation. It seems that more thorough experimental verifications  are needed.
This paper received high variance in the reviews.  I personally agree with AnonReviewer4 that the theoretical results presented in this paper are well known results on the sensitivity analysis of linear programs. See for instance "Introduction to linear optimization" by Bertsimas and Tsitsiklis, Chapter 5.  More generally, these results are a special case of Danskin s theorem and the envelope theorem: https://en.wikipedia.org/wiki/Danskin%27s_theorem https://en.wikipedia.org/wiki/Envelope_theorem  Clarke s generalized gradients are just subgradients in the case of convex functions, which is the case here.  My recommentation to the authors if they want to publish their work is to focus on the applications and to stop claiming novelty on the theoretical side.
The paper questions the use of cross entropy loss for classification tasks and shows that using squared error loss can work just as well for deep neural networks. The authors conduct extensive experiments across ASR, NLP, and CV tasks. Comparing cross entropy to squared error loss is certainly not novel, but the conclusions of the paper, backed by a lot of experimental evidence, are certainly thought provoking.   I would have liked to see a bit more analysis into the results of the paper, and perhaps a bit more theoretical justification. That said, the paper will be of interest to the community, given the ubiquity of classification tasks. 
This is a thought provoking paper which describes a significant problem that plausibly occurs in deployed ML/RL models. The paper is clearly written, describing claims using examples and developing small unit tests to probe models. However, as the reviews and discussion show,  the exposition should be substantially re worked so that the core contributions are more understandable   the core message in the revised manuscript is still very nuanced and easy to mis understand.  Let s say we train an ML model using supervised learning to minimize a loss function on a dataset. Several models may have near optimal loss as measured on a validation set   a learning algorithm is free to return any one of them. Now in a deployed system, these ML models are not merely generating passive predictions; these predictions are driving system operation and potentially influencing future states/contexts/inputs that the model will be invoked on. It is well known that supervised learning makes an iid assumption between training and deployment which is violated in this setting   that is not the main point of this paper. Consider again the set of models with near optimal loss. Some of them, when deployed, may cause the distribution mismatch between training and deployment to be miniscule, while other models may introduce a vast mismatch. We may choose a learning algorithm which just so happens to pick models from the former category; and we may conclude that feedback effects induced by the ML model are not substantial. We then change some unrelated detail in the learning algorithm (but not the objective, datasets, validation criteria, etc.) which just so happens to pick models from the latter category and suddenly witness a large distribution shift. What happened? And could we have developed tests to detect that our learning algorithms have these tendencies? The paper attempts to articulate such questions, and design the first step in answering them.  Moving to RL, where we routinely consider distribution shifts in states visited by different policies, does not fundamentally fix all these issues because the reward function is typically an engineered proxy to elicit desired behavior   and we may again find that some RL algorithms have a tendency to find reward maximizing policies that exploit gaps in reward specification as opposed to following intended behavior.  The core question studied in this paper, scoped to the supervised learning setting, is very related to that of strategic classification (see e.g., https://arxiv.org/pdf/1910.10362.pdf Strategic Classification is Causal Modeling in Disguise). The following sketch is inspired by that literature.  We might hope to augment the training objective of ML/myopic RL/strategic RL to address the Auto induced distribution shift problem as follows. [Supervised learning for content recommendation] Let the training/validation data distribution be D. Assume for now that there is no exogenous factor in the environment that causes any distribution shifts in deployment   so, the only shift is due to feedback effects from the predictions made by the model. For an ML model f, let the corresponding recommendation policy be pi_f, and let the long term distribution of data seen from user interactions with pi_f be D[pi_f]. Then, what we want is: f*   argmin_F Expectation over D [ L(f) ] subject to constraint that D ~  D[pi_f].  For a contextual bandit/myopic RL formulation of the problem, we could similarly constrain the learning problem as pi*   argmax_Pi Expectation over D [ Reward of pi] subject to constraint that D \approx D[pi]. Essentially, both supervised ML and contextual bandit algorithms are assuming that context distribution is unchanged   so let us enforce that the context distribution is indeed unchanged as a consequence of the policy s actions. It is unclear how to generalize this kind of thinking to situations where environmental changes also contribute to distribution shift.  The authors call out precisely this flaw using the cryptic comment    not trying to change X  is not the same as  trying to not change X . The formulation above does  trying to not change X , but that is an insufficient band aid in situations when environment changes X. It s also unclear how one might estimate D[pi] or D[pi_f] and appropriately constrain the learning algorithm   but these are all interesting questions to study.  The paper in its current form is asking an important question. In supervised learning, the desired solution might actually coincide with strategic classification solution concepts. The paper may be asking a generalization of the phenomenon for myopic RL and RL. It may spark interesting discussions and follow up work, but is not yet mature beyond a workshop poster. Generalizing the unit tests, articulating the scope of situations where context swapping may be a useful strategy, and even formalizing the problem and desired goal (as attempted above for the content recommendation example) will substantially strengthen the paper.
The reviewers were unanimous that this submission is not ready for publication at ICLR. Concerns were raised about clarity of the exposition, as well as lack of sufficient experiments comparing to related work.
This paper proposes a method for modeling higher order interactions in Poisson processes. Unfortunately, the reviewers do not feel that the paper, in its current state, meets the bar for ICLR. In particular, reviewers found the descriptions unclear and the justifications lacking. While the responses did aid the reviewers understanding, the paper would benefit from rewriting and more careful thought given to the experimental design.
Paper proposes an approach for scene autoregressive layout generation. Four expert reviewers evaluated the paper outlining the following pros/cons of the work.   > Pros:   Good performance across different domain [R1,R2,R3,R4]   Formulation is general [R1,R2]   Clever separation of different attributes [R1]   The idea of using transformers is interesting [R4]  > Cons:   Missing related works [R3]   Unclear comparison with baselines that [R2]   Lacks of  hyper parameter tuning on the baselines [R2]   The quantitative results do not outperform the state of the art models consistently across all metric [R4]  Authors have addressed some of the concerns in the rebuttal and generally reviewers are more convinced after the rebuttal than before. The fairness of comparison to baselines remains an issue for two of the reviewers, and quality of results for one. AC acknowledges and agrees with these concerns. As such, given the large number of highly qualified submissions to ICLR and in comparison to those submissions, the paper fell slightly bellow the acceptance threshold.   That said, AC believes the approach, overall, is interesting and warrants re submission after the appropriate revisions are implemented. 
This paper was near the borderline, but ultimately, calibrating with the acceptance criteria applied to submissions across the conference, we didn t find sufficient enthusiasm among the reviewers to accept the paper.  Two reviewers put it just above the bar for acceptance, on the strength of its results.  A third reviewer finds the results to be a small improvement over other work, and finds the definitions of class, content, and style used by the authors to be confusing.  The AC agrees with the 3rd reviewer that it is more natural to define (for the class of faces) the identity to be the content and the facial pose to the the style.    Unfortunately, acceptance to ICLR required a stronger case than the reviewers presented for this paper. The remaining concerns which swayed the AC s opinion included:   concern that this was an incremental extension of LORD   the reliance on the nature of transformations applied in the algorithm   lack of any enthusiastic reviewer championing acceptance for the paper.  
This paper takes a step towards understanding the role of nonlinear function approximation  more specifically, function approximation via (two layer) neural nets in some variants of the policy gradient algorithms. The authors borrow the mean field analysis idea recently popularized in studying shallow neural nets, and investigate the mean field limits of the training dynamics in the current RL settings. The results and analyses are interesting as they nicely complement another line of linearization based analyses (i.e., the one based on neural tangent kernels) towards understanding non linear function approximation. As suggested by a reviewer, it would be nice to add discussions in the revised paper regarding when the dynamics can be guaranteed to converge to a stationary point.  
The reviewers agree that the contributions may not be relevant to the ML research community or perhaps are a poor fit for the venue, but otherwise find the work potentially useful and addressing a timely topic. Because the paper focuses on a simulation environment for existing epidemiological models, reviewers comment that the technical and methodological novelty is limited.
The initial reviews were mixed for this paper. On one hand, some of the reviewers highlighted that the proposed datasets could be useful to researchers. On the other, reviewers found a few important flaws with the current manuscript including missing baselines, issues with the proposed tasks, and possibly inaccurate/imprecise statements.  Our discussion after the author s response focussed on whether the positives aspects of the current paper outweighed some of the perceived weaknesses of the paper. In particular, while some of the initial criticisms from the reviewers were successfully addressed by the authors (including possible imprecisions and to a certain extent motivation), all the reviewers remained convinced that standard continual learning baselines could be adapted to this setting. They also conjectured that these missing baselines might not allow readers to appreciate the strength of the proposed datasets.   In their response, the authors argued that adapting models would require research. The reviewers are under the impression that it would be useful to test baselines more or less "as is" even if the authors do not think these baselines will be competitive. For example, in the discussion, a reviewer suggested that "an experience replay baseline could [...] have been implemented" where the replay buffer includes the hidden states of an LSTM. It might also be useful to study baselines that do not strictly obey the proposed setting, again to get a better understanding of the proposed tasks (including how difficult it is).  Overall, having some of these baselines would be one way to better connect the proposed work to the current continual learning literature. 
This paper studies an interesting problem: the landscape of neural networks. I agree with the authors  comment that this work improves our understanding of one aspect of neural networks, and I do find the result of this paper is of interest to some extent. Reviewer 5 pointed out the technique used in the paper is interesting, and Reviewer 3 has shown interest in the techniques (and indicated the possibility of increasing the score). Nevertheless, a few reviewers questioned the requirement of the large width; I do not think having a large width itself is necessarily an issue (even in the presence of convergence results on NTK), but it is necessary to clearly explain the context and the relation/differences with closely related works in the literature. In the current form, the paper probably has not reached the bar of acceptance, thus I recommend reject. 
While the paper has merits, the experiments are lacking in important respects: I agree with Reviewer 1 that it is a serious problem that the approach is not evaluated on truly low resource languages   since a significant pivot to target language bias is to be expected (as also suggested by Reviewer 2). I also agree with the sentiment that the work is not properly baselined, without considering alternative ways of using the pivot language development data. I also agree with Reviewer 3 that the 1:1 assumption is limiting, given that multi source transfer has been de facto standard since 2011 (see, e.g., work by McDonald, Søgaard, Cohen, etc.). I’m also a little worried about using dev data for unlabelled data, since this is data from the exact same sample as the test data. In practice, dev data will be biased, and artificially removing this bias will lead to overly optimistic results. 
Three reviewers have reviewed this manuscript, and they had severe reservations regarding the presentation quality and the lack of sufficient theoretical support behind empirical observations. Even after rebuttal, the reviewers maintained that the above issues are not fully resolved. Unfortunately, this paper cannot be accepted in its current form.
Nominally, the scores on this paper were pretty split.  In reality, I concur with the 2 and the 3.  The 6 acknowledges being unfamiliar w/ the GAN literature, and I think the 7 is being too permissive about the baselines.   The empirical evaluation here is simply not up to par for a major machine learning conference.  As reviewers have mentioned, the baselines are out of date, and even then the improvements are marginal.  It s totally fine to have a marginal improvement if the proposed technique is very new and interesting and the baselines  are taken seriously, but unfortunately I don t believe that s the case here. Thus, I recommend rejection.
This paper is solid. It is correct, the text and author response demonstrate good knowledge of the area, the results are significant and solid, the experiments are strengthened by many independent runs (refreshing to see), the ablation study is well done, and the proposed distributed hyper parameter and NAS alg is simple and practical. The paper is well written and reasonably polished.  The main drawback of the work in the eyes of the reviewers is that the paper is well described as a combination of existing ideas and a significant engineering effort with good but not stellar results. The reviewers found they did not gain any substantial technical insights from the work. As a result no reviewer was willing to champion the paper. However, the discussion, reviews, and author response made it clear that (1) the paper is enjoyable to read and informative, (2) the method is actually useful and performant, and (3) the combination of implementation details and methods is worth documenting. In balance, the paper is just below the bar. The program was extremely competitive this year.   
The paper provides a reformulation of the distributionally robust optimization problem into a (difficult in general) transportation map problem. In the new reformulation, the authors provide strong convergence results albeit requiring strong conditions, such as solvability of the auxiliary problems in reasonable time or complexity per iteration. At this point, it is very difficult to accept the paper   given the issues that reviewers pointed. In particular, authors  method which requires an expensive MCMC implementation is not scalable. Based on the run times reported in Table 2, the proposed method is 10x slower than their baseline (Volpi et al 2018) achieving performance within their statistical error.
There is a consensus among the reviewers that the work is interesting and the paper should be accepted.  Nevertheless, several reviewers struggled with understanding the details. While the authors  (largely successfully) addressed these concerns, I believe that the paper is still too dense and hard to follow, I would encourage the authors to invest more time into improving its readability.  One important point which came late in the discussion is the provenance of baseline scores in the result tables (see the review by AnonReviewer3, the current manuscript claims that the numbers are taken from the original papers while in some cases, the numbers cannot be located in these papers). Unfortunately, the authors did not have a chance to respond to this criticism, and fortunately we could trace the key numbers and establish that the results are strong enough to warrant accepting the submission. Still, we would ask the reviewers to fix this issue in the final version.
Reviewers agreed that overall the two pronged message of the submission has utility.  1. That ObjectNet is continues to be difficult for models to understand and is a challenging test platform even when objects are isolated from their backgrounds. This is significant and not obvious. Cropping objects makes the distribution shift between ObjectNet and ImageNet far smaller, but the large remaining performance gap points to the fact that detectors are limited by their ability to recognize the foregrounds of objects not by their ability to isolate objects from their backgrounds.  2. That segmentation could be a promising direction for robustness to adversarial perturbations which has so far been overlooked.
RCRL is return based contrastive learning for reinforcement learning, where the label is whether two samples belong to the same return bin. The reviewers found this to be a well executed paper with good theoretical and experimental results.
This work proposes a modification of a GNN architecture by feeding random node features to bootstrap the message propagation. This enables the discriminability of automorphic node pairs with a lightweight, simple change. Experiments are reported showing improvements over baselines.  Reviewers had mixed impressions of this work. On one hand, they found the proposed model principled and with strong empirical performance. On the other hand, they perceived a general lack of novelty and a somewhat misleading theoretical analysis. After careful review, the AC ultimately believes that this work does require an extra iteration that further solidifies the contributions and aligns the theoretical analysis with the empirical performance. In particular, the use of random initialization is folklore in the GNN literature, especially with regards to spectral methods (e.g. power iterations are typically initialized using a random vector, and these constitute the simplest forms of linear GNNs). The authors are encouraged to address these comparisons with further detail, as well as the excellent feedback given by the reviewers. 
This paper deals with domain generalization with causal modeling. Specifically, it considers a broader class of distribution shifts, arising from the system intervention perspective, and proposes some robust learning principle to achieve domain generalization. The paper is well written and has some interesting ideas. However, as pointed by Reviewers #1 and #4, the exact problem setting should be made more explicit, the theory and algorithm should be more consistent, and some very relevant contributions in the literature should be discussed or compared with. 
The paper presents new contrastive based self supervised objective based on Chi squared divergence that helps with mini batch sensitivity, training stability and improved downstream performance. An accept.
This paper has been evaluated by four reviewers who overall hesitated between borderline reject/accept. In general, as Rev. 4 points out, this paper appears to cope with over oscillation rather than over smoothing aspect of GCN modeling (something worth clarifying). Rev. 3 also rightly points out that the connection between the heat kernel and GCN in fact was established in previous works. Also, the connection between SGC (polynomial filter) and  the heat diffusion (the spectral filter matrix) is hard to overlook (the impression that this work builds heavily on SGC). Therefore, while AC sympathizes with the idea, it is also difficult to overlook the incremental nature of the paper and therefore the paper cannot be accepted in its current form.
This paper presents a model for video action recognition.  The reviewers appreciated the development of a novel dynamic fusion method that examines channels from feature maps for use in temporal modeling.  After reading the authors  responses, the reviewers converged on an accept rating.  The solid empirical results and analysis, the fact that is is a plug in method that could be used in other models, and the clear exposition were deemed to be positives.  As such, this paper is accepted to ICLR 2021.
This work considers the problem of calibrating a multi class classifier while preserving differential privacy. It proposes a method Accuracy Temperature Scaling, that aims to achieve consistency rather than calibration. The method is particularly easy to implement under the constraint of DP. The paper then evaluates  the calibration algorithm in the context of domain perturbation/shift and, as the authors demonstrate it outperforms adaptations of other technques to DP.  The strong sides of this work are  * the first work to study calibration in this setting (albeit that is also a result of the setting being of a relatively narrow interest) * proposes a new algorithm * evaluation on multiple benchmarks  The weaknesses * The method is not justified either by theoretical analysis or clear intuition * Evaluation of performance in the context of domain shift makes the the presentation somewhat confusing and experiments much more involved but is largely orthogonal to the problem of calibration  Overall the work has merits but also significant issues.
The reviewer seems to reach a consensus that the paper is not ready for publication at ICLR. One of the major issues seems to be that the paper only analyzes the case of $d 2$. (In the AC s opinion,  $d>2$ might be fundamentally more difficult to analyze than $d 2$).   
This paper considers the problem of hardware and software co design for neural accelerators. Specifically, it looks at hardware and the software compiler that maps DNN to hardware. It employs Bayesian Optimization (BO) to perform joint search over hardware and software design parameters in an alternating manner. To handle black box constraints that cannot be evaluated without performing simulations, the method uses constrained BO algorithms.   The paper talks about two technical challenges: 1) Black box constraints. There is a lot of literature on constrained BO. 2) Semi discrete design variables. The paper didn t propose any generic solution. There are some recent papers to handle mixed variables that may be useful. https://arxiv.org/abs/1907.01329 https://arxiv.org/abs/1906.08878  BO methodology is justified. There is recent work on hardware and software co design for neural accelerators and should be taken into account for both qualitative and quantitative comparison.   Overall, my assessment is that the paper in its current form lacks technical novelty for acceptance.
This submission is an interesting case...  The method it presents appears to work quite well, achieving state of the art quantitative reconstruction results (though qualitatively, the reconstructed surfaces are locally noisy).  The method is quite complex, which different reviewers saw as either a strength or a weakness ("a mix of SoA techniques creatively woven together in a fairly sophisticated model" vs. "bulky and ad hoc").  Most critically: it appears that the reasons for the method s significant (14%) improvement over the prior art for this problem (Pixel2Mesh++) are not due to the novel contributions that the paper focuses on (multi headed attention, contrastive depth loss). Rather, it is other system design choices that are not novel research contributions that make up all but 1% of this difference (primarily, using a voxel grid predictor to get the initial mesh, as opposed to an initial ellipsoid mesh).  It might be possible for the authors to write a systems paper supporting these design decisions and showing how they lead to better results. However, this is not the paper the authors have written (the majority of the technical detail in the paper is focused on method components that make minimal impact). I would also argue that this hypothetical paper would not necessarily be appropriate for ICLR, since it does not focus on any new representations. It would be better suited to a venue such as CVPR, ICCV, or 3DV.  p.s. Reviewer 5 deserves all of the credit for noticing this major issue with the paper.
This paper introduces two new quantum neural networks with specific structures: TT QNNs and SC QNNs. The main contribution of this work is to show a theoretical lower bound that the gradient of the two neural networks (at random initialization) with respect to certain training objectives is well lower bounded by 2^{ 2 L}, where L is the number of layers in the network. Previously, the known work only manage to prove this lower bound with less realistic QNNs with 2 design, or prove an 2^{ poly(n)} lower bounds for random QNNs, where the input of the neural network is an n qubit. This paper makes a first step towards solving the vanishing gradient problem of QNNs at random initialization.     The major concern of the paper is the usefulness of these QNNs with proposed architectures: The proposed QNNs might be theoretically easier to train, but what if they can only learn a significantly smaller class of functions? In classical world, such phenomenons are very common: Linear classifiers (or even linear functions over prescribed feature mappings) are much easier to train and have much better theoretical properties, but they fail short in terms of representation power comparing to real neural networks.    In this paper, on the theory side, there is no argument about the representation power of these QNNs: It is unclear which set of functions they can represent efficiently, which limits their theoretical interests to machine learning committee. On the empirical side, the reviewers all agree that the empirical results are weak at this point: The proposed new QNNs did not show significant advantages over random QNNs (especially with early stopping), and other types of QNNs were not compared. Moreover, there seems to be some efficiency issue regarding implementing these QNNs   More convincing empirical evidence or theoretical evidence about the power of these QNNs need to be addressed.  
This paper focuses on two new characteristics of adversarial examples from the channel wise activation perspective, namely the activation magnitudes and the activated channels. The philosophy behind sounds quite interesting to me, namely, suppressing redundant activations from being activated by adversarial perturbations. This philosophy leads to a novel algorithm design I have never seen, i.e., Channel wise Activation Suppressing (CAS) training strategy.  The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all comments in the final version.
In this paper, the authors propose a model for integrating news representations for stock predictions. While the research direction has good value in real applications, it seems that this particular paper has not done a sufficiently good job in pushing the frontier of this direction. The reviewers have raised quite a few concerns, for example: 1)	Paper writing needs significant improvement (e.g., confusion regarding future news). 2)	Limited technical novelty as compared to previous works 3)	Benchmark datasets are out of date, baselines are a little weak and not well explained, more evaluation measures (Such as Sharpe value) are needed 4)	Marginal improvements over the baselines  The authors have not submitted their rebuttals. Therefore the concerns are still there and we do not think the paper is ready for publication at ICLR. 
The paper received negative and borderline reviews. The reviewers have raised several concerns about the novelty of the approach and the lack of convincing experiments. The rebuttal only partially addresses these concerns. Overall, the area chair agrees with the reviewer s assessment and follows their recommendation.
The paper shows that hat if the goal is to find invariant mechanisms in the data, these can be identified by finding explanations (e.g. model parameters) that are hard to vary across examples. To find those "explanations" it then proposes to combine gradients across examples in a "logical AND" fashion, i.e., pooling gradients sing a geometric mean with a logical AND masking. All reviewers agree that the direction is very interesting. While indeed mentioning sum and products of experts might be good, the overall idea is still very much interesting, also to the ICRL community, since it paves the way to apply this to larger set of machine learning methods, as actually shown in the experimental evaluation. Still, the authors should make the link to causality more obvious from the very beginning. This should also involve clarifying that "explanations" here do not refer to "explanations" as used in Explainable AI. Overall, this is an interesting and simple (in a positive sense) contribution to the question of getting at least "more" causal models. 
The authors propose to linearly combine the utility functions of (batch) active learning algorithms. The linear combination coefficients are "learned" with Monte Carlo estimators to adapt the coefficients to different kinds of tasks automatically.  The reviewers find the presentation within the papers generally clear. The simplicity of the approach, which is highlighted in the authors  rebuttal, should be appreciated. The authors also addressed the issue of robustness with respect to the batch size. But the paper left quite a few unanswered issues even after the authors rebuttal. The novelty with respect to several earlier papers require clarification and concrete comparisons, such as the ones in reinforcement learning and bandit learning as pointed out by reviewers. The lack of comparisons to those existing works, both illustratively and empirically, is a key weakness of the current paper. A more careful study of RL setting (such as reward shaping) is also important to understand the value of the work. Finally, the gap between the ensemble approach and the single approach also deserves more investigation to justify the significance of the contribution. 
This paper provides some theoretical perspective on the use of data augmentation in consistency regularization based semi supervised learning. The framework used in the paper argues that high quality data augmentation should move along the data manifold. This generic view allows the paper s ideas to be applied across datasets (as opposed to image specific data augmentation used in state of the art semi supervised learning algorithms). I am not aware of any other work raising these points, and indeed this paper is significant in that it provides a new and potentially useful perspective on the most performative semi supervised learning approach. Reviewers agreed that the paper was clear and useful. The main concern was that the paper only included experiments in toy settings. Indeed, it would have been much more impactful to apply these ideas to state of the art semi supervised learning methods, but I think it can be excused given the theoretical focus of the work.
Reviewers found the construction is very clever and the empirical results are interesting. However, a more thorough theoretical explanation is needed for acceptance. 
This paper addresses stochastic semantic segmentation with a two step approach: a standard segmentation network learned with cross entropy serves as a guide to calibrate a second refinement network to generate diverse predictions while their expectation matches the calibration model.   The reviewers acknowledge the paper merits , e.g. the decoupling between the segmentation and generation networks. However, they also highlight serious concerns on the the clarity of the presentation, and the need for a consolidated evaluation.   The AC carefully reads the paper and the discussion among authors and reviewers. Despite improvements in paper presentation, the AC still considers that the paper would benefit from clarifications, e.g. the fact that the paper does not address calibration, and that stronger baselines as those mentioned by reviewers are needed for fully validating the approach.   Therefore, the AC recommends rejection. 
The paper considers ways to understand label smoothing methods, which are widely used in many applications.  There is some theory on the performance of SGD with and without the methods of the paper, but there is s significant gap in terms of how the theory offers insight into label smoothing.  There are some empirical results, but they are insufficient and there is not much description of the experimental setup.  There was a diversity of reviews.  But, after a discussion among reviewers, it was felt that, overall, another iteration on improving the coherence and presentation of the paper will make it much better for the community. 
The paper study under which condition a classifier can respect the condition of equalized odds. The reviewers find the paper interesting but they also raise some important concerns about it.  First, multiple reviewers pointed out that the results are not particularly novel or surprising and, even after discussing the rebuttal, they consider the result a bit incremental.  Second, the motivation of the paper are also questioned by multiple reviewers that suggested to study the tradeoff between trade off between EO fairness and accuracy.  Overall, the paper contains some interesting ideas but it is below the high acceptance bar of ICLR.
This paper proposes practical improvements to theoretically well founded QTRAN, which is a state of the art technique of cooperative multi agent reinforcement learning.  The improvements include new designs of loss function and action value estimator, which might be widely applicable beyond QTRAN.  However, it is not obvious if the proposed improvements actually improves the performance of QTRAN, and experimental evaluation is essential to this work.  After the discussion, there remain some major concerns about the experimental results.  In particular, the performance of baselines in the experiments is not consistent with those reported in the prior work.
This is a tricky one, hence my low confidence rating.  The reviewers seem to agree that the paper is well written, easy to follow, and that it tests a relevant hypothesis that is of interest to the community. There was some disagreement as to whether the experiments are comprehensive, complete and/or conclusive enough, although on balance it seems reviewers were overall satisfied barring a few additional requests which the authors addressed in their feedback.  However, no reviewers support the paper strongly (borderline accepts) while R5 remains unconvinced and has raised a technical point in their review about the estimator of the Trace of the Fisher information matrix. The question R5 has raised is central to the paper s methods, arguments and conclusions. In a message to ACs and PCs the authors raised concerns about R5. I personally thought that while R5 could have worded their review more carefully and respectfully (as I pointed out in my respose) the concerns raised were otherwise motivated, the reviewer engaged in a discussion, and the arguments were laid out clearly. I side with R5 and I think that the paper should be rewritten with more clarity on this question   the problem R5 found is likely to trip up others who read or build on the paper.  The authors have raised that there are two parallel submissions closely related to this one, complicating the decision making somewhat: [1] https://openreview.net/forum?id rq_Qr0c1Hyo [2] https://openreview.net/forum?id 3q5IqUrkcF 
The paper presents a mathematical framework for encoding and decoding continuous valued signals from spiking neurons, proving both mathematical theory and simulation results. Two reviewers had serious concerns about the mathematical correctness and exactness of some of the presented mathematical results, and the AC raised questions about relationship to (un cited) prior work. The authors aimed to address these shortcomings in the discussion phase, but neither me nor the reviewers were convinced that it provides a correct, and substantial, advance over prior approaches. I do hope that the feedback from the process will useful to the reviewers, and will help them in clarifying their contributions.
This paper presents a model for dynamical systems with multiple interacting components. Each component is modeled as an RNN, and the interactions between components are functions of their distance in a learned embedding space. It s an interesting idea and well motivated inductive bias. The results were made more compelling with the addition of "ablation" studies during the discussion phase, which showed how various aspects of the model combined to yield the best performance.  Overall, this paper should be of interest to many in the ICLR community working on complex, multi agent systems.
This paper focuses on a segmentation of cell imagery (as opposed to the more commonly studied domain of "natural images"). Among its contributions are a novel metric for evaluation of results and a novel dataset. These are acknowledged by the reviewers as strengths. Multiple issues raised in the initial reviews were addressed in the revision (the reviewers agree on this and most of them raised their scores). On the other hand, the concerns remaining have to do with significance and impact. The final evaluation ratings are split, with only a single score clearly in favor of acceptance.   I tend to agree that the contributions, while without a doubt valuable, make this less of a fit to ICLR than to a more specialized venue focusing on biomedical data. 
This was a borderline paper with a split recommendation from the reviewers.  The authors took great care to answer the reviewer questions in detail, and the clarity and precision of the technical exposition was strengthened.  However, substantial technical content was added to the paper during the rebuttal process, which the reviewers were not able to fully and properly assess.  Overall, this is worthwhile research, but the paper is still maturing.  The contribution was perceived as incremental in light of previous work using LTL and FSAs in RL, despite the authors extensively re explaining the significance of the work in the rebuttal.  A resubmission is more likely to resonate with reviewers and ultimately achieve higher impact.  For completeness, it would help to also briefly acknowledge and compare to hierarchical RL work that also seeks to capture composable subtask structures, such as:  Sohn et al. "Hierarchical reinforcement learning for zero shot generalization with subtask dependencies", NeurIPS 2018  Sohn et al. "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies", ICLR 2020
We thank the authors for their submission. The paper feels more like an early draft, with several fundamental factual mistakes (mistake on computational and statistical complexities) as highlighted by the reviewers. There s plenty of material in the reviews to help authors improve their submission, we encourage them to use these recommendations to improve motivation / experiments.
The paper proposes a two stage approach for anomaly detection   first train a low dimensional embedding potentially using self supervised learning methods, and then train a discriminator on top of the embedding that takes in pairs of examples and outputs a score which can be used for anomaly detection. A test example is paired with the next nearest neighbor. A common concern of the reviewers was on the claim of the paper to be a general approach for anomaly detection whereas experiments are reported only on vision datatsets. The authors have addressed this by making changes to the title and to the claims made in the paper. However R1 and R2 still have concerns about insufficient empirical evaluations, in particular lack of non vision datasets.   As the paper aims to tackle the problem where OOD examples are spread through the sphere, appearing mixed with normal examples, I think fitting a nonparametric density model (eg, using KDE) or parametric density model (eg, a mixture model) on the embeddings is a natural baseline to compare with.   I encourage the authors to strengthen the empirical section of the paper based on reviewers  comments and resubmit to a future venue. 
The paper argued some viewpoint about knowledge distillation quite interesting to me: the technically good KD might surprisingly be socially bad in helping outsiders "stealing" commercial models, even if the models are released as black boxes. Then the paper proposed a way called self undermining KD in order to turn a well trained model into a "nasty teacher" (i.e., an undistillable model), and by this way the commercial models and the corresponding intellectual properties for training them from insiders can be nicely protected.  Overall, the quality is quite high. The argument is very conceptually novel and the method is still technically novel. The idea of the method is simple but works for the purpose   that s great! Although the experimental significance seems not too impressive, the paper opens a door to a new world concerning model privacy instead of data privacy, and hence it is of social significance. In my opinion, the paper should have a potentially huge social impact to DL practitioners (and company owners), because KD is being used almost everywhere in the Internet industry to provide the standalone mode of Apps without clouds on personal devices. Based on the quality and the impact, I recommend to accept the paper as a spotlight presentation.
This paper describes a method called  stochastic  inverse reinforcement learning. It is somewhat unclear how this differs from other probabilistic approaches to IRL. In particular Bayesian approaches have been used in the past to obtain distributions over reward functions. However, SIRL tries to estimate a generative model over such distributions. All the reviewers foudn the paper suffering from lack of clarity, in particular with respect to how the model/algorithm is constructed. There are some possible technical problems with respect to claims about inferring demonstrations by different experts (cf. work on multi task IRL). The experiments also seem to be insufficient.
Three experts in the field recommend accepting the paper (ratings 7,7,6) after the author response, appreciating the improvements the authors made. [Note: The AC is mainly disregarding R3 s rating, as R3 did not respond to the early request of the AC to clarify their review, did not respond to the authors request for clarification, and did not participate in any discussion past their initial short review.]  The solid experimental evaluation and an original methodology for zero shot learning speak for accepting the paper.  [The area chair is certain about accepting the paper, but not fully confident if it should be Poster or Spotlight.]     
The paper studies how suboptimal conditioning sets create suboptimal variational approximations in variational inference with amortization in state space models.  While the point made about the role of the conditioning set is not a new one, the point was carried out further and  more clearly in this paper than previous works. Addressing a couple of issues would  make the paper stronger:    Really boiling down in the experiments to know for what models/data   the "full" approach would add value would provide concrete guidance   to the community.     Notation choices in the paper are rough. For example, Appendix A.2   reads like a type mismatch since the w on the left is a function of   z but is also equal to a function of z and C.     Adding a more detailed description of the complement of C in the   main text
This paper adapts the ideas around universal successor features for decentralised multi agent environments, with a particular emphasis on deriving better exploration from them. Like most of the reviewers, I think this is indeed a promising research direction. Given the complexity of the endeavour however, it may take a few more steps until the empirical evidence can back up the authors  ambition: the reviewers  consensus on the current version of the paper is that it is not ready for publication yet.
The paper proposes a method for learning intristic reward from demonstrations. The inartistic reward is computed as time to reach and generalizes to unseen states.  The reviewers agree that the method is novel useful, and of interest to ICLR community.   Although the authors  significantly improved the manuscript during the rebuttal phase with new results, and addressed many of the reviewers  comments, the overall novelty of the paper is still somewhat limited, making it unsuitable for ICLR in its current form.     The future  version of the paper should address the comments below and go through a detailed pass for clarity.  Additional comments that did not influence the final decision:  The idea of learning temporal distance to the goal is not novel [1], although the application as an intristic reward is. The authors should connect the temporal difference to the reachability theory and solving two point boundary problem for systems with non linear dynamics, as a theoretical foundation of the method [1].  I am curious about the decision to use the time to reach as a reward directly, instead of delta between the states. Some empirical work provides evidence [2,3] that delta yield less side effects in behaviors.   [1] https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber 8772207 [2]https://arxiv.org/abs/1803.10227 [3] https://arxiv.org/abs/2003.06906
This paper studies the effect of the discount mismatch in actor critics: the discount used for evaluation (often 1), the discount used for the critic and the discount used for the actor. There’s notably a representation learning argument supported by a series of experiments. The initial reviews pointed out that this paper addresses very relevant research questions, sometimes in a quite original way, with a large set of experiments. However, they also raised concerns about the organization/clarity of the paper, and possible weaknesses about the experimental studies. The authors provided a rebuttal and a revision, that clarified some points and triggered additional discussions. However, if the revision improved the initial submission, the shared assessment is that the clarity and experiments themselves are still somewhat lacking. As such, the AC cannot recommend accepting this paper. Yet, this work does have interesting ideas, and the problem considered is of interest for the community and under studied. The authors are strongly encouraged to submit a revised version to a future venue. 
I thank the authors and reviewers for the lively discussions. Although reviewers agreed the work is interesting, there are some concerns about the significance of the results and experiments. None of the reviewers were strongly supportive of the paper while majority of them suggest that the paper needs a bit more work before being accepted. Also, reviewers suggest that the paper is not easy to follow and its writing should be improved. Given all, I think the paper , at the current stage, is below the accept threshold. I encourage authors to edit the paper according to the suggestions by the reviewers. 
The paper studies the convergence rate and generalization of deep ReLU networks trained with gradient descent and SGD in the NTK regime. Although the analysis technique is not really novel and heavily relies on past results, the paper is easy to follow and does provide some nice improvements compared to prior work (e.g. it require less overparametrization, and the NTRF function class is allowed to misclassify a fraction of the training data). Some of the results are very incremental, e.g. the generalization bound for GD seems to simply combine existing bounds on the Rademacher complexity from Bartlett et al. 2017 and from Cao et al. 2019. Nevertheless, the paper does have the potential to yield further improvements in the field and I therefore recommend acceptance as a poster.
The reviewers all found that the Consensus method introduced seemed sensible and applauded the authors on their extensive experiments.  However, clearly they struggled to understand the paper well and asked for a clearer and more formal definition of the methods introduced.  Unfortunately, the highest scoring review was also the shortest and also indicated issues with clarity.  It seems like the authors have gone a long way to improve the notation, organization and clarity of the paper, but ultimately the reviewers didn t think it was ready for acceptance.  Hopefully the feedback from the reviewers will help to improve the paper for a future submission.
This work proposes to uses an energy based objective combined with generative adversarial networks for imitation learning. While most reviewers find the work easy to follow and come with theoretical justifications, albeit mostly followed from previous works, and good coverage of experimental results, all of them raised questions regarding the limited novelty and added contribution of the work, and missing more recent baselines. Please consider address these feedback in your future submissions.
Two knowledgeable reviewers were positive 7 and very positive 10 about this paper, considering it an important contribution that illuminates previously unknown aspects of two classic models, namely RBMs and Hopfield networks. They considered the work very well developed, theoretically interesting and also of potential practical relevance. A third reviewer initially expressed some reservations in regard to the inverse map from RBMs to HNs and the experiments. Following the authors  responses, which the reviewer found detailed and informative, he/she significantly raised his/her score to 7, also emphasizing that he/she hoped to see the paper accepted. With the unanimously positive feedback, I am recommending the paper to be accepted. 
This paper presents a two step approach to achieve disentangled representation and good reconstruction at the same time in deep generative models: the first step focuses on good disentanglement (e.g., with beta TCVAE) while possibly sacrificing reconstruction reconstruction, while the second step focuses on high quality reconstruction, conditioned on the low quality reconstruction from disentangled representation. In this paper, each step uses an existing method: beta TCVAE is used for the first step and AdaIN is used for the second, so the paper presents an intuitive combination of two existing methods to achieve both goals. Some useful ablation studies are provided to empirically justify the specific method choices. The concern is whether the two step approach is necessary to achieve both; the authors  argument is that models learning only one set of latent variables may not have the capacity to achieve both goals, and methods jointly learning two set of variables ("disentangled" and "correlated" variables) can not guarantee they represent disjoint structures of data. Some of these statements seem somewhat handwavy (including the d separation argument, I am not very sure if it applies when the variables are learned separately), and shall be made rigorous and justified (theoretically and/or empirically).  The reviewers rate this paper to be borderline.
Unfortunately some of the reviewers  reactions to the author feedback won t be visible to the authors. The reviewers highly appreciated the replies and revision of the paper  Pros:   The paper renders Generalized Exploration tractable for deep RL.   The idea is applicable to many DRL methods and is potentially very valuable to deal with the headaches associated to DRL.  Cons:   R2 and R4 are still concerned about whether  smart  exploration will always be advantageous, and whether the added complexity is a good trade off for the (potentially) better performance. A comparison to  pure  exploration would still be insightful.    the new  SAC with Deep Coherent Exploration  only partially addresses the concerns of R2 and R4, especially in terms of performance  While the paper has improved drastically during the reviewing process, there are still a few too many doubts.
The paper proposes a sequential meta learning method over few shot sequential domains, which meta learns both model parameters and learning rate vectors to capture task general representations.  Reviewers raised many insightful and constructive comments. The main themes are as follows:   The problem setting needs further motivation and clarifications, to make it more realistic and applicable.   The novelty is relatively weak, e.g. the approach is too simple, and learning the learning rate is a common trick.   The method needs great effort for better presentation and justification. The current presentation simply lists several equations in a dense way without detailed explanation. Some main claims such as mitigating catastrophic forgetting are not elaborated extensively.  AC scanned through the paper and agreed with the reviewers  main points. Authors  rebuttal in general did not address these concerns to the satisfaction. For example, even after revision, the readability of this paper is not good enough. The authors are encouraged to perform a thorough revision.
The paper introduces convex reformulations of problems arising in the training of two and three layer convolutional neural networks with ReLU activations. These formulations allow shallow CNNs to be training in time polynomial in the number of data samples, neurons and data dimension (albeit exponential in filter lengths). These problems are regularized in different ways (L2 regularization for two layers, L1 regularization for three layers), providing new insights into the connection between architectural choices and regularization. The paper also provides experiments showing convex training of neural networks on small datasets.    Pros and cons:  [+] The theoretical results show that globally optimal training of shallow CNNs can be achieved in time fully polynomial, i.e., polynomial in the number of data samples, neurons and data dimension. This is significant theoretical progress, since the corresponding results for fully connected neural networks require time exponential in the rank of the data matrix. There is, however, an exponential dependence on the filter length (or the rank of the patch matrix). In particular, the computational complexity is proportional to $(nK/r_c)^{3r_c}$, where $n$ is the number of data points. While CNNs do use relatively small filters, this becomes prohibitive even when $r_c$ is a moderate constant. E.g., the experiments use filters of length $3$. Here, the comments of the reviewers about generalization may be appropriate; perhaps experiments that evaluate the performance of these networks in terms of generalization may show the disadvantages of using very small filters.   [+] The work provides interesting and rigorous insights into the relationship between architecture and implicit regularization, with different network architectures leading to different regularizers (L1, L2, nuclear). Developing these insights for deeper architectures could lead to important insights even in situations where the convex relaxation is challenging to solve efficiently.   [+] Although the theoretical results require overparameterization, in the sense that strong duality holds when the number of filters is large relative to the number of data points, the authors convincingly argue that this degree of overparameterization is commensurate with, or even smaller than, the degree of overparameterization present in many experimental/theoretical works in the literature.   [+/ ] The paper is mathematically precise and is written in a rigorous fashion, but is occasionally heavy on notation. The paper could be more impactful on empirical work on neural networks if it could provide more intuition about how the various forms of equivalent regularization arise from different architectures.   All three reviewers express appreciation for the paper’s fresh insights into global optimization of shallow CNNs and the connection between architectural choices and regularization. The AC recommends acceptance.  
The paper tackles a very important problem. The formulation of the paper is sound as under lightweight assumptions, the supervised loss follows an f divergence formulation (see "Information, Divergence and Risk for Binary Experiments" by Reid and Williamson (JMLR 2011), in particular Section 4.7). It would make sense to dig in the loss in the context of label noise; the variational formulation provides an interesting direction along those lines. The rebuttal on the experimental concerns of reviewers is appreciated (Cf authors’ rebuttal summary). 
  This paper studies the difference between cross entropy and contrastive learning losses in the feature representations that they learn, specifically looking at class imbalanced datasets. The authors show that contrastive losses result in a more "balanced" representation, as measured by the balance of accuracy across the classes when a linear classifier is learned mapping from the feature representation to the class labels. They also show that empirically this tends to result in better generalization to downstream tasks. Inspired by this, they devise a simple modification of the prior supervised contrastive loss method and show that it can improve performance on ImageNet LT and even generalization performance when trained on balanced datasets and applied to downstream tasks.     The reviewers identified several weaknesses, including some clarity issues (R1), limitations of how balancedness is measured and lack of theoretical/statistical rigor in terms of the resulting claims (R2), and differences with respect to concurrent work (R4). A lengthy discussion occurred between reviewers and authors, as well as input from a co author of the concurrent work. In the end, the reviewers were not fully satisfied both in terms of the balancedness measure and relationship to the concurrent work.     Overall, despite this and the valid limitations of the work, I recommend accepting this paper as I believe the contributions outweigh the limitations, and that the findings would be interesting to the community. First, the paper provides some interesting analysis of balancedness and differences across these two loss functions, as well as connections to generalization, which even the concurrent work does not provide. The resulting method, while being a simple modification of the supervised contrastive loss work, is effective both for long tailed datasets and generalization to downstream tasks (even when trained in a balanced manner) which is nice. In the end, we should not use [3] to reject this paper since it was accepted right before the ICLR deadline.     However, I **strongly** recommend that the paper address the valid limitations mentioned in the discussions. Specifically:    1) While I agree that [3] is concurrent work, this paper should none the less tone down its claims of being the first in exploring balance for the camera ready version and clearly address differences between this paper and that one (even if mentioned as concurrent work). It is important to give credit when it is due, and while I think [3] is a different perspective it should be mentioned. Further, the claim that their methodology is not correct is highly arguable, so this should not be mentioned; rather the differences in perspectives and what each paper shows should be emphasized. Even without [3], self supervised pre training (initialization) should arguably be included as a baseline given that it is the logical first choice for incorporating self supervised learning.    2) Like R2, I do not believe the balancedness metric shows uniformity of the feature space. This would have to be shown through methods such as t SNE or in some other way. Being linearly separable in a balanced way across classes (which is what you showed) is not sufficient to show that feature space "uniformity". One can draw many feature space distributions that do not have the intuitive meaning of this (which isn t precisely defined by the authors) but still be linearly separable. I recommend authors remove this type of characterization (unless they can define/show it) and instead include a discussion of the limitations of the current methodology for measuring balancedness. Figure 1 should also emphasize that it is notional (not from real data). 
The paper extends the work of “slimmable networks” in that it aims to find a single set of weights suitable for multiple FLOP/accuracy tradeoff (or memory/accuracy tradeoff). The main novelty of the paper is in adapting known techniques from bayesian optimization (BO) to the setting at hand, resulting in a modified training technique. The experiments show a performance lift when compared against the original slimmable networks, as well as other approaches called “two stage” that alternate between optimizing the weights and the architecture. The paper provides a practical approach to an important problem yielding non trivial results. The main weakness of the paper seems to be its novelty. Although it is not possible to naively apply the multi objective optimization with NAS techniques, the reviews seem to indicate that the innovation required to do so is not sufficient to meet the ICLR bar. This is indeed a borderline case, but given the competing papers, my tendency is towards rejecting the paper. 
I agree with the reviewers  comments. The technique proposed in the paper is very interesting, and although the method itself is not particularly surprising (it s "just" chaining two compressors), it s a really nice way of framing and studying the problem. On the other hand, the experiments _are_ relatively weak, and I think there is significant potential for improvement here (especially with an added 9th page of text). I encourage the authors to add some more convincing experiments in future versions of the paper.
The paper addresses the task of context agnostic learning and presents an algorithm to solve the problem while assuming the ability to sample objects and contexts independently. It is reported a theoretical ground proposing to decompose factors contributing to the classification risk in context bias and object error. The method makes use of only one synthetic sample in training, still being able to generalize well.  The paper received contrasting reviews, 2 positive (7 and 6) and 2 below threshold (5 and 5). R2, R3 and R4 raised similar issues, especially regarding the experimental validation, which is the main shortcoming of the work: addressing "simple" datasets only, no comprehensive comparative analysis only in relation to baselines (vanilla SGD) but not in relation to state of the art methods, possibly slightly revised to accomplish the experimental protocol proposed in this work (authors claim that the originality of the work do not allow a proper comparison with SoA method as is). Indeed, I deem R3 s rating (6) a bit overestimated given the provided comments.  AC does not see an issue the use of only one sample and no info about target, rather, it d be interesting to know if the proposed method could use more than one sample in order to make the comparison with SoA methods fair, while assessing performance in comparative terms with SoA, to give value to the method also in relation to performance.   Unfortunately the rebuttal did not lead an increase of the ratings, nor to better comments.  After the rebuttal, R2 and R4 still remained below threshold; R1 was also not changing idea, remaining positive, and R3 did not react after rebuttal.  Overall, the AC deems this paper containing interesting contributions, but it is not sufficiently ready to be accepted at ICLR mainly because the experiental validation is not showing a fully convincing evaluation of the proposed approach (see above).  
*Overview* This paper applies RL to automated theorem proving to eliminate the need for human written proofs as training data. The method uses TF IDF for premise selections. The experiments compared with supervised baseline demonstrate some good performance.  *Pro* The paper provides a side by side comparison of the effect of the availability of human proofs on the final theorem proving.  The experiments compared with supervised baseline show that the proposed method has good performance even without human knowledge. The prosed TF IDF selection algorithm addresses a challenging issue in exploration of RL.   *Con* The reviewers primarily concern about  the novelty of the methods. It appears the method is not new since there exist a body of work leveraging RL to learn theorem provers. The tasks are also not novel.  After rebuttal, the reviewers are not convinced that the novelty is significant enough for ICLR. The reviewers are also concerned that the proposed method might not be easily generalized to other tasks.   *Recommendation* Although the proposed method and experiment demonstrate some merits, there is a lack of novelty in terms of approaches. Since existing results already consider similar methods and similar tasks, it would make the paper stronger if thorough experimental comparisons are performed.  
The paper presents a meta learning for Model based RL that introduces branched rollouts to improve sample efficiency of the learned model.  While the paper addresses an important topic of sample efficiency in RL, and provides theoretical analysis, the reviewers raised concerns with the novelty and clarity. The extension to POMDP setting is certainly important technological contribution, albeit a straightforward. To be suitable for publication the work needs to make stronger case for the significance of the method.  
This is an interesting, controversial paper that contributes to an ongoing debate in Bayesian deep learning.  Bayesian inference with artificially “cooled” posteriors (e.g., trained with Langevin dynamics with down weighted noise) was recently found to outperform over both point estimation and fully Bayesian treatments (Wenzel et al., 2020). This paper proposes a new explanation for these observed phenomena in terms of a data curation mechanism that popular benchmark data sets such as CIFAR underwent. The analysis boils down to an evidence overcounting/undercounting argument and takes into account that curated data sets only contain data points for which all labelers agreed on a label. The authors claim that, when modeling the true generative process of the data, the cold posterior effect (partially) vanishes.  The paper is well written and provides a consistent analysis by modeling the data curation mechanism in terms of an underlying probabilistic graphical model of the labeling mechanism. Unfortunately, several observed phenomena of (Wenzel et al., 2020) remain unexplained by the theoretical arguments, e.g., the fact that “very cold” (T  > 0) posteriors don’t hurt performance, or the observation that the optimal temperature seems to depend on the model capacity. While the proposed explanation doesn’t capture the full picture (upon which both authors and reviewers agree), the paper’s focus on the data curation process, supported extensive experiments, gives a partial explanation and provides an interesting perspective that will spur further discussion and should be of broad interest to the Bayesian deep learning community.  
This paper presents a defense scheme for adversarial attacks, called self supervised online adversarial purification (SOAP), by purifying the adversarial examples at test time. The novelty of this work is in its incorporation of self supervised representation learning into adversarial defense through purification via optimizing an auxiliary self supervised loss. This is done by jointly training the model on a self supervised task while it is learning to perform the target classification task in a multi task learning setting. Compared with existing adversarial defense schemes such as adversarial training and purification techniques, SOAP has a lower computation overhead during the training stage.  **Strengths:**   * It is novel to incorporate self supervised learning for adversarial purification at test time.   * SOAP’s training stage based on multi task learning incurs low computation overhead compared with the original classification task.  **Weaknesses:**   * Although the proposed adversarial defense scheme is computationally cheaper than the other existing methods during the training stage, it does incur some overhead during test time. This may be undesirable for some applications in which efficiency during test time is an important factor to consider.   * The choice of a suitable self supervised auxiliary task is somewhat ad hoc. The performance varies a lot for different auxiliary tasks.   * The experimental evaluation is only based on relatively small and unrealistic datasets even after new experiments on CIFAR 100 have been added by the authors.  It is said in the paper that SOAP can exploit a wider range of self supervised signals for purification and hence conceptually can be applied to any format of data and not just images, given an appropriate self supervised task. However, this claim has not been substantiated in the paper using non image data.  Despite some limitations and that some claims still need to be better substantiated, the paper presents some novel ideas which are expected to arouse interest for follow up work in the adversarial attack and defense research community. 
This paper proposes a method to update the learning rate dynamically by increasing it in areas with higher sharpness and decreasing it otherwise. This would the hopefully leads to escaping sharp valleys and better generalization. Authors further provide some related theoretical results and several experiments to show effectiveness of their models.  All reviewers find the proposed method well motivated, novel and interesting. The paper is well written and easy to follow. However, both theoretical results and empirical evaluations could be improved significantly:  1  The theoretical results as is provides little to no insight about the algorithm and unfortunately, authors do not discuss the insights from the theoretical results adequately in the paper. See for eg. R1 s comments about this.  2  Given that the theoretical results are not strong, the thoroughness in empirical evaluation is important and unfortunately the current empirical results is not convincing. In particular, there are two main areas to improve:  a) Based on the Appendix D, the choice of hyper parameters seem to be made in an arbitrary way and all models are forced to use the same hyper parameters. This way, the choice of hyper parameters could potentially favor one method over the other. A more principled approach is to tune hyper parameters separately for each method.  b) It looks like the choice of #epochs has been made in an arbitrary way. For all experiments, it would be much more informative to have a figure similar to the left panel of Fig. 4 but with much more #epochs so that reader can clearly see if the benefit of SALR would disappear with longer training or not.  c) Based on the current results, SALR s performance  is on par with that of Entropy SGD on CIFAR 100 and WP and there is a very small gap between them on CIFAR 10 and PTB. I highly recommend adding ImageNet results to make the empirical section stronger. The other option is to compare against other methods in fine tuning tasks. That is, take a checkpoint of a trained model on ImageNet and compare SALR with other methods on several fine tuning tasks.  Given the above issues, my final recommendation is to reject the paper. I want to thank authors for engaging with reviewers during the discussion period and adding several empirical results to the revision. I hope authors would address the above issues as well and resubmit their work.
The paper presents a nice analysis of the spectrum of a matrix that is obtained by applying non linear functions to a random matrix. The paper is mostly well written, the result is novel and interesting, and has clear implications for ML problems like spectral clustering.  So I would enthusiastically recommend the paper for acceptance at ICLR.  It would be important for authors to take into account reviewer comments. In particular, instantiating the theorems for simple ML centric examples would be very useful.    
The paper provides an astonishingly simple experiment: the parameters in the network are fixed, but only the parameters in the BatchNorm (taking less than 1% of the total number of parameters) are trained and also the last linear layer is trained.   The resulting networks provide better accuracies than training a random subset of the network. Another part of this work is the study of the effect of $\beta$ and $\gamma$ when doing full training.   Pros:   All the reviewers agree this is an interesting and important observation.               Contribution is clear and paper is well written             In future, better understanding of different parameters may   Cons: A concern has been raised by one of the reviewers that it is more like a technical report            Some previous work which studies the effect of $\gamma$ was not mentioned.   I think, the most interesting part is training only $\beta$ and $\gamma$. It will provide a ground for theoretical investigations of the properties of deep neural network models, and maybe lead to more efficient training algorithms.   
This paper studies an interesting information theoretic trade off between accuracy and invariance by posing it as a minimax problem. The results are of theoretical nature. However, the implications of the results are not clear. Also, the model/assumptions authors consider are not completely justified. Therefore, the paper at this stage is not recommended for acceptance. However, I highly encourage the authors to improve upon their existing work and resubmit to the next ML conference. 
This paper presents an interesting idea for task free continual learning, which makes use of random graphs to represent relational structures among contextual and target samples. The reviewers agreed that the technical idea is novel, the experiments are extensive and the presentation is good. The authors addressed the reviewers  concerns in the rebuttal. I recommend to accept.
This paper aims to improve the training of generative adversarial networks (GANs) by incorporating the principle of contrastive learning into the training of discriminators in GANs. Unlike in an ordinary GAN which seeks to minimize the GAN loss directly, the proposed GAN variant with a contrastive discriminator (ContraD) uses the discriminator network to first learn a contrastive representation from a given set of data augmentations and real/generated examples and then train a discriminator based on the learned contrastive representation. It is noticed that a side effect of such blending is the improvement in contrastive learning as a result of GAN training. The resulting GAN model with a contrastive discriminator is shown to outperform other techniques using data augmentation.  **Strengths:**   * It proposes a new way of training the discriminators of GANs based on the principle of contrastive learning.   * The paper is generally well written to articulate the main points that the authors want to convey.   * The experimental evaluation is well designed and comprehensive.  **Weaknesses:**   * Even though the proposed learning scheme is novel, the building blocks are based on existing techniques in GAN and contrastive learning.   * The claim that GAN helps contrastive learning is not fully substantiated.   * It is claimed in the paper that the proposed contrastive discriminator can lead to much stronger augmentations *without catastrophic forgetting*. However, this “catastrophic forgetting” aspect is not really empirically validated in the experiments.   * The writing has room for improvement.  Despite its weaknesses, this paper explores a novel direction of training GANs that would be of interest to the research community. 
# Paper Summary  This paper considers the problem of distributionally robust optimization (DRO), in which one is attempting to minimize a loss on the worst of all distributions that are some distance (here, measured in terms of KL divergence) from the training set. The main novelty here is that this adversarial distribution is represented as a model, with parameters that are learned jointly with the primary model.  This is an intuitive idea, but as the authors explain, attempting to implement it leads to a number of complications. One of these is that it is challenging to constrain the adversarial distribution model to be a certain KL divergence away from the training set. To address this, they write down the Lagrangian, but do not actually optimize over the Lagrange multiplier resulting from this constraint: instead, they keep it at a fixed constant value (a hyperparameter). A second, and potentially more worrisome, issue is that it is difficult to optimize the KL divergence as written instead, they swap the two parameters, which is of course incorrect but they claim leads to much nicer convergence behavior.  They also propose a stopping condition, which terminates optimization once the robust validation loss (i.e. the validation loss w.r.t. the worst permissible distribution) stops decreasing. Normally, this would require a search for the worst such distribution at every iteration, which would be prohibitively expensive, so they propose instead only checking the distributions that have been found by the adversary during the course of optimization.  They close with a set of experiments that is nicely designed to narrow in on and explore particular details of their approach (e.g. they have an experiment that validates their stopping criterion), and have a realistic experiment on two NLP datasets.  # Pros  1. Reviewers agreed that it was very well written, well organized, and comprehensive 1. Good discussion of background material. The paper is very accessible 1. Intuitive idea, although the details of the approach become somewhat complex 1. Aside from the "realistic" experiment, each is designed to explore a particular facet of their approach  # Cons  1. Some reviewers were concerned that the baselines were insufficient. In response the authors added the new Hu et al. baseline (NonParam), which seemed to be satisfactory 1. While the approach is more general, one reviewer noted that the experiments only consider NLP problems. This is a minor negative point, in my view 1. One reviewer was concerned that the results were "too good", and encouraged the authors to double check their results. My belief is that, at least on the non "realistic" experiments (which were mostly intended to drill down into specific attributes of their approach, rather than demonstrate its overall performance), this is because the problem was constructed to perform especially poorly with a non DRO approach 1. One reviewer was unsatisfied with the idea of swapping the parameters to the KL divergence (I share this concern). The authors clarified, both in the response and in the paper, that swapping the parameters is indeed incorrect, and may in fact be a very bad approximation to the true quantity of interest, but that the performance difference was so dramatic that it couldn t be undone. This seemed to partially satisfy the reviewer  # Conclusion  All four reviewers ultimately recommended acceptance. The major concerns were (i) that the baselines weren t good enough (which the authors addressed by adding a new baseline), and (ii) that swapping the parameters to the KL divergence results in a very poor approximation to the original KL divergence (which the authors now explicitly acknowledge in the paper, with an explanation for why they feel it is necessary). Overall, this is a nice idea, and while bringing it into practice may require more hand waving than would be ideal (which is the main reason I suggested a poster acceptance instead of a spotlight or oral), it seems to work well experimentally, and the experiments are overall very careful and well thought out. Additionally, the writing quality is excellent, as is the organization and presentation of background material.
The reviewers have arrived at the consensus that this is a paper with an interesting idea, both novel and well explained, but not quite backed up with sufficient empirical evidence. Like them, I think there is a lot of potential in modular methods for continual learning, and I know these are challenging advances to demonstrate. So I encourage you to persist, iterate and submit a stronger version of this paper in the future!
High quality theoretical paper that studies the connection between concentration of the data distribution and adversarial robustness. It contributes a method for more accurate estimation of concentration, which allows drawing stronger conclusions about adversarial robustness compared to previous work. The paper is highly technical, but written clearly and precisely. All reviewers give positive scores, with only minor negative comments.  One minor concern I have is that the potential audience of the paper might be small, given its highly technical nature and very specialized line of research it follows. Still, I believe it s a solid contribution, so I m happy to recommend acceptance.
The paper proposes a modification of the well known FILM model for VQA which targets counting problems in particular, which have been a known weakness of existing models. The improvements have also been tested beyond counting. The experimental results are convincing, in particular a scientific competition has been won. The reviewers also appreciated convincing ablation studies.  The idea bas been perceived as interesting enough for publication, and in combination with the experimental results, this compensated several perceived weaknesses (limited novelty w.r.t. the modified FILM model; justifications of some design choices).  All reviewers agreed that this paper is of interest to the community and proposed acceptance. The AC concurs.
Reviewers appreciate the numerical results presented in this paper. However, the paper needs a more rigorous theoretical investigation of the empirical phenomenon, or a more comprehensive empirical exploration to pinpoint the key factors. I recommend the authors to incorporate the suggestions from the reviewers and submit the paper to the next top conference.
This paper provides a new uncertainty measure of examples called "Variance of Gradients" (VoGs); it demonstrates that VoGs are correlated with mistakes, and can be useful for guiding optimization.   On the positive side, the reviewers generally think that the ideas of this paper is nice and contribute to the research thrust in gradient based uncertainty. In addition, the paper provides valuable empirical insights.   However, the reviewers also pointed out a few important limitations:   A more thorough comparison to prior methods is needed to convince the readers for actual usage. There are many other methods (e.g. predicted entropy) for example difficulty estimation / classifier trustworthiness that need to be compared to.   The stability of individual VoG scores needs to be investigated further  The authors are encouraged to address these limitations in the next iteration.
This meta review is written after considering the reviews, the authors’ responses, the discussion, and the paper itself.  The paper has 2 main contributions: 1) analysis of the sensitivity of a deep network predicting steering angle from images w.r.t. different synthetic image perturbations, 2) A training method, based on adaptively adjusted data augmentation, which improves the robustness of a model to seen and previously unseen perturbations.  The reviewers’ opinions are somewhat mixed, leaning towards negative. The reviewers point out that the task is important and the methodology makes sense, but the experiments are limited: only one dataset, only synthetic perturbations, only the steering angle prediction task (which is not necessarily very practical), not strong enough baselines, no results on established datasets like ImageNet C. The authors addressed some of these issues in the updated version of the paper (more datasets, one more baseline), but most reviewers did not change their evaluation.  Based on all this information and reading the paper itself, I recommend rejection at this point. The paper has interesting ideas, but the experimental evaluation is not sufficient. Moreover, I find the use of the steering prediction task confusing   there does not seem to be anything driving specific in the method, so using standard datasets (like ImageNet C) would be more convincing. For driving datasets, using real world (not synthetic) image perturbations would be advisable. As the paper stands, it looks neither like a proper application paper, nor as a fundamental method/analysis paper, but something inbetween, which is not to its favor. 
This paper presents several analyses on the geometry of GAN generators through the lens of Riemannian geometry: showing interpretability of the leading eigenvectors of the Hessian, homogeneity of the space, and more efficient latent space inference through preconditioning. Reviewers found the (revised) paper well written and clear, with a thorough set of experiments to support their main claims. While there were several concerns around the generality of the approach, the authors performed several experiments in the rebuttal period to address many of the reviewer’s concerns (robustness of findings with different image distance functions, inversion on additional GANs and datasets, user study of perceptual properties of axes, and comparison to previous methods for intepretable axes discovery). I found these experiments extensive and convincing, supporting the claims around robustness of the approach to different image distance metrics, GAN architectures, and interpretability of the axes.   There were also strong concerns around similarity with recent work (Chiu et al., SIGGRAPH 2020 and Peebles et al., ECCV 2020), but both of these papers were published at most 1 month before the ICLR submission deadline, and thus should be considered as concurrent work.   Given the strong set of additional experiments and interesting empirical observations, I recommend accepting this paper.  There remain concerts around the extent to which the findings “unify” previous approaches on interpretable axes, and we encourage the authors to update the paper before the camera ready to address these and additional reviewer concerns (especially expanding the discussion of the relationship with concurrent work in Chiu et al. and Peebles et al.).
The authors present BASGD and asynchronous version of SGD that attempts to be robust against byzantine failures/attacks.  The papers is overall well written and clearly presents the results. Some novelty is present as there have been limited work in asynchronous algorithms for byzantine ML.   However, there have been several concerns raised by the reviewers, on which I agree, and they have not been fully addressed: 1) the tradeoff between asynchrony and robustness, as BASGD cannot handle the case of a buffer being straggler, which limits some of the novelty in this work 2) issues with the definition of privacy leakage has not been fully addressed 3) some reviewers mentioned the theoretical results being of limited importance, but arguably this is true for other related work in this area. Perhaps a general criticism is valid as to what is the operational value of the proposed guarantees. That is convergence does not exclude a model that has undesirable properties, eg has bad prediction accuracy for a small subset of tasks. 4) Finally, the motivation of the system model of the paper ( eg storing gradients as opposed to instances) paper is of unclear practical relevance, as was raised by multiple reviewers.   Overall the consensus was that the paper does have merits, however, some of the most major concerns were not properly addressed. This paper can potentially be improved for a future venue.  
This paper introduces an object perception and control method for RL, derived from a control as inference formulation within a POMDP.  The paper provides a theoretical derivation and experiments where the proposed joint inference approach outperforms baselines.  The discussion focussed on understanding the paper s contribution relative to prior work. The reviewers highlighted the similarities with earlier systems (R1, R2, R4), the unclear benefits of joint inference over independently trained modules in the experiments (R3), and the lack of clarity of the presentation (R1, R2, R3).  The authors responded to some of these criticisms, bolstering the paper with additional experiments to show the benefits of joint inference and increasing the discussion of related work.  The reviewers examined the revisions and rebuttal and found the paper still did not resolve all their original concerns.  Two limitations mentioned in the final phase of the discussion were the use of a single environment to evaluate the general framework, and continuing doubts on the contribution of joint inference mechanism to the measured performance.  Four knowledgeable reviewers indicate reject as their concerns were not adequately resolved.  The paper is therefore rejected.
This paper applies methods inspired by neuroscience to analyze the inner workings of LSTM language models. In particular, a simple and clever approach is proposed, in which a sentence is presented in its observed context vs. a random one. The time for a unit activation to become similar in the two contexts is used as a probe of the timescale of contextual effects. The main results are that timescales increase with layer and that there are two classes of long timescale units with different graph theoretical properties. The functionality of syntax sensitive units previously identified in the literature is confirmed. Finally, the analysis is replicated for a character level model.  The paper received detailed and insightful reviews, and there was a lively (but always respectful) discussion between authors and reviewers.  Overall, the reviewers liked the topic of the paper and the overall methodology, however they had several issues with it. One of the issue pertained to the "holistic" approach to time in the paper, which is measured in number of tokens, rather than in terms of syntactic distance. More in general, there was a feeling that the paper was somewhat short on actual insights on the exact functional role of units in a linguistic context. The reviewer who assigned the most severe score was mostly concerned about one specific instance of this, namely the fact that the authors focus on syntax tracking and number agreement units whose scope should not really extend across sentences. Moreover, the reviewer was surprised that the syntax tracking units maintain information across longer distances than the number agreement units, that should, by definition, keep track of long distance relations.  I am divided. I welcome work that focuses on novel qualitative and quantitative analyses of an existing model. I wished there were clearer take home messages on how LSTMs process language, but I recognize that our knowledge of deep learning models is very preliminary, and I am thus not surprised that the conclusions are not entirely clear.  The reviewers raised important concerns, but I would not confidently claim that we know enough about the relevant units to be genuinely surprised by some of the results. For example, can we really say that number agreement units are only limited to clause internal agreement tracking? Couldn t it be, say, that we will discover in the future they also play a role in tracking discourse determined pronominal number (going out on a random limb, here, of course)?  Overall, I would like to see this at least as a poster at the conference, but I am assigning low confidence to my recommendation as I respect the reviewers  point of view. 
The paper proposes to use projective clustering to compress the embedding layers of DNN. This is a novel interesting idea which can  impact the area of Knowledge distillation. There were some concerns about the empirical study which was addressed to some extent  by the authors during the rebuttal.
The paper proposed a shot conditional form of episodic fine tuning approach for few shot image classification. There were a number of concerns raised, e.g., there lacks of sufficient comparison with SOTA baselines, the justification on the significance of shot aware approach is not entirely convincing, and incremental contributions in both novelty and improvements. While some of these issues were improved in the rebuttal, the revision remains not satisfied by the reviewers. Overall, I think the paper has some interesting idea, but is still not ready for publication. 
The paper presents an approach that supports better performance when out of distribution cases occur, by letting neurons be of only compact support and thus if the input is out of distribution (OOD).   Pros:   The proposed strategy is interesting and may be useful.  Cons:   The choice of the parameter alpha, whose value is crucial to the success in experiments, is left murky. The approach suggested by the authors was not validated experimentally.    There is insufficient comparison to recent works.
This paper is concerned with finding causal relations from temporal processes and extends the Convergent Cross Mapping (CCM) method.  It focuses on finding information of chaotic dynamical systems from short, noisy and sporadic time series, and the idea of using the latent space of neural ODEs to replace the delay embeddings in CCM seems interesting. All reviewers like the idea. Please try to make the paper more self contained and provide some of the justifications suggested by the reviewers.
The authors investigate different tokenization methods for the translation between French and Fon (an African low resource language). Low resource machine translation is a very important topic and it is great to see work on African languages   we need more of this!  Unfortunately, the reviewers unanimously agree that this work might be better suited for a different conference, for example LREC, since the machine learning contributions are small. The AC encourages the authors to consider submitting this work to LREC or a similar conference.
This work proposes a method for generating candidate molecules using a novel fragment based MCMC proposal mechanism.  Pros: * Well written paper * Novel idea for an important application * Very good empirical performance compared to the state of the art in multi objective molecule generation * Careful ablation studies  Cons: * Some details were missing (runtime, experimental details) and have been added to the revised version.  The authors engaged in an extensive discussion with the reviewers and modified their paper to address the reviewer concerns.  After discussions three reviewers recommend accepting the work and consider it a novel and useful contribution to the field.  One reviewer (Reviewer 3) is not satisfied by the authors comments and has concerns about the work regarding: asymptotic correctness of the sampling; fairness of the experimental comparison; and computational complexity.  The authors provide detailed justifications for their choices.  After looking at the discussion there are two factors: 1. technical arguments regarding the correctness of the sampling method; the authors justify the correctness by known results for adaptive MCMC methods, and the argument is sound, and the area chair fully accepts the authors  arguments as correct and applicable. 2. extend of the experimental evaluation and suitable baseline methods; this is partially subjective.  The authors provide extensive experiments in their work and justify exclusion of certain methods in that they do not easily apply to the multi objective setting.  In addition, Reviewer 3 demands a comparison of generated molecules per time, which is plausibly useful, however, none of the prior works have used such a metric in a consistent manner and it is clearly challenging to do so fairly as such metric would depend on specifics of the implementation and computer.  The authors have updated their paper and added runtime information for their method.  The area chair fully accepts the authors  arguments and justification for the current experimental scope.  In summary the area chair considers the remaining concerns by Reviewer 3 as invalid; in particular, the authors have made extensive efforts to engage and educate the reviewer.
The paper introduces an adaptive label smoothing technique, where the smoothing factor is computed based on the relative object size within an image, in order to address the problem of overconfident predictions. All reviewers recommend rejection based on limited technical contribution and unclear benefits of the proposed method. During the rebuttal phase, the authors carried out more experiments and clarified several other questions asked by the reviewers. The response was well received, but did not eliminate the main concerns about the paper. While the idea is interesting and has potential, the AC agrees with the reviewers that the paper is not ready for ICLR, and encourages the authors to improve the paper according to the reviews and submit it to another top conference.
This paper proposes a simple yet powerful generalisation of graph scattering transforms that allows a flexible scale dilation structure, retaining the stability guarantees of dyadic transforms. Experiments with strong empirical performance are reported on a variety of biochemical tasks.  Reviewers acknowledged the soundness of the approach as well as the quality of the empirical evaluation, but also raised some concerns about lack of novelty. Ultimately this AC believes that, although this work solidifies Graph Scattering Transforms as a good alternative to GNNs on certain structured physical domains, it provides little advancements on the theory front. Unfortunately not all good papers can be accepted, and therefore the AC recommends rejection at this time, encouraging a resubmission.
This paper investigates how to align word senses across languages. This has not been studied much as past work has primarily considered aligning word (embeddings) across languages. The paper is well written and well motivated. Unfortunately the empirical results are not very strong. The baselines are somewhat low and the gains are modest (the excuse that it is difficult to train BERT sized models in academia is acknowledged). Overall, there is not enough support for acceptance at such a competitive venue as ICLR. 
This paper concerns data augmentation techniques for NLP. In particular, the authors introduce a general augmentation framework they call CoDA and demonstrate its utility on a few benchmark NLP tasks, reporting promising empirical results. The authors addressed some key concerns (e.g., regarding hyperparameters, reporting of variances) during the discussion period. The consensus, then, is that this work provides a useful and relatively general method for augmentation in NLP and the ICLR audience is likely to find this useful.
All reviewers agree that this is a well written and interesting paper that will be of interest to the ICLR and broader ML community.
In this paper, the authors proposed a new approach by the name of LoCal + SGD (Localized Updates) to replace the traditional Backpropagation method. The key idea is to selectively update some layers’ weights using localized learning rules, so as to reduce the computational complexity of training these layers so as to achieve a better tradeoff between overall speed and accuracy. The paper received quite mixed reviewers. Some reviewers criticized the incremental nature of the proposed technology, while some other reviewers thought that this is one of the very early papers that demonstrates the practical effectiveness of localized learning.   The reviewers have made several rounds of discussions, and as a result of that, we think while this direction (localized learning) is very important and promising, this particular paper might not have provided a sufficiently novel and good solution to it.  Specifically, in terms of localized learning, this paper has not proposed brand new concepts or methodologies, instead it adopts existing methods in selective layers. In this sense, it does not really resolve the accuracy issue of localized learning, rather, it achieves the tradeoff by only applying localized learning in some layers. In other words, the current results still heavily rely on BP and has not brought a real breakthrough to localized learning. 
The authors propose a new approach to topology optimization to address over smoothing in GCNs. This is a borderline paper. Topology optimization is clearly important and relevant and the approach tries to optimize the topology (add/delete edges) by viewing the problem as a latent variable model and aiming to optimize the graph together with the GCN parameters to maximize the likelihood of observed node labels. A number of related joint topology optimization approaches exist, however, as discussed in the reviews and the responses. The proposed methodology is termed variational EM but is a bit heuristic in the sense that E and M steps do not follow a consistent criterion (the direction of KL is flipped between the steps). A number of comparisons are provided with consistent gains though the gains appear relatively small. No error bars are provided despite request to add them to better assess the significance of these results. It remains unclear whether the gains are worth the added complexity.  
This paper presents a way to aggregate and precompute node features on a graph to enable fast parallel training of neural models on massive graphs for various node prediction tasks.  We have seen quite a few papers in this line of work (precompute node features without training, and then treat the nodes as independent during training) recently and this is a continuation in this trend.  Most reviewers lean toward rejection.  The main concern is the lack of novelty and the marginally better results reported in the experiments.  In some sense, the proposed method could be thought of as replacing the concatenation operation of node features across multiple hops used in the SIGN paper with a sequence model, either conv + pool, or attention.  Given this, the novelty of this paper is indeed a bit limited.  Additionally, it is unclear why this sequence model perspective is better than concatenation, which should be in principle more expressive and in practice faster and more efficient (as also reported in Table 6).  I recommend rejecting this paper, but do encourage the authors to position their work better with respect to prior work and really consider what’s the defining advantage of their approach compared to alternatives, like SIGN.
The paper shows that under a very restrictive assumption on the data, ReLU networks with one hidden layer and zero bias trained by gradient flow converge two a meaningful predictor provided that the network weights are randomly initialized with sufficiently small variances. While there is some overlap with a paper by Lyu & Li (2020), the paper under review establishes its results for networks with arbitrary widths whereas using the results of Lyu & Li (2020) works, at least so far, only for sufficiently wide networks. The assumption on the data is anything than realistic and actually any "simple, conventional" learning algorithm can easily learn in this regime. Nonetheless, getting meaningful results for neural networks is still a notoriously difficult task and for this reason, the paper deserves publication.   
The paper addresses the difficult problem of combining ILP in a meta interpretive framework with noisy inputs from a neural system.   The essential idea is to use MIL to "efficiently" search for constraints on the neural outputs (eg z1 + z2 + z3   7, or z2< z3) as well as logic programs, with a score related to program complexity as well as probability of the best constraint satisfying neural outputs.  It is interesting work for the right audience but it s clear from the reviews that the presentation was difficult for ICLR readers, even ones with appropriate background.   Some potential weaknesses of the approach include:  1   it s unclear how scalable the MIL framework is   presumably the intrinsic difficultly of the search means that programs and constraint sets must be small  2   it s unclear how general the approach is beyond the digits as separate inputs setting of the two experimental studies, and its unclear how accurate the perceptual layer needs to be   MNIST obviously being an example of a case where there is little noise with a modern classifier.  3   it s unclear how constraints can in general be used to backprop any information to the underlying neural system, and without this the joint training seems to be quite limited.  Overall the paper is judged as inappropriate for ICLR.
Summary: The authors propose a method for representing a posterior over discrete latent variables in representation learning problems using a neural network. Two applications are discussed: One are certain clustering problems, in which clusters are sufficiently separated. Another is the computation of mutual information of discrete random variables. This is applied to learning image representations.  Discussion: The authors have not provided a response to the reviews.  Recommendation: Four detailed reviews unanimously recommend rejection. Main points of criticism are lack of novelty, limited and unconvincing experimental evaluation, and a poor presentation that also lacks technical detail. This work is clearly not ready for publication. 
The article is easy to read, of interest for the community, and provide some advance towards understanding the implicit bias of gradient descent. The results and the methodology for the rank 1 case are very interesting and convincing. Yet, some results could be made more explicit and the comments by the reviewers should be addressed for the camera ready paper, in particular the one on the organization. 
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with the reviewers that the paper touches an important topic(scale up training). However, as some of the reviewers pointed out, the paper could be further improved by clarifying the novelty and more thorough evaluation justification of the metric being used. Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.  
Congratulations!  The reviewers unanimously viewed this work positively and were in favor of acceptance to ICLR.  While the current revision already addresses many reviewer concerns, it may be worth adding some of the datasets pointed out by R3 or comparing to some of the papers suggested by R1.
The paper looks into generalization performance of NNs in supervised learning setting. The authors propose a regularizer to enhance neuron diversity in each layer(within layer activation diversity) as a regularizer to improve generalization.  The proposed idea is an extension of Cogswell s work with different regularization terms. The appearance of the term related to the layer output diversity in the generalization bound provides theoretical support for the proposed idea.They use Radamacher complexity as a tool to show this and bound the estimation error.  pros.  The paper looks into an interesting problem. Designing a regularizer to improve generalization performance of NNs is of huge importance.  The paper is well presented and clear.  cons.  The main drawback of the paper is lack of proper comparison to other regularizers and showing the uniqueness/superiority of this regularizer and how it improves over existing methods either theoretically or with experiments. Without that the significance of this work is limited.   The authors response to Reviewer 2 s comment was not convincing enough. I encourage the authors to improve this in the next iteration of the paper.    i suggest doing a better job in including the related work as also mentioned by the reviewer.  The experiment section can use more explanation and details on choice of hyper parameters, etc    showing performance improvement for a deep architecture would definitely  improve the paper. In the current version only 2 and 3 layer toy examples are shown.
This paper introduces an HRL method that uses slow features to define subgoals (or abstract states), which can then be used by goal conditioned policies. It is said that such an approach allows for efficient exploration. Most reviewers are recommending the acceptance of this paper, they found the method interesting and they think it introduces interesting ideas that are not that common to the HRL literature. Thus, I’m recommending the acceptance of this paper.  I’d still encourage the authors to take the reviewers comments into consideration when preparing the final version of the paper. Specifically, it would be useful to explicitly discuss the “chicken and egg problem” and the fact that the agent has access to a function defining the distance to the goal before the goal was observed for the first time. Some baselines have the same assumption, but it is somewhat weird to discuss exploration in this setting without further clarifications.  
The paper proposes three discretization schemes for two first order optimization flows, and proves the "convergence" to the minimizers of the problem that the optimization flows approach. The methods are tested on the DNN training problem and show comparable performance.  Pros: 1. The problem being studied, the discretization of optimization flows, is of interest to the community. 2. "Convergence" guarantee is provided.  Cons: 1. The theoretical analysis is somewhat preliminary, as the authors have admitted. There is a prescribed \epsilon in the approximation error (23) that prevents the right hand side of (23) from approaching zero. The parameter \eta, depending on the chosen accuracy \epsilon, should be provided so that a user can implement the discretization schemes if s/he is interested. Moreover, by specifying \eta, it may be possible to compare the numbers of iterations to approach an \epsilon solution between the proposed discretization schemes and other optimization methods for solving the original optimization problem. By doing this, the motivation issue from Reviewer #1 (and the AC) can be resolved. Purely discretizing an optimization flow is of less interest to the machine learning community. 2. Although the comparison on academic problem is obviously advantageous, the comparison on DNN training is only comparable or marginally better.   The author responses resolved part of the challenges from the reviewers, but the key issues remained (as communicated in confidential comments). Since the final average score is below threshold, the AC decided to reject the paper.
This paper presents an approach to domain adaptation in reinforcement learning. The main idea behind this approach, DARC, is to modify the reward function in the source domain so that the learned policy is optimal in the target domain. This is achieved by learning a classifier that learns to discriminate between the data from the source domain and those from the target domain.   Overall, reviewers appreciated the intuitiveness of the approach as well as its formal analysis. They had some concerns with respect to experiments, which was sorted out in the author response period. Given the overall positive reviews, I recommend accepting the paper.  
In this paper, the authors propose to adapt the recent paper by Yu et al. (ICML 2020), namely FedAwS. In that paper, the authors solved a potential failure mode in federated learning, when all the users only have access to one class in their devices. In this paper, the authors extend FedAwS to a setting in which federated learning is used for User Verification (UV), namely FedUV. The authors argue that the previous paper could not be the solution to learning UV because FedAwS share the embedding vectors with the server.   The authors then show a procedure in which they can learn a classifier in which the embedding vectors are not needed to be shared with the classifier. They use error correcting codes to make the mapping sufficiently different and that allows the training to succeed without sharing the embedding. The proposed change is only marginally worse than FedAwS and centralized learning. This is the part of the paper that has attracted positive comments and is praised by all the reviewers.   The authors take as given that by not sharing the embedding vectors and by using randomly generated error correcting codes, the whole procedure is privacy preserving and secure. The 4th reviewer indicates that these guarantees need to be proven and points out several references that hint toward flaws in the argument by the authors. Reviewer 4th does say that not sharing the embeddings might not be enough, but that self evident arguments are not enough.   This paper provides a significant improvement for a federated machine learning algorithm that deserves publication, but the rationale of the paper is flawed from a privacy and security viewpoint. I think if the paper is published as is, especially with the proposed title, it will create a negative reaction by the security and privacy community for not adhering to their standards. We cannot lower those standards.    I suggest to the authors that they can follow two potential paths for publishing this work:   1 Change the scope of their algorithm. For example, I can imagine that by not sharing the embedding the communication load with the server might be significantly reduced or that adding new users with new classes can be easier.   2 Follow the recommendation from Reviewer 4 and show that the proposed method is robust against the different attacks.   Minor comments:   For a paper that is trying to solve the AU problem, I would expect a discussion about why learning is better than a private algorithm. In a way, learning is sharing, and that increases the risk of mischief by malicious users.      The discussion about error correcting codes and the minimum distance is quite old fashion. In high dimensions, the minimum distance is not the whole story. LDPC codes make sense when we stop focusing on minimum distance codes and minimum distance decoding. I would recommend having a look at the Berlekamp’s Bat discussion in David MacKay’s book (Chapter 13).
Pros: Reviewers generally agreed the paper was well written and is easy to follow. The goal of learning loss functions also seems quite promising.  Cons: There were concerns about whether credit for experimental performance was attributable to the core algorithm+functional form presented in the paper. There was also some skepticism about the specific form of the learned loss. Of greatest concern, no reviewer argued for acceptance during discussion, and one reviewer lowered their score during discussion.
This paper proposes a new metric to measure symmetry based disentanglement and uses this metric to optimize diffusion VAEs on a set of small, synthetic datasets. In general, reviewers found the theoretical framework introduced to be interesting and relevant, but there were a number of concerns regarding the empirical evaluation in the paper and the clarity of many of the claims, particularly wrt the need for strong supervision (pairs of data points with a known transformation between them) for both evaluating the metric and for training by regularizing the proposed metric. I d encourage the authors to focus on the improvement points suggested by reviewers, most notably by improving the empirical evaluation by adding detailed ablations and comparisons (e.g., exploring the relative amount of supervision needed, comparisons to previous approaches) and clarity regarding the supervision required. As such, I recommend that it be rejected in its current form.  
This paper addresses a method for generating meta tasks via latent space interpolation using a generative model, trained on the unlabeled dataset, to solve the unsupervised meta learning. The method seems to be sound, but it lacks MiniImage Net experiments, which is the main concern raised by most of reviewers. During the author responses, new empirical results on MiniImage Net were added. During the discussion period with reviewers, I communicated with the reviewer with most negative comments. He/she was not fully satisfied, claiming that the protocol used for obtaining new MinImage Net results was slightly unfair. It is suspected that the authors used a generative model trained on the ImageNet training set (of which miniImageNet is a subset) for LASIUM and  did not present results from only using miniImageNet meta training data because it was not competitive with prior work. It should be clarified in the final paper. However, we arrived at the consensus that the paper is worth being presented.   
The majority of the reviewers believe that this paper is not ready for publication. Among their concerns is that the paper has limited novelty, especially in relation to existing work that use the KL constraint. Some of the reviewers also believe that the arguments are sometimes hand wavy and not rigorous. For example, in the discussion period after Nov. 24th, it is mentioned that the argument by the authors that "KL constraint >decrease the approximation error >increase performance" is not precise enough. I encourage the authors to take these comments into account and improve their paper.
This paper addresses automatically learning the neighborhood size (they call adaptive neighbor support) for unsupervised representation learning with a VAE.  The neighborhood size is determined based on z scores from by estimating a normal distribution in the latent space.   The paper is poorly written.  There are several grammatical errors and typos that distracts from understanding the paper.  In addition, the use of terminology is not precise, which adds to the confusion, as pointed out by the reviewers.  AC VAE is better than VAE+KNN in Table 1 but worse in SCAN with KNN in Table 3.  Further analysis to understand why this is so is needed.  Additional measures of cluster quality is recommended.  As pointed out by the reviewers, this paper is below the acceptance threshold for ICLR.  The reviewers provided several constructive suggestions.  Please refer to detailed reviewer comments to help you improve your paper. 
This paper proposes a refinement, and analysis of, continuous time inference schemes.  This paper got in depth criticism from some very thoughtful and expert reviewers, and the authors seem to have taken it to heart.  I m still worried about the similarity to GRU ODE Bayes, but I feel that the clarifications to the general theory of continuous time belief updates is a worthy contribution, and the method proposed is a practical one.  One reviewer didn t update their score, but the other reviewers put a lot of thought into the discussion and also raised their scores.  I do think the title and name of the method is a bit misleading   I would call it something like "Consistent continuous time filtering", because the jump ODE is really describing beliefs about an SDE.
The paper investigates the effect of soft labels in knowledge distillation from the perspective of sample wise bias variance tradeoff. They observe that during training the bias variance tradeoff varies sample wisely. and under the same distillation temperature setting, we  distillation performance is negatively associated with the number of regularization samples. But removing them altogether hurts the performance (the authors show empirical evidence of this). Based on some observations about regularization samples, the authors propose the weighted soft labels to handle the tradeoff. Experiments on standard datasets show that the proposed method can improve the standard knowledge distillation.  pros.  the paper is written clearly.  through the review period the authors added additional experiments suggested by the reviewers and enhances experimental results. The experiment results are convincing and the authors have now added explanations on hyperparameter choices.  the mathematical setting is now clear after incorporating reviewer s comments.  the missing related work as suggested by reviewers is added  cons.  comparison with results of Zitong Yang et al 2020[1] is missing.  I thank the authors for incorporating the changes requested by reviewers. Please add comparison with result of [1] in the final version.  [1] Rethinking Bias Variance Trade off for Generalization of Neural Networks Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, Yi Ma 
The paper studies the problem of estimating high quality prediction intervals for deep regression models. The paper argues that one (relatively under studied) avenue to improve these intervals is to accurately estimate conditional coverage   traditional PIs only reason about marginal coverage. The paper argues that in the presence of heteroskedastic errors or model mis specification, conditional coverage can be dramatically different than marginal coverage. Concrete examples for each of these cases would be useful to establish the claim   a synthetic experiment later in the paper illustrates the gap using heteroskedastic errors. The paper introduces a "Confidence Assessment" module that estimates the probability that the model s confidence interval is correct. In spirit, this is akin to learning a calibrated probabilistic classifier. Theoretical analysis shows that the CA module can provably assess the reliability of the confidence intervals while jointly training the confidence interval method   some reviewers appreciated the rigor in this analysis.  However, the reviewers also pointed out that the main message of the paper is muddled, and the confusion spills over into the experimental execution of the paper. Many of the complaints about baselines and experiment setup can be traced back to this confusion. There are several claims in the paper:   Conditional coverage estimation is useful. The synthetic experiments demonstrate this sufficiently.   The CA module achieves conditional coverage estimation reliably and efficiently. There are missing baselines (e.g., other approaches implementing a probabilistic classifier) in the experiments to establish this claim. The authors added an experiment to address this, but reviewers are concerned that the baseline classifier is unnecessarily handicapped (e.g., training a new coverage model from scratch instead of using existing learned features). Reviewers also note that there are missing metrics   the existing metrics can plausibly be gamed by simply outputting the marginal coverage estimate.   Incorporating CA module leads to better prediction intervals. Some experiments suggest that this is not the case, and that there is negligible improvement (the lambda_2   0 setting that the authors describe). On the other hand, it is heartening to note that adding the CA module did not adversely affect the quality of the prediction intervals either.  Since a two stage procedure (estimate intervals, followed by estimating CA module) is empirically inferior to joint training, reviewers rightly ask for some insight into why estimating conditional coverage jointly would reliably lead to prediction intervals that are more precise on average. The theoretical analysis in the paper applies to the 2 stage procedure too (proving that the 2nd stage CA module indeed estimates the reliability of the confidence intervals); so there is some missing insight on why joint training could be beneficial.  A clearer message, making weaker claims and experiments that clearly back those claims will make the paper stronger. For example, (softening claims about CA module:) The paper introduces one ad hoc procedure (CA module) and shows that it is fit for purpose. No claim that it is efficient relative to baselines, but it still needs to justify why CA module should be preferred compared to any other probabilistic classification approach. (softening claims about better intervals:) joint training works better than stage wise training (which, by definition, leaves the prediction intervals unaffected). Unclear as to why that should happen in general; two special cases are mis specification and heteroskedasticity.  
The initial round of reviews showed a consensus among the reviewers that the presentation of the paper was poor, the novelty was unclear, claims were not properly justified, and the experimental evaluation and discussion were quite insufficient. The authors provided a rebuttal and an updated version of the paper. Although the updated paper demonstrated that the proposed approach indeed provides some benefits, it appears that the authors were not successful to address the numerous but constructive reviewers  comments.  The paper is not ready for publication in ICLR 2021 and can benefit from major revisions and careful proofreading. 
After carefully reading the reviews and the rebuttal, and after going over the paper itself, I m not sure the paper it ready for ICLR. I do believe there is a lot of useful content in the current manuscript, and I urge the authors to keep working on the manuscript and resubmit it in due time.   My concerns are as follows:  (a) there is a lot of discussion about *relational information retrieval*   however there is lack of any formalization of what this term means. I don t mind relational reasoning to be used as motivation, but when it is used to consider what are valid baselines and what are not, I feel compelled to understand what exactly it means. Why is *self attention* retrieval not *relational*? Beside the task being seemingly relational in spirit, how do we test whether the retrieved mechanism carries any relational information whatsoever? I think the community had a learning lesson here in CLEVER dataset, which arguably does not require as much relational reasoning as it seemed. So I agree with Rev5, that there is a decent probability that the task we are using do not require relational information retrieval. While I understand that some of these systems are Transformer inspired, I feel transformer should be a baseline.   (b) I also feel the paper should take one of two paths.           Either embrace larger scale tasks and baseline outside of the relational reasoning literature (like transformer) and particularly settings where potentially self attention will struggle due to the quadratic term or where they tend to be hard to train due to the difficulty of doing credit assignment through the attention mechanism           Provide more careful ablation studies and formalize the claims a bit more. Regarding e.g. the discussion of a single larger memory vs multiple memory blocks. One of the main difference comes from the attention over which memory block to use in the proposed approach, which due to softmax has a unimodal behavior. So is the reason why it works better this potential hiding of part of the memory representation (so a better way of reading a subset of the memory entry). This could potentially be done differently (e.g. multiplicative interaction in the same style, for e.g. that they were used in WaveNet). This is just a random thought on this particular aspect. I have similar questions about the self supervised loss.    I find the paper focusing on improving performance (unfortunately on toy domains) rather than ablation studies and an understanding and careful understanding of how things works. I realize there is some such analysis in the appendix. But I feel more of it should be in the main text. The paper is either proposing something that scales and works well at scale (and then understanding why is less important as it has direct application) or explores a very specific phenomena and then is fine to stay on toy tasks but there should be a bit of clarity in the claims, and an investigation whether the hypothesis (or intuition) put forward initially is the reason why the model works.  
This paper was referred to the ICLR 2021 Ethics Review Committee based on concerns about a potential violation of the ICLR 2021 Code of Ethics (https://iclr.cc/public/CodeOfEthics) raised by reviewers. The paper was carefully reviewed by two committe members, who provided a binding decision. The decision is "Significant concerns (Do not publish)". Details are provided in the Ethics Meta Review. As a result, the paper is rejected based Ethics Review Committee s decision .  The technical review and meta reviewing process moved proceeded independently of the ethics review. The result is as follows:  This paper studies the problem of evaluating optimiser s performance, which is important to show whether real progress in research has been made. It proposes several evaluation protocols, and used Hyperband (Li et al. 2017) to automate the tuning of each optimiser in the bench marking study. Evaluations have been conducted on a wide range of deep learning tasks, and the paper reaches to a conclusion that none of the recently proposed optimisers in evaluation can uniformly out perform Adam in all the tasks in consideration.  Reviewers agreed that the evaluations are extensive, however there are some shared concerns among reviewers. The paper argues that manual hyper parameter tuning by humans is the right behavior to target for, which is the motivation to use Hyperband as an automating tool, and there is a human study to demonstrate that Hyperband tuning resembles human tuning behaviour. Some reviewers questioned about this desiderata choice that favours human tuning behaviour, also concerns on how the human study is conducted (and to what extend the human study itself is reflective enough for the human tuning behaviour in general).  Personally I welcome any empirical study that aims at understanding the real progress of a research topic, and I agree it is important to make rigorous automation tools in order to enable such a large scale study. Therefore, while the presented results are extensive, I would encourage the authors to incorporate the feedback from the reviewers to better examine their assumptions. 
The paper proposes a method to improve adversarial robustness by diversifying the ensemble.   Novelty: As pointed out by several reviewers, promoting diversity of ensembles has been done in the literature, but there s still a moderate novelty in proposing the DML layer.   Empirical validations: The original submission lacks many important comparisons (e.g., with [1]). Despite the authors implicitly compared their method with (Pang et al) via Auto attack in the rebuttal, it will be better if the comparisons are conducted in a well controlled way to confirm that the improved robustness comes from DML instead of other hyperparameter settings. Further, it is not clear whether the proposed method is robust to hyperparameters.   Based on these, we recommend rejection but encourage the authors to improve their paper based on the comments. 
This work proposes a new architecture for solving Ravens Progressive Matrices (RPM), a well known form of visual reasoning problem. The method relies on an operation that directly compares the final row and column (completed with different candidate answers) with the first two rows and columns. Doing this allows the network to perform better than previous approaches when measured on an in distribution test set on two RPM datasets, RAVEN and PGM.   As the reviewers pointed out, a strength of this work is the strong performance on the neutral split of these datasets and the fact that the methods do not (unlike some other approaches) require access to any annotations from the dataset other that knowledge of the structure of the RPM task and access to the candidate answers and the correct answer.   However, a noted weakness is the fact that the network reflects the structure of the task more directly than other approaches, which means that the insight is specific to the problem of solving RPMs. Another weakness is that the authors focus on the neutral (in distribution) splits. Reading the PGM paper, it is clear that the neutral split is not really the main focus of that dataset (it accounts for only 1/7 of the dataset), which seems to have been specifically developed as a benchmark for out of distribution generalisation. Indeed, the whole point of the RPM task is to measure the ability to induce abstract rules and principles from pixels, but without measuring out of distribution generalisation, can we really claim that any model has induced a  rule ?   The authors mitigate this issue to a small degree during the rebuttal by adding scores on the  interpolation  and  extrapolation  splits of the PGM dataset, but still do not consider the other splits where rule application is most clearly tested.   I note that the weakness described above also applies to lots of other published work involving PGM and RAVEN datasets.   In summary, this is a well executed, neat piece of work that shows a better way to fit a large dataset by incorporating knowledge of the structure of the data into the task. Because it does not consider the full benchmarks, only the in distribution splits, it falls short of showing that this enables better induction of abstract principles or rules. On the majority opinion of the reviewers, and because there are no scientific flaws in the work, I recommend acceptance with weak confidence pending wider calibration across the program. 
This paper builds upon recent iterative refinement approaches NMT with an evaluator model that controls the termination of the translation process, yielding a “rewriter evaluator framework” for multi pass decoding. Their approach is an alternative to the policy network used in Geng et al (EMNLP 2018). The main delta wrt previous studies is that the evaluator offers this framework the capability of flexibly controlling the termination. While the idea behind the rewriter evaluator framework is sensible and well described, and the proposed method achieves significant performance improvement against reported baselines, reviewers pointed out some concerns with the baselines and model optimization details. More analysis of the termination procedure against the RL based model of Geng et al. 2018 could shed some light on why the proposed approach is better. Some analysis testifying how many iterations the model uses for translating one sentence, and what factors could affect the iteration number, such as sentence length, would greatly improve the paper. A second weakness pointed out by reviewers is related to the results of WMT’15 En De reported in Table 1, where the reported baseline numbers seem to be weaker than expected. As pointed out by one the reviewers, pre trained checkpoints on English >German (available at https://github.com/pytorch/fairseq/tree/master/examples/translation) exist which achieve much higher sacre BLEU than the reported baseline. I found the authors’ answer not very convincing regarding this point. Therefore, I recommend rejection. I suggest the authors, in future iterations of their work, address some of the issues pointed out by the reviewers and re implement their method following the settings in (Ott et al., 2019) to get more convincing results. 
The paper considers learning settings with distributional change. It makes a lot of assumptions to obtain sample complexities that justify the use of empirical invariant risk minimization, and falls a bit short by not giving a formal converse for the inadequacy of plan empirical risk minimization, despite making the claim. Nevertheless, the contributions are insightful, and the paper may be worth sharing with the community. The grading were overall positive from the reviewers, though particularly critical, and I doubt the whole paper could be fully double checked: one could question the ability of the reviewers to perform a deep analysis on a 48 pages theoretical paper in the time constraints imposed by a conference model... 
The novelty of the claims of the paper has been challenged by one of the reviewers, and in addition, one reviewer also raised concerns about the validity of the proof of the main result (Theorem 1). I looked into the proof myself, and I agree with these concerns. The authors did not use the rebuttal phase to clarify these issues.
The paper addresses an interesting problem of clustering/link prediction/representation learning of signed graphs, where edge weights are allowed to take either positive or negative values. The paper proposed an end to end pipeline targeted at link sign prediction and the feature diffusion step. The reviewers think the proposed method is a straightforward integration of existing methods, and the convergence result is straightforward. The paper can be improved by including more novel ideas or analysis. 
This paper proposes an approach for improving MultiTaskLearning by providing a way of incorporating task specific information.  Pros: 1) All reviewers agreed that the paper is clearly written 2) Interesting to see a single model for AST, STS (speech to speech translation) and MT   Cons: 1) The work is not adequately compared with related work (some important references are also missing)   The authors did perform some additional experiments with T5  and pointed out some drawbacks but this needs to be explored a bit more. 2) The answers about scalability are not very convincing and need more empirical results.   Overall, none of the reviewers were very positive about the paper and felt that while this is a good first attempt, more work is needed to make it suitable for acceptance. 
This paper presents an analysis of different tricks for training the super network in NAS. While all reviewers see value in some of the many experiments, all reviewers also have substantial criticisms of the paper, and all reviewers gave weak rejection scores.  Looking at the paper myself, I agree with this assessment. Several of the experiments are valuable, but there are also several substantial issues.  One question that confused two reviewers and myself is about using sparse Kendall s tau as a metric that the authors in the rebuttal again state can be computed during super net training to evaluate the quality, just like super net accuracy. I don t see how that is possible. Kendall s tau measures the correlation between the ranks of the performances of the stand alone architectures and the ordering implied by the super net. Computing this requires access to the performance ranks of the stand alone architectures. For tabular benchmarks this is of course available, but not in practical NAS applications.  I would also like to echo the concern of AnonReviewer2 that too little information is given to fully understand what is shown in Figure 10.  Some reviewers also questioned inhowfar the results generalize to the setting of the Once for all network or BigNAS. This was not a deciding factor for me, since insights based on NAS Bench101, 201 and a DARTS like search spaces are already very useful.  I agree with the reviewers that the authors  use of "proxy" is highly misleading. It is standard to refer to the low fidelity model used for training as the proxy model. In contrast, the authors use it for the final evaluation model.  Concerning the authors  five final take aways: 1) I don t see how sparse Kendall s tau is actionable. 2) The batch normalization part is interesting, and I agree with the authors that it is useful to spell this out and analyze it, rather than just having one sentence in the paper as NB 201 and TuNAS, but the attribution that this has been done before is broken. "In contrast to X", rather than "Like X" 3) This is interesting, although I agree with AnonReviewer3 that I m lacking intuition why a smaller learning rate should be useful for a less smooth space 4) The experiment on low fidelity estimates is very misleading. The proxy settings used during training are already low fidelity evaluations   for the final evaluation, you would increase the number of channels, number of layers and number of epochs. Stating that the use of low fidelities is not useful is highly misleading. The authors  experiments only shows that the proxy model is already well chosen, and that if you reduce #layers or #channels and proportionally increase #epochs, performance gets worse. I encourage the authors to try searching without this proxy model, and I m sure they will find that (which correlations might increase) the search process will be far too slow. 5) The insight on dynamic channeling appears very useful to me.  In summary, I recommend rejection and encourage the authors to address the points raised by the reviewers and in this meta review.
The idea behind this paper is to develop a training algorithm that chooses among a fixed set of weights for each true weight in a neural network.  The results are reasonable   though difficult to quantify as either good or surprising   performance from the algorithm. A perhaps interesting point is that additional fine tuning from these found networks can, in some cases, best the accuracy of the original network.  The pros of this paper are that it is a neat original idea. With the exception of the limited scale of the benchmarks (i.e., the selected architectures), the paper is largely well executed.  The primary shortcoming of the paper, as discussed by the reviewers, is the lack of clarity in its implications. Specifically, it is difficult to position the result as contributing to a practical aim or leading to additional future work.    Based on the reviews and discussion, my recommendation is Reject. In particular, this paper would be significantly improved by bringing in a strong motivational context and, therefore, additional comparisons.  For example, the context for the work of Ramanujan et al. (2019) is that, perhaps, it is possible to find subnetworks of large initialized networks that will permit more efficient training. In Appendix A, this paper proposes that the technique here could be cast as pruning within a much larger network. Following results from Zhu and Gupta [1] and also Ramanujan et al. (2019), finding a sparse network within a larger network can produce a more accurate network than training a network of equivalent size to the sparse. Therefore, these results could, potentially, be cast and as a more efficient way to perform the techniques of Ramanujan et al. (2019).   Alternatively, the results that demonstrate that fine tuning the identified networks improves performance over the standard network could be more robustly evaluated and perhaps cast as either an alternative training technique or leveraged as a technique like warm starting [2].  This is a very interesting and promising direction. It appears that the paper just needs a bit more distillation.  [1] To prune, or not to prune: Exploring the efficacy of pruning for model compression. Michael Zhu and Suyog Gupta. In International Conference on Learning Representations Workshop Track, 2018.  [2] On Warm Starting Neural Network Training. Jordan T. Ash, Ryan P. Adams. NeurIPS 2020   
The reviewers raised a number of concerns about the novelty of the paper and comparisons. The authors were able to address the concerns regarding the comparisons in the response, and the reviewers unanimously agree that the paper should be published. I do think however that this paper is quite borderline. I agree with the reviewers that the updated experiments are convincing in terms of the provided comparisons. However, the reservations I have about the work can perhaps best be stated as follows: There is quite a bit of work in the area of imitation from observations, which makes a range of different assumptions and utilizes a variety of different domain adaptation techniques. Much of this work is in the robotics domain (which is cited in the paper), and much of it demonstrates results in fairly realistic settings, often with real humans and real robots. In comparison, the experiments in this paper are quite simplistic, using toy domains and "demonstrations" obtained from a computational oracle (i.e., another policy). Given the maturity of this field and the current state of the art, I am skeptical of this evaluation, and I think TPIL is a very weak baseline. That said, I would  defer to the reviewers in this case   I do think the particular technical contributions that the paper makes are a valuable addition to the literature, though somewhat incremental. I am also sympathetic to the authors in that much of the more successful prior work in this area that does evaluate under realistic conditions makes subtly different assumptions, or utilizes different techniques for which it is difficult to provide an apples to apples comparison.  One thing I would request of the authors for the camera ready though is: Please tone down the claims. "Human like 7 DOF Striker" is not human like, it s a crudely simulated robotic arm that was recolored. It would of course be better to have a realistic evaluation (as many prior papers in this field indeed have), but in the absence of that, it is best not to overclaim and be upfront that the evaluation is on relatively simple simulated tasks under conditions that are not necessarily realistic (and have nothing to do with actual humans), but meant rather to evaluate in an apples to apples manner the particular algorithmic innovations in the method.
The paper presents a stochastic variational inference method for posterior estimation in a Cox process with intensity given by the solution to a diffusion stochastic differential equation. The reviewers highlight the novelty of the approach. Some of the concerns with regards to clarity have been addressed by the authors satisfactorily.   However, an important issue of the approach is that of estimating model parameters, which the authors do not address explicitly by simply referring to that as the task of the modeller. I believe this is an important issue and, although some of the parameters can be estimated along with the neural network parameters, this has not been shown empirically. Along a similar vein, the paper only presents results on a single real dataset (the bike sharing dataset), which questions the applicability of the approach and no other baseline method is presented. At the very least, the authors should have provided an objective evaluation to other doubly stochastic point process models, e.g. based on Gaussian processes, where modern stochastic variational inference algorithms have been presented.  
Although the proposed method shows sota results, it is a simple combination of two existing methods, a bit of Bayesian + domain generalization.  It seems that the total improvement by the proposed method is just the sum of improvements by Bayesian and by domain generalization.  No synergy between Bayesian and domain generalization is observed.  I personally doubt that the Bayesian treatment of the domain generalization loss is not essential. The derivation in Sec. 2 is unnecessarily complicated.  In derivation from eq(3) to eq(5), the authors first "extend" (3) to (4), which is not appropriate ((4) can hold even if $p(y_{\zeta}|x_{\zeta})$ is highly diverse. ).  After that the authors apply Jensen to come back to an appropriate form (5), which is a weighted sum of distances (which is an appropriate criterion). If they start from Eq.(5), the proposed objective is simply the sum of the standard ELBO (2) and a natural domain invariance loss (5).  For non Bayesian treatment of the domain invariant loss, you could simply replace the KL by Lp norm between $y_s$ and $y_\zeta$.  I expected that by answering to the question by Reviewer 1 the authors would prove synergy between Bayesian and domain generalization.  But the authors just excused that   The feature distributions  are unknown without Bayesian formalism, leading to an intractable $L_I$. Therefore, we do not conduct the experiment with only the domain invariant loss on both the classifier and the feature extractor.  I don t really understand what the authors mean, but the authors should have explained why you cannot replace the KL with non Bayesian Lp loss.   
This paper proposes a novel technique to learn a disentangled latent space using VAEs and semi supervision. The technique is based on a careful specification of the joint distribution where the labels inform a factorisation of the distribution over continuous latent factors. The technique allows for inference, generation, and intervention in a tractable way.  The paper is well written, the formulation is original, and the experiments convincing. There were some confusions that were mostly resolved during the discussion.    In addition to the expert reviews attached, I would like to remark that I too find the formulation interesting and elegant. And if I may add to the discussion, oiVAE (output interpretable VAEs) by Ainsworth et al presented at ICML18 is a related piece of work that did not occur to me earlier, but which the authors could still relate to (I d certainly enjoy reading about the authors  views on that line of work). 
Summary: This paper introduces a method to try to learn in environments where a person specifies successful outcomes  but there is no environmental reward signal.  I d personally be interested in knowing where people were able to easily provide such successful outcomes instead of, for instance, providing demonstrations or reward feedback. Similarly, I d be interested in how other methods of providing human prior knowledge compared.  Discussion: Reviewers agreed the paper was interesting, but none of the 4 thought the paper should be accepted.  Recommendation: While I do not think this paper should be accepted in its current form, I hope the authors will find the comments and constructive criticism useful.
The paper is about adapting a voice generation model to new speakers with minimal amount of training data. The key insight in this paper is that the voice can be adapted using a small set of variables   the bias and the variance associated with the layer that normalizes the mel spectrogram associated with the decoder. Additionally, they characterize voice at the utterance level to capture stationary factors like background acoustic conditions and at the phoneme level to capture factors such as prosody, though there are no explicit constraints to force such representation.  The strength of the paper are: + Simplicity of the approach + Empirical evaluation that demonstrates its effectiveness  The weakness of the paper are:   analysis of what the crucial parameters of the model represent   lack of clarity that is obvious from several back and forths between the reviewers and the author.  A few examples include:   “There is also a phoneme level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme level idiosyncrasies of the speaker, although this isn t clear to me.”   “ it is only the speaker embedding that is the input to fine tuning, and this only via the normalization parameters. (Both the normalization parameters and the speaker embedding itself are fine tuned.) I am not sure what the speaker embedding is left to do with all this acoustic level input, but OK.”
This paper proposes two methods to speed up the evaluation of neural ODEs: regularizing the ODE to be easier to integrate, and adaptively choosing which integrator to use.  These two ideas are fundamentally sensible, but the execution of the current paper is lacking.  In addition to writing and clarity issues, the main problem is not comparing to Finlay et al.  The Kelly et al paper could potentially be considered concurrent work.  I also suggest broadening the scope of the DISE method to ODE / SDE /PDE solvers in general, in situations where many similar differential equations need to be solved, amortizing the solver selection will be worthwhile even if there are no neural nets in the differential equation.  I also encourage the authors to do experiments that explore the tradeoffs of different approaches, rather than aiming just for bold lines in tables.
After reading the author’s response, all reviewers recommend accepting the paper.   The authors provided an extensive response carefully considering all reviewers  comments. After incorporating the feedback, the manuscript improved in terms of presentation, relation to the literature and empirical results.  The paper is very well written and motivated. On top of the insightful analysis, experimental results are strong, obtaining comparable performance to that of a ResNet 18 on ImageNet.  R1 and R3 strongly support the paper while R2 and R4 consider it borderline.  R2 raised questions about experimental details and reproducibility. While R2 did not comment, these concerns were very clearly addressed by the authors in the view of the AC.  R4 was initially concerned with the novelty of the approach, but changed their mind after the author s response. The AC encourages the authors to further consider the feedback provided by the reviewer after the discussion period was over. 
This paper considers the problem of sequential decision making through the lens of submodular maximization. I read the paper myself and found the idea quite appealing and interesting. The authors also make a very effective rebuttal and brought a borderline paper into a clear accept. 
This paper received overall positive scores. One reviewer (R3) recommended clear reject.   All reviewers agree that the paper introduces a novel idea and its effectiveness is supported by the experimental results. There are concerns about clarity of presentation and certain missing analyses, which have been addressed by the authors in the rebuttal. Thus the ACs recommend acceptance.  
The paper suggests a procedure to efficiently adapting a learned neural compression model to a new test distribution. If this test distribution has low entropy (e.g., a video as a sequence of interrelated frames), large compression gains can be expected. To achieve these gains, the method adapts the decoder model to the new instance, transmitting not only the data but also a compressed model update. Experiments are carried out on compressing I frames from videos, while comparisons comprise baseline approaches that finetune the latent representations of videos as opposed to the decoder.   The paper’s main contribution is very timely and relevant. While it was well known in the classical compression literature that model updates could be sent along with the data (e.g., as already done in “optimized JPEG”), this is the first time the idea was implemented in neural compression. The experiments are arguably the paper’s weaker part and were originally a concern, but they have been significantly improved during the review period such that all reviewers voted for acceptance. We encourage the authors to further strengthen their experimental results by adding more challenging baselines on well established tasks (e.g., image compression). 
After reading the reviews, rebuttal, and looking through the paper I do feel that UPL setting is one that we need to consider. However is not clear to me that proposed approach matches the conditions described by the authors. In particular the scalability constraints seem important. I do feel that for the UPL setting makes sense particularly in the large case scenario of many examples and classes and how the system behaves under strict computational budgets for learning and inference. And I m wondering whether in that limit parametric models would actually becomes relevant again, and whether there is a "burn in" that one has to pay to use parametric models.  That said I don t think current approaches (CURL, AGEM etc.) will do well even in that setting, partially because they were not necessarily thought for that.  So in summary, I find the problem interesting, probably more so than the solution and particularly in an large scale setting.  However I think for the paper to have the impact it needs, and be ready for acceptance it needs a bit more. I think looking at a larger scale setting, and relying on that to motivate the problem will considerably help with its impact.  Also is not clear to me how the proposed solution scales (non parametric approaches don t always do well in large scale settings), which I think is needed for it to be convincing. 
The paper tries to argue the value of making ensembles more reproducible through the use of a correlation loss to try to make components as different as possible. The paper is tough to follow and the high level motivation is unclear. As one of the reviewers points out, don t ensembles provide an estimate of uncertainty and  calibration?  Further, the experiments were quite limited. Studying the proposed approach in a small, controlled setting might also be revealing.
 The paper proposes to use a regularization term for stabilizing the perturbation trajectories in generating adversarial examples for medical image tasks. The authors tested the effectiveness of their proposal on different medical image datasets obtained by different modalities, and the experimental results are generally encouraging. All the reviewers see the value of the paper and give positive comments. At the same time, they also point out some aspects for further improvement, including 1)	The datasets used are relatively small 2)	The title is a little misleading since the paper only tackles the image attacks (but the title is stabilized medical attacks). 3)	Case studies and visualization are needed to help people better understand the paper  The authors have done a good job in their rebuttal and paper revision, by adding experiments on larger datasets, changing the title to “stabilized medical image attacks”, and adding some geometric figures for better illustration. These have largely addressed the concerns of the reviewers, and we see no problem with accepting the paper. 
The authors address the problem of fine grained image classification. They propose a batch based regularizer, called the batch confusion norm (BCN), to encourage less over confident predictions. They also tackle the problem of class imbalance during training by adaptively weighting the BCN loss at the class level to take the imbalances in the underlying label distributions into account. Results are presented on four different fine grained datasets.    Overall, while the reviewers had some positive comments, there was not broad support for the paper. There are questions that need to be resolved related to the evaluation e.g. the best performing model uses GASPP, however there is no reported GASPP variant for the PC baseline. Similarly, it would be valuable to know how much PC would benefit from an additional class imbalance term in the iNaturalist2018 results. Given that the proposed regularizer builds on PC (Dubey et al.), it is very important that the authors provide a like for like comparison so that readers can better understand the merits of the proposed method.    There were also concerns with the presentation of the paper e.g. several typos (which can be easily fixed), issues with the clarity of the text (which require more work), and uninformative figures (e.g. Fig 2 should be revised to more clearly illustrate the differences between the three methods shown). The authors are encouraged to revise the text to resolve these problems.   While the paper has some strengths (e.g. the empirical performance on some of the tasks is promising and the method is conceptually simple), there are still a number of concerns from the reviewers e.g. a lack of a clear motivation as to why the proposed method works, and why it is conceptually better than existing alternatives (e.g. PC). Given this lack of support, it is not possible to recommend the paper in its current form.  
Reviews for this paper were quite mixed (7744), and none were exactly borderline. All reviews were detailed and informative, as was the rebuttal. The main criticisms were (1) lack of detail in the experiments, and some missing evaluation (2) missing related work, (3) overall lack of polish (mentioned among positive reviews too), and (4) some unsubstantiated claims. Positively, reviewers praise the novelty, dataset, the demo, and some reviewers found the experiments mostly convincing.  Ultimately this is still a borderline decision. The rebuttal does appear to address many of the claims about missing evaluation, and the complaints about polish can be easily addressed. I think the unsubstantiated claims are reasonably rebutted too. Related work doesn t seem to be addressed in the rebuttal.
This paper explores methods for pruning binary neural networks. The authors provide algorithms for developing sparse binary networks that perform okay on some basic ML benchmarks. They frame this as providing insights into synaptic pruning in the brain, and potentially providing a method for more efficient edge computing in the future.  All four reviews placed the paper below the acceptance threshold. The reviewers noted that the paper was hard to follow in several places and were unsure as to the motivations. The authors attempted to address these concerns in their replies, but the Area Chair felt that these were insufficient.   As well, the Area Chair notes that some of the claimed contributions of the paper are questionable. Specifically:  (1) The claim that there is anything biologically plausible about the algorithms presented here is very suspect. The brain cannot use a search and test system for synaptic pruning like the algorithms proposed here. Thus, it is unclear how this paper provides any insight for neuroscience. In fact, the authors do not even really try to provide any neuroscience insights in the results or discussion. Moreover, they don t actually appear to use any neuroscience insights to develop their algorithms, other than the stochasticity of the pruning (though note: it is not actually clear in neuroscience data whether pruning is stochastic). Given the ultimately very poor performance on ML tasks, the paper doesn t seem to provide anything particularly useful for application in ML either.  (2) The claim that the provide, "The demonstration that network families with common architectural properties share similar accuracies and structural properties." is odd. Surely this is the null hypothesis anyone would have about ANNs? It would be surprising if networks with common connectivity profiles (which is what the authors mean by "architecture") didn t share similar performance!  (3) The claim that searching in architecture space like this leads to "architecture agnostic networks" is odd... As noted by Reviewer 2, the authors are really just specifying algorithms for sparsifying binary neural networks, which they frame as being "architecture agnosticism" according to a rather strained definition. There are other ways of approaching the sparsification of neural networks, and of doing architecture optimization, but the paper is not framed as contributing to this literature.  Altogether, given these considerations, and the four reviews, a "Reject" decision was delivered.
This paper presents a novel approach to producing saliency maps for interpreting deep neural networks.  In general this paper seems quite close to borderline, although on the positive side, with some low confidence reviews.  The reviewers felt that the proposed approach could be useful to the community and they seemed to feel that the qualitative results in the experiments demonstrated convincing saliency maps.  There were some concerns, however, about the quantitative experiments as the reviewers (e.g. AnonReviewer2) found that while the mean results seemed better, it wasn t clear if they were statistically significant.  Naturally, the qualitative experiments are highly subjective and there was disagreement between reviewers whether the proposed approach did indeed produce better saliency maps than existing approaches such as smoothgrad.  One reviewer indicated that they found it difficult to follow the paper and to understand the decoy concept given the writing.  During discussion AnonReviewer2 updated their score (and very thorough review) by 2 points to indicate a weak preference toward accept.  None of the reviewers argued particularly strongly for acceptance and "championed" the paper.  The low confidence, slightly above borderline reviews seem to suggest that the reviewers thought the paper was above the bar but were reluctant to argue strongly for acceptance.  The method seemed like it could be useful to them but they weren t clearly convinced that it set a new state of the art given the quantitative and qualitative empirical results.  
This paper introduces and analyses a method to train a population of VAEs with mixed continuous (referred to as "style") and discrete (referred to as "labels") latent variables. The population is trained under the constraint that inferred discrete latent variables  to be the same for all models. The paper also investigates a data augmentation mechanism inspired by  (Antoniou et al., 2017). The presentation is overall clear and the idea is interesting, although the language of "agents" is not standard in generative model literature and is a bit confusing. The experiments also show very good clustering results of the proposed method. Unfortunately the pipeline was determined to be quite complex while the motivation for its design choices were unclear. This, combined with multiple concerns about the experimental validation, led to a reject decision. 
This paper got 3 acceptance and 1 marginally below the threshold. After the rebuttal, the rating was raised to above the threshold. All the reviewers are positive about this submission. They agree that the method proposed in the submission is novel, the experiments are comprehensive and convincing. AC agrees and recommend acceptance.  
This paper proposes a regularization approach based on the second order Taylor expansion of the loss objective to improve robustness of the trained models against \ell_inf and \ell_2 attacks. It is interesting to explore the second order based regularization approach for network robustness. However, as pointed out by the reviewer, a major drawback of this approach is that SOAR is broken under a stronger attack   AutoPGD DLR.  In addition, the theoretical bound seems very loose in the \ell_inf case.  
Initially there were some shared concerns about the work being too incremental, lack of technical clarity on the algorithmic side and experiments, and lack of clear mathematical formulations. The authors did a good effort and cleared up many questions and remarks satisfactorily, and several reviewers have increased their scores as a consequence. In its current state I recommend to accept the paper.
Although the paper presents some interesting ideas, in general the reviewers agree that the paper lacks clear results and is not an easy read. The paper proposes a factorisation of value functions, a topic that has received quite some attention in the literature (e.g. QPLEX), and it seems that their is not sufficient innovation in the proposed method in the paper. There are also a number of claims in the paper (e.g. partial observability etc.) with which some of the reviewers disagree, and should be discussed more carefully in a revised version of the article, that all in all seems to need more work.
The paper was discussed by the reviewers that acknowledged the rebuttal and the authors’ responses. In particular, they appreciated the fact that some of their concerns were alleviated (e.g., going beyond the single ImageNet evaluation).   More generally, while all the reviewers thought that the problem tackled by the paper was of clear interest (i.e., full end to end auto ML encompassing DA, NAS and HPO), they still expressed concerns (even after the rebuttal), in particular:  * _Clarity of the methodology_: None of the reviewers could clearly and fully understand the mathematical formulation of the joint optimization, leading to a series of questions regarding the confusing usage of the training/validation set in the experimental setup. This unfortunately made the assessment of (some aspects of) the paper speculative for the reviewers. * _Comparison with AutoHAS_: AutoHAS and DiffAutoML are obviously related methods. Even if AutoHAS has weaknesses compared to the proposed approach DiffAutoML, e.g., discretization of the continuous hyperparameters and no tuning of DA, it is still meaningful to carry out an actual comparison (possibly normalized by the different costs at play since the authors have highlighted the different memory overheads). Though the listed weaknesses of AutoHAS _should_ play in favor of DiffAutoML,  a proper experimental comparison would better support that claim.  Given those remaining concerns and the overall mixed scores, the paper is recommended for rejection. The detailed comments of the reviewers provide an actionable list of items to improve the paper for a future resubmission. 
This paper makes use of the unlikelihood objective from Welleck et al (2019) which was shown in NLP to the problem of forecasting motion trajectories on roads. The unlikelihood term is meant to lower the probability mass in non driveable areas. The paper makes use of Trajectron++, and existing trajectory forecasting model to demonstrate the idea. While the idea is interesting, the notion of using negative examples to lower the likelihood outside a valid domain has been used in multiple occasions. The paper mentions contrastive learning, but I did not see a meaningful discussion on the difference between unlikelihood training and contrastive learning, beyond what exists in the related works section. Also, due to the unlikelihood term having appeared in Welleck et al, reviewers are hesitant to acknowledge novelty of the method. One of the reviewers also questions the significance of the results, which the authors countered by saying that their method reduces the violation rate from 10.6% to 8.9% in their predictions. This is good, but combined with the former issue implies that the paper needs more work before publication.    
In this paper, the authors change the loss function of NNs to reduce the separability of the different classes in one of the hidden layers. The rationale for this assumption that the trained network will be more robust against white box model inversion attack. The reviewers all concur that the paper had some merit, but that the paper is not well presented and believe the paper is not ready to be presented at ICLR.  Also, the separability issue is not totally explained, because a reduced L2 norm might not be the whole story that explains why a white box model inversion would rely on for leaking information. This might need to be proof further and a couple of experiments in which there is still leakage of information shows the additional robustness from the new penalty. 
This paper proposes a weakly supervised model for numerical reasoning. After discussion with the reviewers it seems that it is already known that training NMNs directly on DROP is not successful and requires taking additional measures. Past work (NERD) has resorted to using data augmentation, and this work encodes it directly to the model. This paper needs to show the advantages of their approach and that it generalizes better to other scenarios. Other minor issues include (a) clarity fo writing (b) focus on a subset of questions (c) no evaluation on other numerical datasets (d) mild inaccuracies w.r.t prior work (GenBERT)
The paper proposes the use of topological similarity between conditional submanifolds for a given latent dimension as a metric for measuring disentanglement in generative models. To estimate the topological similarity between conditional submanifolds, the authors build upon an earlier work of Relative Living Times (RLT).   R5 and R4 had concerns on the paper, particularly about the lack of enough novelty in the actual technique (R5) and about the lack of convincing experiments (R4). One of the concerns raised by R4 was around the discrepancies between MIG and FactorVAE. However as noted by other reviewers (R2 and R5), these discrepancies between different popular metrics are well acknowledged in the literature and authors have responded to this point. R2 and R5 also appreciate that avoiding the rotation issue faced by most of these disentanglement metrics is one of the strengths of the proposed metric.  While I tend to agree with R5 that the actual technique is inspired from the earlier work on "Geometry Score", I also think the application to measuring disentanglement in generative models is a novel contribution in itself, especially because current metrics have issues as pointed out by other reviewers   the paper provides a fresh conditional sub manifold perspective on disentanglement and a theoretically sound metric for measuring disentanglement.   Considering this novel perspective and a resulting theoretically sound metric for measuring disentanglement that addresses some of the issues with current metrics, I recommend accepting the paper.  
 The paper proposes the novel task of detecting hallucinated tokens in sequence generation, and a strategy to train such models using artificially generated samples. The methods show reasonable correlation with human judgements.  The expert reviewers are unanimous in their lack of enthusiasm about this work, with overall borderline assessments. The reviewers provided some suggestions for improvement, and it is worth remarking that the authors provided an impressive amount of work in the revised version, addressing the suggestions. Specifically, they added baselines that validate that the task is non trivial, and the case study on improving machine translation.  In the discussion period, the reviewers appreciated the additions, and some increased their rating, but the overall assessment remains borderline.  The reviewers find the work lacks the expected amount of depth.  Some concerns emphasized in the discussion period involve insufficient empirical analysis (e.g., more NMT datasets and and analysis); understandable as this work was added after submission, but still important.  A reviewer stresses concerns about the definition of the task itself, which I agree is vague ("... cannot be entailed by the sentence") and does not match the synthetic data generation entirely, leading to unfortunate edge cases involving synonyms or   worse   slight narrowing that technically would still be entailment but maybe should be considered unfaithful.  This casts doubt on the human evaluations and on considering the task itself a main contribution, therefore leading to the empirical framing that the reviewers perceive and expect.  It also seems to me that there is a incremental, cat and mouse spirit to predicting automatically generated hallucinations. In short, it seems like this paper is caught in between trying to be a significant empirical contribution and a linguistically well motivated task and annotation project, and I understand that the reviewers would prefer committing to one of these directions.  While I encourage the authors to pursue this direction more deeply, in light of the borderline reviews, I do not recommend acceptance.
This paper presents a pre training strategy for learning graph representations using a graph to subgraph contrastive learning objective that also simultaneously discovers motifs. Pre training for graph representation learning is an important research topic and this work presents a unique solution leveraging the fact that graphs sharing a lot of motifs should be similar to one another. The approach is novel and interesting, the ability to simultaneously identify motifs are highly desirable. The results are promising showing that the proposed approach, when pretrained on the ogbn molhiv molecule dataset, worked well for several downstream chemical property prediction tasks.   However, the paper is not without weaknesses and the reviewers noticed several of them. There are many parts of the system, the graph segmenter, which relies on spectral clustering (on the affinity matrix), the EM style clustering component to extract the motifs based on the subgraphs,  the sampling loss based on the subgraph to motif similarity, and the graph to subgraph contrastive learning loss. These parts are tied together through different mechanisms and the training procedure becomes very confusing. It is unclear which parts are updated on the backpropagation path from which loss, and what choices are decided offline (i.e., not integrated into the backpropagation).  This presents great difficulty in understanding and probably using /building on the method. The paper has improved some aspects of its presentation during the review/discussion process, but the training/optimization procedure of the current version still appears quite opaque, and the reviewers heavily relied on the back and forth discussion to understand what is really going on.   Another concern is that the intuition behind some aspects of the approach and the connections between different components of the approach are a bit difficult to get/digest at places.  The intuition behind graph to subgraph contrastive learning appeared weak to the reviewer. It would be desirable to see a directly comparison to the subgraph to subgraph version. The connection between the motif discovery and the representation learning can be somewhat lost as we try to keep the many moving parts straight in the mind.   For these reasons, the paper, in its current form, cannot be accepted.
All reviewers seems in favour of accepting this paper, witht he majority voting for marginally above acceptance threshold.  The authors have taken special heed of the suggestions and improved the clarity of the paper.  From examination of the reviews, the paper achieves enough to warrant publication.  My recommendation is therefore to accept the manuscript. 
The reviewers liked the direction of the paper but unanimously agree that, in its current version, it is not strong enough to justify publication at ICLR. There was no rebuttal from the authors to consider. 
The authors propose an algorithm that learns sparse patterns of images that are highly predictive of a target class, even if added to a non target class. The reviewers agree that the algorithm is novel, is tested on a wide array of experiments, and the paper well written.  Unfortunately, it seems that some of the main claims, such as DNNs trained on clean data "learn abstract shapes along with some texture", resort to qualitative evaluation of the few examples shown in the paper. Furthermore, two reviewers were concerned with how one particular design choice in the algorithm might bias the authors  claims. In particular, pointed out that the patterns learned are highly to the initial canvas used, which is not necessarily strongly motivated.   As these two issues are integral parts of the paper, I hesitate to recommend Acceptance at this point.  That said, the approach looks very promising and I hope the authors continue to pursue this idea.
Clarity: Well written paper with a clear contribution statement; related work is up to date; concise algorithm description and corresponding theoretical guarantees. However, the presentation could be still improved.  Significance: The polynomial running time guarantee makes the practicality of the proposed algorithm marginal. Experimental results do not back up strongly the significance of the algorithm.  Main pros:   Solid theoretical work on the distributed CSSP problem.  Main cons:   The reviewers point out the significance of the experimental results, beyond the theoretical contribution. For the ICLR audience, real, large scale experiments, that really dictate that the proposed solutions is (if not the only) one of the few solutions to follow, are necessary. The reviewers highlight that the theoretical results need to be applied to really large scale scenarios (e.g., the problems considered in the paper can definitely be handled by a single computer, and no distributed implementation is required).    Τhe polynomial time complexity of the method makes the proposed protocol hard to be useful in real applications.   How does the distributed protocol compares with a centralized one? This is not fully addressed in the rebuttal. 
In this work, the authors develop an improved generalization bound for stochastic optimization algorithms. Reviewers agree that the theoretial results are significant. Several reviewers had concerns about the lack of experimental validation, which the authors addressed during the discussion phase. Other more minor concerns were also adequately addressed by the authors. The final recommendation is therefore to accept.
This paper presents an approach for learning disentangled static and dynamic latent variables for sequence data. In terms of learning objective, the paper extends Wasserstein autoencoder to sequential data, and this approach is novel and well motivated; the aggregated posterior for static variables comes out naturally and plays an important role for regularization (this appears to be new for sequence data). The authors also studies how to model additional categorical variables for weakly supervised learning in real scenarios. The main steps (generation and inference) were illustrated by graphical models with clarity, and rigorous statements are provided to back them up. Experimental results demonstrate the advantages of proposed method, in terms of disentanglement performance and generation quality.  The reviewers think this paper makes nice contributions to the sequential generative model community.
The paper addresses an important unsolved problem, i.e. deriving explainable features for use in graph classification. It does it by providing: i)  a simple to implement (local) node aggregation approach; ii) some theoretical support to the proposed approach; iii) empirical evidence that the proposed approach could be effective.  Notwithstanding the above merits, the reported work seems to still be in a preliminary phase. In fact: i) reference to literature is missing some important recent contributions to the addressed problem (e.g.  Gated Graph Sequence Neural Networks, GNNExplainer);  if possibile, also experimental comparisons vs those approaches is desirable; ii) experimental results do not provide a solid evidence that the proposed approach can really help to provide a clear explanation of the output, and the overall performance in classification is mostly below SOTA models; adding more datasets could help to give a more solid support to the main statement about explainability/performance; iii) presentation needs to better highlight the original contribution w.r.t. relevant literature (which is not completely clear in the current version of the paper), to improve the explanation of proofs, to discuss (both from a theoretical and empirical perspective) some important issues, such as computational scalability with the increase of size of local structures, and robustness to noise of the proposed (local) aggregation method.  In summary, although the proposed approach seems to be of some value, more work is needed to better place the proposed approach in the context of current literature and to gain a stronger experimental support to the main claim of the paper w.r.t. explainability.
 This paper has been reviewed by four knowledgeable referees. Two of them slightly leaned towards acceptance, whereas the other two suggested rejection. The main issues raised by the reviewers were (1) limited novelty [R1,R2], (2) missing baselines and ablations [R1,R3], (3) limited insights on the spectral analysis [R2], and (4) missing motivation behind modeling choices [R1,R3]. The rebuttal included a number of experiments requested by the reviewers (e.g. ablation with diffusion only [R1,R3], extended Diffusion GCN [R1], APPNP baseline [R3]), and adequately motivated some of the modeling choices.   The central question of the reviewers  discussion was whether the contribution of this paper was significant enough or too incremental. The discussion emphasized relevant literature which already considers multi hop attention (e.g. https://openreview.net/forum?id rkKvBAiiz [Cucurull et al.], https://ieeexplore.ieee.org/document/8683050 [Feng et al.], https://arxiv.org/abs/2001.07620 [Isufi et al.]), and which should have served as baseline. In particular, the experiment suggested by R3 was in line with some of these previous works, which consider "a multi hop adjacency matrix " as a way to increase the GAT s receptive field. This was as opposed to preserving the 1 hop adjacency matrix used in the original GAT and stacking multiple layers to enlarge the receptive field, which as noted by the authors, may result in over smoothed node features. The reviewers acknowledged that there is indeed as slight difference between the formulation proposed in the paper and the one in e.g. [Cucurull et al.]. The difference consists in calculating attention and then computing the powers with a decay factor vs. increasing the receptive field first by using powers of the adjacency matrix and then computing attention. Still, the multi hop GAT baseline of [Cucurull et al.] could be extended to use a multi hop adjacency matrix computed with the diffusion process from [Klicpera 2019], as suggested by R3. In light of these works and the above mentioned missing baselines, the reviewers agreed that the contribution may be viewed as rather incremental (combining multi hop graph attention with graph diffusion). The discussion also highlighted the potential of the presented spectral analysis, which could be strengthened by developing new insights in order to become a stronger contribution (see R2 s suggestions).   To sum up, this was a very discussed paper, where the reviewers ultimately reached a consensus to reject, with no strong opposition. I agree with the reviewers  assessment and therefore must reject. I encourage the authors to follow the reviewers  suggestions and consider the multi hop baselines as well as the hints provided by the reviewers about the spectral analysis to strengthen their work. 
Pros:   Provides a practical technique which can dramatically speed up PDE solving   this is an important and widely applicable contribution.   Paper is simultaneously clearly written and mathematically sophisticated.   The experimental results as impressive.  Cons:   There were concerns that the paper lacks novelty compared to Li et al 2020b, where the underlying theoretical framework was developed. The primary novelty would seem to be:     using Fourier transforms as the specific neural operator     the strength of the experimental results  Overall, I recommend acceptance. I believe the techniques in this paper will be practically useful for future research.
All the reviewers agreed that the paper lacks novelty. The overall framework is based on FOMM (Siarohin et al. 2019); the mixing operation is similar to CutMix (Yun et al. 2019). The improvements over the prior work are very subtle. R1 and R3 mentioned the paper is not well written. The rebuttal didn’t change the reviewers’ mind. In particular, the reviewers pointed out that only one out of six videos showed clear visual improvement in the added video results. After reading the paper, reviewer’s comments, the rebuttal with added results, the AC agrees with the reviewers that the paper is not ready for publication.
The reviewers all agreed on accepting this paper, stating that it makes a compelling point about the usefulness of saliency methods to diagnose generalization.  The reviewers found that the experiments were a strong point and applauded the thorough hyperparameter tuning and re runs for statistical significance.  One reviewer commented that the paper was too dense with information, so much so as to make it difficult to digest.  However, overall this seems like an interesting paper that is relevant to the community and will hopefully foster some good discussion about the shortcomings and future directions of saliency methods.
This paper received borderline negative scores. The reviewers all agree that the proposed approach is interesting. However, there are also common concerns around the clarity of the paper, as well as lacking sufficient empirical evaluation. One reviewer also argues that technical contribution is relatively limited. The author responses were taken into account but it didn t manage to swing the reviews. Therefore, I recommend reject and wish the authors can incorporate the feedback in the revision. 
The paper considers an attack of the recently proposed InstaHide algorithm mixing up public and private images by convex combination to achieve security of sensitive data. The paper formulates the problem as a multi task phase retrieval problem with missing data, and shows that under Gaussian data distribution setting, we can recover a small number of private data samples given sufficiently large dimensionality and number of synthetic samples output by InstaHide.  Theoretically, the Gaussian data distribution is quite restrictive in practice, but it could be a good start. The paper also uses some novel techniques in the analysis, which meets the technical standard of ICLR. The reviewer mainly concerns about the general motivation and formulation of "security" studied in the paper, since attacks can be trivial in practical scenarios where data is non Gaussian, which reveals a possible weakness on practical value of this work.   Although the work is probably better suited for a theoretical oriented conference, I nevertheless feel it should be also acceptable for ICLR because it specifically addresses a recent distributed learning problem and the results are non trivial and improving our understanding of the InstaHide s security.
All the reviewers agree that this paper was poorly written, which I agree upon my own reading of this paper. Section 1 is rather telegraphic and difficult to comprehend. Section 2 is cryptic in several respect, including what space of probability distributions the authors consider the Wasserstein distance, what QR task the objective function (5) for the discriminator corresponds to, especially after letting $a +\infty$ and $b  \infty$, and so on. The numerical experiment results do not seem convincing enough to demonstrate advantage of the proposal over existing methods. The authors did not respond to the reviews, so that many concerns raised by the reviewers have not been resolved. I would thus recommend rejection of this paper.
The paper treats a relevant and challenging problem in sequential learning scenarios   how to detect distributional change over time when the pre  and post change distributions are not known up to certainty. All reviewers more or less acknowledge that the paper presents a new approach towards solving this inference problem, where the high level idea is to approximately learn the pre  and post change distribution parameters online using gradient descent and then apply well known tests for change detection (e.g., the Shiryaev or CUSUM rules) with these assumed to be the pre  and post change parameters.  However, beyond the concerns expressed by the reviewers, my finding after going through the manuscript myself is that the presentation of the paper s results leaves a lot to be desired in terms of clarity of exposition, comprehensiveness of performance benchmarking and comparison to existing approaches. Despite some of the reviwers  scores being revised upwards, the overall evaluation of the paper according to me is not adequate to merit acceptance, as per the concerns listed below.   1. There are two settings assumed in the paper (beginning of Sec. 2): (a) a completely Bayesian one, with the pre  and post change distributional parameters drawn from a prior \cal{F} and the change time lambda drawn from a prior pi, and (b) a minmax one, where everything is the same as in (a) except that there is no prior over the change time lambda. However, it is not at all clear, in the algorithm design of the paper, where the prior \cal{F} over the distributions is used in computing (or approximating) conditional probabilities such as P[lambda | v_alpha   n].  2. There seem to be meaningless (or ill defined) expressions in the paper s crucial portion motivating the algorithm design, such as P(X_t ~ f_{theta_0} | v_alpha   n), P(X_t ~ f_{theta_1} | v_alpha   n). It is hard to understand what the event "X_t ~ f_{theta_0}" even means   I find it impossible to relate it to a sample path property. This leads me to question the validity of the technical development in the paper.  3. Another undefined term is "r quickly" in eq. (4); I had to dig through the classical work of Lai, and Tartakovsky Veeravalli to get a formal definition for this term. This is not to be expected of a paper that attempts to develop a new change point detection procedure from scratch, especially to an audience (ICLR) that may largely be unfamiliar with classicalt change detection theory.   4. There are several technical statements made without adequate formal proof, e.g., "Given the optimal stopping time \nu, it s possible to evaluate the posterior distribution of the change point P(lambda t | v_alpha n), which in turn is a good classifier of the pre and post change observation". What the precise meaning of the term "classifier" is, what its "goodness" is, and how exactly it is related to the posterior distribution of lambda given the value of v_alpha, is formally not spelt out for a paper that largely uses formal probability language to develop its main results.  5. While I understand that the final algorithm to detect the change involves several approximations and heuristics along the way, which may very well be intuitively appealing, I do not understand (even after repeated passes over the submission) several key aspects   a concern also expressed by Reviewer 3. Why is it reasonable to assume that the conditional distribution of the change time lambda given the algorithm s stop time v_alpha would be logistic, and with the specific parameters mu and s given in the section "Distribution Approximation"? Moreover, it is hard to discern from the crucial Section 3.2 why the functions f_0^n, f_1^n should be useful in practice as proxies to the actual expected log likelihood ratios under the true parameters   despite Lemma 2 showing that they converge to the true expectations (again, the sense in which this convergence occurs is omitted leading to imprecision in the statement), the rates as a function of n, t_2 may be slow. I agree in this regard with the same concern voiced by Reviewer 1, and do not see a satisfactory explanation to it in the paper s discussion.  6. Comparison to literature. Contrary to the general picture painted in the paper about the lack of sufficient investigation of the "unknown pre and post change parameter" case, there does seem to be a rigorous body of work existing in this line that is not discussed in the manuscript. For instance, "SEQUENTIAL CHANGE POINT DETECTION WHEN THE PRE  AND POST CHANGE PARAMETERS ARE UNKNOWN", Lai and Xing, 2009, and "A BAYESIAN APPROACH TO SEQUENTIAL SURVEILLANCE IN EXPONENTIAL FAMILIES", Lai Liu Xing, 2009, are both works that address this very setting and in a comprehensive manner with theoretical guarantees. What the current manuscript does, in the context of both these works, is highly unclear. Is it trying to suggest an approximate way of computing the natural posterior distribution of the change time lambda given all data up to now, using the proxy P(lambda | v_alpha   n), or using a completely different approach altogether, is not adequately discussed at all, which makes the motivating arguments for the algorithm vague.  7. Finally, but in no lesser measure, the Experimental Results section features a rather narrow set of (two) scenarios for which it presents numerics. For a paper that claims to demonstrate "experimental results (over a wide variety of settings)" [from the author response], this is quite telling as it renders the argument in favor of the paper s approach quite weak. Here again, for the first (synthetic) setting, I do not understand the relevance of the neural network adopted to fix the parameters of a Gaussian distribution. Moreover, the reported distributions of the "regretted detection delay" seem to be quite wide for all the approaches compared (unknown params, adaptive, GLR), precluding a reasonable comparison of their performance. The author(s) would do well to expand the scope of both synthetic and non synthetic experiments to show the validity of their approach, and in each case carry out many more independent trials than just 500 for more accurate benchmarks.  I do note that more experimental results have been reported in the appendix, but I would presume that they have more value being in the main body after the algorithm design is explained in a more succinct and clearer manner. This can only come about by a significant rewriting and reorganizing of the paper, which I am confident the author(s) can carry out in order to make this into a much stronger submission. I wish the author(s) good luck on this, and hope to see the strengths of this new approach brought out in a more impactful manner in the next revision. 
This paper proposes to employ affinity cycle consistency(ACC) for extracting active (or shared) factors of variation across groups. Experiments shows how ACC works in various scenarios.  Pros:   The problem is important and relevant.   The paper is well written.   The proposed method is simple and effective.  Cons:   The experimental section is weak:  It lacks an ablation to validate the contribution of ACC and discussion on   why the method works and the scalability of the proposed method to more complex cases.   The novelty is limited because the proposed ACC is similar to previous work temporal cycle consistency(TCC).   The paper missed some implementation details and could be difficult to reproduce without code  provided.  Reviewers raised the concerns listed in Cons. The authors conducted additional experiments and added more discussions on the experimental results in the revised paper. The authors also explained that ACC is more general than TCC. However, the reviewers were not convinced by the rebuttal and kept their original ratings.  Due to the two main weaknesses   limited novelty and weak experimental analysis, I recommend reject.
Many concerns raised by the reviewers have been addressed by the authors, sometimes through additional experiments. The reviewers have updated their scores in response, and all now recommend acceptance.  Like Reviewer 4, I think that the relation to nested dropout (Rippel et al. 2014) needs to be acknowledged and discussed appropriately, so I encourage the authors to carefully consider the reviewers  most recent comments about this when preparing the final version of the manuscript.  I disagree somewhat with Reviewer 3 that the motivation provided for this work is insufficient; controlling the quality/speed trade off at inference time seems like a compelling application. So does progressive generation, as suggested by Reviewers 3 & 4. I appreciate that this is highly subjective, however. Perhaps a few more concrete examples of practical situations where such trade offs are useful could be mentioned in the introduction.
While all reviewers agree that the topic is interesting and the work has merit, several issues have been pointed out, especially by R1 and R3, that indicate that the work is not  ready for acceptance at this stage. the authors are strongly encouraged to continue to work on this topic, taking into account the feedback received.
The paper proves new rates of convergence for stochastic subgradient under an interpolation condition. The analysis is rather simple but it produces better rates than previously known, which all reviewers agree is interesting. As pointed out by the reviewers, this work has the potential to help the community better understand optimization with over parametrized neural networks (where convexity or other related assumptions play a role).  To the authors, please add a citation to Pegasos as requested by the reviewers.  
The paper proposes a method to generate attention masks to interpret the performance of RL agents. Results are presented on a few ATARI games. Reviewers unanimously vote for rejecting the papers. R1, R3 give a score of 5, whereas R4, R5 give a score of 4. Their concerns are best explained in their own words:   R1 says, "The use of attention maps to analyze and explain deep neural networks is not new in itself, and learning attention maps to improve vision tasks is not new either."  R3 says, "the analysis of the learned attention masks seems selective. Some automatic metrics or systematic studies of different game categories (shooting, maze like, and ball and paddle) may shed light on the learned attention s general property."  R5 says, "I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive"  In their rebuttal, to address R1 s concern authors suggested that the use of attention on both value and policy networks is novel. This is not sufficient, because it does not show why such attention maps are more useful than ones proposed by prior work. As suggested by reviewers, a systematic study or a human study clearly showing that the proposed method adds more interpretability is critical. However, this is missing.  In response to R3, the authors provided experiments on more games. But this is not the point   because it s not about the number of environments in which experiments are provided, but rather the nature of the analysis that is performed. Finally, R5 comments that it s unclear whether attention actually provided interpretability or not.   Due to the lack of convincing analysis that demonstrates the utility of the proposed method in advancing the understanding of decisions made by RL agents, I recommend that the paper be rejected.  
The authors proposed a meta learning framework for NAS, namely MetaD2A (Meta Dataset to Architecture), that can stochastically generate graphs (architectures) from a given set (dataset) via a dataset architecture latent space learned with amortized meta learning. Each dataset is encoded via a set encoder and the architecutres are obtained via a graph decoder. MetaD2A is trained once on a database consisting of datasets and pretrained networks and can rapidly search a neural architecture for a novel dataset. While the set encoder and graph decoder for NAS have been introduced by existing work, the main contribution of the paper is to show that the meta learning of a "dataset conditioned architecture generation" framework can enable fast generation of a good architecture without training on the target dataset. The proposed method is interesting and effective, however it requires an existing pool of good architectures for a given task, which may limit its applicability to a diverse set of real world problems. I strongly encourage the authors to include experiments on a larger pool of architectures than the NAS Bench 201 search space to show the strength of their proposed method in generating good architectures. While training MetaD2A with pairs of MetaImageNet and randomly sampled graph shows that the proposed framework can generate graphs with different types of edges, it doesn t show that it can successfully meta learn to produce better architectures for a new task from an existing pool of good architectures.   We believe that many of the reviewers comments were addressed in the rebuttal, so while the scores are low, they do not reflect neither the contribution nor the reviewers opinion well (e.g., R3, in his last post, seems to suggest that his review should be updated but it has not happened).
The paper proposes a fast, nearly linear time, algorithm for finding a sparsifier for general directed and undirected graphs that approximately preserves the spectral properties of the original graph. The reviewers appreciated the main contribution of the paper, but they were concerned about the correctness and clarity of the paper, as well as the relevance of the contribution to machine learning. Following the discussion with the authors, the reviewers still felt that these concerns had not been fully addressed by the authors  responses and the subsequent revision of the paper. After taking these concerns into account as well as evaluating the paper relative to other ICLR submissions, I recommend reject.
This is a promising idea, without enough empirics to substantiate its potential utility, and also with a lack of clarity on the importance of the outlined task itself (fold based rather than structure based conditional sequence generation). There remain concerns about the lack of a more comprehensive comparison to methods for structure to sequence (e.g. Ingraham was added during the revision but only in a limited capacity), or easy generalizations of them, and about the quality of some of the presented results. Additionally, the concern about sensitivity to rotational invariances, and related issues wrt the fixed size cubic grid were not satisfactorily addressed. As a side note, the quality of the manuscript in terms of scholarliness of presentation was overall lacking.
This paper proposes Adversarial Feature Desensitization (AFD) as a defense against adversarial examples. Specifically, following the spirit of GAN and Adversarial Domain Adaptation, an adversarial discriminator is introduced to distinguish clean and perturbed inputs at the representational level.   This paper receives 3 reject and 1 accept recommendations. On one hand, though the proposed method shares some similarity with the Feature Scattering method at a high level, most of the reviewers still find the proposed method is interesting. The AC also agrees that the paper s organization and typos does not warrant a rejection.   On the other hand, the reviewers have also raised a few concerns. (i) A more careful discussion on the scalability of the proposed method is needed. (ii) Experiments are mostly focused on small datasets, while results on ImageNet is lacking, which makes the paper less convincing. The authors claim that they are trying to at least run Tiny ImageNet experiments; however, this set of results are not provided by the end. (iii) A more detailed analysis and visualization on the learned difference between the distributions of benign and adversary representation is needed, since a discriminator is learned here.   The rebuttal unfortunately did not fully address the reviewers  main concerns. On balance, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere. 
This paper proposes an MCTS approach to goal conditioned planning, where the search generates high level sequences of subgoals for low level policies. This top level planner is basically a search based implementation of SSST for potential gain in computational requirements with the help from the advanced search techniques behind PUCT that combines MCTS and prior information.   Reviewers generally agreed that this is an interesting and novel approach to planning and reinforcement learning. However, reviewers generally expressed that the experiments fall short to convince readers that this technique has greater impact and potential for a wider range of applications, other than GridWorld like environments. Authors are encouraged to provide a more extensive set of experiments, adding more variety to the domains and ablation studies such as the impact of incorrect prior on the overall search performance.  
The paper touches upon the problem of catastrophic forgetting in continual learning. The idea is to enhance experience reply by explanations of the decision/predictions made. Technically, this "Remembering for the Right Reasons" loss adds an explanation loss to continual learning. This is an interesting idea as also the reviewers agree on. I would like to encourage the authors to have consider a different abbreviation. RRR also stand for "Right for the Right Reasons" loss due to Ross et al.; the authors should use a different abbreviation and also mention the work of Ross et al. (Andrew Slavin Ross, Michael C. Hughes, Finale Doshi Velez: Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations. IJCAI 2017: 2662 2670). Moreover, it might actually be interesting in moving towards interactive learning here as well, because continual learning may also suffer from confounders. Moreover, there is also a connection to HINT (Ramprasaath Ramasamy Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry P. Heck, Dhruv Batra, Devi Parikh: Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded. ICCV 2019: 2591 2600) as it also aims at keeping explanations close to each other. Indeed, they use a ranking loss and do not consider continual learning. Overall, a simple method that is shown empirically to help improving existing replay methods for class incremental learning.
This paper studies the problem of learning from data that have been corrupted by label noise. The authors define a natural data dependent noise condition, that allows the noise rate to be large close to the decision boundary, and provide a simple iterative method that eventually converges to the Bayes optimal classifier. The method is evaluated on both synthetic a real datasets. There was a consensus among the reviewers that this is an interesting contribution and I propose acceptance.
This paper presents an extension of the neural ODE approach to include discrete changes in the continuous time dynamics. All reviewers agree the contribution made by this paper is worth publishing. Most of the reviewers  concerns have been answered in the rebuttal and I therefore recommend accepting this paper. 
This paper studies the relationship between adversarial transferability and knowledge transferability. It develops two metrics to measure adversarial transferability and a theoretical framework to justify the positive correlation between adversarial transferability and knowledge transferability. Synthetic experiments show that adversarial transferability measured by the proposed metrics indicates knowledge transferability.  While the paper studies an interesting and fundamental problem, with a sound theoretical analysis and a clear presentation, reviewers still have several reservations to directly accept it.   Lack of interpretation. How this observation can be used to gain better understanding of either fields of adversarial examples or knowledge transfer?   Lack of inspiration. How the insights can lead to better transfer techniques, apply to practical applications, and foster future research?   Lack of justification. Why such definitions of metrics are the intrinsic ways of measuring adversarial transferability? How well do they correlate with the practical experience with advanced attack, defense, and transfer methods?  AC believes the endeavor made by this paper towards a fundamental problem is highly necessary to our field. But given the above reservations, AC would encourage the authors to further strengthen their work to make it more inspiring and useful.
Dear authors,  the paper contains many interesting and novel ideas. Indeed, tuning step size is very time and energy consuming, and deriving and analyzing new adaptive algorithms has not only theoretical benefits but, more importantly, is a key when training more complicated ML models.  The paper contains many weaknesses as noted by reviewers. I know that you have addressed many of them one of the reviewers is still concerned about the other issues involving Theorem 1 and the assumption of the bounded preconditioner. He thinks the preconditioner bound is troublesome. In the overparameterized regime, he would expect the gradients to become near zero as the algorithm converges, which would actually cause the preconditioner to NOT be bounded below. It seems that the analysis might actually improve if the authors abandoned AMSGrad/Adam and instead just considered SGD for which the preconditioner assumption is not an assumption but just a property of the algorithm.      Thank you  
This paper proposes and investigates an approach for audiovisual synthesis based on the so called exemplar autoencoders.  The proposed approach is shown to be able to convert an audio input to audiovisual outputs using only very small amount of training data.  All reviewers consider the paper interesting with a lot of potentials in a variety of applications and appreciate the novelty of the work in this domain.  But there are also concerns on the technical presentation and the quality of the samples in the demo.  The authors addressed most of the concerns in the rebuttal but agreed that the quality of the results still had room for further improvements.  Overall, the work presented is interesting. The paper can be accepted. 
This paper proposes to do a fine grained analysis of how shape and texture play a role in the decisions made by CNNs. Lots of recent evidence suggests that CNNs exhibit a texture bias, and there has been considerable effort in understanding where this comes from and how to overcome it. The paper focuses in particular on understanding: (a) what fraction of the neurons are devoted to shape vs texture (roughly speaking), and (b) per pixel results using a convolutional readout function. The reviewers were divided at the time of submission and remained so at the end of discussion. At the end of discussion, the reviewers were split, with scores ranging from 4 (R1,R4), 7 (R2), and 8 (R3). The AC wants to thank and acknowledge the authors as well as all of the reviewers for their engagement in the discussion.    R2 and R3 are largely positive, driven by the extent of the experiments and the number of interesting results (e.g., how the fraction of the dimensions used for shape changes as a function of depth in the network). Both had smaller non critical concerns that were addressed (as far as the AC can tell) in the discussion.    R4’s most important concern, in the AC s view, is the question: could these results / different conclusions have been obtained via linear probe methods like Hermann et al.? The authors argue that analyzing the fraction of neurons used and at a per pixel is more fine grained than linear probes. This boils down to an intangible question of contribution, on which the AC is inclined to agree with the authors and R2 and R3: analyzing the dimensions contribute provides, at least to the AC, a complementary view to the linear probe and that this will be of interest to the ICLR community (although see final comment). R4 also had a number of smaller concerns that seem to be largely addressed (e.g., about correctness).   R1 argues primarily that the paper does not have a clear point or methodological contribution, for instance pointing out that readout modules were used in Hermann et al. or (as an example) arguing the readout function design is too simple.  The AC is inclined to agree with the authors’ response that the other reviewers seem to largely agree on the contribution (especially contributions via experiments rather than method) but disagree on how to weigh these contributions. The AC would also add that readout modules are a core idea for understanding neural representations that long predate Hermann and are by design (as the authors note) almost always as simple as possible.  At the end of the day, the AC is agrees with R2 and R3 for the contribution of the work and is inclined to accept. Given the other reviews, the AC does not agree with R1’s arguments, but would suggest that the authors think about how to sharpen their claims further. The AC is sympathetic to the concerns of R4, and urges the authors to think about a more concise and clean argument for R4’s concerns   many other readers will have similar concerns and as clean of an illustration will be helpful. Overall, the AC believes that the paper’s methods, experiments and analysis are of interest and value and is thus in favor of acceptance.
The reviewers were excited by this work, which focuses on lifelong RL in non stationary, non episodic environments.  They found the approach compelling with exciting results on the tested domains.  However, even the more positive reviewers were concerned with the somewhat narrow scope of evaluation, which makes the paper somewhat less ambitious.    In response to the reviews, the authors added extra experiments, clarifying text, and requested details that provide more depth and insight to the paper.  Still, the approach and paper is somewhat narrowly focused, but it does yield insights that should be useful for future works that solve this problem in a more general manner.
We want to acknowledge that there has been a tremendous amount of work done during the discussion period on this paper for clarifying multiple points, adding multiple new comparison methods and new analysis. This is a very different draft than what was submitted and the reviewers acknowledged that. The draft is much closer from acceptance at ICLR than it was at submission time. However, despite all those additions, we do not support a publication at ICLR.   The main issues with the current draft are its positioning and motivations.  Right now the draft is in between a paper about Knowledge Base Construction (KBC) and a paper analyzing the knowledge contained within a large language model. This in between came up in multiple places during the discussion and is what causes the biggest confusion around this work. And, since there is no clear choice, the draft has limitation on either side.  * If the main point is around KBC to build general purpose KBs, then one would expect experiments on downstream tasks powered by a KB, language understanding tasks for instance. Indeed, KBs are just a means to an end and the latest advances in very large language models have shown that KBs were not essential to be state of the art in language understanding tasks (GLUE, QA datasets, etc). So we would like to see whether these enhanced KBs could be beneficial. Or the KBs are studied as a way to encode commonsense like in (Bosselut et al., 2019) or (Davison et al., 2019), but this is not the point of the current draft. * If the main impact of the draft is around what the language models learn, bridging the deep language model and knowledge graph communities through enhanced model transparency, as it has been said in the discussion, then the discussion with (Petroni et al. 19) should be more prominent and the introduction, motivation and experiments of the draft should reflect that.  That s why, even if this work is of solid quality, the current draft can not be accepted.
The paper presents some exciting results on the convergence of averaged SGD for overparameterized two layer neural networks. The AC and reviewers all agree that the contributions are significant and well presented, and appreciate the author feedback to the reviews. The corresponding revisions on assumptions and references, and the added simplified proposition in the introduction have nicely improved the manuscript. 
This paper investigates safe reinforcement learning with distinct reward function and safety function. The authors present theoretical analysis and simulation results. The representation of safety is a critical step. The authors define the safety function values based on various events and use linear combination of them to construct safety score. Theoretical guarantees on safety and efficiency are presented. Simulation results also show safety and efficiency of the method.   This was a tricky case as the paper is borderline. Based on reviewers comments, we decided that the paper is not ready for publication in its current form and would benefit from another round revisions.  
This paper tackles the problem of long term time series forecasting. One challenge in long term forecasting is that often no sufficient date may be available. This paper proposes to use GANs to generate data that can be used to improve long range forecasts.  While reviewers agree that this paper presents an interesting idea towards tackling the problem of long term time series forecasting and appreciate the effort authors put forward in addressing their concerns and comments during the response period, the reviewers believe that in the current form paper is not ready for the publication. In particular:  1. Reviewers find that overall technical contribution of this work is limited compared to other submissions. 2. Comparison of LSTM and cWGAN GEP in the response is only on synthetic data, which does not address reviewers concerns.    
After the rebuttal stage, three of four reviewers recommend acceptance, and one gives a borderline score but argues they lean positive. Concerns seem well addressed; the method is simple yet effective.
This paper proposes a new variant of adaptive stochastic gradient method that has notable differences from Adam, and claims the advantage of adaptive variance reduction. While the algorithm construction looks novel, there are several concerns by the expert reviewers on the theoretical results of the paper, including lack of clarity and its guidance/relevance to the practical performance. There are also questions on the practical merit of the paper pointing to in limitations on the numerical experiments. I recommend rejection of the paper in the current form, and hope the reviews can help the authors to improve it in both theoretical and empirical aspects. 
This paper introduces the recall loss for dealing with imbalance training contexts. The authors propose to perform a class wise weighting of examples based on the instantaneous recall performance during training.   The reviewers like the clarity of the presentation, but raise several concerns regarding novelty of the approach, comparison to more baseline loss functions and state of the art methods.  The AC carefully reads the paper and discussions. The AC appreciates the discussion with respect to competitive loss functions, e.g. focal loss or segmentation loss approximations (SoftDice or Lovasz), which clearly highlights some limitations in existing approaches. \ However, the AC  considers that the approach is essentially a new way to setup the compromise between precision and recall. In that respect, the claims in the paper and in discussion regarding the performances of the proposed method are often exaggerated. For example in segmentation, the method obtaining the best accuracy is CB CE in 3 out of 4 experiments (on Syntia the mIOU improvement of the recall loss compared to CB CE corresponds to about the same drop in mACC). This is also verified on Fig 2 where CB CE outperforms the proposed method by a large margin on mean accuracy for small classes. For classification, the performance gains  compared to SDN (CE) are small. \ The AC thus considers that the paper in the current form falls short of the ICLR acceptance threshold. 
There was some slight disagreement on the paper, but the majority of reviewers agree that although some answers of the authors on questions brought good clarification, other issues still remain problematic. Some of the assumptions remain unclear (w.r.t CDTE), and reviewers still have doubts about the global convergence and weak stable fixed point concept, that lack clear math details. The experiments are also still a bit too immature, more comparison is needed, as well as an evaluation on other domains.
The authors propose an alternative fine tuning procedure by introducing a projection head and two new losses to be combined with the vanilla cross entropy loss. The authors introduce and jointly optimize the standard cross entropy loss, the contrastive cross entropy loss for classifier head and the categorical contrastive learning loss for projector head in an end to end fashion. The authors empirically confirmed that this setup compares favorably to existing baselines.  The reviewers found the setting challenging and worth investigating. The idea of exploring the intrinsic structure of the downstream task to help with fine tuning was deemed useful. The reviewers appreciated the thorough empirical validation. While the proposed approach was not yet explored in this specific context, most reviewers were concerned with the lack of novelty. In addition, there seems to be a large gap between the quality of exposition in the introduction and results section with respect to the rest of the paper which introduces confusion.   As it currently stands, the paper is not yet ready for publication and I will recommend rejection. To improve the manuscript the authors should incorporate the received feedback and significantly improve the exposition and justification of the proposed loss. In terms of empirical results, the authors should also explore alternative neural architectures to validate whether the proposed approach is general and whether the need for hyperparameter tuning arises. 
The equivalence rules for the margin are quite interesting, but I have two main concerns with the current paper (1) the theory does not seem to justify why increasing the number of negative examples helps in contrastive learning   in fact Table 9 shows that the bound gets smaller as K increases. (2) The experiments use a large batch size (N) but use a smaller number of negative examples (K), which does not reduce the computation cost by much. The theoretical issue (1) can be a matter of improving the writing to put less emphasis on the theory. I invite the authors to address these issues and resubmit to other ML venues.   Detailed feedback: I believe you are missing a negative sign in your definition of optimal "loss" $\mathcal{L}_{opt}$ in Eq. (3), which resulted in AnonReviewer4 s final comment.
The paper introduces a new method to probe contextualized word embeddings for syntax and sentiment properties using hyperbolic geometry. The paper is written well and relevant to the ICLR community. Reviewers highlight that the proposed Poincaré probe offers solid results, extensive experiments that support the benefits of the approach, and proposes a new approach to analyze the geometry of BERT models. The revised version clarified various concerns of the initial reviews and improved the manuscript (comparison to Euclidean probes, low dimensional examples, new results on edge length distributions etc.). Overall, the paper makes valuable contributions to probing contextualized word embeddings and the majority of reviewers and the AC support acceptance for its contributions. Please revise your paper to take feedback from reviewers after rebuttal into account (especially to further improve clarity and discussion of the method).
The paper proposes an insightful study on the robustness and accuracy of the model. It was hard to simultaneously keep the robustness and accuracy. A few works tried to improve accuracy while maintaining the robustness by investigating more data, early stopping or dropout. From a different perspective, this paper aims to improve robustness while maintaining accuracy.   There are some interesting findings in this paper, which could deepen our understanding of adversarial training. For example, the authors conducted experiments with different sizes of the network in standard training and adversarial training. The capacity of an overparameterized network can be sufficient for standard training, but it may be far from enough to fit adversarial data, because of the smoothing effect. Hence given the limited model capacity, adversarial data all have unequal importance. Though this technique is simple and widely studied in traditional ML, it is an interesting attempt in adversarial ML and the authors provide extensive experimental results to justify its effectiveness.   In the authors  responses, the concerns raised by the reviewers have been well addressed. The new version becomes more complete by including more results on different PGD steps and the insights on designing weight assignment function. Also, the authors gave an interesting discussion on enough model size for the adversarial training, though it is still kind of an open question. I would thus like to recommend the acceptance of this paper.  
The paper introduces MUSIC, a method for unsupervised learning of control policies, which partitions state variables into exogenous and endogenous collections and maximizes mutual information between them. Reviewers were uniformly positive, agreeing that the  approach was interesting and well motivated, and the experiments convincing. Some concerns were raised as to clarity, which were addressed through several revisions of the manuscript. I am happy to recommend acceptance.
This paper proposes using a neural network to learn an approximate solution for desired boundary conditions to accelerate the semiconductor device simulation. The work shows that speed up simulation is increased significantly. However, the major concern about this work is the limited contribution to the machine learning community as exposed by the reviewers. 
Before the discussion phase nearly all reviewers had doubts about the comparison of the current work with state of the art works (notably Yan et al., 2020, RetroXpert, and GraphRETRO). The authors then compared with these works and emphasized that these works rely on hand crafted features. They argue that the fairest comparison is the one where each method uses the same sort of features during train/test time. This is because in certain real world settings we may not have accurate estimates of such features (e.g., atom mappings, templates, reaction centers). However, in the revised version of the paper the authors did not adhere to this concept of fair comparison in Table 4 of Appendix A.4. Here their method uses reaction centers as input while baselines do not. While the authors claimed that the comparison here was designed to show how reaction centers provided as input improved performance, this doesn t seem like a good way to show it: to isolate the improvement due to reaction center inputs you should fix everything else, i.e., the rest of the method.   Apart from the above contradiction, I buy the arguments of reviewers that distinguishing between methods that use hand crafted features and those that do not is not a meaningful distinction. One can apply atom mapping or reaction center discovery algorithms as data preprocessing before applying other methods. Ablation studies where such preprocessing is added or removed are interesting, but it is completely fair for any method to use such preprocessing before applying their method, it is up to the modeller.   I would have argued for acceptance had the authors either (a) just included results from SOTA methods (one, RetroXpert was published 1 month after the ICLR submission deadline), and/or (b) reran their approach with such preprocessing. However the authors ended up hurting the submission by emphasizing a difference between using handcrafted features and not, then contradicting their experimental setup in Table 4.  This is a good paper, but I agree it is not ready to be accepted at ICLR. I recommend the authors do the following: (a) use any preprocessing they want for their method and compare with the state of the art, (b) if they want they can run their method without any preprocessing as an interesting ablation study, (c) remove Table 4 (as (b) already does this type of an ablation study), (d) describe recent work through the lense of EBM, (e) resubmit to a strong ML conference. The new submission will be much stronger.
The paper proposes to recalibrate predictive models by fitting a normalizing flow on top of the predictive model on a held out validation set using side information. At a high level this idea has some potential, especially in the multivariate setting, but there are several directions for improvement:    Comparison with a broader set of baselines as suggested by the reviewers     Clarity on why recalibrate with a normalizing flow especially in the 1 d case     Why not any other model with explicit density? Are there other important desiderata?     A motivating experiment that makes the potential value clear
The paper analyses several approaches to pruning at initialization, compared to after training. There was a large gap in reviewers appreciation of the paper, but I think that the pros outdo the cons as the paper show a lot of insights overall. I recommend accepting the paper.
This paper proposes a new source code modeling benchmark, with the unique twist being that we not only have code source text, but we also have build information, which allows extracting richer information to construct labels from. This enables, for example, a null pointer prediction task with labels coming from an inter procedural static analysis tool. AC and reviewers agree that this is a valuable framing for a benchmark suite. Unfortunately, it’s not clear that the benchmark in its current form delivers on the promise of the framing. Much of the interest and novelty is limited to just the one NullToken task, and reviewers raise a number of concerns including dataset size and whether the task truly measures the inter procedural reasoning that it sets out to measure. AnonReviewer2 raised some good questions here that the authors promised to address in a forthcoming comment, but that didn’t come before the discussion deadline. I’d encourage the authors to use the reviewer suggestions to more strongly establish that these tasks measure what they set out to measure, and also to consider adding other tasks that measure whether our ML models are capable of deeper / longer range reasoning. In total, there is a lot of potential here, but the work needs another iteration before it’s ready for publication.
The reviewers had a number of concerns:  not state of the art recommend analysis and comparison with [1] Temporal Shift Module writing needs to be improved appreciate the motivation for the paper, but needs more extensive experimentation.  Need larger scene related datasets.  We hope you find the reviewers  comments helpful as you revise the work.
This paper analyzes the implicit bias of gradient descent of infinite width 2 layer neural networks with ReLU activation. It is shown that the dynamics of gradient descent to optimize the 2 layer NN converges to the optimization dynamics on the random feature model in the infinite width limit. Then, it is shown that the gradient descent converges the minimal L2 norm solution from the initial parameters which yields regularization on a weighted integration of second order differentiation. Although this type of analysis has been given in the existing work, this paper gives its explicit form in 1 dimension input setting.  This paper reveals an interesting fact about the implicit regularization that would be educationally valuable. On the other hand, I should mention that there is room for improvement in its theoretical contribution and moreover its novelty is rather limited. 1. Although the explicit formulation of the implicit regularization is informative, the minimum norm bias itself is already pointed out by existing work and this work follows the line. Especially, regularization on the second order derivative has been already pointed out by previous work (although they are 1 norm regularization). 2. The logical jump from the original data to the adjusted data is still not convincing. It is explained that some numerical experiments show the linear term is negligibly small, which means the problems (15) and (17) are very close. However, this excuse does not make sense for this kind of "theoretical" work. The logic used here should be clarified to make the theoretical framework complete.  The evaluations by the reviewers indicate that this paper is on the borderline, and I also feel that some more additional strong point would be required so that this paper is accepted. I encourage the authors to go in this direction and make the analysis more detailed so that the theoretical framework would get more completed.  Minor comment: Theorem 4 overlaps the result given by the following paper [R1]. It is recommended that the relation and novelty compared with that paper is discussed.  [R1] E, W., Ma, C. & Wu, L. A comparative analysis of optimization and generalization properties of two layer neural network and random feature models under gradient descent dynamics. Sci. China Math. 63, 1235–1258 (2020).
This paper proposes a new method to combine non autoregressive (NAT) and autoregressive (AT) NMT. Compared with the original iterative refinement for non autoregressive NMT, their method first generates a translation candidate using AT and then fill in the gap using NAT.  All of the reviewers think the idea is interesting and this research topic is not well studied. However, the empirical part did not convince all the reviewers. The revised version and response is good; however, it still does not solve some major concerns of reviewers. 
The paper tackles the interesting area of cooperative multi agent learning and presents a promising method to make MAL robust to mistakes of teammates, while learning correlated equilibria. Reviewers find the presented setting and theoretical contributions limited and the experiments not extensive enough; also some technical details about the architecture are lacking, and the notation and writing can be substantially improved upon. As such the paper does not seem ready for publication at this stage.
The paper has been discussed by the reviewers that have acknowledged the rebuttal and the authors’ responses. However, the reviewers still had the following weaknesses and concerns (not solved post rebuttal):  * Expensive procedure (e.g., exhaustive enumeration before finding Pareto frontier) * The experiments should be more rigorous, with more realistic real world problems. * Missing comparison with baselines (unanimously acknowledged by the reviewers). * No explanations and insights provided as to why the method should work well * Clarity of the presentation  As a result, the paper is recommended for rejection. The detailed comments of the reviewers provide an actionable list of items to improve the paper for a future resubmission. 
This paper presents a refreshening insight into the classical idea of using external memory for reinforcement learning agents that learn and act in partially observable environments. The authors investigate a number of different memory architectures (Ok, OAk, Kk) and provide an insightful discussion on why we want to restrict the structure of the memory.   Reviewers generally appreciated the technical contribution of the paper, although not very convinced that this work will have a significant impact on future work. AC is also not sure about the conclusion drawn from the paper, where policies with external memory could have better sample complexity compared to rnn based policies. BTTT is computationally expensive, but it shall give better direction of which state to jump to, compared to the authors approach where the gradients are stopped at every timestep. So there should be pros and cons about this approach, and AC suspects that the sample complexity improvement actually comes from the fact that authors are explicitly limiting what can be stored in the memory, e.g. O or OA. This advantage can be broken in some other domains. AC admits that this is only a speculation at this point, but the motivation to use the external memory framework proposed in the paper needs to be more carefully investigated. 
This paper shows that linear layers can be replaced by butterfly networks. Put simple, the paper follows the idea of sketching to design new architectures that can reduce the number of trainable parameters and also gives the theoretical and empirical analysis to validate this claim. In this regard, the paper would be  appealing.  But the theoretical results given in this paper are incremental.
One referee supports acceptance, whereas three referees lean towards rejection. All referees agree that the idea introduced in the paper is interesting but find that the motivation and evaluation of the proposed aggregation functions could be significantly strengthened. The rebuttal addresses R1 s concerns about novelty and unfair comparisons, R2 s concerns about computational efficiency of the methods, R3 s concerns about motivation of the proposed approach and some missing baselines, and R4 s concerns about motivation. However, the rebuttal does not address the reviewers  concerns related to improvements achieved by the proposed approach, statistical significance nor appropriate comparison with SOTA. I agree with the reviewers that the paper tries to address a relevant problem and proposes interesting ideas, which are worth exploring. However, after discussion, the referees agree that further work should be devoted to strengthen the contribution. I agree with their assessment and hence must reject. In particular, I would strongly recommend to follow their suggestions to either provide strong theoretical motivation to support the claims of the paper or work on a strengthened empirical evaluation, following OGB guidelines to report the std of the results and including a proper comparison with the state of the art. 
While the results are promising, several concerns were raised in the reviews, leading to the reject recommendation at this time.  There is an agreement among all reviewers that the paper would benefit from a revision.  Most reviewers felt that the paper lacks a rigorous and compelling theoretical justification for the proposed algorithm, making suggestions for what would make the paper stronger.  AnonReviewer4 would like to additionally convey the following message:  I would like to thank the authors to revise the statement of the assumptions according to my suggestions. However, the wording "with high probability" has rigorous mathematically meaning, so should be used with care. Their Figures still don t justify the current statement of the assumptions in my opinion. Their experiment in Appendix E, which only contains one single example, is still too simplistic and cannot fully justify their claim. I would encourage the authors to test on more examples.
This paper is a study of neural network scaling, with models containing hundred of billions of parameters. To that end, the paper introduce a new module called GShard, consisting of annotations APIs on how to split computations across accelerators, which is integrated in the XLA compiler. This enables the training of models with hundreds billions of parameters. To scale efficiently to very large models, the paper proposes to use transformer networks, where every other feed forward sub layer is replaced by a sparse mixture of experts (similar to Shazeer et al. 2017). This model is then evaluated on a multilingual machine translation task, from 100 languages to English.  On the one hand, I believe that the contributions of the paper are significant: scaling to 600B parameters, and showing that this leads to better translation quality are important achievement. The analysis of transformer networks scaling could also have an important impact. Finally I think that GShard and its integration in XLA could be very valuable. On the other hand, I agree with some of the concerns raised by the reviewers, regarding the writing of the paper and the reproducibility. I found the paper not well written, and hard to identify the differences with previous work. As GShard is one of the main contribution, I would expect a better description of it in the main text (compared to the MoE which seems more incremental). Regarding reproducibility, I do not think that the authors provided a good reason not to evaluate on standard benchmarks: the test sets could be excluded from the train set through various deduplication heuristics.   To conclude, I am leaning toward accepting the paper, but believe it is borderline. The reason is that the contributions are significant, and worth publishing. But I would not oppose a rejection based on the reproducibility and writing issues.
This paper studies how to efficiently expose failures of "top performing" segmentation models in the real world and how to leverage such counterexamples to rectify the models. The key idea is to discover most "controversial" samples from massive online unlabeled images. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness.  However, there exists some limitations coming from R2 and R3, for example, 1) Segmentation benchmarks may not require pixel level dense annotation. There are also examples of benchmarks where the groundtruth consists of computer segmentations corrected by humans. 2) It is much harder for segmentation data to be class balanced in the pixel level, making highly skewed class distributions common for this particular task. 3) Citing the field of computer assisted annotation as relevant work.  In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed.
This paper is very interesting and timely, but as the reviewers note there is significant room for improvement in the clarity of the presentation and evaluation. In addition to the references mentioned by the reviewers, some other relevant references are the following:   [1] Evan Rosenman, Nitin Viswanathan, "Using Poisson Binomial GLMs to Reveal Voter Preferences," https://arxiv.org/abs/1802.01053  [2] Law, H. C. L., Sutherland, D., Sejdinovic, D., & Flaxman, S. (2018, March). "Bayesian approaches to distribution regression." In International Conference on Artificial Intelligence and Statistics (pp. 1167 1176). 
**Problem significance** This paper proposes an attack mechanism in the latent space of a neural network f(x), which produces out of distribution examples. The AC agrees reviewers on the significance of the OOD detection problem, particularly addressing the vulnerability aspect is relevant and of great interest to the community.   **Technical contribution** The AC shares the concern with several reviewers on the limited technical novelty as well as the problem formulation. While the authors have clarified the difference between adversarial attack vs. OOD attack, the underlying attack mechanism is not new to the community (except for allowing for a larger degree of search space without constrained by the visual imperceptibility). In some sense, the search is made easier than the standard adversarial attack by removing the similarity constraint. Given the unrealisticness of the created OOD examples (largely noisy patches), the AC thinks perhaps a more interesting problem is to look at naturally occurring OOD examples that would lead to the similar latent encoding w.r.t in distribution data, or adversarial robustness w.r.t the OOD detector.  This to me, would steer the community in the right direction.   From a problem formulation perspective, the AC thinks it s useful to differentiate three highly related attacks (that are distinct but can cause confusions):    adversarial attack w.r.t the classifier   OOD attack w.r.t the classifier    adversarial attack w.r.t the OOD detector (see recent works [1][2][3] which considered the robustness aspect of OOD detector)   **Rebuttal feedback** The AC recognizes the effort made by the authors to address the concerns and comments raised by reviewers. The AC agrees with R1/R2/R3 that the additional experiments are valuable, however, the changes to the manuscript are substantial enough to deem another round of review in the future venue. The paper can improve with better organization and presentation, moving the results in the appendix to the main paper.   **Recommendation** The AC recommends rejection.   References  [1] Sehwag et al. Analyzing the robustness of open world machine learning. 2019  [2] Hein et al. Why relu networks yield high confidence predictions far away from the training data and how to mitigate the problem. 2019  [3] Chen et al. Informative Outlier Matters: Robustifying Out of distribution Detection Using Outlier Mining. arXiv:2006.15207    
The paper is about a reinforcement learning algorithm that operates in a Constrained MDP and is provided with a baseline policy. Although the reviewers acknowledge that the paper has some merits (well written, clearly organized, significant empirical evaluation, reproducible experimental results), some concerns have been raised about the novelty of the proposed solution and of its theoretical analysis. The reviewers feel that the authors  responses have not properly addressed all their doubts. The paper is borderline and I think that it is not ready for publication in the current form. I encourage the authors to update their paper following the reviewers  suggestions and try to submit it in one of the forthcoming machine learning conferences. 
The focus of this paper is to analyze an end to end network to reconstruct matrices originating from non Euclidean data which are corrupted. The authors present an untrained network for this task. In the review period the reviewers raised a variety of concerns including concerns about novelty of the paper with respect to existing work, technical depth and clarity. The authors did not respond to these concerns. Therefore, I recommend rejection.
The paper introduces a novel dataset condensation technique that generates synthetic samples (images) by matching model gradients with those obtained on the original input samples (images). The authors also show that these synthetic images  are not architecture dependent and can be used to train different deep neural networks. The approach is validated on several smaller datasets like MNIST, SVHN and CIFAR10. This work is well motivated and the methodological contributions convincing. All reviewers were enthusiastic and indicated that there were no flaws in this work. The rebuttal clarified outstanding questions and made the paper stronger.
There was some positive consensus towards this paper, which slightly improved after the very strong author rebuttal. Reviewers, in general, appreciate the simplicity of the approach as well as its effectiveness. The most acute criticisms derived from several theoretical and technical points, similarity with [Mizadeh, 2020], and missing baseline comparisons. The author rebuttal responds to each of these points very clearly and convincingly, as well as with new experimental baseline comparisons that clearly demonstrate the effectiveness of the CPR approach. I encourage the authors to include the extensive comparison with [Mizadeh, 2020] provided in the rebuttal, especially given the similarity to the proposed approach. and to also tone down the strong claims of novelty in light of the similarities. 
The paper presents a bidirectional pooling layer inspired by the classical Lifting scheme from signal processing. LiftDownPool is able to preserve structure and details in different sub bands, whereas LiftUpPool is able to generate a refined up sampled feature map using the detail sub bands. This is very useful for image to image translation tasks and all tasks that involve up scaling. This is a solid contribution with extensive and thorough experiments and direct practical usage, clear accept. 
The paper considers the problem of learning to imitate behaviors from visual demonstrations, without access to expert actions. Consistent with recent approaches, the proposed method uses a neural network to measure the similarity between visual demonstrations and the agent s behavior, and employs this metric as a reward in RL. The primary contribution is the use of a recurrent siamese network that is trained to measure the distance between motions, as a means of better dealing with the challenges of imitation learning from a small number of (as few as one) noisy visual demonstrations. Experiments on a variety of simulated domains show that the proposed approach achieves reasonable results.  The paper was reviewed by four knowledgeable referees, who read the author responses and engaged in extensive discussion. The reviewers agree that learning to imitate behaviors from a small amount of noisy demonstrations is a challenging and important problem that is of significant interest. The proposed method nicely extends existing approaches to visual imitation learning, and the results reveal that the method performs well in a variety of continuous control domains. The reviewers raise several concerns regarding the clarity of the technical presentation and the sufficiency of the experimental evaluation. The authors have made a significant effort to address these concerns in their responses and updates to the paper, which the reviewers very much appreciate. However, some of the reviewers  primary concerns regarding clarity and the thoroughness of the experimental evaluation remain. This work has the potential to make a really nice contribution and the authors are encouraged to take this feedback into account for any future version of the manuscript.
Accept. The paper proposes Deformable DETR that builds on DETR and solves the slow convergence and limited spatial resolution problem while getting impressive results. The authors should think about comparing with other linear attention mechanisms to show the applicability of the method.